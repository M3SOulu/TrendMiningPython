,Unnamed: 0,Unnamed: 0.1,AuthorId,Q_id,Title,Abstract,Views,Answers,Cites,Tags_n,Tags,Date,CR_Date,LA_Date,Abstract_clean,Title_clean,Abstrat_without_stopwords,Title_without_stopwords,Merged_title_and_abs,Tokenized_data,Stem_data
0,0,0,19651049,73177651,Ansible Command Line output,"<p>I am having an issue learning Ansible and getting feedback from it.</p>
<p>Here is the code I am running</p>
<pre class=""lang-bash prettyprint-override""><code>clear &amp;&amp; sudo ansible-playbook patching.yml -u stackoverflow -k --ask-become-pass -v
</code></pre>
<p>Here is my patching.yml file</p>
<pre class=""lang-yaml prettyprint-override""><code>---
- hosts: all
  gather_facts: no
  become: yes

  tasks:

  - name: Patching
    apt: update_cache=yes force_apt_get=yes

  - name: Proof of Patching
    shell: apt -q -y --ignore-hold --allow-change-held-packages --allow-unauthenticated -s dist-upgrade | /bin/grep  ^Inst | wc -l
    register: output

  - debug: var=output
</code></pre>
<p>What I am trying to accomplish is one to learn Ansible and two to patch my various Linux machines with some sort of feedback that they have patched I am getting this feedback after reading stack overflow for some time. It is long and un-human-friendly I would like it to be just a # or even better hostname: #.</p>
<p>Thank You</p>
<p>-Jared</p>
",32,0,2,1,ansible,2022-07-30 20:12:22,2022-07-30 20:12:22,2022-07-31 12:36:43,i am having an issue learning ansible and getting feedback from it  here is the code i am running here is my patching yml file what i am trying to accomplish is one to learn ansible and two to patch my various linux machines with some sort of feedback that they have patched i am getting this feedback after reading stack overflow for some time  it is long and un human friendly i would like it to be just a   or even better hostname     thank you  jared,ansible command line output,issue learning ansible getting feedback code running patching yml file trying accomplish one learn ansible two patch various linux machines sort feedback patched getting feedback reading stack overflow time long un human friendly would like even better hostname thank jared,ansible command line output,ansible command line outputissue learning ansible getting feedback code running patching yml file trying accomplish one learn ansible two patch various linux machines sort feedback patched getting feedback reading stack overflow time long un human friendly would like even better hostname thank jared,"['ansible', 'command', 'line', 'outputissue', 'learning', 'ansible', 'getting', 'feedback', 'code', 'running', 'patching', 'yml', 'file', 'trying', 'accomplish', 'one', 'learn', 'ansible', 'two', 'patch', 'various', 'linux', 'machines', 'sort', 'feedback', 'patched', 'getting', 'feedback', 'reading', 'stack', 'overflow', 'time', 'long', 'un', 'human', 'friendly', 'would', 'like', 'even', 'better', 'hostname', 'thank', 'jared']","['ansibl', 'command', 'line', 'outputissu', 'learn', 'ansibl', 'get', 'feedback', 'code', 'run', 'patch', 'yml', 'file', 'tri', 'accomplish', 'one', 'learn', 'ansibl', 'two', 'patch', 'variou', 'linux', 'machin', 'sort', 'feedback', 'patch', 'get', 'feedback', 'read', 'stack', 'overflow', 'time', 'long', 'un', 'human', 'friendli', 'would', 'like', 'even', 'better', 'hostnam', 'thank', 'jare']"
1,1,1,12403204,73181667,aws sagemaker deploy machine estimation,"<p>I trained a machine learning model that predict some human faces and I want to deploy it to AWS SageMaker to process images in real time by using AWS Kinesis Vides Stream. But I am not sure to estimate pricing and performance of the SageMaker. In my use case I feed the SageMaker instance with 1000 real time cameras and it sometimes goes down 400 or sometimes increased 1200 and for that reason I do not know which type of machine should I use. The machine that deployed model can affected the delay of the stream? I measured the 3-3.5 seconds in one machine but if the cameras count go up, does the delay increase?</p>
<p>Note: The cameras produce 1080p video and the quality of the videos are also important.</p>
",8,0,0,5,amazon-web-services;machine-learning;computer-vision;data-science;amazon-kinesis-video-streams,2022-07-31 11:22:13,2022-07-31 11:22:13,2022-07-31 11:22:13,i trained a machine learning model that predict some human faces and i want to deploy it to aws sagemaker to process images in real time by using aws kinesis vides stream  but i am not sure to estimate pricing and performance of the sagemaker  in my use case i feed the sagemaker instance with  real time cameras and it sometimes goes down  or sometimes increased  and for that reason i do not know which type of machine should i use  the machine that deployed model can affected the delay of the stream  i measured the    seconds in one machine but if the cameras count go up  does the delay increase  note  the cameras produce p video and the quality of the videos are also important ,aws sagemaker deploy machine estimation,trained machine learning model predict human faces want deploy aws sagemaker process images real time using aws kinesis vides stream sure estimate pricing performance sagemaker use case feed sagemaker instance real time cameras sometimes goes sometimes increased reason know type machine use machine deployed model affected delay stream measured seconds one machine cameras count go delay increase note cameras produce p video quality videos also important,aws sagemaker deploy machine estimation,aws sagemaker deploy machine estimationtrained machine learning model predict human faces want deploy aws sagemaker process images real time using aws kinesis vides stream sure estimate pricing performance sagemaker use case feed sagemaker instance real time cameras sometimes goes sometimes increased reason know type machine use machine deployed model affected delay stream measured seconds one machine cameras count go delay increase note cameras produce p video quality videos also important,"['aws', 'sagemaker', 'deploy', 'machine', 'estimationtrained', 'machine', 'learning', 'model', 'predict', 'human', 'faces', 'want', 'deploy', 'aws', 'sagemaker', 'process', 'images', 'real', 'time', 'using', 'aws', 'kinesis', 'vides', 'stream', 'sure', 'estimate', 'pricing', 'performance', 'sagemaker', 'use', 'case', 'feed', 'sagemaker', 'instance', 'real', 'time', 'cameras', 'sometimes', 'goes', 'sometimes', 'increased', 'reason', 'know', 'type', 'machine', 'use', 'machine', 'deployed', 'model', 'affected', 'delay', 'stream', 'measured', 'seconds', 'one', 'machine', 'cameras', 'count', 'go', 'delay', 'increase', 'note', 'cameras', 'produce', 'p', 'video', 'quality', 'videos', 'also', 'important']","['aw', 'sagemak', 'deploy', 'machin', 'estimationtrain', 'machin', 'learn', 'model', 'predict', 'human', 'face', 'want', 'deploy', 'aw', 'sagemak', 'process', 'imag', 'real', 'time', 'use', 'aw', 'kinesi', 'vide', 'stream', 'sure', 'estim', 'price', 'perform', 'sagemak', 'use', 'case', 'feed', 'sagemak', 'instanc', 'real', 'time', 'camera', 'sometim', 'goe', 'sometim', 'increas', 'reason', 'know', 'type', 'machin', 'use', 'machin', 'deploy', 'model', 'affect', 'delay', 'stream', 'measur', 'second', 'one', 'machin', 'camera', 'count', 'go', 'delay', 'increas', 'note', 'camera', 'produc', 'p', 'video', 'qualiti', 'video', 'also', 'import']"
2,2,2,5236295,61010532,How can you specify local path of InputPath or OutputPath in Kubeflow Pipelines,"<p>I've started using Kubeflow Pipelines to run data processing, training and predicting for a machine learning project, and I'm using InputPath and OutputhPath to pass large files between components. </p>

<p>I'd like to know how, if it's possible, do I set the path that OutputPath would look for a file in in a component, and where InputPath would load a file in a component. </p>

<p>Currently, the code stores them in a pre-determined place (e.g. <code>data/my_data.csv</code>), and it would be ideal if I could 'tell' InputPath/OutputPath this is the file it should copy, instead of having to rename all the files to match what OutputPath expects, as per below minimal example.</p>

<pre class=""lang-py prettyprint-override""><code>@dsl.pipelines(name='test_pipeline')
def pipeline():
    pp = create_component_from_func(func=_pre_process_data)()
    # use pp['pre_processed']...

def pre_process_data(pre_processed_path: OutputPath('csv')):
    import os

    print('do some processing which saves file to data/pre_processed.csv')

    # want to avoid this:
    print('move files to OutputPath locations...')
    os.rename(f'data/pre_processed.csv', pre_processed_path)
</code></pre>

<p>Naturally I would prefer not to update the code to adhere to Kubeflow pipeline naming convention, as that seems like very bad practice to me.</p>

<p>Thanks!</p>
",2725,2,2,1,kubeflow-pipelines,2020-04-03 14:09:17,2020-04-03 14:09:17,2022-07-31 10:46:41,i ve started using kubeflow pipelines to run data processing  training and predicting for a machine learning project  and i m using inputpath and outputhpath to pass large files between components   i d like to know how  if it s possible  do i set the path that outputpath would look for a file in in a component  and where inputpath would load a file in a component   currently  the code stores them in a pre determined place  e g  data my_data csv   and it would be ideal if i could  tell  inputpath outputpath this is the file it should copy  instead of having to rename all the files to match what outputpath expects  as per below minimal example  naturally i would prefer not to update the code to adhere to kubeflow pipeline naming convention  as that seems like very bad practice to me  thanks ,how can you specify local path of inputpath or outputpath in kubeflow pipelines,started using kubeflow pipelines run data processing training predicting machine learning project using inputpath outputhpath pass large files components like know possible set path outputpath would look file component inputpath would load file component currently code stores pre determined place e g data my_data csv would ideal could tell inputpath outputpath file copy instead rename files match outputpath expects per minimal example naturally would prefer update code adhere kubeflow pipeline naming convention seems like bad practice thanks,specify local path inputpath outputpath kubeflow pipelines,specify local path inputpath outputpath kubeflow pipelinesstarted using kubeflow pipelines run data processing training predicting machine learning project using inputpath outputhpath pass large files components like know possible set path outputpath would look file component inputpath would load file component currently code stores pre determined place e g data my_data csv would ideal could tell inputpath outputpath file copy instead rename files match outputpath expects per minimal example naturally would prefer update code adhere kubeflow pipeline naming convention seems like bad practice thanks,"['specify', 'local', 'path', 'inputpath', 'outputpath', 'kubeflow', 'pipelinesstarted', 'using', 'kubeflow', 'pipelines', 'run', 'data', 'processing', 'training', 'predicting', 'machine', 'learning', 'project', 'using', 'inputpath', 'outputhpath', 'pass', 'large', 'files', 'components', 'like', 'know', 'possible', 'set', 'path', 'outputpath', 'would', 'look', 'file', 'component', 'inputpath', 'would', 'load', 'file', 'component', 'currently', 'code', 'stores', 'pre', 'determined', 'place', 'e', 'g', 'data', 'my_data', 'csv', 'would', 'ideal', 'could', 'tell', 'inputpath', 'outputpath', 'file', 'copy', 'instead', 'rename', 'files', 'match', 'outputpath', 'expects', 'per', 'minimal', 'example', 'naturally', 'would', 'prefer', 'update', 'code', 'adhere', 'kubeflow', 'pipeline', 'naming', 'convention', 'seems', 'like', 'bad', 'practice', 'thanks']","['specifi', 'local', 'path', 'inputpath', 'outputpath', 'kubeflow', 'pipelinesstart', 'use', 'kubeflow', 'pipelin', 'run', 'data', 'process', 'train', 'predict', 'machin', 'learn', 'project', 'use', 'inputpath', 'outputhpath', 'pass', 'larg', 'file', 'compon', 'like', 'know', 'possibl', 'set', 'path', 'outputpath', 'would', 'look', 'file', 'compon', 'inputpath', 'would', 'load', 'file', 'compon', 'current', 'code', 'store', 'pre', 'determin', 'place', 'e', 'g', 'data', 'my_data', 'csv', 'would', 'ideal', 'could', 'tell', 'inputpath', 'outputpath', 'file', 'copi', 'instead', 'renam', 'file', 'match', 'outputpath', 'expect', 'per', 'minim', 'exampl', 'natur', 'would', 'prefer', 'updat', 'code', 'adher', 'kubeflow', 'pipelin', 'name', 'convent', 'seem', 'like', 'bad', 'practic', 'thank']"
3,3,3,19639204,73181280,How to forecast log-out time based on the history of log-ins and log-outs?,"<p>I have a dataset of log-ins and log-outs of users. I want to train a machine learning model that for a given log-in time I will receive a log-out time. I have tried to use time series in order to achieve that but it didn't worked as I expected. I am searching for machine learning algorithms that can help me to forecast the log-out time of a user based on their log-in time and history of their log-ins and log-outs</p>
",8,0,0,5,machine-learning;artificial-intelligence;prediction;predict;forecast,2022-07-31 10:02:33,2022-07-31 10:02:33,2022-07-31 10:02:33,i have a dataset of log ins and log outs of users  i want to train a machine learning model that for a given log in time i will receive a log out time  i have tried to use time series in order to achieve that but it didn t worked as i expected  i am searching for machine learning algorithms that can help me to forecast the log out time of a user based on their log in time and history of their log ins and log outs,how to forecast log out time based on the history of log ins and log outs ,dataset log ins log outs users want train machine learning model given log time receive log time tried use time series order achieve worked expected searching machine learning algorithms help forecast log time user based log time history log ins log outs,forecast log time based history log ins log outs,forecast log time based history log ins log outsdataset log ins log outs users want train machine learning model given log time receive log time tried use time series order achieve worked expected searching machine learning algorithms help forecast log time user based log time history log ins log outs,"['forecast', 'log', 'time', 'based', 'history', 'log', 'ins', 'log', 'outsdataset', 'log', 'ins', 'log', 'outs', 'users', 'want', 'train', 'machine', 'learning', 'model', 'given', 'log', 'time', 'receive', 'log', 'time', 'tried', 'use', 'time', 'series', 'order', 'achieve', 'worked', 'expected', 'searching', 'machine', 'learning', 'algorithms', 'help', 'forecast', 'log', 'time', 'user', 'based', 'log', 'time', 'history', 'log', 'ins', 'log', 'outs']","['forecast', 'log', 'time', 'base', 'histori', 'log', 'in', 'log', 'outsdataset', 'log', 'in', 'log', 'out', 'user', 'want', 'train', 'machin', 'learn', 'model', 'given', 'log', 'time', 'receiv', 'log', 'time', 'tri', 'use', 'time', 'seri', 'order', 'achiev', 'work', 'expect', 'search', 'machin', 'learn', 'algorithm', 'help', 'forecast', 'log', 'time', 'user', 'base', 'log', 'time', 'histori', 'log', 'in', 'log', 'out']"
4,4,4,19473399,73177235,Can I fit and predict a dataframe with array elements in scikit learn?,"<p>I have a dataframe with 1D numpy array elements which are of different size and i need to try   different scikit learn machine learning models (such as KNN) but when I call the fit() function I recive: &quot;ValueError: setting an array element with a sequence.&quot;.
To get you an idea of my dataframe is made i put here an excel of it</p>
<p><a href=""https://i.stack.imgur.com/M1uod.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M1uod.png"" alt=""1"" /></a></p>
<p>and here is how I called the fit() function:</p>
<pre><code>y=df.drop(columns=['csiA','csiC','csiI'])
X=df.drop(columns=['activity'])
X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)
model = KNeighborsClassifier()
model.fit(X_train, Y_train)
#p_test = model.predict(X_validation)
#acc = accuracy_score(Y_validation, p_test)
</code></pre>
",33,0,0,4,python;dataframe;machine-learning;scikit-learn,2022-07-30 19:14:58,2022-07-30 19:14:58,2022-07-31 06:24:46, and here is how i called the fit   function ,can i fit and predict a dataframe with array elements in scikit learn ,called fit function,fit predict dataframe array elements scikit learn,fit predict dataframe array elements scikit learncalled fit function,"['fit', 'predict', 'dataframe', 'array', 'elements', 'scikit', 'learncalled', 'fit', 'function']","['fit', 'predict', 'datafram', 'array', 'element', 'scikit', 'learncal', 'fit', 'function']"
5,5,5,16300045,73162330,Configuring Machine learning model in Flask,"<p><strong>Project Overview:</strong></p>
<p>I was testing Linear logic algorithm to predict House pricing using the &quot;House Price prediction&quot; data from Kaggle and Flask app. The prediction works fine when executed separately but doesn't work when used with Flask. I'm getting the following error:</p>
<blockquote>
<p>&quot;ValueError
ValueError: Input X contains NaN.
LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See <a href=""https://scikit-learn.org/stable/modules/impute.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html</a> You can find a list of all estimators that handle NaN values at the following page: <a href=""https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values%22"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values&quot;</a></p>
</blockquote>
<p>I checked the data, and there are no null or NAN Values.
<a href=""https://i.stack.imgur.com/wSMrb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wSMrb.png"" alt=""enter image description here"" /></a></p>
<p>You can find the complete code with data and html files in my <a href=""https://github.com/arwavis/ML_PredictionsinHTML/tree/main/House_Price_Pred"" rel=""nofollow noreferrer"">github</a>.</p>
<p>Here is the <code>app.py</code> code:</p>
<pre><code># from requests import request
from flask import Flask, render_template, request
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

app = Flask(__name__)


@app.route(&quot;/&quot;)
def root_page():
return render_template('index.html')


@app.route(&quot;/predict&quot;)
def form_page():
return render_template('forms.html')


@app.route(&quot;/form&quot;, methods=[&quot;GET&quot;, &quot;POST&quot;])
def predict_page():
no_bed_room = request.form.get(&quot;bedroom&quot;)
no_bath_room = request.form.get(&quot;bathroom&quot;)
square_feet_living = request.form.get(&quot;sqftliv&quot;)
square_feet_lot = request.form.get(&quot;sqftlot&quot;)
no_of_floors = request.form.get(&quot;floors&quot;)
water_front = request.form.get(&quot;waterfront&quot;)
no_of_views = request.form.get(&quot;view&quot;)
condition = request.form.get(&quot;condition&quot;)
square_feet_above = request.form.get(&quot;sqftabv&quot;)
square_feet_basement = request.form.get(&quot;sqftbas&quot;)
year_built = request.form.get(&quot;yrbuilt&quot;)
year_renovated = request.form.get(&quot;yrrenov&quot;)
city = request.form.get(&quot;city&quot;)

# return f&quot;Test Output {no_bed_room}+{no_bath_room}+{square_feet_living}+{square_feet_lot}+{no_of_floors}+{water_front}+&quot; \
#        f&quot;{no_of_views}+{condition}+{square_feet_above}+{square_feet_basement}+{year_built}+{year_renovated}+{city}&quot;
df = pd.read_csv(&quot;data.csv&quot;)
# print(df)

# *************************** DATA PROCESSING **********************************
# Removing the columns that are not required for the analysis
df.drop(df.columns[[0, 14, 16, 17]], axis=1, inplace=True)
df = df.reset_index()

# Converting categorical variables of the dataset into numerical variables - using ONE HOT ENCODING technique

df['city'] = df['city'].apply(
    {'Shoreline': 0, 'Seattle': 1, 'Kent': 2, 'Bellevue': 3, 'Redmond': 4, 'Maple Valley': 5, 'North Bend': 6,
     'Lake Forest Park': 7, 'Sammamish': 8, 'Auburn': 9, 'Des Moines': 10, 'Bothell': 11, 'Federal Way': 12,
     'Kirkland': 13, 'Issaquah': 14, 'Woodinville': 15, 'Normandy Park': 16, 'Fall City': 17, 'Renton': 18,
     'Carnation': 19, 'Snoqualmie': 20, 'Duvall': 21, 'Burien': 22, 'Covington': 23, 'Inglewood-Finn Hill': 24,
     'Kenmore': 25, 'Newcastle': 26, 'Mercer Island': 27, 'Black Diamond': 28, 'Ravensdale': 29,
     'Clyde Hill': 30,
     'Algona': 31, 'Skykomish': 32, 'Tukwila': 33, 'Vashon': 34, 'Yarrow Point': 35, 'SeaTac': 36, 'Medina': 37,
     'Enumclaw': 38, 'Snoqualmie Pass': 39, 'Pacific': 40, 'Beaux Arts Village': 41, 'Preston': 42,
     'Milton': 43}.get)

# Dividing the dataset into dependent and independent column Here X is Independent Column and y is dependent column

X = df.drop('price', axis=1)
y = df['price']

# ***************** SPLITTING THE DATASET INTO TRAINING AND TESTING DATA ***********************
# 20% of the dataset will be used for testing(evaluation) and 80% of the data will be used for training purposes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# creating the machine learning model
linearmodel = LinearRegression()
linearmodel.fit(X_train, y_train)  # training is taking place

# predictions
linearmodel.predict(X_test)
# calculating the accuracy of the model
linearmodel.score(X, y)  # evaluation is taking place
accuracy_sc = linearmodel.score(X, y)
rounded_acc = np.round(accuracy_sc, 2)
print(f&quot;The Accuracy of the Model is: {rounded_acc * 100}&quot;)

# predictions with respect to new dataset

data_new = {'bedrooms': float(no_bed_room), 'bathrooms': float(no_bath_room),
            'sqft_living': int(square_feet_living),
            'sqft_lot': int(square_feet_lot), 'floors': float(no_of_floors),
            'waterfront': int(water_front), 'view': int(no_of_views), 'condition': int(condition),
            'sqft_above': int(square_feet_above),
            'sqft_basement': int(square_feet_basement),
            'yr_built': int(year_built), 'yr_renovated': int(year_renovated), 'city': str(city)}

# with open('test.csv', 'w') as f:
#     for key in data_new:
#         f.write(&quot;%s,%s\n&quot; % (key, data_new[key]))
#
# df1 = pd.read_csv(&quot;test.csv&quot;)
# tdf1 = df1.T
# tdf1.columns = tdf1.iloc[0]
# df_new = tdf1[1:]
df1 = pd.DataFrame.from_dict([data_new])
df1.to_csv('test.csv', index=None)  # index=None prevents index being added as column 1
df2 = pd.read_csv('test.csv')

df2['city'] = df2['city'].apply(
    {'Shoreline': 0, 'Seattle': 1, 'Kent': 2, 'Bellevue': 3, 'Redmond': 4, 'Maple Valley': 5, 'North Bend': 6,
     'Lake Forest Park': 7, 'Sammamish': 8, 'Auburn': 9, 'Des Moines': 10, 'Bothell': 11, 'Federal Way': 12,
     'Kirkland': 13, 'Issaquah': 14, 'Woodinville': 15, 'Normandy Park': 16, 'Fall City': 17, 'Renton': 18,
     'Carnation': 19, 'Snoqualmie': 20, 'Duvall': 21, 'Burien': 22, 'Covington': 23, 'Inglewood-Finn Hill': 24,
     'Kenmore': 25, 'Newcastle': 26, 'Mercer Island': 27, 'Black Diamond': 28, 'Ravensdale': 29,
     'Clyde Hill': 30,
     'Algona': 31, 'Skykomish': 32, 'Tukwila': 33, 'Vashon': 34, 'Yarrow Point': 35, 'SeaTac': 36, 'Medina': 37,
     'Enumclaw': 38, 'Snoqualmie Pass': 39, 'Pacific': 40, 'Beaux Arts Village': 41, 'Preston': 42,
     'Milton': 43}.get)

index = [1]  # serial number

my_data = pd.DataFrame(df2, index)

# Pricing allocated
# Here the linearmodel is the variable  used in 13
my_data_price = linearmodel.predict(my_data)
rounded_price = np.round(my_data_price, 2)
return f&quot;The predicted price for the given data is :{rounded_price}&quot;
# print(f&quot; The predicted price for the given data is :{rounded_price}&quot;)


print(predict_page)
if __name__ == &quot;__main__&quot;:
app.run(debug=True, port=8000)
</code></pre>
<p>Could you please help me with solving this issue?</p>
<p>Also, here is the <a href=""https://www.kaggle.com/code/arwavis/house-price-prediction-linear-regression"" rel=""nofollow noreferrer"">Kaggle data set</a>.</p>
",38,0,0,4,python;pandas;flask;scikit-learn,2022-07-29 09:16:53,2022-07-29 09:16:53,2022-07-31 02:30:06,project overview  i was testing linear logic algorithm to predict house pricing using the  house price prediction  data from kaggle and flask app  the prediction works fine when executed separately but doesn t work when used with flask  i m getting the following error  you can find the complete code with data and html files in my   here is the app py code  could you please help me with solving this issue  also  here is the  ,configuring machine learning model in flask,project overview testing linear logic algorithm predict house pricing using house price prediction data kaggle flask app prediction works fine executed separately work used flask getting following error find complete code data html files app py code could please help solving issue also,configuring machine learning model flask,configuring machine learning model flaskproject overview testing linear logic algorithm predict house pricing using house price prediction data kaggle flask app prediction works fine executed separately work used flask getting following error find complete code data html files app py code could please help solving issue also,"['configuring', 'machine', 'learning', 'model', 'flaskproject', 'overview', 'testing', 'linear', 'logic', 'algorithm', 'predict', 'house', 'pricing', 'using', 'house', 'price', 'prediction', 'data', 'kaggle', 'flask', 'app', 'prediction', 'works', 'fine', 'executed', 'separately', 'work', 'used', 'flask', 'getting', 'following', 'error', 'find', 'complete', 'code', 'data', 'html', 'files', 'app', 'py', 'code', 'could', 'please', 'help', 'solving', 'issue', 'also']","['configur', 'machin', 'learn', 'model', 'flaskproject', 'overview', 'test', 'linear', 'logic', 'algorithm', 'predict', 'hous', 'price', 'use', 'hous', 'price', 'predict', 'data', 'kaggl', 'flask', 'app', 'predict', 'work', 'fine', 'execut', 'separ', 'work', 'use', 'flask', 'get', 'follow', 'error', 'find', 'complet', 'code', 'data', 'html', 'file', 'app', 'py', 'code', 'could', 'pleas', 'help', 'solv', 'issu', 'also']"
6,6,6,18292875,73175870,InvalidArgumentError: required broadcastable shapes [Op:Add] tensorflow model,"<p>I am a using CLIP model. Where I have two models. One model output is <code>(20, 128, 256)</code> and the other one output is <code>(20, 256)</code>.</p>
<pre><code>image_model_output = (20, 256)
text_model_output = (20, 128, 256)
</code></pre>
<p>I use the following to calculate this</p>
<pre><code>logits = (tf.matmul(caption_embeddings, image_embeddings, transpose_b=True))
so it will be like `(20, 256) * (256, 128, 20)`
it's ouput will be `(20, 128, 20)`
</code></pre>
<p>Similarly I calculate like this</p>
<pre><code>images_similarity = tf.matmul(
        image_embeddings, image_embeddings, transpose_b=True
    )
(Output)--&gt; (20, 256) * (256, 20) = (20,20) 
</code></pre>
<p>and this</p>
<pre><code>captions_similarity = tf.matmul(
        caption_embeddings, caption_embeddings, transpose_b=True
    )
(Output)--&gt; (20, 128, 256) * (256, 128, 20) = (20, 128, 128) 
</code></pre>
<p>The problem arises here</p>
<pre><code>targets = keras.activations.softmax(
        (captions_similarity + images_similarity) / (2 * self.temperature)
    )
</code></pre>
<p>So do I need to change the activation function or there is any way to add these 3d matrices with different shapes?
Sorry to technically explain like this but people with solid deep learning and machine learning backgorund will understand.</p>
<h2>NOTE: After adding <code>axis 1</code> like this <code>tf.expand_dims(image_embeddings, axis=1)</code> the below part runs successfully</h2>
<pre><code>targets = keras.activations.softmax(
    (captions_similarity + images_similarity) / (2 * self.temperature)
)
</code></pre>
<p>However after this there is a loss funtion like below</p>
<pre><code>captions_loss = keras.losses.categorical_crossentropy(
        y_true=targets, y_pred=logits, from_logits=True
    )
</code></pre>
<p>which generates this error</p>
<pre><code>ValueError: Shapes (2, 128, 128) and (2, 128, 1) are incompatible
</code></pre>
<p>Is it possible to solve this error?</p>
",41,1,0,5,python;tensorflow;machine-learning;deep-learning;computer-vision,2022-07-30 16:04:59,2022-07-30 16:04:59,2022-07-31 00:37:15,i am a using clip model  where i have two models  one model output is        and the other one output is       i use the following to calculate this similarly i calculate like this and this the problem arises here however after this there is a loss funtion like below which generates this error is it possible to solve this error ,invalidargumenterror  required broadcastable shapes  op add  tensorflow model,using clip model two models one model output one output use following calculate similarly calculate like problem arises however loss funtion like generates error possible solve error,invalidargumenterror required broadcastable shapes op tensorflow model,invalidargumenterror required broadcastable shapes op tensorflow modelusing clip model two models one model output one output use following calculate similarly calculate like problem arises however loss funtion like generates error possible solve error,"['invalidargumenterror', 'required', 'broadcastable', 'shapes', 'op', 'tensorflow', 'modelusing', 'clip', 'model', 'two', 'models', 'one', 'model', 'output', 'one', 'output', 'use', 'following', 'calculate', 'similarly', 'calculate', 'like', 'problem', 'arises', 'however', 'loss', 'funtion', 'like', 'generates', 'error', 'possible', 'solve', 'error']","['invalidargumenterror', 'requir', 'broadcast', 'shape', 'op', 'tensorflow', 'modelus', 'clip', 'model', 'two', 'model', 'one', 'model', 'output', 'one', 'output', 'use', 'follow', 'calcul', 'similarli', 'calcul', 'like', 'problem', 'aris', 'howev', 'loss', 'funtion', 'like', 'gener', 'error', 'possibl', 'solv', 'error']"
7,7,7,19643209,73158468,"How do I create a unique array for each row in CSV, appending all columns in that row to the array?","<p>I'm currently working on a machine learning project to predict the number of Covid cases in each US county. This is what the header for my CSV looks like:</p>
<p><img src=""https://i.stack.imgur.com/JK530.png"" alt=""CSV header"" /></p>
<p>I need to create a unique array for each row in this CSV, appending each element (the elements with case data, not the counties) to their respective array. This is the code I currently have:</p>
<pre><code>only_cases = formatted_cases.drop(columns=['Combined_Key'])

for row in only_cases:
    if &quot;/&quot; not in row:
        global county_cases
        county_cases = []
        for column in row:
            county_cases.append(column)
</code></pre>
<p>I'm creating a new CSV without the county column and making sure not to somehow append any non-case data to the array. However, when I try to print the array, the output is simply:</p>
<pre><code>['7', '/', '2', '7', '/', '2', '2']. 
</code></pre>
<p>My desired output would be like this for Autauga, Alabama, US:</p>
<p>[0 0 0 0 0 0 0 0 0 ... 17127 17186 17203 17232 17268 17268 17268 17349 17392 17425].</p>
",43,0,1,4,python;arrays;dataframe;csv,2022-07-28 22:51:29,2022-07-28 22:51:29,2022-07-30 22:47:22,i m currently working on a machine learning project to predict the number of covid cases in each us county  this is what the header for my csv looks like   i need to create a unique array for each row in this csv  appending each element  the elements with case data  not the counties  to their respective array  this is the code i currently have  i m creating a new csv without the county column and making sure not to somehow append any non case data to the array  however  when i try to print the array  the output is simply  my desired output would be like this for autauga  alabama  us                           ,how do i create a unique array for each row in csv  appending all columns in that row to the array ,currently working machine learning project predict number covid cases us county header csv looks like need create unique array row csv appending element elements case data counties respective array code currently creating csv without county column making sure somehow append non case data array however try print array output simply desired output would like autauga alabama us,create unique array row csv appending columns row array,create unique array row csv appending columns row arraycurrently working machine learning project predict number covid cases us county header csv looks like need create unique array row csv appending element elements case data counties respective array code currently creating csv without county column making sure somehow append non case data array however try print array output simply desired output would like autauga alabama us,"['create', 'unique', 'array', 'row', 'csv', 'appending', 'columns', 'row', 'arraycurrently', 'working', 'machine', 'learning', 'project', 'predict', 'number', 'covid', 'cases', 'us', 'county', 'header', 'csv', 'looks', 'like', 'need', 'create', 'unique', 'array', 'row', 'csv', 'appending', 'element', 'elements', 'case', 'data', 'counties', 'respective', 'array', 'code', 'currently', 'creating', 'csv', 'without', 'county', 'column', 'making', 'sure', 'somehow', 'append', 'non', 'case', 'data', 'array', 'however', 'try', 'print', 'array', 'output', 'simply', 'desired', 'output', 'would', 'like', 'autauga', 'alabama', 'us']","['creat', 'uniqu', 'array', 'row', 'csv', 'append', 'column', 'row', 'arraycurr', 'work', 'machin', 'learn', 'project', 'predict', 'number', 'covid', 'case', 'us', 'counti', 'header', 'csv', 'look', 'like', 'need', 'creat', 'uniqu', 'array', 'row', 'csv', 'append', 'element', 'element', 'case', 'data', 'counti', 'respect', 'array', 'code', 'current', 'creat', 'csv', 'without', 'counti', 'column', 'make', 'sure', 'somehow', 'append', 'non', 'case', 'data', 'array', 'howev', 'tri', 'print', 'array', 'output', 'simpli', 'desir', 'output', 'would', 'like', 'autauga', 'alabama', 'us']"
8,8,8,143931,73178409,How to get absolute path to &quot;outputs&quot; folder in Azure ML,"<p>In the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-save-write-experiment-files"" rel=""nofollow noreferrer"">documentation</a> of Azure Machine Learning, under &quot;Where to write files&quot;, it says</p>
<blockquote>
<p>Otherwise, write files to the <code>./outputs</code> and/or <code>./logs</code> folder.</p>
</blockquote>
<p>These are relative paths, i.e. relative to the folder where my script is run by the Azure ML framework. I was not able to find a function in the Azure ML SDK that would return the absolute path -- have I missed it or is there none? (Meaning that I should read the <code>cwd</code> at the beginning of my script and store it myself.)</p>
",11,0,0,2,azure-machine-learning-studio;azure-machine-learning-service,2022-07-30 22:11:39,2022-07-30 22:11:39,2022-07-30 22:11:39,in the  of azure machine learning  under  where to write files   it says otherwise  write files to the   outputs and or   logs folder  these are relative paths  i e  relative to the folder where my script is run by the azure ml framework  i was not able to find a function in the azure ml sdk that would return the absolute path    have i missed it or is there none   meaning that i should read the cwd at the beginning of my script and store it myself  ,how to get absolute path to  outputs  folder in azure ml,azure machine learning write files says otherwise write files outputs logs folder relative paths e relative folder script run azure ml framework able find function azure ml sdk would return absolute path missed none meaning read cwd beginning script store,get absolute path outputs folder azure ml,get absolute path outputs folder azure mlazure machine learning write files says otherwise write files outputs logs folder relative paths e relative folder script run azure ml framework able find function azure ml sdk would return absolute path missed none meaning read cwd beginning script store,"['get', 'absolute', 'path', 'outputs', 'folder', 'azure', 'mlazure', 'machine', 'learning', 'write', 'files', 'says', 'otherwise', 'write', 'files', 'outputs', 'logs', 'folder', 'relative', 'paths', 'e', 'relative', 'folder', 'script', 'run', 'azure', 'ml', 'framework', 'able', 'find', 'function', 'azure', 'ml', 'sdk', 'would', 'return', 'absolute', 'path', 'missed', 'none', 'meaning', 'read', 'cwd', 'beginning', 'script', 'store']","['get', 'absolut', 'path', 'output', 'folder', 'azur', 'mlazur', 'machin', 'learn', 'write', 'file', 'say', 'otherwis', 'write', 'file', 'output', 'log', 'folder', 'rel', 'path', 'e', 'rel', 'folder', 'script', 'run', 'azur', 'ml', 'framework', 'abl', 'find', 'function', 'azur', 'ml', 'sdk', 'would', 'return', 'absolut', 'path', 'miss', 'none', 'mean', 'read', 'cwd', 'begin', 'script', 'store']"
9,9,9,8095843,73176798,How to create a score for many Branch for being probability of high risk to low risk?,"<p>I have around 900 branches which has into banking business and all the variables like age of branch,portfolio, defaulters, No of employees ext..</p>
<p>If I want to build a machine learning model to score or giving of probability of branch being high to low risk.</p>
<p>How to go about on this?</p>
",17,0,-2,4,python;pandas;machine-learning;risk-analysis,2022-07-30 18:17:41,2022-07-30 18:17:41,2022-07-30 19:43:41,i have around  branches which has into banking business and all the variables like age of branch portfolio  defaulters  no of employees ext   if i want to build a machine learning model to score or giving of probability of branch being high to low risk  how to go about on this ,how to create a score for many branch for being probability of high risk to low risk ,around branches banking business variables like age branch portfolio defaulters employees ext want build machine learning model score giving probability branch high low risk go,create score many branch probability high risk low risk,create score many branch probability high risk low riskaround branches banking business variables like age branch portfolio defaulters employees ext want build machine learning model score giving probability branch high low risk go,"['create', 'score', 'many', 'branch', 'probability', 'high', 'risk', 'low', 'riskaround', 'branches', 'banking', 'business', 'variables', 'like', 'age', 'branch', 'portfolio', 'defaulters', 'employees', 'ext', 'want', 'build', 'machine', 'learning', 'model', 'score', 'giving', 'probability', 'branch', 'high', 'low', 'risk', 'go']","['creat', 'score', 'mani', 'branch', 'probabl', 'high', 'risk', 'low', 'riskaround', 'branch', 'bank', 'busi', 'variabl', 'like', 'age', 'branch', 'portfolio', 'default', 'employe', 'ext', 'want', 'build', 'machin', 'learn', 'model', 'score', 'give', 'probabl', 'branch', 'high', 'low', 'risk', 'go']"
10,10,10,19652870,73174559,Machine Learning Model on Paillier Homomorphic Encryption,"<p>I want to run machine learning model on <strong>Paillier Homomorphic Encrypted data</strong>, on passing encrypted data into the Machine Learning Model it shows error</p>
<pre><code>TypeError: float() argument must be a string or a number, not 'EncryptedNumber'
</code></pre>
<p>I wanted to know is there any method or  how can I build ML models on top of encrypted data. Please guide me.</p>
",12,0,-1,3,machine-learning;encryption;paillier,2022-07-30 12:50:30,2022-07-30 12:50:30,2022-07-30 16:57:46,i want to run machine learning model on paillier homomorphic encrypted data  on passing encrypted data into the machine learning model it shows error i wanted to know is there any method or  how can i build ml models on top of encrypted data  please guide me ,machine learning model on paillier homomorphic encryption,want run machine learning model paillier homomorphic encrypted data passing encrypted data machine learning model shows error wanted know method build ml models top encrypted data please guide,machine learning model paillier homomorphic encryption,machine learning model paillier homomorphic encryptionwant run machine learning model paillier homomorphic encrypted data passing encrypted data machine learning model shows error wanted know method build ml models top encrypted data please guide,"['machine', 'learning', 'model', 'paillier', 'homomorphic', 'encryptionwant', 'run', 'machine', 'learning', 'model', 'paillier', 'homomorphic', 'encrypted', 'data', 'passing', 'encrypted', 'data', 'machine', 'learning', 'model', 'shows', 'error', 'wanted', 'know', 'method', 'build', 'ml', 'models', 'top', 'encrypted', 'data', 'please', 'guide']","['machin', 'learn', 'model', 'paillier', 'homomorph', 'encryptionw', 'run', 'machin', 'learn', 'model', 'paillier', 'homomorph', 'encrypt', 'data', 'pass', 'encrypt', 'data', 'machin', 'learn', 'model', 'show', 'error', 'want', 'know', 'method', 'build', 'ml', 'model', 'top', 'encrypt', 'data', 'pleas', 'guid']"
11,11,11,19653741,73175875,Detect a specific object from an image using openCV,"<p>I have trained a machine learning model on about 1600 images of arrows (up, down, left, right) a down arrow (1). I have downloaded those images from google.
What I want to do is: detecting to which class (down, up, left, right) my input image belongs to. When I input a noisy image an arrow in a traffic signal (2) it inferences a wrong class. I tried sliding window approach, but this approach is too slow. I am looking for faster methods which can detect an arrow in a image using <strong>openCV</strong>.</p>
<p>arrow:</p>
<p><a href=""https://i.stack.imgur.com/sFLTv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sFLTv.png"" alt=""1"" /></a></p>
<p>picture:</p>
<p><a href=""https://i.stack.imgur.com/aLmxY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aLmxY.jpg"" alt=""2"" /></a></p>
",19,0,-2,5,opencv;machine-learning;computer-vision;object-detection;image-classification,2022-07-30 16:05:48,2022-07-30 16:05:48,2022-07-30 16:38:39,arrow   picture  ,detect a specific object from an image using opencv,arrow picture,detect specific object image using opencv,detect specific object image using opencvarrow picture,"['detect', 'specific', 'object', 'image', 'using', 'opencvarrow', 'picture']","['detect', 'specif', 'object', 'imag', 'use', 'opencvarrow', 'pictur']"
12,12,12,5413304,73174425,How to classify string sequence from large corpus of string sequence using machine learning,"<p>We have corpus of 100k log data in one day, we have some pre-defined sequences/classes, Our task is to identify, Out of 100K string sequences, which pre-defined sequence user had followed. Issue is some of string sequences can be irrelevant or Noisy data.</p>
<p>We have some computer print logs. Now let's say user performed any activity on screen (say any button clicked). we just record it as log (by saying submit button clicked ).</p>
<p>Now we need to do a log analysis. What user is doing? How he is doing?</p>
<p>We have some set of actions/steps that should be perform in sequence, say A B C D. But in real world, sometimes user do not follow the correct actions/steps and deviates. Expected steps is
A B C D; but actual performed steps are &quot;<strong>A</strong> N <strong>B</strong> V G <strong>C</strong> F <strong>D</strong>&quot;. You can see, user is deviating from original steps ie 'N', 'V', 'G', 'F' is a deviation.</p>
<p>We have some predefine journeys <br>
Note : Journey is nothing but set of actions/ steps.
step 1 + step 2 = Journey 1</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Steps</th>
<th>Journeys</th>
</tr>
</thead>
<tbody>
<tr>
<td>A B C D</td>
<td>pre-defined journey 1</td>
</tr>
<tr>
<td>P Q R S</td>
<td>pre-defined journey 2</td>
</tr>
<tr>
<td>A B C</td>
<td>pre-defined journey 3</td>
</tr>
<tr>
<td>W X Y Z</td>
<td>pre-defined journey 4</td>
</tr>
<tr>
<td>J K L M N O</td>
<td>pre-defined journey 5</td>
</tr>
<tr>
<td>A B C D</td>
<td>pre-defined journey 1</td>
</tr>
<tr>
<td>P Q</td>
<td>pre-defined journey 6</td>
</tr>
<tr>
<td>E F G</td>
<td>pre-defined journey 7</td>
</tr>
</tbody>
</table>
</div>
<p>In one day, ideally user should follow below journeys:</p>
<blockquote>
<p>say, A B C D P Q R S A B C W X Y Z J K L M N O A B C P Q E F G</p>
</blockquote>
<p>A B C D | P Q R S | A B C | W X Y Z | J K L M N O | A B C | P Q | E F G</p>
<p>total <em>8 journeys</em> performed by user in one day. First user performed &quot;pre-defined journey 1&quot; then he performed &quot;pre-defined journey 2&quot; and so on.</p>
<p>But in reality user is doing like below,</p>
<blockquote>
<p><strong>A</strong> T <strong>B</strong> H <strong>C</strong> <strong>D</strong> | <strong>P</strong> A <strong>Q</strong> <strong>R</strong> U <strong>S</strong> | <strong>A</strong> P F Z Q <strong>B</strong> <strong>C</strong> | <strong>W</strong> M <strong>X</strong> M <strong>Y</strong> J <strong>Z</strong> | <strong>J</strong> V <strong>K</strong> <strong>L</strong> <strong>M</strong> K  <strong>N</strong> <strong>O</strong>| <strong>A</strong> <strong>B</strong> H <strong>C</strong> | <strong>P</strong> <strong>Q</strong> | <strong>E</strong> C <strong>F</strong> X <strong>G</strong></p>
</blockquote>
<ul>
<li><strong>A</strong> T <strong>B</strong> H <strong>C</strong> <strong>D</strong> (ideally its 4 steps journey, in reality user took T,H extra step to finish the journey)</li>
<li><strong>P</strong> A <strong>Q</strong> <strong>R</strong> U <strong>S</strong> (ideally its 4 steps journey, in reality user took A,U extra step to finish the journey)</li>
<li><strong>A</strong> P F Z Q <strong>B</strong> <strong>C</strong> (ideally its 3 steps journey, in reality user took P,F,Z,Q extra step to finish the journey)</li>
<li><strong>W</strong> M <strong>X</strong> M <strong>Y</strong> J <strong>Z</strong> (ideally its 4 steps journey, in reality user took M,M and J extra step to finish the journey)</li>
<li><strong>J</strong> V <strong>K</strong> <strong>L</strong> <strong>M</strong> K  <strong>N</strong> <strong>O</strong> (ideally its 6 steps journey, in reality user took V,K extra step to finish the journey)</li>
<li><strong>A</strong> <strong>B</strong> H <strong>C</strong> (ideally its 3 steps journey, in reality user took H extra step to finish the journey)</li>
<li><strong>P</strong> <strong>Q</strong> (ideally its 2 steps journey, in reality user took 0 extra step to finish the journey)</li>
<li><strong>E</strong> C <strong>F</strong> X <strong>G</strong> (ideally its 3 steps journey, in reality user took C extra step to finish the journey) <br></li>
</ul>
<p>You can see in reality user is deviating from original journey.
One more thing to note, In whole day user has to perform various pre-defined journeys.</p>
<p>Our task is,</p>
<ol>
<li>Out of all the n number of journeys performed by user in one day, find out, which pre-defined journey user had followed correctly.</li>
<li>Out of all the n number of journeys performed by user in one day, find out, which pre-defined journey user had followed incorrectly.</li>
<li>Rest all mark as Noise</li>
</ol>
<p>There are some issues in real world,</p>
<ol>
<li>As already explained, user is deviating while performing journeys</li>
<li>It can have irrelevant steps as well. Explain below,</li>
</ol>
<blockquote>
<p>A B C F H J K L M N R A B C G D J G J L P C D J K H</p>
</blockquote>
<p>out of them</p>
<p>R E <strong>A B C</strong> F H J K L M N R <strong>A B C G D</strong> J G J L P C D J K H <br>
Highlights two are correct journeys</p>
<ul>
<li>ABC is perfect</li>
<li>ABCGD is less perfect, because (G) is unnecessary step user performed.</li>
<li>Rest all steps are wrong steps user performed, we have to mark it as &quot;Noise&quot;
So <em>we have to classify perfect journey, less perfect journey and Noise journey</em>.</li>
</ul>
<p>In simple words, there is mess of 1000s of different journeys, we have to classify which predefined journey user had performed.</p>
<p>We have checked Process Mining. But it's not working in our case.
Because Process Mining has definite start and end point. In our case, whole day user will keep doing different different journeys with deviation, so Process Mining is not helping.</p>
<p>Which ML algorithm/ technique/ approach we can take?
No straight forward ML algorithm can help here I guess.</p>
",11,0,-1,4,machine-learning;artificial-intelligence;log-analysis;process-mining,2022-07-30 12:30:00,2022-07-30 12:30:00,2022-07-30 15:30:42,we have corpus of k log data in one day  we have some pre defined sequences classes  our task is to identify  out of k string sequences  which pre defined sequence user had followed  issue is some of string sequences can be irrelevant or noisy data  we have some computer print logs  now let s say user performed any activity on screen  say any button clicked   we just record it as log  by saying submit button clicked    now we need to do a log analysis  what user is doing  how he is doing  in one day  ideally user should follow below journeys  say  a b c d p q r s a b c w x y z j k l m n o a b c p q e f g a b c d   p q r s   a b c   w x y z   j k l m n o   a b c   p q   e f g total  journeys performed by user in one day  first user performed  pre defined journey   then he performed  pre defined journey   and so on  but in reality user is doing like below  a t b h c d   p a q r u s   a p f z q b c   w m x m y j z   j v k l m k  n o  a b h c   p q   e c f x g our task is  there are some issues in real world  a b c f h j k l m n r a b c g d j g j l p c d j k h out of them in simple words  there is mess of s of different journeys  we have to classify which predefined journey user had performed ,how to classify string sequence from large corpus of string sequence using machine learning,corpus k log data one day pre defined sequences classes task identify k string sequences pre defined sequence user followed issue string sequences irrelevant noisy data computer print logs let say user performed activity screen say button clicked record log saying submit button clicked need log analysis user one day ideally user follow journeys say b c p q r b c w x z j k l n b c p q e f g b c p q r b c w x z j k l n b c p q e f g total journeys performed user one day first user performed pre defined journey performed pre defined journey reality user like b h c p q r u p f z q b c w x j z j v k l k n b h c p q e c f x g task issues real world b c f h j k l n r b c g j g j l p c j k h simple mess different journeys classify predefined journey user performed,classify string sequence large corpus string sequence using machine learning,classify string sequence large corpus string sequence using machine learningcorpus k log data one day pre defined sequences classes task identify k string sequences pre defined sequence user followed issue string sequences irrelevant noisy data computer print logs let say user performed activity screen say button clicked record log saying submit button clicked need log analysis user one day ideally user follow journeys say b c p q r b c w x z j k l n b c p q e f g b c p q r b c w x z j k l n b c p q e f g total journeys performed user one day first user performed pre defined journey performed pre defined journey reality user like b h c p q r u p f z q b c w x j z j v k l k n b h c p q e c f x g task issues real world b c f h j k l n r b c g j g j l p c j k h simple mess different journeys classify predefined journey user performed,"['classify', 'string', 'sequence', 'large', 'corpus', 'string', 'sequence', 'using', 'machine', 'learningcorpus', 'k', 'log', 'data', 'one', 'day', 'pre', 'defined', 'sequences', 'classes', 'task', 'identify', 'k', 'string', 'sequences', 'pre', 'defined', 'sequence', 'user', 'followed', 'issue', 'string', 'sequences', 'irrelevant', 'noisy', 'data', 'computer', 'print', 'logs', 'let', 'say', 'user', 'performed', 'activity', 'screen', 'say', 'button', 'clicked', 'record', 'log', 'saying', 'submit', 'button', 'clicked', 'need', 'log', 'analysis', 'user', 'one', 'day', 'ideally', 'user', 'follow', 'journeys', 'say', 'b', 'c', 'p', 'q', 'r', 'b', 'c', 'w', 'x', 'z', 'j', 'k', 'l', 'n', 'b', 'c', 'p', 'q', 'e', 'f', 'g', 'b', 'c', 'p', 'q', 'r', 'b', 'c', 'w', 'x', 'z', 'j', 'k', 'l', 'n', 'b', 'c', 'p', 'q', 'e', 'f', 'g', 'total', 'journeys', 'performed', 'user', 'one', 'day', 'first', 'user', 'performed', 'pre', 'defined', 'journey', 'performed', 'pre', 'defined', 'journey', 'reality', 'user', 'like', 'b', 'h', 'c', 'p', 'q', 'r', 'u', 'p', 'f', 'z', 'q', 'b', 'c', 'w', 'x', 'j', 'z', 'j', 'v', 'k', 'l', 'k', 'n', 'b', 'h', 'c', 'p', 'q', 'e', 'c', 'f', 'x', 'g', 'task', 'issues', 'real', 'world', 'b', 'c', 'f', 'h', 'j', 'k', 'l', 'n', 'r', 'b', 'c', 'g', 'j', 'g', 'j', 'l', 'p', 'c', 'j', 'k', 'h', 'simple', 'mess', 'different', 'journeys', 'classify', 'predefined', 'journey', 'user', 'performed']","['classifi', 'string', 'sequenc', 'larg', 'corpu', 'string', 'sequenc', 'use', 'machin', 'learningcorpu', 'k', 'log', 'data', 'one', 'day', 'pre', 'defin', 'sequenc', 'class', 'task', 'identifi', 'k', 'string', 'sequenc', 'pre', 'defin', 'sequenc', 'user', 'follow', 'issu', 'string', 'sequenc', 'irrelev', 'noisi', 'data', 'comput', 'print', 'log', 'let', 'say', 'user', 'perform', 'activ', 'screen', 'say', 'button', 'click', 'record', 'log', 'say', 'submit', 'button', 'click', 'need', 'log', 'analysi', 'user', 'one', 'day', 'ideal', 'user', 'follow', 'journey', 'say', 'b', 'c', 'p', 'q', 'r', 'b', 'c', 'w', 'x', 'z', 'j', 'k', 'l', 'n', 'b', 'c', 'p', 'q', 'e', 'f', 'g', 'b', 'c', 'p', 'q', 'r', 'b', 'c', 'w', 'x', 'z', 'j', 'k', 'l', 'n', 'b', 'c', 'p', 'q', 'e', 'f', 'g', 'total', 'journey', 'perform', 'user', 'one', 'day', 'first', 'user', 'perform', 'pre', 'defin', 'journey', 'perform', 'pre', 'defin', 'journey', 'realiti', 'user', 'like', 'b', 'h', 'c', 'p', 'q', 'r', 'u', 'p', 'f', 'z', 'q', 'b', 'c', 'w', 'x', 'j', 'z', 'j', 'v', 'k', 'l', 'k', 'n', 'b', 'h', 'c', 'p', 'q', 'e', 'c', 'f', 'x', 'g', 'task', 'issu', 'real', 'world', 'b', 'c', 'f', 'h', 'j', 'k', 'l', 'n', 'r', 'b', 'c', 'g', 'j', 'g', 'j', 'l', 'p', 'c', 'j', 'k', 'h', 'simpl', 'mess', 'differ', 'journey', 'classifi', 'predefin', 'journey', 'user', 'perform']"
13,13,13,16789776,73173373,"how to differentiate one dog from another using openCV, python","<p>I am new to machine learning I am trying to build a project where I can identify my dogs when they came in front of a camera. it should be able to tell which dog is which, I have 2 dogs of the same breed and I would like to monitor them when I am not around.</p>
<p>can someone tell me how it can be done?</p>
<p>thank you</p>
",29,0,-1,3,python;python-3.x;opencv,2022-07-30 09:29:40,2022-07-30 09:29:40,2022-07-30 14:21:49,i am new to machine learning i am trying to build a project where i can identify my dogs when they came in front of a camera  it should be able to tell which dog is which  i have  dogs of the same breed and i would like to monitor them when i am not around  can someone tell me how it can be done  thank you,how to differentiate one dog from another using opencv  python,machine learning trying build project identify dogs came front camera able tell dog dogs breed would like monitor around someone tell done thank,differentiate one dog another using opencv python,differentiate one dog another using opencv pythonmachine learning trying build project identify dogs came front camera able tell dog dogs breed would like monitor around someone tell done thank,"['differentiate', 'one', 'dog', 'another', 'using', 'opencv', 'pythonmachine', 'learning', 'trying', 'build', 'project', 'identify', 'dogs', 'came', 'front', 'camera', 'able', 'tell', 'dog', 'dogs', 'breed', 'would', 'like', 'monitor', 'around', 'someone', 'tell', 'done', 'thank']","['differenti', 'one', 'dog', 'anoth', 'use', 'opencv', 'pythonmachin', 'learn', 'tri', 'build', 'project', 'identifi', 'dog', 'came', 'front', 'camera', 'abl', 'tell', 'dog', 'dog', 'breed', 'would', 'like', 'monitor', 'around', 'someon', 'tell', 'done', 'thank']"
14,14,14,12777302,73170005,Machine learning model working very good on dataset audio but works mostly false on recorded or other downloaded audio,"<p>I've created a support vector machine model in Matlab for detecting language spoken in an audio clip. The model was trained using around 70,000 audio clips of 3 different languages. I used the Matlab cvpartition function with 20% holdout for test train split.</p>
<p>The trained model shows around 98% accuracy while tested on the 20% holdout test data. But if I download an audio clip from youtube or record an audio clip of mine, the model gives wrong result almost always. The same is true for KNN, random tree models.</p>
<p>I am not understanding why it would work so well on partitioned test data of the dataset, but work so horribly on other downloaded or recorded data.</p>
",10,0,-2,3,machine-learning;audio;multiclass-classification,2022-07-29 22:08:12,2022-07-29 22:08:12,2022-07-30 05:45:08,i ve created a support vector machine model in matlab for detecting language spoken in an audio clip  the model was trained using around   audio clips of  different languages  i used the matlab cvpartition function with   holdout for test train split  the trained model shows around   accuracy while tested on the   holdout test data  but if i download an audio clip from youtube or record an audio clip of mine  the model gives wrong result almost always  the same is true for knn  random tree models  i am not understanding why it would work so well on partitioned test data of the dataset  but work so horribly on other downloaded or recorded data ,machine learning model working very good on dataset audio but works mostly false on recorded or other downloaded audio,created support vector machine model matlab detecting language spoken audio clip model trained using around audio clips different languages used matlab cvpartition function holdout test train split trained model shows around accuracy tested holdout test data download audio clip youtube record audio clip mine model gives wrong result almost always true knn random tree models understanding would work well partitioned test data dataset work horribly downloaded recorded data,machine learning model working good dataset audio works mostly false recorded downloaded audio,machine learning model working good dataset audio works mostly false recorded downloaded audiocreated support vector machine model matlab detecting language spoken audio clip model trained using around audio clips different languages used matlab cvpartition function holdout test train split trained model shows around accuracy tested holdout test data download audio clip youtube record audio clip mine model gives wrong result almost always true knn random tree models understanding would work well partitioned test data dataset work horribly downloaded recorded data,"['machine', 'learning', 'model', 'working', 'good', 'dataset', 'audio', 'works', 'mostly', 'false', 'recorded', 'downloaded', 'audiocreated', 'support', 'vector', 'machine', 'model', 'matlab', 'detecting', 'language', 'spoken', 'audio', 'clip', 'model', 'trained', 'using', 'around', 'audio', 'clips', 'different', 'languages', 'used', 'matlab', 'cvpartition', 'function', 'holdout', 'test', 'train', 'split', 'trained', 'model', 'shows', 'around', 'accuracy', 'tested', 'holdout', 'test', 'data', 'download', 'audio', 'clip', 'youtube', 'record', 'audio', 'clip', 'mine', 'model', 'gives', 'wrong', 'result', 'almost', 'always', 'true', 'knn', 'random', 'tree', 'models', 'understanding', 'would', 'work', 'well', 'partitioned', 'test', 'data', 'dataset', 'work', 'horribly', 'downloaded', 'recorded', 'data']","['machin', 'learn', 'model', 'work', 'good', 'dataset', 'audio', 'work', 'mostli', 'fals', 'record', 'download', 'audiocr', 'support', 'vector', 'machin', 'model', 'matlab', 'detect', 'languag', 'spoken', 'audio', 'clip', 'model', 'train', 'use', 'around', 'audio', 'clip', 'differ', 'languag', 'use', 'matlab', 'cvpartit', 'function', 'holdout', 'test', 'train', 'split', 'train', 'model', 'show', 'around', 'accuraci', 'test', 'holdout', 'test', 'data', 'download', 'audio', 'clip', 'youtub', 'record', 'audio', 'clip', 'mine', 'model', 'give', 'wrong', 'result', 'almost', 'alway', 'true', 'knn', 'random', 'tree', 'model', 'understand', 'would', 'work', 'well', 'partit', 'test', 'data', 'dataset', 'work', 'horribl', 'download', 'record', 'data']"
15,15,15,1709142,73170408,Hyper-parameter Tuning for a machine learning model,"<p>Why a hyper-parameter like regularization parameter (a real number) cannot be trained over training data along with model parameters? What will go wrong?</p>
",19,1,0,3,machine-learning;deep-learning;hyperparameters,2022-07-29 22:52:49,2022-07-29 22:52:49,2022-07-30 05:44:03,why a hyper parameter like regularization parameter  a real number  cannot be trained over training data along with model parameters  what will go wrong ,hyper parameter tuning for a machine learning model,hyper parameter like regularization parameter real number cannot trained training data along model parameters go wrong,hyper parameter tuning machine learning model,hyper parameter tuning machine learning modelhyper parameter like regularization parameter real number cannot trained training data along model parameters go wrong,"['hyper', 'parameter', 'tuning', 'machine', 'learning', 'modelhyper', 'parameter', 'like', 'regularization', 'parameter', 'real', 'number', 'can', 'not', 'trained', 'training', 'data', 'along', 'model', 'parameters', 'go', 'wrong']","['hyper', 'paramet', 'tune', 'machin', 'learn', 'modelhyp', 'paramet', 'like', 'regular', 'paramet', 'real', 'number', 'can', 'not', 'train', 'train', 'data', 'along', 'model', 'paramet', 'go', 'wrong']"
16,16,16,19650328,73171144,Machine learning model for data with many features of binary variables,"<p>I have a training dataset containing 35,000 data with 350 features that are exclusively binary 0-1. The label is also binary. What would be the best model in this case? And with which parameters? Most of the features are very rarely on value 1. That means that removing features with &gt;95% of 0 would lead to removing a lot of information.</p>
",15,1,-2,5,python;machine-learning;scikit-learn;data-science;xgboost,2022-07-30 00:26:44,2022-07-30 00:26:44,2022-07-30 05:40:57,i have a training dataset containing   data with  features that are exclusively binary    the label is also binary  what would be the best model in this case  and with which parameters  most of the features are very rarely on value   that means that removing features with  gt   of  would lead to removing a lot of information ,machine learning model for data with many features of binary variables,training dataset containing data features exclusively binary label also binary would best model case parameters features rarely value means removing features gt would lead removing lot information,machine learning model data many features binary variables,machine learning model data many features binary variablestraining dataset containing data features exclusively binary label also binary would best model case parameters features rarely value means removing features gt would lead removing lot information,"['machine', 'learning', 'model', 'data', 'many', 'features', 'binary', 'variablestraining', 'dataset', 'containing', 'data', 'features', 'exclusively', 'binary', 'label', 'also', 'binary', 'would', 'best', 'model', 'case', 'parameters', 'features', 'rarely', 'value', 'means', 'removing', 'features', 'gt', 'would', 'lead', 'removing', 'lot', 'information']","['machin', 'learn', 'model', 'data', 'mani', 'featur', 'binari', 'variablestrain', 'dataset', 'contain', 'data', 'featur', 'exclus', 'binari', 'label', 'also', 'binari', 'would', 'best', 'model', 'case', 'paramet', 'featur', 'rare', 'valu', 'mean', 'remov', 'featur', 'gt', 'would', 'lead', 'remov', 'lot', 'inform']"
17,17,17,761100,73171019,Issue with passing url in Ajax call through Javascript in a Laravel Project,"<p>I started learning Laravel a few months ago. First I developed it on my local machine and later tried to move it to my shared dreamhost hosting. While using the ajax calls in the Javascript code in Vue components, I realized that I need to pass full URL for the route. Hence I created a global variable in resources/js/app.js</p>
<pre><code> window.siteURL = (window.location.host.substr(0,9) == '127.0.0.1') ? &quot;http://127.0.0.1:8000/&quot; : &quot;http://example.com/&quot;;   
</code></pre>
<p>And I created url in my ajax calls like this:</p>
<pre><code> $.ajax({
        url: siteURL+'client/notesAjax',
        headers: {'X-CSRF-TOKEN': $('meta[name=&quot;csrf-token&quot;]').attr('content')},

        method: 'post',
</code></pre>
<p>I am not sure if this was a good scheme, but it worked.</p>
<p>A few days ago, I registered a domain and tried to run my Laravel project on my AWS EC2 server. After a few hurdles, my Laravel project started running of my domain. However, I realized that I need to empty siteURL for the server on AWS EC2, hence I updated window.siteURL as</p>
<pre><code>window.siteURL = (window.location.host.substr(0,9) == '127.0.0.1') ? &quot;http://127.0.0.1:8000/&quot; : &quot;&quot;;  
</code></pre>
<p>However, some of my ajax calls were still not working. For example, I had an ajax call on the client/notes page:</p>
<pre><code> $.ajax({
      url: siteURL+'client/notesAjax',
</code></pre>
<p>But this was failing (everything was still working fine on my local pc and the site running on my shared hosting on Dreamhost. Then I figured out that the url that this call was going to was like this:</p>
<pre><code>http://myawsdomain.ca/client/notes/client/notesAjax
</code></pre>
<p>Looks like that as the call is being originated from the client/notes page, it was being prepended to 'client/notesAjax' (siteURL is empty). As a hack, I created an extra route in routes/web.php</p>
<pre><code>Route::post('/client/notesAjax',[clientController::class,'notesAjax'])-&gt;name('client.notesAjax');
// on AWS, it looks for the route /client/notes/client/notesAjax (client/notesAjax is called from client/notes page)
Route::post('/client/notes/client/notesAjax',[clientController::class,'notesAjax'])-&gt;name('client.notesAjax'); 
</code></pre>
<p>I have many other ajax calls that originate from the client/notes page. Using that hack on all those calls may not be appropriate. What is the best way to handle my situation?
Thanks.</p>
",20,2,0,3,javascript;laravel;amazon-web-services,2022-07-30 00:09:37,2022-07-30 00:09:37,2022-07-30 03:43:23,i started learning laravel a few months ago  first i developed it on my local machine and later tried to move it to my shared dreamhost hosting  while using the ajax calls in the javascript code in vue components  i realized that i need to pass full url for the route  hence i created a global variable in resources js app js and i created url in my ajax calls like this  i am not sure if this was a good scheme  but it worked  a few days ago  i registered a domain and tried to run my laravel project on my aws ec server  after a few hurdles  my laravel project started running of my domain  however  i realized that i need to empty siteurl for the server on aws ec  hence i updated window siteurl as however  some of my ajax calls were still not working  for example  i had an ajax call on the client notes page  but this was failing  everything was still working fine on my local pc and the site running on my shared hosting on dreamhost  then i figured out that the url that this call was going to was like this  looks like that as the call is being originated from the client notes page  it was being prepended to  client notesajax   siteurl is empty   as a hack  i created an extra route in routes web php,issue with passing url in ajax call through javascript in a laravel project,started learning laravel months ago first developed local machine later tried move shared dreamhost hosting using ajax calls javascript code vue components realized need pass full url route hence created global variable resources js app js created url ajax calls like sure good scheme worked days ago registered domain tried run laravel project aws ec server hurdles laravel project started running domain however realized need empty siteurl server aws ec hence updated window siteurl however ajax calls still working example ajax call client notes page failing everything still working fine local pc site running shared hosting dreamhost figured url call going like looks like call originated client notes page prepended client notesajax siteurl empty hack created extra route routes web php,issue passing url ajax call javascript laravel project,issue passing url ajax call javascript laravel projectstarted learning laravel months ago first developed local machine later tried move shared dreamhost hosting using ajax calls javascript code vue components realized need pass full url route hence created global variable resources js app js created url ajax calls like sure good scheme worked days ago registered domain tried run laravel project aws ec server hurdles laravel project started running domain however realized need empty siteurl server aws ec hence updated window siteurl however ajax calls still working example ajax call client notes page failing everything still working fine local pc site running shared hosting dreamhost figured url call going like looks like call originated client notes page prepended client notesajax siteurl empty hack created extra route routes web php,"['issue', 'passing', 'url', 'ajax', 'call', 'javascript', 'laravel', 'projectstarted', 'learning', 'laravel', 'months', 'ago', 'first', 'developed', 'local', 'machine', 'later', 'tried', 'move', 'shared', 'dreamhost', 'hosting', 'using', 'ajax', 'calls', 'javascript', 'code', 'vue', 'components', 'realized', 'need', 'pass', 'full', 'url', 'route', 'hence', 'created', 'global', 'variable', 'resources', 'js', 'app', 'js', 'created', 'url', 'ajax', 'calls', 'like', 'sure', 'good', 'scheme', 'worked', 'days', 'ago', 'registered', 'domain', 'tried', 'run', 'laravel', 'project', 'aws', 'ec', 'server', 'hurdles', 'laravel', 'project', 'started', 'running', 'domain', 'however', 'realized', 'need', 'empty', 'siteurl', 'server', 'aws', 'ec', 'hence', 'updated', 'window', 'siteurl', 'however', 'ajax', 'calls', 'still', 'working', 'example', 'ajax', 'call', 'client', 'notes', 'page', 'failing', 'everything', 'still', 'working', 'fine', 'local', 'pc', 'site', 'running', 'shared', 'hosting', 'dreamhost', 'figured', 'url', 'call', 'going', 'like', 'looks', 'like', 'call', 'originated', 'client', 'notes', 'page', 'prepended', 'client', 'notesajax', 'siteurl', 'empty', 'hack', 'created', 'extra', 'route', 'routes', 'web', 'php']","['issu', 'pass', 'url', 'ajax', 'call', 'javascript', 'laravel', 'projectstart', 'learn', 'laravel', 'month', 'ago', 'first', 'develop', 'local', 'machin', 'later', 'tri', 'move', 'share', 'dreamhost', 'host', 'use', 'ajax', 'call', 'javascript', 'code', 'vue', 'compon', 'realiz', 'need', 'pass', 'full', 'url', 'rout', 'henc', 'creat', 'global', 'variabl', 'resourc', 'js', 'app', 'js', 'creat', 'url', 'ajax', 'call', 'like', 'sure', 'good', 'scheme', 'work', 'day', 'ago', 'regist', 'domain', 'tri', 'run', 'laravel', 'project', 'aw', 'ec', 'server', 'hurdl', 'laravel', 'project', 'start', 'run', 'domain', 'howev', 'realiz', 'need', 'empti', 'siteurl', 'server', 'aw', 'ec', 'henc', 'updat', 'window', 'siteurl', 'howev', 'ajax', 'call', 'still', 'work', 'exampl', 'ajax', 'call', 'client', 'note', 'page', 'fail', 'everyth', 'still', 'work', 'fine', 'local', 'pc', 'site', 'run', 'share', 'host', 'dreamhost', 'figur', 'url', 'call', 'go', 'like', 'look', 'like', 'call', 'origin', 'client', 'note', 'page', 'prepend', 'client', 'notesajax', 'siteurl', 'empti', 'hack', 'creat', 'extra', 'rout', 'rout', 'web', 'php']"
18,18,18,13591943,73172104,How to receive in django backend from client webcam using django channels,"<p>I am creating a django api that has an endpoint point that receives live video streams from the client webcam and passes the frames through a machine learning model to make real-time predictions which are sent back to the client side. I am trying to use django-channels. I am fairly new at this webrtc thing. I want to know how to implement the django backend to do exactly this.</p>
",15,0,-2,5,javascript;python;django;webrtc;django-channels,2022-07-30 03:36:43,2022-07-30 03:36:43,2022-07-30 03:36:43,i am creating a django api that has an endpoint point that receives live video streams from the client webcam and passes the frames through a machine learning model to make real time predictions which are sent back to the client side  i am trying to use django channels  i am fairly new at this webrtc thing  i want to know how to implement the django backend to do exactly this ,how to receive in django backend from client webcam using django channels,creating django api endpoint point receives live video streams client webcam passes frames machine learning model make real time predictions sent back client side trying use django channels fairly webrtc thing want know implement django backend exactly,receive django backend client webcam using django channels,receive django backend client webcam using django channelscreating django api endpoint point receives live video streams client webcam passes frames machine learning model make real time predictions sent back client side trying use django channels fairly webrtc thing want know implement django backend exactly,"['receive', 'django', 'backend', 'client', 'webcam', 'using', 'django', 'channelscreating', 'django', 'api', 'endpoint', 'point', 'receives', 'live', 'video', 'streams', 'client', 'webcam', 'passes', 'frames', 'machine', 'learning', 'model', 'make', 'real', 'time', 'predictions', 'sent', 'back', 'client', 'side', 'trying', 'use', 'django', 'channels', 'fairly', 'webrtc', 'thing', 'want', 'know', 'implement', 'django', 'backend', 'exactly']","['receiv', 'django', 'backend', 'client', 'webcam', 'use', 'django', 'channelscr', 'django', 'api', 'endpoint', 'point', 'receiv', 'live', 'video', 'stream', 'client', 'webcam', 'pass', 'frame', 'machin', 'learn', 'model', 'make', 'real', 'time', 'predict', 'sent', 'back', 'client', 'side', 'tri', 'use', 'django', 'channel', 'fairli', 'webrtc', 'thing', 'want', 'know', 'implement', 'django', 'backend', 'exactli']"
19,19,19,18413363,73170016,Is it possible write an executable program from start to finish?,"<p>Would it be possible for someone to learn (theoretically) to write a complete executable file by writing it byte-by-byte (i.e. without an assembler or any other third-party program)?</p>
<p>I know that assembly language has a 1 to 1 mapping to machine code, but it is still a human-readable abstraction above it.</p>
<p>If this were possible, how would one go about learning to do so?</p>
",33,0,0,3,assembly;executable;machine-code,2022-07-29 22:09:23,2022-07-29 22:09:23,2022-07-29 23:05:35,would it be possible for someone to learn  theoretically  to write a complete executable file by writing it byte by byte  i e  without an assembler or any other third party program   i know that assembly language has a  to  mapping to machine code  but it is still a human readable abstraction above it  if this were possible  how would one go about learning to do so ,is it possible write an executable program from start to finish ,would possible someone learn theoretically write complete executable file writing byte byte e without assembler third party program know assembly language mapping machine code still human readable abstraction possible would one go learning,possible write executable program start finish,possible write executable program start finishwould possible someone learn theoretically write complete executable file writing byte byte e without assembler third party program know assembly language mapping machine code still human readable abstraction possible would one go learning,"['possible', 'write', 'executable', 'program', 'start', 'finishwould', 'possible', 'someone', 'learn', 'theoretically', 'write', 'complete', 'executable', 'file', 'writing', 'byte', 'byte', 'e', 'without', 'assembler', 'third', 'party', 'program', 'know', 'assembly', 'language', 'mapping', 'machine', 'code', 'still', 'human', 'readable', 'abstraction', 'possible', 'would', 'one', 'go', 'learning']","['possibl', 'write', 'execut', 'program', 'start', 'finishwould', 'possibl', 'someon', 'learn', 'theoret', 'write', 'complet', 'execut', 'file', 'write', 'byte', 'byte', 'e', 'without', 'assembl', 'third', 'parti', 'program', 'know', 'assembl', 'languag', 'map', 'machin', 'code', 'still', 'human', 'readabl', 'abstract', 'possibl', 'would', 'one', 'go', 'learn']"
20,20,20,19648811,73169596,Coif linear regression issue,"<p>I am completing data analysis for a project and keep meeting the same error for the code below:</p>
<pre><code>coeff = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient']) 
</code></pre>
<blockquote>
<p>AttributeError: 'LinearRegression' object has no attribute 'coef_'</p>
</blockquote>
<p>preceding line of code outlines the following about lr variable:</p>
<pre><code>lr= LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
</code></pre>
<p>Here are the imported modules (I think)</p>
<pre><code>import six
import sys
sys.modules['sklearn.externals.six'] = six

from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from six import StringIO
from IPython.display import Image  
import pydotplus

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from pmdarima.arima import auto_arima
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math

# Also:

# Statistics packages 

import statsmodels as statsmodels
import statsmodels.api as sm 
import statsmodels.tsa.api as smt 
from statsmodels.tsa.stattools import grangercausalitytests
from statsmodels.stats.diagnostic import het_arch
from statsmodels.compat import lzip 
from pandas.plotting import lag_plot

# Machine learning 

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree
from sklearn.svm import SVC, SVR
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import classification_report 
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn import metrics

# Added print(type(lr), lr) to the data frame with the following outcome

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

&lt;class 'sklearn.linear_model._base.LinearRegression'&gt; LinearRegression()
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-175-3e14ef09cf34&gt;&quot;, line 2, in &lt;module&gt;
    coeff = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient'])
AttributeError: 'LinearRegression' object has no attribute 'coef_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 1823, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'AttributeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py&quot;, line 1132, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py&quot;, line 313, in wrapped
    return f(*args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py&quot;, line 358, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File &quot;/usr/lib/python3.7/inspect.py&quot;, line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File &quot;/usr/lib/python3.7/inspect.py&quot;, line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File &quot;/usr/lib/python3.7/inspect.py&quot;, line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File &quot;/usr/lib/python3.7/inspect.py&quot;, line 725, in getmodule
    file = getabsfile(object, _filename)
  File &quot;/usr/lib/python3.7/inspect.py&quot;, line 709, in getabsfile
    return os.path.normcase(os.path.abspath(_filename))
  File &quot;/usr/lib/python3.7/posixpath.py&quot;, line 383, in abspath
    cwd = os.getcwd()
OSError: [Errno 107] Transport endpoint is not connected

</code></pre>
<p>Could anyone recommend a solution to this error?</p>
",37,0,0,2,python;scikit-learn,2022-07-29 21:26:17,2022-07-29 21:26:17,2022-07-29 22:16:26,i am completing data analysis for a project and keep meeting the same error for the code below  attributeerror   linearregression  object has no attribute  coef_  preceding line of code outlines the following about lr variable  here are the imported modules  i think  could anyone recommend a solution to this error ,coif linear regression issue,completing data analysis project keep meeting error code attributeerror linearregression object attribute coef_ preceding line code outlines following lr variable imported modules think could anyone recommend solution error,coif linear regression issue,coif linear regression issuecompleting data analysis project keep meeting error code attributeerror linearregression object attribute coef_ preceding line code outlines following lr variable imported modules think could anyone recommend solution error,"['coif', 'linear', 'regression', 'issuecompleting', 'data', 'analysis', 'project', 'keep', 'meeting', 'error', 'code', 'attributeerror', 'linearregression', 'object', 'attribute', 'coef_', 'preceding', 'line', 'code', 'outlines', 'following', 'lr', 'variable', 'imported', 'modules', 'think', 'could', 'anyone', 'recommend', 'solution', 'error']","['coif', 'linear', 'regress', 'issuecomplet', 'data', 'analysi', 'project', 'keep', 'meet', 'error', 'code', 'attributeerror', 'linearregress', 'object', 'attribut', 'coef_', 'preced', 'line', 'code', 'outlin', 'follow', 'lr', 'variabl', 'import', 'modul', 'think', 'could', 'anyon', 'recommend', 'solut', 'error']"
21,21,21,18729811,73168646,How to calculate the weights with machine learning?,"<p>Assuming there is a cooking competition. An agent will evaluate some dishes and choose the best one, each dish is separated into a few aspects(ex: Appearance, Tastes etc.).</p>
<p>Each dish's score in different aspects is known (ex: 20 points for taste, and 15 points for appearance), but the agents weight in these aspects is unknown and the weight adds to 100% (ex: An agent might be 60% taste and 40% appearance assuming there're only two aspects). The final score for the dish is the sum of the product of the score in all aspects with their weight. So the score for this dish will be 20 x 60% + 15 x 40% = 18. There will be several dishes in each competition and the one with the highest point wins.</p>
<p>Now the problem is if we know the scores of each aspect of the dishes, but all we get from the agent is the winning dish. If we run this competition many times, can we use reinforcement learning to find out the weight of each aspect for this agent? Or in general, how do we solve this problem?</p>
",27,1,-2,3,machine-learning;reinforcement-learning;weighted,2022-07-29 19:52:16,2022-07-29 19:52:16,2022-07-29 20:09:52,assuming there is a cooking competition  an agent will evaluate some dishes and choose the best one  each dish is separated into a few aspects ex  appearance  tastes etc    each dish s score in different aspects is known  ex   points for taste  and  points for appearance   but the agent s weight in these aspects is unknown and the weight adds to    ex  an agent might be   taste and   appearance assuming there re only two aspects   the final score for the dish is the sum of the product of the score in all aspects with their weight  so the score for this dish will be  x      x       there will be several dishes in each competition and the one with the highest point wins  now the problem is if we know the scores of each aspect of the dishes  but all we get from the agent is the winning dish  if we run this competition many times  can we use reinforcement learning to find out the weight of each aspect for this agent  or in general  how do we solve this problem ,how to calculate the weights with machine learning ,assuming cooking competition agent evaluate dishes choose best one dish separated aspects ex appearance tastes etc dish score different aspects known ex points taste points appearance agent weight aspects unknown weight adds ex agent might taste appearance assuming two aspects final score dish sum product score aspects weight score dish x x several dishes competition one highest point wins problem know scores aspect dishes get agent winning dish run competition many times use reinforcement learning find weight aspect agent general solve problem,calculate weights machine learning,calculate weights machine learningassuming cooking competition agent evaluate dishes choose best one dish separated aspects ex appearance tastes etc dish score different aspects known ex points taste points appearance agent weight aspects unknown weight adds ex agent might taste appearance assuming two aspects final score dish sum product score aspects weight score dish x x several dishes competition one highest point wins problem know scores aspect dishes get agent winning dish run competition many times use reinforcement learning find weight aspect agent general solve problem,"['calculate', 'weights', 'machine', 'learningassuming', 'cooking', 'competition', 'agent', 'evaluate', 'dishes', 'choose', 'best', 'one', 'dish', 'separated', 'aspects', 'ex', 'appearance', 'tastes', 'etc', 'dish', 'score', 'different', 'aspects', 'known', 'ex', 'points', 'taste', 'points', 'appearance', 'agent', 'weight', 'aspects', 'unknown', 'weight', 'adds', 'ex', 'agent', 'might', 'taste', 'appearance', 'assuming', 'two', 'aspects', 'final', 'score', 'dish', 'sum', 'product', 'score', 'aspects', 'weight', 'score', 'dish', 'x', 'x', 'several', 'dishes', 'competition', 'one', 'highest', 'point', 'wins', 'problem', 'know', 'scores', 'aspect', 'dishes', 'get', 'agent', 'winning', 'dish', 'run', 'competition', 'many', 'times', 'use', 'reinforcement', 'learning', 'find', 'weight', 'aspect', 'agent', 'general', 'solve', 'problem']","['calcul', 'weight', 'machin', 'learningassum', 'cook', 'competit', 'agent', 'evalu', 'dish', 'choos', 'best', 'one', 'dish', 'separ', 'aspect', 'ex', 'appear', 'tast', 'etc', 'dish', 'score', 'differ', 'aspect', 'known', 'ex', 'point', 'tast', 'point', 'appear', 'agent', 'weight', 'aspect', 'unknown', 'weight', 'add', 'ex', 'agent', 'might', 'tast', 'appear', 'assum', 'two', 'aspect', 'final', 'score', 'dish', 'sum', 'product', 'score', 'aspect', 'weight', 'score', 'dish', 'x', 'x', 'sever', 'dish', 'competit', 'one', 'highest', 'point', 'win', 'problem', 'know', 'score', 'aspect', 'dish', 'get', 'agent', 'win', 'dish', 'run', 'competit', 'mani', 'time', 'use', 'reinforc', 'learn', 'find', 'weight', 'aspect', 'agent', 'gener', 'solv', 'problem']"
22,23,23,5353461,52830307,conda: remove all installed packages from base/root environment,"<h2>TL:DR: How can I remove all installed packages from <code>base</code>?</h2>
<p>I installed a bunch of machine learning packages in my <code>base</code> conda environment.</p>
<p>I've now created a <code>ml</code> environment for machine learning, and wish to reset my <code>base</code> environment by removing all the packages installed there.</p>
<p>I've tried:</p>
<pre><code>% activate base
% conda uninstall -n base --all

CondaEnvironmentError: cannot remove current environment. deactivate and run conda remove again
</code></pre>
<p>Apparently, I can't remove packages from the current environment(?!), so lets switch to my <code>ml</code> environment first:</p>
<pre><code>% source activate ml
% conda uninstall -n base --all

CondaEnvironmentError: cannot remove root environment,
       add -n NAME or -p PREFIX option
</code></pre>
<p>Orright, I'll use <code>-p</code> then...</p>
<pre><code>% conda uninstall -p ~/.local/share/miniconda3 --all

CondaEnvironmentError: cannot remove root environment,
       add -n NAME or -p PREFIX option
</code></pre>
<p>How do I uninstall all installed packages in the <code>base</code> or <code>root</code> environment?</p>
",115809,6,80,3,python;conda;miniconda,2018-10-16 10:42:22,2018-10-16 10:42:22,2022-07-29 14:47:27,i installed a bunch of machine learning packages in my base conda environment  i ve now created a ml environment for machine learning  and wish to reset my base environment by removing all the packages installed there  i ve tried  apparently  i can t remove packages from the current environment      so lets switch to my ml environment first  orright  i ll use  p then    how do i uninstall all installed packages in the base or root environment ,conda  remove all installed packages from base root environment,installed bunch machine learning packages base conda environment created ml environment machine learning wish reset base environment removing packages installed tried apparently remove packages current environment lets switch ml environment first orright use p uninstall installed packages base root environment,conda remove installed packages base root environment,conda remove installed packages base root environmentinstalled bunch machine learning packages base conda environment created ml environment machine learning wish reset base environment removing packages installed tried apparently remove packages current environment lets switch ml environment first orright use p uninstall installed packages base root environment,"['conda', 'remove', 'installed', 'packages', 'base', 'root', 'environmentinstalled', 'bunch', 'machine', 'learning', 'packages', 'base', 'conda', 'environment', 'created', 'ml', 'environment', 'machine', 'learning', 'wish', 'reset', 'base', 'environment', 'removing', 'packages', 'installed', 'tried', 'apparently', 'remove', 'packages', 'current', 'environment', 'lets', 'switch', 'ml', 'environment', 'first', 'orright', 'use', 'p', 'uninstall', 'installed', 'packages', 'base', 'root', 'environment']","['conda', 'remov', 'instal', 'packag', 'base', 'root', 'environmentinstal', 'bunch', 'machin', 'learn', 'packag', 'base', 'conda', 'environ', 'creat', 'ml', 'environ', 'machin', 'learn', 'wish', 'reset', 'base', 'environ', 'remov', 'packag', 'instal', 'tri', 'appar', 'remov', 'packag', 'current', 'environ', 'let', 'switch', 'ml', 'environ', 'first', 'orright', 'use', 'p', 'uninstal', 'instal', 'packag', 'base', 'root', 'environ']"
23,24,24,18610866,71958972,How to Send an Audio file (.wav) from My Flutter Application to a Flask Server Instance?,"<p>I have a machine learning model that is saved as .h5 and used in a flask server. The server is supposed to take an audio file as input and return a prediction string.
My Flask server code:</p>
<pre><code>  @app.route(&quot;/predict&quot;, methods=[&quot;POST&quot;])
def predict():
# get file from POST request and save it
audio_file = request.files[&quot;file&quot;]
file_name = str(random.randint(0, 100000)) # generate file name as a dummy random number
#wav_filename = str(random.randint(0, 100000))
audio_file.save(file_name)

# instantiate keyword spotting service singleton and get prediction
kss = Keyword_Spotting_Service() # Where our model is hold

predicted_emotion = kss.predict(file_name)

# we don't need the audio file any more - let's delete it!
os.remove(file_name)

# send back result as a json file (dictionary)
result = {&quot;emotion&quot;: predicted_emotion}
return jsonify(result)
</code></pre>
<p>I tested my server using python client and it worked.</p>
<p>in my flutter app I created a predict method:</p>
<pre><code> final uri = Uri.parse('http://192.168.1.14:5000/predict');
  final request = new http.MultipartRequest(&quot;POST&quot;, uri);
  request.fields['audio'] = &quot;audio&quot;;
  //myStreamController.stream.asBroadcastStream().listen(request);
  final multipartFile = new http.MultipartFile.fromBytes('file', (await rootBundle.load(&quot;assets/audioFile.wav&quot;)).buffer.asUint8List( ), filename: 'audioFile.wav');
  request.files.add(multipartFile);
  request.headers[&quot;Content-Type&quot;] = 'multipart/form-data';
  final streamedResponse = await request.send();
  // final x =  await streamedResponse.stream.toBytes();
  Response response = await http.Response.fromStream(streamedResponse);

  Map&lt;String, dynamic&gt; result = jsonDecode(response.body);
  var resultx = jsonDecode(json.encode(response.body));
  predic = &quot;${resultx['emotion']}&quot;;
  // resultx.clear();
  return predic;
</code></pre>
<p>It keeps giving me this error: File contains data in an unknown format (Runtime Error).</p>
<p>What am I missing?
Any help will be highly appreciated.</p>
",249,0,0,3,flutter;flask;audio,2022-04-21 21:02:41,2022-04-21 21:02:41,2022-07-29 14:13:53,i tested my server using python client and it worked  in my flutter app i created a predict method  it keeps giving me this error  file contains data in an unknown format  runtime error  ,how to send an audio file   wav  from my flutter application to a flask server instance ,tested server using python client worked flutter app created predict method keeps giving error file contains data unknown format runtime error,send audio file wav flutter application flask server instance,send audio file wav flutter application flask server instancetested server using python client worked flutter app created predict method keeps giving error file contains data unknown format runtime error,"['send', 'audio', 'file', 'wav', 'flutter', 'application', 'flask', 'server', 'instancetested', 'server', 'using', 'python', 'client', 'worked', 'flutter', 'app', 'created', 'predict', 'method', 'keeps', 'giving', 'error', 'file', 'contains', 'data', 'unknown', 'format', 'runtime', 'error']","['send', 'audio', 'file', 'wav', 'flutter', 'applic', 'flask', 'server', 'instancetest', 'server', 'use', 'python', 'client', 'work', 'flutter', 'app', 'creat', 'predict', 'method', 'keep', 'give', 'error', 'file', 'contain', 'data', 'unknown', 'format', 'runtim', 'error']"
24,25,25,5465000,73146325,Set &quot;azureML.CLI Compatibility Mode&quot; in Visual Studio Code,"<p>In <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-debug-visual-studio-code"" rel=""nofollow noreferrer"">this</a> tutorial, it is asked to set the <code>azureML.CLI Compatibility Mode</code> setting to 1.0 for the Azure Machine Learning extension. However, when checking the settings of the extension, such option is not available.</p>
",51,1,0,3,azure;visual-studio-code;azure-machine-learning-service,2022-07-28 04:41:59,2022-07-28 04:41:59,2022-07-29 12:57:00,in  tutorial  it is asked to set the azureml cli compatibility mode setting to   for the azure machine learning extension  however  when checking the settings of the extension  such option is not available ,set  azureml cli compatibility mode  in visual studio code,tutorial asked set azureml cli compatibility mode setting azure machine learning extension however checking settings extension option available,set azureml cli compatibility mode visual studio code,set azureml cli compatibility mode visual studio codetutorial asked set azureml cli compatibility mode setting azure machine learning extension however checking settings extension option available,"['set', 'azureml', 'cli', 'compatibility', 'mode', 'visual', 'studio', 'codetutorial', 'asked', 'set', 'azureml', 'cli', 'compatibility', 'mode', 'setting', 'azure', 'machine', 'learning', 'extension', 'however', 'checking', 'settings', 'extension', 'option', 'available']","['set', 'azureml', 'cli', 'compat', 'mode', 'visual', 'studio', 'codetutori', 'ask', 'set', 'azureml', 'cli', 'compat', 'mode', 'set', 'azur', 'machin', 'learn', 'extens', 'howev', 'check', 'set', 'extens', 'option', 'avail']"
25,26,26,13237420,73163771,os.walk and glob.glob return different results,"<p>I am trying to pre-process some data for a machine learning project, where I have already cloned different repositories from GitHub, and am now working on cleaning the data to only get the Python files. I came across <code>os.walk</code> and <code>glob.glob</code>, but they seem to return a different number of files. Below are my code for each of the methods:</p>
<pre><code># Using os.walk
abs_paths = []
repos_path = f&quot;{(os.path.dirname(__file__))}/repos&quot;
    for dirpath, dirnames, filenames in os.walk(repos_path):
        for f in filenames:
            abs_path = os.path.join(dirpath, f)
            if abs_path.endswith(&quot;.py&quot;):
                abs_paths.append(abs_path)
print(len(abs_paths)) # 10235
print(len(set(abs_path))) # 10235
</code></pre>
<pre><code># Using glob.glob
pyfiles = glob.glob(f&quot;{(os.path.dirname(__file__))}/repos/**/*.py&quot;, recursive=True)
print(len(pyfiles)) # 10375
print(len(set(pyfile))) # 10375
</code></pre>
<p><strong>As a result, <code>os.walk</code> returns 10235 files, but <code>glob.glob</code> returns 10375.</strong> When I compare the (set) difference between the two results, I see that <code>os.walk</code> does not return some files containing <code>/venv/</code> in their path, but <code>glob.glob</code> does. However, I also see that <code>glob.glob</code> does not return some files containing <code>/.venv/</code> in their path, which <code>os.walk</code> does.</p>
<p>For example, the following is returned by <code>os.walk</code>, but not by <code>glob.glob</code>:</p>
<p><code>[pathtorepos]/mreyes2017/api_python_flask_apispec/.venv/Lib/site-packages/setuptools/_vendor/pyparsing.py'</code></p>
<p>Also, out of the 10375 files returned using <code>glob.glob</code>, only 8874 of them appears in the result of <code>os.walk</code>.</p>
<p>Therefore, I am confused by what the differences in these two methods are. Any help would be appreciated!</p>
",34,0,1,5,python;web-scraping;filesystems;glob;os.walk,2022-07-29 11:30:43,2022-07-29 11:30:43,2022-07-29 12:16:02,i am trying to pre process some data for a machine learning project  where i have already cloned different repositories from github  and am now working on cleaning the data to only get the python files  i came across os walk and glob glob  but they seem to return a different number of files  below are my code for each of the methods  as a result  os walk returns  files  but glob glob returns   when i compare the  set  difference between the two results  i see that os walk does not return some files containing  venv  in their path  but glob glob does  however  i also see that glob glob does not return some files containing   venv  in their path  which os walk does  for example  the following is returned by os walk  but not by glob glob   pathtorepos  mreyes api_python_flask_apispec  venv lib site packages setuptools _vendor pyparsing py  also  out of the  files returned using glob glob  only  of them appears in the result of os walk  therefore  i am confused by what the differences in these two methods are  any help would be appreciated ,os walk and glob glob return different results,trying pre process data machine learning project already cloned different repositories github working cleaning data get python files came across os walk glob glob seem return different number files code methods result os walk returns files glob glob returns compare set difference two results see os walk return files containing venv path glob glob however also see glob glob return files containing venv path os walk example following returned os walk glob glob pathtorepos mreyes api_python_flask_apispec venv lib site packages setuptools _vendor pyparsing py also files returned using glob glob appears result os walk therefore confused differences two methods help would appreciated,os walk glob glob return different results,os walk glob glob return different resultstrying pre process data machine learning project already cloned different repositories github working cleaning data get python files came across os walk glob glob seem return different number files code methods result os walk returns files glob glob returns compare set difference two results see os walk return files containing venv path glob glob however also see glob glob return files containing venv path os walk example following returned os walk glob glob pathtorepos mreyes api_python_flask_apispec venv lib site packages setuptools _vendor pyparsing py also files returned using glob glob appears result os walk therefore confused differences two methods help would appreciated,"['os', 'walk', 'glob', 'glob', 'return', 'different', 'resultstrying', 'pre', 'process', 'data', 'machine', 'learning', 'project', 'already', 'cloned', 'different', 'repositories', 'github', 'working', 'cleaning', 'data', 'get', 'python', 'files', 'came', 'across', 'os', 'walk', 'glob', 'glob', 'seem', 'return', 'different', 'number', 'files', 'code', 'methods', 'result', 'os', 'walk', 'returns', 'files', 'glob', 'glob', 'returns', 'compare', 'set', 'difference', 'two', 'results', 'see', 'os', 'walk', 'return', 'files', 'containing', 'venv', 'path', 'glob', 'glob', 'however', 'also', 'see', 'glob', 'glob', 'return', 'files', 'containing', 'venv', 'path', 'os', 'walk', 'example', 'following', 'returned', 'os', 'walk', 'glob', 'glob', 'pathtorepos', 'mreyes', 'api_python_flask_apispec', 'venv', 'lib', 'site', 'packages', 'setuptools', '_vendor', 'pyparsing', 'py', 'also', 'files', 'returned', 'using', 'glob', 'glob', 'appears', 'result', 'os', 'walk', 'therefore', 'confused', 'differences', 'two', 'methods', 'help', 'would', 'appreciated']","['os', 'walk', 'glob', 'glob', 'return', 'differ', 'resultstri', 'pre', 'process', 'data', 'machin', 'learn', 'project', 'alreadi', 'clone', 'differ', 'repositori', 'github', 'work', 'clean', 'data', 'get', 'python', 'file', 'came', 'across', 'os', 'walk', 'glob', 'glob', 'seem', 'return', 'differ', 'number', 'file', 'code', 'method', 'result', 'os', 'walk', 'return', 'file', 'glob', 'glob', 'return', 'compar', 'set', 'differ', 'two', 'result', 'see', 'os', 'walk', 'return', 'file', 'contain', 'venv', 'path', 'glob', 'glob', 'howev', 'also', 'see', 'glob', 'glob', 'return', 'file', 'contain', 'venv', 'path', 'os', 'walk', 'exampl', 'follow', 'return', 'os', 'walk', 'glob', 'glob', 'pathtorepo', 'mrey', 'api_python_flask_apispec', 'venv', 'lib', 'site', 'packag', 'setuptool', '_vendor', 'pypars', 'py', 'also', 'file', 'return', 'use', 'glob', 'glob', 'appear', 'result', 'os', 'walk', 'therefor', 'confus', 'differ', 'two', 'method', 'help', 'would', 'appreci']"
26,27,27,8223846,73163816,"For mobile, is tflite always best?","<p>As TensorFlow Lite is a set of tools that enables on-device machine learning by helping developers run their models on mobile (<a href=""https://www.tensorflow.org/lite/guide"" rel=""nofollow noreferrer"">https://www.tensorflow.org/lite/guide</a>), is tflite the optimal file type for mobile?</p>
<p>If there is a decent model (pb file or whatever file type) then always need to be converted to tflite file for mobile?</p>
",9,0,0,2,mobile;tflite,2022-07-29 11:33:56,2022-07-29 11:33:56,2022-07-29 11:33:56,as tensorflow lite is a set of tools that enables on device machine learning by helping developers run their models on mobile     is tflite the optimal file type for mobile  if there is a decent model  pb file or whatever file type  then always need to be converted to tflite file for mobile ,for mobile  is tflite always best ,tensorflow lite set tools enables device machine learning helping developers run models mobile tflite optimal file type mobile decent model pb file whatever file type always need converted tflite file mobile,mobile tflite always best,mobile tflite always besttensorflow lite set tools enables device machine learning helping developers run models mobile tflite optimal file type mobile decent model pb file whatever file type always need converted tflite file mobile,"['mobile', 'tflite', 'always', 'besttensorflow', 'lite', 'set', 'tools', 'enables', 'device', 'machine', 'learning', 'helping', 'developers', 'run', 'models', 'mobile', 'tflite', 'optimal', 'file', 'type', 'mobile', 'decent', 'model', 'pb', 'file', 'whatever', 'file', 'type', 'always', 'need', 'converted', 'tflite', 'file', 'mobile']","['mobil', 'tflite', 'alway', 'besttensorflow', 'lite', 'set', 'tool', 'enabl', 'devic', 'machin', 'learn', 'help', 'develop', 'run', 'model', 'mobil', 'tflite', 'optim', 'file', 'type', 'mobil', 'decent', 'model', 'pb', 'file', 'whatev', 'file', 'type', 'alway', 'need', 'convert', 'tflite', 'file', 'mobil']"
27,28,28,19642189,73156532,Can queue trigger in azure send more than one data in one go?,"<p>My project flow contains-</p>
<ol>
<li>a machine learning container instance which receives some data and return a output.</li>
<li>function app for queue trigger.</li>
</ol>
<p>from queues, data goes to container instance and returns output,then output is stored in a different queue. Now this data flow is happening with FIFO. It is sending one data at a time. It is making my process very slow.
Is there any chance I can send whole queue data in just one go to container instance?
You can provide alternative solution as well.</p>
",31,1,0,4,azure;machine-learning;azure-queues;azure-blob-trigger,2022-07-28 19:54:36,2022-07-28 19:54:36,2022-07-29 11:19:39,my project flow contains ,can queue trigger in azure send more than one data in one go ,project flow contains,queue trigger azure send one data one go,queue trigger azure send one data one goproject flow contains,"['queue', 'trigger', 'azure', 'send', 'one', 'data', 'one', 'goproject', 'flow', 'contains']","['queue', 'trigger', 'azur', 'send', 'one', 'data', 'one', 'goproject', 'flow', 'contain']"
28,29,29,19381301,73136961,np.pad() converting my array into a new array with only values equal to 0,"<p>I am working on a NLP machine learning project and I want to add some additional data to the original dataset used in training to compare results. To this end, the shape of my new dataset which is a numpy array (i.e. numpy array with previous data + additional data) needs to match the shape of the test set used in the first round of training.</p>
<p>When I used the code below to pad my numpy array, I noticed that all the float numbers disappeared and I got a new array containing only zeros specially if I reassign with a new variable.</p>
<pre><code>Xfeatures_new_pad =np.pad(Xfeatures_new, (2039, 0), 'constant')
</code></pre>
<p>The same happens if I use pad_sequences():</p>
<pre><code>Xfeatures_new_pad=pad_sequences(Xfeatures_new, maxlen=Xfeatures_train.shape[1], 
padding='pre')
</code></pre>
<p>I have also tried the below code:</p>
<pre><code>result = np.zeros(Xfeatures_train.shape)
result[:Xfeatures_new.shape[0],:Xfeatures_new.shape[1]] = 
Xfeatures_new
new_Xfeatures=result[:Xfeatures_new.shape[0]]
new_Xfeatures_train = np.concatenate((Xfeatures_train, 
new_Xfeatures), axis=0)


result_y = np.zeros(y_train.shape)
result_y[:y_train_new.shape[0],:y_train_new.shape[1]] = 
y_train_new
new_y=result_y[:y_train_new.shape[0]]
new_y_train = np.concatenate((y_train, new_y), axis=0)
</code></pre>
<p>But I am getting an error:</p>
<pre><code>ValueError                                Traceback (most recent 
call last)
&lt;ipython-input-33-a696a6ccc21c&gt; in &lt;module&gt;()
  1 
  2 result = np.zeros(Xfeatures_train.shape)
----&gt; 3 result[:Xfeatures_new.shape[0],:Xfeatures_new.shape[1]] = 
Xfeatures_new
  4 new_Xfeatures=result[:Xfeatures_new.shape[0]]
  5 new_Xfeatures_train = np.concatenate((Xfeatures_train, 
new_Xfeatures), axis=0)

ValueError: could not broadcast input array from shape 
(600,13072) into shape (400,13072)
</code></pre>
<p>Any ideas why this is happening?</p>
",28,1,0,5,python;arrays;numpy;keras;padding,2022-07-27 14:17:11,2022-07-27 14:17:11,2022-07-29 09:54:55,i am working on a nlp machine learning project and i want to add some additional data to the original dataset used in training to compare results  to this end  the shape of my new dataset which is a numpy array  i e  numpy array with previous data   additional data  needs to match the shape of the test set used in the first round of training  when i used the code below to pad my numpy array  i noticed that all the float numbers disappeared and i got a new array containing only zeros specially if i reassign with a new variable  the same happens if i use pad_sequences    i have also tried the below code  but i am getting an error  any ideas why this is happening ,np pad   converting my array into a new array with only values equal to ,working nlp machine learning project want additional data original dataset used training compare results end shape dataset numpy array e numpy array previous data additional data needs match shape test set used first round training used code pad numpy array noticed float numbers disappeared got array containing zeros specially reassign variable happens use pad_sequences also tried code getting error ideas happening,np pad converting array array values equal,np pad converting array array values equalworking nlp machine learning project want additional data original dataset used training compare results end shape dataset numpy array e numpy array previous data additional data needs match shape test set used first round training used code pad numpy array noticed float numbers disappeared got array containing zeros specially reassign variable happens use pad_sequences also tried code getting error ideas happening,"['np', 'pad', 'converting', 'array', 'array', 'values', 'equalworking', 'nlp', 'machine', 'learning', 'project', 'want', 'additional', 'data', 'original', 'dataset', 'used', 'training', 'compare', 'results', 'end', 'shape', 'dataset', 'numpy', 'array', 'e', 'numpy', 'array', 'previous', 'data', 'additional', 'data', 'needs', 'match', 'shape', 'test', 'set', 'used', 'first', 'round', 'training', 'used', 'code', 'pad', 'numpy', 'array', 'noticed', 'float', 'numbers', 'disappeared', 'got', 'array', 'containing', 'zeros', 'specially', 'reassign', 'variable', 'happens', 'use', 'pad_sequences', 'also', 'tried', 'code', 'getting', 'error', 'ideas', 'happening']","['np', 'pad', 'convert', 'array', 'array', 'valu', 'equalwork', 'nlp', 'machin', 'learn', 'project', 'want', 'addit', 'data', 'origin', 'dataset', 'use', 'train', 'compar', 'result', 'end', 'shape', 'dataset', 'numpi', 'array', 'e', 'numpi', 'array', 'previou', 'data', 'addit', 'data', 'need', 'match', 'shape', 'test', 'set', 'use', 'first', 'round', 'train', 'use', 'code', 'pad', 'numpi', 'array', 'notic', 'float', 'number', 'disappear', 'got', 'array', 'contain', 'zero', 'special', 'reassign', 'variabl', 'happen', 'use', 'pad_sequ', 'also', 'tri', 'code', 'get', 'error', 'idea', 'happen']"
29,30,30,9804130,73160650,"NestJS, Typeorm, SQL Server - failed to connect to localhost:1433 - Could not connect (sequence)","<p>I am trying to connect to a locally running a SQL Server database using Typeorm and nestJS.</p>
<p>I have created a new database in SSMS Express, and created a new login, giving the user <code>db_owner</code> permissions for the given database.</p>
<p>I have installed the following packages:</p>
<pre><code>@nestjs/typeorm&quot;: &quot;^9.0.0&quot;,
&quot;mssql&quot;: &quot;^8.1.2&quot;,
</code></pre>
<p>This is the error that I receive:</p>
<blockquote>
<p>[Nest] 25180  - 07/29/2022, 10:50:42 AM   ERROR [ExceptionHandler] Failed to connect to localhost:1433 - Could not connect (sequence)</p>
<p>ConnectionError: Failed to connect to localhost:1433 - Could not connect (sequence)</p>
<p>at connectListener (C:\Projects\lucas\node_modules\mssql\lib\tedious\connection-pool.js:70:17)<br />
at Connection.onConnect (C:\Projects\lucas\node_modules\tedious\src\connection.ts:1763:9)<br />
at Object.onceWrapper (events.js:417:26)<br />
at Connection.emit (events.js:310:20)<br />
at Connection.emit (C:\Projects\lucas\node_modules\tedious\src\connection.ts:1906:18)<br />
at Connection.socketError (C:\Projects\lucas\node_modules\tedious\src\connection.ts:2221:12)<br />
at callback (C:\Projects\lucas\node_modules\tedious\src\connection.ts:2006:21)<br />
at SequentialConnectionStrategy.connect (C:\Projects\lucas\node_modules\tedious\src\connector.ts:118:14)<br />
at Socket.onError (C:\Projects\lucas\node_modules\tedious\src\connector.ts:144:12)<br />
at Socket.emit (events.js:310:20)</p>
</blockquote>
<p>This is my configuration for the connection to the database:</p>
<pre><code>TypeOrmModule.forRoot({
      type: 'mssql',
      host: 'localhost',
      port: 1433,
      username: 'timetracker_user',
      password: 'ihaveagun',
      database: 'timetracker',
      // entities: [User], // define entities manually
      autoLoadEntities: true, // auto loads all entities registered through the forFeature() method
      synchronize: true, // should be set to false in production.
    }),
</code></pre>
<p>The largest issue that I am having is that I cannot find the resources to help me. When googling, I found articles on enabling TCP/IP for SQL Server, however when researching on how to do this, all resources point to MS Server tools. I am doing this on Windows 10 machine and I am using SQL Server Management Studio 18.</p>
<p>At this stage, I simply want to connect to my db locally, this is for learning purposes.</p>
<p>Could I please get some help on how to debug and fix this issue?</p>
<p>Many thanks in advance!</p>
",35,0,0,4,node.js;sql-server;nestjs;typeorm,2022-07-29 04:09:42,2022-07-29 04:09:42,2022-07-29 06:59:24,i am trying to connect to a locally running a sql server database using typeorm and nestjs  i have created a new database in ssms express  and created a new login  giving the user db_owner permissions for the given database  i have installed the following packages  this is the error that i receive   nest             am   error  exceptionhandler  failed to connect to localhost    could not connect  sequence  connectionerror  failed to connect to localhost    could not connect  sequence  this is my configuration for the connection to the database  the largest issue that i am having is that i cannot find the resources to help me  when googling  i found articles on enabling tcp ip for sql server  however when researching on how to do this  all resources point to ms server tools  i am doing this on windows  machine and i am using sql server management studio   at this stage  i simply want to connect to my db locally  this is for learning purposes  could i please get some help on how to debug and fix this issue  many thanks in advance ,nestjs  typeorm  sql server   failed to connect to localhost    could not connect  sequence ,trying connect locally running sql server database using typeorm nestjs created database ssms express created login giving user db_owner permissions given database installed following packages error receive nest error exceptionhandler failed connect localhost could connect sequence connectionerror failed connect localhost could connect sequence configuration connection database largest issue cannot find resources help googling found articles enabling tcp ip sql server however researching resources point ms server tools windows machine using sql server management studio stage simply want connect db locally learning purposes could please get help debug fix issue many thanks advance,nestjs typeorm sql server failed connect localhost could connect sequence,nestjs typeorm sql server failed connect localhost could connect sequencetrying connect locally running sql server database using typeorm nestjs created database ssms express created login giving user db_owner permissions given database installed following packages error receive nest error exceptionhandler failed connect localhost could connect sequence connectionerror failed connect localhost could connect sequence configuration connection database largest issue cannot find resources help googling found articles enabling tcp ip sql server however researching resources point ms server tools windows machine using sql server management studio stage simply want connect db locally learning purposes could please get help debug fix issue many thanks advance,"['nestjs', 'typeorm', 'sql', 'server', 'failed', 'connect', 'localhost', 'could', 'connect', 'sequencetrying', 'connect', 'locally', 'running', 'sql', 'server', 'database', 'using', 'typeorm', 'nestjs', 'created', 'database', 'ssms', 'express', 'created', 'login', 'giving', 'user', 'db_owner', 'permissions', 'given', 'database', 'installed', 'following', 'packages', 'error', 'receive', 'nest', 'error', 'exceptionhandler', 'failed', 'connect', 'localhost', 'could', 'connect', 'sequence', 'connectionerror', 'failed', 'connect', 'localhost', 'could', 'connect', 'sequence', 'configuration', 'connection', 'database', 'largest', 'issue', 'can', 'not', 'find', 'resources', 'help', 'googling', 'found', 'articles', 'enabling', 'tcp', 'ip', 'sql', 'server', 'however', 'researching', 'resources', 'point', 'ms', 'server', 'tools', 'windows', 'machine', 'using', 'sql', 'server', 'management', 'studio', 'stage', 'simply', 'want', 'connect', 'db', 'locally', 'learning', 'purposes', 'could', 'please', 'get', 'help', 'debug', 'fix', 'issue', 'many', 'thanks', 'advance']","['nestj', 'typeorm', 'sql', 'server', 'fail', 'connect', 'localhost', 'could', 'connect', 'sequencetri', 'connect', 'local', 'run', 'sql', 'server', 'databas', 'use', 'typeorm', 'nestj', 'creat', 'databas', 'ssm', 'express', 'creat', 'login', 'give', 'user', 'db_owner', 'permiss', 'given', 'databas', 'instal', 'follow', 'packag', 'error', 'receiv', 'nest', 'error', 'exceptionhandl', 'fail', 'connect', 'localhost', 'could', 'connect', 'sequenc', 'connectionerror', 'fail', 'connect', 'localhost', 'could', 'connect', 'sequenc', 'configur', 'connect', 'databas', 'largest', 'issu', 'can', 'not', 'find', 'resourc', 'help', 'googl', 'found', 'articl', 'enabl', 'tcp', 'ip', 'sql', 'server', 'howev', 'research', 'resourc', 'point', 'ms', 'server', 'tool', 'window', 'machin', 'use', 'sql', 'server', 'manag', 'studio', 'stage', 'simpli', 'want', 'connect', 'db', 'local', 'learn', 'purpos', 'could', 'pleas', 'get', 'help', 'debug', 'fix', 'issu', 'mani', 'thank', 'advanc']"
30,31,31,138645,73161162,Building a decision/diagnostic tree,"<p>I am building a diagnostic tree to debug a distributed system and interested to know if there are any tools/libraries in Python/Go</p>
<ol>
<li>Help in representing a decision/diagnostic tree graphically.</li>
<li>Converting this tree representation to code.</li>
</ol>
<p>Each node in the tree would run a specific check and based on the outcome (True/False) traverse to the corresponding children node till it reaches the leaf node. So in future user can add a new node anywhere in the visual tree, generated code would incorporate this new node.</p>
<p>I looked into scikit and others but its very specific to Machine learning usecases. A simple binary tree implementation would suffice for my tree.</p>
",28,0,-6,3,python;go;decision-tree,2022-07-29 05:54:32,2022-07-29 05:54:32,2022-07-29 05:58:38,i am building a diagnostic tree to debug a distributed system and interested to know if there are any tools libraries in python go each node in the tree would run a specific check and based on the outcome  true false  traverse to the corresponding children node till it reaches the leaf node  so in future user can add a new node anywhere in the visual tree  generated code would incorporate this new node  i looked into scikit and others but its very specific to machine learning usecases  a simple binary tree implementation would suffice for my tree ,building a decision diagnostic tree,building diagnostic tree debug distributed system interested know tools libraries python go node tree would run specific check based outcome true false traverse corresponding children node till reaches leaf node future user node anywhere visual tree generated code would incorporate node looked scikit others specific machine learning usecases simple binary tree implementation would suffice tree,building decision diagnostic tree,building decision diagnostic treebuilding diagnostic tree debug distributed system interested know tools libraries python go node tree would run specific check based outcome true false traverse corresponding children node till reaches leaf node future user node anywhere visual tree generated code would incorporate node looked scikit others specific machine learning usecases simple binary tree implementation would suffice tree,"['building', 'decision', 'diagnostic', 'treebuilding', 'diagnostic', 'tree', 'debug', 'distributed', 'system', 'interested', 'know', 'tools', 'libraries', 'python', 'go', 'node', 'tree', 'would', 'run', 'specific', 'check', 'based', 'outcome', 'true', 'false', 'traverse', 'corresponding', 'children', 'node', 'till', 'reaches', 'leaf', 'node', 'future', 'user', 'node', 'anywhere', 'visual', 'tree', 'generated', 'code', 'would', 'incorporate', 'node', 'looked', 'scikit', 'others', 'specific', 'machine', 'learning', 'usecases', 'simple', 'binary', 'tree', 'implementation', 'would', 'suffice', 'tree']","['build', 'decis', 'diagnost', 'treebuild', 'diagnost', 'tree', 'debug', 'distribut', 'system', 'interest', 'know', 'tool', 'librari', 'python', 'go', 'node', 'tree', 'would', 'run', 'specif', 'check', 'base', 'outcom', 'true', 'fals', 'travers', 'correspond', 'children', 'node', 'till', 'reach', 'leaf', 'node', 'futur', 'user', 'node', 'anywher', 'visual', 'tree', 'gener', 'code', 'would', 'incorpor', 'node', 'look', 'scikit', 'other', 'specif', 'machin', 'learn', 'usecas', 'simpl', 'binari', 'tree', 'implement', 'would', 'suffic', 'tree']"
31,32,32,7942315,47118187,Cqlsh does now work on newly installed Cassandra on Ubuntu 16.04,"<p>I am getting this error when I try cqlsh or cqlsh 127.0.0.1:9160 :</p>

<pre><code>Connection error: ('Unable to connect to any servers', {'127.0.0.1': error(111, ""Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused"")})
</code></pre>

<p>I just started learning Cassandra. My machine had an installation of Cassandra for something else, so I uninstalled and installed Cassandra 3.11.1 to follow tutorials. I followed <a href=""https://stackoverflow.com/questions/13390775/how-can-i-reinstall-a-cassandra-on-ubuntu"">this</a> to uninstall, I used <a href=""https://www.rosehosting.com/blog/how-to-install-apache-cassandra-on-ubuntu-16-04/"" rel=""nofollow noreferrer"">this</a> instruction to install.</p>

<p>My hosts file is:</p>

<pre><code>127.0.0.1   localhost
</code></pre>

<p>My python is version 2.7.12 and some Cassandra settings are:</p>

<pre><code>start_rpc: false    
rpc_address: localhost
rpc_port: 9160
</code></pre>

<p>Cassandra status is active, and node was up and normal when I first tried ""sudo nodetool status"", but now it is erroting after I changed start_rpc to true and change it back to false :( Node error message is:</p>

<pre><code>nodetool: Failed to connect to '127.0.0.1:7199' - ConnectException: 'Connection refused'.
</code></pre>

<p>I read <a href=""http://thelastpickle.com/blog/2016/08/16/cqlsh-broken-on-fresh-installs.html"" rel=""nofollow noreferrer"">this</a> and tried his suggestion, and it did not work.</p>

<p>Also, ideally, I want to use python 3. Is it a bad idea?</p>
",370,2,0,5,python;ubuntu;cassandra;cassandra-3.0;cqlsh,2017-11-05 06:31:18,2017-11-05 06:31:18,2022-07-29 05:35:58,i am getting this error when i try cqlsh or cqlsh        i just started learning cassandra  my machine had an installation of cassandra for something else  so i uninstalled and installed cassandra    to follow tutorials  i followed  to uninstall  i used  instruction to install  my hosts file is  my python is version    and some cassandra settings are  cassandra status is active  and node was up and normal when i first tried sudo nodetool status  but now it is erroting after i changed start_rpc to true and change it back to false    node error message is  i read  and tried his suggestion  and it did not work  also  ideally  i want to use python   is it a bad idea ,cqlsh does now work on newly installed cassandra on ubuntu  ,getting error try cqlsh cqlsh started learning cassandra machine installation cassandra something else uninstalled installed cassandra follow tutorials followed uninstall used instruction install hosts file python version cassandra settings cassandra status active node normal first tried sudo nodetool status erroting changed start_rpc true change back false node error message read tried suggestion work also ideally want use python bad idea,cqlsh work newly installed cassandra ubuntu,cqlsh work newly installed cassandra ubuntugetting error try cqlsh cqlsh started learning cassandra machine installation cassandra something else uninstalled installed cassandra follow tutorials followed uninstall used instruction install hosts file python version cassandra settings cassandra status active node normal first tried sudo nodetool status erroting changed start_rpc true change back false node error message read tried suggestion work also ideally want use python bad idea,"['cqlsh', 'work', 'newly', 'installed', 'cassandra', 'ubuntugetting', 'error', 'try', 'cqlsh', 'cqlsh', 'started', 'learning', 'cassandra', 'machine', 'installation', 'cassandra', 'something', 'else', 'uninstalled', 'installed', 'cassandra', 'follow', 'tutorials', 'followed', 'uninstall', 'used', 'instruction', 'install', 'hosts', 'file', 'python', 'version', 'cassandra', 'settings', 'cassandra', 'status', 'active', 'node', 'normal', 'first', 'tried', 'sudo', 'nodetool', 'status', 'erroting', 'changed', 'start_rpc', 'true', 'change', 'back', 'false', 'node', 'error', 'message', 'read', 'tried', 'suggestion', 'work', 'also', 'ideally', 'want', 'use', 'python', 'bad', 'idea']","['cqlsh', 'work', 'newli', 'instal', 'cassandra', 'ubuntuget', 'error', 'tri', 'cqlsh', 'cqlsh', 'start', 'learn', 'cassandra', 'machin', 'instal', 'cassandra', 'someth', 'els', 'uninstal', 'instal', 'cassandra', 'follow', 'tutori', 'follow', 'uninstal', 'use', 'instruct', 'instal', 'host', 'file', 'python', 'version', 'cassandra', 'set', 'cassandra', 'statu', 'activ', 'node', 'normal', 'first', 'tri', 'sudo', 'nodetool', 'statu', 'errot', 'chang', 'start_rpc', 'true', 'chang', 'back', 'fals', 'node', 'error', 'messag', 'read', 'tri', 'suggest', 'work', 'also', 'ideal', 'want', 'use', 'python', 'bad', 'idea']"
32,33,33,19637029,73148852,ufunc &#39;boxcox1p&#39; not supported for the input types. the inputs could not be safely coerced to any supported types according to the casting rule &#39;safe&#39;,"<p>I'm having this code (for machine learning) below:</p>
<pre><code>from scipy.special import boxcox1p
from scipy.special import boxcox
from scipy.special import inv_boxcox
df_trans=df1.apply(lambda x: boxcox1p(x,0.0))
</code></pre>
<p>With <code>df1</code> being a dataframe containing date and some other values</p>
<p>However, after running the above codes, I got this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Input In [585], in &lt;cell line: 4&gt;()
      2 from scipy.special import boxcox
      3 from scipy.special import inv_boxcox
----&gt; 4 df_trans=df1.apply(lambda x: boxcox1p(x,0.0))

TypeError: ufunc 'boxcox1p' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>
<p>How do I fix this?</p>
<p>Edited: This is part of the code sample:</p>
<pre><code>    Quantity   Price        Difference  Money Received
0   55419      12.908304    8.518790    69665.133754
1   45179      28.492719    8.518790    125359.752289
2   11985      17.040535    18.776097   19888.813469
</code></pre>
",31,0,0,3,python;pandas;scipy,2022-07-28 10:28:58,2022-07-28 10:28:58,2022-07-29 04:33:20,i m having this code  for machine learning  below  with df being a dataframe containing date and some other values however  after running the above codes  i got this error  how do i fix this  edited  this is part of the code sample ,ufunc    boxcoxp    not supported for the input types  the inputs could not be safely coerced to any supported types according to the casting rule    safe   ,code machine learning df dataframe containing date values however running codes got error fix edited part code sample,ufunc boxcoxp supported input types inputs could safely coerced supported types according casting rule safe,ufunc boxcoxp supported input types inputs could safely coerced supported types according casting rule safecode machine learning df dataframe containing date values however running codes got error fix edited part code sample,"['ufunc', 'boxcoxp', 'supported', 'input', 'types', 'inputs', 'could', 'safely', 'coerced', 'supported', 'types', 'according', 'casting', 'rule', 'safecode', 'machine', 'learning', 'df', 'dataframe', 'containing', 'date', 'values', 'however', 'running', 'codes', 'got', 'error', 'fix', 'edited', 'part', 'code', 'sample']","['ufunc', 'boxcoxp', 'support', 'input', 'type', 'input', 'could', 'safe', 'coerc', 'support', 'type', 'accord', 'cast', 'rule', 'safecod', 'machin', 'learn', 'df', 'datafram', 'contain', 'date', 'valu', 'howev', 'run', 'code', 'got', 'error', 'fix', 'edit', 'part', 'code', 'sampl']"
33,34,34,17903077,72707777,ml.net image classification categories problem,"<p>So i made a Machine Learning project with images classification auto trained and it actually work good!</p>
<p>The only problem im facing is:</p>
<p>let say i have 2 categories, Cars and Boats. The machine is 100% accurate when it come to cars or boats but if i put a random picture like a dog or an house, it still validate it as a car or a boat...? So i guess its just run an algorythm and chose the higher score of similarity.</p>
<p>So i checked on youtube, stack and many others forums and website for an answer but i found nothing about. So my last hope is to ask the question in here.</p>
<p>Can someone can tell me what should i do to have the machine have a mismatch when the image analyzed is not one of the categories? I tried to implement a code that would be based on score to reject scores under X number but the thing is that some image that are out of the categories get an higher score than an image in the categories... at this point im pretty lost.</p>
<p>thank you</p>
",23,1,3,2,c#;ml.net,2022-06-22 01:31:00,2022-06-22 01:31:00,2022-07-29 00:48:46,so i made a machine learning project with images classification auto trained and it actually work good  the only problem im facing is  let say i have  categories  cars and boats  the machine is   accurate when it come to cars or boats but if i put a random picture like a dog or an house  it still validate it as a car or a boat     so i guess its just run an algorythm and chose the higher score of similarity  so i checked on youtube  stack and many others forums and website for an answer but i found nothing about  so my last hope is to ask the question in here  can someone can tell me what should i do to have the machine have a mismatch when the image analyzed is not one of the categories  i tried to implement a code that would be based on score to reject scores under x number but the thing is that some image that are out of the categories get an higher score than an image in the categories    at this point im pretty lost  thank you,ml net image classification categories problem,made machine learning project images classification auto trained actually work good problem im facing let say categories cars boats machine accurate come cars boats put random picture like dog house still validate car boat guess run algorythm chose higher score similarity checked youtube stack many others forums website answer found nothing last hope ask question someone tell machine mismatch image analyzed one categories tried implement code would based score reject scores x number thing image categories get higher score image categories point im pretty lost thank,ml net image classification categories problem,ml net image classification categories problemmade machine learning project images classification auto trained actually work good problem im facing let say categories cars boats machine accurate come cars boats put random picture like dog house still validate car boat guess run algorythm chose higher score similarity checked youtube stack many others forums website answer found nothing last hope ask question someone tell machine mismatch image analyzed one categories tried implement code would based score reject scores x number thing image categories get higher score image categories point im pretty lost thank,"['ml', 'net', 'image', 'classification', 'categories', 'problemmade', 'machine', 'learning', 'project', 'images', 'classification', 'auto', 'trained', 'actually', 'work', 'good', 'problem', 'im', 'facing', 'let', 'say', 'categories', 'cars', 'boats', 'machine', 'accurate', 'come', 'cars', 'boats', 'put', 'random', 'picture', 'like', 'dog', 'house', 'still', 'validate', 'car', 'boat', 'guess', 'run', 'algorythm', 'chose', 'higher', 'score', 'similarity', 'checked', 'youtube', 'stack', 'many', 'others', 'forums', 'website', 'answer', 'found', 'nothing', 'last', 'hope', 'ask', 'question', 'someone', 'tell', 'machine', 'mismatch', 'image', 'analyzed', 'one', 'categories', 'tried', 'implement', 'code', 'would', 'based', 'score', 'reject', 'scores', 'x', 'number', 'thing', 'image', 'categories', 'get', 'higher', 'score', 'image', 'categories', 'point', 'im', 'pretty', 'lost', 'thank']","['ml', 'net', 'imag', 'classif', 'categori', 'problemmad', 'machin', 'learn', 'project', 'imag', 'classif', 'auto', 'train', 'actual', 'work', 'good', 'problem', 'im', 'face', 'let', 'say', 'categori', 'car', 'boat', 'machin', 'accur', 'come', 'car', 'boat', 'put', 'random', 'pictur', 'like', 'dog', 'hous', 'still', 'valid', 'car', 'boat', 'guess', 'run', 'algorythm', 'chose', 'higher', 'score', 'similar', 'check', 'youtub', 'stack', 'mani', 'other', 'forum', 'websit', 'answer', 'found', 'noth', 'last', 'hope', 'ask', 'question', 'someon', 'tell', 'machin', 'mismatch', 'imag', 'analyz', 'one', 'categori', 'tri', 'implement', 'code', 'would', 'base', 'score', 'reject', 'score', 'x', 'number', 'thing', 'imag', 'categori', 'get', 'higher', 'score', 'imag', 'categori', 'point', 'im', 'pretti', 'lost', 'thank']"
34,35,35,17979746,72442820,How to make a recomandation machine learning with ML.NET with a tag property,"<p>For my project I would like to integrate a recomendation system. my dataset looks like this.</p>
<pre><code>userId, projectId, projectCategory
1,1, API
1,5, Database
2,6, Arduino
</code></pre>
<p>Each user joins a project with a specific tag. I would like to recommend projects to my users based on the projects they join. Could I do machine learning based on the tags.</p>
",29,1,0,5,c#;machine-learning;artificial-intelligence;recommendation-engine;ml.net,2022-05-31 08:53:36,2022-05-31 08:53:36,2022-07-29 00:33:32,for my project i would like to integrate a recomendation system  my dataset looks like this  each user joins a project with a specific tag  i would like to recommend projects to my users based on the projects they join  could i do machine learning based on the tags ,how to make a recomandation machine learning with ml net with a tag property,project would like integrate recomendation system dataset looks like user joins project specific tag would like recommend projects users based projects join could machine learning based tags,make recomandation machine learning ml net tag property,make recomandation machine learning ml net tag propertyproject would like integrate recomendation system dataset looks like user joins project specific tag would like recommend projects users based projects join could machine learning based tags,"['make', 'recomandation', 'machine', 'learning', 'ml', 'net', 'tag', 'propertyproject', 'would', 'like', 'integrate', 'recomendation', 'system', 'dataset', 'looks', 'like', 'user', 'joins', 'project', 'specific', 'tag', 'would', 'like', 'recommend', 'projects', 'users', 'based', 'projects', 'join', 'could', 'machine', 'learning', 'based', 'tags']","['make', 'recomand', 'machin', 'learn', 'ml', 'net', 'tag', 'propertyproject', 'would', 'like', 'integr', 'recomend', 'system', 'dataset', 'look', 'like', 'user', 'join', 'project', 'specif', 'tag', 'would', 'like', 'recommend', 'project', 'user', 'base', 'project', 'join', 'could', 'machin', 'learn', 'base', 'tag']"
35,37,37,15915389,73150567,Python Jupiter and google collaboratory typing issues,"<p>I am new in Machine Learning, and I am taking courses on line. I always make the practices in Anaconda/Jupiter. But the tutor uses google Collaboratory and I have seen that every time he types a directory (for instances sklearn) as he typed the different possibilities of sklearn appeared in screen, to be honest I found that very helpful for beginners in order to memorizes the structure of directories, however, I dont know how to activate it in Jupiter, if someone can help me Id really appreciate</p>
<p><a href=""https://i.stack.imgur.com/uPgKx.png"" rel=""nofollow noreferrer"">this is an example</a></p>
",25,0,-1,1,python,2022-07-28 12:40:14,2022-07-28 12:40:14,2022-07-28 19:29:24,i am new in machine learning  and i am taking courses on line  i always make the practices in anaconda jupiter  but the tutor uses google collaboratory and i have seen that every time he types a directory  for instances sklearn  as he typed the different possibilities of sklearn appeared in screen  to be honest i found that very helpful for beginners in order to memorizes the structure of directories  however  i don t know how to activate it in jupiter  if someone can help me i d really appreciate ,python jupiter and google collaboratory typing issues,machine learning taking courses line always make practices anaconda jupiter tutor uses google collaboratory seen every time types directory instances sklearn typed different possibilities sklearn appeared screen honest found helpful beginners order memorizes structure directories however know activate jupiter someone help really appreciate,python jupiter google collaboratory typing issues,python jupiter google collaboratory typing issuesmachine learning taking courses line always make practices anaconda jupiter tutor uses google collaboratory seen every time types directory instances sklearn typed different possibilities sklearn appeared screen honest found helpful beginners order memorizes structure directories however know activate jupiter someone help really appreciate,"['python', 'jupiter', 'google', 'collaboratory', 'typing', 'issuesmachine', 'learning', 'taking', 'courses', 'line', 'always', 'make', 'practices', 'anaconda', 'jupiter', 'tutor', 'uses', 'google', 'collaboratory', 'seen', 'every', 'time', 'types', 'directory', 'instances', 'sklearn', 'typed', 'different', 'possibilities', 'sklearn', 'appeared', 'screen', 'honest', 'found', 'helpful', 'beginners', 'order', 'memorizes', 'structure', 'directories', 'however', 'know', 'activate', 'jupiter', 'someone', 'help', 'really', 'appreciate']","['python', 'jupit', 'googl', 'collaboratori', 'type', 'issuesmachin', 'learn', 'take', 'cours', 'line', 'alway', 'make', 'practic', 'anaconda', 'jupit', 'tutor', 'use', 'googl', 'collaboratori', 'seen', 'everi', 'time', 'type', 'directori', 'instanc', 'sklearn', 'type', 'differ', 'possibl', 'sklearn', 'appear', 'screen', 'honest', 'found', 'help', 'beginn', 'order', 'memor', 'structur', 'directori', 'howev', 'know', 'activ', 'jupit', 'someon', 'help', 'realli', 'appreci']"
36,39,39,19635128,73143024,&#39;Method not found: &#39;Void Module.train()&#39;.&#39;,"<p>I am currently walking through the text classification instructions using Bidirectional Encoder Representations from Transformers:</p>
<p><a href=""https://github.com/dotnet/csharp-notebooks/blob/main/machine-learning/E2E-Text-Classification-API-with-Yelp-Dataset.ipynb"" rel=""nofollow noreferrer"">https://github.com/dotnet/csharp-notebooks/blob/main/machine-learning/E2E-Text-Classification-API-with-Yelp-Dataset.ipynb</a></p>
<p>I'm just playing around with this library and wanted to train it to determine if based on a note string whether or not it would be rejected or approved. Currently I have a small console app that follows the instructions from the above link but I keep getting an error after calling Fit on the training set</p>
<p>This line:
<code>(_trainedModel = trainingPipeline.Fit(trainTestSplit.TrainSet);).</code></p>
<p>I know this is a very new feature, but I was wondering if anyone else has ran into this issue</p>
<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Microsoft.ML;
using Microsoft.ML.Data;
using Microsoft.ML.TorchSharp;
using TorchSharp;

namespace ConsoleApp5 {
    internal class TextNLPClassifier {
        MLContext _mlContext;
        PredictionEngine&lt;ExpenseNote, IssuePrediction&gt; _predEngine;
        ITransformer _trainedModel;
        IDataView _trainingDataView;
        public void FitData() {
            // Initialize MLContext
            _mlContext = new MLContext(seed: 0);

            // Load your data
            //var reviews = RawData.GetData();

            var reviews = new List&lt;ExpenseNote&gt;
            {
                new ExpenseNote() { Note = &quot;test&quot;, Sentiment = &quot;Approved&quot;}
            };

            var reviewsDV = _mlContext.Data.LoadFromEnumerable&lt;ExpenseNote&gt;(reviews);



            //Define your training pipeline
            var pipeline = ProcessData();

            BuildAndTrainModel(reviewsDV, pipeline);

            foreach(var rev in reviews) {
                var review = rev;
                review.Sentiment = &quot;&quot;;
                var prediction = _predEngine.Predict(review);
                Console.WriteLine(prediction.Sentiment);
            }

            // Train the model
            var model = pipeline.Fit(reviewsDV);
        }

        IEstimator&lt;ITransformer&gt; ProcessData() {
            var pipeline = _mlContext.Transforms.Conversion.MapValueToKey(inputColumnName: &quot;Sentiment&quot;, outputColumnName: &quot;Label&quot;)
                .Append(_mlContext.Transforms.Text.FeaturizeText(inputColumnName: &quot;Note&quot;, outputColumnName: &quot;NoteFeaturized&quot;))
                .Append(_mlContext.Transforms.Concatenate(&quot;Features&quot;, &quot;NoteFeaturized&quot;))
                .AppendCacheCheckpoint(_mlContext);
            return pipeline;
        }
        void BuildAndTrainModel(IDataView trainingDataView, IEstimator&lt;ITransformer&gt; pipeline) {
            
            var trainTestSplit = _mlContext.Data.TrainTestSplit(trainingDataView, testFraction:0.2);
            
            var trainingPipeline = pipeline.Append(_mlContext.MulticlassClassification.Trainers.TextClassification(numberOfClasses: 2, sentence1ColumnName: &quot;Note&quot;))
                .Append(_mlContext.Transforms.Conversion.MapKeyToValue(&quot;PredictedLabel&quot;));
            _trainedModel = trainingPipeline.Fit(trainTestSplit.TrainSet);
            _predEngine = _mlContext.Model.CreatePredictionEngine&lt;ExpenseNote, IssuePrediction&gt;(_trainedModel);
        }
    }
    public class ExpenseNote {
        [LoadColumn(0)]
        public string Note { get; set; }
        [LoadColumn(1)]
        public string Sentiment { get; set; }
    }

    public class IssuePrediction {
        [ColumnName(&quot;PredictedLabel&quot;)]
        public string Sentiment;
    }
}
</code></pre>
",24,1,0,3,c#;machine-learning;ml.net,2022-07-27 21:34:17,2022-07-27 21:34:17,2022-07-28 17:20:22,i am currently walking through the text classification instructions using bidirectional encoder representations from transformers   i m just playing around with this library and wanted to train it to determine if based on a note string whether or not it would be rejected or approved  currently i have a small console app that follows the instructions from the above link but i keep getting an error after calling fit on the training set i know this is a very new feature  but i was wondering if anyone else has ran into this issue,   method not found     void module train         ,currently walking text classification instructions using bidirectional encoder representations transformers playing around library wanted train determine based note string whether would rejected approved currently small console app follows instructions link keep getting error calling fit training set know feature wondering anyone else ran issue,method found void module train,method found void module traincurrently walking text classification instructions using bidirectional encoder representations transformers playing around library wanted train determine based note string whether would rejected approved currently small console app follows instructions link keep getting error calling fit training set know feature wondering anyone else ran issue,"['method', 'found', 'void', 'module', 'traincurrently', 'walking', 'text', 'classification', 'instructions', 'using', 'bidirectional', 'encoder', 'representations', 'transformers', 'playing', 'around', 'library', 'wanted', 'train', 'determine', 'based', 'note', 'string', 'whether', 'would', 'rejected', 'approved', 'currently', 'small', 'console', 'app', 'follows', 'instructions', 'link', 'keep', 'getting', 'error', 'calling', 'fit', 'training', 'set', 'know', 'feature', 'wondering', 'anyone', 'else', 'ran', 'issue']","['method', 'found', 'void', 'modul', 'traincurr', 'walk', 'text', 'classif', 'instruct', 'use', 'bidirect', 'encod', 'represent', 'transform', 'play', 'around', 'librari', 'want', 'train', 'determin', 'base', 'note', 'string', 'whether', 'would', 'reject', 'approv', 'current', 'small', 'consol', 'app', 'follow', 'instruct', 'link', 'keep', 'get', 'error', 'call', 'fit', 'train', 'set', 'know', 'featur', 'wonder', 'anyon', 'els', 'ran', 'issu']"
37,40,40,11387441,58410187,How to plot predicted values vs the true value,"<p>I'm new to visualization using matplotlib. I will like to make a plot of my machine learning model's predicted value vs the actual value.</p>
<p>I made a prediction using random forest algorithm and will like to visualize the plot of true values and predicted values.</p>
<p>I used the below code, but the plot isn't showing clearly the relationship between the predicted and actual values.</p>
<pre><code> plt.scatter(y_test1, y_pred_test_Forestreg)
 plt.xlabel('True Values ')
 plt.ylabel('Predictions ')
 plt.axis('equal')
 plt.axis('square')
 plt.xlim([0, plt.xlim()])
 plt.ylim([0, plt.ylim()])
 _ = plt.plot([-100, 100], [-100, 100])
</code></pre>
<p>This is what I get:</p>
<p><img src=""https://i.stack.imgur.com/xHKSa.png"" alt=""plot of true vs predicted values"" /></p>
<p>This is what I expect the plot to look like:</p>
<p><img src=""https://i.stack.imgur.com/9iBcL.jpg"" alt=""Expected Plot of true value vs predicted value with 5% and 10% scatter band "" /></p>
<p>Below is the table of data i want to plot.(Note: This is just part of the result as it is impossible to include the full data (shape 8221, 1) here. i look forward to your help.</p>
<pre><code>True_value  Predicted_value
19.624  15.144
4.685   4.815
2.924   3.038
3.113   3.784
10.512  10.400
9.176   9.066
6.375   5.983
4.412   4.232
8.273   7.917
3.166   3.251
68.971  107.703
181.666 237.296
7.701   8.048
2.447   6.054
131.302 207.189
13.768  13.457
11.623  13.137
8.528   8.807
15.098  17.706
56.473  54.183
59.310  167.495
3.348   3.328
32.844  34.156
578.226 505.921
1.448   1.446
10.062  9.766
7.570   7.265
8.616   8.672
3.674   3.644
3.288   2.931
3.540   3.562
4.560   5.061
5.887   5.541
1.665   1.688
1.871   1.904
1.410   1.439
9.912   403.442
2.935   2.997
12.787  12.957
3.457   3.596
11.299  11.967
8.130   8.460
8.865   11.949
7.540   7.515
60.140  84.870
16.552  17.161
10.865  11.791
6.067   6.578
11.295  16.454
75.891  185.727
10.326  11.284
34.206  107.315
22.264  22.015
3.950   4.260
28.428  27.939
12.290  12.022
5.473   9.635
6.745   7.254
634.100 673.322
15.266  16.482
15.521  43.444
18.474  17.949
3.755   3.572
46.217  69.086
16.910  14.501
2.680   2.753
274.212 316.699
8.235   8.440
6.427   6.307
6.089   5.979
28.649  29.809
4.168   4.382
2.547   2.708
4.315   4.311
7.585   7.409
6.233   6.248
34.533  31.312
10.258  10.079
8.695   9.437
4.033   12.747
4.125   4.098
525.944 219.438
2.579   2.611
523.896 282.774
8.701   8.535
7.240   7.155
176.189 200.219
3.428   3.463
2.585   2.813
27.354  34.487
3.001   3.338
38.852  82.933
12.774  16.158
16.984  17.053
11.137  11.219
4.082   4.084
3.328   3.262
274.311 209.084
6.897   7.223
62.672  114.585
43.145  72.709
2.984   3.033
9.826   10.398
3.516   3.742
7.338   7.184
7.378   7.162
9.957   9.932
6.911   22.346
7.950   10.278
5.116   4.820
7.892   8.124
14.289  18.204
6.993   6.660
19.128  20.634
3.115   3.211
3.542   4.578
21.191  115.314
8.054   8.121
4.050   3.860
9.886   11.048
4.155   6.156
11.709  11.088
8.132   8.471
3.890   3.949
4.378   4.437
6.988   10.504
14.657  25.161
24.183  34.785
10.967  798.643
3.996   4.247
3.198   3.327
156.253 351.941
5.146   5.262
11.318  11.291
16.291  16.949
158.091 203.395
22.975  21.835
5.912   6.467
12.273  12.490
3.539   3.542
16.078  16.097
51.275  80.729
3.488   4.741
2.925   3.088
3.778   3.881
6.571   6.429
2.901   2.811
1.601   2.957
3.696   3.577
31.660  32.617
7.704   8.171
20.296  22.126
4.045   4.334
134.317 190.880
2.555   2.852
5.464   7.617
12.790  12.009
4.284   4.556
6.270   6.779
1.671   1.670
226.813 91.195
72.333  71.087
3.791   3.813
7.525   8.456
2.172   2.399
2.959   2.909
30.524  69.432
2.827   2.830
3.085   3.134
2.872   2.932
3.742   3.929
3.649   3.566
5.980   15.945
2.526   2.584
1.368   1.437
3.601   3.655
10.210  9.142
12.890  13.373
10.297  18.741
4.448   4.461
2.445   2.441
7821.052    209.961
5.288   5.424
6.344   7.370
4.965   4.934
2.613   2.645
3.185   3.252
124.729 167.799
2.405   2.563
30.347  116.190
3.292   3.305
665.125 973.483
4.164   4.251
6.322   6.311
11.213  407.169
95.240  343.970
5.783   5.994
5.373   5.870
19.684  21.314
17.965  25.495
30.212  30.220
61.062  63.275
3.549   3.490
5.964   5.915
21.034  25.812
47.966  47.694
14.440  13.870
5.449   6.194
5.259   5.309
6.598   6.491
104.152 133.026
10.739  10.297
11.346  11.375
17.738  16.433
5.873   5.891
3.844   3.874
3.964   3.939
21.967  22.237
2.246   2.198
17.728  14.837
3.784   3.879
22.299  26.496
644.469 675.529
17.276  18.275
5.231   5.172
18.520  32.575
7.318   7.207
3.755   3.952
283.689 237.367
35.856  38.537
573.022 587.498
28.416  66.907
7.308   7.136
4.968   5.126
10.330  11.180
1.610   1.738
16.856  17.902
42.227  43.223
2.051   2.041
7.593   9.966
7.569   8.319
5.441   5.746
5.972   6.653
3.717   3.599
8.827   9.307
7.706   8.501
3.623   3.295
9.030   13.022
6.355   6.297
7.335   9.017
13.303  12.695
4.113   4.121
3.723   3.737
111.807 898.529
2.851   2.891
11.799  13.946
2394.623    1824.993
4.082   4.092
3.056   3.046
2.624   2.730
3.470   3.984
4.257   5.701
2.896   3.084
2.443   2.478
2.870   2.964
3.321   2.960
2.828   2.932
3.141   3.127
12.469  12.952
16.836  16.809
9.443   9.719
2.554   2.638
5.645   5.492
5.714   5.886
4.343   4.475
14.376  15.382
10.272  27.267
5.985   5.618
4.007   3.866
4.131   4.157
2.406   2.544
6.889   7.021
7.578   7.749
3.672   3.600
906.078 216.823
2.902   3.025
9.181   9.414
8.592   8.066
6.513   18.057
271.303 228.073
5.702   5.848
5.085   5.392
2.616   2.593
3.754   3.874
30.282  35.751
21.143  25.404
14.135  15.484
36.088  40.671
3.123   3.576
4.275   4.755
6.851   6.882
2.818   2.761
2.159   2.164
9.910   9.536
3.049   3.067
4.427   5.804
6.712   6.458
4.494   4.221
3.068   3.197
5.406   5.613
3.227   3.241
24.215  53.796
637.213 286.607
5.956   6.193
1.471   1.628
55.357  40.091
210.939 179.626
10.495  30.618
4.570   4.749
543.716 600.721
149.483 303.777
6.426   8.019
3.584   4.201
5.645   5.716
345.349 248.498
6.279   5.735
3.202   3.244
203.829 195.321
10.781  12.432
4.101   3.965
8.068   8.434
2.857   3.038
3.087   3.080
12.415  12.642
4.565   4.695
549.052 613.451
18.186  23.562
16.835  18.274
4.791   6.422
71.954  70.883
4.768   4.833
3.521   3.604
19.906  17.715
16.679  39.652
130.312 104.834
6.184   6.200
140.157 143.435
3.544   3.559
57.671  98.001
17.373  20.190
7.149   7.182
11.680  11.834
21.702  21.113
22.296  21.578
13.011  13.667
10.163  10.251
4.846   4.961
3.140   3.136
13.378  23.330
2.997   3.053
5.985   5.649
13.253  14.494
11.334  13.650
28.669  28.714
10.286  10.428
9.503   9.448
4.742   4.682
2.221   2.284
3.861   3.902
240.606 291.496
15.891  18.820
8.417   9.890
5.489   5.405
6.948   6.772
5.827   5.797
2.000   2.097
5.365   5.523
21.660  42.945
14.776  14.856
11.559  11.872
113.205 68.657
27.932  58.427
3461.739    1284.346
4.265   4.264
4.679   4.776
4.158   4.167
5.433   5.745
4.630   4.672
3.234   3.273
2.979   3.008
2.973   3.000
65.804  192.535
9.779   9.668
4.859   5.321
25.096  25.863
31.760  32.688
45.694  88.227
9.456   9.014
3.848   3.757
3.219   3.663
3.437   3.555
3.145   3.880
4.071   4.734
9.924   10.470
1.803   2.191
8.169   9.736
2.865   2.903
9.904   56.968
4.630   5.931
9.509   12.341
3.601   3.610
20.892  29.847
12.044  12.784
4.555   4.787
5.870   11.672
6.595   7.227
6.838   6.873
4.685   4.716
5.192   6.754
9.431   16.747
2.668   2.737
13.617  14.081
2.232   2.274
7.903   8.343
2.499   2.615
34.243  48.755
4.698   4.900
3.748   3.432
37.223  66.586
68.727  361.602
25.718  36.754
18.440  18.247
15.377  15.465
3.886   3.931
2.643   2.600
9.831   9.503
39.471  40.691
3.029   3.156
7.123   6.307
9.489   9.209
3.149   3.287
7.776   7.646
3.390   3.544
10.181  14.724
8.250   8.084
193.590 261.347
9.793   12.250
70.579  69.578
7.832   7.399
5.046   5.176
3.968   4.005
9.784   12.865
7.610   7.236
4996.689    2691.915
313.615 422.989
6.895   7.304
3.470   3.484
11.665  18.933
3.292   3.317
1.783   1.947
3.219   3.111
3.985   3.964
3.498   3.610
36.447  36.004
8.682   9.461
5.307   5.283
70.309  68.247
3.070   3.118
24.358  22.845
11.658  16.996
4.120   4.151
4.298   4.632
14.703  27.946
3.584   3.608
821.402 464.270
5.953   6.212
128.394 98.013
19.772  20.482
52.685  56.871
15.331  47.899
3.063   3.138
27.708  29.416
5.710   5.702
5.179   5.176
6.794   7.548
5.535   5.903
7.756   7.542
13.773  15.158
42.209  47.055
9.589   9.636
4.101   4.053
11.070  10.378
9.900   10.381
23.599  27.321
6.342   7.113
237.329 265.999
4.236   4.156
3.725   3.765
3.288   3.761
12.502  13.748
22.315  23.830
460.784 499.877
37.721  59.371
3.329   3.455
2.656   2.734
7.192   13.859
3.141   3.169
16.235  17.393
9.122   11.052
4.592   5.448
4.822   4.917
3.775   3.841
23.833  30.813
3.330   3.408
32.084  43.318
2.922   2.642
9.614   9.788
19.096  19.256
3.442   3.273
4.007   4.938
30.032  30.929
4.988   5.175
3.160   3.197
3.550   3.606
10.242  10.115
3.102   3.137
5.496   5.485
78.592  170.062
20.358  21.758
3.878   4.560
7.540   7.334
3.525   3.586
41.475  42.571
2.526   2.551
284.630 211.248
2.610   2.621
15.534  17.391
20.425  33.944
4.757   4.765
3.913   4.076
3.830   3.574
10.342  9.655
10.169  10.913
30.062  50.935
3.767   3.821
10.695  13.182
3.992   3.987
12.472  12.897
7.534   7.612
5.622   5.747
3.971   3.960
3.435   3.686
1326.840    1219.852
46107.740   316.479
3.811   3.797
2.531   2.616
6.154   5.978
45.078  70.688
36.858  35.887
13.847  14.226
21.346  32.181
16.678  18.144
15.503  15.724
2.691   2.736
27.847  36.464
6.376   6.316
14.914  15.570
9.088   11.115
12.111  13.716
55.573  47.872
16.263  17.161
3.524   3.513
7.709   8.567
5.546   5.526
2.949   2.814
5.711   5.824
1.900   1.992
4.627   4.638
7.726   8.888
1.879   2.139
8.284   8.346
45.501  46.389
9.511   9.486
6.682   7.590
7.960   16.404
2.684   2.647
4.696   4.752
5.750   5.675
15.713  15.559
3.617   3.625
44.469  45.952
20.249  20.487
5.670   6.105
107.327 262.087
8.889   8.471
13.256  13.335
126.793 136.720
137.222 168.966
3.026   3.041
8.653   9.073
3.465   4.198
25.399  44.397
16.268  68.009
7.730   7.676
26.813  63.690
5.427   6.090
3.672   3.716
26.927  32.404
2.879   2.922
488.947 187.509
13.759  17.262
17.620  18.346
3.768   4.381
2.410   2.652
38.413  83.543
3.581   3.688
9.117   8.473
49.507  44.383
12.744  9.823
23.463  15.088
152.177 156.684
35.534  74.871
15.581  12.622
3.262   3.295
3.054   3.089
9.100   11.311
9.668   10.491
2.909   2.924
3.783   3.696
10.671  13.134
5.098   5.271
14.355  131.551
4.601   4.558
73.732  522.207
15.599  16.085
99.343  171.043
9.426   10.030
16.628  18.044
11.698  11.487
3.561   3.583
5.189   5.167
4.687   4.769
12.656  12.308
3.325   3.444
3.948   4.025
60.056  152.943
14.180  16.198
9.861   9.616
63.960  69.110
4.679   4.675
16.040  16.687
7.904   7.643
6.450   6.727
3.803   4.413
2.553   2.739
40.290  97.088
2.708   2.835
425.787 314.400
2.439   2.477
2.785   2.805
3.270   3.284
2.647   2.710
5.165   5.211
48.268  40.837
3.257   3.247
214.791 332.489
5.842   6.338
17.314  17.595
7.217   7.600
11.369  10.983
4.525   12.805
9.691   35.084
7.733   8.054
47.099  44.539
4.428   4.658
3.050   3.160
21.687  21.427
3.499   3.571
4.851   4.774
2.977   3.060
2.545   2.566
3.662   4.037
22.456  22.634
2.181   2.239
326.994 374.272
55.825  55.422
2.393   2.478
4.400   6.259
3.782   3.799
2.809   2.804
9.876   13.799
2.576   2.653
16.874  16.959
21.571  23.953
15.590  17.355
42.106  51.814
10.481  10.497
2.916   2.968
3.334   3.302
2.954   3.059
1.696   1.735
5.395   6.021
5.418   5.255
42.656  49.237
5.596   5.675
3.480   3.554
17.537  21.359
3.228   3.383
58.281  179.127
25.906  63.865
21.146  25.153
4.658   4.720
3.850   3.888
9.028   15.569
4.629   4.711
3.091   3.171
24.311  41.592
2.652   2.698
14.238  14.362
12.500  12.204
3.574   3.627
321.192 6054.332
4.070   4.263
13.435  13.500
2.249   2.341
10.612  10.822
3.224   3.409
27.689  27.566
3.954   4.244
20.670  22.052
6.427   6.765
3.392   3.515
2.920   3.359
14.821  15.202
2.611   2.794
6.555   7.040
9.217   12.450
5.632   5.729
6.226   5.949
4.872   6.035
3.619   4.020
8.413   9.601
1.448   1.504
7.171   7.861
3.952   3.864
3.377   3.390
11.497  12.984
8.768   7.989
11.831  12.099
3.136   3.121
9.831   12.960
9.540   9.640
10.653  11.002
4.646   5.055
18.888  14.569
3.136   3.150
185.894 281.490
30.000  33.611
3.099   3.383
14528.128   194.832
3.533   3.551
60.248  72.399
16.598  15.403
5.506   6.254
2.885   2.785
10.409  10.430
6.957   6.359
10.874  17.594
5.967   6.343
105.277 135.997
173.652 857.814
2.381   3.225
9.035   9.054
2.968   3.385
10.200  10.618
5.132   5.480
462.597 203.613
3.955   4.076
18.293  26.279
3.258   3.353
3.629   3.519
3.624   3.667
4.140   17.326
3.448   3.726
176.988 72.779
21.992  33.420
1.912   1.915
20.365  21.570
2.801   3.024
7.667   9.698
73.205  68.196
11.238  11.440
12.600  12.502
2.826   2.911
13.567  13.484
5.286   5.429
2.749   2.858
7.208   7.190
8.269   8.003
162.883 215.015
4.572   4.541
59.605  95.131
143.216 199.214
11.269  12.128
11.469  14.168
34.084  31.335
6.867   15.177
4.481   4.457
7.499   6.741
4.513   4.767
3.141   3.254
3.221   3.214
2.948   2.875
5.513   5.298
7.164   8.900
13.643  13.920
13.516  15.751
228.455 264.090
18.596  25.985
2.572   2.641
3.588   3.526
184.955 296.952
5.161   5.870
5.834   8.090
3.114   3.125
4.721   4.766
7.596   7.547
17.221  15.741
6.401   6.706
5.301   5.285
5.072   5.416
3.559   7.562
4.951   5.511
13.149  45.857
17.839  20.007
25.825  27.040
2.947   3.143
2.954   2.977
19.163  36.026
6.853   46.787
1234.533    895.424
9.103   9.127
6.063   5.949
4.596   4.656
20.167  36.586
132.208 129.966
64.140  93.127
12.166  11.759
4.699   5.181
4.833   5.464
7.117   36.724
42.634  65.560
4.988   5.685
3.252   3.175
14.238  15.520
5.948   6.027
3.099   3.123
4.190   4.883
40.309  42.843
3.063   3.196
5.789   5.911
2.668   2.714
27.305  24.457
13.130  14.262
5.462   5.335
230.848 297.006
2.131   2.182
2.918   2.999
4.971   5.090
3.121   3.378
2.103   2.115
17.212  16.520
2.063   2.076
17.047  17.497
29.930  48.084
2.474   2.593
19.437  15.786
4.036   4.011
6.311   7.566
32.844  39.152
4.086   4.163
4.841   5.930
216.971 90.661
3.811   4.976
2.958   3.018
10.057  10.921
3.111   3.126
2.402   2.468
103.789 160.448
38.330  41.226
12.148  13.005
3.876   3.643
4.960   4.957
19.842  19.848
16.860  18.693
19.083  25.635
16.207  20.152
10.292  11.449
18.104  19.176
3.244   3.268
6.349   6.967
9.476   9.581
24.041  23.769
3.753   4.275
10.291  13.313
7.082   7.471
9.135   9.262
88.004  113.825
5.438   5.238
427.816 326.175
39.240  72.889
2.434   2.467
2.626   2.742
4.965   5.306
23.282  20.708
2.487   2.595
122.099 118.899
3.201   3.152
8.655   8.895
9.244   9.042
3.264   3.455
21.233  31.791
7.346   9.535
10.145  12.891
3.188   3.207
81.958  75.353
14.312  14.969
111.029 144.639
9.118   10.859
275.693 149.173
4.416   4.747
3.075   3.085
4.944   4.785
3.749   3.844
10.440  15.537
35.442  34.194
1903.978    246.478
7.105   7.157
28.782  42.077
141.881 265.094
4.897   9.252
29.811  39.802
2.399   2.546
15.536  15.934
2.323   2.485
15.379  20.478
8.901   10.844
2.494   2.526
2.943   3.579
3.808   3.828
5.006   5.371
46.338  58.896
6.285   6.131
7.067   7.692
10.146  9.935
18.963  18.006
3.821   3.849
3.374   3.089
4.176   4.267
1.867   1.962
3.029   2.933
10.424  11.745
7.899   14.366
41.736  43.484
203.775 242.494
20.162  38.360
6.337   6.425
4.034   6.067
4.241   4.346
8.871   9.049
2.915   2.928
3.382   3.415
1.808   1.915
2.835   2.913
7.117   7.156
2.290   2.399
8.650   9.025
3.798   3.821
3.474   3.482
2.639   2.792
3.687   3.756
13.404  13.450
6.119   6.688
12.387  16.997
45.936  55.680
11.247  11.161
4.274   4.423
7.325   10.756
29.293  27.371
9.515   19.688
7.857   7.680
5.348   22.322
163.558 178.067
24.362  20.704
20.334  19.389
3.535   3.546
7.405   7.502
30.687  28.936
12.820  13.067
16.036  15.349
4.525   4.644
7.361   7.496
10.054  11.879
7.697   9.671
11.423  11.470
2.973   3.038
1314.315    323.847
112.133 160.072
16.433  23.824
4.906   5.328
7.876   8.760
10.229  9.743
2.814   2.821
257.298 249.414
2.467   2.913
5.176   5.347
5.191   9.566
6.346   6.879
9.219   8.968
8.048   8.219
3.832   3.834
4.459   4.636
25.413  39.491
4.700   4.472
347.022 287.293
1.345   1.381
2.813   2.908
9.625   9.323
3.809   3.995
7.431   22.802
3.661   3.820
5.383   9.702
3.712   3.785
4.763   4.771
8.235   8.958
19.655  23.900
15.520  13.607
7.013   6.968
14.973  15.679
2.384   2.420
4.971   5.077
6.074   6.479
10.907  14.398
10.633  10.592
100.205 272.179
5.507   8.602
3.933   5.477
6.311   6.562
3.729   4.175
19.241  19.845
4.872   4.800
9.470   9.167
13.976  18.381
2.110   2.134
4.407   6.087
12.468  34.135
45.424  50.214
2.512   5.133
22.283  23.099
6.261   6.630
15.590  21.447
23.178  35.645
39.043  36.060
2.670   2.843
19.230  30.284
3.077   3.088
3.273   3.360
3.264   3.304
44.335  210.250
82.392  74.348
3.973   4.747
30.960  70.890
6.265   6.221
7.608   8.167
5.943   797.595
6.186   9.305
10.559  10.650
10.691  11.225
7.879   7.851
21.246  25.182
3.607   3.576
6.703   7.297
106.397 110.987
7.925   15.494
19.990  29.775
7.284   8.833
156.078 174.563
38.052  39.191
5.875   6.148
94.980  570.359
2.569   2.566
2.688   2.770
3.080   3.076
34.402  35.595
3.145   3.269
303.919 241.618
2.988   3.362
2.344   2.479
4.419   4.500
16.500  16.542
3.214   3.219
6.524   6.263
15.548  14.508
49.636  112.217
81.555  95.624
38.713  39.742
35.177  35.511
6.376   6.757
12.303  13.147
15.831  15.487
8.664   8.499
13.038  14.052
76.699  79.075
6.567   6.763
30.068  30.138
4.166   4.190
11.244  11.023
10.033  15.945
8.026   8.410
20.400  24.974
25.895  56.055
5.347   5.551
2.639   2.639
4.799   4.557
10.292  11.111
466.511 201.463
5.570   6.146
3.581   3.887
114.262 240.503
2.394   2.408
14.285  14.559
5.548   6.802
94.413  54.871
5.914   5.657
2.996   2.985
12.743  17.174
64.850  343.782
6.416   6.853
30.839  30.897
6.602   6.345
183.528 206.723
9.141   10.174
3.501   3.512
27.424  87.668
4.738   4.886
2.816   2.760
17.365  30.646
4.007   4.085
7.485   8.774
7.654   7.444
11.835  14.526
294.052 270.140
3.662   3.713
115.129 208.145
4.381   4.253
3.638   4.308
2.752   3.336
3.500   4.949
3.442   3.406
5.175   5.302
5.695   6.043
3.417   3.384
5.643   6.373
7.287   6.973
4.445   5.089
225.768 189.505
3.695   3.759
2.665   2.820
16.550  16.458
17.384  16.734
26.914  31.025
3.397   3.361
3.006   3.054
2.089   2.122
34.676  35.022
10.833  11.133
1049.306    350.535
15.384  28.722
19.489  18.079
775.681 731.995
4.548   5.418
6.270   6.606
68.405  66.981
3.851   4.227
21.010  75.327
26.540  30.676
13.190  13.393
29.683  31.399
86.971  227.074
7.432   7.444
12.055  15.133
99.511  74.751
7.418   8.342
28.807  24.266
52.762  52.212
3.951   4.839
4.244   4.105
3.908   3.852
3.580   3.579
28.467  68.300
11.045  11.432
2.776   2.826
4.181   3.967
7.051   7.164
4.962   4.696
5.242   5.742
2.662   2.931
2.666   2.678
10.889  10.831
2.493   2.534
15.825  18.569
4.334   4.414
16.147  35.420
270.914 298.895
18.300  17.052
5.218   5.480
2.892   2.928
5.884   5.699
4.923   5.001
4.180   4.316
14.932  14.942
41.254  75.577
2.507   2.601
3.261   3.285
3.323   6.875
3.284   3.267
27.438  32.004
19.371  20.212
3.170   3.193
5.018   5.555
42.568  36.890
25.968  30.364
9.335   9.489
272.611 255.764
13.364  13.961
5.729   5.642
12.335  19.017
38.416  207.078
3.702   3.696
48.208  76.352
6.136   7.892
3.452   3.803
3.975   3.951
17.466  19.923
11.703  11.391
82.279  120.894
3.020   3.018
45.694  67.196
3.047   3.248
5.188   5.270
32.589  46.707
3.283   3.296
3.532   3.867
24.104  52.124
11.111  42.011
2.617   2.647
9.136   9.944
3.258   3.267
9.458   24.309
8.300   8.317
16.536  34.283
17.828  18.889
5.224   5.479
20.401  1346.159
18.276  17.085
4.969   5.033
11.977  11.986
10.110  10.653
31.651  31.643
11.656  11.726
 
</code></pre>
",52926,4,9,2,python;matplotlib,2019-10-16 12:34:56,2019-10-16 12:34:56,2022-07-28 16:23:56,i m new to visualization using matplotlib  i will like to make a plot of my machine learning model s predicted value vs the actual value  i made a prediction using random forest algorithm and will like to visualize the plot of true values and predicted values  i used the below code  but the plot isn t showing clearly the relationship between the predicted and actual values  this is what i get   this is what i expect the plot to look like   below is the table of data i want to plot  note  this is just part of the result as it is impossible to include the full data  shape     here  i look forward to your help ,how to plot predicted values vs the true value,visualization using matplotlib like make plot machine learning model predicted value vs actual value made prediction using random forest algorithm like visualize plot true values predicted values used code plot showing clearly relationship predicted actual values get expect plot look like table data want plot note part result impossible include full data shape look forward help,plot predicted values vs true value,plot predicted values vs true valuevisualization using matplotlib like make plot machine learning model predicted value vs actual value made prediction using random forest algorithm like visualize plot true values predicted values used code plot showing clearly relationship predicted actual values get expect plot look like table data want plot note part result impossible include full data shape look forward help,"['plot', 'predicted', 'values', 'vs', 'true', 'valuevisualization', 'using', 'matplotlib', 'like', 'make', 'plot', 'machine', 'learning', 'model', 'predicted', 'value', 'vs', 'actual', 'value', 'made', 'prediction', 'using', 'random', 'forest', 'algorithm', 'like', 'visualize', 'plot', 'true', 'values', 'predicted', 'values', 'used', 'code', 'plot', 'showing', 'clearly', 'relationship', 'predicted', 'actual', 'values', 'get', 'expect', 'plot', 'look', 'like', 'table', 'data', 'want', 'plot', 'note', 'part', 'result', 'impossible', 'include', 'full', 'data', 'shape', 'look', 'forward', 'help']","['plot', 'predict', 'valu', 'vs', 'true', 'valuevisu', 'use', 'matplotlib', 'like', 'make', 'plot', 'machin', 'learn', 'model', 'predict', 'valu', 'vs', 'actual', 'valu', 'made', 'predict', 'use', 'random', 'forest', 'algorithm', 'like', 'visual', 'plot', 'true', 'valu', 'predict', 'valu', 'use', 'code', 'plot', 'show', 'clearli', 'relationship', 'predict', 'actual', 'valu', 'get', 'expect', 'plot', 'look', 'like', 'tabl', 'data', 'want', 'plot', 'note', 'part', 'result', 'imposs', 'includ', 'full', 'data', 'shape', 'look', 'forward', 'help']"
38,41,41,11616033,57203915,Grouping low frequency levels of categorical variables to improve machine learning performance,"<p>I'm trying to find ways to improve performance of machine learning models either binary classification, regression or multinomial classification.</p>

<p>I'm now looking at the topic categorical variables and trying to combine low occuring levels together. Let's say a categorical variable has 10 levels where 5 levels account for 85% of the total frequency count and the 5 levels remaining account for the 15% remaining.</p>

<p>I'm currently trying different thresholds (30%, 20%, 10%) to combine levels together. This means I combine together the levels which represent either 30%, 20% or 10% of the remaining counts.</p>

<p>I was wondering if grouping these ""low frequency groups"" into a new level called ""others"" would have any benefit in improving the performance.</p>

<p>I further use a random forest for feature selection and I know that having fewer levels than orignally may create a loss of information and therefore not improve my performance.</p>

<p>Also, I tried discretizing numeric variables but noticed that my performance was weaker because random forests benefit from having the hability to split on their preferred split point rather than being forced to split on an engineered split point that I would have created by discretizing.</p>

<p>In your experience, would grouping low occuring levels together have a positive impact on performance ? If yes, would you recommend any techniques ?</p>

<p>Thank you for your help !</p>
",434,2,1,2,python;machine-learning,2019-07-25 17:04:14,2019-07-25 17:04:14,2022-07-28 15:29:56,i m trying to find ways to improve performance of machine learning models either binary classification  regression or multinomial classification  i m now looking at the topic categorical variables and trying to combine low occuring levels together  let s say a categorical variable has  levels where  levels account for   of the total frequency count and the  levels remaining account for the   remaining  i m currently trying different thresholds           to combine levels together  this means i combine together the levels which represent either      or   of the remaining counts  i was wondering if grouping these low frequency groups into a new level called others would have any benefit in improving the performance  i further use a random forest for feature selection and i know that having fewer levels than orignally may create a loss of information and therefore not improve my performance  also  i tried discretizing numeric variables but noticed that my performance was weaker because random forests benefit from having the hability to split on their preferred split point rather than being forced to split on an engineered split point that i would have created by discretizing  in your experience  would grouping low occuring levels together have a positive impact on performance   if yes  would you recommend any techniques   thank you for your help  ,grouping low frequency levels of categorical variables to improve machine learning performance,trying find ways improve performance machine learning models either binary classification regression multinomial classification looking topic categorical variables trying combine low occuring levels together let say categorical variable levels levels account total frequency count levels remaining account remaining currently trying different thresholds combine levels together means combine together levels represent either remaining counts wondering grouping low frequency groups level called others would benefit improving performance use random forest feature selection know fewer levels orignally may create loss information therefore improve performance also tried discretizing numeric variables noticed performance weaker random forests benefit hability split preferred split point rather forced split engineered split point would created discretizing experience would grouping low occuring levels together positive impact performance yes would recommend techniques thank help,grouping low frequency levels categorical variables improve machine learning performance,grouping low frequency levels categorical variables improve machine learning performancetrying find ways improve performance machine learning models either binary classification regression multinomial classification looking topic categorical variables trying combine low occuring levels together let say categorical variable levels levels account total frequency count levels remaining account remaining currently trying different thresholds combine levels together means combine together levels represent either remaining counts wondering grouping low frequency groups level called others would benefit improving performance use random forest feature selection know fewer levels orignally may create loss information therefore improve performance also tried discretizing numeric variables noticed performance weaker random forests benefit hability split preferred split point rather forced split engineered split point would created discretizing experience would grouping low occuring levels together positive impact performance yes would recommend techniques thank help,"['grouping', 'low', 'frequency', 'levels', 'categorical', 'variables', 'improve', 'machine', 'learning', 'performancetrying', 'find', 'ways', 'improve', 'performance', 'machine', 'learning', 'models', 'either', 'binary', 'classification', 'regression', 'multinomial', 'classification', 'looking', 'topic', 'categorical', 'variables', 'trying', 'combine', 'low', 'occuring', 'levels', 'together', 'let', 'say', 'categorical', 'variable', 'levels', 'levels', 'account', 'total', 'frequency', 'count', 'levels', 'remaining', 'account', 'remaining', 'currently', 'trying', 'different', 'thresholds', 'combine', 'levels', 'together', 'means', 'combine', 'together', 'levels', 'represent', 'either', 'remaining', 'counts', 'wondering', 'grouping', 'low', 'frequency', 'groups', 'level', 'called', 'others', 'would', 'benefit', 'improving', 'performance', 'use', 'random', 'forest', 'feature', 'selection', 'know', 'fewer', 'levels', 'orignally', 'may', 'create', 'loss', 'information', 'therefore', 'improve', 'performance', 'also', 'tried', 'discretizing', 'numeric', 'variables', 'noticed', 'performance', 'weaker', 'random', 'forests', 'benefit', 'hability', 'split', 'preferred', 'split', 'point', 'rather', 'forced', 'split', 'engineered', 'split', 'point', 'would', 'created', 'discretizing', 'experience', 'would', 'grouping', 'low', 'occuring', 'levels', 'together', 'positive', 'impact', 'performance', 'yes', 'would', 'recommend', 'techniques', 'thank', 'help']","['group', 'low', 'frequenc', 'level', 'categor', 'variabl', 'improv', 'machin', 'learn', 'performancetri', 'find', 'way', 'improv', 'perform', 'machin', 'learn', 'model', 'either', 'binari', 'classif', 'regress', 'multinomi', 'classif', 'look', 'topic', 'categor', 'variabl', 'tri', 'combin', 'low', 'occur', 'level', 'togeth', 'let', 'say', 'categor', 'variabl', 'level', 'level', 'account', 'total', 'frequenc', 'count', 'level', 'remain', 'account', 'remain', 'current', 'tri', 'differ', 'threshold', 'combin', 'level', 'togeth', 'mean', 'combin', 'togeth', 'level', 'repres', 'either', 'remain', 'count', 'wonder', 'group', 'low', 'frequenc', 'group', 'level', 'call', 'other', 'would', 'benefit', 'improv', 'perform', 'use', 'random', 'forest', 'featur', 'select', 'know', 'fewer', 'level', 'orign', 'may', 'creat', 'loss', 'inform', 'therefor', 'improv', 'perform', 'also', 'tri', 'discret', 'numer', 'variabl', 'notic', 'perform', 'weaker', 'random', 'forest', 'benefit', 'habil', 'split', 'prefer', 'split', 'point', 'rather', 'forc', 'split', 'engin', 'split', 'point', 'would', 'creat', 'discret', 'experi', 'would', 'group', 'low', 'occur', 'level', 'togeth', 'posit', 'impact', 'perform', 'ye', 'would', 'recommend', 'techniqu', 'thank', 'help']"
39,42,42,19079915,73146779,ML Studio language studio failing to detect the source language,"<p>I am running a program in python to detect a language and translate that to English using azure machine learning studio. The code block mentioned below throwing error when trying to detect the language.</p>
<blockquote>
<p>Error 0002: Failed to parse parameter.</p>
</blockquote>
<pre><code>def sample_detect_language():
    print(
        &quot;This sample statement will be translated to english from any other foreign language&quot;
       
    )
    
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.textanalytics import TextAnalyticsClient

    endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]
    key = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]

    text_analytics_client = TextAnalyticsClient(endpoint=endpoint)
    documents = [
        &quot;&quot;&quot;
        The feedback was awesome
        &quot;&quot;&quot;,
        &quot;&quot;&quot;
        la recensione  stata fantastica
        &quot;&quot;&quot;
    ]

    result = text_analytics_client.detect_language(documents)
    reviewed_docs = [doc for doc in result if not doc.is_error]

    print(&quot;Check the languages we got review&quot;)

    for idx, doc in enumerate(reviewed_docs):
        print(&quot;Number#{} is in '{}', which has ISO639-1 name '{}'\n&quot;.format(
            idx, doc.primary_language.name, doc.primary_language.iso6391_name
        ))
        if doc.is_error:
            print(doc.id, doc.error)
    
    print(
        &quot;Storing reviews and mapping to their respective ISO639-1 name &quot;
        
    )

    review_to_language = {}
    for idx, doc in enumerate(reviewed_docs):
        review_to_language[documents[idx]] = doc.primary_language.iso6391_name


if __name__ == '__main__':
    sample_detect_language()
</code></pre>
<p>Any help to solve the issue is appreciated.</p>
",30,1,0,2,azure;azure-machine-learning-studio,2022-07-28 06:02:43,2022-07-28 06:02:43,2022-07-28 14:21:24,i am running a program in python to detect a language and translate that to english using azure machine learning studio  the code block mentioned below throwing error when trying to detect the language  error   failed to parse parameter  any help to solve the issue is appreciated ,ml studio language studio failing to detect the source language,running program python detect language translate english using azure machine learning studio code block mentioned throwing error trying detect language error failed parse parameter help solve issue appreciated,ml studio language studio failing detect source language,ml studio language studio failing detect source languagerunning program python detect language translate english using azure machine learning studio code block mentioned throwing error trying detect language error failed parse parameter help solve issue appreciated,"['ml', 'studio', 'language', 'studio', 'failing', 'detect', 'source', 'languagerunning', 'program', 'python', 'detect', 'language', 'translate', 'english', 'using', 'azure', 'machine', 'learning', 'studio', 'code', 'block', 'mentioned', 'throwing', 'error', 'trying', 'detect', 'language', 'error', 'failed', 'parse', 'parameter', 'help', 'solve', 'issue', 'appreciated']","['ml', 'studio', 'languag', 'studio', 'fail', 'detect', 'sourc', 'languagerun', 'program', 'python', 'detect', 'languag', 'translat', 'english', 'use', 'azur', 'machin', 'learn', 'studio', 'code', 'block', 'mention', 'throw', 'error', 'tri', 'detect', 'languag', 'error', 'fail', 'pars', 'paramet', 'help', 'solv', 'issu', 'appreci']"
40,43,43,14736700,73150809,How to refrence path from python script into yaml file,"<p>I have a folder called data which is created from the following code</p>
<pre><code>import shutil
import os

folderName = &quot;..Data&quot;
current_directory = os.getcwd()
final_directory = os.path.join(current_directory,folderName)
if os.path.exists(final_directory):
    shutil.rmtree(final_directory)
if not os.path.exists(final_directory):
    os.makedirs(final_directory)
</code></pre>
<p>I want to get the path of this data folder and refrence it in a YAML file. I dont want to manually state the path as it will be run on azure using azure pipelines. is there a way i can get this path from  a python script into a YAML file? for reference <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-data-assets?tabs=CLI"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-data-assets?tabs=CLI</a> trying to create a URI folder</p>
",14,1,0,4,python;python-3.x;yaml;azure-pipelines,2022-07-28 12:55:35,2022-07-28 12:55:35,2022-07-28 14:19:08,i have a folder called data which is created from the following code i want to get the path of this data folder and refrence it in a yaml file  i dont want to manually state the path as it will be run on azure using azure pipelines  is there a way i can get this path from  a python script into a yaml file  for reference  trying to create a uri folder,how to refrence path from python script into yaml file,folder called data created following code want get path data folder refrence yaml file dont want manually state path run azure using azure pipelines way get path python script yaml file reference trying create uri folder,refrence path python script yaml file,refrence path python script yaml filefolder called data created following code want get path data folder refrence yaml file dont want manually state path run azure using azure pipelines way get path python script yaml file reference trying create uri folder,"['refrence', 'path', 'python', 'script', 'yaml', 'filefolder', 'called', 'data', 'created', 'following', 'code', 'want', 'get', 'path', 'data', 'folder', 'refrence', 'yaml', 'file', 'dont', 'want', 'manually', 'state', 'path', 'run', 'azure', 'using', 'azure', 'pipelines', 'way', 'get', 'path', 'python', 'script', 'yaml', 'file', 'reference', 'trying', 'create', 'uri', 'folder']","['refrenc', 'path', 'python', 'script', 'yaml', 'filefold', 'call', 'data', 'creat', 'follow', 'code', 'want', 'get', 'path', 'data', 'folder', 'refrenc', 'yaml', 'file', 'dont', 'want', 'manual', 'state', 'path', 'run', 'azur', 'use', 'azur', 'pipelin', 'way', 'get', 'path', 'python', 'script', 'yaml', 'file', 'refer', 'tri', 'creat', 'uri', 'folder']"
41,45,45,12269058,73134983,machine learning correlation coefficientR2 for each independent variable?,"<p>How to do this kind of picture by machine learning? Each feature on the horizontal axis has R2 value, and the vertical axis is categorical variable (such as POD like, OXD like).</p>
<p>R2 is used to calculate the correlation between the predicted value and the real value. I don't understand why every feature has R2 value?</p>
<p><img src=""https://i.stack.imgur.com/DPxBu.png"" alt=""picture links"" /></p>
",39,0,0,4,python;r;machine-learning;bioinformatics,2022-07-27 11:55:43,2022-07-27 11:55:43,2022-07-28 07:25:00,how to do this kind of picture by machine learning  each feature on the horizontal axis has r value  and the vertical axis is categorical variable  such as pod like  oxd like   r is used to calculate the correlation between the predicted value and the real value  i don t understand why every feature has r value  ,machine learning correlation coefficient r  for each independent variable ,kind picture machine learning feature horizontal axis r value vertical axis categorical variable pod like oxd like r used calculate correlation predicted value real value understand every feature r value,machine learning correlation coefficient r independent variable,machine learning correlation coefficient r independent variablekind picture machine learning feature horizontal axis r value vertical axis categorical variable pod like oxd like r used calculate correlation predicted value real value understand every feature r value,"['machine', 'learning', 'correlation', 'coefficient', 'r', 'independent', 'variablekind', 'picture', 'machine', 'learning', 'feature', 'horizontal', 'axis', 'r', 'value', 'vertical', 'axis', 'categorical', 'variable', 'pod', 'like', 'oxd', 'like', 'r', 'used', 'calculate', 'correlation', 'predicted', 'value', 'real', 'value', 'understand', 'every', 'feature', 'r', 'value']","['machin', 'learn', 'correl', 'coeffici', 'r', 'independ', 'variablekind', 'pictur', 'machin', 'learn', 'featur', 'horizont', 'axi', 'r', 'valu', 'vertic', 'axi', 'categor', 'variabl', 'pod', 'like', 'oxd', 'like', 'r', 'use', 'calcul', 'correl', 'predict', 'valu', 'real', 'valu', 'understand', 'everi', 'featur', 'r', 'valu']"
42,48,48,1977614,73144333,Matrix inversion speed in Numpy,"<p>I have been reading the book &quot;Hands-on Machine Learning with Scikit-Learn and TensorFlow&quot;. In Chapter 4 (page 110), it says: <em>The computational complexity of inverting such a matrix is typically about O(n^2.4) to O(n^3) (depending on the implementation). In
other words, if you double the number of features, you multiply the computation time by roughly 2^2.4 = 5.3 to 2^3 = 8.</em></p>
<p>I tested the above in Python:</p>
<pre><code>import numpy as np
import timeit
nsize = 100
A = np.random.rand(nsize, nsize)
x = np.random.rand(nsize, 1)
b = A.dot(x)
# np.linalg.inv(A) or np.linalg.inv(A).dot(b) - not a big difference in time
timeit.timeit('np.linalg.inv(A)','import numpy as np\nfrom __main__ import A, b', number=10000)
</code></pre>
<p>I get about 2.5 seconds for <code>nsize=100</code> and get about 8.5 seconds for <code>nsize=200</code>. The compute cost seems to increase a factor of 3.4 (8.5/2.5) which is well below the range mentioned by the author. Can someone explain what causes this &quot;speedup&quot; ?</p>
<p>P.S: I tested the code with <code>nsize</code> of 5000 and 10000. The compute times were 3.2 and 25 seconds respectively - implying a 7.8 fold increase in time compared to 3.4 for the smaller <code>nsize</code> in the original question. Is this due to cache effects?</p>
",48,1,1,2,python;numpy,2022-07-27 23:35:15,2022-07-27 23:35:15,2022-07-28 06:23:28,i tested the above in python  i get about   seconds for nsize  and get about   seconds for nsize   the compute cost seems to increase a factor of         which is well below the range mentioned by the author  can someone explain what causes this  speedup    p s  i tested the code with nsize of  and   the compute times were   and  seconds respectively   implying a   fold increase in time compared to   for the smaller nsize in the original question  is this due to cache effects ,matrix inversion speed in numpy,tested python get seconds nsize get seconds nsize compute cost seems increase factor well range mentioned author someone explain causes speedup p tested code nsize compute times seconds respectively implying fold increase time compared smaller nsize original question due cache effects,matrix inversion speed numpy,matrix inversion speed numpytested python get seconds nsize get seconds nsize compute cost seems increase factor well range mentioned author someone explain causes speedup p tested code nsize compute times seconds respectively implying fold increase time compared smaller nsize original question due cache effects,"['matrix', 'inversion', 'speed', 'numpytested', 'python', 'get', 'seconds', 'nsize', 'get', 'seconds', 'nsize', 'compute', 'cost', 'seems', 'increase', 'factor', 'well', 'range', 'mentioned', 'author', 'someone', 'explain', 'causes', 'speedup', 'p', 'tested', 'code', 'nsize', 'compute', 'times', 'seconds', 'respectively', 'implying', 'fold', 'increase', 'time', 'compared', 'smaller', 'nsize', 'original', 'question', 'due', 'cache', 'effects']","['matrix', 'invers', 'speed', 'numpytest', 'python', 'get', 'second', 'nsize', 'get', 'second', 'nsize', 'comput', 'cost', 'seem', 'increas', 'factor', 'well', 'rang', 'mention', 'author', 'someon', 'explain', 'caus', 'speedup', 'p', 'test', 'code', 'nsize', 'comput', 'time', 'second', 'respect', 'impli', 'fold', 'increas', 'time', 'compar', 'smaller', 'nsize', 'origin', 'question', 'due', 'cach', 'effect']"
43,49,49,4594525,73146233,Preparing dataset over clients in federated learning,"<p>I am working on google cluster trace. I am working on small resource usage data only for 100 physical machine. Sample of data for two machine:</p>
<pre><code>Sample_time      machine_id     CPU_Usage  Memory_Usage
</code></pre>
<p>My goal is predicting the resource usage, so I used classical ANN. Now In my model I don't need the machine to send the usage information to cloud server to train and predict. Instead, I need to use federated learning for more privacy.</p>
<p>Since most of FL open source is based on simulation. Most of tutorial using MNIST dataset, and they do couple TFF steps to prepare the data over clients. For example, In TFF Load data method  use is an instance of TFF simulation client data interface, that allow to
enumerate a set of clients to construct a TFF dataset that could represent of like a single user's data</p>
<p>If use google trace dataset, Should I follow the same. in other words, does FL (such as TFF) assume that data is trained locally on each machine, and the cloud server will aggregate the parameters, etc. Or there  is something I need to code from scratch while preparing the dataset into all clients.</p>
",14,0,0,3,python;tensorflow;tensorflow-federated,2022-07-28 04:23:34,2022-07-28 04:23:34,2022-07-28 04:23:34,i am working on google cluster trace  i am working on small resource usage data only for  physical machine  sample of data for two machine  my goal is predicting the resource usage  so i used classical ann  now in my model i don t need the machine to send the usage information to cloud server to train and predict  instead  i need to use federated learning for more privacy  if use google trace dataset  should i follow the same  in other words  does fl  such as tff  assume that data is trained locally on each machine  and the cloud server will aggregate the parameters  etc  or there  is something i need to code from scratch while preparing the dataset into all clients ,preparing dataset over clients in federated learning,working google cluster trace working small resource usage data physical machine sample data two machine goal predicting resource usage used classical ann model need machine send usage information cloud server train predict instead need use federated learning privacy use google trace dataset follow fl tff assume data trained locally machine cloud server aggregate parameters etc something need code scratch preparing dataset clients,preparing dataset clients federated learning,preparing dataset clients federated learningworking google cluster trace working small resource usage data physical machine sample data two machine goal predicting resource usage used classical ann model need machine send usage information cloud server train predict instead need use federated learning privacy use google trace dataset follow fl tff assume data trained locally machine cloud server aggregate parameters etc something need code scratch preparing dataset clients,"['preparing', 'dataset', 'clients', 'federated', 'learningworking', 'google', 'cluster', 'trace', 'working', 'small', 'resource', 'usage', 'data', 'physical', 'machine', 'sample', 'data', 'two', 'machine', 'goal', 'predicting', 'resource', 'usage', 'used', 'classical', 'ann', 'model', 'need', 'machine', 'send', 'usage', 'information', 'cloud', 'server', 'train', 'predict', 'instead', 'need', 'use', 'federated', 'learning', 'privacy', 'use', 'google', 'trace', 'dataset', 'follow', 'fl', 'tff', 'assume', 'data', 'trained', 'locally', 'machine', 'cloud', 'server', 'aggregate', 'parameters', 'etc', 'something', 'need', 'code', 'scratch', 'preparing', 'dataset', 'clients']","['prepar', 'dataset', 'client', 'feder', 'learningwork', 'googl', 'cluster', 'trace', 'work', 'small', 'resourc', 'usag', 'data', 'physic', 'machin', 'sampl', 'data', 'two', 'machin', 'goal', 'predict', 'resourc', 'usag', 'use', 'classic', 'ann', 'model', 'need', 'machin', 'send', 'usag', 'inform', 'cloud', 'server', 'train', 'predict', 'instead', 'need', 'use', 'feder', 'learn', 'privaci', 'use', 'googl', 'trace', 'dataset', 'follow', 'fl', 'tff', 'assum', 'data', 'train', 'local', 'machin', 'cloud', 'server', 'aggreg', 'paramet', 'etc', 'someth', 'need', 'code', 'scratch', 'prepar', 'dataset', 'client']"
44,50,50,17573228,72930868,errors in the tutorial (Interpreting Machine Learning Models with the iml Package),"<p>I am getting the following error when trying to execute the following code in section entitled &quot;Replication requirements&quot; (<a href=""https://uc-r.github.io/iml-pkg"" rel=""nofollow noreferrer"">https://uc-r.github.io/iml-pkg</a>):</p>
<pre><code>#classification data
df &lt;- rsample::attrition %&gt;%
mutate_if(is.ordered, factor, ordered = FALSE) %&gt;%
mutate(Attrition = recode(Attrition, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;0&quot;) %&gt;% factor(levels = c(&quot;1&quot;, &quot;0&quot;)))

&gt; Error: 'attrition' is not an exported object from 'namespace:rsample'
</code></pre>
<p>The problem was solved using the following code:</p>
<pre><code>#data
library(modeldata)
data(&quot;attrition&quot;, package = &quot;modeldata&quot;)
#classification data
df &lt;- attrition %&gt;%
mutate_if(is.ordered, factor, ordered = FALSE) %&gt;%
mutate(Attrition = recode(Attrition, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;0&quot;) %&gt;% factor(levels = c(&quot;1&quot;, &quot;0&quot;)))
</code></pre>
<p>Unfortunately, I got another error after trying to execute the following code (section entitled &quot;Global interpretation/Feature importance&quot; (<a href=""https://uc-r.github.io/iml-pkg"" rel=""nofollow noreferrer"">https://uc-r.github.io/iml-pkg</a>):</p>
<pre><code>#compute feature importance with specified loss metric
imp.glm &lt;- FeatureImp$new(predictor.glm, loss = &quot;mse&quot;)
imp.rf &lt;- FeatureImp$new(predictor.rf, loss = &quot;mse&quot;)
imp.gbm &lt;- FeatureImp$new(predictor.gbm, loss = &quot;mse&quot;)

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected
</code></pre>
<p>I use R 4.2.0/ Win10</p>
",124,2,3,2,r;iml,2022-07-10 21:21:37,2022-07-10 21:21:37,2022-07-28 02:33:39,i am getting the following error when trying to execute the following code in section entitled  replication requirements      the problem was solved using the following code  unfortunately  i got another error after trying to execute the following code  section entitled  global interpretation feature importance      i use r     win,errors in the tutorial  interpreting machine learning models with the iml package ,getting following error trying execute following code section entitled replication requirements problem solved using following code unfortunately got another error trying execute following code section entitled global interpretation feature importance use r win,errors tutorial interpreting machine learning models iml package,errors tutorial interpreting machine learning models iml packagegetting following error trying execute following code section entitled replication requirements problem solved using following code unfortunately got another error trying execute following code section entitled global interpretation feature importance use r win,"['errors', 'tutorial', 'interpreting', 'machine', 'learning', 'models', 'iml', 'packagegetting', 'following', 'error', 'trying', 'execute', 'following', 'code', 'section', 'entitled', 'replication', 'requirements', 'problem', 'solved', 'using', 'following', 'code', 'unfortunately', 'got', 'another', 'error', 'trying', 'execute', 'following', 'code', 'section', 'entitled', 'global', 'interpretation', 'feature', 'importance', 'use', 'r', 'win']","['error', 'tutori', 'interpret', 'machin', 'learn', 'model', 'iml', 'packageget', 'follow', 'error', 'tri', 'execut', 'follow', 'code', 'section', 'entitl', 'replic', 'requir', 'problem', 'solv', 'use', 'follow', 'code', 'unfortun', 'got', 'anoth', 'error', 'tri', 'execut', 'follow', 'code', 'section', 'entitl', 'global', 'interpret', 'featur', 'import', 'use', 'r', 'win']"
45,51,51,1937197,73144210,How to multiply Int8 matrices (fast) on a Mac Studio?,"<p><code>Int8</code> matrix multiplication is important in machine learning (so NVidia GPUs and some CPUs even include special hardware to do it these days). If I want to do <code>Int8 x Int8 -&gt; Int32</code> matrix multiplications on a Mac Studio or similar, how should I do it?</p>
<ul>
<li><p>Would using <code>MPSMatrixMultiplication</code> (from Metal Performance Shaders) be possible and get reasonable performance?</p>
</li>
<li><p>If not, could I use <code>CoreML</code> to multiply arbitrary <code>Int8</code> matrices both of whose values are unknown at compile time?</p>
</li>
</ul>
<hr />
<p><em>Effort to answer this, on my part:</em></p>
<p>I found this (old) <a href=""https://developer.apple.com/forums/thread/105534"" rel=""nofollow noreferrer"">script</a> that does <code>Float32 x Float32 -&gt; Float32</code> matrix multiplication on the GPU, and tried to adapt it to use <code>Int8</code> and <code>Int32</code> instead:</p>

<pre class=""lang-swift prettyprint-override""><code>import Foundation
import MetalPerformanceShaders

func gflops(time: Double, size: Int) -&gt; Double {
    return 2.0 * pow(Double(size), 3) / time / 1E9
}

// Prepare some data
let N = 4096
let rowsA = N
let columnsA = N

let a = UnsafeMutablePointer&lt;Int8&gt;.allocate(capacity: rowsA * columnsA)
let arrayA = UnsafeMutableBufferPointer(start: a, count: rowsA * columnsA)
arrayA.assign(repeating: Int8(1))
print(&quot;Values in input array: \(arrayA[0])&quot;)
print()

// Get the device
let device = MTLCreateSystemDefaultDevice()!
let commandQueue = device.makeCommandQueue()!
let commandBuffer = commandQueue.makeCommandBuffer()!
let blitEncoder = commandBuffer.makeBlitCommandEncoder()!

// 1. Prepare managed buffers
let rowBytesA = columnsA * MemoryLayout&lt;Int8&gt;.stride
let bufferA = device.makeBuffer(bytes: arrayA.baseAddress!, length: rowsA * rowBytesA, options: [.storageModeManaged])!
let rowBytesC = columnsA * MemoryLayout&lt;Int32&gt;.stride
let bufferC = device.makeBuffer(length: columnsA * rowBytesC, options: [.storageModeManaged])!

// 2. Encode matrix multiplication
let descrA = MPSMatrixDescriptor(rows: rowsA, columns: columnsA, rowBytes: rowBytesA, dataType: .int8)
let descrC = MPSMatrixDescriptor(rows: columnsA, columns: columnsA, rowBytes: rowBytesA, dataType: .int32)

let matrixA = MPSMatrix(buffer: bufferA, descriptor: descrA)
let matrixC = MPSMatrix(buffer: bufferC, descriptor: descrC)
let matMul = MPSMatrixMultiplication(device: device, resultRows: columnsA, resultColumns: columnsA, interiorColumns: rowsA)

let startTime = CFAbsoluteTimeGetCurrent()
matMul.encode(commandBuffer: commandBuffer, leftMatrix: matrixA, rightMatrix: matrixA, resultMatrix: matrixC)

// 3. Get data back from GPU
blitEncoder.synchronize(resource: bufferC)

// 4. Run buffer
commandBuffer.commit()
commandBuffer.waitUntilCompleted()

let elapsed = CFAbsoluteTimeGetCurrent() - startTime
let gf = gflops(time: elapsed / 1.0, size: N)
print(&quot;Run at \(Int(gf)) GFlops total&quot;)

// Read results
let resultPointer = bufferC.contents().bindMemory(to: Int32.self, capacity: columnsA * columnsA)
let result = UnsafeBufferPointer(start: resultPointer, count: columnsA * columnsA)
print(&quot;Resulting values: [\(result[0])...\(result[columnsA * columnsA - 1])]&quot;)
</code></pre>
<p>But I don't have a Mac Studio, so I cannot test it (I'm considering getting one if it does these matrix multiplication well)</p>
<p>Assuming this script works,</p>
<ul>
<li>Mac Studio has unified RAM, while the script was written for a discrete GPU. Are there any adjustments that would be beneficial here? Does the unified RAM allow the CPU to access the results directly instead of copying data back and forth? How?</li>
<li>Does the performance indicate that it's actually utilizing any specialized hardware? (Is it faster than <code>Float16 x Float16 -&gt; Float16</code> and much faster than <code>Float32 x Float32 -&gt; Float32</code>?)</li>
</ul>
",59,0,0,3,matrix-multiplication;metal;coreml,2022-07-27 23:23:57,2022-07-27 23:23:57,2022-07-28 02:14:33,int matrix multiplication is important in machine learning  so nvidia gpus and some cpus even include special hardware to do it these days   if i want to do int x int   gt  int matrix multiplications on a mac studio or similar  how should i do it  would using mpsmatrixmultiplication  from metal performance shaders  be possible and get reasonable performance  if not  could i use coreml to multiply arbitrary int matrices both of whose values are unknown at compile time  effort to answer this  on my part  i found this  old   that does float x float   gt  float matrix multiplication on the gpu  and tried to adapt it to use int and int instead  but i don t have a mac studio  so i cannot test it  i m considering getting one if it does these matrix multiplication well  assuming this script works ,how to multiply int matrices  fast  on a mac studio ,int matrix multiplication important machine learning nvidia gpus cpus even include special hardware days want int x int gt int matrix multiplications mac studio similar would using mpsmatrixmultiplication metal performance shaders possible get reasonable performance could use coreml multiply arbitrary int matrices whose values unknown compile time effort answer part found old float x float gt float matrix multiplication gpu tried adapt use int int instead mac studio cannot test considering getting one matrix multiplication well assuming script works,multiply int matrices fast mac studio,multiply int matrices fast mac studioint matrix multiplication important machine learning nvidia gpus cpus even include special hardware days want int x int gt int matrix multiplications mac studio similar would using mpsmatrixmultiplication metal performance shaders possible get reasonable performance could use coreml multiply arbitrary int matrices whose values unknown compile time effort answer part found old float x float gt float matrix multiplication gpu tried adapt use int int instead mac studio cannot test considering getting one matrix multiplication well assuming script works,"['multiply', 'int', 'matrices', 'fast', 'mac', 'studioint', 'matrix', 'multiplication', 'important', 'machine', 'learning', 'nvidia', 'gpus', 'cpus', 'even', 'include', 'special', 'hardware', 'days', 'want', 'int', 'x', 'int', 'gt', 'int', 'matrix', 'multiplications', 'mac', 'studio', 'similar', 'would', 'using', 'mpsmatrixmultiplication', 'metal', 'performance', 'shaders', 'possible', 'get', 'reasonable', 'performance', 'could', 'use', 'coreml', 'multiply', 'arbitrary', 'int', 'matrices', 'whose', 'values', 'unknown', 'compile', 'time', 'effort', 'answer', 'part', 'found', 'old', 'float', 'x', 'float', 'gt', 'float', 'matrix', 'multiplication', 'gpu', 'tried', 'adapt', 'use', 'int', 'int', 'instead', 'mac', 'studio', 'can', 'not', 'test', 'considering', 'getting', 'one', 'matrix', 'multiplication', 'well', 'assuming', 'script', 'works']","['multipli', 'int', 'matric', 'fast', 'mac', 'studioint', 'matrix', 'multipl', 'import', 'machin', 'learn', 'nvidia', 'gpu', 'cpu', 'even', 'includ', 'special', 'hardwar', 'day', 'want', 'int', 'x', 'int', 'gt', 'int', 'matrix', 'multipl', 'mac', 'studio', 'similar', 'would', 'use', 'mpsmatrixmultipl', 'metal', 'perform', 'shader', 'possibl', 'get', 'reason', 'perform', 'could', 'use', 'coreml', 'multipli', 'arbitrari', 'int', 'matric', 'whose', 'valu', 'unknown', 'compil', 'time', 'effort', 'answer', 'part', 'found', 'old', 'float', 'x', 'float', 'gt', 'float', 'matrix', 'multipl', 'gpu', 'tri', 'adapt', 'use', 'int', 'int', 'instead', 'mac', 'studio', 'can', 'not', 'test', 'consid', 'get', 'one', 'matrix', 'multipl', 'well', 'assum', 'script', 'work']"
46,52,52,16673416,73142393,I am builiding my resume on Overleaf.com platform. I am encountering a problem that there is a &quot;4&quot; at the end of the line. I am unable to remove it,"<p>The below provided is the complete latex code.</p>
<pre><code>%-------------------------
% Resume in Latex
% Author : Jake Gutierrez
% Based off of: https://github.com/sb2nov/resume
% License : MIT
%------------------------

\documentclass[letterpaper,11pt]{article}

\usepackage{latexsym}
\usepackage[empty]{fullpage}
\usepackage{titlesec}
\usepackage{marvosym}
\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{fontawesome5}
\usepackage{multicol}
\setlength{\multicolsep}{-3.0pt}
\setlength{\columnsep}{-1pt}
\input{glyphtounicode}


%----------FONT OPTIONS----------
% sans-serif
% \usepackage[sfdefault]{FiraSans}
% \usepackage[sfdefault]{roboto}
% \usepackage[sfdefault]{noto-sans}
% \usepackage[default]{sourcesanspro}

% serif
% \usepackage{CormorantGaramond}
% \usepackage{charter}


\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Adjust margins
\addtolength{\oddsidemargin}{-0.6in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\textwidth}{1.19in}
\addtolength{\topmargin}{-.7in}
\addtolength{\textheight}{1.4in}

\urlstyle{same}

\raggedbottom
\raggedright
\setlength{\tabcolsep}{0in}

% Sections formatting
\titleformat{\section}{
\vspace{-4pt}\scshape\raggedright\large\bfseries
}{}{0em}{}[\color{black}\titlerule \vspace{-5pt}]

% Ensure that generate pdf is machine readable/ATS parsable
\pdfgentounicode=1

%-------------------------
% Custom commands
\newcommand{\resumeItem}[1]{
\item\small{
    {#1 \vspace{-2pt}}
}
}

\newcommand{\classesList}[4]{
    \item\small{
        {#1 #2 #3 #4 \vspace{-2pt}}
}
}

\newcommand{\resumeSubheading}[4]{
\vspace{-2pt}\item
    \begin{tabular*}{1.0\textwidth}[t]{l@{\extracolsep{\fill}}r}
    \textbf{#1} &amp; \textbf{\small #2} \\
    \textit{\small#3} &amp; \textit{\small #4} \\
    \end{tabular*}\vspace{-7pt}
}

\newcommand{\resumeSubSubheading}[2]{
    \item
    \begin{tabular*}{0.97\textwidth}{l@{\extracolsep{\fill}}r}
    \textit{\small#1} &amp; \textit{\small #2} \\
    \end{tabular*}\vspace{-7pt}
}

\newcommand{\resumeProjectHeading}[2]{
    \item
    \begin{tabular*}{1.001\textwidth}{l@{\extracolsep{\fill}}r}
    \small#1 &amp; \textbf{\small #4}\\
    \end{tabular*}\vspace{-7pt}
}

\newcommand{\resumeSubItem}[1]{\resumeItem{#1}\vspace{-4pt}}

\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}
\renewcommand\labelitemii{$\vcenter{\hbox{\tiny$\bullet$}}$}

\newcommand{\resumeSubHeadingListStart}{\begin{itemize}[leftmargin=0.0in, label={}]}
\newcommand{\resumeSubHeadingListEnd}{\end{itemize}}
\newcommand{\resumeItemListStart}{\begin{itemize}}
\newcommand{\resumeItemListEnd}{\end{itemize}\vspace{-5pt}}

%-------------------------------------------
%%%%%%  RESUME STARTS HERE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%----------HEADING----------
% \begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}r}
%   \textbf{\href{http://sourabhbajaj.com/}{\Large Sourabh Bajaj}} &amp; Email : \href{mailto:sourabh@sourabhbajaj.com}{sourabh@sourabhbajaj.com}\\
%   \href{http://sourabhbajaj.com/}{http://www.sourabhbajaj.com} &amp; Mobile : +1-123-456-7890 \\
% \end{tabular*}

\begin{center}
    {\Huge \scshape Harsh Jain} \\ \vspace{1pt}
    Alwar, Rajasthan \\ \vspace{1pt}
    \small \raisebox{-0.1\height}\faPhone\ +91 8769826027 ~ \href{hrsjain.hp@gmail.com}{\raisebox{-0.2\height}\faEnvelope\  \underline{hrsjain.hp@gmail.com}} ~
    \href{https://linkedin.com/in//}{\raisebox{-0.2\height}\faLinkedin\ \underline{www.linkedin.com/in/hrsjain}}
    \vspace{-8pt}
\end{center}


%-----------EDUCATION-----------
\section{Education}
\resumeSubHeadingListStart
    \resumeSubheading
    {MBM University, Jodhpur (Pursuing)}{August 2019 - June 2023}
    {Bachelor of Technology in Information Technology (CGPA of 8.50 aggregate; 9.44 latest)}{Rajasthan, India}

    \resumeSubheading
    {Chinar Public School}{April 2017 - March 2018}
    {Class XII (83.8\%)}{Rajasthan, India}

    \resumeSubheading
    {Chinar Public School}{April 2015 - March 2016}
    {Class X (10 CGPA) equivalent to 95\% }{Rajasthan, India}
\resumeSubHeadingListEnd



%-----------EXPERIENCE-----------
\section{Experience}
\resumeSubHeadingListStart
    \resumeSubheading
    {CodePlanet Technologies}{June 2022 - present}
    {Java Intern}{Rajasthan, India}
    \resumeItemListStart
        \resumeItem{Creating a backend + frontend internal tool to  do online shopping with ease.}
        \resumeItem{Tool and Technoligies used will be JAVA, spring-MVC, JSP and servlet }
    \resumeItemListEnd
    
\resumeSubHeadingListEnd
\vspace{-16pt}

%-----------PROJECTS-----------
\section{Projects}
    \vspace{-5pt}
    \resumeSubHeadingListStart
    \resumeProjectHeading
        {\textbf{Flight Reservation Application} $|$ \emph{Java Spring Boot, MySQL} $|$ {https://github.com/hrsjain/Flight-ReservationApplication}}
        \resumeItemListStart
            \resumeItem{    An user interactive web application that allows users to book their flights and also to allow Airline Companies to add their flights.}
            \vspace{2pt}
            \resumeItem{    Created the email utility and PDF utility so that the Flight tickets and details can be sent automatically as the flight is booked. Password encoding system while logging in to enhance the security of the system.Exposed API for the Flight Check-in System to access the data.}
        \resumeItemListEnd
        \vspace{-13pt}

    \resumeProjectHeading
        {\textbf{Flight Check-in Application} $|$ \emph{Java Spring Boot, MySQL}  $|$ {https://github.com/hrsjain/FlightCheckinSystem}}
        \\
        \resumeItemListStart
            \resumeItem{  A user interactive web application that facilitates check-in of passengers of the flight.}
            \resumeItem{    Integrated with Flight Reservation Application, the application accesses data from Flight Reservation Application.}
        \resumeItemListEnd
    \resumeSubHeadingListEnd



%
%-----------Achievements-----------
\section{Achievements}
\begin{itemize}[leftmargin=0.15in, label={}]
    \resumeItemListStart
        \resumeItem{ 150+ Programming questions solved with 2 badges on Data Structures and Algorithms on LeetCode. leetcode.com/hrsjain/}
        \resumeItem{Distinction in Science in International Assessment for Indian Schools, by UNSW Global, Australia which is one of the top institutes which assess students across the globe Student ID-8504, Class XII, Year 2018.}
        \resumeItem{Distinction in Maths in International Assessment for Indian Schools, by UNSW Global, Australia which is one of the top institutes which assess students across the globe Student ID-8504, Class XII, Year 2018.}
        \resumeItem{AIMUN Certification for Model United Nations Conference held in Amity, Noida with participants from all across the world demonstrating critical thinking, speaking skills, etc.}
        \resumeItem{ Management Core Team Member at Google Developer Students Club - MBM, through which we help bridge the gap between theory and practice by empowering a peer-to-peer learning environment.}
    \resumeItemListEnd
\end{itemize}
\vspace{-20pt}

%
%-----------SKILLS-----------
\section{Technical Skills}
\begin{itemize}[leftmargin=0.15in, label={}]
    \small{\item{
    \textbf{Languages}{: C, C++, Java} \\
    \textbf{Technologies/Frameworks/Libraries}{: MySQL, Spring, AWS} \\
    \textbf{Core}{: Data Structure and Algorithms, Operating System, Computer Networks, Database Management Systems, etc. }
    }}
\end{itemize}

%-----------Certifications-----------
\section{Certifications}
\begin{itemize}[leftmargin=0.15in, label={}]
    \resumeItemListStart
        \resumeItem{Data Structure and Algorithms-certified by Dmatics Technologies}
        \resumeItem{ C, C++, Java Programming Language- Certified by Dmatics Technologies}
        \resumeItem{AWS CLOUD TECH. ESSENTIALS -AWS through Coursera -BDED2WG6RM9H}
        \resumeItem{Building Modern Java Applications on AWS -AWS through Coursera -LYGNJ45TAZP6}
        \resumeItem{ RS-CIT certificate in Information Technology (by VMOU, Rajasthan)}
        
    \resumeItemListEnd
\end{itemize}
\vspace{-16pt}

%

\vspace{-16pt}

\end{document}
</code></pre>
<blockquote>
<p>I have attached both code snippet and the output. I was unable to attach the complete code because of the formatting issue.
Please share your suggestions so as to remove the 4 at the end of the each project description line.</p>
</blockquote>
<p>I have attached both code snippet and the output. I was unable to attach the complete code because of the formatting issue.
Please share your suggestions so as to remove the 4 at the end of the each project description line.</p>
",18,1,-1,2,latex;overleaf,2022-07-27 20:38:40,2022-07-27 20:38:40,2022-07-27 22:07:11,the below provided is the complete latex code ,i am builiding my resume on overleaf com platform  i am encountering a problem that there is a    at the end of the line  i am unable to remove it,provided complete latex code,builiding resume overleaf com platform encountering problem end line unable remove,builiding resume overleaf com platform encountering problem end line unable removeprovided complete latex code,"['builiding', 'resume', 'overleaf', 'com', 'platform', 'encountering', 'problem', 'end', 'line', 'unable', 'removeprovided', 'complete', 'latex', 'code']","['builid', 'resum', 'overleaf', 'com', 'platform', 'encount', 'problem', 'end', 'line', 'unabl', 'removeprovid', 'complet', 'latex', 'code']"
47,53,53,15022285,73143027,"Is there a quick way to know what the books(book name:Understanding Machine Learning,from theory to algorithm)math,symbol,and their relations mean?","<p>these are some example:</p>
<p>1.H, |HC|  |{B  C : H shatters B}|.
i don't know the the meaning of :. exactly,i don't know what the formula mean.</p>
<p>2.
<a href=""https://i.stack.imgur.com/T323e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T323e.png"" alt=""enter image description here"" /></a>
I know the middle part formula,i don't understand,the last.</p>
<p>is there any book could help to understand these math.exactly say,i don't know. the :.</p>
",16,0,-1,2,machine-learning;math,2022-07-27 21:34:48,2022-07-27 21:34:48,2022-07-27 21:34:48,these are some example  is there any book could help to understand these math exactly say i don t know  the   ,is there a quick way to know what the book s book name understanding machine learning from theory to algorithm math symbol and their relations mean ,example book could help understand math exactly say know,quick way know book book name understanding machine learning theory algorithm math symbol relations mean,quick way know book book name understanding machine learning theory algorithm math symbol relations meanexample book could help understand math exactly say know,"['quick', 'way', 'know', 'book', 'book', 'name', 'understanding', 'machine', 'learning', 'theory', 'algorithm', 'math', 'symbol', 'relations', 'meanexample', 'book', 'could', 'help', 'understand', 'math', 'exactly', 'say', 'know']","['quick', 'way', 'know', 'book', 'book', 'name', 'understand', 'machin', 'learn', 'theori', 'algorithm', 'math', 'symbol', 'relat', 'meanexampl', 'book', 'could', 'help', 'understand', 'math', 'exactli', 'say', 'know']"
48,54,54,12162608,67354106,Detecting Bull Flag pattern in stock market Data,"<p>I want to detect multiple type of flag patterns from the Stock market the first one that I want to identify is the <strong>Bull Flag Pattern</strong> i have tried some formula's but they all missed the point and gave me lot of stock name which did not have the pattern.
In the recent way I did</p>
<ol>
<li>find the continuous rise and then check that the following values are lying b/w the mean of the continuous rise.</li>
<li>I'm also wondering if I plot this data in graph using matplot or plotly and then apply machine learning to it will that be a solution or not.</li>
</ol>
<p>The code to get the data is as below</p>
<pre><code>from pprint import print
from nsepy import get_history
from datetime import date
from datetime import datetime, timedelta
import matplotlib
from nsetools import Nse
nse = Nse() 

old_date=date.today()-timedelta(days=30)
for stock in  nse.get_stock_codes():
    print(&quot;Stock&quot;,stock)
    stock_data = get_history(symbol=stock,
                start=date(old_date.year,old_date.month,old_date.day), 
                end=date(datetime.now().year,datetime.now().month,datetime.now().day)))
</code></pre>
<p>Any help will be useful. Thanks in advance</p>
",1121,2,4,3,python;machine-learning;stock,2021-05-02 10:52:13,2021-05-02 10:52:13,2022-07-27 20:20:53,the code to get the data is as below any help will be useful  thanks in advance,detecting bull flag pattern in stock market data,code get data help useful thanks advance,detecting bull flag pattern stock market data,detecting bull flag pattern stock market datacode get data help useful thanks advance,"['detecting', 'bull', 'flag', 'pattern', 'stock', 'market', 'datacode', 'get', 'data', 'help', 'useful', 'thanks', 'advance']","['detect', 'bull', 'flag', 'pattern', 'stock', 'market', 'datacod', 'get', 'data', 'help', 'use', 'thank', 'advanc']"
49,55,55,16572743,73142075,DialogFlow ES - how to get chatbot to perform a &quot;monologue&quot; type message,"<p>I'm new to Google's DialogFlow and am using it with Soul Machines to create a digital person. I've been learning it through brute force methods. I want this person to present some content, basically just walking through some information in a monologue style rather than a normal chatbot dialog. It would be as if I was standing in front of a camera talking and explaining some concept that I'd eventually put on a website. How do I do this?</p>
<p>I can't quite seem to figure out how to add a bunch of intents that would be triggered by the previous one like how a human would be triggered by new cue cards. I don't want to shove all my content in one intent and make it a long stream of consciousness, but that's all I've got figured out right now. Is there a way to make a custom payload to trigger them? The documentation from both DialogFlow and Soul Machines aren't quite helpful with this.</p>
<p>Appreciate any advice and tips!</p>
",17,0,0,3,dialogflow-es;chatbot;digital-persona-sdk,2022-07-27 20:12:29,2022-07-27 20:12:29,2022-07-27 20:12:29,i m new to google s dialogflow and am using it with soul machines to create a digital person  i ve been learning it through brute force methods  i want this person to present some content  basically just walking through some information in a monologue style rather than a normal chatbot dialog  it would be as if i was standing in front of a camera talking and explaining some concept that i d eventually put on a website  how do i do this  i can t quite seem to figure out how to add a bunch of intents that would be triggered by the previous one like how a human would be triggered by new cue cards  i don t want to shove all my content in one intent and make it a long stream of consciousness  but that s all i ve got figured out right now  is there a way to make a custom payload to trigger them  the documentation from both dialogflow and soul machines aren t quite helpful with this  appreciate any advice and tips ,dialogflow es   how to get chatbot to perform a  monologue  type message,google dialogflow using soul machines create digital person learning brute force methods want person present content basically walking information monologue style rather normal chatbot dialog would standing front camera talking explaining concept eventually put website quite seem figure bunch intents would triggered previous one like human would triggered cue cards want shove content one intent make long stream consciousness got figured right way make payload trigger documentation dialogflow soul machines quite helpful appreciate advice tips,dialogflow es get chatbot perform monologue type message,dialogflow es get chatbot perform monologue type messagegoogle dialogflow using soul machines create digital person learning brute force methods want person present content basically walking information monologue style rather normal chatbot dialog would standing front camera talking explaining concept eventually put website quite seem figure bunch intents would triggered previous one like human would triggered cue cards want shove content one intent make long stream consciousness got figured right way make payload trigger documentation dialogflow soul machines quite helpful appreciate advice tips,"['dialogflow', 'es', 'get', 'chatbot', 'perform', 'monologue', 'type', 'messagegoogle', 'dialogflow', 'using', 'soul', 'machines', 'create', 'digital', 'person', 'learning', 'brute', 'force', 'methods', 'want', 'person', 'present', 'content', 'basically', 'walking', 'information', 'monologue', 'style', 'rather', 'normal', 'chatbot', 'dialog', 'would', 'standing', 'front', 'camera', 'talking', 'explaining', 'concept', 'eventually', 'put', 'website', 'quite', 'seem', 'figure', 'bunch', 'intents', 'would', 'triggered', 'previous', 'one', 'like', 'human', 'would', 'triggered', 'cue', 'cards', 'want', 'shove', 'content', 'one', 'intent', 'make', 'long', 'stream', 'consciousness', 'got', 'figured', 'right', 'way', 'make', 'payload', 'trigger', 'documentation', 'dialogflow', 'soul', 'machines', 'quite', 'helpful', 'appreciate', 'advice', 'tips']","['dialogflow', 'es', 'get', 'chatbot', 'perform', 'monologu', 'type', 'messagegoogl', 'dialogflow', 'use', 'soul', 'machin', 'creat', 'digit', 'person', 'learn', 'brute', 'forc', 'method', 'want', 'person', 'present', 'content', 'basic', 'walk', 'inform', 'monologu', 'style', 'rather', 'normal', 'chatbot', 'dialog', 'would', 'stand', 'front', 'camera', 'talk', 'explain', 'concept', 'eventu', 'put', 'websit', 'quit', 'seem', 'figur', 'bunch', 'intent', 'would', 'trigger', 'previou', 'one', 'like', 'human', 'would', 'trigger', 'cue', 'card', 'want', 'shove', 'content', 'one', 'intent', 'make', 'long', 'stream', 'conscious', 'got', 'figur', 'right', 'way', 'make', 'payload', 'trigger', 'document', 'dialogflow', 'soul', 'machin', 'quit', 'help', 'appreci', 'advic', 'tip']"
50,56,56,19556868,72995259,"Input 0 of layer &quot;dense3&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4096,)","<p>I am pretty new to keras and machine learning, but my code is not working and I am pretty sure it is because of my model.  I am trying to train my model with a csv file that has my images all flattened into one row.  Essentially, each row of data is one image.  However, this error message keeps popping up</p>
<p>ValueError: Exception encountered when calling layer &quot;sequential&quot; (type Sequential).</p>
<pre><code>Input 0 of layer &quot;dense3&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4096,)

Call arguments received by layer &quot;sequential&quot; (type Sequential):
   inputs=tf.Tensor(shape=(4096,), dtype=uint8)
   training=False
   mask=None
</code></pre>
<p>I have looked online and tried a lot of things, but I am new, so I do not completely understand everything.  The image is 64x64, so I assumed that the columns were the input size.... so set the input size equal to 64<em>64 and also changed the predict batch size to be 64</em>64 that way it matches (since I am getting the real-time flattened image to be predicted).  It solved the problem for the first layer (the input layer), but now the dense layers after are not working.  I also have the image label at the first index of every row on the csv file.</p>
<pre><code>def modelTraining():
model = keras.models.Sequential()
model.add(keras.Input(shape=(4096,)))
model.add(keras.layers.Dense(4096, activation='relu', name = &quot;dense2&quot;), )
model.add(keras.layers.Dense(1, activation='softmax', name = &quot;dense3&quot;, ))

model.compile(optimizer='rmsprop',
          loss='binary_crossentropy',
          metrics=['accuracy'])

#Get our training image data
trainingImageMatrix = []
trainingLabelsArray = []
temp = 0
with open(r'C:\Users\Jason\Desktop\signLanguage\testOpenCV\training.csv', 'r', newline='') as myFile:
    reader = csv.reader(myFile, delimiter=',')
    for row in reader:
        temp+=1
        imageDataToAdd = np.zeros(4096)
        for i in range (1, 4097):
            imageDataToAdd[i - 1] = row[i]
        trainingLabelsArray.append(int(row[0])) #need to get the first index of every row
        trainingImageMatrix.append(imageDataToAdd)
        print(&quot;reading row: &quot;+str(temp))
        #if temp &gt;=10:
            #break
    trainingImageMatrix = np.array(trainingImageMatrix)
    trainingLabelsArray = np.array(trainingLabelsArray)
model.fit(trainingImageMatrix, trainingLabelsArray, epochs = 20, batch_size = 4096)
return model
</code></pre>
<h1></h1>
<pre><code>model.predict(grayFrame.flatten(), batch_size = 4096) 
</code></pre>
<p>this line is in another part of my code, but this is the predict I am using atm.</p>
<p>Would appreciate some help and clarification on how to specify the size and how the layers should be customized for my task.</p>
<p><a href=""https://i.stack.imgur.com/bWbeb.png"" rel=""nofollow noreferrer"">each row has the 0th index as a label and the rest is 64*64 worth of pixel brightness ---- CSV FILE IMAGE</a></p>
",57,1,1,5,python;tensorflow;machine-learning;keras;neural-network,2022-07-15 17:06:08,2022-07-15 17:06:08,2022-07-27 19:20:17,i am pretty new to keras and machine learning  but my code is not working and i am pretty sure it is because of my model   i am trying to train my model with a csv file that has my images all flattened into one row   essentially  each row of data is one image   however  this error message keeps popping up valueerror  exception encountered when calling layer  sequential   type sequential   i have looked online and tried a lot of things  but i am new  so i do not completely understand everything   the image is x  so i assumed that the columns were the input size     so set the input size equal to  and also changed the predict batch size to be  that way it matches  since i am getting the real time flattened image to be predicted    it solved the problem for the first layer  the input layer   but now the dense layers after are not working   i also have the image label at the first index of every row on the csv file  this line is in another part of my code  but this is the predict i am using atm  would appreciate some help and clarification on how to specify the size and how the layers should be customized for my task  ,input  of layer  dense  is incompatible with the layer  expected min_ndim   found ndim   full shape received     ,pretty keras machine learning code working pretty sure model trying train model csv file images flattened one row essentially row data one image however error message keeps popping valueerror exception encountered calling layer sequential type sequential looked online tried lot things completely understand everything image x assumed columns input size set input size equal also changed predict batch size way matches since getting real time flattened image predicted solved problem first layer input layer dense layers working also image label first index every row csv file line another part code predict using atm would appreciate help clarification specify size layers customized task,input layer dense incompatible layer expected min_ndim found ndim full shape received,input layer dense incompatible layer expected min_ndim found ndim full shape receivedpretty keras machine learning code working pretty sure model trying train model csv file images flattened one row essentially row data one image however error message keeps popping valueerror exception encountered calling layer sequential type sequential looked online tried lot things completely understand everything image x assumed columns input size set input size equal also changed predict batch size way matches since getting real time flattened image predicted solved problem first layer input layer dense layers working also image label first index every row csv file line another part code predict using atm would appreciate help clarification specify size layers customized task,"['input', 'layer', 'dense', 'incompatible', 'layer', 'expected', 'min_ndim', 'found', 'ndim', 'full', 'shape', 'receivedpretty', 'keras', 'machine', 'learning', 'code', 'working', 'pretty', 'sure', 'model', 'trying', 'train', 'model', 'csv', 'file', 'images', 'flattened', 'one', 'row', 'essentially', 'row', 'data', 'one', 'image', 'however', 'error', 'message', 'keeps', 'popping', 'valueerror', 'exception', 'encountered', 'calling', 'layer', 'sequential', 'type', 'sequential', 'looked', 'online', 'tried', 'lot', 'things', 'completely', 'understand', 'everything', 'image', 'x', 'assumed', 'columns', 'input', 'size', 'set', 'input', 'size', 'equal', 'also', 'changed', 'predict', 'batch', 'size', 'way', 'matches', 'since', 'getting', 'real', 'time', 'flattened', 'image', 'predicted', 'solved', 'problem', 'first', 'layer', 'input', 'layer', 'dense', 'layers', 'working', 'also', 'image', 'label', 'first', 'index', 'every', 'row', 'csv', 'file', 'line', 'another', 'part', 'code', 'predict', 'using', 'atm', 'would', 'appreciate', 'help', 'clarification', 'specify', 'size', 'layers', 'customized', 'task']","['input', 'layer', 'dens', 'incompat', 'layer', 'expect', 'min_ndim', 'found', 'ndim', 'full', 'shape', 'receivedpretti', 'kera', 'machin', 'learn', 'code', 'work', 'pretti', 'sure', 'model', 'tri', 'train', 'model', 'csv', 'file', 'imag', 'flatten', 'one', 'row', 'essenti', 'row', 'data', 'one', 'imag', 'howev', 'error', 'messag', 'keep', 'pop', 'valueerror', 'except', 'encount', 'call', 'layer', 'sequenti', 'type', 'sequenti', 'look', 'onlin', 'tri', 'lot', 'thing', 'complet', 'understand', 'everyth', 'imag', 'x', 'assum', 'column', 'input', 'size', 'set', 'input', 'size', 'equal', 'also', 'chang', 'predict', 'batch', 'size', 'way', 'match', 'sinc', 'get', 'real', 'time', 'flatten', 'imag', 'predict', 'solv', 'problem', 'first', 'layer', 'input', 'layer', 'dens', 'layer', 'work', 'also', 'imag', 'label', 'first', 'index', 'everi', 'row', 'csv', 'file', 'line', 'anoth', 'part', 'code', 'predict', 'use', 'atm', 'would', 'appreci', 'help', 'clarif', 'specifi', 'size', 'layer', 'custom', 'task']"
51,57,57,10775269,73137433,ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml,"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:</p>
<pre><code># Loading run config
print(&quot;Loading run config&quot;)
task_1_run_config = RunConfiguration.load(
    os.path.join(WORKING_DIR + '/pipeline/task_runconfigs/T01_Test_Task.yml')
    ) 

task_1_script_run_config = ScriptRunConfig(
    source_directory=os.path.join(WORKING_DIR + '/pipeline/task_scripts'),
    run_config=task_1_run_config    
)

task_1_py_script_step = PythonScriptStep(
    name='Task_1_Step',
    script_name=task_1_script_run_config.script,
    source_directory=task_1_script_run_config.source_directory,
    compute_target=compute_target
)

pipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])

pipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)
pipeline_run.wait_for_completion()
</code></pre>
<p>The environment.yml</p>
<pre><code>name: phinmo_pipeline_env
dependencies:
- python=3.8
- pip:
  - pandas
  - azureml-core==1.43.0
  - azureml-sdk
  - scipy
  - scikit-learn
  - numpy
  - pyyaml==6.0
  - datetime
  - azure
channels:
  - conda-forge
</code></pre>
<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:</p>
<pre><code># The script to run.
script: T01_Test_Task.py
# The arguments to the script file.
arguments: [
  &quot;--test&quot;, False,
  &quot;--date&quot;, &quot;2022-07-26&quot;
]
# The name of the compute target to use for this run.
compute_target: phinmo-compute-cluster
# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.
framework: Python
# Maximum allowed duration for the run.
maxRunDurationSeconds: 6000
# Number of nodes to use for running job.
nodeCount: 1

#Environment details.
environment:
  # Environment name
  name: phinmo_pipeline_env
  # Environment version
  version:
  # Environment variables set for the run.
  #environmentVariables:
  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE
  # Python details
  python:
    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.
    userManagedDependencies: false
    # The python interpreter path
    interpreterPath: python
    # Path to the conda dependencies file to use for this run. If a project
    # contains multiple programs with different sets of dependencies, it may be
    # convenient to manage those environments with separate files.
    condaDependenciesFile: environment.yml
    # The base conda environment used for incremental environment creation.
    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu
  # Docker details
  
# History details.
history:
  # Enable history tracking -- this allows status, logs, metrics, and outputs
  # to be collected for a run.
  outputCollection: true
  # Whether to take snapshots for history.
  snapshotProject: true
  # Directories to sync with FileWatcher.
  directoriesToWatch:
  - logs
# data reference configuration details
dataReferences: {}
# The configuration details for data.
data: {}
# Project share datastore reference.
sourceDirectoryDataStore:
</code></pre>
<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.</p>
",17,1,0,4,python;azure-machine-learning-service;azureml-python-sdk;azuremlsdk,2022-07-27 14:53:18,2022-07-27 14:53:18,2022-07-27 19:00:35,i am running into a modulenotfounderror for pandas while using the following code to orchestrate my azure machine learning pipeline  the environment yml the loaded runconfiguration in t_test_task yml looks like this  i already tried a few things like overwriting the environment attribute in the runconfiguration object with a environment python conda_dependencies object or assigning a version number to pandas in the environment yml  changing the location of the environment yml  but i am at a loss at what else to try  the t_test_task py runs without issues on its own  but putting it into a pipeline just does not seem to work ,modulenotfounderror while using azureml pipeline with yml file based runconfiguration and environment yml,running modulenotfounderror pandas using following code orchestrate azure machine learning pipeline environment yml loaded runconfiguration t_test_task yml looks like already tried things like overwriting environment attribute runconfiguration object environment python conda_dependencies object assigning version number pandas environment yml changing location environment yml loss else try t_test_task py runs without issues putting pipeline seem work,modulenotfounderror using azureml pipeline yml file based runconfiguration environment yml,modulenotfounderror using azureml pipeline yml file based runconfiguration environment ymlrunning modulenotfounderror pandas using following code orchestrate azure machine learning pipeline environment yml loaded runconfiguration t_test_task yml looks like already tried things like overwriting environment attribute runconfiguration object environment python conda_dependencies object assigning version number pandas environment yml changing location environment yml loss else try t_test_task py runs without issues putting pipeline seem work,"['modulenotfounderror', 'using', 'azureml', 'pipeline', 'yml', 'file', 'based', 'runconfiguration', 'environment', 'ymlrunning', 'modulenotfounderror', 'pandas', 'using', 'following', 'code', 'orchestrate', 'azure', 'machine', 'learning', 'pipeline', 'environment', 'yml', 'loaded', 'runconfiguration', 't_test_task', 'yml', 'looks', 'like', 'already', 'tried', 'things', 'like', 'overwriting', 'environment', 'attribute', 'runconfiguration', 'object', 'environment', 'python', 'conda_dependencies', 'object', 'assigning', 'version', 'number', 'pandas', 'environment', 'yml', 'changing', 'location', 'environment', 'yml', 'loss', 'else', 'try', 't_test_task', 'py', 'runs', 'without', 'issues', 'putting', 'pipeline', 'seem', 'work']","['modulenotfounderror', 'use', 'azureml', 'pipelin', 'yml', 'file', 'base', 'runconfigur', 'environ', 'ymlrun', 'modulenotfounderror', 'panda', 'use', 'follow', 'code', 'orchestr', 'azur', 'machin', 'learn', 'pipelin', 'environ', 'yml', 'load', 'runconfigur', 't_test_task', 'yml', 'look', 'like', 'alreadi', 'tri', 'thing', 'like', 'overwrit', 'environ', 'attribut', 'runconfigur', 'object', 'environ', 'python', 'conda_depend', 'object', 'assign', 'version', 'number', 'panda', 'environ', 'yml', 'chang', 'locat', 'environ', 'yml', 'loss', 'els', 'tri', 't_test_task', 'py', 'run', 'without', 'issu', 'put', 'pipelin', 'seem', 'work']"
52,58,58,1896222,73130599,Tensorflow Fused conv implementation does not support grouped convolutions,"<p>I did a neural network machine learning on colored images (3 channels). It worked but now I want to try to do it in grayscale to see if I can improve accuracy.
Here is the code:</p>
<pre><code>train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)
 
test_datagen = ImageDataGenerator(rescale=1. / 255)
 
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True)
 
validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    color_mode='grayscale',
    class_mode='binary',
    shuffle=True)
model = tf.keras.Sequential()
input_shape = (img_width, img_height, 1)
model.add(Conv2D(32, 2, input_shape=input_shape, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
 
model.add(Conv2D(32, 2, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
 
model.add(Conv2D(64, 2, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
 
model.add(Flatten())
model.add(Dense(128))
model.add(Dense(len(classes)))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model.fit(train_generator,
         validation_data=validation_generator,
         epochs=EPOCHS)
</code></pre>
<p>You can see that I have changed the input_shape to have 1 single channel for grayscale.
I'm getting an error:</p>
<p><code>Node: 'sequential_26/conv2d_68/Relu' Fused conv implementation does not support grouped convolutions for now. [[{{node sequential_26/conv2d_68/Relu}}]] [Op:__inference_train_function_48830]</code></p>
<p>Any idea how to fix this?</p>
",31,2,0,4,python;tensorflow;keras;deep-learning,2022-07-27 02:01:24,2022-07-27 02:01:24,2022-07-27 18:48:32,node   sequential_ convd_ relu  fused conv implementation does not support grouped convolutions for now      node sequential_ convd_ relu      op __inference_train_function_  any idea how to fix this ,tensorflow fused conv implementation does not support grouped convolutions,node sequential_ convd_ relu fused conv implementation support grouped convolutions node sequential_ convd_ relu op __inference_train_function_ idea fix,tensorflow fused conv implementation support grouped convolutions,tensorflow fused conv implementation support grouped convolutionsnode sequential_ convd_ relu fused conv implementation support grouped convolutions node sequential_ convd_ relu op __inference_train_function_ idea fix,"['tensorflow', 'fused', 'conv', 'implementation', 'support', 'grouped', 'convolutionsnode', 'sequential_', 'convd_', 'relu', 'fused', 'conv', 'implementation', 'support', 'grouped', 'convolutions', 'node', 'sequential_', 'convd_', 'relu', 'op', '__inference_train_function_', 'idea', 'fix']","['tensorflow', 'fuse', 'conv', 'implement', 'support', 'group', 'convolutionsnod', 'sequential_', 'convd_', 'relu', 'fuse', 'conv', 'implement', 'support', 'group', 'convolut', 'node', 'sequential_', 'convd_', 'relu', 'op', '__inference_train_function_', 'idea', 'fix']"
53,59,59,19000938,73139665,Method does not take the last argument as input,"<p>I have wrote machine learning algorithm  and when I try to call fit method, python cannot get n_trees as input. how to overcome this problem? Code returns TypeError: fit() missing 1 required positional argument: 'n_trees'</p>
<pre><code>class GradientBoosting:
    global trees
    trees = []
    
    def __init__(self, nu, n_trees):
        self.nu =  nu
        self.n_trees = n_trees
    def fit(self, X, y, nu, n_trees):
        y_pred = y.mean()
        residual = y - y_pred
        for i in range(n_trees):
            dt = DecisionTreeRegressor(max_depth = 1)
            tree = dt.fit(X, residual)
            residual = tree.predict(X)
            y_pred += nu * residual
            trees.append(tree)
        return trees
    def predict(self, X, trees):
        y_pred = y.mean()
        for tree in trees:
            residual = tree.predict(X)
            y_pred += nu * residual
        return y_pred
GradientBoosting.fit( X, y, 0.1 , 100)
</code></pre>
",24,0,0,3,python;oop;methods,2022-07-27 17:20:38,2022-07-27 17:20:38,2022-07-27 17:20:38,i have wrote machine learning algorithm  and when i try to call fit method  python cannot get n_trees as input  how to overcome this problem  code returns typeerror  fit   missing  required positional argument   n_trees ,method does not take the last argument as input,wrote machine learning algorithm try call fit method python cannot get n_trees input overcome problem code returns typeerror fit missing required positional argument n_trees,method take last argument input,method take last argument inputwrote machine learning algorithm try call fit method python cannot get n_trees input overcome problem code returns typeerror fit missing required positional argument n_trees,"['method', 'take', 'last', 'argument', 'inputwrote', 'machine', 'learning', 'algorithm', 'try', 'call', 'fit', 'method', 'python', 'can', 'not', 'get', 'n_trees', 'input', 'overcome', 'problem', 'code', 'returns', 'typeerror', 'fit', 'missing', 'required', 'positional', 'argument', 'n_trees']","['method', 'take', 'last', 'argument', 'inputwrot', 'machin', 'learn', 'algorithm', 'tri', 'call', 'fit', 'method', 'python', 'can', 'not', 'get', 'n_tree', 'input', 'overcom', 'problem', 'code', 'return', 'typeerror', 'fit', 'miss', 'requir', 'posit', 'argument', 'n_tree']"
54,60,60,19445509,73126677,Partial dependence plots with the random forest output of MERF algorithm,"<p>I have implemented the MERF function from the longituRF package (<a href=""https://www.rdocumentation.org/packages/LongituRF/versions/0.9/topics/MERF"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/LongituRF/versions/0.9/topics/MERF</a>) to predict values of PM2.5 using some of the other variables in the following dataframe:</p>
<pre><code>  PM1 PM2_5  PM10    NO2     CO2    TVOC Temperature Relative_Humidity Boiling Frying Oven Toasting Cleaning Alt_combust PM25_OAP NO2_OAP hour cooking taeFry taeBoil taeOven
1 1.135 1.557 2.392 33.875 871.678 802.800      19.659            58.306       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
2 1.347 1.611 3.023 33.897 877.900 811.987      19.663            58.306       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
3 1.347 1.611 3.023 33.897 877.900 811.987      19.663            58.306       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
4 1.347 1.611 3.023 33.897 877.900 811.987      19.663            58.306       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
5 1.239 1.295 2.485 33.821 884.501 813.560      19.655            58.390       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
6 1.239 1.295 2.485 33.821 884.501 813.560      19.655            58.390       0      0    0        0        0           0        4   15.28   19       0  54000   54000   54000
  taeToast lasthourFry lasthalfhourFry lasthourBoil lasthalfhourBoil lasthourToast lasthalfhourToast lasthourOven lasthalfhourOven group time
1    54000           0               0            0                0             0                 0            0                0     1    1
2    54000           0               0            0                0             0                 0            0                0     1    2
3    54000           0               0            0                0             0                 0            0                0     1    3
4    54000           0               0            0                0             0                 0            0                0     1    4
5    54000           0               0            0                0             0                 0            0                0     1    5
6    54000           0               0            0                0             0                 0            0                0     1    6
</code></pre>
<p>I now wish to use the output to calculate partial dependencies for each variable and am attempting to use the partial() function from pdp. I have the following code:</p>
<pre><code>model &lt;- readRDS('practice_reemforest.rds')

model_rf &lt;- model$forest #to extract the random forest part of the output

df &lt;- readRDS('combined_dataset_for_reemforest.rds') #the dataframe i used for the MERF algorithm

df_s &lt;- cbind(df$lasthalfhourBoil,df$lasthalfhourFry,df$lasthalfhourOven,df$lasthalfhourToast) #Creating a dataframe of the  joint values of interest for the variables listed in pred.var (I ideally 
#would look at all variables used by the MERF model which include 7 other variables from the dataframe)

colnames(df_s) &lt;- c('lasthalfhourBoil','lasthalfhourFry','lasthalfhourOven','lasthalfhourToast')
df_s &lt;- as.data.frame(df_s)

partial &lt;- partial(model_rf,
                    pred.var=c('lasthalfhourBoil','lasthalfhourFry','lasthalfhourOven','lasthalfhourToast'),
                    pred.grid=(df_s),
                    train=df)
</code></pre>
<p>However, I get the following error when I run the code, and am also unsure if I have used the right data.</p>
<pre><code>Error in { : task 1 failed - &quot;missing values in newdata&quot;
</code></pre>
<p>After reading the post <a href=""https://stackoverflow.com/questions/30097730/error-when-using-predict-on-a-randomforest-object-trained-with-carets-train"">Error when using predict() on a randomForest object trained with caret&#39;s train() using formula</a> , I could get this error because I have some binary variables, however all variables are numeric so I am not sure that this necessarily the correct explanation to why I get that particular error message. I am also unsure how I specify the 'new data' or what counts as the new data in this case, in the documentation for the predict function for randomForest it states that if newdata is not given, the out-of-bag prediction in object is returned. As far as I can tell, the model object produced by MERF is a normal randomForest object, so this is an issue with the partial() function.</p>
<p>I am relatively new to R and machine learning methods, so any advice how to fix this issue would be appreciated. I also apologise if I have not provided sufficient information or been clear enough in what I am asking.</p>
<p><strong>Edit:</strong></p>
<p>I think my dataframe is too large for the whole output to show in the console as it is ~118000 rows. The bottom part of what is output looks like: <code>44532, 44533, 44534, 44535, 44536, 44537, 44538, 44539, 44540,  44541, 44542, 44543, 44544, 44545, 44546, 44547, 44548, 44549,  44550, 44551, 44552, 44553, 44554, 44555, 44556, 44557, 44558,  44559, 44560, 44561, 44562, 44563, 44564, 44565, 44566, 44567,  44568, 44569, 44570, 44571, 44572, 44573, 44574, 44575, 44576,  44577, 44578, 44579, 44580, 44581, 44582, 44583, 44584, 44585,  44586, 44587, 44588, 44589, 44590, 44591, 44592, 44593, 44594,  44595, 44596, 44597, 44598, 44599, 44600, 44601, 44602, 44603,  44604, 44605, 44606, 44607, 44608, 44609, 44610, 44611, 44612,  44613, 44614, 44615, 44616, 44617, 44618, 44619, 44620, 44621,  44622, 44623, 44624, 44625, 44626, 44627, 44628, 44629, 44630,  44631, 44632, 44633, 44634, 44635, 44636, 44637, 44638, 44639,  44640, 44641, 44642, 44643, 44644, 44645, 44646, 44647, 44648,  44649, 44650, 44651, 44652, 44653, 44654, 44655, 44656, 44657,  44658, 44659, 44660, 44661, 44662, 44663, 44664)), row.names = c(NA,  -118255L), na.action = structure(c(</code>1<code>= 1L,</code>2<code>= 2L,</code>3<code>= 3L, </code>4<code>= 4L,</code>5<code>= 5L,</code>6<code>= 6L,</code>7<code>= 7L,</code>8<code>= 8L,</code>9<code>= 9L,</code>10<code>= 10L, </code>11<code>= 11L,</code>12<code>= 12L,</code>13<code>= 13L,</code>14<code>= 14L,</code>15<code>= 15L,</code>16<code>= 16L, </code>17<code>= 17L,</code>18<code>= 18L,</code>19<code>= 19L,</code>20<code>= 20L,</code>21<code>= 21L,</code>22<code>= 22L, </code>23<code>= 23L,</code>24<code>= 24L,</code>25<code>= 25L,</code>26<code>= 26L,</code>27<code>= 27L,</code>28<code>= 28L, </code>29<code>= 29L,</code>30<code>= 30L,</code>530<code>= 530L,</code>531<code>= 531L,</code>532<code>= 532L, </code>533<code>= 533L,</code>534<code>= 534L,</code>535<code>= 535L,</code>536<code>= 536L,</code>537<code>= 537L, </code>538<code>= 538L,</code>539<code>= 539L,</code>540<code>= 540L,</code>541<code>= 541L,</code>542<code>= 542L, </code>543<code>= 543L,</code>544<code>= 544L,</code>545<code>= 545L,</code>546<code>= 546L,</code>547<code>= 547L, </code>548<code>= 548L,</code>549<code>= 549L,</code>550<code>= 550L,</code>551<code>= 551L,</code>552<code>= 552L, </code>553<code>= 553L,</code>9034<code>= 9034L,</code>9035<code>= 9035L,</code>9036<code>= 9036L, </code>9037<code>= 9037L,</code>9038<code>= 9038L,</code>9039<code>= 9039L,</code>9040<code>= 9040L, </code>9041<code>= 9041L,</code>9042<code>= 9042L,</code>9043<code>= 9043L,</code>9044<code>= 9044L, </code>9045<code>= 9045L,</code>9260<code>= 9260L,</code>9261<code>= 9261L,</code>9262<code>= 9262L, </code>9263<code>= 9263L,</code>9264<code>= 9264L,</code>9265<code>= 9265L,</code>9266<code>= 9266L, </code>9267<code>= 9267L,</code>9268<code>= 9268L,</code>21779<code>= 21779L,</code>21780<code>= 21780L, </code>21781<code>= 21781L,</code>21782<code>= 21782L,</code>23602<code>= 23602L,</code>23603<code>= 23603L, </code>23604<code>= 23604L,</code>23605<code>= 23605L,</code>23606<code>= 23606L,</code>23607<code>= 23607L, </code>23608<code>= 23608L,</code>23609<code>= 23609L,</code>23610<code>= 23610L,</code>23611<code>= 23611L, </code>23612<code>= 23612L,</code>23613<code>= 23613L,</code>23614<code>= 23614L,</code>23615<code>= 23615L, </code>23616<code>= 23616L,</code>23617<code>= 23617L,</code>23618<code>= 23618L,</code>23619<code>= 23619L, </code>23620<code>= 23620L,</code>51397<code>= 51397L,</code>51398<code>= 51398L,</code>51399<code>= 51399L, </code>51400<code>= 51400L,</code>51401<code>= 51401L,</code>51402<code>= 51402L,</code>55055<code>= 55055L, </code>55056<code>= 55056L,</code>55057<code>= 55057L,</code>55058<code>= 55058L,</code>55059<code>= 55059L, </code>55060<code>= 55060L,</code>55061<code>= 55061L,</code>55062<code>= 55062L,</code>55063<code>= 55063L, </code>55064<code>= 55064L,</code>55065<code>= 55065L,</code>55066<code>= 55066L,</code>58338<code>= 58338L, </code>58339<code>= 58339L,</code>58340<code>= 58340L,</code>58341<code>= 58341L,</code>58342<code>= 58342L, </code>58343<code>= 58343L,</code>58344<code>= 58344L,</code>58345<code>= 58345L,</code>58346<code>= 58346L, </code>58347<code>= 58347L,</code>58348<code>= 58348L,</code>58349<code>= 58349L,</code>58350<code>= 58350L, </code>58351<code>= 58351L,</code>58352<code>= 58352L,</code>58353<code>= 58353L,</code>58354<code>= 58354L, </code>58355<code>= 58355L,</code>58356<code>= 58356L,</code>58357` = 58357L), class = &quot;omit&quot;), class = &quot;data.frame&quot;)</p>
",23,0,0,2,r;random-forest,2022-07-26 19:12:34,2022-07-26 19:12:34,2022-07-27 15:01:57,i have implemented the merf function from the longiturf package    to predict values of pm  using some of the other variables in the following dataframe  i now wish to use the output to calculate partial dependencies for each variable and am attempting to use the partial   function from pdp  i have the following code  however  i get the following error when i run the code  and am also unsure if i have used the right data  after reading the post    i could get this error because i have some binary variables  however all variables are numeric so i am not sure that this necessarily the correct explanation to why i get that particular error message  i am also unsure how i specify the  new data  or what counts as the new data in this case  in the documentation for the predict function for randomforest it states that if newdata is not given  the out of bag prediction in object is returned  as far as i can tell  the model object produced by merf is a normal randomforest object  so this is an issue with the partial   function  i am relatively new to r and machine learning methods  so any advice how to fix this issue would be appreciated  i also apologise if i have not provided sufficient information or been clear enough in what i am asking  edit  i think my dataframe is too large for the whole output to show in the console as it is   rows  the bottom part of what is output looks like                                                                                                                                                                                                                                                                                            row names   c na    l   na action   structure c   l   l   l    l   l   l   l   l   l   l    l   l   l   l   l   l    l   l   l   l   l   l    l   l   l   l   l   l    l   l   l   l   l    l   l   l   l   l    l   l   l   l   l    l   l   l   l   l    l   l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l   l   l    l   l     l   class    omit    class    data frame  ,partial dependence plots with the random forest output of merf algorithm,implemented merf function longiturf package predict values pm using variables following dataframe wish use output calculate partial dependencies variable attempting use partial function pdp following code however get following error run code also unsure used right data reading post could get error binary variables however variables numeric sure necessarily correct explanation get particular error message also unsure specify data counts data case documentation predict function randomforest states newdata given bag prediction object returned far tell model object produced merf normal randomforest object issue partial function relatively r machine learning methods advice fix issue would appreciated also apologise provided sufficient information clear enough asking edit think dataframe large whole output show console rows bottom part output looks like row names c na l na action structure c l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l class omit class data frame,partial dependence plots random forest output merf algorithm,partial dependence plots random forest output merf algorithmimplemented merf function longiturf package predict values pm using variables following dataframe wish use output calculate partial dependencies variable attempting use partial function pdp following code however get following error run code also unsure used right data reading post could get error binary variables however variables numeric sure necessarily correct explanation get particular error message also unsure specify data counts data case documentation predict function randomforest states newdata given bag prediction object returned far tell model object produced merf normal randomforest object issue partial function relatively r machine learning methods advice fix issue would appreciated also apologise provided sufficient information clear enough asking edit think dataframe large whole output show console rows bottom part output looks like row names c na l na action structure c l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l class omit class data frame,"['partial', 'dependence', 'plots', 'random', 'forest', 'output', 'merf', 'algorithmimplemented', 'merf', 'function', 'longiturf', 'package', 'predict', 'values', 'pm', 'using', 'variables', 'following', 'dataframe', 'wish', 'use', 'output', 'calculate', 'partial', 'dependencies', 'variable', 'attempting', 'use', 'partial', 'function', 'pdp', 'following', 'code', 'however', 'get', 'following', 'error', 'run', 'code', 'also', 'unsure', 'used', 'right', 'data', 'reading', 'post', 'could', 'get', 'error', 'binary', 'variables', 'however', 'variables', 'numeric', 'sure', 'necessarily', 'correct', 'explanation', 'get', 'particular', 'error', 'message', 'also', 'unsure', 'specify', 'data', 'counts', 'data', 'case', 'documentation', 'predict', 'function', 'randomforest', 'states', 'newdata', 'given', 'bag', 'prediction', 'object', 'returned', 'far', 'tell', 'model', 'object', 'produced', 'merf', 'normal', 'randomforest', 'object', 'issue', 'partial', 'function', 'relatively', 'r', 'machine', 'learning', 'methods', 'advice', 'fix', 'issue', 'would', 'appreciated', 'also', 'apologise', 'provided', 'sufficient', 'information', 'clear', 'enough', 'asking', 'edit', 'think', 'dataframe', 'large', 'whole', 'output', 'show', 'console', 'rows', 'bottom', 'part', 'output', 'looks', 'like', 'row', 'names', 'c', 'na', 'l', 'na', 'action', 'structure', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'class', 'omit', 'class', 'data', 'frame']","['partial', 'depend', 'plot', 'random', 'forest', 'output', 'merf', 'algorithmimpl', 'merf', 'function', 'longiturf', 'packag', 'predict', 'valu', 'pm', 'use', 'variabl', 'follow', 'datafram', 'wish', 'use', 'output', 'calcul', 'partial', 'depend', 'variabl', 'attempt', 'use', 'partial', 'function', 'pdp', 'follow', 'code', 'howev', 'get', 'follow', 'error', 'run', 'code', 'also', 'unsur', 'use', 'right', 'data', 'read', 'post', 'could', 'get', 'error', 'binari', 'variabl', 'howev', 'variabl', 'numer', 'sure', 'necessarili', 'correct', 'explan', 'get', 'particular', 'error', 'messag', 'also', 'unsur', 'specifi', 'data', 'count', 'data', 'case', 'document', 'predict', 'function', 'randomforest', 'state', 'newdata', 'given', 'bag', 'predict', 'object', 'return', 'far', 'tell', 'model', 'object', 'produc', 'merf', 'normal', 'randomforest', 'object', 'issu', 'partial', 'function', 'rel', 'r', 'machin', 'learn', 'method', 'advic', 'fix', 'issu', 'would', 'appreci', 'also', 'apologis', 'provid', 'suffici', 'inform', 'clear', 'enough', 'ask', 'edit', 'think', 'datafram', 'larg', 'whole', 'output', 'show', 'consol', 'row', 'bottom', 'part', 'output', 'look', 'like', 'row', 'name', 'c', 'na', 'l', 'na', 'action', 'structur', 'c', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'class', 'omit', 'class', 'data', 'frame']"
55,61,61,9093067,73135573,ML.NET machine learning model not producing the same results using the same data,"<p>We are running the code below for a number of different days. There is a set of data used for each day but for a given day the data does not change.</p>
<p>If we run the algorithm a number of times for one day then the cluster results created are consistent.</p>
<p>If we run the algorithm a number of times for a number of days then sometimes the cluster results created are NOT consistent for one (or more) days.</p>
<pre><code>                    for (int i = 1; i &lt;= 4; i++)
                    {
                        MLContext mlContext = new MLContext(seed: 100);

                        IDataView trainingData = mlContext.Data.LoadFromEnumerable&lt;Data&gt;(filteredData);

                        var options = new KMeansTrainer.Options
                        {
                            NumberOfClusters = i,
                            NumberOfThreads = (i == 1) ? 1 : 50,
                            InitializationAlgorithm = InitializationAlgorithm.Random
                        };

                        // Set up a learning pipeline
                        // Step 1: concatenate input features into a single column
                        var pipeline = mlContext.Transforms.Concatenate(
                            &quot;Features&quot;,
                            &quot;Value&quot;)

                            // Step 2: use the k-means clustering algorithm
                            .Append(mlContext.Clustering.Trainers.KMeans(options));

                        try
                        {
                            using (TransformerChain&lt;ClusteringPredictionTransformer&lt;KMeansModelParameters&gt;&gt; model = pipeline.Fit(trainingData))
                            {
                                VBuffer&lt;float&gt;[] centroids = null;

                                var last = model.LastTransformer;
                                last.Model.GetClusterCentroids(ref centroids, out int k);

#if DUMP_METRICS
                                // Evaluate the overall metrics
                                IDataView transformedData = model.Transform(trainingData);
                                ClusteringMetrics metrics = mlContext.Clustering.Evaluate(transformedData, null, &quot;Score&quot;, &quot;Features&quot;);
                                Debug.WriteLine($&quot;Average Distance: &quot; + $&quot;{metrics.AverageDistance:F2}&quot;);
#endif

                                // Get all centroids
                                ClusterInfo clusterInfo = new ClusterInfo();
                                clusterInfo.NumberOfClusters = k;

                                List&lt;CenterInfo&gt; centerInfos = new List&lt;CenterInfo&gt;(k);

                                for (int j = 0; j &lt; k; j++)
                                {
                                    CenterInfo centerInfo = new CenterInfo();
                                    centerInfo.Value = centroids[j].GetValues().ToArray().FirstOrDefault();
                                    centerInfo.WithinSS = 0;
                                    centerInfos.Add(centerInfo);
                                }

                                // For each reading in the data find out which value is closest
                                for (int y = 0; y &lt; filteredData.Count; y++)
                                {
                                    float value = filteredData[y].Value;

                                    List&lt;double&gt; distances = new List&lt;double&gt;();
                                    for (int j = 0; j &lt; k; j++) distances.Add(Math.Pow(value - centerInfos[j].Value, 2));

                                    double minDistance = distances.Min();
                                    int index = distances.FindIndex(a =&gt; a == minDistance);

                                    Debug.Assert(value == data[y + indexRange.StartIndex]);
                                    centerInfos[index].AddSample(new Sample&lt;float&gt;(data.FlagEncodedTimeAtIndex(y + indexRange.StartIndex), value));
                                    centerInfos[index].WithinSS += minDistance;
                                }

                                Debug.Assert(centerInfos.Sum(a =&gt; a.NumberOfSamples) == filteredData.Count());

                                clusterInfo.TotalWithinSS = centerInfos.Sum(a =&gt; a.WithinSS);
                                clusterInfo.CenterInfos = centerInfos.OrderBy(a =&gt; a.WithinSS).ToList();
                                clusterInfos.Add(clusterInfo);

#if EXTRA_LOGGING
                                foreach (CenterInfo centerInfo in centerInfos)
                                {
                                    Debug.WriteLine(&quot;Centre = &quot; + $&quot;{centerInfo.Value:F2}&quot; + &quot;, No. Samples = &quot; + $&quot;{centerInfo.NumberOfSamples}&quot; + &quot;, WithinSS = &quot; + $&quot;{centerInfo.WithinSS:F2}&quot;);
                                }
#endif
                            }
                        }
                        catch (InvalidOperationException e)
                        {
                            if (s_log.IsErrorEnabled) s_log.ErrorFormat(&quot;Error calculating cluster values ('{0}').&quot;, e.Message);
                        }
                    }
</code></pre>
<p>There is clearly some sort of reset that needs to be done in the code but I cannot see what I am missing.</p>
",21,0,0,2,machine-learning;ml.net,2022-07-27 12:37:53,2022-07-27 12:37:53,2022-07-27 13:09:51,we are running the code below for a number of different days  there is a set of data used for each day but for a given day the data does not change  if we run the algorithm a number of times for one day then the cluster results created are consistent  if we run the algorithm a number of times for a number of days then sometimes the cluster results created are not consistent for one  or more  days  there is clearly some sort of reset that needs to be done in the code but i cannot see what i am missing ,ml net machine learning model not producing the same results using the same data,running code number different days set data used day given day data change run algorithm number times one day cluster results created consistent run algorithm number times number days sometimes cluster results created consistent one days clearly sort reset needs done code cannot see missing,ml net machine learning model producing results using data,ml net machine learning model producing results using datarunning code number different days set data used day given day data change run algorithm number times one day cluster results created consistent run algorithm number times number days sometimes cluster results created consistent one days clearly sort reset needs done code cannot see missing,"['ml', 'net', 'machine', 'learning', 'model', 'producing', 'results', 'using', 'datarunning', 'code', 'number', 'different', 'days', 'set', 'data', 'used', 'day', 'given', 'day', 'data', 'change', 'run', 'algorithm', 'number', 'times', 'one', 'day', 'cluster', 'results', 'created', 'consistent', 'run', 'algorithm', 'number', 'times', 'number', 'days', 'sometimes', 'cluster', 'results', 'created', 'consistent', 'one', 'days', 'clearly', 'sort', 'reset', 'needs', 'done', 'code', 'can', 'not', 'see', 'missing']","['ml', 'net', 'machin', 'learn', 'model', 'produc', 'result', 'use', 'datarun', 'code', 'number', 'differ', 'day', 'set', 'data', 'use', 'day', 'given', 'day', 'data', 'chang', 'run', 'algorithm', 'number', 'time', 'one', 'day', 'cluster', 'result', 'creat', 'consist', 'run', 'algorithm', 'number', 'time', 'number', 'day', 'sometim', 'cluster', 'result', 'creat', 'consist', 'one', 'day', 'clearli', 'sort', 'reset', 'need', 'done', 'code', 'can', 'not', 'see', 'miss']"
56,62,62,19628380,73130285,Convolutional Autoencoder won&#39;t train data,"<p>I'm trying to create a convolutional Autoencoder that will work with seismic waveforms. The problem I have is that my model doesn't seem to be learning anything from the data, and when I test the model by comparing one waveform to the same, reconstructed version of it, I get a straight line. It should be outputting a similar waveform to the original:</p>
<p><a href=""https://i.stack.imgur.com/nYfTa.png"" rel=""nofollow noreferrer"">original_vs_reconstructed_image</a> (blue is original, orange is reconstructed)</p>
<p>In addition, the validation and test epochs don't change over time at all on my epoch graph.</p>
<p>I'm honestly not sure where to focus my debugging, but I suspect the problem lies with either the Autoencoder itself, the way my training function is structured, or the loss function I'm using (MSE).</p>
<p>I'm also fairly new to machine learning, so I might be missing something obvious or doing something totally wrong.</p>
<p>Here is my code that relates to this problem:</p>
<pre><code>import glob
import numpy as np
import obspy as obs
import sklearn.model_selection
import torch
import torch.nn as nn
import torch.nn.functional
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import random
import sys

files = glob.glob('/loggerhead/coke/wf_Tony/trim/15_62.5/1108/DH1' + '/*.mseed')
#  empty list to store the properly read waveforms
waves = []
#  read all the files
for f in files:
    temp_wave = obs.read(f)
    A = temp_wave[0].data
    # normalization
    B = A/np.max(np.abs(A))
    # ensures every wave is size 3126
    waves.append(np.pad(B, (0, 3126 - B.size), 'constant'))
wave_arr = np.vstack(waves)
train_arr, test_arr = sklearn.model_selection.train_test_split(wave_arr, train_size=0.95)
train_torch = torch.tensor(train_arr, requires_grad=True).clone()
test_torch = torch.tensor(test_arr, requires_grad=True).clone()

train_waves = train_torch.unsqueeze_(1)
test_waves = test_torch.unsqueeze_(1)

k = 7
p = k//2


class AutoEncoder(nn.Module):
    def __init__(self):
        #  make sure to always initialize the super class when using outside methods
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=k, padding=p), nn.LeakyReLU(),
            nn.Conv1d(64, 64, kernel_size=k, padding=p), nn.LeakyReLU(), nn.MaxPool1d(kernel_size=2, stride=2),
            nn.Conv1d(64, 128, kernel_size=k, padding=p), nn.LeakyReLU(), nn.Conv1d(128, 128, kernel_size=k, padding=p),
            nn.LeakyReLU(), nn.MaxPool1d(kernel_size=2, stride=2),
            nn.Conv1d(128, 256, kernel_size=k, padding=p), nn.LeakyReLU(), nn.Conv1d(256, 256, kernel_size=k, padding=p),
            nn.LeakyReLU(), nn.MaxPool1d(kernel_size=2, stride=2),
            nn.Conv1d(256, 512, kernel_size=k, padding=p), nn.LeakyReLU(), nn.Conv1d(512, 512, kernel_size=k, padding=p),
            nn.LeakyReLU(), nn.MaxPool1d(kernel_size=2, stride=2),
            nn.Conv1d(512, 1024, kernel_size=k, padding=p), nn.LeakyReLU(), nn.Conv1d(1024, 1024, kernel_size=k, padding=p),
            nn.LeakyReLU()
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(1024, 512, kernel_size=2, stride=2), nn.LeakyReLU(),
            nn.ConvTranspose1d(512, 256, kernel_size=2, stride=2), nn.LeakyReLU(),
            nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2), nn.LeakyReLU(),
            nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2), nn.LeakyReLU(),
            nn.Conv1d(64, 1, kernel_size=1, padding=p), nn.Tanh()
        )  

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
    return x

model = AutoEncoder()
loss_function_MSE = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)

# Check if the GPU is available
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
print(f'Selected device: {device}')

model.to(device)

# Training function
def train_epoch(model, device, loss_fn, optimizer):
    # Set train mode for both the encoder and the decoder
    model.train()
    train_loss = []
    train_tester = train_waves.clone().detach()
    # shuffle the training dataset
    train_tester = train_tester[torch.randperm(train_tester.size()[0])]
    for wave in train_tester:
        wave = wave.to(device)
        output_thing = model(wave)
        loss = loss_fn(output_thing, wave)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        #   Print batch loss
        print('\t partial train loss (single batch): %f' % (loss.data))
        train_loss.append(loss.detach().cpu().numpy())

    return np.mean(train_loss)

# Testing function
def test_epoch(model, device, loss_fn):
    # Set evaluation mode for model
    model.eval()

    with torch.no_grad(): # No need to track the gradients
        # Define the lists to store the outputs for each batch
        conc_out = []
        conc_label = []
        for wave in test_waves:
            # Move tensor to the proper device
            wave = wave.to(device)
            # model data
            output_thing = model(wave)
            # Append the network output and the original image to the lists
            conc_out.append(output_thing.cpu())
            conc_label.append(wave.cpu())
        # Create a single tensor with all the values in the lists
        conc_out = torch.cat(conc_out)
        conc_label = torch.cat(conc_label)
        # Evaluate global loss
        val_loss = loss_fn(conc_out, conc_label)
    return val_loss.data

def plot_outputs(model):
    rand_num = random.randint(0, 4000)
    reconstructed = wave_torch_best[rand_num].to(device)
    reconstructed = model(reconstructed)
    new_numpy = reconstructed.detach().cpu().numpy()
    og = wave_torch_best[rand_num].detach().cpu().numpy()
    plt.plot(og[0, :])
    plt.plot(new_numpy[0, :])
    plt.savefig('/loggerhead/lwrigh89/Plots/Comparing Plots/reconstructed.png')

num_epochs = 4
diz_loss = {'train_loss':[],'val_loss':[]}
    for epoch in range(num_epochs):
        train_loss = train_epoch(model, device, loss_function_MSE, optimizer)
        val_loss = test_epoch(model, device, loss_function_MSE)
        print('\n EPOCH {}/{} \t train loss {} \t val loss {}'.format(epoch + 1, num_epochs, train_loss, val_loss))
        diz_loss['train_loss'].append(train_loss)
        diz_loss['val_loss'].append(val_loss)
        if epoch == num_epochs + 1:
            torch.save(model.state_dict(), '/loggerhead/lwrigh89/Model/newmodel.pt')
            # plot og vs reconstructed
            plot_outputs(model)
            plt.figure(figsize=(10, 8))
            plt.semilogy(diz_loss['train_loss'], label='Train')
            plt.semilogy(diz_loss['val_loss'], label='Valid')
            plt.xlabel('Epoch')
            plt.ylabel('Average Loss')
            plt.legend()
            plt.savefig('/loggerhead/lwrigh89/Plots/Epochs/epochgraph.png')
            # exit program
            sys.exit()
</code></pre>
<p>I'm using a GPU server with CUDA version 11.4, Python version 3.10.5, and PyTorch version 1.12.0.</p>
<p>I would appreciate any help/guidance given.</p>
",29,1,0,4,python;machine-learning;pytorch;autoencoder,2022-07-27 01:11:25,2022-07-27 01:11:25,2022-07-27 12:15:24,i m trying to create a convolutional autoencoder that will work with seismic waveforms  the problem i have is that my model doesn t seem to be learning anything from the data  and when i test the model by comparing one waveform to the same  reconstructed version of it  i get a straight line  it should be outputting a similar waveform to the original    blue is original  orange is reconstructed  in addition  the validation and test epochs don t change over time at all on my epoch graph  i m honestly not sure where to focus my debugging  but i suspect the problem lies with either the autoencoder itself  the way my training function is structured  or the loss function i m using  mse   i m also fairly new to machine learning  so i might be missing something obvious or doing something totally wrong  here is my code that relates to this problem  i m using a gpu server with cuda version    python version     and pytorch version     i would appreciate any help guidance given ,convolutional autoencoder won   t train data,trying create convolutional autoencoder work seismic waveforms problem model seem learning anything data test model comparing one waveform reconstructed version get straight line outputting similar waveform original blue original orange reconstructed addition validation test epochs change time epoch graph honestly sure focus debugging suspect problem lies either autoencoder way training function structured loss function using mse also fairly machine learning might missing something obvious something totally wrong code relates problem using gpu server cuda version python version pytorch version would appreciate help guidance given,convolutional autoencoder train data,convolutional autoencoder train datatrying create convolutional autoencoder work seismic waveforms problem model seem learning anything data test model comparing one waveform reconstructed version get straight line outputting similar waveform original blue original orange reconstructed addition validation test epochs change time epoch graph honestly sure focus debugging suspect problem lies either autoencoder way training function structured loss function using mse also fairly machine learning might missing something obvious something totally wrong code relates problem using gpu server cuda version python version pytorch version would appreciate help guidance given,"['convolutional', 'autoencoder', 'train', 'datatrying', 'create', 'convolutional', 'autoencoder', 'work', 'seismic', 'waveforms', 'problem', 'model', 'seem', 'learning', 'anything', 'data', 'test', 'model', 'comparing', 'one', 'waveform', 'reconstructed', 'version', 'get', 'straight', 'line', 'outputting', 'similar', 'waveform', 'original', 'blue', 'original', 'orange', 'reconstructed', 'addition', 'validation', 'test', 'epochs', 'change', 'time', 'epoch', 'graph', 'honestly', 'sure', 'focus', 'debugging', 'suspect', 'problem', 'lies', 'either', 'autoencoder', 'way', 'training', 'function', 'structured', 'loss', 'function', 'using', 'mse', 'also', 'fairly', 'machine', 'learning', 'might', 'missing', 'something', 'obvious', 'something', 'totally', 'wrong', 'code', 'relates', 'problem', 'using', 'gpu', 'server', 'cuda', 'version', 'python', 'version', 'pytorch', 'version', 'would', 'appreciate', 'help', 'guidance', 'given']","['convolut', 'autoencod', 'train', 'datatri', 'creat', 'convolut', 'autoencod', 'work', 'seismic', 'waveform', 'problem', 'model', 'seem', 'learn', 'anyth', 'data', 'test', 'model', 'compar', 'one', 'waveform', 'reconstruct', 'version', 'get', 'straight', 'line', 'output', 'similar', 'waveform', 'origin', 'blue', 'origin', 'orang', 'reconstruct', 'addit', 'valid', 'test', 'epoch', 'chang', 'time', 'epoch', 'graph', 'honestli', 'sure', 'focu', 'debug', 'suspect', 'problem', 'lie', 'either', 'autoencod', 'way', 'train', 'function', 'structur', 'loss', 'function', 'use', 'mse', 'also', 'fairli', 'machin', 'learn', 'might', 'miss', 'someth', 'obviou', 'someth', 'total', 'wrong', 'code', 'relat', 'problem', 'use', 'gpu', 'server', 'cuda', 'version', 'python', 'version', 'pytorch', 'version', 'would', 'appreci', 'help', 'guidanc', 'given']"
57,63,63,896802,35074209,How to copy/paste a dataframe from iPython into Google Sheets or Excel?,"<p>I've been using iPython (aka Jupyter) quite a bit lately for data analysis and some machine learning. But one big headache is copying results from the notebook app (browser) into either Excel or Google Sheets so I can manipulate results or share them with people who don't use iPython.</p>

<p>I know how to convert results to csv and save. But then I have to dig through my computer, open the results and paste them into Excel or Google Sheets. That takes too much time.</p>

<p>And just highlighting a resulting dataframe and copy/pasting usually completely messes up the formatting, with columns overflowing. (Not to mention the issue of long resulting dataframes being truncated when printed in iPython.)</p>

<p>How can I easily copy/paste an iPython result into a spreadsheet? </p>
",18212,7,20,5,python;excel;google-sheets;ipython;ipython-notebook,2016-01-29 01:33:19,2016-01-29 01:33:19,2022-07-27 12:11:30,i ve been using ipython  aka jupyter  quite a bit lately for data analysis and some machine learning  but one big headache is copying results from the notebook app  browser  into either excel or google sheets so i can manipulate results or share them with people who don t use ipython  i know how to convert results to csv and save  but then i have to dig through my computer  open the results and paste them into excel or google sheets  that takes too much time  and just highlighting a resulting dataframe and copy pasting usually completely messes up the formatting  with columns overflowing   not to mention the issue of long resulting dataframes being truncated when printed in ipython   how can i easily copy paste an ipython result into a spreadsheet  ,how to copy paste a dataframe from ipython into google sheets or excel ,using ipython aka jupyter quite bit lately data analysis machine learning one big headache copying results notebook app browser either excel google sheets manipulate results share people use ipython know convert results csv save dig computer open results paste excel google sheets takes much time highlighting resulting dataframe copy pasting usually completely messes formatting columns overflowing mention issue long resulting dataframes truncated printed ipython easily copy paste ipython result spreadsheet,copy paste dataframe ipython google sheets excel,copy paste dataframe ipython google sheets excelusing ipython aka jupyter quite bit lately data analysis machine learning one big headache copying results notebook app browser either excel google sheets manipulate results share people use ipython know convert results csv save dig computer open results paste excel google sheets takes much time highlighting resulting dataframe copy pasting usually completely messes formatting columns overflowing mention issue long resulting dataframes truncated printed ipython easily copy paste ipython result spreadsheet,"['copy', 'paste', 'dataframe', 'ipython', 'google', 'sheets', 'excelusing', 'ipython', 'aka', 'jupyter', 'quite', 'bit', 'lately', 'data', 'analysis', 'machine', 'learning', 'one', 'big', 'headache', 'copying', 'results', 'notebook', 'app', 'browser', 'either', 'excel', 'google', 'sheets', 'manipulate', 'results', 'share', 'people', 'use', 'ipython', 'know', 'convert', 'results', 'csv', 'save', 'dig', 'computer', 'open', 'results', 'paste', 'excel', 'google', 'sheets', 'takes', 'much', 'time', 'highlighting', 'resulting', 'dataframe', 'copy', 'pasting', 'usually', 'completely', 'messes', 'formatting', 'columns', 'overflowing', 'mention', 'issue', 'long', 'resulting', 'dataframes', 'truncated', 'printed', 'ipython', 'easily', 'copy', 'paste', 'ipython', 'result', 'spreadsheet']","['copi', 'past', 'datafram', 'ipython', 'googl', 'sheet', 'excelus', 'ipython', 'aka', 'jupyt', 'quit', 'bit', 'late', 'data', 'analysi', 'machin', 'learn', 'one', 'big', 'headach', 'copi', 'result', 'notebook', 'app', 'browser', 'either', 'excel', 'googl', 'sheet', 'manipul', 'result', 'share', 'peopl', 'use', 'ipython', 'know', 'convert', 'result', 'csv', 'save', 'dig', 'comput', 'open', 'result', 'past', 'excel', 'googl', 'sheet', 'take', 'much', 'time', 'highlight', 'result', 'datafram', 'copi', 'past', 'usual', 'complet', 'mess', 'format', 'column', 'overflow', 'mention', 'issu', 'long', 'result', 'datafram', 'truncat', 'print', 'ipython', 'easili', 'copi', 'past', 'ipython', 'result', 'spreadsheet']"
58,64,64,16244168,73134521,How to train on a tensorflow_datasets dataset,"<p>I'm playing around with tensorflow to become a bit more familiar with the overall workflow. To do this I thought I should start with creating a simple classifier for the well known Iris dataset.</p>
<p>I load the dataset using:</p>
<pre><code>ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
</code></pre>
<p>I use the following classifier:</p>
<pre class=""lang-py prettyprint-override""><code>model = keras.Sequential([
    keras.layers.Dense(10,activation=&quot;relu&quot;),
    keras.layers.Dense(10,activation=&quot;relu&quot;),
    keras.layers.Dense(3, activation=&quot;softmax&quot;)
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)
</code></pre>
<p>I then try to fit the model using:</p>
<pre class=""lang-py prettyprint-override""><code>model.fit(ds,batch_size=50, epochs=100)
</code></pre>
<p>This gives the following error:</p>
<pre><code>Input 0 of layer &quot;dense&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)

    Call arguments received by layer &quot;sequential&quot; (type Sequential):
       inputs=tf.Tensor(shape=(4,), dtype=float32)
       training=True
       mask=None
</code></pre>
<p>I also tried defining the model using the functional API(as this was my orignal goal to learn)</p>
<pre class=""lang-py prettyprint-override""><code>inputs = keras.Input(shape=(4,), name='features')

first_hidden = keras.layers.Dense(10, activation='relu')(inputs)
second_hidden = keras.layers.Dense(10, activation=&quot;relu&quot;)(first_hidden)

outputs = keras.layers.Dense(3, activation='softmax')(second_hidden)

model = keras.Model(inputs=inputs, outputs=outputs, name=&quot;test_iris_classification&quot;)
</code></pre>
<p>I now get the same error as before but this time with a warning:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='features'), name='features', description=&quot;created by layer 'features'&quot;), but it was called on an input with incompatible shape (4,).
</code></pre>
<p>I suspect this is something quite fundamental that haven't understood but I have not been able to figure it out, despite several hours of googling.</p>
<p>PS:
I also tried to download the whole dataset from the <a href=""https://archive.ics.uci.edu/ml/datasets/iris"" rel=""noreferrer"">UCI Machine Learning Repository</a> as a CSV file.</p>
<p>I read it in like this:</p>
<pre><code>ds = pd.read_csv(&quot;iris.data&quot;, header=None)
labels = []
for name in ds[4]:
    if name == &quot;Iris-setosa&quot;:
        labels.append(0)
    elif name == &quot;Iris-versicolor&quot;:
        labels.append(1)
    elif name == &quot;Iris-virginica&quot;:
        labels.append(2)
    else:
        raise ValueError(f&quot;Name wrong name: {name}&quot;)
labels = np.array(labels)
features = np.array(ds[[0,1,2,3]])
</code></pre>
<p>And fit it like this:</p>
<pre><code>model.fit(features, labels,batch_size=50, epochs=100)
</code></pre>
<p>And I'm able to fit the model to this dataset without any problems for both the sequential and the functional API. Which makes me suspect my misunderstanding has something to do with how the tensorflow_datasets works.</p>
",56,1,7,5,python;tensorflow;machine-learning;keras;tensorflow-datasets,2022-07-27 11:23:24,2022-07-27 11:23:24,2022-07-27 11:39:30,i m playing around with tensorflow to become a bit more familiar with the overall workflow  to do this i thought i should start with creating a simple classifier for the well known iris dataset  i load the dataset using  i use the following classifier  i then try to fit the model using  this gives the following error  i also tried defining the model using the functional api as this was my orignal goal to learn  i now get the same error as before but this time with a warning  i suspect this is something quite fundamental that haven t understood but i have not been able to figure it out  despite several hours of googling  i read it in like this  and fit it like this  and i m able to fit the model to this dataset without any problems for both the sequential and the functional api  which makes me suspect my misunderstanding has something to do with how the tensorflow_datasets works ,how to train on a tensorflow_datasets dataset,playing around tensorflow become bit familiar overall workflow thought start creating simple classifier well known iris dataset load dataset using use following classifier try fit model using gives following error also tried defining model using functional api orignal goal learn get error time warning suspect something quite fundamental understood able figure despite several hours googling read like fit like able fit model dataset without problems sequential functional api makes suspect misunderstanding something tensorflow_datasets works,train tensorflow_datasets dataset,train tensorflow_datasets datasetplaying around tensorflow become bit familiar overall workflow thought start creating simple classifier well known iris dataset load dataset using use following classifier try fit model using gives following error also tried defining model using functional api orignal goal learn get error time warning suspect something quite fundamental understood able figure despite several hours googling read like fit like able fit model dataset without problems sequential functional api makes suspect misunderstanding something tensorflow_datasets works,"['train', 'tensorflow_datasets', 'datasetplaying', 'around', 'tensorflow', 'become', 'bit', 'familiar', 'overall', 'workflow', 'thought', 'start', 'creating', 'simple', 'classifier', 'well', 'known', 'iris', 'dataset', 'load', 'dataset', 'using', 'use', 'following', 'classifier', 'try', 'fit', 'model', 'using', 'gives', 'following', 'error', 'also', 'tried', 'defining', 'model', 'using', 'functional', 'api', 'orignal', 'goal', 'learn', 'get', 'error', 'time', 'warning', 'suspect', 'something', 'quite', 'fundamental', 'understood', 'able', 'figure', 'despite', 'several', 'hours', 'googling', 'read', 'like', 'fit', 'like', 'able', 'fit', 'model', 'dataset', 'without', 'problems', 'sequential', 'functional', 'api', 'makes', 'suspect', 'misunderstanding', 'something', 'tensorflow_datasets', 'works']","['train', 'tensorflow_dataset', 'datasetplay', 'around', 'tensorflow', 'becom', 'bit', 'familiar', 'overal', 'workflow', 'thought', 'start', 'creat', 'simpl', 'classifi', 'well', 'known', 'iri', 'dataset', 'load', 'dataset', 'use', 'use', 'follow', 'classifi', 'tri', 'fit', 'model', 'use', 'give', 'follow', 'error', 'also', 'tri', 'defin', 'model', 'use', 'function', 'api', 'orign', 'goal', 'learn', 'get', 'error', 'time', 'warn', 'suspect', 'someth', 'quit', 'fundament', 'understood', 'abl', 'figur', 'despit', 'sever', 'hour', 'googl', 'read', 'like', 'fit', 'like', 'abl', 'fit', 'model', 'dataset', 'without', 'problem', 'sequenti', 'function', 'api', 'make', 'suspect', 'misunderstand', 'someth', 'tensorflow_dataset', 'work']"
59,65,65,14993973,73132096,How to Speed Up For-Loop with GPU using User-Defined Functions and Lambda Functions,"<p>I am trying to implement a machine learning algorithm in Python from scratch. There is a part of the algorithm where I have to evaluate the error of the function over the entire dataset (~28,000 input vectors) of which I was using a basic <em>for-loop</em> as a part of list comprehension to evaluate the classifier and compute how many it gets right. It looks like the following:</p>
<pre><code>def computeClassifer(parameters):
    alpha, d = computeAlphaD(parameters)
    return lambda X: sign(sum([alpha[i] * y_train[i] * computeKernel(d, kernels, X_train[i], X) for i in range(len(alpha))]))
def computeError(clf, X_train, y_train):
    return 1 - np.sum([clf(X_train[i]) == y_train[i] for i in range(len(X_train))/len(y_train)
</code></pre>
<p>right now the biggest problem I am having is runtime, as it takes an obscenely long time to compute the error. I have attempted to parallelize <code>computeError</code> function using <strong>Ray</strong> but it still runs slow. I also tried using <strong>numba</strong> but unfortunately I kept running into errors since my <code>computeClassifer</code> function returns a lambda function.</p>
<p>Is there a more efficient way to do this, so that the runtime is not super long, preferably using GPU?</p>
<p>Let me add that <code>kernels</code> is a list of lambda functions that take in an array of feature vectors and spits out a corresponding kernel matrix.</p>
",50,0,0,5,python;for-loop;machine-learning;parallel-processing;gpu,2022-07-27 06:56:14,2022-07-27 06:56:14,2022-07-27 10:44:42,i am trying to implement a machine learning algorithm in python from scratch  there is a part of the algorithm where i have to evaluate the error of the function over the entire dataset     input vectors  of which i was using a basic for loop as a part of list comprehension to evaluate the classifier and compute how many it gets right  it looks like the following  right now the biggest problem i am having is runtime  as it takes an obscenely long time to compute the error  i have attempted to parallelize computeerror function using ray but it still runs slow  i also tried using numba but unfortunately i kept running into errors since my computeclassifer function returns a lambda function  is there a more efficient way to do this  so that the runtime is not super long  preferably using gpu  let me add that kernels is a list of lambda functions that take in an array of feature vectors and spits out a corresponding kernel matrix ,how to speed up for loop with gpu using user defined functions and lambda functions,trying implement machine learning algorithm python scratch part algorithm evaluate error function entire dataset input vectors using basic loop part comprehension evaluate classifier compute many gets right looks like following right biggest problem runtime takes obscenely long time compute error attempted parallelize computeerror function using ray still runs slow also tried using numba unfortunately kept running errors since computeclassifer function returns lambda function efficient way runtime super long preferably using gpu let kernels lambda functions take array feature vectors spits corresponding kernel matrix,speed loop gpu using user defined functions lambda functions,speed loop gpu using user defined functions lambda functionstrying implement machine learning algorithm python scratch part algorithm evaluate error function entire dataset input vectors using basic loop part comprehension evaluate classifier compute many gets right looks like following right biggest problem runtime takes obscenely long time compute error attempted parallelize computeerror function using ray still runs slow also tried using numba unfortunately kept running errors since computeclassifer function returns lambda function efficient way runtime super long preferably using gpu let kernels lambda functions take array feature vectors spits corresponding kernel matrix,"['speed', 'loop', 'gpu', 'using', 'user', 'defined', 'functions', 'lambda', 'functionstrying', 'implement', 'machine', 'learning', 'algorithm', 'python', 'scratch', 'part', 'algorithm', 'evaluate', 'error', 'function', 'entire', 'dataset', 'input', 'vectors', 'using', 'basic', 'loop', 'part', 'comprehension', 'evaluate', 'classifier', 'compute', 'many', 'gets', 'right', 'looks', 'like', 'following', 'right', 'biggest', 'problem', 'runtime', 'takes', 'obscenely', 'long', 'time', 'compute', 'error', 'attempted', 'parallelize', 'computeerror', 'function', 'using', 'ray', 'still', 'runs', 'slow', 'also', 'tried', 'using', 'numba', 'unfortunately', 'kept', 'running', 'errors', 'since', 'computeclassifer', 'function', 'returns', 'lambda', 'function', 'efficient', 'way', 'runtime', 'super', 'long', 'preferably', 'using', 'gpu', 'let', 'kernels', 'lambda', 'functions', 'take', 'array', 'feature', 'vectors', 'spits', 'corresponding', 'kernel', 'matrix']","['speed', 'loop', 'gpu', 'use', 'user', 'defin', 'function', 'lambda', 'functionstri', 'implement', 'machin', 'learn', 'algorithm', 'python', 'scratch', 'part', 'algorithm', 'evalu', 'error', 'function', 'entir', 'dataset', 'input', 'vector', 'use', 'basic', 'loop', 'part', 'comprehens', 'evalu', 'classifi', 'comput', 'mani', 'get', 'right', 'look', 'like', 'follow', 'right', 'biggest', 'problem', 'runtim', 'take', 'obscen', 'long', 'time', 'comput', 'error', 'attempt', 'parallel', 'computeerror', 'function', 'use', 'ray', 'still', 'run', 'slow', 'also', 'tri', 'use', 'numba', 'unfortun', 'kept', 'run', 'error', 'sinc', 'computeclassif', 'function', 'return', 'lambda', 'function', 'effici', 'way', 'runtim', 'super', 'long', 'prefer', 'use', 'gpu', 'let', 'kernel', 'lambda', 'function', 'take', 'array', 'featur', 'vector', 'spit', 'correspond', 'kernel', 'matrix']"
60,66,66,7839997,73126888,Siamese network accuracy stuck after adding more training data,"<p>I am new to machine learning and I am currently trying to create a siamese network that can predict the similarity of brand logos.
I have a dataset with ~210.000 brand logos.
The CNN for the siamese network looks like the following:</p>
<pre><code>def build_cnn(inputShape, embeddingDim=48):
# specify the inputs for the feature extractor network
inputs = Input(shape=inputShape)
# define the first set of CONV =&gt; RELU =&gt; POOL =&gt; DROPOUT layers
x = Conv2D(64, (2, 2), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inputs)
x = MaxPooling2D(pool_size=(5, 5))(x)
x = Dropout(0.3)(x)

# second set of CONV =&gt; RELU =&gt; POOL =&gt; DROPOUT layers
x = Conv2D(64, (2, 2), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = MaxPooling2D(pool_size=2)(x)
x = Dropout(0.3)(x)

pooledOutput = GlobalAveragePooling2D()(x)
outputs = Dense(embeddingDim)(pooledOutput)
# build the model
model = Model(inputs, outputs)
model.summary()
plot_model(model, to_file=os.path.sep.join([config.BASE_OUTPUT,'model_cnn.png']))
# return the model to the calling function
return model
</code></pre>
<p><a href=""https://i.stack.imgur.com/ecu6r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ecu6r.png"" alt=""CNN"" /></a></p>
<p>The siamese network looks like this (the model here is the cnn described above):</p>
<pre><code>imgA = Input(shape=config.IMG_SHAPE)
imgB = Input(shape=config.IMG_SHAPE)
featureExtractor = siamese_network.build_cnn(config.IMG_SHAPE)
featsA = featureExtractor(imgA)
featsB = featureExtractor(imgB)
distance = Lambda(euclidean_distance)([featsA, featsB])
outputs = Dense(1, activation=&quot;sigmoid&quot;)(distance)
model = Model(inputs=[imgA, imgB], outputs=outputs)
</code></pre>
<p><a href=""https://i.stack.imgur.com/aHqBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aHqBD.png"" alt=""Siamese Network"" /></a></p>
<p>My first test was with 800 positive and 800 negative pairs and the accuracy and loss looks like this:
<a href=""https://i.stack.imgur.com/vjQo0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjQo0.png"" alt=""Accuracy and Training"" /></a></p>
<p>My thoughts to this were that there is some overfitting happening and my approach was to create more training data (2000 positive and negative pairs) and train the model again, but unfortunately the model was not improving at all after, even after 20+ epochs.</p>
<p><a href=""https://i.stack.imgur.com/JBIAh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JBIAh.png"" alt=""Training with more data"" /></a></p>
<p>For both cases I used the following to train my network:</p>
<pre><code>model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])
print(&quot;[INFO] training model...&quot;)
history = model.fit(
    [pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],
    validation_data=([pairTest[:, 0], pairTest[:, 1]], labelTest[:]),
    batch_size=10,
    shuffle=True,
    epochs=50)
</code></pre>
<p>I cannot figure out what is happening here, so I am really gratefull for every help.
My question here is why is the siamese network learning (or at least it looks like it is learning) with less training data, but as soon as I add more the accuracy is constant and not improving at all?</p>
<p><strong>EDIT</strong>
According to Albertos comment I tried it with selu (still not working):</p>
<p><a href=""https://i.stack.imgur.com/4Mqnt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Mqnt.png"" alt=""enter image description here"" /></a></p>
<p><strong>EDIT2</strong>
With LeakyReLU it looks like this:</p>
<p><a href=""https://i.stack.imgur.com/bThQD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bThQD.png"" alt=""enter image description here"" /></a></p>
",43,1,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-07-26 19:28:48,2022-07-26 19:28:48,2022-07-27 08:47:48, the siamese network looks like this  the model here is the cnn described above    my thoughts to this were that there is some overfitting happening and my approach was to create more training data   positive and negative pairs  and train the model again  but unfortunately the model was not improving at all after  even after   epochs   for both cases i used the following to train my network   ,siamese network accuracy stuck after adding more training data,siamese network looks like model cnn described thoughts overfitting happening approach create training data positive negative pairs train model unfortunately model improving even epochs cases used following train network,siamese network accuracy stuck adding training data,siamese network accuracy stuck adding training datasiamese network looks like model cnn described thoughts overfitting happening approach create training data positive negative pairs train model unfortunately model improving even epochs cases used following train network,"['siamese', 'network', 'accuracy', 'stuck', 'adding', 'training', 'datasiamese', 'network', 'looks', 'like', 'model', 'cnn', 'described', 'thoughts', 'overfitting', 'happening', 'approach', 'create', 'training', 'data', 'positive', 'negative', 'pairs', 'train', 'model', 'unfortunately', 'model', 'improving', 'even', 'epochs', 'cases', 'used', 'following', 'train', 'network']","['siames', 'network', 'accuraci', 'stuck', 'ad', 'train', 'datasiames', 'network', 'look', 'like', 'model', 'cnn', 'describ', 'thought', 'overfit', 'happen', 'approach', 'creat', 'train', 'data', 'posit', 'neg', 'pair', 'train', 'model', 'unfortun', 'model', 'improv', 'even', 'epoch', 'case', 'use', 'follow', 'train', 'network']"
61,67,67,19585996,73049458,How to extract and sum numbers from a text file,"<p>I need to extract and sum all the numbers from a file.</p>
<p>How can <code>lst.append</code> be used correctly to make a list of numbers?</p>
<pre><code>name = input(&quot;Enter file:&quot;)
if len(name) &lt; 1:
    name = &quot;regex_sum_1603392.txt&quot;
handle = open(name)

total = 0
lst = list()

import re

for line in handle:
    y = re.findall('[0-9]+', line)
    lst.append(y)

for linenew in lst:
    if ' ' in linenew:
        continue
    print(linenew)
</code></pre>
<pre><code># current output
[]
[]
[]
['2993', '5874']
[]
[]
[]
[]
[]
['58', '4266', '6417']
[]
[]
[]
...
[]
[]
[]
[]
[]
['42']
[]
</code></pre>
<pre><code># expected output
[2993, 5874, 58, 4266, 6417, ..., 42]
</code></pre>
<h3>regex_sum_1603392.txt</h3>
<pre class=""lang-none prettyprint-override""><code>2993 Why should you learn to write programs? 5874

Writing programs (or programming) is a very creative 
and rewarding activity.  You can write programs for 
many reasons, ranging from making your living to solving
a difficult data analysis problem to having fun to helping
someone 58 else 4266 solve 6417 a problem.  This book assumes that 
 everyone needs to know how to program, and that once 
you know how to program you will figure out what you want 
to do with your newfound skills.  

We are surrounded in our daily lives with computers ranging 
from laptops to cell phones.  We can think of these computers
as our personal assistants who can take care of many things
on our behalf.  The hardware in our current-day computers 
is essentially built to continuously ask us the question, 
What would you like me to do next?

Programmers add an operating system and a set of applications
to the hardware and we end up with a Personal Digital
Assistant that is quite helpful and capable of helping
us do many different things.

Our computers are fast and have vast amounts of memory and 
could be very helpful to us if we only knew the language to
speak to explain to the computer what we would like it to 
do next.  If we knew this language, we could tell the 
computer to do tasks on our behalf that were repetitive.  
Interestingly, 1976 the 9565 kinds 7652 of things computers can do best
 are often the kinds of things that we humans find boring
and mind-numbing.

For example, look at the first three paragraphs of this
chapter 6112 and 6957 tell 9749 me the most commonly used word and how
 many times the word is used.  While you were able to read
and understand the words in a few seconds, counting them
is almost painful because it is not the kind of problem 
that human minds are designed to solve.  For a computer
the opposite is true, reading and understanding text 
from a piece of paper is hard for a computer to do 
but counting the words and telling you how many times
the most used word was used is very easy for the
computer:

Our personal information analysis assistant quickly 
told us that the word to was used sixteen times in the
first three paragraphs of this chapter.

This very fact that computers are good at things 
that humans are not is why you need to become
skilled at talking computer language.  Once you learn
this 8494 new 7669 language, 395 you can delegate mundane tasks
 to your partner (the computer), leaving more time 
for you to do the 
things that you are uniquely suited for.  You bring 
creativity, intuition, and inventiveness to this
partnership.  

Creativity and motivation

While this book is not intended for professional programmers, professional
programming can be a very rewarding job both financially and personally.
Building useful, elegant, and clever programs for others to use is a very
creative activity.  Your computer or Personal Digital Assistant (PDA) 
usually contains many different programs from many different groups of 
programmers, each competing for your attention and interest.  They try 
their best to meet your needs and give you a great user experience in the
process.   In some situations, when you choose a piece of software, the 
programmers are directly compensated because of your choice.

6940 If we think of programs as the creative output of groups of programmers,
perhaps the following figure is a more sensible version of our PDA:

For now, our primary motivation is not to make money or please end users, but
instead for us to be more productive in handling the data and 
information that we will encounter in our lives.
When 4604 you 1069 first 540 start, you will be both the programmer and the end user of
 your programs.  As you gain skill as a programmer and
programming feels more creative to you, your thoughts may turn
toward developing programs for others.

Computer hardware architecture
9046 2173 158
Before we start learning the language we 
1330 speak to give instructions to computers to 5644
develop software, we need to learn a small amount about 
how computers are built.  

Central Processing Unit (or CPU) is 
the part of the computer that is built to be obsessed 
with what is next?  If your computer is rated
at three Gigahertz, it means that the CPU will ask What next?
three billion times per second.  You are going to have to 
learn how to talk fast to keep up with the CPU.

Main Memory is used to store information
that the CPU needs in a hurry.  The main memory is nearly as 
fast as the CPU.  But the information stored in the main
memory vanishes when the computer is turned off.

Secondary Memory is also used to store
information, but it is much slower than the main memory.
The advantage of the secondary memory is that it can
store information even when there is no power to the
computer.  Examples of secondary memory are disk drives
or flash memory (typically found in USB sticks and portable
9528 4312 1128
633  9310
5216 Input and Output Devices are simply our
screen, keyboard, mouse, microphone, speaker, touchpad, etc.  
They are all of the ways we interact with the computer.

These days, most computers also have a
7768 Network Connection to retrieve information over a network.
We can think of the network as a very slow place to store and
retrieve data that might not always be up.  So in a sense,
the network is a slower and at times unreliable form of
5888 Secondary Memory.

While most of the detail of how these components work is best left 
to computer builders, it helps to have some terminology
so we can talk about these different parts as we write our programs.

As a programmer, your job is to use and orchestrate 
each of these resources to solve the problem that you need to solve
and analyze the data you get from the solution.  As a programmer you will 
mostly be talking to the CPU and telling it what to 
do next.  Sometimes you will tell the CPU to use the main memory,
secondary memory, network, or the input/output devices.
2613 381 9203
You need to be the person who answers the CPU's What next? 
question.  But it would be very uncomfortable to shrink you 
down to five mm  tall and insert you into the computer just so you 
could issue a command three billion times per second.  So instead,
you must write down your instructions in advance.
We call these stored instructions a program and the act 
of writing these instructions down and getting the instructions to 
be correct programming.

Understanding programming

In the rest of this book, we will try to turn you into a person
who is skilled in the art of programming.  In the end you will be a 
3175 programmer --- perhaps not a professional programmer, but 
at least you will have the skills to look at a data/information
analysis problem and develop a program to solve the problem.
2466 9374 9787
problem solving

In a sense, you need two skills to be a programmer:

5868 First, you need to know the programming language (Python) -
you need to know the vocabulary and the grammar.  You need to be able 
to spell the words in this new language properly and know how to construct 
well-formed sentences in this new language.

Second, you need to tell a story.  In writing a story,
you combine words and sentences to convey an idea to the reader. 
There is a skill and art in constructing the story, and skill in
story writing is improved by doing some writing and getting some
feedback.  In programming, our program is the story and the 
problem you are trying to solve is the idea.

itemize

Once you learn one programming language such as Python, you will 
find it much easier to learn a second programming language such
as JavaScript or C++.  The new programming language has very different 
5100 vocabulary and grammar but the problem-solving skills 9484
will be the same across all programming languages.

6756 You will learn the vocabulary and sentences of Python pretty quickly.
It will take longer for you to be able to write a coherent program
to solve a brand-new problem.  We teach programming much like we teach
writing. 3848  4527 We 8718 start reading and explaining programs, then we write 
 simple programs, and then we write increasingly complex programs over time.
At some point you get your muse and see the patterns on your own
and can see more naturally how to take a problem and 
write a program that solves that problem.  And once you get 
to that point, programming becomes a very pleasant and creative process.  

We start with the vocabulary and structure of Python programs.  Be patient
as the simple examples remind you of when you started reading for the first
time. 

Words and sentences

Unlike human languages, the Python vocabulary is actually pretty small.
We 3326 call 6471 this 6474 vocabulary the reserved words.  These are words that
 have very special meaning to Python.  When Python sees these words in 
a Python program, they have one and only one meaning to Python.  Later
as you write programs you will make up your own words that have meaning to 
you called variables.   You will have great latitude in choosing
your names for your variables, but you cannot use any of Python's 
reserved words as a name for a variable.
4048  4312
6832 When we train a dog, we use special words like
sit, stay, and fetch.  When you talk to a dog and
don't use any of the reserved words, they just look at you with a 
quizzical look on their face until you say a reserved word.  
For example, if you say, 
I wish more people would walk to improve their overall health, 
38 what most dogs likely hear is,
blah blah blah walk blah blah blah blah.
That is because walk is a reserved word in dog language.  

The reserved words in the language where humans talk to 
Python include the following:

and       del       from      not       while    
as        elif      global    or        with     
assert    else      if        pass      yield    
break     except    import    print              
class     exec      in        raise              
continue  finally   is        return             
def       for       lambda    try

That is it, and unlike a dog, Python is already completely trained.
When you say try, Python will try every time you say it without
fail.

We will learn these reserved words and how they are used in good time,
but for now we will focus on the Python equivalent of speak (in 
human-to-dog language).  The nice thing about telling Python to speak
is that we can even tell it what to say by giving it a message in quotes:

4985 And we have even written our first syntactically correct Python sentence.
Our sentence starts with the reserved word print followed
by a string of text of our choosing enclosed in single quotes.

Conversing with Python

Now that we have a word and a simple sentence that we know in Python,
we need to know how to start a conversation with Python to test 
our new language skills.

6012 Before you can converse with Python, you must first install the Python 8762
software on your computer and learn how to start Python on your 
computer.  That is too much detail for this chapter so I suggest
that you consult www.py4e.com where I have detailed
instructions and screencasts of setting up and starting Python 
on Macintosh and Windows systems.  At some point, you will be in 
a terminal or command window and you will type python and 
725 the Python interpreter will start executing in interactive mode
and appear somewhat as follows:
interactive mode

The &gt;&gt;&gt; prompt is the Python interpreter's way of asking you, What
do you want me to do next?  Python is ready to have a conversation with
you.  All you have to know is how to speak the Python language.

Let's say for example that you did not know even the simplest Python language
5359 words or sentences. You might want to use the standard line that astronauts 789
use when they land on a faraway planet and try to speak with the inhabitants
of the planet:

This is not going so well.  Unless you think of something quickly,
the inhabitants of the planet are likely to stab you with their spears, 
put you on a spit, roast you over a fire, and eat you for dinner.

At this point, you should also realize that while Python 
is amazingly complex and powerful and very picky about 
the syntax you use to communicate with it, Python is 
not intelligent.  You are really just having a conversation
with yourself, but using proper syntax.

In a sense, when you use a program written by someone else
the conversation is between you and those other
programmers with Python acting as an intermediary.  Python
is a way for the creators of programs to express how the 
conversation is supposed to proceed.  And
in just a few more chapters, you will be one of those
programmers using Python to talk to the users of your program.

832 Before we leave our first conversation with the Python 5931
interpreter, you should probably know the proper way
to 8584 say 3859 good-bye 6794 when interacting with the inhabitants
 of Planet Python:

You will notice that the error is different for the first two
incorrect attempts.   The second error is different because 
if is a reserved word and Python saw the reserved word
and 2490 thought 1925 we 3795 were trying to say something but got the syntax
 of the sentence wrong.
7  4068
Terminology: interpreter and compiler

Python is a high-level language intended to be relatively
straightforward for humans to read and write and for computers
to read and process.  Other high-level languages include Java, C++,
PHP, Ruby, Basic, Perl, JavaScript, and many more.  The actual hardware
inside the Central Processing Unit (CPU) does not understand any
of these high-level languages.

The CPU understands a language we call machine language.  Machine
language is very simple and frankly very tiresome to write because it 
is represented all in zeros and ones.

Machine language seems quite simple on the surface, given that there 
are only zeros and ones, but its syntax is even more complex
and far more intricate than Python.  So very few programmers ever write
machine language.  Instead we build various translators to allow
programmers 4204 to 3095 write 9829 in high-level languages like Python or JavaScript
 and these translators convert the programs to machine language for actual
execution by the CPU.

Since machine language is tied to the computer hardware, machine language
is not portable across different types of hardware.  Programs written in 
high-level languages can be moved between different computers by using a 
different interpreter on the new machine or recompiling the code to create
a machine language version of the program for the new machine.

These programming language translators fall into two general categories:
(one) interpreters and (two) compilers.

An interpreter reads the source code of the program as written by the
programmer, parses the source code, and interprets the instructions on the fly.
Python is an interpreter and when we are running Python interactively, 
we can type a line of Python (a sentence) and Python processes it immediately
and is ready for us to type another line of Python.   

Some of the lines of Python tell Python that you want it to remember some 
value for later.   We need to pick a name for that value to be remembered and
8524 we can use that symbolic name to retrieve the value later.  We use the 4897
term variable to refer to the labels we use to refer to this stored data.

In this example, we ask Python to remember the value six and use the label x
so we can retrieve the value later.   We verify that Python has actually remembered
2443 the value using x and multiply
5923 it by seven and put the newly computed value in y.  Then we ask Python to print out 2213
the value currently in y.

Even though we are typing these commands into Python one line at a time, Python
is treating them as an ordered sequence of statements with later statements able
to retrieve data created in earlier statements.   We are writing our first 
simple paragraph with four sentences in a logical and meaningful order.

It is the nature of an interpreter to be able to have an interactive conversation
as shown above.  A compiler needs to be handed the entire program in a file, and then 
it runs a process to translate the high-level source code into machine language
and then the compiler puts the resulting machine language into a file for later
execution.

If you have a Windows system, often these executable machine language programs have a
suffix of .exe or .dll which stand for executable and dynamic link
library respectively.  In Linux and Macintosh, there is no suffix that uniquely marks
a file as executable.

If you were to open an executable file in a text editor, it would look 
completely crazy and be unreadable:

It is not easy to read or write machine language, so it is nice that we have
compilers that allow us to write in high-level
languages like Python or C.

Now at this point in our discussion of compilers and interpreters, you should 
be wondering a bit about the Python interpreter itself.  What language is 
it written in?  Is it written in a compiled language?  When we type
python, what exactly is happening?

The Python interpreter is written in a high-level language called C.  
You can look at the actual source code for the Python interpreter by
going to www.python.org and working your way to their source code.
So Python is a program itself and it is compiled into machine code.
When you installed Python on your computer (or the vendor installed it),
you copied a machine-code copy of the translated Python program onto your
2127 system.   In Windows, the executable machine code for Python itself is likely 6790
in a file.

That is more than you really need to know to be a Python programmer, but
sometimes it pays to answer those little nagging questions right at 
the beginning.

Writing a program

Typing commands into the Python interpreter is a great way to experiment 
with Python's features, but it is not recommended for solving more complex problems.

When we want to write a program, 
we use a text editor to write the Python instructions into a file,
which is called a script.  By
convention, Python scripts have names that end with .py.

script
2449 8830 5326
To execute the script, you have to tell the Python interpreter 
the name of the file.  In a Unix or Windows command window, 
you would type python hello.py as follows:

We call the Python interpreter and tell it to read its source code from
the file hello.py instead of prompting us for lines of Python code
574 7924 1550

You will notice that there was no need to have quit() at the end of
the Python program in the file.   When Python is reading your source code
from a file, it knows to stop when it reaches the end of the file.

What is a program?

The definition of a program at its most basic is a sequence
of Python statements that have been crafted to do something.
Even our simple hello.py script is a program.  It is a one-line
program and is not particularly useful, but in the strictest definition,
it is a Python program.

It might be easiest to understand what a program is by thinking about a problem 
that a program might be built to solve, and then looking at a program
that would solve that problem.

Lets say you are doing Social Computing research on Facebook posts and 
you are interested in the most frequently used word in a series of posts.
You could print out the stream of Facebook posts and pore over the text
looking for the most common word, but that would take a long time and be very 
mistake prone.  You would be smart to write a Python program to handle the
task quickly and accurately so you can spend the weekend doing something 
fun.

For example, look at the following text about a clown and a car.  Look at the 
text and figure out the most common word and how many times it occurs.

Then imagine that you are doing this task looking at millions of lines of 
text.  Frankly it would be quicker for you to learn Python and write a 
Python program to count the words than it would be to manually 
scan the words.

The even better news is that I already came up with a simple program to 
find the most common word in a text file.  I wrote it,
tested it, and now I am giving it to you to use so you can save some time.

You don't even need to know Python to use this program.  You will need to get through 
Chapter ten of this book to fully understand the awesome Python techniques that were
used to make the program.  You are the end user, you simply use the program and marvel
at its cleverness and how it saved you so much manual effort.
You simply type the code 
into a file called words.py and run it or you download the source 
code from http://www.py4e.com/code3/ and run it.

This is a good example of how Python and the Python language are acting as an intermediary
between you (the end user) and me (the programmer).  Python is a way for us to exchange useful
instruction sequences (i.e., programs) in a common language that can be used by anyone who 
installs Python on their computer.  So neither of us are talking to Python,
instead we are communicating with each other through Python.

The building blocks of programs

In the next few chapters, we will learn more about the vocabulary, sentence structure,
paragraph structure, and story structure of Python.  We will learn about the powerful
capabilities of Python and how to compose those capabilities together to create useful
programs.

There are some low-level conceptual patterns that we use to construct programs.  These
constructs are not just for Python programs, they are part of every programming language
from machine language up to the high-level languages.

description

Get data from the outside world.  This might be 
reading data from a file, or even some kind of sensor like 
a microphone or GPS.  In our initial programs, our input will come from the user
typing data on the keyboard.

Display the results of the program on a screen
or store them in a file or perhaps write them to a device like a
speaker to play music or speak text.

Perform statements one after
another in the order they are encountered in the script.

Check for certain conditions and
then execute or skip a sequence of statements.

Perform some set of statements 
repeatedly, usually with
some variation.

Write a set of instructions once and give them a name
and then reuse those instructions as needed throughout your program.

description

It sounds almost too simple to be true, and of course it is never
so simple.  It is like saying that walking is simply
putting one foot in front of the other.  The art 
of writing a program is composing and weaving these
basic elements together many times over to produce something
that is useful to its users.

The word counting program above directly uses all of 
these patterns except for one.

What could possibly go wrong?

As we saw in our earliest conversations with Python, we must
communicate very precisely when we write Python code.  The smallest
deviation or mistake will cause Python to give up looking at your
program.

Beginning programmers often take the fact that Python leaves no
room for errors as evidence that Python is mean, hateful, and cruel.
While Python seems to like everyone else, Python knows them 
personally and holds a grudge against them.  Because of this grudge,
Python takes our perfectly written programs and rejects them as 
unfit just to torment us.

There is little to be gained by arguing with Python.  It is just a tool.
It has no emotions and it is happy and ready to serve you whenever you
need it.  Its error messages sound harsh, but they are just Python's
call for help.  It has looked at what you typed, and it simply cannot
understand what you have entered.

Python is much more like a dog, loving you unconditionally, having a few
key words that it understands, looking you with a sweet look on its
face (&gt;&gt;&gt;), and waiting for you to say something it understands.
When Python says SyntaxError: invalid syntax, it is simply wagging
its tail and saying, You seemed to say something but I just don't
understand what you meant, but please keep talking to me (&gt;&gt;&gt;).

As your programs become increasingly sophisticated, you will encounter three 
general types of errors:

description

These are the first errors you will make and the easiest
to fix.  A syntax error means that you have violated the grammar rules of Python.
Python does its best to point right at the line and character where 
it noticed it was confused.  The only tricky bit of syntax errors is that sometimes
the mistake that needs fixing is actually earlier in the program than where Python
noticed it was confused.  So the line and character that Python indicates in 
a syntax error may just be a starting point for your investigation.

A logic error is when your program has good syntax but there is a mistake 
in the order of the statements or perhaps a mistake in how the statements relate to one another.
A good example of a logic error might be, take a drink from your water bottle, put it 
in your backpack, walk to the library, and then put the top back on the bottle.

A semantic error is when your description of the steps to take 
is syntactically perfect and in the right order, but there is simply a mistake in 
the program.  The program is perfectly correct but it does not do what
you intended for it to do. A simple example would
be if you were giving a person directions to a restaurant and said, ...when you reach
the intersection with the gas station, turn left and go one mile and the restaurant
is a red building on your left.  Your friend is very late and calls you to tell you that
they are on a farm and walking around behind a barn, with no sign of a restaurant.  
Then you say did you turn left or right at the gas station? and 
they say, I followed your directions perfectly, I have 
them written down, it says turn left and go one mile at the gas station.  Then you say,
I am very sorry, because while my instructions were syntactically correct, they 
sadly contained a small but undetected semantic error.. 

description

Again in all three types of errors, Python is merely trying its hardest to 
do exactly what you have asked.

The learning journey

As you progress through the rest of the book, don't be afraid if the concepts 
don't seem to fit together well the first time.  When you were learning to speak, 
it was not a problem  for your first few years that you just made cute gurgling noises.
And it was OK if it took six months for you to move from simple vocabulary to 
simple sentences and took five or six more years to move from sentences to paragraphs, and a
few more years to be able to write an interesting complete short story on your own.

We want you to learn Python much more rapidly, so we teach it all at the same time
over the next few chapters.  
But it is like learning a new language that takes time to absorb and understand
before it feels natural.
That leads to some confusion as we visit and revisit
topics to try to get you to see the big picture while we are defining the tiny
fragments that make up that big picture.  While the book is written linearly, and
if you are taking a course it will progress in a linear fashion, don't hesitate
to be very nonlinear in how you approach the material.  Look forwards and backwards
and read with a light touch.  By skimming more advanced material without 
fully understanding the details, you can get a better understanding of the why? 
of programming.  By reviewing previous material and even redoing earlier 
exercises, you will realize that you actually learned a lot of material even 
if the material you are currently staring at seems a bit impenetrable.

Usually when you are learning your first programming language, there are a few
wonderful Ah Hah! moments where you can look up from pounding away at some rock
with a hammer and chisel and step away and see that you are indeed building 
a beautiful sculpture.

If something seems particularly hard, there is usually no value in staying up all 
night and staring at it.   Take a break, take a nap, have a snack, explain what you 
are having a problem with to someone (or perhaps your dog), and then come back to it with
fresh eyes.  I assure you that once you learn the programming concepts in the book
you will look back and see that it was all really easy and elegant and it simply 
took you a bit of time to absorb it.
42
The end
</code></pre>
",95,2,-2,5,python;list;text;python-re;data-extraction,2022-07-20 12:52:22,2022-07-20 12:52:22,2022-07-27 08:43:46,i need to extract and sum all the numbers from a file  how can lst append be used correctly to make a list of numbers ,how to extract and sum numbers from a text file,need extract sum numbers file lst append used correctly make numbers,extract sum numbers text file,extract sum numbers text fileneed extract sum numbers file lst append used correctly make numbers,"['extract', 'sum', 'numbers', 'text', 'fileneed', 'extract', 'sum', 'numbers', 'file', 'lst', 'append', 'used', 'correctly', 'make', 'numbers']","['extract', 'sum', 'number', 'text', 'filene', 'extract', 'sum', 'number', 'file', 'lst', 'append', 'use', 'correctli', 'make', 'number']"
62,68,68,19590958,73132252,"ValueError: X has 1 features, but LinearRegression is expecting 2 features as input","<p>I am using pywebio to create a small script-run user interface for my machine learning program. When not using the small UI, I don't have any errors when running the linear regression predict() function.</p>
<p>The UI is retrieving two numbers from a user, an 'age' and a 'gross yearly salary'. Those two numbers are entered into a numpy array, and the numpy array has been reshaped from a 1D array to a 2D array as I was receiving an error on the numpy array shape.</p>
<p>Now, I'm getting an error message on the predict() method only receiving 1 feature instead of 2, when sklearn documentation states that the linear regression predict() method is always getting 'self' and one other feature. How can I fix this error?</p>
<p>Here's my code for the UI:</p>
<pre><code>#Web UI using pywebio
import pywebio
from pywebio.input import *
from pywebio.output import *

def retirement_ui():
    
    age = input(&quot;Please enter your age as a number: &quot;, type = NUMBER)
    salary = input(&quot;please enter your gross yearly salary as a number: &quot;, type = NUMBER)
    
    age_entry = int(age)
    salary_entry = int(salary)
    
    entry = np.array([age_entry, salary_entry])
    reshaped_entry = entry.reshape(-1, 1)

    estimate = regr.predict(reshaped_entry)    

    #Output to the screen
    put_text('Your ideal amount to be putting away for retirement each year is: ' + estimate)
    
retirement_ui()    
</code></pre>
<p>Here is the error message:</p>
<pre><code>ValueError                                Traceback (most recent call last)
Input In [21], in &lt;cell line: 22&gt;()

Input In [21], in retirement_ui()

File ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:362, in LinearModel.predict(self, X)
    348 def predict(self, X):
    349     &quot;&quot;&quot;
    350     Predict using the linear model.
    351 
   (...)
    360         Returns predicted values.
    361     &quot;&quot;&quot;
--&gt; 362     return self._decision_function(X)

File ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:345, in LinearModel._decision_function(self, X)
    342 def _decision_function(self, X):
    343     check_is_fitted(self)
--&gt; 345     X = self._validate_data(X, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;], reset=False)
    346     return safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_

File ~\anaconda3\lib\site-packages\sklearn\base.py:585, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, **check_params)
    582     out = X, y
    584 if not no_val_X and check_params.get(&quot;ensure_2d&quot;, True):
--&gt; 585     self._check_n_features(X, reset=reset)
    587 return out

File ~\anaconda3\lib\site-packages\sklearn\base.py:400, in BaseEstimator._check_n_features(self, X, reset)
    397     return
    399 if n_features != self.n_features_in_:
--&gt; 400     raise ValueError(
    401         f&quot;X has {n_features} features, but {self.__class__.__name__} &quot;
    402         f&quot;is expecting {self.n_features_in_} features as input.&quot;
    403     )

ValueError: X has 1 features, but LinearRegression is expecting 2 features as input.




 
</code></pre>
",41,0,1,4,python;numpy;machine-learning;scikit-learn,2022-07-27 07:29:07,2022-07-27 07:29:07,2022-07-27 07:29:07,i am using pywebio to create a small script run user interface for my machine learning program  when not using the small ui  i don t have any errors when running the linear regression predict   function  the ui is retrieving two numbers from a user  an  age  and a  gross yearly salary   those two numbers are entered into a numpy array  and the numpy array has been reshaped from a d array to a d array as i was receiving an error on the numpy array shape  now  i m getting an error message on the predict   method only receiving  feature instead of   when sklearn documentation states that the linear regression predict   method is always getting  self  and one other feature  how can i fix this error  here s my code for the ui  here is the error message ,valueerror  x has  features  but linearregression is expecting  features as input,using pywebio create small script run user interface machine learning program using small ui errors running linear regression predict function ui retrieving two numbers user age gross yearly salary two numbers entered numpy array numpy array reshaped array array receiving error numpy array shape getting error message predict method receiving feature instead sklearn documentation states linear regression predict method always getting self one feature fix error code ui error message,valueerror x features linearregression expecting features input,valueerror x features linearregression expecting features inputusing pywebio create small script run user interface machine learning program using small ui errors running linear regression predict function ui retrieving two numbers user age gross yearly salary two numbers entered numpy array numpy array reshaped array array receiving error numpy array shape getting error message predict method receiving feature instead sklearn documentation states linear regression predict method always getting self one feature fix error code ui error message,"['valueerror', 'x', 'features', 'linearregression', 'expecting', 'features', 'inputusing', 'pywebio', 'create', 'small', 'script', 'run', 'user', 'interface', 'machine', 'learning', 'program', 'using', 'small', 'ui', 'errors', 'running', 'linear', 'regression', 'predict', 'function', 'ui', 'retrieving', 'two', 'numbers', 'user', 'age', 'gross', 'yearly', 'salary', 'two', 'numbers', 'entered', 'numpy', 'array', 'numpy', 'array', 'reshaped', 'array', 'array', 'receiving', 'error', 'numpy', 'array', 'shape', 'getting', 'error', 'message', 'predict', 'method', 'receiving', 'feature', 'instead', 'sklearn', 'documentation', 'states', 'linear', 'regression', 'predict', 'method', 'always', 'getting', 'self', 'one', 'feature', 'fix', 'error', 'code', 'ui', 'error', 'message']","['valueerror', 'x', 'featur', 'linearregress', 'expect', 'featur', 'inputus', 'pywebio', 'creat', 'small', 'script', 'run', 'user', 'interfac', 'machin', 'learn', 'program', 'use', 'small', 'ui', 'error', 'run', 'linear', 'regress', 'predict', 'function', 'ui', 'retriev', 'two', 'number', 'user', 'age', 'gross', 'yearli', 'salari', 'two', 'number', 'enter', 'numpi', 'array', 'numpi', 'array', 'reshap', 'array', 'array', 'receiv', 'error', 'numpi', 'array', 'shape', 'get', 'error', 'messag', 'predict', 'method', 'receiv', 'featur', 'instead', 'sklearn', 'document', 'state', 'linear', 'regress', 'predict', 'method', 'alway', 'get', 'self', 'one', 'featur', 'fix', 'error', 'code', 'ui', 'error', 'messag']"
63,69,69,9723370,73130723,matplotlib does not upgrade beyound 3.2.1,"<p>I am using Notebook on Azure Machine Learning Studio and I am trying to install latest version of matplotlib, but it continues using version 3.2.1 I tried conda, pip, and also tried to uninstall and install again, nothing works.
sample commands tried:</p>
<pre><code>!pip install matplotlib
!conda install matplotlib
</code></pre>
<p>Am I missing sth? thanks</p>
",35,0,0,4,python;matplotlib;upgrade;azure-machine-learning-studio,2022-07-27 02:27:23,2022-07-27 02:27:23,2022-07-27 02:27:23,am i missing sth  thanks,matplotlib does not upgrade beyound   ,missing sth thanks,matplotlib upgrade beyound,matplotlib upgrade beyoundmissing sth thanks,"['matplotlib', 'upgrade', 'beyoundmissing', 'sth', 'thanks']","['matplotlib', 'upgrad', 'beyoundmiss', 'sth', 'thank']"
64,70,70,10910232,73127243,"Using python to fill fields on a website, perform some actions on the returned results?","<p>A little background - I'm a student, broke and I'd like to travel on the cheapest trains to some nice destinations. Hence this idea was born.</p>
<p>I am comfortable with Python, I have worked with it for its data analysis and machine learning libraries.</p>
<p>Now, I want to be able to use a train ticket booking website to fill the Departure, Arrival, Date, Time, Number of Passengers fields for all destinations offered and find the cheapest train to take. I want to be able to do some actions of the results returned like, for example, choosing the 2nd earliest train while leaving or the 2nd last train while departing. I don't want to automate the booking part, I will do that part myself.</p>
<p>In a nutshell, I just want to automate the part where I input multiple destinations and waste 2 hours of my day trying to find cheap trains.</p>
<p>I have no idea where to start and I looked into some blogs, most of them offered solutions with JavaScript and I have zero knowledge of it. Can I not achieve the above goal with just Python? What libraries will I need for this?</p>
",32,0,0,2,python;web-scraping,2022-07-26 20:05:29,2022-07-26 20:05:29,2022-07-27 01:34:37,a little background   i m a student  broke and i d like to travel on the cheapest trains to some nice destinations  hence this idea was born  i am comfortable with python  i have worked with it for its data analysis and machine learning libraries  now  i want to be able to use a train ticket booking website to fill the departure  arrival  date  time  number of passengers fields for all destinations offered and find the cheapest train to take  i want to be able to do some actions of the results returned like  for example  choosing the nd earliest train while leaving or the nd last train while departing  i don t want to automate the booking part  i will do that part myself  in a nutshell  i just want to automate the part where i input multiple destinations and waste  hours of my day trying to find cheap trains  i have no idea where to start and i looked into some blogs  most of them offered solutions with javascript and i have zero knowledge of it  can i not achieve the above goal with just python  what libraries will i need for this ,using python to fill fields on a website  perform some actions on the returned results ,little background student broke like travel cheapest trains nice destinations hence idea born comfortable python worked data analysis machine learning libraries want able use train ticket booking website fill departure arrival date time number passengers fields destinations offered find cheapest train take want able actions results returned like example choosing nd earliest train leaving nd last train departing want automate booking part part nutshell want automate part input multiple destinations waste hours day trying find cheap trains idea start looked blogs offered solutions javascript zero knowledge achieve goal python libraries need,using python fill fields website perform actions returned results,using python fill fields website perform actions returned resultslittle background student broke like travel cheapest trains nice destinations hence idea born comfortable python worked data analysis machine learning libraries want able use train ticket booking website fill departure arrival date time number passengers fields destinations offered find cheapest train take want able actions results returned like example choosing nd earliest train leaving nd last train departing want automate booking part part nutshell want automate part input multiple destinations waste hours day trying find cheap trains idea start looked blogs offered solutions javascript zero knowledge achieve goal python libraries need,"['using', 'python', 'fill', 'fields', 'website', 'perform', 'actions', 'returned', 'resultslittle', 'background', 'student', 'broke', 'like', 'travel', 'cheapest', 'trains', 'nice', 'destinations', 'hence', 'idea', 'born', 'comfortable', 'python', 'worked', 'data', 'analysis', 'machine', 'learning', 'libraries', 'want', 'able', 'use', 'train', 'ticket', 'booking', 'website', 'fill', 'departure', 'arrival', 'date', 'time', 'number', 'passengers', 'fields', 'destinations', 'offered', 'find', 'cheapest', 'train', 'take', 'want', 'able', 'actions', 'results', 'returned', 'like', 'example', 'choosing', 'nd', 'earliest', 'train', 'leaving', 'nd', 'last', 'train', 'departing', 'want', 'automate', 'booking', 'part', 'part', 'nutshell', 'want', 'automate', 'part', 'input', 'multiple', 'destinations', 'waste', 'hours', 'day', 'trying', 'find', 'cheap', 'trains', 'idea', 'start', 'looked', 'blogs', 'offered', 'solutions', 'javascript', 'zero', 'knowledge', 'achieve', 'goal', 'python', 'libraries', 'need']","['use', 'python', 'fill', 'field', 'websit', 'perform', 'action', 'return', 'resultslittl', 'background', 'student', 'broke', 'like', 'travel', 'cheapest', 'train', 'nice', 'destin', 'henc', 'idea', 'born', 'comfort', 'python', 'work', 'data', 'analysi', 'machin', 'learn', 'librari', 'want', 'abl', 'use', 'train', 'ticket', 'book', 'websit', 'fill', 'departur', 'arriv', 'date', 'time', 'number', 'passeng', 'field', 'destin', 'offer', 'find', 'cheapest', 'train', 'take', 'want', 'abl', 'action', 'result', 'return', 'like', 'exampl', 'choos', 'nd', 'earliest', 'train', 'leav', 'nd', 'last', 'train', 'depart', 'want', 'autom', 'book', 'part', 'part', 'nutshel', 'want', 'autom', 'part', 'input', 'multipl', 'destin', 'wast', 'hour', 'day', 'tri', 'find', 'cheap', 'train', 'idea', 'start', 'look', 'blog', 'offer', 'solut', 'javascript', 'zero', 'knowledg', 'achiev', 'goal', 'python', 'librari', 'need']"
65,71,71,14598633,73013020,PyTorch CUDA : the provided PTX was compiled with an unsupported toolchain,"<p>I am using Nvidia V100 with the following specs:</p>
<pre class=""lang-bash prettyprint-override""><code>(pytorch) [s.1915438@cl1 aneurysm]$ srun nvidia-smi
Sun Jul 17 16:17:27 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |
| N/A   31C    P0    25W / 250W |      0MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>The Python, Pytorch and CUDA version is as follows:</p>
<pre class=""lang-bash prettyprint-override""><code>Python 3.8.13 (default, Mar 28 2022, 11:38:47) 
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.__version__
'1.12.0+cu113'
</code></pre>
<p>When I run a python file, containing a machine learning model, I get the following error.</p>
<pre class=""lang-bash prettyprint-override""><code>(pytorch) [s.1915438@cl1 aneurysm]$ srun python aneurysm.py
terminate called after throwing an instance of 'std::runtime_error'
  what():  the provided PTX was compiled with an unsupported toolchain.
srun: error: ccs2114: task 0: Aborted
</code></pre>
<p>Is it some kind of compatibility issue? Should I fallback to CUDA 10
.2 as the V100 is very old GPU?</p>
",59,1,-2,3,pytorch;cuda;ptx,2022-07-17 18:26:10,2022-07-17 18:26:10,2022-07-27 00:19:33,i am using nvidia v with the following specs  the python  pytorch and cuda version is as follows  when i run a python file  containing a machine learning model  i get the following error ,pytorch cuda   the provided ptx was compiled with an unsupported toolchain,using nvidia v following specs python pytorch cuda version follows run python file containing machine learning model get following error,pytorch cuda provided ptx compiled unsupported toolchain,pytorch cuda provided ptx compiled unsupported toolchainusing nvidia v following specs python pytorch cuda version follows run python file containing machine learning model get following error,"['pytorch', 'cuda', 'provided', 'ptx', 'compiled', 'unsupported', 'toolchainusing', 'nvidia', 'v', 'following', 'specs', 'python', 'pytorch', 'cuda', 'version', 'follows', 'run', 'python', 'file', 'containing', 'machine', 'learning', 'model', 'get', 'following', 'error']","['pytorch', 'cuda', 'provid', 'ptx', 'compil', 'unsupport', 'toolchainus', 'nvidia', 'v', 'follow', 'spec', 'python', 'pytorch', 'cuda', 'version', 'follow', 'run', 'python', 'file', 'contain', 'machin', 'learn', 'model', 'get', 'follow', 'error']"
66,73,73,19604840,73114453,Removing a node in an XML before flattening via the flatxml package in R,"<p>I have multiple XMLs that I'd like to extract data from that were created from PDFs using a machine learning library.</p>
<p>When using the flatxml package in R, I am running into an issue importing the XML via <code>fxml_importXMLFlat()</code> where whenever there is a <code>&lt;ref&gt;</code> tag within a <code>&lt;p&gt;</code> node, it cuts off all the data after <code>&lt;/ref&gt;</code> that is in the <code>&lt;p&gt;</code> node.  So in the example XML below, everything starting at</p>
<blockquote>
<p>. Predominately amorphous (non-crystalline ...&quot;</p>
</blockquote>
<p>until <code>&lt;/p&gt;</code> would get cut off.</p>
<pre><code>&lt;text xml:lang=&quot;en&quot;&gt;
&lt;body&gt;
&lt;div xmlns=&quot;http://www.tei-c.org/ns/1.0&quot;&gt;
&lt;head n=&quot;1.&quot;&gt;Introduction&lt;/head&gt;
&lt;p&gt;
Thermal properties of confectionary products, including the melting temperature (Tm) of crystalline components and glass transition temperature (Tg) of amorphous components, as well as the crystalline to amorphous ratio, significantly impact system texture and stability
&lt;ref type=&quot;bibr&quot; target=&quot;#b23&quot;&gt;(Levine and Slade, 1986)&lt;/ref&gt;
. Predominantly amorphous (non-crystalline, disordered solid) candies are formed by heating ingredients to a set temperature and then quickly cooling the resultant supersatured sugar solution to below the temperature range in which recrystallization of sugars can occur, between Tg and Tm of the material. 
&lt;/p&gt;
&lt;/body&gt;
&lt;/text&gt;
</code></pre>
<p>To get around this issue, I planned to import the files into R as XMLs initially, removing the <code>&lt;ref&gt;</code> tags, then flattening via flatxml.</p>
<p>I have tried using the XML package to find and remove the <code>&lt;ref&gt;</code> tags using the following code:</p>
<pre><code>xml1 &lt;- read_xml(&quot;https://file.io/vAiDRi5s68Gm&quot;)

ref &lt;- xml_find_all(xml1, &quot;//ref&quot;)

rm(ref)
</code></pre>
<p>and it returns nothing in the ref object. When I look at the xml after it is read in, it doesn't look like any of the tags are <code>&lt;ref&gt;</code> either.</p>
<p>I also tried
<code> xml1 &lt;- xmltoList(&quot;https://file.io/vAiDRi5s68Gm&quot;)</code> but it doesn't seem to find the <code>&lt;ref&gt;</code> tag either.</p>
<p>I also tried
<code> xml1 &lt;- xmlToDateFrame(&quot;https://file.io/vAiDRi5s68Gm&quot;)</code> and I get the following error:</p>
<blockquote>
<p>Error in <code>[&lt;-.data.frame</code>(<code>*tmp*</code>, i, names(nodes[[i]]), value = c(text = &quot;\n\t\t&quot;, :
duplicate subscripts for columns</p>
</blockquote>
<p>which as I understand is because the XML file is super nested.</p>
<p>My aim is to extract data from hundreds of XMLs so I need something that I can apply to all the XMLs, not just one specific XML file. Any thoughts would be greatly appreciated!</p>
",27,1,0,4,r;xml;xml-parsing;xml2,2022-07-25 22:31:15,2022-07-25 22:31:15,2022-07-26 23:07:42,i have multiple xmls that i d like to extract data from that were created from pdfs using a machine learning library  when using the flatxml package in r  i am running into an issue importing the xml via fxml_importxmlflat   where whenever there is a  lt ref gt  tag within a  lt p gt  node  it cuts off all the data after  lt  ref gt  that is in the  lt p gt  node   so in the example xml below  everything starting at   predominately amorphous  non crystalline      until  lt  p gt  would get cut off  to get around this issue  i planned to import the files into r as xmls initially  removing the  lt ref gt  tags  then flattening via flatxml  i have tried using the xml package to find and remove the  lt ref gt  tags using the following code  and it returns nothing in the ref object  when i look at the xml after it is read in  it doesn t look like any of the tags are  lt ref gt  either  which as i understand is because the xml file is super nested  my aim is to extract data from hundreds of xmls so i need something that i can apply to all the xmls  not just one specific xml file  any thoughts would be greatly appreciated ,removing a node in an xml before flattening via the flatxml package in r,multiple xmls like extract data created pdfs using machine learning library using flatxml package r running issue importing xml via fxml_importxmlflat whenever lt ref gt tag within lt p gt node cuts data lt ref gt lt p gt node example xml everything starting predominately amorphous non crystalline lt p gt would get cut get around issue planned import files r xmls initially removing lt ref gt tags flattening via flatxml tried using xml package find remove lt ref gt tags using following code returns nothing ref object look xml read look like tags lt ref gt either understand xml file super nested aim extract data hundreds xmls need something apply xmls one specific xml file thoughts would greatly appreciated,removing node xml flattening via flatxml package r,removing node xml flattening via flatxml package rmultiple xmls like extract data created pdfs using machine learning library using flatxml package r running issue importing xml via fxml_importxmlflat whenever lt ref gt tag within lt p gt node cuts data lt ref gt lt p gt node example xml everything starting predominately amorphous non crystalline lt p gt would get cut get around issue planned import files r xmls initially removing lt ref gt tags flattening via flatxml tried using xml package find remove lt ref gt tags using following code returns nothing ref object look xml read look like tags lt ref gt either understand xml file super nested aim extract data hundreds xmls need something apply xmls one specific xml file thoughts would greatly appreciated,"['removing', 'node', 'xml', 'flattening', 'via', 'flatxml', 'package', 'rmultiple', 'xmls', 'like', 'extract', 'data', 'created', 'pdfs', 'using', 'machine', 'learning', 'library', 'using', 'flatxml', 'package', 'r', 'running', 'issue', 'importing', 'xml', 'via', 'fxml_importxmlflat', 'whenever', 'lt', 'ref', 'gt', 'tag', 'within', 'lt', 'p', 'gt', 'node', 'cuts', 'data', 'lt', 'ref', 'gt', 'lt', 'p', 'gt', 'node', 'example', 'xml', 'everything', 'starting', 'predominately', 'amorphous', 'non', 'crystalline', 'lt', 'p', 'gt', 'would', 'get', 'cut', 'get', 'around', 'issue', 'planned', 'import', 'files', 'r', 'xmls', 'initially', 'removing', 'lt', 'ref', 'gt', 'tags', 'flattening', 'via', 'flatxml', 'tried', 'using', 'xml', 'package', 'find', 'remove', 'lt', 'ref', 'gt', 'tags', 'using', 'following', 'code', 'returns', 'nothing', 'ref', 'object', 'look', 'xml', 'read', 'look', 'like', 'tags', 'lt', 'ref', 'gt', 'either', 'understand', 'xml', 'file', 'super', 'nested', 'aim', 'extract', 'data', 'hundreds', 'xmls', 'need', 'something', 'apply', 'xmls', 'one', 'specific', 'xml', 'file', 'thoughts', 'would', 'greatly', 'appreciated']","['remov', 'node', 'xml', 'flatten', 'via', 'flatxml', 'packag', 'rmultipl', 'xml', 'like', 'extract', 'data', 'creat', 'pdf', 'use', 'machin', 'learn', 'librari', 'use', 'flatxml', 'packag', 'r', 'run', 'issu', 'import', 'xml', 'via', 'fxml_importxmlflat', 'whenev', 'lt', 'ref', 'gt', 'tag', 'within', 'lt', 'p', 'gt', 'node', 'cut', 'data', 'lt', 'ref', 'gt', 'lt', 'p', 'gt', 'node', 'exampl', 'xml', 'everyth', 'start', 'predomin', 'amorph', 'non', 'crystallin', 'lt', 'p', 'gt', 'would', 'get', 'cut', 'get', 'around', 'issu', 'plan', 'import', 'file', 'r', 'xml', 'initi', 'remov', 'lt', 'ref', 'gt', 'tag', 'flatten', 'via', 'flatxml', 'tri', 'use', 'xml', 'packag', 'find', 'remov', 'lt', 'ref', 'gt', 'tag', 'use', 'follow', 'code', 'return', 'noth', 'ref', 'object', 'look', 'xml', 'read', 'look', 'like', 'tag', 'lt', 'ref', 'gt', 'either', 'understand', 'xml', 'file', 'super', 'nest', 'aim', 'extract', 'data', 'hundr', 'xml', 'need', 'someth', 'appli', 'xml', 'one', 'specif', 'xml', 'file', 'thought', 'would', 'greatli', 'appreci']"
67,74,74,8811373,47359629,Yolo net label output from PC to an Arduino?,"<p>I have recently gained interest over this machine learning topic about image classification.<br />
I am in no way a programmer, but I am a farmer who is very interested about it, and detecting the quality of fruits and vegetables is a very tedious and time consuming task, specially if you don't have the money to buy industrial machinery to perform this task at a small-medium scale.</p>
<p>I recently came across this tutorial (had to fix a lot of errors because it is really bad written, but it works):</p>
<p><a href=""https://imaginghub.com/projects/148-how-to-distinguish-apples-and-pears-with-raspberry-pi/documentation"" rel=""nofollow noreferrer"">https://imaginghub.com/projects/148-how-to-distinguish-apples-and-pears-with-raspberry-pi/documentation</a></p>
<p>Which basically is the building block of a future fruit/vegetable quality grader.</p>
<p>This conveyor belt is going to have an Arduino that will receive an output from the Python program, that output should activate servos to redirect each fruit/vegetable to its own basket.</p>
<p>Now I would like to know how can I get the label output from the net and transform it to a number for example:</p>
<p>apple = 1, orange = 2, cucumber = 3...</p>
<p>So whenever it's apple, Arduino receives a 1 that will light an LED (first this, then servo), same happens for orange and cucumber and so on.</p>
<p>Here are the 2 codes I believe have to do something with the box label output Deploy.py and yolo_net.py</p>
<p>Deploy.py is the one I run to get the live camera detector:</p>
<p><a href=""https://github.com/jackersson/FruitsRecognition/blob/master/Deploy.ipynb"" rel=""nofollow noreferrer"">Deploy.ipynb</a></p>
<p>And this is yolo_net.py (if it helps somehow to answer my question):</p>
<p><a href=""https://github.com/jackersson/FruitsRecognition/blob/master/yolo_net.py"" rel=""nofollow noreferrer"">yolo_net.py</a></p>
",364,1,-3,2,arduino;darkflow,2017-11-17 23:34:07,2017-11-17 23:34:07,2022-07-26 22:12:22,i recently came across this tutorial  had to fix a lot of errors because it is really bad written  but it works    which basically is the building block of a future fruit vegetable quality grader  this conveyor belt is going to have an arduino that will receive an output from the python program  that output should activate servos to redirect each fruit vegetable to its own basket  now i would like to know how can i get the label output from the net and transform it to a number for example  apple     orange     cucumber       so whenever it s apple  arduino receives a  that will light an led  first this  then servo   same happens for orange and cucumber and so on  here are the  codes i believe have to do something with the box label output deploy py and yolo_net py deploy py is the one i run to get the live camera detector   and this is yolo_net py  if it helps somehow to answer my question   ,yolo net label output from pc to an arduino ,recently came across tutorial fix lot errors really bad written works basically building block future fruit vegetable quality grader conveyor belt going arduino receive output python program output activate servos redirect fruit vegetable basket would like know get label output net transform number example apple orange cucumber whenever apple arduino receives light led first servo happens orange cucumber codes believe something box label output deploy py yolo_net py deploy py one run get live camera detector yolo_net py helps somehow answer question,yolo net label output pc arduino,yolo net label output pc arduinorecently came across tutorial fix lot errors really bad written works basically building block future fruit vegetable quality grader conveyor belt going arduino receive output python program output activate servos redirect fruit vegetable basket would like know get label output net transform number example apple orange cucumber whenever apple arduino receives light led first servo happens orange cucumber codes believe something box label output deploy py yolo_net py deploy py one run get live camera detector yolo_net py helps somehow answer question,"['yolo', 'net', 'label', 'output', 'pc', 'arduinorecently', 'came', 'across', 'tutorial', 'fix', 'lot', 'errors', 'really', 'bad', 'written', 'works', 'basically', 'building', 'block', 'future', 'fruit', 'vegetable', 'quality', 'grader', 'conveyor', 'belt', 'going', 'arduino', 'receive', 'output', 'python', 'program', 'output', 'activate', 'servos', 'redirect', 'fruit', 'vegetable', 'basket', 'would', 'like', 'know', 'get', 'label', 'output', 'net', 'transform', 'number', 'example', 'apple', 'orange', 'cucumber', 'whenever', 'apple', 'arduino', 'receives', 'light', 'led', 'first', 'servo', 'happens', 'orange', 'cucumber', 'codes', 'believe', 'something', 'box', 'label', 'output', 'deploy', 'py', 'yolo_net', 'py', 'deploy', 'py', 'one', 'run', 'get', 'live', 'camera', 'detector', 'yolo_net', 'py', 'helps', 'somehow', 'answer', 'question']","['yolo', 'net', 'label', 'output', 'pc', 'arduinorec', 'came', 'across', 'tutori', 'fix', 'lot', 'error', 'realli', 'bad', 'written', 'work', 'basic', 'build', 'block', 'futur', 'fruit', 'veget', 'qualiti', 'grader', 'conveyor', 'belt', 'go', 'arduino', 'receiv', 'output', 'python', 'program', 'output', 'activ', 'servo', 'redirect', 'fruit', 'veget', 'basket', 'would', 'like', 'know', 'get', 'label', 'output', 'net', 'transform', 'number', 'exampl', 'appl', 'orang', 'cucumb', 'whenev', 'appl', 'arduino', 'receiv', 'light', 'led', 'first', 'servo', 'happen', 'orang', 'cucumb', 'code', 'believ', 'someth', 'box', 'label', 'output', 'deploy', 'py', 'yolo_net', 'py', 'deploy', 'py', 'one', 'run', 'get', 'live', 'camera', 'detector', 'yolo_net', 'py', 'help', 'somehow', 'answer', 'question']"
68,75,75,18094735,73126564,"JSON decode failing, unclear errors, SwiftUI","<p>I followed the Apple Developer SwiftUI tutorial to learn about iOS development, and now am trying to adapt their app to make my own demo app to learn more about Swift. I have a json decoding error when I try and add a second .json file to my ModelData file. The app works as expected when I remove drivers on line 6 and I have validated my json multiple times. (I had to shorten some of the descriptions to post this)</p>
<p>Here is the ModelData file:</p>
<pre><code>import Foundation
import Combine

final class ModelData: ObservableObject {
    @Published var teams: [Team] = load(&quot;teamData.json&quot;)
    @Published var drivers: [Driver] = load(&quot;driverData.json&quot;)
    @Published var profile = Profile.default
    
    var features: [Team] {
        teams.filter { $0.isFeatured}
    }
    
    var categories: [String: [Team]] {
        Dictionary(
            grouping: teams,
            by: { $0.category.rawValue }
        )
    }
}

func load&lt;T: Decodable&gt;(_ filename: String) -&gt; T {
    let data: Data

    guard let file = Bundle.main.url(forResource: filename, withExtension: nil)
    else {
        fatalError(&quot;Couldn't find \(filename) in main bundle.&quot;)
    }
    
    do {
        data = try Data(contentsOf: file)
    } catch {
        fatalError(&quot;Couldn't load \(filename) from main bundle:\n\(error)&quot;)
    }

    do {
        let decoder = JSONDecoder()
        return try decoder.decode(T.self, from: data)
    } catch {
        fatalError(&quot;Couldn't parse \(filename) as \(T.self):\n\(error)&quot;)
    }
}

</code></pre>
<p>And here is the json file that is failing:</p>
<pre><code>[
    {
        &quot;name&quot;: &quot;Max Verstappen&quot;,
        &quot;id&quot;: 1011,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;1&quot;,
        &quot;abbrev&quot;: &quot;VER&quot;,
        &quot;team&quot;: &quot;Red Bull Racing&quot;,
        &quot;championships&quot;: &quot;1&quot;,
        &quot;isFeatured&quot;: true,
        &quot;isFavorite&quot;: true,
        &quot;description&quot;: &quot;Hes Max by name, and max by nature...skys the limit.&quot;
    },
    {
        &quot;name&quot;: &quot;Sergio Perez&quot;,
        &quot;id&quot;: 1012,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;11&quot;,
        &quot;abbrev&quot;: &quot;PER&quot;,
        &quot;team&quot;: &quot;Red Bull Racing&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: true,
        &quot;description&quot;: &quot;Hes the fighter with a gentle touch from the land of the Lucha Libre...but Perez working hard and racing with his heart are.&quot;
    },
    {
        &quot;name&quot;: &quot;Charles Leclerc&quot;,
        &quot;id&quot;: 1013,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;16&quot;,
        &quot;abbrev&quot;: &quot;LEC&quot;,
        &quot;team&quot;: &quot;Ferrari&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: true,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Born in the Mediterranean idyll of Monaco, Leclerc arrived in F1 on a tidal wave of expectation...he is doing them both proud.&quot;
    },
    {
        &quot;name&quot;: &quot;Carlos Sainz&quot;,
        &quot;id&quot;: 1014,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;55&quot;,
        &quot;abbrev&quot;: &quot;SAI&quot;,
        &quot;team&quot;: &quot;Ferrari&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Hes the matador from Madrid racing royalty...Vamos!&quot;
    },
    {
        &quot;name&quot;: &quot;George Russell&quot;,
        &quot;id&quot;: 1015,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;63&quot;,
        &quot;abbrev&quot;: &quot;RUS&quot;,
        &quot;team&quot;: &quot;Mercedes&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Hes the driver with the motto: If in doubt, go flat out. George Russell has lived by it in his F1 career to date, out-qualifying seasoned team mate Robert Kubica at all 21 Grands Prix in his rookie season, putting Williams back on the podium in 2021, and landing a Mercedes race seat alongside Lewis Hamilton for 2022. That brilliant baseline speed served Russell well as he totted up titles on his way to Formula 1. The Briton stormed to the 2017 GP3 championship and delivered the 2018 Formula 2 crown under immense pressure. Spotting his potential, world champions Mercedes swooped to sign him to their junior programme in 2017, when Russell already had a DTM deal on the table. He banked more experience with practice sessions with Force India and tests for the Silver Arrows, before landing his Mercedes-powered Williams race drive. A refusal to cede ground to his rivals - and commitment to a tricky pass  underpins Russells winning mentality. And its what got him the call-up to replace Lewis Hamilton for a one-off Mercedes appearance for Sakhir 2020 when the reigning champ was struck down by Covid-19. That star turn saw Russell miss out on pole by just 0.026s and then outrace Mercedes stalwart Valtteri Bottas. Only a bungled pit stop and a heart-breaking late puncture prevented a near-certain maiden win for the up-and-coming super-sub. He kept his head down at Williams in 2021, scoring his first points and podium, all the while keeping his eye on the bigger prize. Having proved himself a hard worker and a tenacious talent, that prize arrived in the form of a chance to take on compatriot and seven-time champion Hamilton in identical machinery. A huge challenge, but as always, Russell the Rocket will be going flat out.&quot;
    },
    {
        &quot;name&quot;: &quot;Lewis Hamilton&quot;,
        &quot;id&quot;: 1016,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;44&quot;,
        &quot;abbrev&quot;: &quot;HAM&quot;,
        &quot;team&quot;: &quot;Mercedes&quot;,
        &quot;championships&quot;: &quot;7&quot;,
        &quot;isFeatured&quot;: true,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Still I Rise  these are the words emblazoned across the back of Lewis Hamiltons helmet and tattooed across his shoulders, and ever since annihilating expectations with one of the greatest rookie performances in F1 history in 2007, thats literally all hes done: risen to the top of the all-time pole positions list ahead of his hero Ayrton Senna, surged into first place in the wins column surpassing the inimitable Michael Schumacher, and then matched the legendary Germans seven world titles. Is he the G.O.A.T? Few would deny that hes in the conversation  and whats more hes got there his way, twinning his relentless speed with a refusal to conform to stereotypes for how a racing driver should think, dress or behave. Respect is hard earned in F1, but Hamilton  now Sir Lewis Hamilton to be precise  has it from every one of his peers. Why? Because they know that whatever the track, whatever the conditions, whatever the situation, when his visor goes down and the lights go out, its Hammertime.&quot;
    },
    {
        &quot;name&quot;: &quot;Lando Norris&quot;,
        &quot;id&quot;: 1017,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;4&quot;,
        &quot;abbrev&quot;: &quot;NOR&quot;,
        &quot;team&quot;: &quot;McLaren&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: true,
        &quot;description&quot;: &quot;Lando Norris may not be named after Star Wars rebel Lando Calrissian - his Mum just liked the moniker - but he has flair and fighting spirit in bountiful supply. McLaren had the British teenager on their books for two years before fast-tracking him into F1s galaxy of stars in 2019. A firecracker in his junior career, with a penchant for pole positions and wheel-to-wheel tussles, Norris didnt let them down. Paired with the highly-rated  and far more experienced  Carlos Sainz, his rookie season was impressive, edging the Spaniard in their head-to-head qualifying battle, scoring points on 11 occasions, and only narrowly missing out on a top-10 championship placing. It was a similar pattern in 2020, with the affable Brit securing a maiden podium and moving up to ninth overall. His unstoppable rise continued in 2021, with a further four podiums and almost a race win as he dominated another more senior team mate, Daniel Ricciardo, to move up to P6 in the final driver standings. An exciting talent on track, away from it Norris brims with a modest charm and an artistic side sees him design and paint his own race gear as a hobby. The focus for the future is allying artistry and ambition on track as McLaren rely on the promise of youth to take them back to the top. Norris hopes the downforce will be with him&quot;
    },
    {
        &quot;name&quot;: &quot;Daniel Ricciardo&quot;,
        &quot;id&quot;: 1018,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;3&quot;,
        &quot;abbrev&quot;: &quot;RIC&quot;,
        &quot;team&quot;: &quot;McLaren&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: true,
        &quot;description&quot;: &quot;The self-styled Honey Badger is fuzzy on the outside and feisty on the inside. Drivers beware because behind Ricciardos laidback persona and big grin is a razor-sharp racer with a bite. The Australian combines all-out speed with impressive race craft. Never afraid to push to the limits if it means pulling off a pass, Ricciardo is a proven race-winner, capable of consistently finishing at the business end of the championship table given the right machinery. A regular podium-finisher in his days with Red Bull, Ricciardo has christened the steps around the world with a dousing of Aussie culture  the Shoey  as he quaffed champagne from a soggy racing boot. Yes its goofy, but the trademark celebration illustrates why he is loved for his sense of humour but never underestimated on track. His careers next move to Renaults works team in 2019 brought fresh challenges for the Perth pilot, but failed to deliver his dream of following Jack Brabham and Alan Jones as the next world champion from Down Under and he moved on to McLaren for 2021. There he has found a tough young team mate in Lando Norris, but has nevertheless returned to winning ways. Whatever happens next, Ricciardo is sure to keep on smiling.&quot;
    },
    {
        &quot;name&quot;: &quot;Fernando Alonso&quot;,
        &quot;id&quot;: 1019,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;14&quot;,
        &quot;abbrev&quot;: &quot;ALO&quot;,
        &quot;team&quot;: &quot;Alpine&quot;,
        &quot;championships&quot;: &quot;2&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Michael Schumacher was the undisputed king of Formula 1 in the early 2000s, picking up wins and championships at a rate that was simply unheard of at the time. It was going to take someone very special to topple the Ferrari legend from his throne  and that it was Fernando Alonso who did it, tells you all you need to know about the Spaniard. Fiercely competitive, Alonso is not shy about his talent, rating himself as 9/10 in everything, and few in the know would disagree, with his performances in F1 characterised by blistering speed, brilliant tactical thinking, exemplary race craft, a razor-sharp eye for detail and a relentless determination to win. A serial record breaker in his early days, he was  at one time  F1s youngest polesitter, race winner, world champion and double world champion as he gobbled up success with the Renault team. Even Alonso couldnt continue that amazing run in his later career though, failing to add another title to his collection despite spells at McLaren and Ferrari. But after two years away from Formula 1 racing  and with two Le Mans wins in his pocket  Alonso returned with Alpine in 2021. And he has unfinished business with F1&quot;
    },
    {
        &quot;name&quot;: &quot;Esteban Ocon&quot;,
        &quot;id&quot;: 1020,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;31&quot;,
        &quot;abbrev&quot;: &quot;OCO&quot;,
        &quot;team&quot;: &quot;Alpine&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;If theres one word that dominates Esteban Ocons career, its sacrifice. Back when he was just a promising karter, Ocons parents sold their house, put their jobs on hold, and began a life on the road, living in a caravan and travelling from circuit to circuit to support their sons burgeoning career. Sacrifice, see  but it worked. 2014 saw Ocon break through in the world of single-seaters, as he beat a certain Max Verstappen to the European F3 title. Backed by Mercedes, he won the GP3 title the following year and was halfway through a season of DTM in 2016 when he was offered the chance to replace Rio Haryanto at the minnow Manor team from the Belgian Grand Prix onwards. That opportunity led to a full-time seat the following year with Force India, where his wheel-to-wheel duels with highly-rated team mate Sergio Perez quickly marked him out as a rising star. But when Lawrence Stroll, father of racer Lance, stepped in midway through 2018 to secure the squads financial future, the writing was on the wall for Ocon, who was moved aside at the end of the year to allow Stroll Jnr to join from Williams. Ocon bided his time, though, and after a year on the sidelines as Mercedes reserve driver, he found his way back into a race seat for 2020 with Renault, who became Alpine for 2021  when his wait finally paid off, as he scored his  and the famous French marques  first F1 win. Nothing in Ocons motorsport career has come easy  but if Ocon has managed to return to the F1 grid and step atop the podium, its through a combination of self-belief, determination and a talent thats up there with the very best.&quot;
    },
    {
        &quot;name&quot;: &quot;Valterri Bottas&quot;,
        &quot;id&quot;: 1021,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;77&quot;,
        &quot;abbrev&quot;: &quot;BOT&quot;,
        &quot;team&quot;: &quot;Alfa Romeo&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Learning his craft on Finnish roads of ice and snow, he was born to be a Grand Prix racer. Bottas explains that if you can drive on the frozen roads of his homeland then you can drive anywhere. Then theres the Finnish mentality reserved, diligent and calm the fast lane of F1 doesnt faze him. Making his F1 debut with Williams in 2013, Bottas soon became part of the family. Points and podiums followed with the reliable racer even amassing the most points without a win, a record he resented but that showcased his ability. The fact the Finn was such a points machine saw him suddenly promoted to the most coveted seat in F1 - Nico Rosbergs vacant championship-winning seat at Mercedes. Bottas blossomed at the Silver Arrows in 2017, unleashing his pace to clock up personal pole positions and victories as well as a team championship for the famous Mercedes marque alongside Lewis Hamilton. He even tied with Hamilton and Sebastian Vettel with 13 podiums. For a shy guy, it brought a confidence boost and a new swagger  albeit in a very demur Finnish fashion. He would need all that confidence in 2018  a season Bottas described as his worst in F1, as he took zero wins to Hamiltons 11. That, though, was a reflection more of his team mates brilliance than of any shortcomings on his own part. Bottas stepped it up a level in 2019, four victories securing a convincing second in the championship behind Hamilton, but that dropped to two wins to his team mate's 11 in 2020 and then just one in 2021, prompting Mercedes to drop him after five seasons. For 2022 he starts a new chapter in his F1 career, replacing compatriot Kimi Raikkonen to lead an all-new line-up at Alfa Romeo, where he is charged with taking Chinese rookie Zhou Guanyu under his wing.&quot;
    },
    {
        &quot;name&quot;: &quot;Zhou Guanyu&quot;,
        &quot;id&quot;: 1022,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;24&quot;,
        &quot;abbrev&quot;: &quot;ZHO&quot;,
        &quot;team&quot;: &quot;Alfa Romeo&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;China has never boasted a Grand Prix starter among its citizens  but Zhou Guanyu is the driver charged with changing that state of affairs, after receiving the call-up to make his F1 debut for Alfa Romeo in 2022. The Shanghai-born racer attended his home citys inaugural Grand Prix in 2004 at the age of five, cheering on his hero Fernando Alonso. But having caught the racing bug, the Chinese driver then set himself the ambitious goal of becoming his countrys first-ever F1 racer  achieving a feat that Ma Qinghua, the only other Chinese driver to take part in a Formula 1 weekend, never managed. Showing boldness and dedication, Zhou put his plan into action by moving to England with his family aged just 12 to pursue his motorsport ambitions. A second-place finish in the 2015 Italian F4 championship proved Zhou was possessed of the right stuff  a fact already noted by Ferrari, whod signed him to their driver academy a year earlier. A move to Renaults academy for 2019 coincided with his debut in Formula 2, with Zhou building his confidence in the series via multiple wins and pole positions across three seasons, leading to him challenging for the drivers title in 2021. That was enough to convince Alfa Romeo Team Principal Fred Vasseur to put his faith in Zhou for 2022 and field him alongside ex-Mercedes racer Valtteri Bottas  allowing Zhou to achieve his dream of racing in F1, and even to see how he fares against his own childhood hero, Fernando Alonso.&quot;
    },
    {
        &quot;name&quot;: &quot;Kevin Magnussen&quot;,
        &quot;id&quot;: 1023,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;20&quot;,
        &quot;abbrev&quot;: &quot;MAG&quot;,
        &quot;team&quot;: &quot;Haas&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Call him a lone ranger or a maverick, but Magnussen is back in Formula 1 for one reason only  to race. He may be a second-generation F1 driver  following his father, Jan, onto the grid  but Magnussens idols are from the golden era of Grand Prix racing when the likes of Juan Manuel Fangio and Stirling Moss risked it all for the love of the sport. The Roskilde racers own prowess was proven on debut for McLaren, who guided him through the junior ranks, when he cruised into the top-three at the 2014 Australian Grand Prix, becoming the first Dane to claim a podium in F1. Other champagne moments have been more difficult to find, as he left McLaren behind for a season with Renault, before settling in for four among kindred spirits at Haas. And now hes back with the US team  after a year away in the States, racing Indy and sportscars among other things. His meaty manoeuvres and elbows-out approach have earned him a bad-boy reputation on track, something that still leaves him baffled. Out of the car Magnussen is laidback and affable. After all he has his dream job  and he is only here to race.&quot;
    },
    {
        &quot;name&quot;: &quot;Mick Schumacher&quot;,
        &quot;id&quot;: 1024,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;47&quot;,
        &quot;abbrev&quot;: &quot;MSC&quot;,
        &quot;team&quot;: &quot;Haas&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Plenty of sons of former F1 drivers have joined the sport over the years  two have even emulated their fathers to become world champions  but carrying the Schumacher name is surely an extra level of pressure for Mick, given his father Michaels extraordinary achievements in Formula 1. But Schumacher Junior, a member of the Ferrari Driver Academy, arrived in F1 in 2021 with Haas after clinching the previous seasons F2 title in 2020, to add to the F3 championship he won in 2018  so there was zero suggestion he had been elevated to the top level of motorsport because of his name. Its talent that brought him this far and he showed more of the same as he acclimatised to F1 in a tough rookie season, consistently outclassing his team mate in their uncompetitive machinery. Now Schumacher must continue to impress if he is to one day follow in his fathers footsteps and claim a race seat with the Scuderia.&quot;
    },
    {
        &quot;name&quot;: &quot;Pierre Gasly&quot;,
        &quot;id&quot;: 1025,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;10&quot;,
        &quot;abbrev&quot;: &quot;GAS&quot;,
        &quot;team&quot;: &quot;AlphaTauri&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;If theres one man who knows how big a rollercoaster ride an F1 drivers career can be, its Pierre Gasly! The flying Frenchman was called up to make his 2017 debut in Malaysia in place of Daniil Kvyat and, after proving his mettle, he was named a Toro Rosso driver the following year.  A further 21 races into his fledgling career, Gasly was moved up again  this time to replace Red Bull big gun Daniel Ricciardo. Gasly seemed to have a knack of being in the right place at the right time  a quality thats equally handy on track. A series of impressive 2018 performances for Toro Rosso  including a brilliant fourth place in Bahrain  showed exciting promise for what he might do with the A team in 2019. Unfortunately that promise only appeared in flashes  and he quickly suffered from unfavourable comparisons with superstar team mate Max Verstappen. So much so that after the summer break, he was sent back to Toro Rosso, with another young up-and-comer  Alex Albon  being given a shot in the senior Red Bull seat. But Gasly bounced back, as only Gasly can. In the seasons remaining nine races he scored almost as many points as team mate Kvyat managed over the entire year  and secured his best-ever race result with P2 in Brazil. That trajectory continued in 2020, peaking with an emotional maiden win at the renamed AlphaTauri teams home race in Italy, and didnt let up in 2021 when he was back on the podium and scored 110 of the squads 142 points. The question now is can he maintain momentum and earn himself another shot at the F1 bigtime&quot;
    },
    {
        &quot;name&quot;: &quot;Yuki Tsunoda&quot;,
        &quot;id&quot;: 1026,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;22&quot;,
        &quot;abbrev&quot;: &quot;TSU&quot;,
        &quot;team&quot;: &quot;AlphaTauri&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;In the entire history of Formula 1, no Japanese driver has ever won a World Championship Grand Prix. Could Yuki Tsunoda be the first? Red Bull certainly think so, with the youngster very much on the path to their senior team if he continues to impress as he has done over the past few years. Tsunoda's ascent to the top tier of motorsport was astonishingly rapid: he went from racing in Japanese F4 to a Formula 1 seat with AlphaTauri in just over three years, having arrived in Europe in 2019 with no knowledge of the circuits. But after a slow start in F3, followed by a hugely impressive debut F2 campaign that saw him finish third in the championship and pick up three wins along the way, Tsunoda proved he had the speed and the race craft to force his way on to the F1 grid. He may not have adapted to Grand Prix racing quite as quickly as he did to F2, but given time he could yet find himself battling at the very sharp end in a Red Bull.&quot;
    },
    {
        &quot;name&quot;: &quot;Sebastian Vettel&quot;,
        &quot;id&quot;: 1027,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;5&quot;,
        &quot;abbrev&quot;: &quot;VET&quot;,
        &quot;team&quot;: &quot;Aston Martin&quot;,
        &quot;championships&quot;: &quot;4&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Born and raised a Bull, then a Prancing Horse, and now the face of Aston Martins Formula 1 revival, F1's poster boy of early achievement had won more than all but two drivers in history by the time he was just 26, including back-to-back world titles between 2010 and 2013. Vettels trademark is pure pace  and of course his one-finger victory salute. In the chase to the chequered flag, he likes to lead from the front and just like his hero, Michael Schumacher. But for all his competitive streak, Vettel has a playful side too and has been known to let loose with a spot of Beatles karaoke - and baby can he drive a car. Alongside his four world crowns he can boast more than 50 pole positions and race victories, ranking him  statistically - above many of the biggest names in F1 history. No wonder then that he has twice been hand-picked to return some of Grand Prix oldest names to glory. Following his move to Maranello, that mission didnt exactly go to plan as Vettels rivalry with Lewis Hamilton intensified. Then came an additional thorn in his side  young-gun Ferrari team mate Charles Leclerc, the first man to outscore him over a season at the Scuderia. His latest challenge is as Aston Martins team leader. He has already put them on the podium, but Vettel will need to call on all his speed and experience if hes to regularly reassert himself over his rivals  and re-establish his reputation as one of the sports all-time greats.&quot;
    },
    {
        &quot;name&quot;: &quot;Lance Stroll&quot;,
        &quot;id&quot;: 1028,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;18&quot;,
        &quot;abbrev&quot;: &quot;STR&quot;,
        &quot;team&quot;: &quot;Aston Martin&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;There is no such thing as too much too soon for Stroll, a teenage sensation with a wet weather predilection. One of the cool kids on the grid, Stroll was unveiled shortly after his 18th birthday by Williams  before he finished high school and got his road licence. Stroll meant business in his debut 2017 season, setting records on the way. An opportunistic racer he bounded onto the podium in Baku, the youngest rookie to do so. As the son of a wealthy entrepreneur, Stroll is used to a champagne lifestyle but now he knows the fizz tastes all the sweeter on the rostrum. Then in Monza he mastered the downpours to become the youngest driver in history to line up on the front row. A single-minded starter, the Canadian loves to make up places on the opening lap and fight through to the points. Stroll has the potential to be a long-term fixture in Formula 1  as amply illustrated by a maiden pole and another two podiums in 2020. Those came after his father Lawrence led the consortium that took over Force India midway through the 2018 season, and then transformed it from Racing Point to Aston Martin for 2021. The future looks bright for both the team and their young driver  and even if it rains then Stroll can keep on motoring at the sharp end of the pack.&quot;
    },
    {
        &quot;name&quot;: &quot;Alexander Albon&quot;,
        &quot;id&quot;: 1029,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;23&quot;,
        &quot;abbrev&quot;: &quot;ALB&quot;,
        &quot;team&quot;: &quot;Williams&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: true,
        &quot;description&quot;: &quot;Born in London but racing under the flag of Thailand, Alexander Albons first word was in fact Italian. That word was Ferrari  though it was with another Italian team that he got his big F1 break. Idolising Michael Schumacher and dreaming of one day racing in Formula 1, the junior Albon was pipped to the 2016 GP3 title by a certain Charles Leclerc. He then left his great friendship with George Russell trackside as he took the 2018 Formula 2 title fight down to the wire. Graduating to the F1 big league along with yet another contemporary  Lando Norris  in 2019, Albon did his talking on track with Toro Rosso in the opening races, earning a mid-season promotion to Red Bull Racing. A stylish over-taker with a championship mentality, Albon was unfazed by partnering Max Verstappen for the second half of his rookie season, taking top-six finishes in eight of his nine 2019 races with Red Bull. Staying in touch with the future champion proved tougher in 2020 and Red Bull dropped him from their race line-up. Crucially, though, Albon was retained as test and reserve driver, keeping him very much on team bosses radar, leading to his 2022 return to the grid with Williams. Laidback and cheerful with a cheeky grin, the Thai racer is popular among his peers  not always easy in motorsports cauldron of competition  but you dont succeed in Formula 1 by being popular. Albons challenge now is a big one  to make the most of a rare second F1 opportunity.&quot;
    },
    {
        &quot;name&quot;: &quot;Nicholas Latifi&quot;,
        &quot;id&quot;: 1030,
        &quot;category&quot;: &quot;Drivers&quot;,
        &quot;num&quot;: &quot;6&quot;,
        &quot;abbrev&quot;: &quot;LAT&quot;,
        &quot;team&quot;: &quot;Williams&quot;,
        &quot;championships&quot;: &quot;0&quot;,
        &quot;isFeatured&quot;: false,
        &quot;isFavorite&quot;: false,
        &quot;description&quot;: &quot;Thirteen is an advanced age to begin your karting career these days. But thats how old Toronto native Nicholas Latifi was when he took his first steps in motorsport. Just 11 years later, he became a fully-fledged Formula 1 driver. That ascension into racing's top category was largely thanks to his most impressive season to date in Formula 2 in 2019, with Latifi  whod finished a disappointing ninth in the series 2018 standings  pulling up his bootstraps to claim second in the championship. That result, combined with the Williams/Robert Kubica union failing to mesh in 2019, meant Williams made the call to promote their affable Canadian reserve driver to a full-time drive alongside George Russell for 2020. Latifis first taste of F1 machinery actually came all the way back in 2017, when he was given a test by Renault. Further duties with Force India followed in 2018 before he joined the Williams family in 2019. It was an annus horribilis for the squad, no doubt  but Latifis straightforward, friendly attitude and insightful feedback helped swing the vote in his favour for 2020. Now, after two seasons living in Russells shadow  and with Williams' form on the rise and a new team mate in the shape of ex-Red Bull racer Alex Albon  the goal is to show that that his eye-catching F2 year wasnt just a fluke, and finally prove that he really has got what it takes to mix it with the best drivers in the world.&quot;
    }
]
</code></pre>
<p>Here is the error:</p>
<pre><code>Couldn't parse driverData.json as Array&lt;Driver&gt;:

----------------------------------------

CrashReportError: Fatal Error in ModelData.swift

F1Gallery crashed due to fatalError in ModelData.swift at line 46.

Couldn't parse driverData.json as Array&lt;Driver&gt;:

Process: F1Gallery[4708]
Date/Time: 2022-07-26 16:33:41 +0000
Log File: &lt;none&gt;
</code></pre>
",35,1,-1,3,json;swift;swiftui,2022-07-26 19:04:05,2022-07-26 19:04:05,2022-07-26 19:58:06,i followed the apple developer swiftui tutorial to learn about ios development  and now am trying to adapt their app to make my own demo app to learn more about swift  i have a json decoding error when i try and add a second  json file to my modeldata file  the app works as expected when i remove drivers on line  and i have validated my json multiple times   i had to shorten some of the descriptions to post this  here is the modeldata file  and here is the json file that is failing  here is the error ,json decode failing  unclear errors  swiftui,followed apple developer swiftui tutorial learn ios development trying adapt app make demo app learn swift json decoding error try second json file modeldata file app works expected remove drivers line validated json multiple times shorten descriptions post modeldata file json file failing error,json decode failing unclear errors swiftui,json decode failing unclear errors swiftuifollowed apple developer swiftui tutorial learn ios development trying adapt app make demo app learn swift json decoding error try second json file modeldata file app works expected remove drivers line validated json multiple times shorten descriptions post modeldata file json file failing error,"['json', 'decode', 'failing', 'unclear', 'errors', 'swiftuifollowed', 'apple', 'developer', 'swiftui', 'tutorial', 'learn', 'ios', 'development', 'trying', 'adapt', 'app', 'make', 'demo', 'app', 'learn', 'swift', 'json', 'decoding', 'error', 'try', 'second', 'json', 'file', 'modeldata', 'file', 'app', 'works', 'expected', 'remove', 'drivers', 'line', 'validated', 'json', 'multiple', 'times', 'shorten', 'descriptions', 'post', 'modeldata', 'file', 'json', 'file', 'failing', 'error']","['json', 'decod', 'fail', 'unclear', 'error', 'swiftuifollow', 'appl', 'develop', 'swiftui', 'tutori', 'learn', 'io', 'develop', 'tri', 'adapt', 'app', 'make', 'demo', 'app', 'learn', 'swift', 'json', 'decod', 'error', 'tri', 'second', 'json', 'file', 'modeldata', 'file', 'app', 'work', 'expect', 'remov', 'driver', 'line', 'valid', 'json', 'multipl', 'time', 'shorten', 'descript', 'post', 'modeldata', 'file', 'json', 'file', 'fail', 'error']"
69,76,76,721666,73126242,How can Named Entity Recognition work without NLP?,"<p>I have a question about Machine Learning and Names Entity Recognition.</p>
<p>My goal is to extract named entities from an invoice document. Invoices are typical structured text and this kind of data is usually not useful for Natural Language processing (NLP). I already tried to train a model with the NLP Library Spacy to detect invoice meta data like <em>Date, Total, Customer name</em>. This works more or less good. As far as I understand, an invoice does not provide the unstructured plain text which is usually expected from NLP.</p>
<p>An typical text example for an invoice text analyzed with NLP ML which I found often in the Internet, looks like this:</p>
<p><em>Partial invoice (100,000, so roughly 40%) for the consignment C27655 we shipped on 15th August to London from the Make Believe Town depot. INV2345 is for the balance.. Customer contact (Sigourney) says they will pay this on the usual credit terms (30 days).</em></p>
<p>NLP loves this kind of text. But text extracted form a Invoice PDF (using Apache Tika) usually looks more like this:</p>
<pre><code>Client no: Invoice no: Invoice date: Due date:
1000011128 DEAXXXD220012269 26-Jul-2022 02-Aug-2022
Invoice to: Booking Reference
LOGISTCS GMBH Client Reference :
DEMOSTRASSE 2-6 Comments:
28195 BREMEN
Germany
Vessel : Voy : Place of Receipt : POL: B/LNo:
XXX JUBILEE NUBBBW SAV33NAH, GA ME000243
ETA: Final Destination : POD:
15-Jul-2022 ANTWERP, BELGIUM
Charge Quantity(days) x Rate Currency Total ROE Total EUR VAT
STORAGE_IMP_FOREIGN 1 day(s) x 30,00 EUR EUR 30,00 1,000000 30,00 0,00
</code></pre>
<p>So I guess NLP is in general the wrong approach to train the recognition of meta data from an invoice document. I think the problem is more like recognizing cats in a picture.</p>
<p>What could be a more promising approach for Named Entity Recognition to train structured text with a machine learning framework?</p>
",21,0,0,3,machine-learning;nlp;named-entity-recognition,2022-07-26 18:39:31,2022-07-26 18:39:31,2022-07-26 18:39:31,i have a question about machine learning and names entity recognition  my goal is to extract named entities from an invoice document  invoices are typical structured text and this kind of data is usually not useful for natural language processing  nlp   i already tried to train a model with the nlp library spacy to detect invoice meta data like date  total  customer name  this works more or less good  as far as i understand  an invoice does not provide the unstructured plain text which is usually expected from nlp  an typical text example for an invoice text analyzed with nlp ml which i found often in the internet  looks like this   partial invoice      so roughly    for the consignment c we shipped on th august to london from the make believe town depot  inv is for the balance   customer contact  sigourney  says they will pay this on the usual credit terms   days    nlp loves this kind of text  but text extracted form a invoice pdf  using apache tika  usually looks more like this  so i guess nlp is in general the wrong approach to train the recognition of meta data from an invoice document  i think the problem is more like recognizing cats in a picture  what could be a more promising approach for named entity recognition to train structured text with a machine learning framework ,how can named entity recognition work without nlp ,question machine learning names entity recognition goal extract named entities invoice document invoices typical structured text kind data usually useful natural language processing nlp already tried train model nlp library spacy detect invoice meta data like date total customer name works less good far understand invoice provide unstructured plain text usually expected nlp typical text example invoice text analyzed nlp ml found often internet looks like partial invoice roughly consignment c shipped th august london make believe town depot inv balance customer contact sigourney says pay usual credit terms days nlp loves kind text text extracted form invoice pdf using apache tika usually looks like guess nlp general wrong approach train recognition meta data invoice document think problem like recognizing cats picture could promising approach named entity recognition train structured text machine learning framework,named entity recognition work without nlp,named entity recognition work without nlpquestion machine learning names entity recognition goal extract named entities invoice document invoices typical structured text kind data usually useful natural language processing nlp already tried train model nlp library spacy detect invoice meta data like date total customer name works less good far understand invoice provide unstructured plain text usually expected nlp typical text example invoice text analyzed nlp ml found often internet looks like partial invoice roughly consignment c shipped th august london make believe town depot inv balance customer contact sigourney says pay usual credit terms days nlp loves kind text text extracted form invoice pdf using apache tika usually looks like guess nlp general wrong approach train recognition meta data invoice document think problem like recognizing cats picture could promising approach named entity recognition train structured text machine learning framework,"['named', 'entity', 'recognition', 'work', 'without', 'nlpquestion', 'machine', 'learning', 'names', 'entity', 'recognition', 'goal', 'extract', 'named', 'entities', 'invoice', 'document', 'invoices', 'typical', 'structured', 'text', 'kind', 'data', 'usually', 'useful', 'natural', 'language', 'processing', 'nlp', 'already', 'tried', 'train', 'model', 'nlp', 'library', 'spacy', 'detect', 'invoice', 'meta', 'data', 'like', 'date', 'total', 'customer', 'name', 'works', 'less', 'good', 'far', 'understand', 'invoice', 'provide', 'unstructured', 'plain', 'text', 'usually', 'expected', 'nlp', 'typical', 'text', 'example', 'invoice', 'text', 'analyzed', 'nlp', 'ml', 'found', 'often', 'internet', 'looks', 'like', 'partial', 'invoice', 'roughly', 'consignment', 'c', 'shipped', 'th', 'august', 'london', 'make', 'believe', 'town', 'depot', 'inv', 'balance', 'customer', 'contact', 'sigourney', 'says', 'pay', 'usual', 'credit', 'terms', 'days', 'nlp', 'loves', 'kind', 'text', 'text', 'extracted', 'form', 'invoice', 'pdf', 'using', 'apache', 'tika', 'usually', 'looks', 'like', 'guess', 'nlp', 'general', 'wrong', 'approach', 'train', 'recognition', 'meta', 'data', 'invoice', 'document', 'think', 'problem', 'like', 'recognizing', 'cats', 'picture', 'could', 'promising', 'approach', 'named', 'entity', 'recognition', 'train', 'structured', 'text', 'machine', 'learning', 'framework']","['name', 'entiti', 'recognit', 'work', 'without', 'nlpquestion', 'machin', 'learn', 'name', 'entiti', 'recognit', 'goal', 'extract', 'name', 'entiti', 'invoic', 'document', 'invoic', 'typic', 'structur', 'text', 'kind', 'data', 'usual', 'use', 'natur', 'languag', 'process', 'nlp', 'alreadi', 'tri', 'train', 'model', 'nlp', 'librari', 'spaci', 'detect', 'invoic', 'meta', 'data', 'like', 'date', 'total', 'custom', 'name', 'work', 'less', 'good', 'far', 'understand', 'invoic', 'provid', 'unstructur', 'plain', 'text', 'usual', 'expect', 'nlp', 'typic', 'text', 'exampl', 'invoic', 'text', 'analyz', 'nlp', 'ml', 'found', 'often', 'internet', 'look', 'like', 'partial', 'invoic', 'roughli', 'consign', 'c', 'ship', 'th', 'august', 'london', 'make', 'believ', 'town', 'depot', 'inv', 'balanc', 'custom', 'contact', 'sigourney', 'say', 'pay', 'usual', 'credit', 'term', 'day', 'nlp', 'love', 'kind', 'text', 'text', 'extract', 'form', 'invoic', 'pdf', 'use', 'apach', 'tika', 'usual', 'look', 'like', 'guess', 'nlp', 'gener', 'wrong', 'approach', 'train', 'recognit', 'meta', 'data', 'invoic', 'document', 'think', 'problem', 'like', 'recogn', 'cat', 'pictur', 'could', 'promis', 'approach', 'name', 'entiti', 'recognit', 'train', 'structur', 'text', 'machin', 'learn', 'framework']"
70,77,77,8752312,73120408,Problem with combination of Python Multiprocessing and MPI on SLURM cluster,"<p>I have a machine learning Python program that makes some calculations and then runs a C++ finite-volume code (<a href=""https://www.openfoam.com/"" rel=""nofollow noreferrer"">OpenFOAM</a>) repeatedly (thousands of times). The Python code uses &quot;multiprocessing&quot; for parallel processing which means running several instances of that C++ solver at the same time. Additionally, the C++ solver itself is also parallelized with MPI.</p>
<p>The whole framework works just fine on my local computer. But when I use SLURM clusters (<a href=""https://www.c3se.chalmers.se/"" rel=""nofollow noreferrer"">Vera</a> and <a href=""https://www.nsc.liu.se/systems/tetralith/"" rel=""nofollow noreferrer"">Tetralith</a>) the procedure becomes extremely slow. Although, each instance of the finite volume solver runs rather fast, when one instance is finished the code waits a significant amount of time to run the next one. It appears that the code needs to wait until some specific cores are freed, which is strange as I reserve the required number of cores through a SBATCH script.</p>
<p>Let's say I run the Python code on 8 cores and each core runs a C++ solver using 50 cores with MPI. Thus, I reserve 400 (8 times 50) cores for the whole job through the following script (I even tried requesting twice the number of cores but did not work):</p>
<pre><code>#!/bin/bash
#SBATCH -A MY_PROJECT_NAME  
#SBATCH -p MY_CLUSTER_NAME
#SBATCH -J MY_CASE_NAME
#SBATCH -n 400 
#SBATCH -t 100:00:00
#SBATCH -o slurm-%j.out
#SBATCH --exclusive
#-----------------------------------------------------------
module load ALL_THE_REQUIRED_MODULES
#-----------------------------------------------------------

python3 -u training.py &amp;&gt; log.training
</code></pre>
<p>Sometimes after finishing one C++ solver and before the next one, I get the following messages which I guess indicate that the code is waiting for the cores to be freed. But the cores should already be free as many unused cores exist.</p>
<pre><code>srun: Job 21234173 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 21234173 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 21234173 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 21234173 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for job 21234173
</code></pre>
<p>Any help or idea would be much appreciated.</p>
<p>Saeed</p>
",29,0,0,3,mpi;python-multiprocessing;slurm,2022-07-26 11:52:45,2022-07-26 11:52:45,2022-07-26 18:18:33,i have a machine learning python program that makes some calculations and then runs a c   finite volume code    repeatedly  thousands of times   the python code uses  multiprocessing  for parallel processing which means running several instances of that c   solver at the same time  additionally  the c   solver itself is also parallelized with mpi  the whole framework works just fine on my local computer  but when i use slurm clusters   and   the procedure becomes extremely slow  although  each instance of the finite volume solver runs rather fast  when one instance is finished the code waits a significant amount of time to run the next one  it appears that the code needs to wait until some specific cores are freed  which is strange as i reserve the required number of cores through a sbatch script  let s say i run the python code on  cores and each core runs a c   solver using  cores with mpi  thus  i reserve    times   cores for the whole job through the following script  i even tried requesting twice the number of cores but did not work   sometimes after finishing one c   solver and before the next one  i get the following messages which i guess indicate that the code is waiting for the cores to be freed  but the cores should already be free as many unused cores exist  any help or idea would be much appreciated  saeed,problem with combination of python multiprocessing and mpi on slurm cluster,machine learning python program makes calculations runs c finite volume code repeatedly thousands times python code uses multiprocessing parallel processing means running several instances c solver time additionally c solver also parallelized mpi whole framework works fine local computer use slurm clusters procedure becomes extremely slow although instance finite volume solver runs rather fast one instance finished code waits significant amount time run next one appears code needs wait specific cores freed strange reserve required number cores sbatch script let say run python code cores core runs c solver using cores mpi thus reserve times cores whole job following script even tried requesting twice number cores work sometimes finishing one c solver next one get following messages guess indicate code waiting cores freed cores already free many unused cores exist help idea would much appreciated saeed,problem combination python multiprocessing mpi slurm cluster,problem combination python multiprocessing mpi slurm clustermachine learning python program makes calculations runs c finite volume code repeatedly thousands times python code uses multiprocessing parallel processing means running several instances c solver time additionally c solver also parallelized mpi whole framework works fine local computer use slurm clusters procedure becomes extremely slow although instance finite volume solver runs rather fast one instance finished code waits significant amount time run next one appears code needs wait specific cores freed strange reserve required number cores sbatch script let say run python code cores core runs c solver using cores mpi thus reserve times cores whole job following script even tried requesting twice number cores work sometimes finishing one c solver next one get following messages guess indicate code waiting cores freed cores already free many unused cores exist help idea would much appreciated saeed,"['problem', 'combination', 'python', 'multiprocessing', 'mpi', 'slurm', 'clustermachine', 'learning', 'python', 'program', 'makes', 'calculations', 'runs', 'c', 'finite', 'volume', 'code', 'repeatedly', 'thousands', 'times', 'python', 'code', 'uses', 'multiprocessing', 'parallel', 'processing', 'means', 'running', 'several', 'instances', 'c', 'solver', 'time', 'additionally', 'c', 'solver', 'also', 'parallelized', 'mpi', 'whole', 'framework', 'works', 'fine', 'local', 'computer', 'use', 'slurm', 'clusters', 'procedure', 'becomes', 'extremely', 'slow', 'although', 'instance', 'finite', 'volume', 'solver', 'runs', 'rather', 'fast', 'one', 'instance', 'finished', 'code', 'waits', 'significant', 'amount', 'time', 'run', 'next', 'one', 'appears', 'code', 'needs', 'wait', 'specific', 'cores', 'freed', 'strange', 'reserve', 'required', 'number', 'cores', 'sbatch', 'script', 'let', 'say', 'run', 'python', 'code', 'cores', 'core', 'runs', 'c', 'solver', 'using', 'cores', 'mpi', 'thus', 'reserve', 'times', 'cores', 'whole', 'job', 'following', 'script', 'even', 'tried', 'requesting', 'twice', 'number', 'cores', 'work', 'sometimes', 'finishing', 'one', 'c', 'solver', 'next', 'one', 'get', 'following', 'messages', 'guess', 'indicate', 'code', 'waiting', 'cores', 'freed', 'cores', 'already', 'free', 'many', 'unused', 'cores', 'exist', 'help', 'idea', 'would', 'much', 'appreciated', 'saeed']","['problem', 'combin', 'python', 'multiprocess', 'mpi', 'slurm', 'clustermachin', 'learn', 'python', 'program', 'make', 'calcul', 'run', 'c', 'finit', 'volum', 'code', 'repeatedli', 'thousand', 'time', 'python', 'code', 'use', 'multiprocess', 'parallel', 'process', 'mean', 'run', 'sever', 'instanc', 'c', 'solver', 'time', 'addit', 'c', 'solver', 'also', 'parallel', 'mpi', 'whole', 'framework', 'work', 'fine', 'local', 'comput', 'use', 'slurm', 'cluster', 'procedur', 'becom', 'extrem', 'slow', 'although', 'instanc', 'finit', 'volum', 'solver', 'run', 'rather', 'fast', 'one', 'instanc', 'finish', 'code', 'wait', 'signific', 'amount', 'time', 'run', 'next', 'one', 'appear', 'code', 'need', 'wait', 'specif', 'core', 'freed', 'strang', 'reserv', 'requir', 'number', 'core', 'sbatch', 'script', 'let', 'say', 'run', 'python', 'code', 'core', 'core', 'run', 'c', 'solver', 'use', 'core', 'mpi', 'thu', 'reserv', 'time', 'core', 'whole', 'job', 'follow', 'script', 'even', 'tri', 'request', 'twice', 'number', 'core', 'work', 'sometim', 'finish', 'one', 'c', 'solver', 'next', 'one', 'get', 'follow', 'messag', 'guess', 'indic', 'code', 'wait', 'core', 'freed', 'core', 'alreadi', 'free', 'mani', 'unus', 'core', 'exist', 'help', 'idea', 'would', 'much', 'appreci', 'saeed']"
71,78,78,15221728,73121136,How to convert multiple 2D arrays to 1D columns using xarray and dask in python?,"<p>I have 7 2D cloud optimised geotiffs stacked into one data array in xarray. They are very large so I am using the intake-xarray extension and dask for streaming the data from s3 without using any RAM. I have concatenated them along their &quot;band&quot; dimension to stack them.</p>
<pre><code>catalog = intake.open_catalog(&quot;s3://example-data/datasets.yml&quot;)
datasets = ['dem',
            'dem_slope',
            'dem_slope_aspect',
            'distance_railways',
            'distance_river',
            'distance_road',
            'worldcover']
to_concat = []
for data in datasets:
    x = catalog[data].to_dask()
    to_concat.append(x)


merged = xr.concat(to_concat, dim='band')
merged.coords['band'] = datasets  # add labels to band dimension

y_array = catalog[&quot;NASA_global&quot;]
y_array.coords['band'] = 'NASA_global'

merged 
&lt;xarray.DataArray (band: 7, y: 225000, x: 450000)&gt;
dask.array&lt;concatenate, shape=(7, 225000, 450000), dtype=float32, chunksize=(1, 256, 256), chunktype=numpy.ndarray&gt;
Coordinates:
  * band     (band) &lt;U31 'dem' ... 'worldcover'
  * y        (y) float64 90.0 90.0 90.0 90.0 90.0 ... -90.0 -90.0 -90.0 -90.0
  * x        (x) float64 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0 180.0
Attributes:
    transform:      (0.0008, 0.0, -180.0, 0.0, -0.0008, 90.0)
    crs:            +init=epsg:4326
    res:            (0.0008, 0.0008)
    is_tiled:       1
    nodatavals:     (32767.0,)
    scales:         (1.0,)
    offsets:        (0.0,)
    AREA_OR_POINT:  Area
</code></pre>
<p>My question is how I could now convert the data into a several 1D columns, equivalent to flattening a 2D array in numpy? I have looked at .squeeze() to remove dimension but can't get it into the desired format. I want to do some machine learning and need it in a suitable format. New to dask and xarray.</p>
<p>I'd really appreciate any help or advice.</p>
",38,1,0,4,python;dask;python-xarray;dask-ml,2022-07-26 12:43:46,2022-07-26 12:43:46,2022-07-26 17:35:48,i have  d cloud optimised geotiffs stacked into one data array in xarray  they are very large so i am using the intake xarray extension and dask for streaming the data from s without using any ram  i have concatenated them along their  band  dimension to stack them  my question is how i could now convert the data into a several d columns  equivalent to flattening a d array in numpy  i have looked at  squeeze   to remove dimension but can t get it into the desired format  i want to do some machine learning and need it in a suitable format  new to dask and xarray  i d really appreciate any help or advice ,how to convert multiple d arrays to d columns using xarray and dask in python ,cloud optimised geotiffs stacked one data array xarray large using intake xarray extension dask streaming data without using ram concatenated along band dimension stack question could convert data several columns equivalent flattening array numpy looked squeeze remove dimension get desired format want machine learning need suitable format dask xarray really appreciate help advice,convert multiple arrays columns using xarray dask python,convert multiple arrays columns using xarray dask pythoncloud optimised geotiffs stacked one data array xarray large using intake xarray extension dask streaming data without using ram concatenated along band dimension stack question could convert data several columns equivalent flattening array numpy looked squeeze remove dimension get desired format want machine learning need suitable format dask xarray really appreciate help advice,"['convert', 'multiple', 'arrays', 'columns', 'using', 'xarray', 'dask', 'pythoncloud', 'optimised', 'geotiffs', 'stacked', 'one', 'data', 'array', 'xarray', 'large', 'using', 'intake', 'xarray', 'extension', 'dask', 'streaming', 'data', 'without', 'using', 'ram', 'concatenated', 'along', 'band', 'dimension', 'stack', 'question', 'could', 'convert', 'data', 'several', 'columns', 'equivalent', 'flattening', 'array', 'numpy', 'looked', 'squeeze', 'remove', 'dimension', 'get', 'desired', 'format', 'want', 'machine', 'learning', 'need', 'suitable', 'format', 'dask', 'xarray', 'really', 'appreciate', 'help', 'advice']","['convert', 'multipl', 'array', 'column', 'use', 'xarray', 'dask', 'pythoncloud', 'optimis', 'geotiff', 'stack', 'one', 'data', 'array', 'xarray', 'larg', 'use', 'intak', 'xarray', 'extens', 'dask', 'stream', 'data', 'without', 'use', 'ram', 'concaten', 'along', 'band', 'dimens', 'stack', 'question', 'could', 'convert', 'data', 'sever', 'column', 'equival', 'flatten', 'array', 'numpi', 'look', 'squeez', 'remov', 'dimens', 'get', 'desir', 'format', 'want', 'machin', 'learn', 'need', 'suitabl', 'format', 'dask', 'xarray', 'realli', 'appreci', 'help', 'advic']"
72,79,79,19392483,73122824,Predicting car prices with machine learning - advice on best model,"<p>I am trying to predict the price for cars using the following DataFrame <code>data</code>. Data types: model, transmission, and fuel type as <code>obj</code>, the rest as <code>float</code>/<code>int</code>.</p>
<p>The full dataset contains ~6500 samples (only <code>data.head()</code> listed here)</p>
<pre><code>    model   year    price   transmission    mileage fuelType    tax mpg engineSize
0   Bolt    2016    16000   Manual          24089   Petrol      265 36.2    2.0
1   Bolt    2017    15995   Manual          18615   Petrol      145 36.2    2.0
2   Bolt    2015    13998   Manual          27469   Petrol      265 36.2    2.0
3   Bolt    2017    18998   Manual          14736   Petrol      150 36.2    2.0
4   Bolt    2017    17498   Manual          36284   Petrol      145 36.2    2.0
</code></pre>
<p>I start by dropping all duplicates and encoding the categorical variables:</p>
<pre><code># Drop duplicates
data = data.drop_duplicates(keep=&quot;first&quot;)

# Categorical variable encoding
cat_features = [&quot;model&quot;, &quot;transmission&quot;, &quot;fuelType&quot;]
encoder = LabelEncoder()
encoded = data[cat_features].apply(encoder.fit_transform)
data = data.drop(cat_features, axis=1)
data = pd.concat([encoded, data], axis=1)
</code></pre>
<p>Output:</p>
<pre><code>    model transmission fuelType year    price   mileage tax  mpg    engineSize
0   1     1            3        2016    16000   24089   265  36.2   2.0
1   1     1            3        2017    15995   18615   145 36.2    2.0
2   1     1            3        2015    13998   27469   265 36.2    2.0
3   1     1            3        2017    18998   14736   150 36.2    2.0
4   1     1            3        2017    17498   36284   145 36.2    2.0
</code></pre>
<p>Following the scikit-learn documentation (<a href=""https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a>), I tried regression using Lasso, ElasticNet, Ridge, and SVR.</p>
<p>I got the best results using the Ridge regression (see code below) with R^2 of 0.79 and MSE of 2941.73. However, my success criteria is predicting the price within a certain range of the actual price (e.g. +/- 1000).</p>
<p>Even with the Ridge model below, most predictions don't make the cut. Do you have any ideas how I could optimize the regression? Have I made any mistakes in the Ridge regression below or with the hyperparameters? Are there more appropriate models for this case?</p>
<p>Ridge:</p>
<pre><code>X = data.iloc[:, [0, 1, 2, 3, 5, 6, 7, 8]]
y = data.iloc[:, 4]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

std_slc = StandardScaler()
pca = decomposition.PCA()
ridge = linear_model.Ridge()

pipe = Pipeline(steps=[(&quot;std_slc&quot;, std_slc),
                        (&quot;pca&quot;, pca),
                        (&quot;ridge&quot;, ridge)])

n_components = list(range(1,X.shape[1]+1,1))

parameters = dict(pca__n_components=n_components,
                      ridge__solver=[&quot;auto&quot;, &quot;svd&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sparse_cg&quot;, &quot;sag&quot;, &quot;saga&quot;],
                      ridge__alpha=np.linspace(0, 1, 11),
                      ridge__fit_intercept=[True, False])

clf = GridSearchCV(pipe, parameters, scoring='r2', verbose=1)
clf.fit(X_train, y_train)
y_pred_ridge = clf.predict(X_test)

print(np.sqrt(mean_squared_error(y_test,y_pred_ridge))) 
print(r2_score(y_test, y_pred_ridge))
</code></pre>
<p>Output:</p>
<pre><code>Fitting 5 folds for each of 56 candidates, totalling 280 fits
2941.734786303254
0.7909623313908631
</code></pre>
<p>Voting Regressor:</p>
<pre><code>eclf = VotingRegressor(estimators=[ 
    ('ridge', linear_model.Ridge()),
    ('lasso', linear_model.Lasso()),
    ('elasticnet', linear_model.ElasticNet())
    ])

#Use the key for the classifier followed by __ and the attribute
params_eclf = {'ridge__solver': [&quot;auto&quot;, &quot;svd&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sparse_cg&quot;, &quot;sag&quot;, &quot;saga&quot;],
        'lasso__selection': [&quot;cyclic&quot;, &quot;random&quot;],
        'elasticnet__selection': ['cyclic', 'random'],
        'ridge__alpha': np.linspace(0, 1, 10),
        'ridge__fit_intercept': [True, False]}

grid_eclf = RandomizedSearchCV(estimator=eclf, param_distributions=params_eclf, cv=3, n_iter=250, verbose=1, scoring='r2')

grid_eclf.fit(X_train, y_train)

y_pred_eclf = grid_eclf.predict(X_test)

print(np.sqrt(mean_squared_error(y_test,y_pred_eclf))) 
print(r2_score(y_test, y_pred_eclf))
</code></pre>
<p>Output:</p>
<pre><code>Fitting 3 folds for each of 250 candidates, totalling 750 fits
3082.2257067911637
0.7705191776922907
</code></pre>
",50,1,0,5,python;machine-learning;scikit-learn;regression;supervised-learning,2022-07-26 14:47:18,2022-07-26 14:47:18,2022-07-26 15:38:21,i am trying to predict the price for cars using the following dataframe data  data types  model  transmission  and fuel type as obj  the rest as float int  the full dataset contains   samples  only data head   listed here  i start by dropping all duplicates and encoding the categorical variables  output  following the scikit learn documentation     i tried regression using lasso  elasticnet  ridge  and svr  i got the best results using the ridge regression  see code below  with r  of   and mse of    however  my success criteria is predicting the price within a certain range of the actual price  e g         even with the ridge model below  most predictions don t make the cut  do you have any ideas how i could optimize the regression  have i made any mistakes in the ridge regression below or with the hyperparameters  are there more appropriate models for this case  ridge  output  voting regressor  output ,predicting car prices with machine learning   advice on best model,trying predict price cars using following dataframe data data types model transmission fuel type obj rest float int full dataset contains samples data head listed start dropping duplicates encoding categorical variables output following scikit learn documentation tried regression using lasso elasticnet ridge svr got best results using ridge regression see code r mse however success criteria predicting price within certain range actual price e g even ridge model predictions make cut ideas could optimize regression made mistakes ridge regression hyperparameters appropriate models case ridge output voting regressor output,predicting car prices machine learning advice best model,predicting car prices machine learning advice best modeltrying predict price cars using following dataframe data data types model transmission fuel type obj rest float int full dataset contains samples data head listed start dropping duplicates encoding categorical variables output following scikit learn documentation tried regression using lasso elasticnet ridge svr got best results using ridge regression see code r mse however success criteria predicting price within certain range actual price e g even ridge model predictions make cut ideas could optimize regression made mistakes ridge regression hyperparameters appropriate models case ridge output voting regressor output,"['predicting', 'car', 'prices', 'machine', 'learning', 'advice', 'best', 'modeltrying', 'predict', 'price', 'cars', 'using', 'following', 'dataframe', 'data', 'data', 'types', 'model', 'transmission', 'fuel', 'type', 'obj', 'rest', 'float', 'int', 'full', 'dataset', 'contains', 'samples', 'data', 'head', 'listed', 'start', 'dropping', 'duplicates', 'encoding', 'categorical', 'variables', 'output', 'following', 'scikit', 'learn', 'documentation', 'tried', 'regression', 'using', 'lasso', 'elasticnet', 'ridge', 'svr', 'got', 'best', 'results', 'using', 'ridge', 'regression', 'see', 'code', 'r', 'mse', 'however', 'success', 'criteria', 'predicting', 'price', 'within', 'certain', 'range', 'actual', 'price', 'e', 'g', 'even', 'ridge', 'model', 'predictions', 'make', 'cut', 'ideas', 'could', 'optimize', 'regression', 'made', 'mistakes', 'ridge', 'regression', 'hyperparameters', 'appropriate', 'models', 'case', 'ridge', 'output', 'voting', 'regressor', 'output']","['predict', 'car', 'price', 'machin', 'learn', 'advic', 'best', 'modeltri', 'predict', 'price', 'car', 'use', 'follow', 'datafram', 'data', 'data', 'type', 'model', 'transmiss', 'fuel', 'type', 'obj', 'rest', 'float', 'int', 'full', 'dataset', 'contain', 'sampl', 'data', 'head', 'list', 'start', 'drop', 'duplic', 'encod', 'categor', 'variabl', 'output', 'follow', 'scikit', 'learn', 'document', 'tri', 'regress', 'use', 'lasso', 'elasticnet', 'ridg', 'svr', 'got', 'best', 'result', 'use', 'ridg', 'regress', 'see', 'code', 'r', 'mse', 'howev', 'success', 'criteria', 'predict', 'price', 'within', 'certain', 'rang', 'actual', 'price', 'e', 'g', 'even', 'ridg', 'model', 'predict', 'make', 'cut', 'idea', 'could', 'optim', 'regress', 'made', 'mistak', 'ridg', 'regress', 'hyperparamet', 'appropri', 'model', 'case', 'ridg', 'output', 'vote', 'regressor', 'output']"
73,80,80,18609189,71649163,Can azureml pass variables from one step to another?,"<p>I have a requirement to use azure machine learning to develop a pipeline. In this pipeline we don't pass data as inputs/outputs but variables (for example a list or an int). I have looked on the Microsoft documentation but could not seem to find something fitting my case. Also tried to use the PipelineData class but could not retrieve my variables.</p>
<ol>
<li>Is this possible?</li>
<li>Is this a good approach?</li>
</ol>
<p>Thanks for your help.</p>
",248,2,0,3,python;azure;azure-machine-learning-studio,2022-03-28 17:35:11,2022-03-28 17:35:11,2022-07-26 12:12:27,i have a requirement to use azure machine learning to develop a pipeline  in this pipeline we don t pass data as inputs outputs but variables  for example a list or an int   i have looked on the microsoft documentation but could not seem to find something fitting my case  also tried to use the pipelinedata class but could not retrieve my variables  thanks for your help ,can azureml pass variables from one step to another ,requirement use azure machine learning develop pipeline pipeline pass data inputs outputs variables example int looked microsoft documentation could seem find something fitting case also tried use pipelinedata class could retrieve variables thanks help,azureml pass variables one step another,azureml pass variables one step anotherrequirement use azure machine learning develop pipeline pipeline pass data inputs outputs variables example int looked microsoft documentation could seem find something fitting case also tried use pipelinedata class could retrieve variables thanks help,"['azureml', 'pass', 'variables', 'one', 'step', 'anotherrequirement', 'use', 'azure', 'machine', 'learning', 'develop', 'pipeline', 'pipeline', 'pass', 'data', 'inputs', 'outputs', 'variables', 'example', 'int', 'looked', 'microsoft', 'documentation', 'could', 'seem', 'find', 'something', 'fitting', 'case', 'also', 'tried', 'use', 'pipelinedata', 'class', 'could', 'retrieve', 'variables', 'thanks', 'help']","['azureml', 'pass', 'variabl', 'one', 'step', 'anotherrequir', 'use', 'azur', 'machin', 'learn', 'develop', 'pipelin', 'pipelin', 'pass', 'data', 'input', 'output', 'variabl', 'exampl', 'int', 'look', 'microsoft', 'document', 'could', 'seem', 'find', 'someth', 'fit', 'case', 'also', 'tri', 'use', 'pipelinedata', 'class', 'could', 'retriev', 'variabl', 'thank', 'help']"
74,81,81,16697682,73115780,Python Machine Learning Beginner Question,"<p>I just started to studying machine learning and I saw a code. I don't know anything about it. Also I don't know how to search it... I am stuck here please help. Here is the example code:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn import datasets, model_selection
import matplotlib.pyplot as plt
import numpy

X, y = datasets.load_diabetes(return_X_y=True)
X = X[:, numpy.newaxis, 2] # I didn't understand this part
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
plt.scatter(X_test, y_test,  color='black')
plt.show()
</code></pre>
<p>Where does 2 come from? What is np.newaxis (I think this is a method which returns None but I am not sure) Also what are these parameters separated with commas inside square brackets?
Please tell me the name of it or explain what it is.
Thank you :)</p>
",29,1,-2,3,python;python-3.x;machine-learning,2022-07-26 01:02:45,2022-07-26 01:02:45,2022-07-26 11:43:28,i just started to studying machine learning and i saw a code  i don t know anything about it  also i don t know how to search it    i am stuck here please help  here is the example code ,python machine learning beginner question,started studying machine learning saw code know anything also know search stuck please help example code,python machine learning beginner question,python machine learning beginner questionstarted studying machine learning saw code know anything also know search stuck please help example code,"['python', 'machine', 'learning', 'beginner', 'questionstarted', 'studying', 'machine', 'learning', 'saw', 'code', 'know', 'anything', 'also', 'know', 'search', 'stuck', 'please', 'help', 'example', 'code']","['python', 'machin', 'learn', 'beginn', 'questionstart', 'studi', 'machin', 'learn', 'saw', 'code', 'know', 'anyth', 'also', 'know', 'search', 'stuck', 'pleas', 'help', 'exampl', 'code']"
75,82,82,19356852,72655408,How to properly write a machine learning algorithm?,"<p>I am new to machine learning, and I wanted to make a model that can predict a time series graph, and I keep getting errors. Is there something I am missing? I find out that I learn from getting reference code, then modifiying it, and learning over time what each component does.</p>
<pre><code>&quot;&quot;&quot; Original Repository (Reference)
https://github.com/nicknochnack/Tensorflow-in-10-Minutes/blob/main/Tensorflow%20in%2010.ipynb
&quot;&quot;&quot;
import pandas as pd
import numpy
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

df = pd.read_csv('Marble &amp; Slope Internal Data.csv')
x = pd.get_dummies(df['Distance Travelled(CM)'])
y = df['Height Off Ground(CM)']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)
x_train.head()
y_train.head()

model = Sequential()
model.add(Dense(units=32, activation='relu',input_dim=25))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))


model.compile(loss='binary_crossentropy', optimizer='sgd', metrics='accuracy')
model.fit(x_train, y_train, epochs=20, batch_size=32)

myline = numpy.linspace(0, 100, 100)

plt.plot(myline, model.predict(myline),color='#ff8003',linewidth=3)
plt.show()
</code></pre>
<p>Output from SHELL</p>
<pre><code>Epoch 1/20

1/1 [==============================] - ETA: 0s - loss: 0.9882 - accuracy: 0.0000e+00
...

...
1/1 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0000e+00
1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.0000e+00
Epoch 20/20
WARNING:tensorflow:Model was constructed with shape (None, 25) for input KerasTensor(type_spec=TensorSpec(shape=(None, 25), dtype=tf.float32, name='dense_input'), name='dense_input', description=&quot;created by layer 'dense_input'&quot;), but it was called on an input with incompatible shape (None,).
Traceback (most recent call last):
  File &quot;C:\Users\___\OneDrive\Documents\Python\ALGA.py&quot;, line 30, in &lt;module&gt;
    plt.plot(myline, model.predict(myline),color='#ff8003',linewidth=3)
  File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\___\AppData\Local\Temp\__autograph_generated_file5t873kv5.py&quot;, line 15, in tf__predict_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
ValueError: in user code:

    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1845, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1834, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1823, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1791, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\input_spec.py&quot;, line 228, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;sequential&quot; (type Sequential).
    
    Input 0 of layer &quot;dense&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)
    
    Call arguments received by layer &quot;sequential&quot; (type Sequential):
       inputs=tf.Tensor(shape=(None,), dtype=float32)
       training=False
       mask=None
</code></pre>
",69,1,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-06-17 09:54:29,2022-06-17 09:54:29,2022-07-26 10:52:11,i am new to machine learning  and i wanted to make a model that can predict a time series graph  and i keep getting errors  is there something i am missing  i find out that i learn from getting reference code  then modifiying it  and learning over time what each component does  output from shell,how to properly write a machine learning algorithm ,machine learning wanted make model predict time series graph keep getting errors something missing find learn getting reference code modifiying learning time component output shell,properly write machine learning algorithm,properly write machine learning algorithmmachine learning wanted make model predict time series graph keep getting errors something missing find learn getting reference code modifiying learning time component output shell,"['properly', 'write', 'machine', 'learning', 'algorithmmachine', 'learning', 'wanted', 'make', 'model', 'predict', 'time', 'series', 'graph', 'keep', 'getting', 'errors', 'something', 'missing', 'find', 'learn', 'getting', 'reference', 'code', 'modifiying', 'learning', 'time', 'component', 'output', 'shell']","['properli', 'write', 'machin', 'learn', 'algorithmmachin', 'learn', 'want', 'make', 'model', 'predict', 'time', 'seri', 'graph', 'keep', 'get', 'error', 'someth', 'miss', 'find', 'learn', 'get', 'refer', 'code', 'modifiy', 'learn', 'time', 'compon', 'output', 'shell']"
76,83,83,9798210,73119302,How to merge the list of words in PySpark dataframe?,"<p>I have a dataframe that contains a list of words and I need to merge them into a single sentence.</p>
<p><strong>Dataframe:</strong></p>
<pre class=""lang-py prettyprint-override""><code>temp = spark.createDataFrame([
    (0, ['Julia', 'is', 'awesome']),
    (2, ['Data-science', 'is','cool']),
    (3, ['Machine','learning'])
], [&quot;id&quot;, &quot;words&quot;])

# +---+------------------------+
# |id |words                   |
# +---+------------------------+
# |0  |[Julia, is, awesome]    |
# |2  |[Data-science, is, cool]|
# |3  |[Machine, learning]     |
# +---+------------------------+

temp.printSchema()
# root
#  |-- id: long (nullable = true)
#  |-- words: array (nullable = true)
#  |    |-- element: string (containsNull = true)
</code></pre>
<p>I am applying the rdd.</p>
<pre class=""lang-py prettyprint-override""><code>rdd_df = temp.rdd.map(lambda x: [x['id'], ' '.join(x['words'])])
spark.createDataFrame(rdd_df, temp.schema).show(10, False)

# +---+---------------------------------------------------------+
# |id |words                                                    |
# +---+---------------------------------------------------------+
# |0  |[ ' J u l i a ' ,   ' i s ' ,   ' a w e s o m e ' ]      |
# |2  |[ ' D a t a - s c i e n c e ' ,   ' i s ' , ' c o o l ' ]|
# |3  |[ ' M a c h i n e ' , ' l e a r n i n g ' ]              |
# +---+---------------------------------------------------------+
</code></pre>
<p>But the above code is not returning the desired output. Is there any other solution that we can apply without the use of RDD?</p>
<p><strong>Desired output:</strong></p>
<pre class=""lang-none prettyprint-override""><code>+---+--------------------+
|id |words               |
+---+--------------------+
|0  |Julia is awesome    |
|1  |Data-science is cool|
|2  |Machine             |
+---+--------------------+
</code></pre>
",22,1,0,5,arrays;string;apache-spark;pyspark;merge,2022-07-26 10:28:06,2022-07-26 10:28:06,2022-07-26 10:51:15,i have a dataframe that contains a list of words and i need to merge them into a single sentence  dataframe  i am applying the rdd  but the above code is not returning the desired output  is there any other solution that we can apply without the use of rdd  desired output ,how to merge the list of words in pyspark dataframe ,dataframe contains need merge single sentence dataframe applying rdd code returning desired output solution apply without use rdd desired output,merge pyspark dataframe,merge pyspark dataframedataframe contains need merge single sentence dataframe applying rdd code returning desired output solution apply without use rdd desired output,"['merge', 'pyspark', 'dataframedataframe', 'contains', 'need', 'merge', 'single', 'sentence', 'dataframe', 'applying', 'rdd', 'code', 'returning', 'desired', 'output', 'solution', 'apply', 'without', 'use', 'rdd', 'desired', 'output']","['merg', 'pyspark', 'dataframedatafram', 'contain', 'need', 'merg', 'singl', 'sentenc', 'datafram', 'appli', 'rdd', 'code', 'return', 'desir', 'output', 'solut', 'appli', 'without', 'use', 'rdd', 'desir', 'output']"
77,84,84,1419775,70911428,"is there an open source, voice trainable speech or sound recognition suite that can differentiate between the sounds of &quot;B-4&quot; and &quot;D-4&quot;?","<p>I'm trying to tackle a challenge which has proven very difficult. I want to build a prototype of a chess game that can receive voice inputs for moving the pieces.</p>
<p>To keep it &quot;simple&quot;, my initial goal is to be able to have a program reliably understand the sounds &quot;A-1&quot; through &quot;A-8&quot; and then through the first 8 letters of the alphabet, representing the 8x8 squares of a chess board.</p>
<p>I tried using several speech-to-text APIs and all have proven to be very unreliable. This is in part due to my own accent, but also because the sounds made by saying things like B4 and C2 are just apparently very difficult to interpret by these APIs.</p>
<p>I've come to believe that it may be more feasible to pre-record the 64 sounds and then have an algorithm attempt to match incoming sounds on one of 64 samples.</p>
<p>The problem is that while using speech-to-text APIs is very straightforward, I have no idea how to approach this alternate path. I've been a web dev for 20 years but have no experience with machine learning or training models. Hopefully this is nothing too crazy, I just want to train an algo with 64 sounds, or maybe there's simply an available speech-recognition software that supports training it with my own peculiar accent and is able to recognize the subtle differences between the sounds of letters and numbers reliably.</p>
<p>Would really appreciate any advice on where to get started with this. Thanks!</p>
",61,1,0,5,speech-recognition;speech-to-text;voice-recognition;google-speech-api;sound-recognition,2022-01-30 04:46:35,2022-01-30 04:46:35,2022-07-26 09:47:25,i m trying to tackle a challenge which has proven very difficult  i want to build a prototype of a chess game that can receive voice inputs for moving the pieces  to keep it  simple   my initial goal is to be able to have a program reliably understand the sounds  a   through  a   and then through the first  letters of the alphabet  representing the x squares of a chess board  i tried using several speech to text apis and all have proven to be very unreliable  this is in part due to my own accent  but also because the sounds made by saying things like b and c are just apparently very difficult to interpret by these apis  i ve come to believe that it may be more feasible to pre record the  sounds and then have an algorithm attempt to match incoming sounds on one of  samples  the problem is that while using speech to text apis is very straightforward  i have no idea how to approach this alternate path  i ve been a web dev for  years but have no experience with machine learning or training models  hopefully this is nothing too crazy  i just want to train an algo with  sounds  or maybe there s simply an available speech recognition software that supports training it with my own peculiar accent and is able to recognize the subtle differences between the sounds of letters and numbers reliably  would really appreciate any advice on where to get started with this  thanks ,is there an open source  voice trainable speech or sound recognition suite that can differentiate between the sounds of  b   and  d   ,trying tackle challenge proven difficult want build prototype chess game receive voice inputs moving pieces keep simple initial goal able program reliably understand sounds first letters alphabet representing x squares chess board tried using several speech text apis proven unreliable part due accent also sounds made saying things like b c apparently difficult interpret apis come believe may feasible pre record sounds algorithm attempt match incoming sounds one samples problem using speech text apis straightforward idea approach alternate path web dev years experience machine learning training models hopefully nothing crazy want train algo sounds maybe simply available speech recognition software supports training peculiar accent able recognize subtle differences sounds letters numbers reliably would really appreciate advice get started thanks,open source voice trainable speech sound recognition suite differentiate sounds b,open source voice trainable speech sound recognition suite differentiate sounds btrying tackle challenge proven difficult want build prototype chess game receive voice inputs moving pieces keep simple initial goal able program reliably understand sounds first letters alphabet representing x squares chess board tried using several speech text apis proven unreliable part due accent also sounds made saying things like b c apparently difficult interpret apis come believe may feasible pre record sounds algorithm attempt match incoming sounds one samples problem using speech text apis straightforward idea approach alternate path web dev years experience machine learning training models hopefully nothing crazy want train algo sounds maybe simply available speech recognition software supports training peculiar accent able recognize subtle differences sounds letters numbers reliably would really appreciate advice get started thanks,"['open', 'source', 'voice', 'trainable', 'speech', 'sound', 'recognition', 'suite', 'differentiate', 'sounds', 'btrying', 'tackle', 'challenge', 'proven', 'difficult', 'want', 'build', 'prototype', 'chess', 'game', 'receive', 'voice', 'inputs', 'moving', 'pieces', 'keep', 'simple', 'initial', 'goal', 'able', 'program', 'reliably', 'understand', 'sounds', 'first', 'letters', 'alphabet', 'representing', 'x', 'squares', 'chess', 'board', 'tried', 'using', 'several', 'speech', 'text', 'apis', 'proven', 'unreliable', 'part', 'due', 'accent', 'also', 'sounds', 'made', 'saying', 'things', 'like', 'b', 'c', 'apparently', 'difficult', 'interpret', 'apis', 'come', 'believe', 'may', 'feasible', 'pre', 'record', 'sounds', 'algorithm', 'attempt', 'match', 'incoming', 'sounds', 'one', 'samples', 'problem', 'using', 'speech', 'text', 'apis', 'straightforward', 'idea', 'approach', 'alternate', 'path', 'web', 'dev', 'years', 'experience', 'machine', 'learning', 'training', 'models', 'hopefully', 'nothing', 'crazy', 'want', 'train', 'algo', 'sounds', 'maybe', 'simply', 'available', 'speech', 'recognition', 'software', 'supports', 'training', 'peculiar', 'accent', 'able', 'recognize', 'subtle', 'differences', 'sounds', 'letters', 'numbers', 'reliably', 'would', 'really', 'appreciate', 'advice', 'get', 'started', 'thanks']","['open', 'sourc', 'voic', 'trainabl', 'speech', 'sound', 'recognit', 'suit', 'differenti', 'sound', 'btri', 'tackl', 'challeng', 'proven', 'difficult', 'want', 'build', 'prototyp', 'chess', 'game', 'receiv', 'voic', 'input', 'move', 'piec', 'keep', 'simpl', 'initi', 'goal', 'abl', 'program', 'reliabl', 'understand', 'sound', 'first', 'letter', 'alphabet', 'repres', 'x', 'squar', 'chess', 'board', 'tri', 'use', 'sever', 'speech', 'text', 'api', 'proven', 'unreli', 'part', 'due', 'accent', 'also', 'sound', 'made', 'say', 'thing', 'like', 'b', 'c', 'appar', 'difficult', 'interpret', 'api', 'come', 'believ', 'may', 'feasibl', 'pre', 'record', 'sound', 'algorithm', 'attempt', 'match', 'incom', 'sound', 'one', 'sampl', 'problem', 'use', 'speech', 'text', 'api', 'straightforward', 'idea', 'approach', 'altern', 'path', 'web', 'dev', 'year', 'experi', 'machin', 'learn', 'train', 'model', 'hope', 'noth', 'crazi', 'want', 'train', 'algo', 'sound', 'mayb', 'simpli', 'avail', 'speech', 'recognit', 'softwar', 'support', 'train', 'peculiar', 'accent', 'abl', 'recogn', 'subtl', 'differ', 'sound', 'letter', 'number', 'reliabl', 'would', 'realli', 'appreci', 'advic', 'get', 'start', 'thank']"
78,85,85,14514085,73114943,How I use numerical methods to calculate roots in R,"<p>A machine learning model predicted probability <code>p</code> using input <code>x</code>. It is unknown how model calculates the probability.</p>
<p>In the example below,</p>
<p>We have 100 <code>x</code>and <code>p</code> values.</p>
<p>Can someone please show an algorithm to find all values of x for which <code>p</code> is 0.5.</p>
<p>There are two challenges</p>
<ol>
<li>I don't know the function <code>p = f(x)</code>. I don't wish to fit some smooth polynomial curves which will remove the noise. The noises are important.</li>
<li><code>x</code> values are discrete. So, we need to interpolate to find the desired values of <code>x</code>.</li>
</ol>
<pre><code>library(tidyverse)

x &lt;- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0, 2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8,3.9,4.0,4.1, 4.2,4.3,4.4,4.5,4.6,4.7,4.8,4.9,5.0,5.1,5.2,5.3,5.4,5.5,5.6,5.7,5.8,5.9,6.0,6.1,6.2, 6.3,6.4,6.5,6.6,6.7,6.8,6.9,7.0,7.1,7.2,7.3,7.4,7.5,7.6,7.7,7.8,7.9,8.0,8.1,8.2,8.3, 8.4,8.5,8.6,8.7,8.8,8.9,9.0,9.1,9.2,9.3,9.4,9.5,9.6,9.7,9.8,9.9, 10.0)
p &lt;- c(0.69385203,0.67153592,0.64868391,0.72205029,0.64917218,0.66818861,0.55532616,0.58631660,0.65013198,0.53695673,0.57401464,0.57812980,0.39889101,0.41922821,0.44022287,0.48610191,0.34235438,0.30877592,0.20408235,0.17221558,0.23667792,0.29237938,0.10278049,0.20981142,0.08563396,0.12080935,0.03266140,0.12362265,0.11210208,0.08364931,0.04746024,0.14754152,0.09865584,0.16588175,0.16581508,0.14036209,0.20431540,0.19971309,0.23336415,0.12444293,0.14120138,0.21566896,0.18490258,0.34261082,0.38338941,0.41828079,0.34217964,0.38137610,0.41641546,0.58767796,0.45473784,0.60015956,0.63484702,0.55080768,0.60981219,0.71217369,0.60736818,0.78073246,0.68643671,0.79230105,0.76443958,0.74410139,0.63418201,0.64126278,0.63164615,0.68326471,0.68154362,0.75890922,0.72917978,0.55839943,0.55452549,0.69419777,0.64160572,0.63205751,0.60118916,0.40162340,0.38523375,0.39309260,0.47021037,0.33391614,0.22400555,0.20929558,0.20003229,0.15848124,0.11589228,0.13326047,0.11848593,0.17024106,0.11184393,0.12506915,0.07740497,0.02548386,0.07381765,0.02610759,0.13271803,0.07034573,0.02549706,0.02503864,0.11621910,0.08636754)


tbl &lt;- tibble(x, p)


# plot for visualization
ggplot(data = tbl,
       aes(x = x,
           y = p)) + 
  geom_line() + 
  geom_point() + 
  geom_hline(yintercept = 0.5) +
  theme_bw() + 
  theme(aspect.ratio = 0.4) 

</code></pre>
<p>The figure below shows that there are five roots.</p>
<p><a href=""https://i.stack.imgur.com/kSRpw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kSRpw.png"" alt=""enter image description here"" /></a></p>
",52,1,0,3,r;optimization;numerical-methods,2022-07-25 23:22:43,2022-07-25 23:22:43,2022-07-26 05:31:14,a machine learning model predicted probability p using input x  it is unknown how model calculates the probability  in the example below  we have  xand p values  can someone please show an algorithm to find all values of x for which p is    there are two challenges the figure below shows that there are five roots  ,how i use numerical methods to calculate roots in r,machine learning model predicted probability p using input x unknown model calculates probability example xand p values someone please show algorithm find values x p two challenges figure shows five roots,use numerical methods calculate roots r,use numerical methods calculate roots rmachine learning model predicted probability p using input x unknown model calculates probability example xand p values someone please show algorithm find values x p two challenges figure shows five roots,"['use', 'numerical', 'methods', 'calculate', 'roots', 'rmachine', 'learning', 'model', 'predicted', 'probability', 'p', 'using', 'input', 'x', 'unknown', 'model', 'calculates', 'probability', 'example', 'xand', 'p', 'values', 'someone', 'please', 'show', 'algorithm', 'find', 'values', 'x', 'p', 'two', 'challenges', 'figure', 'shows', 'five', 'roots']","['use', 'numer', 'method', 'calcul', 'root', 'rmachin', 'learn', 'model', 'predict', 'probabl', 'p', 'use', 'input', 'x', 'unknown', 'model', 'calcul', 'probabl', 'exampl', 'xand', 'p', 'valu', 'someon', 'pleas', 'show', 'algorithm', 'find', 'valu', 'x', 'p', 'two', 'challeng', 'figur', 'show', 'five', 'root']"
79,86,86,14514085,73087587,How I use numerical methods to calculate roots in R,"<p>Edit  - The question was modified and asked again. Here is the link  -
<a href=""https://stackoverflow.com/questions/73114943/how-i-use-numerical-methods-to-calculate-roots-in-r"">How I use numerical methods to calculate roots in R</a></p>
<p>Some machine learning model was used to predict probability <code>p</code> using input <code>x</code>.
The input <code>x</code> and predicted probability <code>p</code> are in a tibble. We don't know how ML model calculated the probability <code>p</code> using input <code>x</code>.</p>
<p>For this example below, I have assumed a function to calculate <code>p</code> from <code>x</code>. I have intentionally added noise in the function bso that we cannot solve the equation analytically.</p>
<p>Can some please tell me how do I calculate value of <code>x</code> for which <code>p = 0.5</code>.
(I know that we have to use numerical methods to calculate roots since the function <code>p = f(x)</code> is not known in real problem)</p>
<p>Note - I don't know the suitable tags for this, please help me add the tags so the question can reach to the write people for help.</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

x &lt;- seq(from = 0, to = 10, by = 0.1)
y &lt;- (1.2 + cos(x))/3 + runif(n = 101, min = -0.1, max = 0.1)

# tibble
tbl &lt;- tibble(x = x, 
              y = y)

# plot for illustration
ggplot(data = tbl,
       aes(x = x,
           y = y)) + 
  geom_line() + 
  theme_bw() + 
  theme(aspect.ratio = 0.4)
</code></pre>
<p>Find roots of y - 0.5 = 0</p>
",61,0,-1,3,r;optimization;numerical-methods,2022-07-23 05:15:06,2022-07-23 05:15:06,2022-07-26 04:22:30,for this example below  i have assumed a function to calculate p from x  i have intentionally added noise in the function bso that we cannot solve the equation analytically  note   i don t know the suitable tags for this  please help me add the tags so the question can reach to the write people for help  find roots of y       ,how i use numerical methods to calculate roots in r,example assumed function calculate p x intentionally added noise function bso cannot solve equation analytically note know suitable tags please help tags question reach write people help find roots,use numerical methods calculate roots r,use numerical methods calculate roots rexample assumed function calculate p x intentionally added noise function bso cannot solve equation analytically note know suitable tags please help tags question reach write people help find roots,"['use', 'numerical', 'methods', 'calculate', 'roots', 'rexample', 'assumed', 'function', 'calculate', 'p', 'x', 'intentionally', 'added', 'noise', 'function', 'bso', 'can', 'not', 'solve', 'equation', 'analytically', 'note', 'know', 'suitable', 'tags', 'please', 'help', 'tags', 'question', 'reach', 'write', 'people', 'help', 'find', 'roots']","['use', 'numer', 'method', 'calcul', 'root', 'rexampl', 'assum', 'function', 'calcul', 'p', 'x', 'intent', 'ad', 'nois', 'function', 'bso', 'can', 'not', 'solv', 'equat', 'analyt', 'note', 'know', 'suitabl', 'tag', 'pleas', 'help', 'tag', 'question', 'reach', 'write', 'peopl', 'help', 'find', 'root']"
80,88,88,16525160,73116455,"X[:,1]=le_Area_Unit.transform(X[:,1]) AttributeError: &#39;list&#39; object has no attribute &#39;transform&#39;","<p>I'm newbie in machine learning and web application. I try to run this prediction model on localhost,
from the begining It successfully run on Jupyter notebook. Then, I copy code to VScode using  pickle and streamlit. After save VScode and refresh localhost. Then,I got error</p>
<p><strong>&quot;X[:,1]=le_Area_Unit.transform(X[:,1])
AttributeError: 'list' object has no attribute 'transform'&quot;</strong></p>
<p>I'm following this video and use my dataset
<a href=""https://www.youtube.com/watch?v=xl0N7tHiwlw"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=xl0N7tHiwlw</a>
What am I did wrong?</p>
<p>here is my code.</p>
<pre><code>import numpy as np import streamlit as st import pickle 
 
 
def load_model():
     with open('save_steps.pkl','rb') as file:
         data= pickle.load(file)
         return data
 
 data = load_model() cls_rf_loaded= data[&quot;model&quot;]
 le_Area_Unit=data[&quot;le_Area_Unit&quot;]
 
 def show_predict_page():
 
 le_Area_Unit=[&quot;Adventure&quot;,&quot;Akatarawa&quot;,&quot;Allen Road&quot;,&quot;Allenton
 West&quot;,&quot;Appleby-Kew&quot;,&quot;Aramoana&quot;,&quot;Aranui&quot;]
 
 
 Month =(1,2,3,4,5,6,7,8,9,10,11,12) 
day ={1: &quot;Sunday&quot;,2: &quot;Monday&quot;,3:&quot;Tuesday&quot;,4: &quot;Wednesday&quot;,5: &quot;Thursday&quot;,6: &quot;Friday&quot;,7:
 &quot;Saturday&quot;,9:&quot;UNKNOWN&quot;} 
time ={1: &quot;00:00-3.59&quot;,2: &quot;04:00-07:59&quot;,3:
 &quot;08.00-11:59&quot;,4: &quot;12:00-15:59&quot;,5: &quot;16:00-19:59&quot;,6: &quot;20:00-23:59&quot;}
 Locn_Type_DivisionID={1,2,3,4} Types_of_Crime={1,2,3,4,5,6}
 
 Month = st.selectbox(&quot;Month&quot;,Month) City=st.selectbox(&quot;Area
 unit&quot;,le_Area_Unit) day=st.selectbox(&quot;day&quot;,day)
 time=st.selectbox(&quot;time&quot;,time)
 Locn_Type_DivisionID=st.selectbox(&quot;Location Type&quot;,Locn_Type_DivisionID) Types_of_Crime=st.selectbox(&quot;Types of Crime&quot;,Types_of_Crime)
 
 ok=st.button(&quot;Click Here to Predict Crime case!!&quot;) if ok:
     X=np.array([[Month,City,day,time,Locn_Type_DivisionID,Types_of_Crime]])
 
 X[:,1]=le_Area_Unit.transform(X[:,1]) 
X=X.astype(float) 
print(X) 
Crime = cls_rf_loaded.predict(X) st.subheader(f&quot;crime case is....... {Crime[0]:.0f}&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/6fJPn.png"" rel=""nofollow noreferrer"">jupyter notebook code2</a></p>
<p><a href=""https://i.stack.imgur.com/1xiXL.png"" rel=""nofollow noreferrer"">jupyter notebook code</a></p>
<p>Thank you. I stuck this part long time already. </p>
",32,0,0,3,python;machine-learning;streamlit,2022-07-26 03:00:05,2022-07-26 03:00:05,2022-07-26 03:13:20,here is my code    thank you  i stuck this part long time already   ,x     le_area_unit transform x      attributeerror     list    object has no attribute    transform   ,code thank stuck part long time already,x le_area_unit transform x attributeerror object attribute transform,x le_area_unit transform x attributeerror object attribute transformcode thank stuck part long time already,"['x', 'le_area_unit', 'transform', 'x', 'attributeerror', 'object', 'attribute', 'transformcode', 'thank', 'stuck', 'part', 'long', 'time', 'already']","['x', 'le_area_unit', 'transform', 'x', 'attributeerror', 'object', 'attribut', 'transformcod', 'thank', 'stuck', 'part', 'long', 'time', 'alreadi']"
81,89,89,5452008,37172289,How would you interpret an ensemble tree model?,"<p>In machine learning ensemble tree models such as random forest are common. This models consist of an ensemble of so called decision tree models. How can we analyse, however, what those models have specifically learned?</p>
",140,2,0,3,machine-learning;random-forest;ensemble-learning,2016-05-11 23:04:37,2016-05-11 23:04:37,2022-07-26 00:13:10,in machine learning ensemble tree models such as random forest are common  this models consist of an ensemble of so called decision tree models  how can we analyse  however  what those models have specifically learned ,how would you interpret an ensemble tree model ,machine learning ensemble tree models random forest common models consist ensemble called decision tree models analyse however models specifically learned,would interpret ensemble tree model,would interpret ensemble tree modelmachine learning ensemble tree models random forest common models consist ensemble called decision tree models analyse however models specifically learned,"['would', 'interpret', 'ensemble', 'tree', 'modelmachine', 'learning', 'ensemble', 'tree', 'models', 'random', 'forest', 'common', 'models', 'consist', 'ensemble', 'called', 'decision', 'tree', 'models', 'analyse', 'however', 'models', 'specifically', 'learned']","['would', 'interpret', 'ensembl', 'tree', 'modelmachin', 'learn', 'ensembl', 'tree', 'model', 'random', 'forest', 'common', 'model', 'consist', 'ensembl', 'call', 'decis', 'tree', 'model', 'analys', 'howev', 'model', 'specif', 'learn']"
82,90,90,19604002,73085670,flask swagger is executing but not showing results in browser,"<p><a href=""https://i.stack.imgur.com/HLmFt.png"" rel=""nofollow noreferrer"">enter image description here</a>I have a flask app and im trying to use <code>flasgger</code> for Swagger Docs:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle
from flask import Flask, request
from flasgger import Swagger
import numpy as np
import pandas as pd

with open('/media/arpon/allin1/Deploy Machine Learning &amp; NLP Models with Dockers (DevOps)/Deploy Machine Learning &amp; NLP Models with Dockers (DevOps)/iris/rf.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

app = Flask(__name__)
swagger = Swagger(app)

@app.route('/predict')
def predict_iris():
    &quot;&quot;&quot;Example endpoint returning a prediction of iris
    ---
    parameters:
      - name: s_length
        in: query
        type: integer
        required: true
      - name: s_width
        in: query
        type: integer
        required: true
      - name: p_length
        in: query
        type: integer
        required: true
      - name: p_width
        in: query
        type: integer
        required: true

    &quot;&quot;&quot;
    s_length = request.args.get(&quot;s_length&quot;)
    s_width = request.args.get(&quot;s_width&quot;)
    p_length = request.args.get(&quot;p_length&quot;)
    p_width = request.args.get(&quot;p_width&quot;)
    
    prediction = model.predict(np.array([[s_length, s_width, p_length, p_width]]))
    return str(prediction)

@app.route('/predict_file', methods=[&quot;POST&quot;])
def predict_iris_file():
    &quot;&quot;&quot;Example file endpoint returning a prediction of iris
    ---
    parameters:
      - name: input_file
        in: formData
        type: file
        required: true   
        
    &quot;&quot;&quot;
    input_data = pd.read_csv(request.files.get(&quot;input_file&quot;), header=None)
    prediction = model.predict(input_data)
    return str(list(prediction))
if __name__ == '__main__':
    app.run()
</code></pre>
<p>i also add</p>
<pre><code>responses:
200:
</code></pre>
<p>but after adding this i got Fetch error. swagger is not starting and without this responses app is running but not giving any output.</p>
",33,0,0,3,python;flask;swagger,2022-07-22 23:15:06,2022-07-22 23:15:06,2022-07-26 00:12:36,i have a flask app and im trying to use flasgger for swagger docs  i also add but after adding this i got fetch error  swagger is not starting and without this responses app is running but not giving any output ,flask swagger is executing but not showing results in browser,flask app im trying use flasgger swagger docs also adding got fetch error swagger starting without responses app running giving output,flask swagger executing showing results browser,flask swagger executing showing results browserflask app im trying use flasgger swagger docs also adding got fetch error swagger starting without responses app running giving output,"['flask', 'swagger', 'executing', 'showing', 'results', 'browserflask', 'app', 'im', 'trying', 'use', 'flasgger', 'swagger', 'docs', 'also', 'adding', 'got', 'fetch', 'error', 'swagger', 'starting', 'without', 'responses', 'app', 'running', 'giving', 'output']","['flask', 'swagger', 'execut', 'show', 'result', 'browserflask', 'app', 'im', 'tri', 'use', 'flasgger', 'swagger', 'doc', 'also', 'ad', 'got', 'fetch', 'error', 'swagger', 'start', 'without', 'respons', 'app', 'run', 'give', 'output']"
83,91,91,16251325,73115011,Populate from database and export array,"<p>I'm new to react. I have data in my MongoDB collection. I can fetch this data. Now I'm using a template that I found online. It has a Grid component that I'm using to display all my data.</p>
<p>Here's how I'm fetching the data</p>
<pre><code>const [recruitList, setRecruitList] = useState([]);
useEffect(() =&gt; {
  axios.get(&quot;http://localhost:8080/read&quot;).then((res) =&gt; {
    setRecruitList(res.data);
  });
}, []);
</code></pre>
<p>Now, I want to populate this variable with as many rows or objects as there are in my collection.</p>
<pre><code>export const recruitsData = [
  {
    Degree: 10248,
    RecruitName: 'Vinet',
    RecruitEmail: 32.38,
    PhoneNumber: '51481369',
    Skills: 'USA',
    Status: 'pending',
    StatusBg: '#FB9678',
    ProductImage:
      product6,
  },
];
</code></pre>
<p>For now, I'm just passing static data like this. I want to actually populate it with the data I'm getting from MongoDB.</p>
<p>Here's what the localhost:8080/read gives</p>
<pre><code>{
  &quot;_id&quot;: {
    &quot;$oid&quot;: &quot;62dc175ee1c7efe9fca97968&quot;
  },
  &quot;college_name&quot;: null,
  &quot;company_names&quot;: [
    &quot;Marathwada Mitra Mandals College of Engineering&quot;
  ],
  &quot;degree&quot;: [
    &quot;B.E. IN COMPUTER ENGINEERING&quot;
  ],
  &quot;designation&quot;: [
    &quot;Machine Learning&quot;,
    &quot;Schlumberger\nDATA ENGINEER&quot;,
    &quot;TECHNICAL CONTENT WRITER&quot;
  ],
  &quot;email&quot;: &quot;omkarpathak27@gmail.com&quot;,
  &quot;experience&quot;: [
    &quot;Schlumberger&quot;,
    &quot;DATA ENGINEER&quot;,
    &quot;July 2018 - Present&quot;,
    &quot; Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which&quot;,
    &quot;Pune, Maharashtra, India&quot;,
    &quot;brought migration time to 1.5 hours from 9 hours without any manual intervention&quot;,
    &quot; Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business&quot;,
    &quot;units&quot;,
    &quot; Working with L3 developer team to ensure the discussed Scrum PBIs are delivered on time for data ingestions&quot;,
    &quot; Planning and Executing QA and Production Release Cycle activities&quot;,
    &quot;Truso&quot;,
    &quot;FULL STACK DEVELOPER INTERN&quot;,
    &quot; Created RESTful apis&quot;,
    &quot; Tried my hands on Angular 5/6&quot;,
    &quot; Was responsible for Django backend development&quot;,
    &quot;Pune, Maharashtra, India&quot;,
    &quot;June 2018 - July 2018&quot;,
    &quot;Propeluss&quot;,
    &quot;DATA ENGINEERING INTERN&quot;,
    &quot; Wrote various automation scripts to scrape data from various websites.&quot;,
    &quot; Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity&quot;,
    &quot;Pune, Maharashtra, India&quot;,
    &quot;October 2017 - January 2018&quot;,
    &quot;extraction algorithms and applying Machine Learning to classify these articles.&quot;,
    &quot; Also applied KNN with LSA for extracting relevant tags for various startups based on their works.&quot;,
    &quot;GeeksForGeeks&quot;,
    &quot;TECHNICAL CONTENT WRITER&quot;,
    &quot; Published 4 articles for the topics such as Data Structures and Algorithms and Python&quot;,
    &quot;Pune, Maharashtra, India&quot;,
    &quot;July 2017 - September 2017&quot;,
    &quot;Softtestlab Technologies&quot;,
    &quot;WEB DEVELOPER INTERN&quot;,
    &quot; Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes&quot;,
    &quot; Worked on a live project for creating closure reports using PHP and Excel&quot;
  ],
  &quot;mobile_number&quot;: &quot;8087996634&quot;,
  &quot;name&quot;: &quot;Omkar Pathak&quot;,
  &quot;no_of_pages&quot;: &quot;3&quot;,
  &quot;skills&quot;: [
    &quot;Parser&quot;,
    &quot;Html&quot;,
    &quot;Css&quot;,
    &quot;Javascript&quot;,
    &quot;Operating systems&quot;,
    &quot;Github&quot;,
    &quot;Website&quot;,
    &quot;Flask&quot;,
    &quot;Linux&quot;,
    &quot;System&quot;,
    &quot;Shell&quot;,
    &quot;Apis&quot;,
    &quot;Reports&quot;,
    &quot;Engineering&quot;,
    &quot;Opencv&quot;,
    &quot;C&quot;,
    &quot;Writing&quot;,
    &quot;Php&quot;,
    &quot;Cloud&quot;,
    &quot;Analytics&quot;,
    &quot;Scrum&quot;,
    &quot;Algorithms&quot;,
    &quot;Django&quot;,
    &quot;Windows&quot;,
    &quot;Automation&quot;,
    &quot;Unix&quot;,
    &quot;Photography&quot;,
    &quot;Content&quot;,
    &quot;Testing&quot;,
    &quot;C++&quot;,
    &quot;Machine learning&quot;,
    &quot;Mysql&quot;,
    &quot;Auditing&quot;,
    &quot;Python&quot;,
    &quot;Excel&quot;,
    &quot;Training&quot;,
    &quot;Migration&quot;,
    &quot;Programming&quot;,
    &quot;Security&quot;,
    &quot;Technical&quot;,
    &quot;Api&quot;
  ],
  &quot;total_experience&quot;: &quot;4.5&quot;,
  &quot;__v&quot;: 0
}
</code></pre>
<p>I have many other &quot;rows&quot; such as this one. How do I populate my recruitData array with them. Thanks a lot in advance.</p>
<p><strong>EDIT: Forgot to add the Grid structure, Here it is</strong></p>
<pre><code>export const ordersGrid = [
  {
    headerText: 'Image',
    template: gridOrderImage,
    textAlign: 'Center',
    width: '120',
  },
  {
    field: 'RecruitName',
    headerText: 'Name',
    width: '150',
    editType: 'dropdownedit',
    textAlign: 'Center',
  },
  { field: 'RecruitEmail',
    headerText: 'Email',
    width: '150',
    textAlign: 'Center',
  },
  { field: 'PhoneNumber',
    headerText: 'Phone Number',
    width: '150',
    textAlign: 'Center',
  },
  {
    headerText: 'Status',
    template: gridOrderStatus,
    field: 'OrderItems',
    textAlign: 'Center',
    width: '120',
  },
  {
    field: 'Degree',
    headerText: 'Degree',
    width: '120',
    textAlign: 'Center',
  },

  {
    field: 'Skills',
    headerText: 'Skills',
    width: '150',
    textAlign: 'Center',
  },
];
</code></pre>
",35,0,0,1,reactjs,2022-07-25 23:29:31,2022-07-25 23:29:31,2022-07-25 23:38:46,i m new to react  i have data in my mongodb collection  i can fetch this data  now i m using a template that i found online  it has a grid component that i m using to display all my data  here s how i m fetching the data now  i want to populate this variable with as many rows or objects as there are in my collection  for now  i m just passing static data like this  i want to actually populate it with the data i m getting from mongodb  here s what the localhost  read gives i have many other  rows  such as this one  how do i populate my recruitdata array with them  thanks a lot in advance  edit  forgot to add the grid structure  here it is,populate from database and export array,react data mongodb collection fetch data using template found online grid component using display data fetching data want populate variable many rows objects collection passing static data like want actually populate data getting mongodb localhost read gives many rows one populate recruitdata array thanks lot advance edit forgot grid structure,populate database export array,populate database export arrayreact data mongodb collection fetch data using template found online grid component using display data fetching data want populate variable many rows objects collection passing static data like want actually populate data getting mongodb localhost read gives many rows one populate recruitdata array thanks lot advance edit forgot grid structure,"['populate', 'database', 'export', 'arrayreact', 'data', 'mongodb', 'collection', 'fetch', 'data', 'using', 'template', 'found', 'online', 'grid', 'component', 'using', 'display', 'data', 'fetching', 'data', 'want', 'populate', 'variable', 'many', 'rows', 'objects', 'collection', 'passing', 'static', 'data', 'like', 'want', 'actually', 'populate', 'data', 'getting', 'mongodb', 'localhost', 'read', 'gives', 'many', 'rows', 'one', 'populate', 'recruitdata', 'array', 'thanks', 'lot', 'advance', 'edit', 'forgot', 'grid', 'structure']","['popul', 'databas', 'export', 'arrayreact', 'data', 'mongodb', 'collect', 'fetch', 'data', 'use', 'templat', 'found', 'onlin', 'grid', 'compon', 'use', 'display', 'data', 'fetch', 'data', 'want', 'popul', 'variabl', 'mani', 'row', 'object', 'collect', 'pass', 'static', 'data', 'like', 'want', 'actual', 'popul', 'data', 'get', 'mongodb', 'localhost', 'read', 'give', 'mani', 'row', 'one', 'popul', 'recruitdata', 'array', 'thank', 'lot', 'advanc', 'edit', 'forgot', 'grid', 'structur']"
84,92,92,16778823,73115054,Object `train_cats()` not found,"<p>i'm studing machine learning course and use fastai library and when i call train_cats() functioin i get this error Object <code>train_cats()</code> not found.</p>
",11,0,-3,3,machine-learning;deep-learning;fast-ai,2022-07-25 23:33:17,2022-07-25 23:33:17,2022-07-25 23:33:17,i m studing machine learning course and use fastai library and when i call train_cats   functioin i get this error object train_cats   not found ,object  train_cats    not found,studing machine learning course use fastai library call train_cats functioin get error object train_cats found,object train_cats found,object train_cats foundstuding machine learning course use fastai library call train_cats functioin get error object train_cats found,"['object', 'train_cats', 'foundstuding', 'machine', 'learning', 'course', 'use', 'fastai', 'library', 'call', 'train_cats', 'functioin', 'get', 'error', 'object', 'train_cats', 'found']","['object', 'train_cat', 'foundstud', 'machin', 'learn', 'cours', 'use', 'fastai', 'librari', 'call', 'train_cat', 'functioin', 'get', 'error', 'object', 'train_cat', 'found']"
85,93,93,19620398,73114878,How can I use python machine learning to complete missing data in excel,"<p>I've tried searching the internet, and I can find solutions to the two parts of my puzzle, but I can't find a way of combining them!</p>
<p>I have a set of excel files, and I need to manually go through them to fill in missing data using an additional new column.   There's probably a couple of million rows in total across the files.</p>
<p><strong>Part 1 - Adding the new data</strong> -
I have found many great methods for appending data into a new column using python, typically using openpyxl.  No problem!</p>
<p><strong>Part 2 - Machine learning to extract the data</strong> -
I have found some great ML code for text mining (an example being <a href=""https://towardsai.net/p/data-mining/text-mining-in-python-steps-and-examples-78b3f8fd913b"" rel=""nofollow noreferrer"">https://towardsai.net/p/data-mining/text-mining-in-python-steps-and-examples-78b3f8fd913b</a>)</p>
<p>My problem is, that I can't find any method for combining the two.</p>
<p>I'd like to go through the excel files, row by row, mining text that appears across multiple columns (which is more often than not a greater string length than the text I need), and then add text into a new column at the end with the text I need.</p>
<p>If you know of a method to do this, please let me know!!</p>
",31,0,-2,4,python;excel;machine-learning;text-mining,2022-07-25 23:15:03,2022-07-25 23:15:03,2022-07-25 23:15:03,i ve tried searching the internet  and i can find solutions to the two parts of my puzzle  but i can t find a way of combining them  i have a set of excel files  and i need to manually go through them to fill in missing data using an additional new column    there s probably a couple of million rows in total across the files  my problem is  that i can t find any method for combining the two  i d like to go through the excel files  row by row  mining text that appears across multiple columns  which is more often than not a greater string length than the text i need   and then add text into a new column at the end with the text i need  if you know of a method to do this  please let me know  ,how can i use python machine learning to complete missing data in excel,tried searching internet find solutions two parts puzzle find way combining set excel files need manually go fill missing data using additional column probably couple million rows total across files problem find method combining two like go excel files row row mining text appears across multiple columns often greater string length text need text column end text need know method please let know,use python machine learning complete missing data excel,use python machine learning complete missing data exceltried searching internet find solutions two parts puzzle find way combining set excel files need manually go fill missing data using additional column probably couple million rows total across files problem find method combining two like go excel files row row mining text appears across multiple columns often greater string length text need text column end text need know method please let know,"['use', 'python', 'machine', 'learning', 'complete', 'missing', 'data', 'exceltried', 'searching', 'internet', 'find', 'solutions', 'two', 'parts', 'puzzle', 'find', 'way', 'combining', 'set', 'excel', 'files', 'need', 'manually', 'go', 'fill', 'missing', 'data', 'using', 'additional', 'column', 'probably', 'couple', 'million', 'rows', 'total', 'across', 'files', 'problem', 'find', 'method', 'combining', 'two', 'like', 'go', 'excel', 'files', 'row', 'row', 'mining', 'text', 'appears', 'across', 'multiple', 'columns', 'often', 'greater', 'string', 'length', 'text', 'need', 'text', 'column', 'end', 'text', 'need', 'know', 'method', 'please', 'let', 'know']","['use', 'python', 'machin', 'learn', 'complet', 'miss', 'data', 'exceltri', 'search', 'internet', 'find', 'solut', 'two', 'part', 'puzzl', 'find', 'way', 'combin', 'set', 'excel', 'file', 'need', 'manual', 'go', 'fill', 'miss', 'data', 'use', 'addit', 'column', 'probabl', 'coupl', 'million', 'row', 'total', 'across', 'file', 'problem', 'find', 'method', 'combin', 'two', 'like', 'go', 'excel', 'file', 'row', 'row', 'mine', 'text', 'appear', 'across', 'multipl', 'column', 'often', 'greater', 'string', 'length', 'text', 'need', 'text', 'column', 'end', 'text', 'need', 'know', 'method', 'pleas', 'let', 'know']"
86,94,94,1999833,72797783,Machine Learning to predict time-series multi-class signal changes,"<p>I would like to predict the switching behavior of time-dependent signals. Currently the signal has 3 states (1, 2, 3), but it could be that this will change in the future. For the moment, however, it is absolutely okay to assume three states.</p>
<p>I can make the following assumptions about these states (see picture):</p>
<ol>
<li>the signals repeat periodically, possibly with variations concerning the time of day.</li>
<li>the duration of state 2 is always constant and relatively short for all signals.</li>
<li>the duration of states 1 and 3 are also constant, but vary for the different signals.</li>
<li>the switching sequence is always the same: 1 --&gt; 2 --&gt; 3 --&gt; 2 --&gt; 1 --&gt; [...]</li>
<li>there is a constant but unknown time reference between the different signals.</li>
<li>There is no constant time reference between my observations for the different signals. They are simply measured one after the other, but always at different times.</li>
<li>I am able to rebuild my model periodically after i obtained more samples.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/0EMVo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0EMVo.png"" alt=""Sketch of signal model and observations"" /></a></p>
<p>I have the following problems:</p>
<ol>
<li>I can only observe one signal at a time.</li>
<li>I can only observe the signals at different times.</li>
<li>I cannot trigger my measurement with the state transition. That means, when I measure, I am always &quot;in the middle&quot; of a state. Therefore I don't know when this state has started and also not exactly when this state will end.</li>
<li>I cannot observe a certain signal for a long duration. So, i am not able to observe a complete period.</li>
<li>My samples (observations) are widespread in time.</li>
<li>I would like to get a prediction either for the state change or the current state for the current time. It is likely to happen that i will never have measured my signals for that requested time.</li>
</ol>
<p>So far I have tested the TimeSeriesPredictor from the ML.NET Toolbox, as it seemed suitable to me. However, in my opinion, this algorithm requires that you always pass only the data of one signal. This means that assumption 5 is not included in the prediction, which is probably suboptimal. Also, in this case I had problems with the prediction not changing, which should actually happen time-dependently when I query multiple predictions. This behavior led me to believe that only the order of the values entered the model, but not the associated timestamp. If I have understood everything correctly, then exactly this timestamp is my most important &quot;feature&quot;...
So far, i did not do any tests on Regression-based approaches, e.g. FastTree, since my data is not linear, but keeps changing states. Maybe this assumption is not valid and regression-based methods could also be suitable?</p>
<p>I also don't know if a multiclassifier is required, because I had understood that the TimeSeriesPredictor would also be suitable for this, since it works with the single data type. Whether the prediction is 1.3 or exactly 1.0 would be fine for me.</p>
<p><strong>To sum it up:</strong>
I am looking for a algorithm which is able to recognize the switching patterns based on lose and widespread samples. It would be okay to define boundaries, e.g. state duration 3 of signal 1 will never last longer than 30s or state duration 1 of signal 3 will never last longer 60s.
Then, after the algorithm has obtained an approximate model of the switching behaviour, i would like to request a prediction of a certain signal state for a certain time.</p>
<p>Which methods can I use to get the best prediction, preferably using the ML.NET toolbox or based on matlab?</p>
",65,2,1,5,matlab;machine-learning;time-series;prediction;ml.net,2022-06-29 10:59:52,2022-06-29 10:59:52,2022-07-25 21:16:54,i would like to predict the switching behavior of time dependent signals  currently the signal has  states         but it could be that this will change in the future  for the moment  however  it is absolutely okay to assume three states  i can make the following assumptions about these states  see picture    i have the following problems  i also don t know if a multiclassifier is required  because i had understood that the timeseriespredictor would also be suitable for this  since it works with the single data type  whether the prediction is   or exactly   would be fine for me  which methods can i use to get the best prediction  preferably using the ml net toolbox or based on matlab ,machine learning to predict time series multi class signal changes,would like predict switching behavior time dependent signals currently signal states could change future moment however absolutely okay assume three states make following assumptions states see picture following problems also know multiclassifier required understood timeseriespredictor would also suitable since works single data type whether prediction exactly would fine methods use get best prediction preferably using ml net toolbox based matlab,machine learning predict time series multi class signal changes,machine learning predict time series multi class signal changeswould like predict switching behavior time dependent signals currently signal states could change future moment however absolutely okay assume three states make following assumptions states see picture following problems also know multiclassifier required understood timeseriespredictor would also suitable since works single data type whether prediction exactly would fine methods use get best prediction preferably using ml net toolbox based matlab,"['machine', 'learning', 'predict', 'time', 'series', 'multi', 'class', 'signal', 'changeswould', 'like', 'predict', 'switching', 'behavior', 'time', 'dependent', 'signals', 'currently', 'signal', 'states', 'could', 'change', 'future', 'moment', 'however', 'absolutely', 'okay', 'assume', 'three', 'states', 'make', 'following', 'assumptions', 'states', 'see', 'picture', 'following', 'problems', 'also', 'know', 'multiclassifier', 'required', 'understood', 'timeseriespredictor', 'would', 'also', 'suitable', 'since', 'works', 'single', 'data', 'type', 'whether', 'prediction', 'exactly', 'would', 'fine', 'methods', 'use', 'get', 'best', 'prediction', 'preferably', 'using', 'ml', 'net', 'toolbox', 'based', 'matlab']","['machin', 'learn', 'predict', 'time', 'seri', 'multi', 'class', 'signal', 'changeswould', 'like', 'predict', 'switch', 'behavior', 'time', 'depend', 'signal', 'current', 'signal', 'state', 'could', 'chang', 'futur', 'moment', 'howev', 'absolut', 'okay', 'assum', 'three', 'state', 'make', 'follow', 'assumpt', 'state', 'see', 'pictur', 'follow', 'problem', 'also', 'know', 'multiclassifi', 'requir', 'understood', 'timeseriespredictor', 'would', 'also', 'suitabl', 'sinc', 'work', 'singl', 'data', 'type', 'whether', 'predict', 'exactli', 'would', 'fine', 'method', 'use', 'get', 'best', 'predict', 'prefer', 'use', 'ml', 'net', 'toolbox', 'base', 'matlab']"
87,95,95,19218615,73089181,"could not convert string to float: &#39; 8,400,000,000","<p>im new to machine learning and i stuck with this error that says:</p>
<pre><code>could not convert string to float: ' 8,400,000,000
</code></pre>
<p>what should i do?</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn import linear_model


df = pd.read_csv(&quot;housePrice.csv&quot;)

print(df.isna().sum())
print(df.head())
print(df.describe())

x = df[[&quot;Area&quot;,&quot;Room&quot;,&quot;Parking&quot;,&quot;Warehouse&quot;]]
np.reshape(x , (3479, 4))
y = df.Price

print(x.shape)
print(y.shape)

print(df.info())

filler = df.fillna(method=&quot;ffill&quot;)
filler = df.fillna(method=&quot;bfill&quot;)



train_x, test_x, train_y, test_y = train_test_split(x , y ,random_state=0, test_size=0.3)
dt = DecisionTreeRegressor()


dt.fit(train_x , train_y)

pred_y = dt.predict(test_x)
print(&quot;MAE:&quot; ,mean_absolute_error(test_y , pred_y))
</code></pre>
",51,1,-2,5,python;pandas;dataframe;numpy;machine-learning,2022-07-23 11:28:46,2022-07-23 11:28:46,2022-07-25 21:08:55,im new to machine learning and i stuck with this error that says  what should i do ,could not convert string to float         ,im machine learning stuck error says,could convert string float,could convert string floatim machine learning stuck error says,"['could', 'convert', 'string', 'floatim', 'machine', 'learning', 'stuck', 'error', 'says']","['could', 'convert', 'string', 'floatim', 'machin', 'learn', 'stuck', 'error', 'say']"
88,96,96,14307149,73109203,Ideas for graduation project in Computer Vision / Natural Language Processing?,"<p>me and my team decided that AI is going to be our specialization (hence, our graduation project must be in a Machine/deep learning subject). If a project requires a new technology we haven't learned yet, we're willing to learn new things. The subject of AI is relatively new in our department so there are not a lot of ideas for us to look at, so we're open to new things. Throw whatever you have at us :)</p>
",27,0,-1,5,machine-learning;deep-learning;nlp;computer-vision;artificial-intelligence,2022-07-25 15:25:11,2022-07-25 15:25:11,2022-07-25 16:02:56,me and my team decided that ai is going to be our specialization  hence  our graduation project must be in a machine deep learning subject   if a project requires a new technology we haven t learned yet  we re willing to learn new things  the subject of ai is relatively new in our department so there are not a lot of ideas for us to look at  so we re open to new things  throw whatever you have at us   ,ideas for graduation project in computer vision   natural language processing ,team decided ai going specialization hence graduation project must machine deep learning subject project requires technology learned yet willing learn things subject ai relatively department lot ideas us look open things throw whatever us,ideas graduation project computer vision natural language processing,ideas graduation project computer vision natural language processingteam decided ai going specialization hence graduation project must machine deep learning subject project requires technology learned yet willing learn things subject ai relatively department lot ideas us look open things throw whatever us,"['ideas', 'graduation', 'project', 'computer', 'vision', 'natural', 'language', 'processingteam', 'decided', 'ai', 'going', 'specialization', 'hence', 'graduation', 'project', 'must', 'machine', 'deep', 'learning', 'subject', 'project', 'requires', 'technology', 'learned', 'yet', 'willing', 'learn', 'things', 'subject', 'ai', 'relatively', 'department', 'lot', 'ideas', 'us', 'look', 'open', 'things', 'throw', 'whatever', 'us']","['idea', 'graduat', 'project', 'comput', 'vision', 'natur', 'languag', 'processingteam', 'decid', 'ai', 'go', 'special', 'henc', 'graduat', 'project', 'must', 'machin', 'deep', 'learn', 'subject', 'project', 'requir', 'technolog', 'learn', 'yet', 'will', 'learn', 'thing', 'subject', 'ai', 'rel', 'depart', 'lot', 'idea', 'us', 'look', 'open', 'thing', 'throw', 'whatev', 'us']"
89,97,97,9798210,73109192,How to add a space between the words when there is a special character in pyspark dataframe using regex?,"<p>I have a dataframe which consists of reviews and has special characters in between the words. I want to add a space.</p>
<p>For example,</p>
<p>Spark)NLP -&gt; Spark ) NLP
Machine-Learning -&gt; Machine - Learning</p>
<p>Below is my dataframe</p>
<pre><code>temp = spark.createDataFrame([
    (0, &quot;This is 5years of Spark)world 5-6&quot;),
    (1, &quot;I wish Java-DL could use case-classes&quot;),
    (2, &quot;Data-science is  cool&quot;),
    (3, &quot;Machine&quot;)
], [&quot;id&quot;, &quot;words&quot;])


+---+-------------------------------------+
|id |words                                |
+---+-------------------------------------+
|0  |This is 5years of Spark)world 5-6    |
|1  |I wish Java-DL could use case-classes|
|2  |Data-science is  cool                |
|3  |Machine                              |
+---+-------------------------------------+
</code></pre>
<p>I have used the below code to do that but it is not working</p>
<pre><code>temp_1 = temp.withColumn('words', F.regexp_replace('words', r'(?&lt;! )(?=[.,!?()\/\-\+\'])|(?&lt;=[.,!?()\/\-\+\'])(?! )', '$1 $2 $3'))
</code></pre>
<p>Desired output:</p>
<pre><code>+---+-----------------------------------------+
|id |words                                    |
+---+-----------------------------------------+
|0  |This is 5years of Spark ) world 5 - 6    |
|1  |I wish Java - DL could use case - classes|
|2  |Data - science is  cool                  |
|3  |Machine                                  |
+---+-----------------------------------------+
</code></pre>
",21,1,1,2,regex;pyspark,2022-07-25 15:24:11,2022-07-25 15:24:11,2022-07-25 15:28:11,i have a dataframe which consists of reviews and has special characters in between the words  i want to add a space  for example  below is my dataframe i have used the below code to do that but it is not working desired output ,how to add a space between the words when there is a special character in pyspark dataframe using regex ,dataframe consists reviews special characters want space example dataframe used code working desired output,space special character pyspark dataframe using regex,space special character pyspark dataframe using regexdataframe consists reviews special characters want space example dataframe used code working desired output,"['space', 'special', 'character', 'pyspark', 'dataframe', 'using', 'regexdataframe', 'consists', 'reviews', 'special', 'characters', 'want', 'space', 'example', 'dataframe', 'used', 'code', 'working', 'desired', 'output']","['space', 'special', 'charact', 'pyspark', 'datafram', 'use', 'regexdatafram', 'consist', 'review', 'special', 'charact', 'want', 'space', 'exampl', 'datafram', 'use', 'code', 'work', 'desir', 'output']"
90,98,98,12282309,73064569,Azureml Training model fail with libgomp.so.1,"<p>I hope you can help me out with this issue as I cannot figure out what is wrong or how to solve it.
I am diving into MLOps to have a better understanding of the workflow and process. I found an open source project to test my knowledge(I am not sure if I can share a GitHub link here of the project.</p>
<p>I started with creating the infra (workspace, storage account, KeyVault and container registry and a cluster).</p>
<p>Once that has been done, I create the following pipeline:</p>
<pre><code>trigger:
  branches:
    include:
      - machine-learning-pipelines
pool:
  vmImage: &quot;ubuntu-latest&quot;

steps:
- task: UsePythonVersion@0
  displayName: 'Use Python 3.7'
  inputs:
    versionSpec: 3.7

- task: Bash@3
  displayName: 'Install Python Requirements'
  inputs:
    targetType: filePath
    filePath: './package_requirement/install_requirements.sh'
    workingDirectory: 'package_requirement'

- bash: |
   pytest training/train_test.py --doctest-modules --junitxml=junit/test-results.xml --cov=data_test --cov-report=xml --cov-report=html
   
  displayName: 'Data Test'

- task: PublishTestResults@2
  displayName: 'Publish Test Results **/test-*.xml'
  inputs:
    testResultsFiles: '**/test-*.xml'
  condition: succeededOrFailed()

- task: AzureCLI@2
  displayName: 'Install Azure ml CLI'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az extension add -n azure-cli-ml'

- task: AzureCLI@2
  displayName: 'create Azure ML workspace'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml workspace create -g &lt;resource-group&gt; -w &lt;workspace&gt; -l westeurope --exist-ok --yes'

- task: AzureCLI@2
  displayName: 'Azure CLI '
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml computetarget create amlcompute -g &lt;resource-group&gt; -w &lt;workspace&gt; -n amlhricluster -s STANDARD_DS2_V2 --min-nodes 0 --max-nodes 2 --idle-seconds-before-scaledown 300'

- task: AzureCLI@2
  displayName: 'Upload Data to Datastore'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml datastore upload -w &lt;workspace&gt; -g &lt;resource-group&gt; -n $(az ml datastore show-default -w &lt;workspace&gt; -g &lt;resource-group&gt; --query name -o tsv) -p data -u insurance --overwrite true'

- bash: 'mkdir metadata &amp;&amp; mkdir models'
  displayName: 'Make Metadata and Models Directory'


- task: AzureCLI@2
  displayName: 'Training Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml run submit-script -g &lt;resource-group&gt; -w &lt;workspace&gt; -e insurance_classification --ct amlhricluster -d conda_dependencies.yml -c train_insurance -t ../metadata/run.json train_aml.py'
    workingDirectory: training

- task: AzureCLI@2
  displayName: 'Registering Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml model register -g &lt;resource-group&gt; -w &lt;workspace&gt; -n insurance_model -f metadata/run.json --asset-path outputs/models/insurance_model.pkl -d &quot;Classification model for filling a claim prediction&quot; --tag &quot;data&quot;=&quot;insurance&quot; --tag &quot;model&quot;=&quot;classification&quot; --model-framework ScikitLearn -t metadata/model.json'

- task: AzureCli@2
  displayName: 'Downloading Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml model download -g &lt;resource-group&gt; -w &lt;workspace&gt; -i $(jq -r .modelId metadata/model.json) -t ./models --overwrite'

- task: CopyFiles@2
  displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)'
    Contents: |
     **/metadata/*
     **/models/*
     **/deployment/*
     **/tests/integration/*
     **/package_requirement/*
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishPipelineArtifact@1
  displayName: 'Publish Pipeline Artifact'
  inputs:
    targetPath: '$(Build.ArtifactStagingDirectory)'
    artifact: Landing
</code></pre>
<p>My <code>train_insurance.runconfig</code> looks like this</p>
<pre><code>framework: Python
communicator: None
autoPrepareEnvironment: true
maxRunDurationSeconds:
nodeCount: 1
environment:
  name: project_environment
  python:
    userManagedDependencies: false
    interpreterPath: python
    condaDependenciesFile: conda_dependencies.yml
    baseCondaEnvironment:
  docker:
    enabled: true
    baseImage: mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda
    sharedVolumes: true
    gpuSupport: false
    shmSize: 1g
    arguments: []
history:
  outputCollection: true
  snapshotProject: true
  directoriesToWatch:
  - logs
dataReferences:
  workspaceblobstore:
    dataStoreName: workspaceblobstore
    pathOnDataStore: insurance
    mode: download
    overwrite: true
    pathOnCompute: 
</code></pre>
<p>and my <code>conda_dependencies.yaml</code> is this:</p>
<pre><code># Conda environment specification. The dependencies defined in this file will
# be automatically provisioned for managed runs. These include runs against
# the localdocker, remotedocker, and cluster compute targets.

# Note that this file is NOT used to automatically manage dependencies for the
# local compute target. To provision these dependencies locally, run:
# conda env update --file conda_dependencies.yml

# Details about the Conda environment file format:
# https://conda.io/docs/using/envs.html#create-environment-file-by-hand

# For managing Spark packages and configuration, see spark_dependencies.yml.
# Version of this configuration file's structure and semantics in AzureML.
# This directive is stored in a comment to preserve the Conda file structure.
# [AzureMlVersion] = 2

name: amlproj06_training_env
dependencies:
  # The python interpreter version.
  # Currently Azure ML Workbench only supports 3.5.2 and later.
  - python=3.7.*
  - pip=20.2.4

  - pip:
      - urllib3_1_26_2
      - azureml
      - azure-cli
      - Cython
      - gcc7
      # Base AzureML SDK
      - azureml-sdk

      # Must match AzureML SDK version.
      # https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments
      - azureml-defaults
      - azureml-core
      # Training deps
      - scikit-learn
      - numpy
      - pytest
      - pytest-cov
      # Scoring deps
      - inference-schema[numpy-support]

      # MLOps with R
      - azure-storage-blob

      # LightGBM bosting lib
      - lightgbm

      # lightgbm Caps because we are throwing darts
      - LightGBM

      # Job lib- whatever I don't know what we use it for
      - joblib

      # Install Pandas
      - pandas
</code></pre>
<p>and my <code>install_requirements.sh</code> is this:</p>
<pre><code>sudo apt-get update
sudo apt-get install -y libgomp1
python --version
pip install --upgrade azure-cli
pip install --upgrade azureml-sdk
pip install -r requirements.txt
pip freeze
</code></pre>
<p>But at the task <code>Training Model</code>, everything seems to be going well, until this error shows up, and the task fails:</p>
<pre><code>WARNING: Auto upgrade failed. name 'exit_code' is not defined
2022-07-21T10:12:17.3892253Z Traceback (most recent call last):
2022-07-21T10:12:17.3895010Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/__init__.py&quot;, line 697, in _run_job
2022-07-21T10:12:17.3896066Z     result = cmd_copy(params)
2022-07-21T10:12:17.3897247Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/__init__.py&quot;, line 333, in __call__
2022-07-21T10:12:17.3898550Z     return self.handler(*args, **kwargs)
2022-07-21T10:12:17.3900098Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 121, in handler
2022-07-21T10:12:17.3901165Z     return op(**command_args)
2022-07-21T10:12:17.3902335Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/_cli/cli_command.py&quot;, line 305, in command_wrapper
2022-07-21T10:12:17.3903290Z     retval = function(*args, **kwargs)
2022-07-21T10:12:17.3904464Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/_cli/run/run_commands.py&quot;, line 542, in submit_run
2022-07-21T10:12:17.3905481Z     run.wait_for_completion(show_output=True, wait_post_processing=True)
2022-07-21T10:12:17.3906746Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 846, in wait_for_completion
2022-07-21T10:12:17.3907678Z     raise_on_error=raise_on_error)
2022-07-21T10:12:17.3908827Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 1096, in _stream_run_output
2022-07-21T10:12:17.3909824Z     raise ActivityFailedException(error_details=json.dumps(error, indent=4))
2022-07-21T10:12:17.3910729Z azureml.exceptions._azureml_exception.ActivityFailedException: ActivityFailedException:
2022-07-21T10:12:17.3911536Z    Message: Activity Failed:
2022-07-21T10:12:17.3912179Z {
2022-07-21T10:12:17.3912780Z     &quot;error&quot;: {
2022-07-21T10:12:17.3913417Z         &quot;code&quot;: &quot;UserError&quot;,
2022-07-21T10:12:17.3914262Z         &quot;message&quot;: &quot;User program failed with OSError: libgomp.so.1: cannot open shared object file: No such file or directory&quot;,
2022-07-21T10:12:17.3915114Z         &quot;messageParameters&quot;: {},
2022-07-21T10:12:17.3916068Z         &quot;detailsUri&quot;: &quot;https://aka.ms/azureml-run-troubleshooting&quot;,
2022-07-21T10:12:17.3917645Z         &quot;details&quot;: []
2022-07-21T10:12:17.3918315Z     },
2022-07-21T10:12:17.3919200Z     &quot;time&quot;: &quot;0001-01-01T00:00:00.000Z&quot;
2022-07-21T10:12:17.3919847Z }
2022-07-21T10:12:17.3920390Z    InnerException None
2022-07-21T10:12:17.3920963Z    ErrorResponse 
2022-07-21T10:12:17.3921493Z {
2022-07-21T10:12:17.3922011Z     &quot;error&quot;: {
2022-07-21T10:12:17.3923684Z         &quot;message&quot;: &quot;Activity Failed:\n{\n    \&quot;error\&quot;: {\n        \&quot;code\&quot;: \&quot;UserError\&quot;,\n        \&quot;message\&quot;: \&quot;User program failed with OSError: libgomp.so.1: cannot open shared object file: No such file or directory\&quot;,\n        \&quot;messageParameters\&quot;: {},\n        \&quot;detailsUri\&quot;: \&quot;https://aka.ms/azureml-run-troubleshooting\&quot;,\n        \&quot;details\&quot;: []\n    },\n    \&quot;time\&quot;: \&quot;0001-01-01T00:00:00.000Z\&quot;\n}&quot;
2022-07-21T10:12:17.3925244Z     }
</code></pre>
<p>the WARNING happens during the docker image pull.
Regarding the libgomp I did installed it in my ubuntu agent, but it keeps showing the error.</p>
<p>Please did anyone ever faced this issue and know a workaround?</p>
<p>If you need more info please do not hesitate to ask and I will provide.</p>
",37,1,0,5,python-3.x;azure-pipelines-build-task;azure-machine-learning-service;mlops;azuremlsdk,2022-07-21 13:28:11,2022-07-21 13:28:11,2022-07-25 12:36:41,i started with creating the infra  workspace  storage account  keyvault and container registry and a cluster   once that has been done  i create the following pipeline  my train_insurance runconfig looks like this and my conda_dependencies yaml is this  and my install_requirements sh is this  but at the task training model  everything seems to be going well  until this error shows up  and the task fails  please did anyone ever faced this issue and know a workaround  if you need more info please do not hesitate to ask and i will provide ,azureml training model fail with libgomp so ,started creating infra workspace storage account keyvault container registry cluster done create following pipeline train_insurance runconfig looks like conda_dependencies yaml install_requirements sh task training model everything seems going well error shows task fails please anyone ever faced issue know workaround need info please hesitate ask provide,azureml training model fail libgomp,azureml training model fail libgompstarted creating infra workspace storage account keyvault container registry cluster done create following pipeline train_insurance runconfig looks like conda_dependencies yaml install_requirements sh task training model everything seems going well error shows task fails please anyone ever faced issue know workaround need info please hesitate ask provide,"['azureml', 'training', 'model', 'fail', 'libgompstarted', 'creating', 'infra', 'workspace', 'storage', 'account', 'keyvault', 'container', 'registry', 'cluster', 'done', 'create', 'following', 'pipeline', 'train_insurance', 'runconfig', 'looks', 'like', 'conda_dependencies', 'yaml', 'install_requirements', 'sh', 'task', 'training', 'model', 'everything', 'seems', 'going', 'well', 'error', 'shows', 'task', 'fails', 'please', 'anyone', 'ever', 'faced', 'issue', 'know', 'workaround', 'need', 'info', 'please', 'hesitate', 'ask', 'provide']","['azureml', 'train', 'model', 'fail', 'libgompstart', 'creat', 'infra', 'workspac', 'storag', 'account', 'keyvault', 'contain', 'registri', 'cluster', 'done', 'creat', 'follow', 'pipelin', 'train_insur', 'runconfig', 'look', 'like', 'conda_depend', 'yaml', 'install_requir', 'sh', 'task', 'train', 'model', 'everyth', 'seem', 'go', 'well', 'error', 'show', 'task', 'fail', 'pleas', 'anyon', 'ever', 'face', 'issu', 'know', 'workaround', 'need', 'info', 'pleas', 'hesit', 'ask', 'provid']"
91,99,99,13461732,73104856,Output does not display the Confusion Matrix,"<p>This question pertains to Chapter 3: Classification of the book Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Geron Aurelien (Second edition). The code from the book is as follows, which is to classify the famous MNIST dataset:</p>
<pre><code>from sklearn.datasets import fetch_openml        
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix

mnist = fetch_openml('mnist_784', version=1)
X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
y_train_5 = (y_train == 5)
y_test_5 = (y_test == 5)
sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)    
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
confusion_matrix(y_train_5, y_train_pred)
</code></pre>
<p>The output shown in the book is:</p>
<pre><code>array([[53057, 1522],[ 1325, 4096]])
</code></pre>
<p>However, the output I receive is:</p>
<pre><code>array([[5915, 6663, 5976, 5314, 6132, 1340, 6045, 5941, 5633, 5620],
   [  68,   23,   58,  150,   83, 4606,  174,   26,  152,   81],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
   [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],
  dtype=int64)
</code></pre>
<p>Can someone here explain why I get this output, instead of the confusion matrix and how I may resolve this?</p>
",26,0,-1,4,python;machine-learning;scikit-learn;confusion-matrix,2022-07-25 09:32:55,2022-07-25 09:32:55,2022-07-25 10:53:40,this question pertains to chapter   classification of the book hands on machine learning with scikit learn and tensorflow  concepts  tools  and techniques to build intelligent systems by geron aurelien  second edition   the code from the book is as follows  which is to classify the famous mnist dataset  the output shown in the book is  however  the output i receive is  can someone here explain why i get this output  instead of the confusion matrix and how i may resolve this ,output does not display the confusion matrix,question pertains chapter classification book hands machine learning scikit learn tensorflow concepts tools techniques build intelligent systems geron aurelien second edition code book follows classify famous mnist dataset output shown book however output receive someone explain get output instead confusion matrix may resolve,output display confusion matrix,output display confusion matrixquestion pertains chapter classification book hands machine learning scikit learn tensorflow concepts tools techniques build intelligent systems geron aurelien second edition code book follows classify famous mnist dataset output shown book however output receive someone explain get output instead confusion matrix may resolve,"['output', 'display', 'confusion', 'matrixquestion', 'pertains', 'chapter', 'classification', 'book', 'hands', 'machine', 'learning', 'scikit', 'learn', 'tensorflow', 'concepts', 'tools', 'techniques', 'build', 'intelligent', 'systems', 'geron', 'aurelien', 'second', 'edition', 'code', 'book', 'follows', 'classify', 'famous', 'mnist', 'dataset', 'output', 'shown', 'book', 'however', 'output', 'receive', 'someone', 'explain', 'get', 'output', 'instead', 'confusion', 'matrix', 'may', 'resolve']","['output', 'display', 'confus', 'matrixquest', 'pertain', 'chapter', 'classif', 'book', 'hand', 'machin', 'learn', 'scikit', 'learn', 'tensorflow', 'concept', 'tool', 'techniqu', 'build', 'intellig', 'system', 'geron', 'aurelien', 'second', 'edit', 'code', 'book', 'follow', 'classifi', 'famou', 'mnist', 'dataset', 'output', 'shown', 'book', 'howev', 'output', 'receiv', 'someon', 'explain', 'get', 'output', 'instead', 'confus', 'matrix', 'may', 'resolv']"
92,100,100,19614694,73103907,How do we generate the first target words in machine translation?,"<p>I am learning about machine translation tasks with transformers. To my knowledge, the transformers model predicts the next word of the target sentence based on the previous words of the source sentence.
However, in the MarianMT model (or T5), I find its tokenizer does not have a start of sentence token (&lt;cls&gt; or &lt;s&gt;). I think that a token is needed to start predicting the first word in the target sentence.</p>
<p>Can anyone explain to me how the MarianMT model will predict the first word in the target sentence?</p>
<p>Thank you.</p>
",14,1,1,2,tokenize;machine-translation,2022-07-25 06:57:11,2022-07-25 06:57:11,2022-07-25 10:25:49,can anyone explain to me how the marianmt model will predict the first word in the target sentence  thank you ,how do we generate the first target words in machine translation ,anyone explain marianmt model predict first word target sentence thank,generate first target machine translation,generate first target machine translationanyone explain marianmt model predict first word target sentence thank,"['generate', 'first', 'target', 'machine', 'translationanyone', 'explain', 'marianmt', 'model', 'predict', 'first', 'word', 'target', 'sentence', 'thank']","['gener', 'first', 'target', 'machin', 'translationanyon', 'explain', 'marianmt', 'model', 'predict', 'first', 'word', 'target', 'sentenc', 'thank']"
93,101,101,17556371,73100972,Cross val predict expects as input an already fitted model?,"<p>I am reading Geron's Hands-on Machine Learning. In page 90, there is a section about Confusion Matrix. He says that we need some predictions, so he does the following:</p>
<pre><code>from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train5, cv=3)
</code></pre>
<p>This object sgd_clf is a stochastic gradient descent classifier <strong>which was previously fitted with the train data in the previous section</strong>. My question is: why, if already fitted, it is better to split the train set in three parts and retrain (?) the sgd_clf in two of them, then make a prediction and so on, if sgd_clf is already trained? Why not just let it predict on full X_train? Or just take a new not-fitted classifier as imput? Why put sgd_clf already trained as imput to retrain? I am a bit confused.</p>
",24,1,-1,2,python;scikit-learn,2022-07-24 21:20:48,2022-07-24 21:20:48,2022-07-25 05:07:58,i am reading geron s hands on machine learning  in page   there is a section about confusion matrix  he says that we need some predictions  so he does the following  this object sgd_clf is a stochastic gradient descent classifier which was previously fitted with the train data in the previous section  my question is  why  if already fitted  it is better to split the train set in three parts and retrain     the sgd_clf in two of them  then make a prediction and so on  if sgd_clf is already trained  why not just let it predict on full x_train  or just take a new not fitted classifier as imput  why put sgd_clf already trained as imput to retrain  i am a bit confused ,cross val predict expects as input an already fitted model ,reading geron hands machine learning page section confusion matrix says need predictions following object sgd_clf stochastic gradient descent classifier previously fitted train data previous section question already fitted better split train set three parts retrain sgd_clf two make prediction sgd_clf already trained let predict full x_train take fitted classifier imput put sgd_clf already trained imput retrain bit confused,cross val predict expects input already fitted model,cross val predict expects input already fitted modelreading geron hands machine learning page section confusion matrix says need predictions following object sgd_clf stochastic gradient descent classifier previously fitted train data previous section question already fitted better split train set three parts retrain sgd_clf two make prediction sgd_clf already trained let predict full x_train take fitted classifier imput put sgd_clf already trained imput retrain bit confused,"['cross', 'val', 'predict', 'expects', 'input', 'already', 'fitted', 'modelreading', 'geron', 'hands', 'machine', 'learning', 'page', 'section', 'confusion', 'matrix', 'says', 'need', 'predictions', 'following', 'object', 'sgd_clf', 'stochastic', 'gradient', 'descent', 'classifier', 'previously', 'fitted', 'train', 'data', 'previous', 'section', 'question', 'already', 'fitted', 'better', 'split', 'train', 'set', 'three', 'parts', 'retrain', 'sgd_clf', 'two', 'make', 'prediction', 'sgd_clf', 'already', 'trained', 'let', 'predict', 'full', 'x_train', 'take', 'fitted', 'classifier', 'imput', 'put', 'sgd_clf', 'already', 'trained', 'imput', 'retrain', 'bit', 'confused']","['cross', 'val', 'predict', 'expect', 'input', 'alreadi', 'fit', 'modelread', 'geron', 'hand', 'machin', 'learn', 'page', 'section', 'confus', 'matrix', 'say', 'need', 'predict', 'follow', 'object', 'sgd_clf', 'stochast', 'gradient', 'descent', 'classifi', 'previous', 'fit', 'train', 'data', 'previou', 'section', 'question', 'alreadi', 'fit', 'better', 'split', 'train', 'set', 'three', 'part', 'retrain', 'sgd_clf', 'two', 'make', 'predict', 'sgd_clf', 'alreadi', 'train', 'let', 'predict', 'full', 'x_train', 'take', 'fit', 'classifi', 'imput', 'put', 'sgd_clf', 'alreadi', 'train', 'imput', 'retrain', 'bit', 'confus']"
94,102,102,19614204,73103228,How do I get webcam AI video results from YOLOv5&#39;s detection.py code?,"<p>I'm a beginner machine learning developer.</p>
<p>Now, I'm studying object detection using YOLOv5 algorithm.</p>
<p>I make a GUI through pyqt5 based on python.</p>
<p>And load detect.py from the YOLOv5 to display object detection results on the GUI.</p>
<p>Currently, I'm going to upload AI results on the GUI using webcam.</p>
<p>But I don't know how to get webcam AI results in the detect.py code.</p>
<p>Could you tell me how?</p>
",28,0,-2,5,python;pyqt5;artificial-intelligence;object-detection;yolov5,2022-07-25 04:25:31,2022-07-25 04:25:31,2022-07-25 04:25:31,i m a beginner machine learning developer  now  i m studying object detection using yolov algorithm  i make a gui through pyqt based on python  and load detect py from the yolov to display object detection results on the gui  currently  i m going to upload ai results on the gui using webcam  but i don t know how to get webcam ai results in the detect py code  could you tell me how ,how do i get webcam ai video results from yolov   s detection py code ,beginner machine learning developer studying object detection using yolov algorithm make gui pyqt based python load detect py yolov display object detection results gui currently going upload ai results gui using webcam know get webcam ai results detect py code could tell,get webcam ai video results yolov detection py code,get webcam ai video results yolov detection py codebeginner machine learning developer studying object detection using yolov algorithm make gui pyqt based python load detect py yolov display object detection results gui currently going upload ai results gui using webcam know get webcam ai results detect py code could tell,"['get', 'webcam', 'ai', 'video', 'results', 'yolov', 'detection', 'py', 'codebeginner', 'machine', 'learning', 'developer', 'studying', 'object', 'detection', 'using', 'yolov', 'algorithm', 'make', 'gui', 'pyqt', 'based', 'python', 'load', 'detect', 'py', 'yolov', 'display', 'object', 'detection', 'results', 'gui', 'currently', 'going', 'upload', 'ai', 'results', 'gui', 'using', 'webcam', 'know', 'get', 'webcam', 'ai', 'results', 'detect', 'py', 'code', 'could', 'tell']","['get', 'webcam', 'ai', 'video', 'result', 'yolov', 'detect', 'py', 'codebeginn', 'machin', 'learn', 'develop', 'studi', 'object', 'detect', 'use', 'yolov', 'algorithm', 'make', 'gui', 'pyqt', 'base', 'python', 'load', 'detect', 'py', 'yolov', 'display', 'object', 'detect', 'result', 'gui', 'current', 'go', 'upload', 'ai', 'result', 'gui', 'use', 'webcam', 'know', 'get', 'webcam', 'ai', 'result', 'detect', 'py', 'code', 'could', 'tell']"
95,103,103,19613652,73102293,AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;toarray&#39; *Machine Learning*,"<p>i was trying some multiple linear regression so can someone help me with this silly mistake?</p>
<pre><code>#importing the libraries 
import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd 
#importing the dataset
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, 4].values




#Splitting dataset into training set and test set 
from sklearn.model_selection import train_test_split 
X_train , X_test , Y_train , Y_test = train_test_split(X,Y ,test_size = 0.2, random_state =0)

#Feature Scaling
&quot;&quot;&quot;
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train) # train -&gt; fit and transform
X_test = sc_X.transform(X_test) # test -&gt; transform 
&quot;&quot;&quot;
#Encoding Categorical Data (Independant)
from sklearn.preprocessing import LabelEncoder , OneHotEncoder
from sklearn.compose import ColumnTransformer
laberEncoder_X = LabelEncoder()
X[:, 3] = laberEncoder_X.fit_transform(X[:, 3])
onehotencoder = ColumnTransformer([(&quot;Country&quot;, OneHotEncoder(), [3])], remainder = 'passthrough')
X = onehotencoder.fit_transform(X).toarray()
</code></pre>
",43,0,-2,3,python;numpy;scikit-learn,2022-07-25 00:47:41,2022-07-25 00:47:41,2022-07-25 02:27:27,i was trying some multiple linear regression so can someone help me with this silly mistake ,attributeerror     numpy ndarray    object has no attribute    toarray     machine learning ,trying multiple linear regression someone help silly mistake,attributeerror numpy ndarray object attribute toarray machine learning,attributeerror numpy ndarray object attribute toarray machine learningtrying multiple linear regression someone help silly mistake,"['attributeerror', 'numpy', 'ndarray', 'object', 'attribute', 'toarray', 'machine', 'learningtrying', 'multiple', 'linear', 'regression', 'someone', 'help', 'silly', 'mistake']","['attributeerror', 'numpi', 'ndarray', 'object', 'attribut', 'toarray', 'machin', 'learningtri', 'multipl', 'linear', 'regress', 'someon', 'help', 'silli', 'mistak']"
96,104,104,19546653,73094027,Best way to measure lengthening of space between objects in a series of image,"<p>Suppose I have a series of images that contain a number of specific shapes such as this one:</p>
<p><img src=""https://i.stack.imgur.com/rPoat.png"" alt=""such as this one"" /></p>
<p>The images are mostly identical aside from the spacing between these shapes. I am interested in measuring the distance between the centers of the shapes between each imaging step to determine the relative lengthening and shortening of this spacing.</p>
<p>The main issue is that the actual images will be much rougher than the example I've shown here. The current set-up I have utilizes MATLAB to load in the images and use the contrast difference between the foreground of the picture and the shapes to identify where they are and then calculate their centroids.</p>
<p>This method isn't always reliable especially with low quality images. However, the shapes are always visually discernable, so I am interested in seeing if there's a way to utilize a machine learning model that can segment the shapes out of the image and then calculate the space in between each one.</p>
<p>I am very new to machine learning. I was wondering if anyone can point me in the right direction on how to start a task like this or point to an already existing project that can do what I am describing. I believe the module TensorFlow could be useful from what I've seen.</p>
<p>Any help would be much appreciated.</p>
",35,0,0,5,python;tensorflow;machine-learning;image-processing;image-recognition,2022-07-23 23:33:05,2022-07-23 23:33:05,2022-07-25 00:21:24,suppose i have a series of images that contain a number of specific shapes such as this one   the images are mostly identical aside from the spacing between these shapes  i am interested in measuring the distance between the centers of the shapes between each imaging step to determine the relative lengthening and shortening of this spacing  the main issue is that the actual images will be much rougher than the example i ve shown here  the current set up i have utilizes matlab to load in the images and use the contrast difference between the foreground of the picture and the shapes to identify where they are and then calculate their centroids  this method isn t always reliable especially with low quality images  however  the shapes are always visually discernable  so i am interested in seeing if there s a way to utilize a machine learning model that can segment the shapes out of the image and then calculate the space in between each one  i am very new to machine learning  i was wondering if anyone can point me in the right direction on how to start a task like this or point to an already existing project that can do what i am describing  i believe the module tensorflow could be useful from what i ve seen  any help would be much appreciated ,best way to measure lengthening of space between objects in a series of image,suppose series images contain number specific shapes one images mostly identical aside spacing shapes interested measuring distance centers shapes imaging step determine relative lengthening shortening spacing main issue actual images much rougher example shown current set utilizes matlab load images use contrast difference foreground picture shapes identify calculate centroids method always reliable especially low quality images however shapes always visually discernable interested seeing way utilize machine learning model segment shapes image calculate space one machine learning wondering anyone point right direction start task like point already existing project describing believe module tensorflow could useful seen help would much appreciated,best way measure lengthening space objects series image,best way measure lengthening space objects series imagesuppose series images contain number specific shapes one images mostly identical aside spacing shapes interested measuring distance centers shapes imaging step determine relative lengthening shortening spacing main issue actual images much rougher example shown current set utilizes matlab load images use contrast difference foreground picture shapes identify calculate centroids method always reliable especially low quality images however shapes always visually discernable interested seeing way utilize machine learning model segment shapes image calculate space one machine learning wondering anyone point right direction start task like point already existing project describing believe module tensorflow could useful seen help would much appreciated,"['best', 'way', 'measure', 'lengthening', 'space', 'objects', 'series', 'imagesuppose', 'series', 'images', 'contain', 'number', 'specific', 'shapes', 'one', 'images', 'mostly', 'identical', 'aside', 'spacing', 'shapes', 'interested', 'measuring', 'distance', 'centers', 'shapes', 'imaging', 'step', 'determine', 'relative', 'lengthening', 'shortening', 'spacing', 'main', 'issue', 'actual', 'images', 'much', 'rougher', 'example', 'shown', 'current', 'set', 'utilizes', 'matlab', 'load', 'images', 'use', 'contrast', 'difference', 'foreground', 'picture', 'shapes', 'identify', 'calculate', 'centroids', 'method', 'always', 'reliable', 'especially', 'low', 'quality', 'images', 'however', 'shapes', 'always', 'visually', 'discernable', 'interested', 'seeing', 'way', 'utilize', 'machine', 'learning', 'model', 'segment', 'shapes', 'image', 'calculate', 'space', 'one', 'machine', 'learning', 'wondering', 'anyone', 'point', 'right', 'direction', 'start', 'task', 'like', 'point', 'already', 'existing', 'project', 'describing', 'believe', 'module', 'tensorflow', 'could', 'useful', 'seen', 'help', 'would', 'much', 'appreciated']","['best', 'way', 'measur', 'lengthen', 'space', 'object', 'seri', 'imagesuppos', 'seri', 'imag', 'contain', 'number', 'specif', 'shape', 'one', 'imag', 'mostli', 'ident', 'asid', 'space', 'shape', 'interest', 'measur', 'distanc', 'center', 'shape', 'imag', 'step', 'determin', 'rel', 'lengthen', 'shorten', 'space', 'main', 'issu', 'actual', 'imag', 'much', 'rougher', 'exampl', 'shown', 'current', 'set', 'util', 'matlab', 'load', 'imag', 'use', 'contrast', 'differ', 'foreground', 'pictur', 'shape', 'identifi', 'calcul', 'centroid', 'method', 'alway', 'reliabl', 'especi', 'low', 'qualiti', 'imag', 'howev', 'shape', 'alway', 'visual', 'discern', 'interest', 'see', 'way', 'util', 'machin', 'learn', 'model', 'segment', 'shape', 'imag', 'calcul', 'space', 'one', 'machin', 'learn', 'wonder', 'anyon', 'point', 'right', 'direct', 'start', 'task', 'like', 'point', 'alreadi', 'exist', 'project', 'describ', 'believ', 'modul', 'tensorflow', 'could', 'use', 'seen', 'help', 'would', 'much', 'appreci']"
97,105,105,7952107,73098430,"I am following stefan-jansen/machine-learning-for-trading github installation environment instruction, but stuck in zipline ingest","<p>I am a machine learning novice and interested in quant. I am currently reading Stefan Jansen's book machine learning for algo trading. I am macOS Catalina version 10.15.7, Here is the link: <a href=""https://github.com/stefan-jansen/machine-learning-for-trading/tree/main/installation"" rel=""nofollow noreferrer"">https://github.com/stefan-jansen/machine-learning-for-trading/tree/main/installation</a></p>
<p>I am stuck in the Post-installation instructions at Ingesting Zipline data, after typing in ml4t environment. It will print traceback as follows:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/bin/zipline&quot;, line 7, in &lt;module&gt;
    from zipline.__main__ import main
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/__init__.py&quot;, line 29, in &lt;module&gt;
    from .utils.run_algo import run_algorithm
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/utils/run_algo.py&quot;, line 19, in &lt;module&gt;
    from zipline.data import bundles
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/data/bundles/__init__.py&quot;, line 2, in &lt;module&gt;
    from . import quandl  # noqa
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/data/bundles/quandl.py&quot;, line 15, in &lt;module&gt;
    from . import core as bundles
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/data/bundles/core.py&quot;, line 14, in &lt;module&gt;
    from ..bcolz_daily_bars import BcolzDailyBarReader, BcolzDailyBarWriter
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/zipline/data/bcolz_daily_bars.py&quot;, line 19, in &lt;module&gt;
    from bcolz import carray, ctable
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/bcolz/__init__.py&quot;, line 85, in &lt;module&gt;
    from bcolz.chunked_eval import eval
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/bcolz/chunked_eval.py&quot;, line 25, in &lt;module&gt;
    import dask.array as da
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/dask/array/__init__.py&quot;, line 3, in &lt;module&gt;
    from . import backends, fft, lib, linalg, ma, overlap, random
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/dask/array/backends.py&quot;, line 1, in &lt;module&gt;
    from .core import concatenate_lookup, einsum_lookup, tensordot_lookup
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/dask/array/core.py&quot;, line 19, in &lt;module&gt;
    from fsspec import get_mapper
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/fsspec/__init__.py&quot;, line 12, in &lt;module&gt;
    from .core import get_fs_token_paths, open, open_files, open_local
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/fsspec/core.py&quot;, line 18, in &lt;module&gt;
    from .compression import compr
  File &quot;/Users/zz/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/fsspec/compression.py&quot;, line 125, in &lt;module&gt;
    snappy.compress
AttributeError: module 'snappy' has no attribute 'compress'
</code></pre>
<p>I checked in ml4t environment, the python version is 3.8.8</p>
<p>I install and use miniconda3 of mac version.</p>
<p>I have installed pyenv to switch different python version as instructed.</p>
<p>I don't know what to do, I even use the installation/ml4t-base.yml previously, but somehow it throws error as well.</p>
<p>I think for now I just need to switch to a linux environment to try to install the environment. Any solutions/ways please recommend. Thanks!</p>
",27,0,-2,5,python-3.x;machine-learning;miniconda;quantitative-finance;zipline,2022-07-24 15:32:49,2022-07-24 15:32:49,2022-07-24 23:03:26,i am a machine learning novice and interested in quant  i am currently reading stefan jansen s book machine learning for algo trading  i am macos catalina version     here is the link   i am stuck in the post installation instructions at ingesting zipline data  after typing in mlt environment  it will print traceback as follows  i checked in mlt environment  the python version is    i install and use miniconda of mac version  i have installed pyenv to switch different python version as instructed  i don t know what to do  i even use the installation mlt base yml previously  but somehow it throws error as well  i think for now i just need to switch to a linux environment to try to install the environment  any solutions ways please recommend  thanks ,i am following stefan jansen machine learning for trading github installation environment instruction  but stuck in zipline ingest,machine learning novice interested quant currently reading stefan jansen book machine learning algo trading macos catalina version link stuck post installation instructions ingesting zipline data typing mlt environment print traceback follows checked mlt environment python version install use miniconda mac version installed pyenv switch different python version instructed know even use installation mlt base yml previously somehow throws error well think need switch linux environment try install environment solutions ways please recommend thanks,following stefan jansen machine learning trading github installation environment instruction stuck zipline ingest,following stefan jansen machine learning trading github installation environment instruction stuck zipline ingestmachine learning novice interested quant currently reading stefan jansen book machine learning algo trading macos catalina version link stuck post installation instructions ingesting zipline data typing mlt environment print traceback follows checked mlt environment python version install use miniconda mac version installed pyenv switch different python version instructed know even use installation mlt base yml previously somehow throws error well think need switch linux environment try install environment solutions ways please recommend thanks,"['following', 'stefan', 'jansen', 'machine', 'learning', 'trading', 'github', 'installation', 'environment', 'instruction', 'stuck', 'zipline', 'ingestmachine', 'learning', 'novice', 'interested', 'quant', 'currently', 'reading', 'stefan', 'jansen', 'book', 'machine', 'learning', 'algo', 'trading', 'macos', 'catalina', 'version', 'link', 'stuck', 'post', 'installation', 'instructions', 'ingesting', 'zipline', 'data', 'typing', 'mlt', 'environment', 'print', 'traceback', 'follows', 'checked', 'mlt', 'environment', 'python', 'version', 'install', 'use', 'miniconda', 'mac', 'version', 'installed', 'pyenv', 'switch', 'different', 'python', 'version', 'instructed', 'know', 'even', 'use', 'installation', 'mlt', 'base', 'yml', 'previously', 'somehow', 'throws', 'error', 'well', 'think', 'need', 'switch', 'linux', 'environment', 'try', 'install', 'environment', 'solutions', 'ways', 'please', 'recommend', 'thanks']","['follow', 'stefan', 'jansen', 'machin', 'learn', 'trade', 'github', 'instal', 'environ', 'instruct', 'stuck', 'ziplin', 'ingestmachin', 'learn', 'novic', 'interest', 'quant', 'current', 'read', 'stefan', 'jansen', 'book', 'machin', 'learn', 'algo', 'trade', 'maco', 'catalina', 'version', 'link', 'stuck', 'post', 'instal', 'instruct', 'ingest', 'ziplin', 'data', 'type', 'mlt', 'environ', 'print', 'traceback', 'follow', 'check', 'mlt', 'environ', 'python', 'version', 'instal', 'use', 'miniconda', 'mac', 'version', 'instal', 'pyenv', 'switch', 'differ', 'python', 'version', 'instruct', 'know', 'even', 'use', 'instal', 'mlt', 'base', 'yml', 'previous', 'somehow', 'throw', 'error', 'well', 'think', 'need', 'switch', 'linux', 'environ', 'tri', 'instal', 'environ', 'solut', 'way', 'pleas', 'recommend', 'thank']"
98,106,106,16518807,73099191,The best online machine learning services that cover as many concepts as possible?,"<p>I'm wondering if there are online courses that cover all AI concepts. There are tons of videos on learning to march, but in almost all cases they only focus on a few of them (like deep learning) and don't cover other concepts like (symbolic AI)</p>
<p>I'd like something like &quot;Artificial Intelligence: A Modern Approach by Peter Norvig and Stuart J. Russell&quot; but in a series of videos.</p>
",23,1,-3,1,machine-learning,2022-07-24 17:16:50,2022-07-24 17:16:50,2022-07-24 22:58:40,i m wondering if there are online courses that cover all ai concepts  there are tons of videos on learning to march  but in almost all cases they only focus on a few of them  like deep learning  and don t cover other concepts like  symbolic ai  i d like something like  artificial intelligence  a modern approach by peter norvig and stuart j  russell  but in a series of videos ,the best online machine learning services that cover as many concepts as possible ,wondering online courses cover ai concepts tons videos learning march almost cases focus like deep learning cover concepts like symbolic ai like something like artificial intelligence modern approach peter norvig stuart j russell series videos,best online machine learning services cover many concepts possible,best online machine learning services cover many concepts possiblewondering online courses cover ai concepts tons videos learning march almost cases focus like deep learning cover concepts like symbolic ai like something like artificial intelligence modern approach peter norvig stuart j russell series videos,"['best', 'online', 'machine', 'learning', 'services', 'cover', 'many', 'concepts', 'possiblewondering', 'online', 'courses', 'cover', 'ai', 'concepts', 'tons', 'videos', 'learning', 'march', 'almost', 'cases', 'focus', 'like', 'deep', 'learning', 'cover', 'concepts', 'like', 'symbolic', 'ai', 'like', 'something', 'like', 'artificial', 'intelligence', 'modern', 'approach', 'peter', 'norvig', 'stuart', 'j', 'russell', 'series', 'videos']","['best', 'onlin', 'machin', 'learn', 'servic', 'cover', 'mani', 'concept', 'possiblewond', 'onlin', 'cours', 'cover', 'ai', 'concept', 'ton', 'video', 'learn', 'march', 'almost', 'case', 'focu', 'like', 'deep', 'learn', 'cover', 'concept', 'like', 'symbol', 'ai', 'like', 'someth', 'like', 'artifici', 'intellig', 'modern', 'approach', 'peter', 'norvig', 'stuart', 'j', 'russel', 'seri', 'video']"
99,107,107,10018602,72850230,"Creating and Merging Multiple Datasets Does Not Fit Into Memory, Use Dask?","<p>I'm not quite sure how to ask this question, but I need some clarification on how to make use of Dask's ability to &quot;handle datasets that don't fit into memory&quot;, because I'm a little confused on how it works from the CREATION of these datasets.</p>
<p>I have made a reproducible code below that closely emulates my problem. Although this example DOES fit into my 16Gb memory, we can assume that it doesn't because it does take up ALMOST all of my RAM.</p>
<p>I'm working with 1min, 5min, 15min and Daily stock market datasets, all of which have their own technical indicators, so each of these separate dataframes are 234 columns in width, with the 1min dataset having the most rows (521,811), and going down from there. Each of these datasets can be created and fit into memory on their own, but here's where it gets tricky.</p>
<p>I'm trying to merge them column-wise into 1 dataframe, each column prepended with their respective timeframes so I can tell them apart, but this creates the memory problem. This is what I'm looking to accomplish visually:</p>
<p><a href=""https://i.stack.imgur.com/DEtpN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DEtpN.png"" alt=""DesiredOutcome"" /></a></p>
<p>I'm not really sure if Dask is what I need here, but I assume so. I'm NOT looking to use any kind of &quot;parallel calculations&quot; here (yet), I just need a way to create this dataframe before feeding it into a machine learning pipeline (yes, I know it's a stock market problem, just overlook that for now). I know Dask has a machine learning pipeline I can use, so maybe I'll make use of that in the future, however I need a way to save this big dataframe to disk, or create it upon importing it on the fly.</p>
<p>What I need help with is how to do this. Seeing as each of these datasets on their own fit into memory nicely, an idea I had (and this may not be correct at all so please let me know), would be to save each of the dataframes to separate parquet files to disk, then create a Dask dataframe object to import each of them into, when I go to start the machine learning pipeline. Something like this:</p>
<p><a href=""https://i.stack.imgur.com/rR8Co.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rR8Co.png"" alt=""Idea1"" /></a></p>
<p>Is this conceptually correct with what I need to do, or am I way off? haha. I've read through the documentation on Dask, and also checked out <a href=""https://docs.dask.org/en/latest/10-minutes-to-dask.html"" rel=""nofollow noreferrer"">this guide</a> specifically, which is good, however as a newbie I need some guidance with how to do this for the first time.</p>
<p>How can I create and save this big merged dataframe to disk, if I can't create it in memory in the first place?</p>
<p>Here is my reproducible dataframe/memory problem code. Be careful when you go to run this as it'll eat up your RAM pretty quickly, I have 16Gb of RAM and it does run on my fairly light machine, but not without some red-lining RAM, just wanted to give the Dask gods out there something specific to work with. Thanks!</p>
<pre><code>from pandas import DataFrame, date_range, merge
from numpy import random

# ------------------------------------------------------------------------------------------------ #
#                                         1 MINUTE DATASET                                         #
# ------------------------------------------------------------------------------------------------ #
ONE_MIN_NUM_OF_ROWS = 521811
ONE_MIN_NUM_OF_COLS = 234
main_df = DataFrame(random.randint(0,100, size=(ONE_MIN_NUM_OF_ROWS, ONE_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;col_&quot; + str(x) for x in range(ONE_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;min&quot;, periods=ONE_MIN_NUM_OF_ROWS))


# ------------------------------------------------------------------------------------------------ #
#                                         5 MINUTE DATASET                                         #
# ------------------------------------------------------------------------------------------------ #
FIVE_MIN_NUM_OF_ROWS = 117732
FIVE_MIN_NUM_OF_COLS = 234
five_min_df = DataFrame(random.randint(0,100, size=(FIVE_MIN_NUM_OF_ROWS, FIVE_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;5_min_col_&quot; + str(x) for x in range(FIVE_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;5min&quot;, periods=FIVE_MIN_NUM_OF_ROWS))
# Merge the 5 minute to the 1 minute df
main_df = merge(main_df, five_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                         15 MINUTE DATASET                                        #
# ------------------------------------------------------------------------------------------------ #
FIFTEEN_MIN_NUM_OF_ROWS = 117732
FIFTEEN_MIN_NUM_OF_COLS = 234
fifteen_min_df = DataFrame(random.randint(0,100, size=(FIFTEEN_MIN_NUM_OF_ROWS, FIFTEEN_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;15_min_col_&quot; + str(x) for x in range(FIFTEEN_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;15min&quot;, periods=FIFTEEN_MIN_NUM_OF_ROWS))
# Merge the 15 minute to the main df
main_df = merge(main_df, fifteen_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                           DAILY DATASET                                          #
# ------------------------------------------------------------------------------------------------ #
DAILY_NUM_OF_ROWS = 933
DAILY_NUM_OF_COLS = 234
fifteen_min_df = DataFrame(random.randint(0,100, size=(DAILY_NUM_OF_ROWS, DAILY_NUM_OF_COLS)), 
                    columns=list(&quot;daily_col_&quot; + str(x) for x in range(DAILY_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;D&quot;, periods=DAILY_NUM_OF_ROWS))
# Merge the daily to the main df (don't worry about &quot;forward peaking&quot; dates)
main_df = merge(main_df, fifteen_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                            FFILL NAN's                                           #
# ------------------------------------------------------------------------------------------------ #
main_df = main_df.fillna(method=&quot;ffill&quot;)

# ------------------------------------------------------------------------------------------------ #
#                                              INSPECT                                             #
# ------------------------------------------------------------------------------------------------ #
print(main_df)
</code></pre>
<p><strong>UPDATE</strong></p>
<p>Thanks to the top answer below, I'm getting closer to my solution.</p>
<p>I've fixed a few syntax errors in the code, and have a working example, UP TO the daily timeframe. When I use the <code>1B</code> timeframe for upsampling to business days, the error is:</p>
<p><code>ValueError: &lt;BusinessDay&gt; is a non-fixed frequency</code></p>
<p>I think it has something to do with this line:</p>
<p><code>data_index = higher_resolution_index.floor(data_freq).drop_duplicates()</code></p>
<p>...as that's what I see in the traceback. I don't think Pandas likes the <code>1B</code> timeframe and the <code>floor()</code> function, so is there an alternative?</p>
<p>I need to have daily data in there too, however the code works for every other timeframe. Once I can get this daily thing figured out, I'll be able to apply it to my use case.</p>
<p>Thanks!</p>
<pre><code>from pandas import DataFrame, concat, date_range
from numpy import random
import dask.dataframe as dd, dask.delayed

ROW_CHUNK_SIZE = 5000

def load_data_subset(start_date, freq, data_freq, hf_periods):
    higher_resolution_index = date_range(start_date, freq=freq, periods=hf_periods)
    data_index = higher_resolution_index.floor(data_freq).drop_duplicates()
    dummy_response = DataFrame(
        random.randint(0, 100, size=(len(data_index), 234)),
        columns=list(
            f&quot;{data_freq}_col_&quot; + str(x) for x in range(234)
        ),
        index=data_index
    )
    dummy_response = dummy_response.loc[higher_resolution_index.floor(data_freq)].set_axis(higher_resolution_index)
    return dummy_response

@dask.delayed
def load_all_columns_for_subset(start_date, freq, hf_periods):
    return concat(
        [
            load_data_subset(start_date, freq, &quot;1min&quot;, hf_periods),
            load_data_subset(start_date, freq, &quot;5min&quot;, hf_periods),
            load_data_subset(start_date, freq, &quot;15min&quot;, hf_periods),
            load_data_subset(start_date, freq, &quot;1H&quot;, hf_periods),
            load_data_subset(start_date, freq, &quot;4H&quot;, hf_periods),
            load_data_subset(start_date, freq, &quot;1B&quot;, hf_periods),
        ],
        axis=1,
    )

ONE_MIN_NUM_OF_ROWS = 521811
full_index = date_range(
    start=&quot;2019-12-09 04:00:00&quot;,
    freq=&quot;1min&quot;,
    periods=ONE_MIN_NUM_OF_ROWS,
)

df = dask.dataframe.from_delayed([load_all_columns_for_subset(full_index[i], freq=&quot;1min&quot;, hf_periods=ROW_CHUNK_SIZE) for i in range(0, ONE_MIN_NUM_OF_ROWS, ROW_CHUNK_SIZE)])

# Save df to parquet here when ready
</code></pre>
",117,1,1,5,python;pandas;dataframe;dask;dask-dataframe,2022-07-04 01:10:09,2022-07-04 01:10:09,2022-07-24 19:48:21,i m not quite sure how to ask this question  but i need some clarification on how to make use of dask s ability to  handle datasets that don t fit into memory   because i m a little confused on how it works from the creation of these datasets  i have made a reproducible code below that closely emulates my problem  although this example does fit into my gb memory  we can assume that it doesn t because it does take up almost all of my ram  i m working with min  min  min and daily stock market datasets  all of which have their own technical indicators  so each of these separate dataframes are  columns in width  with the min dataset having the most rows      and going down from there  each of these datasets can be created and fit into memory on their own  but here s where it gets tricky  i m trying to merge them column wise into  dataframe  each column prepended with their respective timeframes so i can tell them apart  but this creates the memory problem  this is what i m looking to accomplish visually   i m not really sure if dask is what i need here  but i assume so  i m not looking to use any kind of  parallel calculations  here  yet   i just need a way to create this dataframe before feeding it into a machine learning pipeline  yes  i know it s a stock market problem  just overlook that for now   i know dask has a machine learning pipeline i can use  so maybe i ll make use of that in the future  however i need a way to save this big dataframe to disk  or create it upon importing it on the fly  what i need help with is how to do this  seeing as each of these datasets on their own fit into memory nicely  an idea i had  and this may not be correct at all so please let me know   would be to save each of the dataframes to separate parquet files to disk  then create a dask dataframe object to import each of them into  when i go to start the machine learning pipeline  something like this   is this conceptually correct with what i need to do  or am i way off  haha  i ve read through the documentation on dask  and also checked out  specifically  which is good  however as a newbie i need some guidance with how to do this for the first time  how can i create and save this big merged dataframe to disk  if i can t create it in memory in the first place  here is my reproducible dataframe memory problem code  be careful when you go to run this as it ll eat up your ram pretty quickly  i have gb of ram and it does run on my fairly light machine  but not without some red lining ram  just wanted to give the dask gods out there something specific to work with  thanks  update thanks to the top answer below  i m getting closer to my solution  i ve fixed a few syntax errors in the code  and have a working example  up to the daily timeframe  when i use the b timeframe for upsampling to business days  the error is  valueerror   lt businessday gt  is a non fixed frequency i think it has something to do with this line  data_index   higher_resolution_index floor data_freq  drop_duplicates      as that s what i see in the traceback  i don t think pandas likes the b timeframe and the floor   function  so is there an alternative  i need to have daily data in there too  however the code works for every other timeframe  once i can get this daily thing figured out  i ll be able to apply it to my use case  thanks ,creating and merging multiple datasets does not fit into memory  use dask ,quite sure ask question need clarification make use dask ability handle datasets fit memory little confused works creation datasets made reproducible code closely emulates problem although example fit gb memory assume take almost ram working min min min daily stock market datasets technical indicators separate dataframes columns width min dataset rows going datasets created fit memory gets tricky trying merge column wise dataframe column prepended respective timeframes tell apart creates memory problem looking accomplish visually really sure dask need assume looking use kind parallel calculations yet need way create dataframe feeding machine learning pipeline yes know stock market problem overlook know dask machine learning pipeline use maybe make use future however need way save big dataframe disk create upon importing fly need help seeing datasets fit memory nicely idea may correct please let know would save dataframes separate parquet files disk create dask dataframe object import go start machine learning pipeline something like conceptually correct need way haha read documentation dask also checked specifically good however newbie need guidance first time create save big merged dataframe disk create memory first place reproducible dataframe memory problem code careful go run eat ram pretty quickly gb ram run fairly light machine without red lining ram wanted give dask gods something specific work thanks update thanks top answer getting closer solution fixed syntax errors code working example daily timeframe use b timeframe upsampling business days error valueerror lt businessday gt non fixed frequency think something line data_index higher_resolution_index floor data_freq drop_duplicates see traceback think pandas likes b timeframe floor function alternative need daily data however code works every timeframe get daily thing figured able apply use case thanks,creating merging multiple datasets fit memory use dask,creating merging multiple datasets fit memory use daskquite sure ask question need clarification make use dask ability handle datasets fit memory little confused works creation datasets made reproducible code closely emulates problem although example fit gb memory assume take almost ram working min min min daily stock market datasets technical indicators separate dataframes columns width min dataset rows going datasets created fit memory gets tricky trying merge column wise dataframe column prepended respective timeframes tell apart creates memory problem looking accomplish visually really sure dask need assume looking use kind parallel calculations yet need way create dataframe feeding machine learning pipeline yes know stock market problem overlook know dask machine learning pipeline use maybe make use future however need way save big dataframe disk create upon importing fly need help seeing datasets fit memory nicely idea may correct please let know would save dataframes separate parquet files disk create dask dataframe object import go start machine learning pipeline something like conceptually correct need way haha read documentation dask also checked specifically good however newbie need guidance first time create save big merged dataframe disk create memory first place reproducible dataframe memory problem code careful go run eat ram pretty quickly gb ram run fairly light machine without red lining ram wanted give dask gods something specific work thanks update thanks top answer getting closer solution fixed syntax errors code working example daily timeframe use b timeframe upsampling business days error valueerror lt businessday gt non fixed frequency think something line data_index higher_resolution_index floor data_freq drop_duplicates see traceback think pandas likes b timeframe floor function alternative need daily data however code works every timeframe get daily thing figured able apply use case thanks,"['creating', 'merging', 'multiple', 'datasets', 'fit', 'memory', 'use', 'daskquite', 'sure', 'ask', 'question', 'need', 'clarification', 'make', 'use', 'dask', 'ability', 'handle', 'datasets', 'fit', 'memory', 'little', 'confused', 'works', 'creation', 'datasets', 'made', 'reproducible', 'code', 'closely', 'emulates', 'problem', 'although', 'example', 'fit', 'gb', 'memory', 'assume', 'take', 'almost', 'ram', 'working', 'min', 'min', 'min', 'daily', 'stock', 'market', 'datasets', 'technical', 'indicators', 'separate', 'dataframes', 'columns', 'width', 'min', 'dataset', 'rows', 'going', 'datasets', 'created', 'fit', 'memory', 'gets', 'tricky', 'trying', 'merge', 'column', 'wise', 'dataframe', 'column', 'prepended', 'respective', 'timeframes', 'tell', 'apart', 'creates', 'memory', 'problem', 'looking', 'accomplish', 'visually', 'really', 'sure', 'dask', 'need', 'assume', 'looking', 'use', 'kind', 'parallel', 'calculations', 'yet', 'need', 'way', 'create', 'dataframe', 'feeding', 'machine', 'learning', 'pipeline', 'yes', 'know', 'stock', 'market', 'problem', 'overlook', 'know', 'dask', 'machine', 'learning', 'pipeline', 'use', 'maybe', 'make', 'use', 'future', 'however', 'need', 'way', 'save', 'big', 'dataframe', 'disk', 'create', 'upon', 'importing', 'fly', 'need', 'help', 'seeing', 'datasets', 'fit', 'memory', 'nicely', 'idea', 'may', 'correct', 'please', 'let', 'know', 'would', 'save', 'dataframes', 'separate', 'parquet', 'files', 'disk', 'create', 'dask', 'dataframe', 'object', 'import', 'go', 'start', 'machine', 'learning', 'pipeline', 'something', 'like', 'conceptually', 'correct', 'need', 'way', 'haha', 'read', 'documentation', 'dask', 'also', 'checked', 'specifically', 'good', 'however', 'newbie', 'need', 'guidance', 'first', 'time', 'create', 'save', 'big', 'merged', 'dataframe', 'disk', 'create', 'memory', 'first', 'place', 'reproducible', 'dataframe', 'memory', 'problem', 'code', 'careful', 'go', 'run', 'eat', 'ram', 'pretty', 'quickly', 'gb', 'ram', 'run', 'fairly', 'light', 'machine', 'without', 'red', 'lining', 'ram', 'wanted', 'give', 'dask', 'gods', 'something', 'specific', 'work', 'thanks', 'update', 'thanks', 'top', 'answer', 'getting', 'closer', 'solution', 'fixed', 'syntax', 'errors', 'code', 'working', 'example', 'daily', 'timeframe', 'use', 'b', 'timeframe', 'upsampling', 'business', 'days', 'error', 'valueerror', 'lt', 'businessday', 'gt', 'non', 'fixed', 'frequency', 'think', 'something', 'line', 'data_index', 'higher_resolution_index', 'floor', 'data_freq', 'drop_duplicates', 'see', 'traceback', 'think', 'pandas', 'likes', 'b', 'timeframe', 'floor', 'function', 'alternative', 'need', 'daily', 'data', 'however', 'code', 'works', 'every', 'timeframe', 'get', 'daily', 'thing', 'figured', 'able', 'apply', 'use', 'case', 'thanks']","['creat', 'merg', 'multipl', 'dataset', 'fit', 'memori', 'use', 'daskquit', 'sure', 'ask', 'question', 'need', 'clarif', 'make', 'use', 'dask', 'abil', 'handl', 'dataset', 'fit', 'memori', 'littl', 'confus', 'work', 'creation', 'dataset', 'made', 'reproduc', 'code', 'close', 'emul', 'problem', 'although', 'exampl', 'fit', 'gb', 'memori', 'assum', 'take', 'almost', 'ram', 'work', 'min', 'min', 'min', 'daili', 'stock', 'market', 'dataset', 'technic', 'indic', 'separ', 'datafram', 'column', 'width', 'min', 'dataset', 'row', 'go', 'dataset', 'creat', 'fit', 'memori', 'get', 'tricki', 'tri', 'merg', 'column', 'wise', 'datafram', 'column', 'prepend', 'respect', 'timefram', 'tell', 'apart', 'creat', 'memori', 'problem', 'look', 'accomplish', 'visual', 'realli', 'sure', 'dask', 'need', 'assum', 'look', 'use', 'kind', 'parallel', 'calcul', 'yet', 'need', 'way', 'creat', 'datafram', 'feed', 'machin', 'learn', 'pipelin', 'ye', 'know', 'stock', 'market', 'problem', 'overlook', 'know', 'dask', 'machin', 'learn', 'pipelin', 'use', 'mayb', 'make', 'use', 'futur', 'howev', 'need', 'way', 'save', 'big', 'datafram', 'disk', 'creat', 'upon', 'import', 'fli', 'need', 'help', 'see', 'dataset', 'fit', 'memori', 'nice', 'idea', 'may', 'correct', 'pleas', 'let', 'know', 'would', 'save', 'datafram', 'separ', 'parquet', 'file', 'disk', 'creat', 'dask', 'datafram', 'object', 'import', 'go', 'start', 'machin', 'learn', 'pipelin', 'someth', 'like', 'conceptu', 'correct', 'need', 'way', 'haha', 'read', 'document', 'dask', 'also', 'check', 'specif', 'good', 'howev', 'newbi', 'need', 'guidanc', 'first', 'time', 'creat', 'save', 'big', 'merg', 'datafram', 'disk', 'creat', 'memori', 'first', 'place', 'reproduc', 'datafram', 'memori', 'problem', 'code', 'care', 'go', 'run', 'eat', 'ram', 'pretti', 'quickli', 'gb', 'ram', 'run', 'fairli', 'light', 'machin', 'without', 'red', 'line', 'ram', 'want', 'give', 'dask', 'god', 'someth', 'specif', 'work', 'thank', 'updat', 'thank', 'top', 'answer', 'get', 'closer', 'solut', 'fix', 'syntax', 'error', 'code', 'work', 'exampl', 'daili', 'timefram', 'use', 'b', 'timefram', 'upsampl', 'busi', 'day', 'error', 'valueerror', 'lt', 'businessday', 'gt', 'non', 'fix', 'frequenc', 'think', 'someth', 'line', 'data_index', 'higher_resolution_index', 'floor', 'data_freq', 'drop_dupl', 'see', 'traceback', 'think', 'panda', 'like', 'b', 'timefram', 'floor', 'function', 'altern', 'need', 'daili', 'data', 'howev', 'code', 'work', 'everi', 'timefram', 'get', 'daili', 'thing', 'figur', 'abl', 'appli', 'use', 'case', 'thank']"
100,108,108,17679327,73099122,Machine learning algorithms,"<p>For the research purpose using ECG signal i need to predict the heart function whether it is normal or abnormal by using the machine learning models. What are the Machine learning algorithm used to predict it? Can anyone suggest me?</p>
",17,0,-1,1,machine-learning,2022-07-24 17:09:04,2022-07-24 17:09:04,2022-07-24 17:09:04,for the research purpose using ecg signal i need to predict the heart function whether it is normal or abnormal by using the machine learning models  what are the machine learning algorithm used to predict it  can anyone suggest me ,machine learning algorithms,research purpose using ecg signal need predict heart function whether normal abnormal using machine learning models machine learning algorithm used predict anyone suggest,machine learning algorithms,machine learning algorithmsresearch purpose using ecg signal need predict heart function whether normal abnormal using machine learning models machine learning algorithm used predict anyone suggest,"['machine', 'learning', 'algorithmsresearch', 'purpose', 'using', 'ecg', 'signal', 'need', 'predict', 'heart', 'function', 'whether', 'normal', 'abnormal', 'using', 'machine', 'learning', 'models', 'machine', 'learning', 'algorithm', 'used', 'predict', 'anyone', 'suggest']","['machin', 'learn', 'algorithmsresearch', 'purpos', 'use', 'ecg', 'signal', 'need', 'predict', 'heart', 'function', 'whether', 'normal', 'abnorm', 'use', 'machin', 'learn', 'model', 'machin', 'learn', 'algorithm', 'use', 'predict', 'anyon', 'suggest']"
101,109,109,10081871,73098288,Pose estimation based on cases generated via Hidden Markov model,"<p>I'm looking to generate a list of pose cases based on sensor data (imu, angle sensors, etc) by feeding gathered data into a hidden Markov model. I'm trying to figure out how to use a hmm to generate pose cases so that my system will be able to predict the movement of a user.</p>
<p>After some brief research online I found <a href=""https://hmmlearn.readthedocs.io/en/latest/tutorial.html"" rel=""nofollow noreferrer"">https://hmmlearn.readthedocs.io/en/latest/tutorial.html</a> and <a href=""https://www.scitepress.org/Papers/2020/93575/93575.pdf"" rel=""nofollow noreferrer"">https://www.scitepress.org/Papers/2020/93575/93575.pdf</a></p>
<p>But I'm pretty new to machine learning and after looking through both I'm pretty confused. I understand how hmm's work on a basic level but I'm not sure how to apply them specifically to this case.</p>
<p>I would greatly appreciate any ideas on how to approach this and what aspects of a hmm to use specifically (and with things like types of emissions, etc. )</p>
<p>Looking to develop the model in python.</p>
",22,0,-4,3,machine-learning;data-science;hidden-markov-models,2022-07-24 15:09:32,2022-07-24 15:09:32,2022-07-24 17:03:17,i m looking to generate a list of pose cases based on sensor data  imu  angle sensors  etc  by feeding gathered data into a hidden markov model  i m trying to figure out how to use a hmm to generate pose cases so that my system will be able to predict the movement of a user  after some brief research online i found  and  but i m pretty new to machine learning and after looking through both i m pretty confused  i understand how hmm s work on a basic level but i m not sure how to apply them specifically to this case  i would greatly appreciate any ideas on how to approach this and what aspects of a hmm to use specifically  and with things like types of emissions  etc    looking to develop the model in python ,pose estimation based on cases generated via hidden markov model,looking generate pose cases based sensor data imu angle sensors etc feeding gathered data hidden markov model trying figure use hmm generate pose cases system able predict movement user brief research online found pretty machine learning looking pretty confused understand hmm work basic level sure apply specifically case would greatly appreciate ideas approach aspects hmm use specifically things like types emissions etc looking develop model python,pose estimation based cases generated via hidden markov model,pose estimation based cases generated via hidden markov modellooking generate pose cases based sensor data imu angle sensors etc feeding gathered data hidden markov model trying figure use hmm generate pose cases system able predict movement user brief research online found pretty machine learning looking pretty confused understand hmm work basic level sure apply specifically case would greatly appreciate ideas approach aspects hmm use specifically things like types emissions etc looking develop model python,"['pose', 'estimation', 'based', 'cases', 'generated', 'via', 'hidden', 'markov', 'modellooking', 'generate', 'pose', 'cases', 'based', 'sensor', 'data', 'imu', 'angle', 'sensors', 'etc', 'feeding', 'gathered', 'data', 'hidden', 'markov', 'model', 'trying', 'figure', 'use', 'hmm', 'generate', 'pose', 'cases', 'system', 'able', 'predict', 'movement', 'user', 'brief', 'research', 'online', 'found', 'pretty', 'machine', 'learning', 'looking', 'pretty', 'confused', 'understand', 'hmm', 'work', 'basic', 'level', 'sure', 'apply', 'specifically', 'case', 'would', 'greatly', 'appreciate', 'ideas', 'approach', 'aspects', 'hmm', 'use', 'specifically', 'things', 'like', 'types', 'emissions', 'etc', 'looking', 'develop', 'model', 'python']","['pose', 'estim', 'base', 'case', 'gener', 'via', 'hidden', 'markov', 'modellook', 'gener', 'pose', 'case', 'base', 'sensor', 'data', 'imu', 'angl', 'sensor', 'etc', 'feed', 'gather', 'data', 'hidden', 'markov', 'model', 'tri', 'figur', 'use', 'hmm', 'gener', 'pose', 'case', 'system', 'abl', 'predict', 'movement', 'user', 'brief', 'research', 'onlin', 'found', 'pretti', 'machin', 'learn', 'look', 'pretti', 'confus', 'understand', 'hmm', 'work', 'basic', 'level', 'sure', 'appli', 'specif', 'case', 'would', 'greatli', 'appreci', 'idea', 'approach', 'aspect', 'hmm', 'use', 'specif', 'thing', 'like', 'type', 'emiss', 'etc', 'look', 'develop', 'model', 'python']"
102,111,111,16228929,73095790,Multicontainer sharing data within a pod,"<p>I have a pod with two containers:</p>
<p>Loader and Logic</p>
<p>Loader container is what client calls via rest api. It will load machine learning model from the cloud, store it on disk, then it will call the Logic container using localhost.</p>
<p>Logic will load the model into memory, do the inference and returns result back to Loader and Loader gives back the result to client.</p>
<p>At any time, the Logic container can remove the loaded model from memory.</p>
<p>How can I achieve this?</p>
<p>Currently I am looking at sharing volume, <a href=""https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/</a></p>
<p>Is there a better way?</p>
",41,0,0,2,kubernetes;containers,2022-07-24 07:31:01,2022-07-24 07:31:01,2022-07-24 07:31:01,i have a pod with two containers  loader and logic loader container is what client calls via rest api  it will load machine learning model from the cloud  store it on disk  then it will call the logic container using localhost  logic will load the model into memory  do the inference and returns result back to loader and loader gives back the result to client  at any time  the logic container can remove the loaded model from memory  how can i achieve this  currently i am looking at sharing volume   is there a better way ,multicontainer sharing data within a pod,pod two containers loader logic loader container client calls via rest api load machine learning model cloud store disk call logic container using localhost logic load model memory inference returns result back loader loader gives back result client time logic container remove loaded model memory achieve currently looking sharing volume better way,multicontainer sharing data within pod,multicontainer sharing data within podpod two containers loader logic loader container client calls via rest api load machine learning model cloud store disk call logic container using localhost logic load model memory inference returns result back loader loader gives back result client time logic container remove loaded model memory achieve currently looking sharing volume better way,"['multicontainer', 'sharing', 'data', 'within', 'podpod', 'two', 'containers', 'loader', 'logic', 'loader', 'container', 'client', 'calls', 'via', 'rest', 'api', 'load', 'machine', 'learning', 'model', 'cloud', 'store', 'disk', 'call', 'logic', 'container', 'using', 'localhost', 'logic', 'load', 'model', 'memory', 'inference', 'returns', 'result', 'back', 'loader', 'loader', 'gives', 'back', 'result', 'client', 'time', 'logic', 'container', 'remove', 'loaded', 'model', 'memory', 'achieve', 'currently', 'looking', 'sharing', 'volume', 'better', 'way']","['multicontain', 'share', 'data', 'within', 'podpod', 'two', 'contain', 'loader', 'logic', 'loader', 'contain', 'client', 'call', 'via', 'rest', 'api', 'load', 'machin', 'learn', 'model', 'cloud', 'store', 'disk', 'call', 'logic', 'contain', 'use', 'localhost', 'logic', 'load', 'model', 'memori', 'infer', 'return', 'result', 'back', 'loader', 'loader', 'give', 'back', 'result', 'client', 'time', 'logic', 'contain', 'remov', 'load', 'model', 'memori', 'achiev', 'current', 'look', 'share', 'volum', 'better', 'way']"
103,112,112,1731972,73094864,How do I do nested cross validation with sequentialfeatureselector?,"<p>I have the following snippet I've written for a nested cross validation loop, but I'm confused how I would incorporate <a href=""http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"" rel=""nofollow noreferrer"">sequentialFeatureSelector</a> into the mix as it has it's own CV statement.  I'm thinking I need to do something similar to the references of &quot;space = dict()&quot; in <a href=""https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/</a> or better yet, how would I use it with <a href=""https://pypi.org/project/nested-cv/"" rel=""nofollow noreferrer"">nested_cv</a></p>
<pre><code># configure the cross-validation procedure
outer_k = 10
inner_k = 10
random_st = sample(list(np.arange(0,10,1)),1)[0]
#print(random_st)

cv_inner = KFold(n_splits=inner_k, shuffle=True, random_state=random_st)
cv_outer = KFold(n_splits=outer_k, shuffle=True, random_state=random_st+1)

outer_results = []
for outer_train_ix, outer_test_ix in cv_outer.split(X.index):

    inner_results = []
    for inner_train_ix, inner_test_ix in cv_outer.split(outer_train_ix):
        print(&quot;inner_train_ix&quot;, inner_train_ix)
        print(&quot;inner_test_ix&quot;,inner_test_ix)
        
        #inner_results.append(inner_errors)
        
    #best_model parms selected from the loop above
    
    #best_model fitted to outer_train_ix, and out of sample errors are derived from outer_test_ix
       
    print(&quot;outer_train_ix&quot;,outer_train_ix)
    print(&quot;outer_test_ix&quot;,outer_test_ix)
    #outer_results.append(outer_errors)
        
#model that performed best on the outer (out of sample) forecasts is selected        
</code></pre>
",16,1,0,3,python;cross-validation;sequentialfeatureselector,2022-07-24 02:35:35,2022-07-24 02:35:35,2022-07-24 04:14:43,i have the following snippet i ve written for a nested cross validation loop  but i m confused how i would incorporate  into the mix as it has it s own cv statement   i m thinking i need to do something similar to the references of  space   dict    in  or better yet  how would i use it with ,how do i do nested cross validation with sequentialfeatureselector ,following snippet written nested cross validation loop confused would incorporate mix cv statement thinking need something similar references space dict better yet would use,nested cross validation sequentialfeatureselector,nested cross validation sequentialfeatureselectorfollowing snippet written nested cross validation loop confused would incorporate mix cv statement thinking need something similar references space dict better yet would use,"['nested', 'cross', 'validation', 'sequentialfeatureselectorfollowing', 'snippet', 'written', 'nested', 'cross', 'validation', 'loop', 'confused', 'would', 'incorporate', 'mix', 'cv', 'statement', 'thinking', 'need', 'something', 'similar', 'references', 'space', 'dict', 'better', 'yet', 'would', 'use']","['nest', 'cross', 'valid', 'sequentialfeatureselectorfollow', 'snippet', 'written', 'nest', 'cross', 'valid', 'loop', 'confus', 'would', 'incorpor', 'mix', 'cv', 'statement', 'think', 'need', 'someth', 'similar', 'refer', 'space', 'dict', 'better', 'yet', 'would', 'use']"
104,113,113,19547009,73073885,How do I put information not in a table into a dataframe from web scraping?,"<pre><code>import pandas as pd
from bs4 import BeautifulSoup
import numpy as np
import requests
from time import sleep
from random import randint
import re

towns = pd.DataFrame()

town_names = [f&quot;Abbeville-Alabama&quot;,
f&quot;Abernant-Alabama&quot;,
f&quot;Alpine-Utah&quot;,
f&quot;Dixon-Montana&quot;,
f&quot;Adak-Alaska&quot;,]

for town_name in town_names:
    page = requests.get(f&quot;https://www.city-data.com/city/{town_name}.html&quot;).text
    doc = BeautifulSoup(page, &quot;html.parser&quot;)

    print(town_name)
    sex_population = str(doc.find(id=&quot;population-by-sex&quot;))
    (males, females) = [float(x) for x in re.findall(r&quot;(?&lt;=\()[0-9]+\.[0-9]+(?=\%\))&quot;, sex_population)]
    print(males, females)

    # poverty_level = str(doc.find(id=&quot;poverty-level&quot;))
    # broke = float(re.findall(&quot;(&lt;?&lt;\/b&gt; )[0-9]*.[0-9]*&quot;, poverty_level))
    # print(broke)

    # religion_population = str(doc.find(id=&quot;religion&quot;))
    # atheist = float(re.findall(&quot;(?&lt;=None&lt;\/td&gt;&lt;td&gt;)[0-9,]*(?=&lt;\/td&gt;&lt;td&gt;)&quot;, religion_population)[0].replace(&quot;,&quot;, &quot;&quot;))
    # print(atheist)

    total_population = str(doc.find(id=&quot;city-population&quot;))
    residents = float(re.findall(&quot;(?&lt;=&lt;/b&gt; )[0-9]*&quot;, total_population)[0].replace(&quot;,&quot;, &quot;&quot;))
    print(residents)

    religion_population = doc.find(id=&quot;religion&quot;).find_all('tr')
    data = []
    for row in religion_population:
        columns = row.find_all('td')
        if columns:
            religion = columns[0].get_text(strip=True)
            number = columns[1].get_text(strip=True).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;,&quot;0&quot;)
            print(f'religion: {religion} | number: {number}')
            data.append([religion, int(number)])
    df = pd.DataFrame(data, columns=['religion', 'number'])
    df['percentage'] = (df['number'] / df['number'].sum()) * 100
    atheist=df[df.religion == &quot;None&quot;].iloc[0][&quot;percentage&quot;]
    evangelicals = df[df.religion == &quot;Evangelical Protestant&quot;].iloc[0][&quot;percentage&quot;]
    print(atheist)
    print(evangelicals)

    education_population = doc.find(id=&quot;education-info&quot;).find_all('b')
    data = []
    for row in education_population:
        columns = row.find_all('b')
        if columns:
            education = columns[0].get_text(strip=True)
            ed_number = columns[1].get_text(strip=True).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;, &quot;0&quot;)
            print(f'education: {education} | number: {ed_number}')
            data.append([education, int(ed_number)])
    df = pd.DataFrame(data, columns=['education', 'number'])
    df['percentage'] = (df['number'] / df['number'].sum()) * 100
    phds = df[df.education == &quot;Graduate or professional degree&quot;].iloc[0][&quot;percentage&quot;]
    highschoolgrads = df[df.education == &quot;High school or higher&quot;].iloc[0][&quot;percentage&quot;]
    print(phds)
    print(highschoolgrads)

    print(&quot;\n&quot;)
</code></pre>
<p>How do I get the education information into a dataframe? I'm attempting to organize the values into education level and percentage.</p>
<p>Also any idea why when I try to cast poverty level as a float in broke, it says it can't because it's a list?</p>
<p>At this point I'm just typing so that stackoverflow will allow me to post because it thinks I don't have enough details. SO... if anyone is interested what this is for I'm doing a datamining/machine learning project where it's going to take the scores from the HRC's municipality equality index and the info about the cities scored and try to learn how to estimate the score of cities which haven't been scored. It's my first machine learning project ever. I worry that I've picked something too big for my first time but I'm really already committed to it.</p>
",33,1,0,5,python;pandas;dataframe;web-scraping;beautifulsoup,2022-07-22 03:17:43,2022-07-22 03:17:43,2022-07-23 23:50:55,how do i get the education information into a dataframe  i m attempting to organize the values into education level and percentage  also any idea why when i try to cast poverty level as a float in broke  it says it can t because it s a list  at this point i m just typing so that stackoverflow will allow me to post because it thinks i don t have enough details  so    if anyone is interested what this is for i m doing a datamining machine learning project where it s going to take the scores from the hrc s municipality equality index and the info about the cities scored and try to learn how to estimate the score of cities which haven t been scored  it s my first machine learning project ever  i worry that i ve picked something too big for my first time but i m really already committed to it ,how do i put information not in a table into a dataframe from web scraping ,get education information dataframe attempting organize values education level percentage also idea try cast poverty level float broke says point typing stackoverflow allow post thinks enough details anyone interested datamining machine learning project going take scores hrc municipality equality index info cities scored try learn estimate score cities scored first machine learning project ever worry picked something big first time really already committed,put information table dataframe web scraping,put information table dataframe web scrapingget education information dataframe attempting organize values education level percentage also idea try cast poverty level float broke says point typing stackoverflow allow post thinks enough details anyone interested datamining machine learning project going take scores hrc municipality equality index info cities scored try learn estimate score cities scored first machine learning project ever worry picked something big first time really already committed,"['put', 'information', 'table', 'dataframe', 'web', 'scrapingget', 'education', 'information', 'dataframe', 'attempting', 'organize', 'values', 'education', 'level', 'percentage', 'also', 'idea', 'try', 'cast', 'poverty', 'level', 'float', 'broke', 'says', 'point', 'typing', 'stackoverflow', 'allow', 'post', 'thinks', 'enough', 'details', 'anyone', 'interested', 'datamining', 'machine', 'learning', 'project', 'going', 'take', 'scores', 'hrc', 'municipality', 'equality', 'index', 'info', 'cities', 'scored', 'try', 'learn', 'estimate', 'score', 'cities', 'scored', 'first', 'machine', 'learning', 'project', 'ever', 'worry', 'picked', 'something', 'big', 'first', 'time', 'really', 'already', 'committed']","['put', 'inform', 'tabl', 'datafram', 'web', 'scrapingget', 'educ', 'inform', 'datafram', 'attempt', 'organ', 'valu', 'educ', 'level', 'percentag', 'also', 'idea', 'tri', 'cast', 'poverti', 'level', 'float', 'broke', 'say', 'point', 'type', 'stackoverflow', 'allow', 'post', 'think', 'enough', 'detail', 'anyon', 'interest', 'datamin', 'machin', 'learn', 'project', 'go', 'take', 'score', 'hrc', 'municip', 'equal', 'index', 'info', 'citi', 'score', 'tri', 'learn', 'estim', 'score', 'citi', 'score', 'first', 'machin', 'learn', 'project', 'ever', 'worri', 'pick', 'someth', 'big', 'first', 'time', 'realli', 'alreadi', 'commit']"
105,114,114,15155605,73093000,How to use custom tensorflow lite models in flutter?,"<p>I am working on a flutter project that requires some machine learning. I have my model already trained and ready to be used.</p>
<p>How to integrate this <code>Tensorflow lite model</code> with flutter?</p>
<p>I have tried couple of packages that exists but unfortunately, I did not worked! Most of these packages are no longer maintained.</p>
",30,0,0,5,flutter;tensorflow;dart;machine-learning;tensorflow-lite,2022-07-23 20:46:27,2022-07-23 20:46:27,2022-07-23 23:36:45,i am working on a flutter project that requires some machine learning  i have my model already trained and ready to be used  how to integrate this tensorflow lite model with flutter  i have tried couple of packages that exists but unfortunately  i did not worked  most of these packages are no longer maintained ,how to use custom tensorflow lite models in flutter ,working flutter project requires machine learning model already trained ready used integrate tensorflow lite model flutter tried couple packages exists unfortunately worked packages longer maintained,use tensorflow lite models flutter,use tensorflow lite models flutterworking flutter project requires machine learning model already trained ready used integrate tensorflow lite model flutter tried couple packages exists unfortunately worked packages longer maintained,"['use', 'tensorflow', 'lite', 'models', 'flutterworking', 'flutter', 'project', 'requires', 'machine', 'learning', 'model', 'already', 'trained', 'ready', 'used', 'integrate', 'tensorflow', 'lite', 'model', 'flutter', 'tried', 'couple', 'packages', 'exists', 'unfortunately', 'worked', 'packages', 'longer', 'maintained']","['use', 'tensorflow', 'lite', 'model', 'flutterwork', 'flutter', 'project', 'requir', 'machin', 'learn', 'model', 'alreadi', 'train', 'readi', 'use', 'integr', 'tensorflow', 'lite', 'model', 'flutter', 'tri', 'coupl', 'packag', 'exist', 'unfortun', 'work', 'packag', 'longer', 'maintain']"
106,115,115,16578308,73088750,Echarts wordcloud 2 onclick,"<p>I am using word-cloud2 with echarts:</p>
 
<p>Is there any option to add onclick to each word on echarts word-cloud2. My code is in js and is as follows:</p>
<pre><code> var chart = echarts.init(document.getElementById('main'));

var option = {
    tooltip: {},
    series: [ {
        type: 'wordCloud',
        gridSize: 2,
        sizeRange: [12, 50],
        rotationRange: [-90, 90],
        shape: 'apple',
        width: 600,
        height: 400,
        tooltip: {
            width: 0,
            height: 0,
          },
        drawOutOfBound: true,
        
        data: [
            {
                name: 'Machine Learning',
                value: 10000,
                
            },
           
        ]
    } ]
};

chart.setOption(option);

window.onresize = chart.resize;
</code></pre>
",23,1,1,3,javascript;java;echarts,2022-07-23 10:12:35,2022-07-23 10:12:35,2022-07-23 19:55:28,i am using word cloud with echarts  is there any option to add onclick to each word on echarts word cloud  my code is in js and is as follows ,echarts wordcloud  onclick,using word cloud echarts option onclick word echarts word cloud code js follows,echarts wordcloud onclick,echarts wordcloud onclickusing word cloud echarts option onclick word echarts word cloud code js follows,"['echarts', 'wordcloud', 'onclickusing', 'word', 'cloud', 'echarts', 'option', 'onclick', 'word', 'echarts', 'word', 'cloud', 'code', 'js', 'follows']","['echart', 'wordcloud', 'onclickus', 'word', 'cloud', 'echart', 'option', 'onclick', 'word', 'echart', 'word', 'cloud', 'code', 'js', 'follow']"
107,116,116,1001242,57977309,mongoexport - issue with JSON query (extended JSON - Invalid JSON input),"<p>I have started learning MongoDB recently. Today the instructor taught us the <strong>mongoexport</strong> command. While practicing the same, I face a typical issue which none of the other batchmates including the instructor faced. I use <strong>MongoDB version 4.2.0</strong> on my <strong>Windows 10</strong> machine. </p>

<p>If I use mongoexport for my collection without any -q parameter to specify any filtering condition, it works fine.  </p>

<pre><code>mongoexport -d trainingdb -c employee -f empId,name,designation -o \mongoexport\all-employees.json

2019-09-17T18:00:30.300+0530    connected to: mongodb://localhost/
2019-09-17T18:00:30.314+0530    exported 3 records
</code></pre>

<p>However, whenever I specify the JSON query as -q (or --query) it gives an error as follows.</p>

<pre><code>mongoexport -d trainingdb -c employee -f empId,name,designation -q {'designation':'Developer'} -o \mongoexport\developers.json

2019-09-17T18:01:45.381+0530    connected to: mongodb://localhost/
2019-09-17T18:01:45.390+0530    Failed: error parsing query as Extended JSON: invalid JSON input
</code></pre>

<p>The same error persists in all the different flavors I had attempted with for the query.</p>

<pre><code>-q {'designation':'Developer'}
--query {'designation':'Developer'}
-q ""{'designation':'Developer'}""
</code></pre>

<p>I had even attempted with a different query condition on the 'empId' as -q {'empId':'1001'} But no luck. I keep getting the same error. </p>

<p>As per <a href=""https://stackoverflow.com/a/34394870/1001242"">one of the suggestions given in the StackOverflow website</a>, I tried with the following option but getting a different error.</p>

<pre><code>  -q '{""designation"":""Developer""}'
</code></pre>

<p>The error is :  '<em>query '[39 123 101 109 112 73 100 58 49 48 48 49 125 39]' is not valid JSON: json: cannot unmarshal string into Go value of type map[string]interface {}</em>'.</p>

<pre><code>2019-09-17T20:24:58.878+0530    query '[39 123 101 109 112 73 100 58 49 48 48 49 125 39]' is not valid JSON: json: cannot unmarshal string into Go value of type map[string]interface {}
2019-09-17T20:24:58.882+0530    try 'mongoexport --help' for more information
</code></pre>

<p>I am really not sure what is missing here ? Tried with a bit of Googling and also gone through the official <a href=""https://docs.mongodb.com/manual/reference/program/mongoexport/"" rel=""noreferrer"">MongoDB documentation of the mongoexport</a> - but no luck. </p>

<p>The employee collection in my system looks like the follows with 3 documents.</p>

<pre><code>&gt; db.employee.find().pretty()
{
        ""_id"" : ObjectId(""5d80d1ae0d4d526a42fd95ad""),
        ""empId"" : 1001,
        ""name"" : ""Raghavan"",
        ""designation"" : ""Developer""
}
{
        ""_id"" : ObjectId(""5d80d1b20d4d526a42fd95ae""),
        ""empId"" : 1002,
        ""name"" : ""Kannan"",
        ""designation"" : ""Architect""
}
{
        ""_id"" : ObjectId(""5d80d1b40d4d526a42fd95af""),
        ""empId"" : 1003,
        ""name"" : ""Sathish"",
        ""designation"" : ""Developer""
}
&gt;
</code></pre>

<p><strong>Update</strong></p>

<p>As suggested by @NikosM, I have saved the query in a .json file (<strong>query.json</strong>) and tried the same mongoexport command with the new approach. Still, no luck. Same Marshal error. </p>

<pre><code>cat query.json
{""designation"":""Developer""}

mongoexport -d trainingdb -c employee -f empId,name,designation -q 'query.json' -o \mongoexport\developers.json

2019-09-17T21:16:32.849+0530    query '[39 113 117 101 114 121 46 106 115 111 110 39]' is not valid JSON: json: cannot unmarshal string into Go value of type map[string]interface {}
2019-09-17T21:16:32.852+0530    try 'mongoexport --help' for more information
</code></pre>

<p>Any help on this will be highly appreciated. </p>
",12217,3,6,4,json;mongodb;mongoexport;json-query,2019-09-17 18:14:34,2019-09-17 18:14:34,2022-07-23 17:42:33,i have started learning mongodb recently  today the instructor taught us the mongoexport command  while practicing the same  i face a typical issue which none of the other batchmates including the instructor faced  i use mongodb version    on my windows  machine   if i use mongoexport for my collection without any  q parameter to specify any filtering condition  it works fine    however  whenever i specify the json query as  q  or   query  it gives an error as follows  the same error persists in all the different flavors i had attempted with for the query  i had even attempted with a different query condition on the  empid  as  q   empid      but no luck  i keep getting the same error   as per   i tried with the following option but getting a different error  the error is     query                   is not valid json  json  cannot unmarshal string into go value of type map string interface      i am really not sure what is missing here   tried with a bit of googling and also gone through the official    but no luck   the employee collection in my system looks like the follows with  documents  update as suggested by  nikosm  i have saved the query in a  json file  query json  and tried the same mongoexport command with the new approach  still  no luck  same marshal error   any help on this will be highly appreciated  ,mongoexport   issue with json query  extended json   invalid json input ,started learning mongodb recently today instructor taught us mongoexport command practicing face typical issue none batchmates including instructor faced use mongodb version windows machine use mongoexport collection without q parameter specify filtering condition works fine however whenever specify json query q query gives error follows error persists different flavors attempted query even attempted different query condition empid q empid luck keep getting error per tried following option getting different error error query valid json json cannot unmarshal string go value type map string interface really sure missing tried bit googling also gone official luck employee collection system looks like follows documents update suggested nikosm saved query json file query json tried mongoexport command approach still luck marshal error help highly appreciated,mongoexport issue json query extended json invalid json input,mongoexport issue json query extended json invalid json inputstarted learning mongodb recently today instructor taught us mongoexport command practicing face typical issue none batchmates including instructor faced use mongodb version windows machine use mongoexport collection without q parameter specify filtering condition works fine however whenever specify json query q query gives error follows error persists different flavors attempted query even attempted different query condition empid q empid luck keep getting error per tried following option getting different error error query valid json json cannot unmarshal string go value type map string interface really sure missing tried bit googling also gone official luck employee collection system looks like follows documents update suggested nikosm saved query json file query json tried mongoexport command approach still luck marshal error help highly appreciated,"['mongoexport', 'issue', 'json', 'query', 'extended', 'json', 'invalid', 'json', 'inputstarted', 'learning', 'mongodb', 'recently', 'today', 'instructor', 'taught', 'us', 'mongoexport', 'command', 'practicing', 'face', 'typical', 'issue', 'none', 'batchmates', 'including', 'instructor', 'faced', 'use', 'mongodb', 'version', 'windows', 'machine', 'use', 'mongoexport', 'collection', 'without', 'q', 'parameter', 'specify', 'filtering', 'condition', 'works', 'fine', 'however', 'whenever', 'specify', 'json', 'query', 'q', 'query', 'gives', 'error', 'follows', 'error', 'persists', 'different', 'flavors', 'attempted', 'query', 'even', 'attempted', 'different', 'query', 'condition', 'empid', 'q', 'empid', 'luck', 'keep', 'getting', 'error', 'per', 'tried', 'following', 'option', 'getting', 'different', 'error', 'error', 'query', 'valid', 'json', 'json', 'can', 'not', 'unmarshal', 'string', 'go', 'value', 'type', 'map', 'string', 'interface', 'really', 'sure', 'missing', 'tried', 'bit', 'googling', 'also', 'gone', 'official', 'luck', 'employee', 'collection', 'system', 'looks', 'like', 'follows', 'documents', 'update', 'suggested', 'nikosm', 'saved', 'query', 'json', 'file', 'query', 'json', 'tried', 'mongoexport', 'command', 'approach', 'still', 'luck', 'marshal', 'error', 'help', 'highly', 'appreciated']","['mongoexport', 'issu', 'json', 'queri', 'extend', 'json', 'invalid', 'json', 'inputstart', 'learn', 'mongodb', 'recent', 'today', 'instructor', 'taught', 'us', 'mongoexport', 'command', 'practic', 'face', 'typic', 'issu', 'none', 'batchmat', 'includ', 'instructor', 'face', 'use', 'mongodb', 'version', 'window', 'machin', 'use', 'mongoexport', 'collect', 'without', 'q', 'paramet', 'specifi', 'filter', 'condit', 'work', 'fine', 'howev', 'whenev', 'specifi', 'json', 'queri', 'q', 'queri', 'give', 'error', 'follow', 'error', 'persist', 'differ', 'flavor', 'attempt', 'queri', 'even', 'attempt', 'differ', 'queri', 'condit', 'empid', 'q', 'empid', 'luck', 'keep', 'get', 'error', 'per', 'tri', 'follow', 'option', 'get', 'differ', 'error', 'error', 'queri', 'valid', 'json', 'json', 'can', 'not', 'unmarsh', 'string', 'go', 'valu', 'type', 'map', 'string', 'interfac', 'realli', 'sure', 'miss', 'tri', 'bit', 'googl', 'also', 'gone', 'offici', 'luck', 'employe', 'collect', 'system', 'look', 'like', 'follow', 'document', 'updat', 'suggest', 'nikosm', 'save', 'queri', 'json', 'file', 'queri', 'json', 'tri', 'mongoexport', 'command', 'approach', 'still', 'luck', 'marshal', 'error', 'help', 'highli', 'appreci']"
108,117,117,15741639,68746891,Separating paragraphs in an image,"<p>I have a large reservoir of PDFs that get converted to images before ultimately being turned into text files.  I ultimately will be performing machine learning on the text and we prefer to have them in paragraph form at that point.</p>
<p>I've found the following thread that explains how to take an image and place a green box around each block the script considered to be a paragraph.  I need something that will (maybe using the coordinates this script generates to draw the boxes?) send the text into another data structure, like a list, and then create the final txt document with carriage returns between each &quot;paragraph&quot;.</p>
<p><a href=""https://stackoverflow.com/questions/57249273/how-to-detect-paragraphs-in-a-text-document-image-for-a-non-consistent-text-stru"">How to detect paragraphs in a text document image for a non-consistent text structure in Python</a></p>
",91,0,0,3,python;opencv;paragraph,2021-08-11 20:58:45,2021-08-11 20:58:45,2022-07-23 11:22:23,i have a large reservoir of pdfs that get converted to images before ultimately being turned into text files   i ultimately will be performing machine learning on the text and we prefer to have them in paragraph form at that point  i ve found the following thread that explains how to take an image and place a green box around each block the script considered to be a paragraph   i need something that will  maybe using the coordinates this script generates to draw the boxes   send the text into another data structure  like a list  and then create the final txt document with carriage returns between each  paragraph   ,separating paragraphs in an image,large reservoir pdfs get converted images ultimately turned text files ultimately performing machine learning text prefer paragraph form point found following thread explains take image place green box around block script considered paragraph need something maybe using coordinates script generates draw boxes send text another data structure like create final txt document carriage returns paragraph,separating paragraphs image,separating paragraphs imagelarge reservoir pdfs get converted images ultimately turned text files ultimately performing machine learning text prefer paragraph form point found following thread explains take image place green box around block script considered paragraph need something maybe using coordinates script generates draw boxes send text another data structure like create final txt document carriage returns paragraph,"['separating', 'paragraphs', 'imagelarge', 'reservoir', 'pdfs', 'get', 'converted', 'images', 'ultimately', 'turned', 'text', 'files', 'ultimately', 'performing', 'machine', 'learning', 'text', 'prefer', 'paragraph', 'form', 'point', 'found', 'following', 'thread', 'explains', 'take', 'image', 'place', 'green', 'box', 'around', 'block', 'script', 'considered', 'paragraph', 'need', 'something', 'maybe', 'using', 'coordinates', 'script', 'generates', 'draw', 'boxes', 'send', 'text', 'another', 'data', 'structure', 'like', 'create', 'final', 'txt', 'document', 'carriage', 'returns', 'paragraph']","['separ', 'paragraph', 'imagelarg', 'reservoir', 'pdf', 'get', 'convert', 'imag', 'ultim', 'turn', 'text', 'file', 'ultim', 'perform', 'machin', 'learn', 'text', 'prefer', 'paragraph', 'form', 'point', 'found', 'follow', 'thread', 'explain', 'take', 'imag', 'place', 'green', 'box', 'around', 'block', 'script', 'consid', 'paragraph', 'need', 'someth', 'mayb', 'use', 'coordin', 'script', 'gener', 'draw', 'box', 'send', 'text', 'anoth', 'data', 'structur', 'like', 'creat', 'final', 'txt', 'document', 'carriag', 'return', 'paragraph']"
109,118,118,5240878,73072705,How to handle alphanumeric values in machine learning,"<p>I am trying to the find the best algorithm for my claims data. The claims data include some diagnosis code which are alphanumeric like 'EA43454' . when i run the below code to evaluate the models</p>
<pre><code>models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    kfold = model_selection.KFold(n_splits=10, random_state=None)
    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = &quot;%s: %f (%f)&quot; % (name, cv_results.mean(), cv_results.std())
    print(msg)
</code></pre>
<p>i get the error</p>
<pre><code>ValueError: could not convert string to float: 'U0003' 
</code></pre>
<p>How to handle these alphanumeric values?</p>
",32,1,-2,3,machine-learning;scikit-learn;encoding,2022-07-22 00:14:53,2022-07-22 00:14:53,2022-07-23 01:44:02,i am trying to the find the best algorithm for my claims data  the claims data include some diagnosis code which are alphanumeric like  ea    when i run the below code to evaluate the models i get the error how to handle these alphanumeric values ,how to handle alphanumeric values in machine learning,trying find best algorithm claims data claims data include diagnosis code alphanumeric like ea run code evaluate models get error handle alphanumeric values,handle alphanumeric values machine learning,handle alphanumeric values machine learningtrying find best algorithm claims data claims data include diagnosis code alphanumeric like ea run code evaluate models get error handle alphanumeric values,"['handle', 'alphanumeric', 'values', 'machine', 'learningtrying', 'find', 'best', 'algorithm', 'claims', 'data', 'claims', 'data', 'include', 'diagnosis', 'code', 'alphanumeric', 'like', 'ea', 'run', 'code', 'evaluate', 'models', 'get', 'error', 'handle', 'alphanumeric', 'values']","['handl', 'alphanumer', 'valu', 'machin', 'learningtri', 'find', 'best', 'algorithm', 'claim', 'data', 'claim', 'data', 'includ', 'diagnosi', 'code', 'alphanumer', 'like', 'ea', 'run', 'code', 'evalu', 'model', 'get', 'error', 'handl', 'alphanumer', 'valu']"
110,119,119,1977614,73085944,KeyError: 0 from Python,"<p>I am trying to follow chapter 3 of Hands-On Machine Learning with Scikit-Learn and TensorFlow for classification of MNIST data. The command runs as follows in Jupyter notebook:</p>
<pre><code>&gt;&gt;&gt; from sklearn.datasets import fetch_openml
&gt;&gt;&gt; mnist = fetch_openml('mnist_784', version=1)
&gt;&gt;&gt; mnist.keys()
dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',
'categories', 'url'])

&gt;&gt;&gt; X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]
&gt;&gt;&gt; X.shape
(70000, 784)
&gt;&gt;&gt; y.shape
(70000,)
</code></pre>
<p>The following command throws error</p>
<pre><code>&gt;&gt;&gt; some_digit = X[0]
</code></pre>
<p>Error message:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3360             try:
-&gt; 3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:

~/anaconda3/lib/python3.7/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

~/anaconda3/lib/python3.7/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
&lt;ipython-input-43-348a6e96ae02&gt; in &lt;module&gt;
----&gt; 1 some_digit = X[0]

~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   3456             if self.columns.nlevels &gt; 1:
   3457                 return self._getitem_multilevel(key)
-&gt; 3458             indexer = self.columns.get_loc(key)
   3459             if is_integer(indexer):
   3460                 indexer = [indexer]

~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:
-&gt; 3363                 raise KeyError(key) from err
   3364 
   3365         if is_scalar(key) and isna(key) and not self.hasnans:

KeyError: 0
</code></pre>
<p>It is hard for me understand what the actual error is as I have not come across similar one for such a simple assignment. What is causing the issue?</p>
",52,1,1,2,python;pandas,2022-07-22 23:52:26,2022-07-22 23:52:26,2022-07-23 00:12:43,i am trying to follow chapter  of hands on machine learning with scikit learn and tensorflow for classification of mnist data  the command runs as follows in jupyter notebook  the following command throws error error message  it is hard for me understand what the actual error is as i have not come across similar one for such a simple assignment  what is causing the issue ,keyerror   from python,trying follow chapter hands machine learning scikit learn tensorflow classification mnist data command runs follows jupyter notebook following command throws error error message hard understand actual error come across similar one simple assignment causing issue,keyerror python,keyerror pythontrying follow chapter hands machine learning scikit learn tensorflow classification mnist data command runs follows jupyter notebook following command throws error error message hard understand actual error come across similar one simple assignment causing issue,"['keyerror', 'pythontrying', 'follow', 'chapter', 'hands', 'machine', 'learning', 'scikit', 'learn', 'tensorflow', 'classification', 'mnist', 'data', 'command', 'runs', 'follows', 'jupyter', 'notebook', 'following', 'command', 'throws', 'error', 'error', 'message', 'hard', 'understand', 'actual', 'error', 'come', 'across', 'similar', 'one', 'simple', 'assignment', 'causing', 'issue']","['keyerror', 'pythontri', 'follow', 'chapter', 'hand', 'machin', 'learn', 'scikit', 'learn', 'tensorflow', 'classif', 'mnist', 'data', 'command', 'run', 'follow', 'jupyt', 'notebook', 'follow', 'command', 'throw', 'error', 'error', 'messag', 'hard', 'understand', 'actual', 'error', 'come', 'across', 'similar', 'one', 'simpl', 'assign', 'caus', 'issu']"
111,120,120,19604111,73085891,I am new to machine learning and AI. how implement object detection for real time product for a company?,"<p>the object detection should be very fast and accurate at the same time. I have seen some algorithms like yolo, yolov4, yolov6, Single Shot Detector, etc. But to use it in an actual product that could be used in the real world. which one is best. should be fast and accurate. kindly guide me with some topics to look into. I have zero knowledge so I am willing to gain some knowledge. and do tesla cars use any of this... if so which one would it be just curious and might be helpful.</p>
",19,0,-1,5,python;tensorflow;object-detection;yolo;single-shot-detector,2022-07-22 23:46:12,2022-07-22 23:46:12,2022-07-22 23:54:13,the object detection should be very fast and accurate at the same time  i have seen some algorithms like yolo  yolov  yolov  single shot detector  etc  but to use it in an actual product that could be used in the real world  which one is best  should be fast and accurate  kindly guide me with some topics to look into  i have zero knowledge so i am willing to gain some knowledge  and do tesla cars use any of this    if so which one would it be just curious and might be helpful ,i am new to machine learning and ai  how implement object detection for real time product for a company ,object detection fast accurate time seen algorithms like yolo yolov yolov single shot detector etc use actual product could used real world one best fast accurate kindly guide topics look zero knowledge willing gain knowledge tesla cars use one would curious might helpful,machine learning ai implement object detection real time product company,machine learning ai implement object detection real time product companyobject detection fast accurate time seen algorithms like yolo yolov yolov single shot detector etc use actual product could used real world one best fast accurate kindly guide topics look zero knowledge willing gain knowledge tesla cars use one would curious might helpful,"['machine', 'learning', 'ai', 'implement', 'object', 'detection', 'real', 'time', 'product', 'companyobject', 'detection', 'fast', 'accurate', 'time', 'seen', 'algorithms', 'like', 'yolo', 'yolov', 'yolov', 'single', 'shot', 'detector', 'etc', 'use', 'actual', 'product', 'could', 'used', 'real', 'world', 'one', 'best', 'fast', 'accurate', 'kindly', 'guide', 'topics', 'look', 'zero', 'knowledge', 'willing', 'gain', 'knowledge', 'tesla', 'cars', 'use', 'one', 'would', 'curious', 'might', 'helpful']","['machin', 'learn', 'ai', 'implement', 'object', 'detect', 'real', 'time', 'product', 'companyobject', 'detect', 'fast', 'accur', 'time', 'seen', 'algorithm', 'like', 'yolo', 'yolov', 'yolov', 'singl', 'shot', 'detector', 'etc', 'use', 'actual', 'product', 'could', 'use', 'real', 'world', 'one', 'best', 'fast', 'accur', 'kindli', 'guid', 'topic', 'look', 'zero', 'knowledg', 'will', 'gain', 'knowledg', 'tesla', 'car', 'use', 'one', 'would', 'curiou', 'might', 'help']"
112,121,121,19602757,73083541,Creating a daily pick lottery prediction program,"<p>Creating a lottery prediction programme: A daily pick game. The aim is to use python codes to predict next number that plays from past daily draws data list, list updates daily from daily draws. One number is chosen by the player from a pool of 36 numbers. The aim of this python programme is to predict-output a set of 12 numbers out of the pool of 36 numbers that is likely to contain the &quot;one&quot; daily &quot;drawn&quot; number. Intend to use or identify some sequence or pattern, variables such as: most popular num drawn, least popular num drawn, most recent, least recent, some machine learning function, etc etc</p>
",27,0,-4,1,python,2022-07-22 19:40:37,2022-07-22 19:40:37,2022-07-22 20:43:06,creating a lottery prediction programme  a daily pick game  the aim is to use python codes to predict next number that plays from past daily draws data list  list updates daily from daily draws  one number is chosen by the player from a pool of  numbers  the aim of this python programme is to predict output a set of  numbers out of the pool of  numbers that is likely to contain the  one  daily  drawn  number  intend to use or identify some sequence or pattern  variables such as  most popular num drawn  least popular num drawn  most recent  least recent  some machine learning function  etc etc,creating a daily pick lottery prediction program,creating lottery prediction programme daily pick game aim use python codes predict next number plays past daily draws data updates daily daily draws one number chosen player pool numbers aim python programme predict output set numbers pool numbers likely contain one daily drawn number intend use identify sequence pattern variables popular num drawn least popular num drawn recent least recent machine learning function etc etc,creating daily pick lottery prediction program,creating daily pick lottery prediction programcreating lottery prediction programme daily pick game aim use python codes predict next number plays past daily draws data updates daily daily draws one number chosen player pool numbers aim python programme predict output set numbers pool numbers likely contain one daily drawn number intend use identify sequence pattern variables popular num drawn least popular num drawn recent least recent machine learning function etc etc,"['creating', 'daily', 'pick', 'lottery', 'prediction', 'programcreating', 'lottery', 'prediction', 'programme', 'daily', 'pick', 'game', 'aim', 'use', 'python', 'codes', 'predict', 'next', 'number', 'plays', 'past', 'daily', 'draws', 'data', 'updates', 'daily', 'daily', 'draws', 'one', 'number', 'chosen', 'player', 'pool', 'numbers', 'aim', 'python', 'programme', 'predict', 'output', 'set', 'numbers', 'pool', 'numbers', 'likely', 'contain', 'one', 'daily', 'drawn', 'number', 'intend', 'use', 'identify', 'sequence', 'pattern', 'variables', 'popular', 'num', 'drawn', 'least', 'popular', 'num', 'drawn', 'recent', 'least', 'recent', 'machine', 'learning', 'function', 'etc', 'etc']","['creat', 'daili', 'pick', 'lotteri', 'predict', 'programcr', 'lotteri', 'predict', 'programm', 'daili', 'pick', 'game', 'aim', 'use', 'python', 'code', 'predict', 'next', 'number', 'play', 'past', 'daili', 'draw', 'data', 'updat', 'daili', 'daili', 'draw', 'one', 'number', 'chosen', 'player', 'pool', 'number', 'aim', 'python', 'programm', 'predict', 'output', 'set', 'number', 'pool', 'number', 'like', 'contain', 'one', 'daili', 'drawn', 'number', 'intend', 'use', 'identifi', 'sequenc', 'pattern', 'variabl', 'popular', 'num', 'drawn', 'least', 'popular', 'num', 'drawn', 'recent', 'least', 'recent', 'machin', 'learn', 'function', 'etc', 'etc']"
113,122,122,19261150,73082025,Creating a dataset for a machine learning project,"<p>I am working to create a video from a transcript where the idea is just to choose some series of images based on the meaning of the text.  I need to create a model that can pick out the images based on the text but am struggling on how to create a meaningful way to choose images and also how to actually format the dataset with the images and text so that it can be used to train a model.  Has anyone done anything similar to this?</p>
",20,0,0,2,machine-learning;dataset,2022-07-22 17:34:17,2022-07-22 17:34:17,2022-07-22 17:34:17,i am working to create a video from a transcript where the idea is just to choose some series of images based on the meaning of the text   i need to create a model that can pick out the images based on the text but am struggling on how to create a meaningful way to choose images and also how to actually format the dataset with the images and text so that it can be used to train a model   has anyone done anything similar to this ,creating a dataset for a machine learning project,working create video transcript idea choose series images based meaning text need create model pick images based text struggling create meaningful way choose images also actually format dataset images text used train model anyone done anything similar,creating dataset machine learning project,creating dataset machine learning projectworking create video transcript idea choose series images based meaning text need create model pick images based text struggling create meaningful way choose images also actually format dataset images text used train model anyone done anything similar,"['creating', 'dataset', 'machine', 'learning', 'projectworking', 'create', 'video', 'transcript', 'idea', 'choose', 'series', 'images', 'based', 'meaning', 'text', 'need', 'create', 'model', 'pick', 'images', 'based', 'text', 'struggling', 'create', 'meaningful', 'way', 'choose', 'images', 'also', 'actually', 'format', 'dataset', 'images', 'text', 'used', 'train', 'model', 'anyone', 'done', 'anything', 'similar']","['creat', 'dataset', 'machin', 'learn', 'projectwork', 'creat', 'video', 'transcript', 'idea', 'choos', 'seri', 'imag', 'base', 'mean', 'text', 'need', 'creat', 'model', 'pick', 'imag', 'base', 'text', 'struggl', 'creat', 'meaning', 'way', 'choos', 'imag', 'also', 'actual', 'format', 'dataset', 'imag', 'text', 'use', 'train', 'model', 'anyon', 'done', 'anyth', 'similar']"
114,123,123,16437586,73081960,How to dump pickle on server while it works fine locally?,"<p>We are deploying machine learning model to a Linux server.
Script wokrs well on local PyCharm, but it fails to save pickle file to directory. The code is:</p>
<pre><code>import pickle
from datetime import date

current_date = date.today()
with open(f&quot;serialized_data/dbh_{str(current_date)}.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(data_balance_histories, f)
</code></pre>
<p>And I get an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;Model_building/main.py&quot;, line 13, in &lt;module&gt;
    balance_histories_part_1_and_2()
  File &quot;/home/mluser/Model_building/Pipeline/Balance_histories_part_1_and_2.py&quot;, line 439, in balance_histories_part_1_and_2
    with open(f&quot;/serialized_data/dbh_{str(current_date)}.pkl&quot;, &quot;wb&quot;) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/serialized_data/dbh_2022-07-22.pkl'
</code></pre>
<p>While the same code runs fine locally.
The folder 'serialized_data' is in place on server, in the very same folder with main.py</p>
<p><a href=""https://i.stack.imgur.com/1tnbl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1tnbl.png"" alt=""server directory"" /></a></p>
<p>I've tried setting directory to</p>
<pre><code>    path='/home/mluser/Model_building'
    os.chdir(path)
</code></pre>
<p>But it doesn't change a thing. Tried to shorted file names, also no result.
What can solve the issue and what can be the difference running the script locally and on server?</p>
",13,0,0,3,python;deployment;server,2022-07-22 17:28:24,2022-07-22 17:28:24,2022-07-22 17:28:24,and i get an error   i ve tried setting directory to,how to dump pickle on server while it works fine locally ,get error tried setting directory,dump pickle server works fine locally,dump pickle server works fine locallyget error tried setting directory,"['dump', 'pickle', 'server', 'works', 'fine', 'locallyget', 'error', 'tried', 'setting', 'directory']","['dump', 'pickl', 'server', 'work', 'fine', 'locallyget', 'error', 'tri', 'set', 'directori']"
115,125,125,19594835,73081228,Error while installing stable-baselines in Kaggle,"<p>i'm trying to run some machine learning environment on Kaggle, but i can't install the packages... I Need to install stable-baselines3[extra] but i cant. I tried with</p>
<pre><code>pip install stable-baselines3[extra]
</code></pre>
<p>but i get this error:</p>
<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) 
after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.HTTPSConnection
object at 0x7f8159233e50&gt;: Failed to establish a new connection: [Errno -3] Temporary failure 
in name resolution')': /simple/stable-baselines3/
</code></pre>
<p>How to resolve this?</p>
",21,0,2,3,python;machine-learning;openai,2022-07-22 16:33:12,2022-07-22 16:33:12,2022-07-22 16:35:18,i m trying to run some machine learning environment on kaggle  but i can t install the packages    i need to install stable baselines extra  but i cant  i tried with but i get this error  how to resolve this ,error while installing stable baselines in kaggle,trying run machine learning environment kaggle install packages need install stable baselines extra cant tried get error resolve,error installing stable baselines kaggle,error installing stable baselines kaggletrying run machine learning environment kaggle install packages need install stable baselines extra cant tried get error resolve,"['error', 'installing', 'stable', 'baselines', 'kaggletrying', 'run', 'machine', 'learning', 'environment', 'kaggle', 'install', 'packages', 'need', 'install', 'stable', 'baselines', 'extra', 'cant', 'tried', 'get', 'error', 'resolve']","['error', 'instal', 'stabl', 'baselin', 'kaggletri', 'run', 'machin', 'learn', 'environ', 'kaggl', 'instal', 'packag', 'need', 'instal', 'stabl', 'baselin', 'extra', 'cant', 'tri', 'get', 'error', 'resolv']"
116,127,127,19599487,73076537,Why are my DALL-E Images Coming Out So Desaturated After Resizing?,"<p>I'm very new to ML image manipulation/creation, so if I confuse you all with my own lack of knowledge on the subject, I apologize in advance.</p>
<p>I'm attempting to increase the resolution of images produced by DALL-E, inspired by this article:
<a href=""https://towardsdatascience.com/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788"" rel=""nofollow noreferrer"">https://towardsdatascience.com/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788</a></p>
<p>However, when I attempt to feed images from DALL-E into the &quot;Generate 1K Image&quot; section of the original author's code/colab, my original image becomes very washed out; likely because in the source, some sort of tensor from a different model is fed in, while in my own version, I'm converting an image to a (poorly made?) tensor in the same section and then feeding that in.</p>
<p>Here's what I have:</p>
<pre><code>#@title Generate 1K Image
from google.colab import files
from io import BytesIO
from PIL import Image
from matplotlib import pyplot as plt
import numpy as np

from torchvision import transforms as T
import IPython
import os.path
import cv2
uploaded = files.upload()
texture_amount = 0.05 #@param {type:&quot;slider&quot;, min:0, max:0.15, step:0.001}
texture_size = 3 #@param {type:&quot;slider&quot;, min:1, max:9, step:2}
enhance_details = True #@param {type:&quot;boolean&quot;}
img = Image.open(BytesIO(uploaded['knight.png']))
plt.imshow(img)
plt.show()
transform = transforms.Compose([
    transforms.ToTensor()
])
tensorImage = transform(img)
selected_img = tensorImage.cuda()
selected_img = selected_img.type(torch.cuda.FloatTensor)
selected_img = selected_img.add(1).div(2)[None, :]

with torch.no_grad():
  torch.cuda.empty_cache()
  resized = bsrgan_model(selected_img)
  torch.cuda.empty_cache()

noise = torch.normal(0, texture_amount, 
  size=[resized.shape[0], 1, resized.shape[2], resized.shape[3]]).to(device)
noise = noise.repeat(1, 3, 1, 1)

noise_blurred = T.GaussianBlur(kernel_size=texture_size, sigma=1)(noise)
noise_blurred = noise*0.25 + noise_blurred*0.75

resized = (resized+noise_blurred)
final_image = resized.to(device)

if enhance_details:
  with torch.no_grad():
    torch.cuda.empty_cache()
    z, *_ = vqgan_model.encode(final_image * 2 - 1)
    final_image = vqgan_model.decode(z)[0].add(1).div(2).clamp(min=0, max=1)
    torch.cuda.empty_cache()
    final_image = final_image.clamp(min=0, max=1)
else:
    final_image = final_image[0].clamp(min=0, max=1)

img = T.ToPILImage()(final_image)
img.save(&quot;output_1k.png&quot;)
IPython.display.Image(&quot;output_1k.png&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/BeSyH.png"" rel=""nofollow noreferrer"">Original Image</a>
<a href=""https://i.stack.imgur.com/InMfI.png"" rel=""nofollow noreferrer"">Resulting Image</a></p>
<p>Any ideas as to  how I can fix this issue is greatly, greatly appreciated!</p>
",35,1,0,5,python;pytorch;artificial-intelligence;google-colaboratory;tensor,2022-07-22 10:20:37,2022-07-22 10:20:37,2022-07-22 15:12:57,i m very new to ml image manipulation creation  so if i confuse you all with my own lack of knowledge on the subject  i apologize in advance  however  when i attempt to feed images from dall e into the  generate k image  section of the original author s code colab  my original image becomes very washed out  likely because in the source  some sort of tensor from a different model is fed in  while in my own version  i m converting an image to a  poorly made   tensor in the same section and then feeding that in  here s what i have  any ideas as to  how i can fix this issue is greatly  greatly appreciated ,why are my dall e images coming out so desaturated after resizing ,ml image manipulation creation confuse lack knowledge subject apologize advance however attempt feed images dall e generate k image section original author code colab original image becomes washed likely source sort tensor different model fed version converting image poorly made tensor section feeding ideas fix issue greatly greatly appreciated,dall e images coming desaturated resizing,dall e images coming desaturated resizingml image manipulation creation confuse lack knowledge subject apologize advance however attempt feed images dall e generate k image section original author code colab original image becomes washed likely source sort tensor different model fed version converting image poorly made tensor section feeding ideas fix issue greatly greatly appreciated,"['dall', 'e', 'images', 'coming', 'desaturated', 'resizingml', 'image', 'manipulation', 'creation', 'confuse', 'lack', 'knowledge', 'subject', 'apologize', 'advance', 'however', 'attempt', 'feed', 'images', 'dall', 'e', 'generate', 'k', 'image', 'section', 'original', 'author', 'code', 'colab', 'original', 'image', 'becomes', 'washed', 'likely', 'source', 'sort', 'tensor', 'different', 'model', 'fed', 'version', 'converting', 'image', 'poorly', 'made', 'tensor', 'section', 'feeding', 'ideas', 'fix', 'issue', 'greatly', 'greatly', 'appreciated']","['dall', 'e', 'imag', 'come', 'desatur', 'resizingml', 'imag', 'manipul', 'creation', 'confus', 'lack', 'knowledg', 'subject', 'apolog', 'advanc', 'howev', 'attempt', 'feed', 'imag', 'dall', 'e', 'gener', 'k', 'imag', 'section', 'origin', 'author', 'code', 'colab', 'origin', 'imag', 'becom', 'wash', 'like', 'sourc', 'sort', 'tensor', 'differ', 'model', 'fed', 'version', 'convert', 'imag', 'poorli', 'made', 'tensor', 'section', 'feed', 'idea', 'fix', 'issu', 'greatli', 'greatli', 'appreci']"
117,128,128,18728772,73027622,Need suggestions for a NLP use case,"<p>I am trying to build a web scraper that can predict the content of a given URL into multiple categories, but I am currently confused about which method is best suited for my use case. Here's the overall use case:</p>
<p>I want to predict a researcher's interest from their biography and categorize them into one or multiple categories based on <a href=""https://sdgs.un.org/goals"" rel=""nofollow noreferrer"">SDG 17 goals</a>. I have three data points to work with:</p>
<ol>
<li>The biography of each researcher (can be scrapped and tokenized)</li>
<li>A list of keywords that are often associated with each of the SDG categories/goals (<a href=""https://docs.google.com/spreadsheets/d/1Y3k06f30j-mXUx1Qo-jelOJCWRiGvJOwRDA0Mwd7rIg/edit#gid=0"" rel=""nofollow noreferrer"">here's the example of said keywords</a>)</li>
<li>Hundreds of categorizations that are done manually by students in the form of binary data (<a href=""https://docs.google.com/spreadsheets/d/1Y3k06f30j-mXUx1Qo-jelOJCWRiGvJOwRDA0Mwd7rIg/edit#gid=1745344524"" rel=""nofollow noreferrer"">here's the example of said data</a>)</li>
</ol>
<p>So far, we have students that read each researcher's biography and decide which SDG category/goal each researcher belongs to. One research can belong to one or more SDG categories. We usually categorize it based on how often SDG keywords listed in our database are present in each researcher's bio.</p>
<p>I have looked up online machine learning models for NLP but couldn't decide on which method would work best with my use case. Any suggestions and references would be super appreciated because I'm a bit lost here.</p>
",40,1,0,5,python;algorithm;machine-learning;nlp;nltk,2022-07-18 22:26:13,2022-07-18 22:26:13,2022-07-22 14:39:49,i am trying to build a web scraper that can predict the content of a given url into multiple categories  but i am currently confused about which method is best suited for my use case  here s the overall use case  i want to predict a researcher s interest from their biography and categorize them into one or multiple categories based on   i have three data points to work with  so far  we have students that read each researcher s biography and decide which sdg category goal each researcher belongs to  one research can belong to one or more sdg categories  we usually categorize it based on how often sdg keywords listed in our database are present in each researcher s bio  i have looked up online machine learning models for nlp but couldn t decide on which method would work best with my use case  any suggestions and references would be super appreciated because i m a bit lost here ,need suggestions for a nlp use case,trying build web scraper predict content given url multiple categories currently confused method best suited use case overall use case want predict researcher interest biography categorize one multiple categories based three data points work far students read researcher biography decide sdg category goal researcher belongs one research belong one sdg categories usually categorize based often sdg keywords listed database present researcher bio looked online machine learning models nlp decide method would work best use case suggestions references would super appreciated bit lost,need suggestions nlp use case,need suggestions nlp use casetrying build web scraper predict content given url multiple categories currently confused method best suited use case overall use case want predict researcher interest biography categorize one multiple categories based three data points work far students read researcher biography decide sdg category goal researcher belongs one research belong one sdg categories usually categorize based often sdg keywords listed database present researcher bio looked online machine learning models nlp decide method would work best use case suggestions references would super appreciated bit lost,"['need', 'suggestions', 'nlp', 'use', 'casetrying', 'build', 'web', 'scraper', 'predict', 'content', 'given', 'url', 'multiple', 'categories', 'currently', 'confused', 'method', 'best', 'suited', 'use', 'case', 'overall', 'use', 'case', 'want', 'predict', 'researcher', 'interest', 'biography', 'categorize', 'one', 'multiple', 'categories', 'based', 'three', 'data', 'points', 'work', 'far', 'students', 'read', 'researcher', 'biography', 'decide', 'sdg', 'category', 'goal', 'researcher', 'belongs', 'one', 'research', 'belong', 'one', 'sdg', 'categories', 'usually', 'categorize', 'based', 'often', 'sdg', 'keywords', 'listed', 'database', 'present', 'researcher', 'bio', 'looked', 'online', 'machine', 'learning', 'models', 'nlp', 'decide', 'method', 'would', 'work', 'best', 'use', 'case', 'suggestions', 'references', 'would', 'super', 'appreciated', 'bit', 'lost']","['need', 'suggest', 'nlp', 'use', 'casetri', 'build', 'web', 'scraper', 'predict', 'content', 'given', 'url', 'multipl', 'categori', 'current', 'confus', 'method', 'best', 'suit', 'use', 'case', 'overal', 'use', 'case', 'want', 'predict', 'research', 'interest', 'biographi', 'categor', 'one', 'multipl', 'categori', 'base', 'three', 'data', 'point', 'work', 'far', 'student', 'read', 'research', 'biographi', 'decid', 'sdg', 'categori', 'goal', 'research', 'belong', 'one', 'research', 'belong', 'one', 'sdg', 'categori', 'usual', 'categor', 'base', 'often', 'sdg', 'keyword', 'list', 'databas', 'present', 'research', 'bio', 'look', 'onlin', 'machin', 'learn', 'model', 'nlp', 'decid', 'method', 'would', 'work', 'best', 'use', 'case', 'suggest', 'refer', 'would', 'super', 'appreci', 'bit', 'lost']"
118,129,129,19427177,73053975,I receive the following error and I can&#39;t solve it: TypeError: &#39;Sequential&#39; object is not subscriptable,"<p>The following is the code for a streamlit based application I am working on. This is the main.py file which is to design the streamlit app. This application is has a machine-learning based backend system. The model itself performs quite well and has no issues, but the file to deploy has an issue. I saved the model as a .pkl file to use it in the application. The webpage is functional but when I try to input an image and run it, i receive an error. The following is the error:</p>
<pre><code>    prediction = predict(Corrosion_Detector['w'], Corrosion_Detector['b'], my_image)
TypeError: 'Sequential' object is not subscriptable
</code></pre>
<p>The streamlit file is as follows:</p>
<pre><code>import streamlit as st
import joblib 
from PIL import Image 
from skimage.transform import resize 
import numpy as np
import time

#The following loads the model developed in model.py as a pickle file
Corrosion_Detector = joblib.load('C:/Users/ayush.kumar/Desktop/projects/CorrosionDet_app/Corrosion_Detector.pkl')

#The following determines the predictability of the code 
def sigmoid(z):
    s = 1/(1+np.exp(-z))
    return s

def predict(w, b, X):
    m = X.shape[1]
    Y_prediction = np.zeros(1,m)
    w = w.reshape(X.shape[0],1)
    Y_prediction = sigmoid((np.dot(w.T, X) + b)) #This computes the probability of a rusted material being present
    return Y_prediction

#The following designs the interface
st.title('Corrosion Detector')
st.write('\n')
image = Image.open(r&quot;C:\Users\ayush.kumar\Pictures\Saved Pictures\1525994797510.jpg&quot;)
show = st.image(image, use_column_width = True)
st.sidebar.title('Upload image')

#st.set_option('depreciation.showfileUploaderEncoding', False)

#The following is for the user to input the subject's picture in .png, .jpg, and .jpeg
uploaded_file = st.sidebar.file_uploader(&quot;&quot;, type = ['png', 'jpg', 'jpeg'])

def img_func(u_img):
    image = np.asarray(u_img)/255
    my_image = resize(image, (64,64)).T
    return my_image

if uploaded_file is not None:
    u_img = Image.open(uploaded_file)
    show.image(u_img, 'Uploading Image', use_column_width = True)
    my_image = img_func(u_img)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
st.sidebar.write('\n')#adds new line

if st.sidebar.button('Click here to initialize the classification'):
    if uploaded_file is None:
         st.sidebar.write('Please upload an image to initialize the classification first!')
    else:
        with st.spinner('Running the CNN algorithm...'):
            prediction = predict(Corrosion_Detector['w'], Corrosion_Detector['b'], my_image)
            time.sleep(2)
            st.success('Done!')
        st.sidebar.header('The model concludes: ')
        probability = '{:.3f}'.format(float(prediction*100))
        #The following classifies the input file if prediction &gt; 0.5
        if prediction &gt; 0.5:
            st.sidebar.write('The image shows rust or traces of potential rust!','\n')
            st.sidebar.write('**Probability: **',probability,'%')
        else:
            st.sidebar.write('This image shows no traces of rust.','\n')
            st.sidebar.write('**Probability: **',probability,'%')
</code></pre>
<p>Python: 3.6.13
Streamlit: 1.10.0</p>
",71,0,0,4,python;machine-learning;computer-vision;streamlit,2022-07-20 18:00:27,2022-07-20 18:00:27,2022-07-22 11:00:27,the following is the code for a streamlit based application i am working on  this is the main py file which is to design the streamlit app  this application is has a machine learning based backend system  the model itself performs quite well and has no issues  but the file to deploy has an issue  i saved the model as a  pkl file to use it in the application  the webpage is functional but when i try to input an image and run it  i receive an error  the following is the error  the streamlit file is as follows ,i receive the following error and i can   t solve it  typeerror     sequential    object is not subscriptable,following code streamlit based application working main py file design streamlit app application machine learning based backend system model performs quite well issues file deploy issue saved model pkl file use application webpage functional try input image run receive error following error streamlit file follows,receive following error solve typeerror sequential object subscriptable,receive following error solve typeerror sequential object subscriptablefollowing code streamlit based application working main py file design streamlit app application machine learning based backend system model performs quite well issues file deploy issue saved model pkl file use application webpage functional try input image run receive error following error streamlit file follows,"['receive', 'following', 'error', 'solve', 'typeerror', 'sequential', 'object', 'subscriptablefollowing', 'code', 'streamlit', 'based', 'application', 'working', 'main', 'py', 'file', 'design', 'streamlit', 'app', 'application', 'machine', 'learning', 'based', 'backend', 'system', 'model', 'performs', 'quite', 'well', 'issues', 'file', 'deploy', 'issue', 'saved', 'model', 'pkl', 'file', 'use', 'application', 'webpage', 'functional', 'try', 'input', 'image', 'run', 'receive', 'error', 'following', 'error', 'streamlit', 'file', 'follows']","['receiv', 'follow', 'error', 'solv', 'typeerror', 'sequenti', 'object', 'subscriptablefollow', 'code', 'streamlit', 'base', 'applic', 'work', 'main', 'py', 'file', 'design', 'streamlit', 'app', 'applic', 'machin', 'learn', 'base', 'backend', 'system', 'model', 'perform', 'quit', 'well', 'issu', 'file', 'deploy', 'issu', 'save', 'model', 'pkl', 'file', 'use', 'applic', 'webpag', 'function', 'tri', 'input', 'imag', 'run', 'receiv', 'error', 'follow', 'error', 'streamlit', 'file', 'follow']"
119,130,130,13448681,73076094,Usage of a classification dataset for regression models in machine learning. Which Metric to use?,"<p>I have a data set with multiple columns as variables. The target variable of my data set is 0/1. Normally, one would speak of a classification problem here. However, we would like to find a way to make it a best regression problem. The models to train and compare are up and running so far. Unfortunately, I am not familiar with the different metrics or cannot clearly determine which of the metrics &quot;MSA&quot;, &quot;RMSE&quot;, &quot;MSE&quot; ,.. could be the best for our project.</p>
<p>Does anyone have experience with this problem? When googling, unfortunately I can't find something like &quot;transforming a classification problem into a regression problem&quot;. Is there anything else we need to consider to get a best possible target variable [0,1]?</p>
<p>I am very grateful for any suggestion / link to Kaggle or similar with an example!</p>
",20,0,-1,4,machine-learning;regression;classification;metrics,2022-07-22 09:41:34,2022-07-22 09:41:34,2022-07-22 09:41:34,i have a data set with multiple columns as variables  the target variable of my data set is    normally  one would speak of a classification problem here  however  we would like to find a way to make it a best regression problem  the models to train and compare are up and running so far  unfortunately  i am not familiar with the different metrics or cannot clearly determine which of the metrics  msa    rmse    mse      could be the best for our project  does anyone have experience with this problem  when googling  unfortunately i can t find something like  transforming a classification problem into a regression problem   is there anything else we need to consider to get a best possible target variable      i am very grateful for any suggestion   link to kaggle or similar with an example ,usage of a classification dataset for regression models in machine learning  which metric to use ,data set multiple columns variables target variable data set normally one would speak classification problem however would like find way make best regression problem models train compare running far unfortunately familiar different metrics cannot clearly determine metrics msa rmse mse could best project anyone experience problem googling unfortunately find something like transforming classification problem regression problem anything else need consider get best possible target variable grateful suggestion link kaggle similar example,usage classification dataset regression models machine learning metric use,usage classification dataset regression models machine learning metric usedata set multiple columns variables target variable data set normally one would speak classification problem however would like find way make best regression problem models train compare running far unfortunately familiar different metrics cannot clearly determine metrics msa rmse mse could best project anyone experience problem googling unfortunately find something like transforming classification problem regression problem anything else need consider get best possible target variable grateful suggestion link kaggle similar example,"['usage', 'classification', 'dataset', 'regression', 'models', 'machine', 'learning', 'metric', 'usedata', 'set', 'multiple', 'columns', 'variables', 'target', 'variable', 'data', 'set', 'normally', 'one', 'would', 'speak', 'classification', 'problem', 'however', 'would', 'like', 'find', 'way', 'make', 'best', 'regression', 'problem', 'models', 'train', 'compare', 'running', 'far', 'unfortunately', 'familiar', 'different', 'metrics', 'can', 'not', 'clearly', 'determine', 'metrics', 'msa', 'rmse', 'mse', 'could', 'best', 'project', 'anyone', 'experience', 'problem', 'googling', 'unfortunately', 'find', 'something', 'like', 'transforming', 'classification', 'problem', 'regression', 'problem', 'anything', 'else', 'need', 'consider', 'get', 'best', 'possible', 'target', 'variable', 'grateful', 'suggestion', 'link', 'kaggle', 'similar', 'example']","['usag', 'classif', 'dataset', 'regress', 'model', 'machin', 'learn', 'metric', 'usedata', 'set', 'multipl', 'column', 'variabl', 'target', 'variabl', 'data', 'set', 'normal', 'one', 'would', 'speak', 'classif', 'problem', 'howev', 'would', 'like', 'find', 'way', 'make', 'best', 'regress', 'problem', 'model', 'train', 'compar', 'run', 'far', 'unfortun', 'familiar', 'differ', 'metric', 'can', 'not', 'clearli', 'determin', 'metric', 'msa', 'rmse', 'mse', 'could', 'best', 'project', 'anyon', 'experi', 'problem', 'googl', 'unfortun', 'find', 'someth', 'like', 'transform', 'classif', 'problem', 'regress', 'problem', 'anyth', 'els', 'need', 'consid', 'get', 'best', 'possibl', 'target', 'variabl', 'grate', 'suggest', 'link', 'kaggl', 'similar', 'exampl']"
120,131,131,16760993,73066522,Convert 2 Azure pipelines into one pipeline,"<p>i have 2 azure pipelines for Devops/MLops</p>
<ol>
<li>first pipeline to train and register model</li>
<li>second pipeline to deploy the model to azure Container Instance</li>
</ol>
<p>i want to convert the second pipeline into a stage in the first pipeline but i have some variables that depends on the first pipeline</p>
<p>here is m first pipeline :</p>
<pre><code>    resources:
  containers:
  - container: mlops
    image: mcr.microsoft.com/mlops/python:latest

pr: none
trigger:
  branches:
    include:
    - master
  paths:
    include:
    - diabetes_regression/
    - ml_service/pipelines/diabetes_regression_build_train_pipeline.py
    - ml_service/pipelines/diabetes_regression_build_train_pipeline_with_r.py
    - ml_service/pipelines/diabetes_regression_build_train_pipeline_with_r_on_dbricks.py

variables:
- template: diabetes_regression-variables-template.yml
- group: devopsforai-aml-vg

pool:
  vmImage: ubuntu-latest

stages:
- stage: 'Model_CI'
  displayName: 'Model CI'
  jobs:
  - job: &quot;Model_CI_Pipeline&quot;
    displayName: &quot;Model CI Pipeline&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - template: code-quality-template.yml
    - task: AzureCLI@1
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          # Invoke the Python building and publishing a training pipeline
          python -m ml_service.pipelines.diabetes_regression_build_train_pipeline
      displayName: 'Publish Azure Machine Learning Pipeline'

- stage: 'Trigger_AML_Pipeline'
  displayName: 'Train and evaluate model'
  condition: succeeded()
  variables:
    BUILD_URI: '$(SYSTEM.COLLECTIONURI)$(SYSTEM.TEAMPROJECT)/_build/results?buildId=$(BUILD.BUILDID)'
  jobs:
  - job: &quot;Get_Pipeline_ID&quot;
    condition: and(succeeded(), eq(coalesce(variables['auto-trigger-training'], 'true'), 'true'))
    displayName: &quot;Get Pipeline ID for execution&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - task: AzureCLI@1
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          python -m ml_service.pipelines.run_train_pipeline --output_pipeline_id_file &quot;pipeline_id.txt&quot; --skip_train_execution
          # Set AMLPIPELINEID variable for next AML Pipeline task in next job
          AMLPIPELINEID=&quot;$(cat pipeline_id.txt)&quot;
          echo &quot;##vso[task.setvariable variable=AMLPIPELINEID;isOutput=true]$AMLPIPELINEID&quot;
      name: 'getpipelineid'
      displayName: 'Get Pipeline ID'
  - job: &quot;Run_ML_Pipeline&quot;
    dependsOn: &quot;Get_Pipeline_ID&quot;
    displayName: &quot;Trigger ML Training Pipeline&quot;
    timeoutInMinutes: 0
    pool: server
    variables:
      AMLPIPELINE_ID: $[ dependencies.Get_Pipeline_ID.outputs['getpipelineid.AMLPIPELINEID'] ]
    steps:
    - task: ms-air-aiagility.vss-services-azureml.azureml-restApi-task.MLPublishedPipelineRestAPITask@0
      displayName: 'Invoke ML pipeline'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        PipelineId: '$(AMLPIPELINE_ID)'
        ExperimentName: '$(EXPERIMENT_NAME)'
        PipelineParameters: '&quot;ParameterAssignments&quot;: {&quot;model_name&quot;: &quot;$(MODEL_NAME)&quot;}, &quot;tags&quot;: {&quot;BuildId&quot;: &quot;$(Build.BuildId)&quot;, &quot;BuildUri&quot;: &quot;$(BUILD_URI)&quot;}, &quot;StepTags&quot;: {&quot;BuildId&quot;: &quot;$(Build.BuildId)&quot;, &quot;BuildUri&quot;: &quot;$(BUILD_URI)&quot;}'
  - job: &quot;Training_Run_Report&quot;
    dependsOn: &quot;Run_ML_Pipeline&quot;
    condition: always()
    displayName: &quot;Publish artifact if new model was registered&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - template: diabetes_regression-publish-model-artifact-template.yml
</code></pre>
<p>my second pipeline (Deploy to Aci):</p>
<pre><code># Continuous Integration (CI) pipeline that orchestrates the deployment of the diabetes_regression model.

# Runtime parameters to select artifacts
parameters:
- name : artifactBuildId
  displayName: Model Train CI Build ID. Default is 'latest'.
  type: string
  default: latest

pr: none

# Trigger this pipeline on model-train pipeline completion
trigger: none
resources:
  containers:
  - container: mlops
    image: mcr.microsoft.com/mlops/python:latest
  pipelines:
  - pipeline: model-train-ci
    source: Model-Train-Register-CI # Name of the triggering pipeline
    trigger:
      branches:
        include:
        - master

variables:
- template: diabetes_regression-variables-template.yml
- group: devopsforai-aml-vg

stages:
- stage: 'Deploy_ACI'
  displayName: 'Deploy to ACI'
  condition: variables['ACI_DEPLOYMENT_NAME']
  jobs:
  - job: &quot;Deploy_ACI&quot;
    displayName: &quot;Deploy to ACI&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - download: none
    - template: diabetes_regression-get-model-id-artifact-template.yml
      parameters:
        projectId: '$(resources.pipeline.model-train-ci.projectID)'
        pipelineId: '$(resources.pipeline.model-train-ci.pipelineID)'
        artifactBuildId: ${{ parameters.artifactBuildId }}
    - task: AzureCLI@1
      displayName: 'Install AzureML CLI'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: 'az extension add --source https://azurecliext.blob.core.windows.net/release/azure_cli_ml-1.27.0-py3-none-any.whl --yes'
    - task: AzureCLI@1
      displayName: &quot;Deploy to ACI (CLI)&quot;
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)/$(SOURCES_DIR_TRAIN)/scoring
        inlineScript: |
          set -e # fail on error
          
          az ml model deploy --name $(ACI_DEPLOYMENT_NAME) --model '$(MODEL_NAME):$(get_model.MODEL_VERSION)' \
          --ic inference_config.yml \
          --dc deployment_config_aci.yml \
          -g $(RESOURCE_GROUP) --workspace-name $(WORKSPACE_NAME) \
          --overwrite -v
    - task: AzureCLI@1
      displayName: 'Smoke test'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          python -m ml_service.util.smoke_test_scoring_service --type ACI --service &quot;$(ACI_DEPLOYMENT_NAME)&quot;
</code></pre>
<p>the pipeline that i don't know how to get are :</p>
<pre><code>  parameters:
    projectId: '$(resources.pipeline.model-train-ci.projectID)'
    pipelineId: '$(resources.pipeline.model-train-ci.pipelineID)'
</code></pre>
<p>because they get the project and pipeline id from model-train-ci pipeline</p>
<p>i looked for how to get the ids from current running pipeline and didn't get anything</p>
<p>i tried to use the variable $AMLPIPELINEID from the job Get_pipline_ID but it's empty</p>
<p>here is what i tried and what the current file looks like  :</p>
<pre><code>    # Continuous Integration (CI) pipeline that orchestrates the training, evaluation, and registration of the diabetes_regression model.
parameters:
- name : artifactBuildId
  displayName: Model Train CI Build ID. Default is 'latest'.
  type: string
  default: latest

resources:
  containers:
  - container: mlops
    image: mcr.microsoft.com/mlops/python:latest

pr: none
trigger:
  branches:
    include:
    - master
  paths:
    include:
    - diabetes_regression/
    - ml_service/pipelines/diabetes_regression_build_train_pipeline.py
    - ml_service/pipelines/diabetes_regression_build_train_pipeline_with_r.py
    - ml_service/pipelines/diabetes_regression_build_train_pipeline_with_r_on_dbricks.py

variables:
- template: diabetes_regression-variables-template.yml
- group: devopsforai-aml-vg

pool:
  vmImage: ubuntu-latest

stages:
- stage: 'Model_CI'
  displayName: 'Model CI'
  jobs:
  - job: &quot;Model_CI_Pipeline&quot;
    displayName: &quot;Model CI Pipeline&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - template: code-quality-template.yml
    - task: AzureCLI@1
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          # Invoke the Python building and publishing a training pipeline
          python -m ml_service.pipelines.diabetes_regression_build_train_pipeline
      displayName: 'Publish Azure Machine Learning Pipeline'

- stage: 'Trigger_AML_Pipeline'
  displayName: 'Train and evaluate model'
  condition: succeeded()
  variables:
    BUILD_URI: '$(SYSTEM.COLLECTIONURI)$(SYSTEM.TEAMPROJECT)/_build/results?buildId=$(BUILD.BUILDID)'
  jobs:
  - job: &quot;Get_Pipeline_ID&quot;
    condition: and(succeeded(), eq(coalesce(variables['auto-trigger-training'], 'true'), 'true'))
    displayName: &quot;Get Pipeline ID for execution&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - task: AzureCLI@1
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          python -m ml_service.pipelines.run_train_pipeline --output_pipeline_id_file &quot;pipeline_id.txt&quot; --skip_train_execution
          # Set AMLPIPELINEID variable for next AML Pipeline task in next job
          AMLPIPELINEID=&quot;$(cat pipeline_id.txt)&quot;
          echo &quot;##vso[task.setvariable variable=AMLPIPELINEID;isOutput=true]$AMLPIPELINEID&quot;
      name: 'getpipelineid'
      displayName: 'Get Pipeline ID'
  - job: &quot;Run_ML_Pipeline&quot;
    dependsOn: &quot;Get_Pipeline_ID&quot;
    displayName: &quot;Trigger ML Training Pipeline&quot;
    timeoutInMinutes: 0
    pool: server
    variables:
      AMLPIPELINE_ID: $[ dependencies.Get_Pipeline_ID.outputs['getpipelineid.AMLPIPELINEID'] ]
    steps:
    - task: ms-air-aiagility.vss-services-azureml.azureml-restApi-task.MLPublishedPipelineRestAPITask@0
      displayName: 'Invoke ML pipeline'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        PipelineId: '$(AMLPIPELINE_ID)'
        ExperimentName: '$(EXPERIMENT_NAME)'
        PipelineParameters: '&quot;ParameterAssignments&quot;: {&quot;model_name&quot;: &quot;$(MODEL_NAME)&quot;}, &quot;tags&quot;: {&quot;BuildId&quot;: &quot;$(Build.BuildId)&quot;, &quot;BuildUri&quot;: &quot;$(BUILD_URI)&quot;}, &quot;StepTags&quot;: {&quot;BuildId&quot;: &quot;$(Build.BuildId)&quot;, &quot;BuildUri&quot;: &quot;$(BUILD_URI)&quot;}'
  - job: &quot;Training_Run_Report&quot;
    dependsOn: &quot;Run_ML_Pipeline&quot;
    condition: always()
    displayName: &quot;Publish artifact if new model was registered&quot;
    container: mlops
    timeoutInMinutes: 0
    steps:
    - template: diabetes_regression-publish-model-artifact-template.yml
- stage: 'Deploy_ACI'
  displayName: 'Deploy to ACI'
  condition: variables['ACI_DEPLOYMENT_NAME']
  jobs:
  - job: &quot;Deploy_ACI&quot;
    displayName: &quot;Deploy to ACI&quot;
    container: mlops
    timeoutInMinutes: 0
    variables:
      AMLPIPELINE_ID: $[ dependencies.Get_Pipeline_ID.outputs['getpipelineid.AMLPIPELINEID'] ]
    steps:
    - download: none
    - template: diabetes_regression-get-model-id-artifact-template.yml
      parameters:
        projectId: '$(AMLPIPELINE_ID)'
        pipelineId: '$(AMLPIPELINE_ID)'
        artifactBuildId: ${{ parameters.artifactBuildId }}
    - task: AzureCLI@1
      displayName: 'Install AzureML CLI'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)
        inlineScript: 'az extension add --source https://azurecliext.blob.core.windows.net/release/azure_cli_ml-1.27.0-py3-none-any.whl --yes'
    - task: AzureCLI@1
      displayName: &quot;Deploy to ACI (CLI)&quot;
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        workingDirectory: $(Build.SourcesDirectory)/$(SOURCES_DIR_TRAIN)/scoring
        inlineScript: |
          set -e # fail on error
          
          az ml model deploy --name $(ACI_DEPLOYMENT_NAME) --model '$(MODEL_NAME):$(get_model.MODEL_VERSION)' \
          --ic inference_config.yml \
          --dc deployment_config_aci.yml \
          -g $(RESOURCE_GROUP) --workspace-name $(WORKSPACE_NAME) \
          --overwrite -v
    - task: AzureCLI@1
      displayName: 'Smoke test'
      inputs:
        azureSubscription: '$(WORKSPACE_SVC_CONNECTION)'
        scriptLocation: inlineScript
        inlineScript: |
          set -e # fail on error
          export SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          python -m ml_service.util.smoke_test_scoring_service --type ACI --service &quot;$(ACI_DEPLOYMENT_NAME)&quot;
</code></pre>
<p>in the 2nd pipeline resources there was  :</p>
<pre><code>resources:
  pipelines:
  - pipeline: model-train-ci
    source: Model-Train-Register-CI 
</code></pre>
<p>which i can't define now because i have 1 pipeline and it can't reference it self</p>
<p>instead of :</p>
<pre><code>'$(resources.pipeline.model-train-ci.pipelineID)'
</code></pre>
<p>i need to change model-train-ci (which is the 1st pipeline name) to the current merged pipeline</p>
<p>should look somehitng like this but i didn't find anything on the internet :</p>
<pre><code> parameters:
projectId: '$(&lt;current-projectID&gt;)'
pipelineId: '$(&lt;current-pipelineID)'
</code></pre>
<p>im really stuck here and new to this if someone could guide me to the right direction</p>
",54,1,0,4,azure-devops;azure-pipelines;devops;azure-pipelines-yaml,2022-07-21 15:49:03,2022-07-21 15:49:03,2022-07-22 09:30:23,i have  azure pipelines for devops mlops i want to convert the second pipeline into a stage in the first pipeline but i have some variables that depends on the first pipeline here is m first pipeline   my second pipeline  deploy to aci   the pipeline that i don t know how to get are   because they get the project and pipeline id from model train ci pipeline i looked for how to get the ids from current running pipeline and didn t get anything i tried to use the variable  amlpipelineid from the job get_pipline_id but it s empty here is what i tried and what the current file looks like    in the nd pipeline resources there was    which i can t define now because i have  pipeline and it can t reference it self instead of   i need to change model train ci  which is the st pipeline name  to the current merged pipeline should look somehitng like this but i didn t find anything on the internet   im really stuck here and new to this if someone could guide me to the right direction,convert  azure pipelines into one pipeline,azure pipelines devops mlops want convert second pipeline stage first pipeline variables depends first pipeline first pipeline second pipeline deploy aci pipeline know get get project pipeline id model train ci pipeline looked get ids current running pipeline get anything tried use variable amlpipelineid job get_pipline_id empty tried current file looks like nd pipeline resources define pipeline reference self instead need change model train ci st pipeline name current merged pipeline look somehitng like find anything internet im really stuck someone could guide right direction,convert azure pipelines one pipeline,convert azure pipelines one pipelineazure pipelines devops mlops want convert second pipeline stage first pipeline variables depends first pipeline first pipeline second pipeline deploy aci pipeline know get get project pipeline id model train ci pipeline looked get ids current running pipeline get anything tried use variable amlpipelineid job get_pipline_id empty tried current file looks like nd pipeline resources define pipeline reference self instead need change model train ci st pipeline name current merged pipeline look somehitng like find anything internet im really stuck someone could guide right direction,"['convert', 'azure', 'pipelines', 'one', 'pipelineazure', 'pipelines', 'devops', 'mlops', 'want', 'convert', 'second', 'pipeline', 'stage', 'first', 'pipeline', 'variables', 'depends', 'first', 'pipeline', 'first', 'pipeline', 'second', 'pipeline', 'deploy', 'aci', 'pipeline', 'know', 'get', 'get', 'project', 'pipeline', 'id', 'model', 'train', 'ci', 'pipeline', 'looked', 'get', 'ids', 'current', 'running', 'pipeline', 'get', 'anything', 'tried', 'use', 'variable', 'amlpipelineid', 'job', 'get_pipline_id', 'empty', 'tried', 'current', 'file', 'looks', 'like', 'nd', 'pipeline', 'resources', 'define', 'pipeline', 'reference', 'self', 'instead', 'need', 'change', 'model', 'train', 'ci', 'st', 'pipeline', 'name', 'current', 'merged', 'pipeline', 'look', 'somehitng', 'like', 'find', 'anything', 'internet', 'im', 'really', 'stuck', 'someone', 'could', 'guide', 'right', 'direction']","['convert', 'azur', 'pipelin', 'one', 'pipelineazur', 'pipelin', 'devop', 'mlop', 'want', 'convert', 'second', 'pipelin', 'stage', 'first', 'pipelin', 'variabl', 'depend', 'first', 'pipelin', 'first', 'pipelin', 'second', 'pipelin', 'deploy', 'aci', 'pipelin', 'know', 'get', 'get', 'project', 'pipelin', 'id', 'model', 'train', 'ci', 'pipelin', 'look', 'get', 'id', 'current', 'run', 'pipelin', 'get', 'anyth', 'tri', 'use', 'variabl', 'amlpipelineid', 'job', 'get_pipline_id', 'empti', 'tri', 'current', 'file', 'look', 'like', 'nd', 'pipelin', 'resourc', 'defin', 'pipelin', 'refer', 'self', 'instead', 'need', 'chang', 'model', 'train', 'ci', 'st', 'pipelin', 'name', 'current', 'merg', 'pipelin', 'look', 'somehitng', 'like', 'find', 'anyth', 'internet', 'im', 'realli', 'stuck', 'someon', 'could', 'guid', 'right', 'direct']"
121,132,132,16152964,73075131,From where this error is rising and which value to cast for solving it?,"<p>I was using TF Agents for training a DQN algorithm to learn to play Atari Breakout but got a unexpected value error.</p>
<p>All Imports,</p>
<pre><code># General Imports
import gym
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

# Env
from tf_agents.environments import suite_gym

# Env Wrapper
from gym.wrappers import TimeLimit
from tf_agents.environments.wrappers import ActionRepeat
from tf_agents.environments.tf_py_environment import TFPyEnvironment

# Atari Environment
from tf_agents.environments import suite_atari
from tf_agents.environments.atari_preprocessing import AtariPreprocessing
from tf_agents.environments.atari_wrappers import FrameStack4


# DQN 
from tf_agents.networks.q_network import QNetwork

# Agents 
from tf_agents.agents.dqn.dqn_agent import DqnAgent

# Replay Buffer
from tf_agents.replay_buffers import py_hashed_replay_buffer
from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer

# Metrics 
from tf_agents.metrics.tf_metrics import NumberOfEpisodes, EnvironmentSteps, ChosenActionHistogram, AverageEpisodeLengthMetric, AverageReturnMetric
from tf_agents.eval.metric_utils import log_metrics

# Driver
from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver

# Policy 
from tf_agents.policies.random_tf_policy import RandomTFPolicy

# Trasition Trajectories
from tf_agents.trajectories.trajectory import to_transition

# Training
from tf_agents.utils.common import function
</code></pre>
<p>Here, is the Environment I created :</p>
<pre><code>max_episode_steps = 27000
env_name = &quot;BreakoutNoFrameskip-v4&quot; # Do not skip any frame 

env = suite_atari.load(
    env_name,
    max_episode_steps=max_episode_steps,
    gym_env_wrappers=[AtariPreprocessing, FrameStack4],
)
tf_env = TFPyEnvironment(env)
</code></pre>
<p>The Deep Q-Network is as followed :</p>
<pre><code>preprocessing_layers = keras.layers.Lambda(lambda x: tf.cast(x, dtype=tf.float32)/255.) # Dtype and Normalize 
# Params : Filters, Kernel Size, Strides
conv_params = [(32,8,4),(64,4,2),(64,3,1)]
fc_laye_params = [128,512]
dropout_rates = [0.4,0.5]

# Implementing a Deep Q-Network
q_net = QNetwork(
    tf_env.observation_spec(), # Observations =&gt; Inputs
    tf_env.action_spec(), # Acitons Space =&gt; Outputs
    preprocessing_layers=preprocessing_layers, # Preprocessing Layers
    conv_layer_params=conv_params, # Conv Layers
    activation_fn=&quot;relu&quot;, # This i by default only for the Conv layers
    fc_layer_params=fc_laye_params, # Dense layers
    dropout_layer_params=dropout_rates # Dropout Rates only for the Dense Layers
)
train_step = tf.Variable(0.)
update_period = 4
optimizer = keras.optimizers.RMSprop(learning_rate=0.00025, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)


# The epsilon decay over time steps
epsilon_fn = keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=1.0, # inital eta
    decay_steps=250000//update_period, # &lt;=&gt; 1,000,000 ALE frames
    end_learning_rate=0.01 # final eta
)

# The main agent
agent = DqnAgent(
    tf_env.time_step_spec(), # Time Steps
    tf_env.action_spec(), # Aciton Space 
    q_network=q_net, # The Network to be used 
    optimizer=optimizer, # The optimizer to be used
    target_update_period=2000, # &lt;=&gt; 32,000 ALE frames
    td_errors_loss_fn=keras.losses.Huber(reduction=&quot;none&quot;), # TD Error Loss
    gamma=0.99, # Discount Factor
    train_step_counter=train_step, # How many Training Steps have been completed
    epsilon_greedy=lambda: epsilon_fn(train_step) # Epsilon Greedy Policy
)
agent.initialize() # Initialized the Agent
</code></pre>
<p>The replay buffer is as followed,</p>
<pre><code>replay_buffer = TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=tf_env.batch_size,
    max_length=tf.cast(10000, dtype=tf.int64) # 1000000 # Not available in Memory
)
</code></pre>
<p>Here is the Observer,</p>
<pre><code>replay_buffer_observer = replay_buffer.add_batch

# I don't know how to add this as a observer
class ShowProgress:

    def __init__(self, total):
        self.counter = 0
        self.total = total
    
    def __call__(self, trajectory):
        if not trajectory.is_boundary():
            self.counter+=1
        if self.counter%100==0:
            print(&quot;\r{}/{}&quot;.format(self.counter, self.total), end=&quot;&quot;)
</code></pre>
<p>Here is the Training Metrics,</p>
<pre><code>train_metrics = [
    NumberOfEpisodes(),
    AverageEpisodeLengthMetric(),
    EnvironmentSteps(),
    AverageReturnMetric(),
    ChosenActionHistogram()
]
</code></pre>
<p>The Driver :</p>
<pre><code>collect_driver = DynamicStepDriver(
    tf_env, # Env to act in 
    agent.collect_policy, # Policy of agent to follow
    observers=[replay_buffer_observer] + train_metrics, #  observers to broadcast
    num_steps=update_period # Update after how many steps
)

initial_collect_policy = RandomTFPolicy(
    tf_env.time_step_spec(),
    tf_env.action_spec()
)

initial_driver = DynamicStepDriver(
    tf_env,
    initial_collect_policy,
    observers=[replay_buffer_observer, ShowProgress(20000)],
    num_steps=20000 # &lt;==&gt; 80k ALE frames
)

# Activate and Run the Driver
final_time_step, final_policy_state = initial_driver.run()
</code></pre>
<p>The Dataset,</p>
<pre><code>dataset = replay_buffer.as_dataset(
    sample_batch_size=64, # Having 64 Trajectories
    num_steps=2, # This mean 1 Complete Transition 
    num_parallel_calls=3 
).prefetch(3)
</code></pre>
<p>The final Training Loop,</p>
<pre><code>collect_driver.run = function(collect_driver.run)
agent.train = function(agent.train)

def train_agent(n_iterations):
    time_step = None
    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)
    iterator = iter(dataset)
    for iteration in range(n_iterations):
        # print(&quot;\r{}/{}&quot;.format(iteration+1,n_iterations),end=&quot;&quot;)
        policy_state = tf.cast(policy_state, dtype=tf.int32)
        time_step, policy_state = collect_driver.run(time_step, policy_state)
        trajectories, buffer_info = iter(iterator)
        train_loss = agent.train(trajectories)
        print(&quot;\r{} Loss : {:.5f}&quot;.format(iteration, train_loss.loss.numpy()),end=&quot;&quot;)
        if iteration%1000:
            log_metrics(train_metrics)
train_agent(10000000)
</code></pre>
<p>The error encountered is,</p>
<pre><code>
ValueError                                Traceback (most recent call last)
i:\The Expedition\Machine Learning\7. Reinforcement Learning (HOML P2)\TF Agents Learning.ipynb Cell 72 in &lt;cell line: 14&gt;()
     12         if iteration%1000:
     13             log_metrics(train_metrics)
---&gt; 14 train_agent(10000000)

i:\The Expedition\Machine Learning\7. Reinforcement Learning (HOML P2)\TF Agents Learning.ipynb Cell 72 in train_agent(n_iterations)
      5 for iteration in range(n_iterations):
      6     # print(&quot;\r{}/{}&quot;.format(iteration+1,n_iterations),end=&quot;&quot;)
      7     policy_state = tf.cast(policy_state, dtype=tf.int32)
----&gt; 8     time_step, policy_state = collect_driver.run(time_step, policy_state)
      9     trajectories, buffer_info = iter(iterator)
     10     train_loss = agent.train(trajectories)

File ~\AppData\Roaming\Python\Python310\site-packages\tensorflow\python\util\traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~\AppData\Roaming\Python\Python310\site-packages\tf_agents\drivers\dynamic_step_driver.py:182, in DynamicStepDriver.run(self, time_step, policy_state, maximum_iterations)
    165 def run(self, time_step=None, policy_state=None, maximum_iterations=None):
    166   &quot;&quot;&quot;Takes steps in the environment using the policy while updating observers.
    167 
    168   Args:
   (...)
    180     policy_state: Tensor with final step policy state.
    181   &quot;&quot;&quot;
--&gt; 182   return self._run_fn(
    183       time_step=time_step,
    184       policy_state=policy_state,
    185       maximum_iterations=maximum_iterations)

File ~\AppData\Roaming\Python\Python310\site-packages\tf_agents\utils\common.py:188, in function_in_tf1.&lt;locals&gt;.maybe_wrap.&lt;locals&gt;.with_check_resource_vars(*fn_args, **fn_kwargs)
    184 check_tf1_allowed()
    185 if has_eager_been_enabled():
    186   # We're either in eager mode or in tf.function mode (no in-between); so
    187   # autodep-like behavior is already expected of fn.
--&gt; 188   return fn(*fn_args, **fn_kwargs)
    189 if not resource_variables_enabled():
    190   raise RuntimeError(MISSING_RESOURCE_VARIABLES_ERROR)

File ~\AppData\Roaming\Python\Python310\site-packages\tf_agents\drivers\dynamic_step_driver.py:202, in DynamicStepDriver._run(self, time_step, policy_state, maximum_iterations)
...
    File &quot;C:\Users\HP\AppData\Roaming\Python\Python310\site-packages\tf_agents\replay_buffers\table.py&quot;, line 128, in write  *
        write_ops = [

    ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: &lt;tf.Tensor 'value:0' shape=() dtype=int64
</code></pre>
<p>In short the main issue is :</p>
<pre><code>    ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: &lt;tf.Tensor 'value:0' shape=() dtype=int64
</code></pre>
",16,0,0,4,tensorflow;machine-learning;reinforcement-learning;tf-agent,2022-07-22 07:30:00,2022-07-22 07:30:00,2022-07-22 07:30:00,i was using tf agents for training a dqn algorithm to learn to play atari breakout but got a unexpected value error  all imports  here  is the environment i created   the deep q network is as followed   the replay buffer is as followed  here is the observer  here is the training metrics  the driver   the dataset  the final training loop  the error encountered is  in short the main issue is  ,from where this error is rising and which value to cast for solving it ,using tf agents training dqn algorithm learn play atari breakout got unexpected value error imports environment created deep q network followed replay buffer followed observer training metrics driver dataset final training loop error encountered short main issue,error rising value cast solving,error rising value cast solvingusing tf agents training dqn algorithm learn play atari breakout got unexpected value error imports environment created deep q network followed replay buffer followed observer training metrics driver dataset final training loop error encountered short main issue,"['error', 'rising', 'value', 'cast', 'solvingusing', 'tf', 'agents', 'training', 'dqn', 'algorithm', 'learn', 'play', 'atari', 'breakout', 'got', 'unexpected', 'value', 'error', 'imports', 'environment', 'created', 'deep', 'q', 'network', 'followed', 'replay', 'buffer', 'followed', 'observer', 'training', 'metrics', 'driver', 'dataset', 'final', 'training', 'loop', 'error', 'encountered', 'short', 'main', 'issue']","['error', 'rise', 'valu', 'cast', 'solvingus', 'tf', 'agent', 'train', 'dqn', 'algorithm', 'learn', 'play', 'atari', 'breakout', 'got', 'unexpect', 'valu', 'error', 'import', 'environ', 'creat', 'deep', 'q', 'network', 'follow', 'replay', 'buffer', 'follow', 'observ', 'train', 'metric', 'driver', 'dataset', 'final', 'train', 'loop', 'error', 'encount', 'short', 'main', 'issu']"
122,133,133,12485480,62839068,"MemoryError: Unable to allocate MiB for an array with shape and data type, when using anymodel.fit() in sklearn","<p>Getting this memory error. But the book/link I am following doesn't get this error.</p>
<p>A part of Code:</p>
<pre><code>from sklearn.linear_model import SGDClassifier
sgd_clf = SGDClassifier()
sgd_clf.fit(x_train, y_train)
</code></pre>
<p>Error:
<code>MemoryError: Unable to allocate 359. MiB for an array with shape (60000, 784) and data type float64</code></p>
<p>I also get this error when I try to scale the data using StandardScaler's fit_transfrom</p>
<p>But works fine in both if I decrease the size of training set (something like : <code>x_train[:1000]</code> ,<code>y_train[:1000]</code>)</p>
<p>Link for the code in the book <a href=""https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb"" rel=""noreferrer"">here</a>. The error I get is in Line 60 and 63 (<code>In [60]</code> and <code>In [63]</code>)</p>
<p>The book : Aurlien Gron - Hands-On Machine Learning with Scikit-Learn  Keras  and Tensorflow 2nd Ed (Page : 149 / 1130)</p>
<h3>So here's my question :</h3>
<p>Does this has anything to do with my ram? and what does &quot;Unable to allocate 359&quot; mean? is it the memory size ?</p>
<p>Just in case my specs :
CPU - ryzen 2400g , ram - 8gb (3.1gb is free when using jupyter notebook)</p>
",67971,3,13,3,python;machine-learning;scikit-learn,2020-07-10 20:00:45,2020-07-10 20:00:45,2022-07-22 06:02:06,getting this memory error  but the book link i am following doesn t get this error  a part of code  i also get this error when i try to scale the data using standardscaler s fit_transfrom but works fine in both if i decrease the size of training set  something like   x_train     y_train     link for the code in the book   the error i get is in line  and   in    and in     the book   aurlien gron   hands on machine learning with scikit learn  keras  and tensorflow nd ed  page        does this has anything to do with my ram  and what does  unable to allocate   mean  is it the memory size  ,memoryerror  unable to allocate mib for an array with shape and data type  when using anymodel fit   in sklearn,getting memory error book link following get error part code also get error try scale data using standardscaler fit_transfrom works fine decrease size training set something like x_train y_train link code book error get line book aurlien gron hands machine learning scikit learn keras tensorflow nd ed page anything ram unable allocate mean memory size,memoryerror unable allocate mib array shape data type using anymodel fit sklearn,memoryerror unable allocate mib array shape data type using anymodel fit sklearngetting memory error book link following get error part code also get error try scale data using standardscaler fit_transfrom works fine decrease size training set something like x_train y_train link code book error get line book aurlien gron hands machine learning scikit learn keras tensorflow nd ed page anything ram unable allocate mean memory size,"['memoryerror', 'unable', 'allocate', 'mib', 'array', 'shape', 'data', 'type', 'using', 'anymodel', 'fit', 'sklearngetting', 'memory', 'error', 'book', 'link', 'following', 'get', 'error', 'part', 'code', 'also', 'get', 'error', 'try', 'scale', 'data', 'using', 'standardscaler', 'fit_transfrom', 'works', 'fine', 'decrease', 'size', 'training', 'set', 'something', 'like', 'x_train', 'y_train', 'link', 'code', 'book', 'error', 'get', 'line', 'book', 'aurlien', 'gron', 'hands', 'machine', 'learning', 'scikit', 'learn', 'keras', 'tensorflow', 'nd', 'ed', 'page', 'anything', 'ram', 'unable', 'allocate', 'mean', 'memory', 'size']","['memoryerror', 'unabl', 'alloc', 'mib', 'array', 'shape', 'data', 'type', 'use', 'anymodel', 'fit', 'sklearnget', 'memori', 'error', 'book', 'link', 'follow', 'get', 'error', 'part', 'code', 'also', 'get', 'error', 'tri', 'scale', 'data', 'use', 'standardscal', 'fit_transfrom', 'work', 'fine', 'decreas', 'size', 'train', 'set', 'someth', 'like', 'x_train', 'y_train', 'link', 'code', 'book', 'error', 'get', 'line', 'book', 'aurlien', 'gron', 'hand', 'machin', 'learn', 'scikit', 'learn', 'kera', 'tensorflow', 'nd', 'ed', 'page', 'anyth', 'ram', 'unabl', 'alloc', 'mean', 'memori', 'size']"
123,134,134,4489777,73074615,How to fix AttributeError: &#39;KerasTensor&#39; object has no attribute &#39;_id&#39; when using custom loss?,"<p>In learning to do machine learning with point clouds I am porting the original <a href=""https://github.com/charlesq34/pointnet"" rel=""nofollow noreferrer"">PointNet</a> code (written in TF1) to TensorFlow2. Everything was going well until I tried porting the custom loss. I subclassed <em>keras.Model</em> to implement a custom training step as follows:</p>
<pre><code>class CustomModel(keras.Model):
    
    def set_params(self, end_points, reg_weight=0.001):
        self.end_points = end_points
        self.reg_weight = reg_weight
        
    def train_step(self, data):
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
     
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=y)
            
            classify_loss = tf.reduce_mean(loss)
            
            # Enforce the transformation as orthogonal matrix
            transform = self.end_points['transform'] # BxKxK
            K = transform.shape[1]
            mat_diff = tf.matmul(transform, tf.transpose(transform, perm=[0,2,1]))
            mat_diff -= tf.constant(np.eye(K), dtype=tf.float32)
            mat_diff_loss = tf.nn.l2_loss(mat_diff) 
            

            loss = classify_loss + mat_diff_loss * self.reg_weight

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        
        self.compiled_metrics.update_state(y, y_pred)
        
        return {m.name: m.result() for m in self.metrics}
</code></pre>
<p>when training, I get an error exactly at <code>gradients = tape.gradient(loss, trainable_vars)</code></p>
<p>The error is:</p>
<blockquote>
<p>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
last) Input In [15], in &lt;cell line: 12&gt;()
9 steps_per_epoch = train_count//BATCH_SIZE
10 validation_steps = validation_count//BATCH_SIZE
---&gt; 12 history = model.fit(training_dataset,
13                     steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps,
epochs=EPOCHS)</p>
<p>File
~/anaconda3/envs/tf_p3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67,
in filter_traceback..error_handler(*args, **kwargs)
65 except Exception as e:  # pylint: disable=broad-except
66   filtered_tb = _process_traceback_frames(e.<strong>traceback</strong>)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
68 finally:
69   del filtered_tb</p>
<p>File /tmp/<em><em>autograph_generated_file9wypl4wp.py:15, in
outer_factory..inner_factory..tf__train_function(iterator)
13 try:
14     do_return = True
---&gt; 15     retval</em> = ag</em>_.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
16 except:
17     do_return = False</p>
<p>Input In [3], in CustomModel.train_step(self, data)
30 # Compute gradients
31 trainable_vars = self.trainable_variables
---&gt; 32 gradients = tape.gradient(loss, trainable_vars)
34 # Update weights
35 self.optimizer.apply_gradients(zip(gradients, trainable_vars))</p>
<p>AttributeError: in user code:</p>
<pre><code>File &quot;/home/carlos/anaconda3/envs/tf_p3.9/lib/python3.9/site-packages/keras/engine/training.py&quot;,
</code></pre>
<p>line 1051, in train_function  *
return step_function(self, iterator)
File &quot;/home/carlos/anaconda3/envs/tf_p3.9/lib/python3.9/site-packages/keras/engine/training.py&quot;,
line 1040, in step_function  **
outputs = model.distribute_strategy.run(run_step, args=(data,))
File &quot;/home/carlos/anaconda3/envs/tf_p3.9/lib/python3.9/site-packages/keras/engine/training.py&quot;,
line 1030, in run_step  **
outputs = model.train_step(data)
File &quot;/tmp/ipykernel_16312/3458445038.py&quot;, line 32, in train_step
gradients = tape.gradient(loss, trainable_vars)</p>
<pre><code>AttributeError: 'KerasTensor' object has no attribute '_id'
</code></pre>
</blockquote>
<p><a href=""https://colab.research.google.com/drive/1VSub1CYmGlG2vUYYdDTgNOIqmV58uSkM?usp=sharing"" rel=""nofollow noreferrer"">Here is a Colab</a> with the main parts, you can run all cells to reproduce the error.</p>
<p>I have spent several hours trying different ways to implement the custom loss without luck, any help would be greatly appreciated. Thanks!</p>
",38,0,0,5,python;tensorflow;keras;deep-learning;loss-function,2022-07-22 05:52:27,2022-07-22 05:52:27,2022-07-22 05:52:27,in learning to do machine learning with point clouds i am porting the original  code  written in tf  to tensorflow  everything was going well until i tried porting the custom loss  i subclassed keras model to implement a custom training step as follows  when training  i get an error exactly at gradients   tape gradient loss  trainable_vars  the error is  attributeerror  in user code   with the main parts  you can run all cells to reproduce the error  i have spent several hours trying different ways to implement the custom loss without luck  any help would be greatly appreciated  thanks ,how to fix attributeerror     kerastensor    object has no attribute    _id    when using custom loss ,learning machine learning point clouds porting original code written tf tensorflow everything going well tried porting loss subclassed keras model implement training step follows training get error exactly gradients tape gradient loss trainable_vars error attributeerror user code main parts run cells reproduce error spent several hours trying different ways implement loss without luck help would greatly appreciated thanks,fix attributeerror kerastensor object attribute _id using loss,fix attributeerror kerastensor object attribute _id using losslearning machine learning point clouds porting original code written tf tensorflow everything going well tried porting loss subclassed keras model implement training step follows training get error exactly gradients tape gradient loss trainable_vars error attributeerror user code main parts run cells reproduce error spent several hours trying different ways implement loss without luck help would greatly appreciated thanks,"['fix', 'attributeerror', 'kerastensor', 'object', 'attribute', '_id', 'using', 'losslearning', 'machine', 'learning', 'point', 'clouds', 'porting', 'original', 'code', 'written', 'tf', 'tensorflow', 'everything', 'going', 'well', 'tried', 'porting', 'loss', 'subclassed', 'keras', 'model', 'implement', 'training', 'step', 'follows', 'training', 'get', 'error', 'exactly', 'gradients', 'tape', 'gradient', 'loss', 'trainable_vars', 'error', 'attributeerror', 'user', 'code', 'main', 'parts', 'run', 'cells', 'reproduce', 'error', 'spent', 'several', 'hours', 'trying', 'different', 'ways', 'implement', 'loss', 'without', 'luck', 'help', 'would', 'greatly', 'appreciated', 'thanks']","['fix', 'attributeerror', 'kerastensor', 'object', 'attribut', '_id', 'use', 'losslearn', 'machin', 'learn', 'point', 'cloud', 'port', 'origin', 'code', 'written', 'tf', 'tensorflow', 'everyth', 'go', 'well', 'tri', 'port', 'loss', 'subclass', 'kera', 'model', 'implement', 'train', 'step', 'follow', 'train', 'get', 'error', 'exactli', 'gradient', 'tape', 'gradient', 'loss', 'trainable_var', 'error', 'attributeerror', 'user', 'code', 'main', 'part', 'run', 'cell', 'reproduc', 'error', 'spent', 'sever', 'hour', 'tri', 'differ', 'way', 'implement', 'loss', 'without', 'luck', 'help', 'would', 'greatli', 'appreci', 'thank']"
124,135,135,12109488,62409290,How to deal with array of string features in traditional machine learning?,"<h1>Problem</h1>
<p>Let's say we have a dataframe that looks like this:</p>
<pre class=""lang-sql prettyprint-override""><code>age  job         friends                                    label
23   'engineer'  ['World of Warcraft', 'Netflix', '9gag']   1
35   'manager'   NULL                                       0
...
</code></pre>
<p>If we are interested in training a classifier that predicts <em>label</em> using <em>age</em>, <em>job</em>, and <em>friends</em> as features, how would we go about transforming the features into a numerical array which can be fed into a model?</p>
<ul>
<li>Age is pretty straightforward since it is already numerical.</li>
<li>Job can be hashed / indexed since it is a categorical variable.</li>
<li>Friends is a list of categorical variables. How would I go about representing this feature?</li>
</ul>
<h1>Approaches:</h1>
<p>Hash each element of the list. Using the example dataframe, let's assume our hashing function has the following mapping:</p>
<pre class=""lang-sql prettyprint-override""><code>NULL                -&gt; 0
engineer            -&gt; 42069
World of Warcraft   -&gt; 9001
Netflix             -&gt; 14
9gag                -&gt; 9
manager             -&gt; 250
 
</code></pre>
<p>Let's further assume that the maximum length of friends is 5. Anything shorter gets zero-padded on the right hand side. If friends size is larger than 5, then the first 5 elements are selected.</p>
<h2>Approach 1: Hash and Stack</h2>
<p>dataframe after feature transformation would look like this:</p>
<pre class=""lang-sql prettyprint-override""><code>feature                             label
[23, 42069, 9001, 14, 9, 0, 0]      1
[35, 250,   0,    0,  0, 0, 0]      0
</code></pre>
<h3>Limitations</h3>
<p>Consider the following:</p>
<pre class=""lang-sql prettyprint-override""><code>age  job           friends                                        label
23   'engineer'    ['World of Warcraft', 'Netflix', '9gag']       1
35   'manager'      NULL                                          0
26   'engineer'    ['Netflix', '9gag', 'World of Warcraft']       1
...
</code></pre>
<p>Compare the features of the first and third record:</p>
<pre class=""lang-sql prettyprint-override""><code>feature                             label
[23, 42069, 9001, 14, 9, 0, 0]      1
[35, 250,   0,    0,  0, 0, 0]      0
[26, 42069, 14,    9, 9001, 0]      1
</code></pre>
<p>Both records have the same set of friends, but are ordered differently resulting in a different feature hashing even though they should be the same.</p>
<h2>Approach 2: Hash, Order, and Stack</h2>
<p>To solve the limitation of Approach 1, simply order the hashes from the <em>friends</em> feature. This  would result in the following feature transform (assuming descending order):</p>
<pre class=""lang-sql prettyprint-override""><code>feature                             label
[23, 42069, 9001, 14, 9, 0, 0]      1
[35, 250,   0,    0,  0, 0, 0]      0
[26, 42069, 9001, 14, 9, 0, 0]      1
</code></pre>
<p>This approach has a limitation too. Consider the following:</p>
<pre class=""lang-sql prettyprint-override""><code>age  job           friends                                        label
23   'engineer'    ['World of Warcraft', 'Netflix', '9gag']       1
35   'manager'      NULL                                          0
26   'engineer'    ['Netflix', '9gag', 'World of Warcraft']       1
42   'manager'     ['Netflix', '9gag']                            1
...
</code></pre>
<p>Applying feature transform with ordering we get:</p>
<pre class=""lang-sql prettyprint-override""><code>row  feature                             label
1    [23, 42069, 9001, 14, 9, 0, 0]      1
2    [35, 250,   0,    0,  0, 0, 0]      0
3    [26, 42069, 9001, 14, 9, 0, 0]      1
4    [44, 250, 14, 9, 0, 0, 0]           1
</code></pre>
<p>What is the problem with the above features? Well, the hashes for Netflix and 9gag in rows 1 and 3 have the same index in the array but not in row 4. This would mess up with the training.</p>
<h2>Approach 3: Convert Array to Columns</h2>
<p>What if we convert <em>friends</em> into a set of 5 columns and deal with each of the resulting columns just like we deal with any categorical variable?</p>
<p>Well, let's assume the <em>friends</em> vocabulary size is large (&gt;100k). It would then be madness to go and create &gt;100k columns where each column is responsible for the hash of the respective vocab element.</p>
<h2>Approach 4: One-Hot-Encoding and then Sum</h2>
<p>How about this? Convert each hash to one-hot-vector, and add up all these vectors.</p>
<p>In this  case, the feature in row one for example would look like this:</p>
<pre><code>[23, 42069, 0<sup>1x8</sup>, 1, 0<sup>1x4</sup>, 1, 0<sup>1x8986</sup>, 1, 0<sup>1x(max_hash_size-8987)</sup>]</code></pre>
<p>Where 0<sup>1x8</sup> denotes a row of 8 zeros.</p>
<p>The problem with this approach is that these vectors will be very huge and sparse.</p>
<h2>Approach 5: Use Embedding Layer and 1D-Conv</h2>
<p>With this approach, we feed each word in the friends array to the embedding layer, then convolve. Similar to the Keras IMDB example: <a href=""https://keras.io/examples/imdb_cnn/"" rel=""noreferrer"">https://keras.io/examples/imdb_cnn/</a></p>
<p>Limitation: requires using deep learning frameworks. I want something which works with traditional machine learning. I want to do logistic regression or decision tree.</p>
<p>What are your thoughts on this?</p>
",1736,1,11,4,machine-learning;deep-learning;feature-extraction;feature-engineering,2020-06-16 16:08:43,2020-06-16 16:08:43,2022-07-22 00:12:41,let s say we have a dataframe that looks like this  if we are interested in training a classifier that predicts label using age  job  and friends as features  how would we go about transforming the features into a numerical array which can be fed into a model  hash each element of the list  using the example dataframe  let s assume our hashing function has the following mapping  let s further assume that the maximum length of friends is   anything shorter gets zero padded on the right hand side  if friends size is larger than   then the first  elements are selected  dataframe after feature transformation would look like this  consider the following  compare the features of the first and third record  both records have the same set of friends  but are ordered differently resulting in a different feature hashing even though they should be the same  to solve the limitation of approach   simply order the hashes from the friends feature  this  would result in the following feature transform  assuming descending order   this approach has a limitation too  consider the following  applying feature transform with ordering we get  what is the problem with the above features  well  the hashes for netflix and gag in rows  and  have the same index in the array but not in row   this would mess up with the training  what if we convert friends into a set of  columns and deal with each of the resulting columns just like we deal with any categorical variable  well  let s assume the friends vocabulary size is large   gt k   it would then be madness to go and create  gt k columns where each column is responsible for the hash of the respective vocab element  how about this  convert each hash to one hot vector  and add up all these vectors  in this  case  the feature in row one for example would look like this  where x denotes a row of  zeros  the problem with this approach is that these vectors will be very huge and sparse  with this approach  we feed each word in the friends array to the embedding layer  then convolve  similar to the keras imdb example   limitation  requires using deep learning frameworks  i want something which works with traditional machine learning  i want to do logistic regression or decision tree  what are your thoughts on this ,how to deal with array of string features in traditional machine learning ,let say dataframe looks like interested training classifier predicts label using age job friends features would go transforming features numerical array fed model hash element using example dataframe let assume hashing function following mapping let assume maximum length friends anything shorter gets zero padded right hand side friends size larger first elements selected dataframe feature transformation would look like consider following compare features first third record records set friends ordered differently resulting different feature hashing even though solve limitation approach simply order hashes friends feature would result following feature transform assuming descending order approach limitation consider following applying feature transform ordering get problem features well hashes netflix gag rows index array row would mess training convert friends set columns deal resulting columns like deal categorical variable well let assume friends vocabulary size large gt k would madness go create gt k columns column responsible hash respective vocab element convert hash one hot vector vectors case feature row one example would look like x denotes row zeros problem approach vectors huge sparse approach feed word friends array embedding layer convolve similar keras imdb example limitation requires using deep learning frameworks want something works traditional machine learning want logistic regression decision tree thoughts,deal array string features traditional machine learning,deal array string features traditional machine learninglet say dataframe looks like interested training classifier predicts label using age job friends features would go transforming features numerical array fed model hash element using example dataframe let assume hashing function following mapping let assume maximum length friends anything shorter gets zero padded right hand side friends size larger first elements selected dataframe feature transformation would look like consider following compare features first third record records set friends ordered differently resulting different feature hashing even though solve limitation approach simply order hashes friends feature would result following feature transform assuming descending order approach limitation consider following applying feature transform ordering get problem features well hashes netflix gag rows index array row would mess training convert friends set columns deal resulting columns like deal categorical variable well let assume friends vocabulary size large gt k would madness go create gt k columns column responsible hash respective vocab element convert hash one hot vector vectors case feature row one example would look like x denotes row zeros problem approach vectors huge sparse approach feed word friends array embedding layer convolve similar keras imdb example limitation requires using deep learning frameworks want something works traditional machine learning want logistic regression decision tree thoughts,"['deal', 'array', 'string', 'features', 'traditional', 'machine', 'learninglet', 'say', 'dataframe', 'looks', 'like', 'interested', 'training', 'classifier', 'predicts', 'label', 'using', 'age', 'job', 'friends', 'features', 'would', 'go', 'transforming', 'features', 'numerical', 'array', 'fed', 'model', 'hash', 'element', 'using', 'example', 'dataframe', 'let', 'assume', 'hashing', 'function', 'following', 'mapping', 'let', 'assume', 'maximum', 'length', 'friends', 'anything', 'shorter', 'gets', 'zero', 'padded', 'right', 'hand', 'side', 'friends', 'size', 'larger', 'first', 'elements', 'selected', 'dataframe', 'feature', 'transformation', 'would', 'look', 'like', 'consider', 'following', 'compare', 'features', 'first', 'third', 'record', 'records', 'set', 'friends', 'ordered', 'differently', 'resulting', 'different', 'feature', 'hashing', 'even', 'though', 'solve', 'limitation', 'approach', 'simply', 'order', 'hashes', 'friends', 'feature', 'would', 'result', 'following', 'feature', 'transform', 'assuming', 'descending', 'order', 'approach', 'limitation', 'consider', 'following', 'applying', 'feature', 'transform', 'ordering', 'get', 'problem', 'features', 'well', 'hashes', 'netflix', 'gag', 'rows', 'index', 'array', 'row', 'would', 'mess', 'training', 'convert', 'friends', 'set', 'columns', 'deal', 'resulting', 'columns', 'like', 'deal', 'categorical', 'variable', 'well', 'let', 'assume', 'friends', 'vocabulary', 'size', 'large', 'gt', 'k', 'would', 'madness', 'go', 'create', 'gt', 'k', 'columns', 'column', 'responsible', 'hash', 'respective', 'vocab', 'element', 'convert', 'hash', 'one', 'hot', 'vector', 'vectors', 'case', 'feature', 'row', 'one', 'example', 'would', 'look', 'like', 'x', 'denotes', 'row', 'zeros', 'problem', 'approach', 'vectors', 'huge', 'sparse', 'approach', 'feed', 'word', 'friends', 'array', 'embedding', 'layer', 'convolve', 'similar', 'keras', 'imdb', 'example', 'limitation', 'requires', 'using', 'deep', 'learning', 'frameworks', 'want', 'something', 'works', 'traditional', 'machine', 'learning', 'want', 'logistic', 'regression', 'decision', 'tree', 'thoughts']","['deal', 'array', 'string', 'featur', 'tradit', 'machin', 'learninglet', 'say', 'datafram', 'look', 'like', 'interest', 'train', 'classifi', 'predict', 'label', 'use', 'age', 'job', 'friend', 'featur', 'would', 'go', 'transform', 'featur', 'numer', 'array', 'fed', 'model', 'hash', 'element', 'use', 'exampl', 'datafram', 'let', 'assum', 'hash', 'function', 'follow', 'map', 'let', 'assum', 'maximum', 'length', 'friend', 'anyth', 'shorter', 'get', 'zero', 'pad', 'right', 'hand', 'side', 'friend', 'size', 'larger', 'first', 'element', 'select', 'datafram', 'featur', 'transform', 'would', 'look', 'like', 'consid', 'follow', 'compar', 'featur', 'first', 'third', 'record', 'record', 'set', 'friend', 'order', 'differ', 'result', 'differ', 'featur', 'hash', 'even', 'though', 'solv', 'limit', 'approach', 'simpli', 'order', 'hash', 'friend', 'featur', 'would', 'result', 'follow', 'featur', 'transform', 'assum', 'descend', 'order', 'approach', 'limit', 'consid', 'follow', 'appli', 'featur', 'transform', 'order', 'get', 'problem', 'featur', 'well', 'hash', 'netflix', 'gag', 'row', 'index', 'array', 'row', 'would', 'mess', 'train', 'convert', 'friend', 'set', 'column', 'deal', 'result', 'column', 'like', 'deal', 'categor', 'variabl', 'well', 'let', 'assum', 'friend', 'vocabulari', 'size', 'larg', 'gt', 'k', 'would', 'mad', 'go', 'creat', 'gt', 'k', 'column', 'column', 'respons', 'hash', 'respect', 'vocab', 'element', 'convert', 'hash', 'one', 'hot', 'vector', 'vector', 'case', 'featur', 'row', 'one', 'exampl', 'would', 'look', 'like', 'x', 'denot', 'row', 'zero', 'problem', 'approach', 'vector', 'huge', 'spars', 'approach', 'feed', 'word', 'friend', 'array', 'embed', 'layer', 'convolv', 'similar', 'kera', 'imdb', 'exampl', 'limit', 'requir', 'use', 'deep', 'learn', 'framework', 'want', 'someth', 'work', 'tradit', 'machin', 'learn', 'want', 'logist', 'regress', 'decis', 'tree', 'thought']"
125,136,136,308827,72970228,Ensemble of machine learning models in scikit-learn,"<pre><code>group        feature_1        feature_2       year            dependent_variable
group_a         12               19           2010               0.4
group_a         11               13           2011               0.9
group_a         10               5            2012               1.2
group_a         16               9            2013               3.2
group_b         8               29            2010               0.6
group_b         9               33            2011               0.1 
group_b         111             15            2012               2.1 
group_b         16              19            2013               12.2  
</code></pre>
<p>In the dataframe above, I want to use <code>feature_1</code>, <code>feature_2</code> to predict <code>dependent_variable</code>. To do this, I want to construct two models: In the first model, I want to construct a separate model for each group. In the second model, I want to use all the available data. In both cases, data from the years 2010 to 2012 will be used for training and 2013 will be used for testing.</p>
<p>How can I construct an ensemble model using the two models outlined above? The data is a toy dataset but in the real dataset, there will be a lot more groups, years and features. In particular, I am interested in an approach that will work with scikit-learn compatible models.</p>
",187,4,2,3,python;scikit-learn;ensemble-learning,2022-07-13 20:17:38,2022-07-13 20:17:38,2022-07-21 22:03:58,in the dataframe above  i want to use feature_  feature_ to predict dependent_variable  to do this  i want to construct two models  in the first model  i want to construct a separate model for each group  in the second model  i want to use all the available data  in both cases  data from the years  to  will be used for training and  will be used for testing  how can i construct an ensemble model using the two models outlined above  the data is a toy dataset but in the real dataset  there will be a lot more groups  years and features  in particular  i am interested in an approach that will work with scikit learn compatible models ,ensemble of machine learning models in scikit learn,dataframe want use feature_ feature_ predict dependent_variable want construct two models first model want construct separate model group second model want use available data cases data years used training used testing construct ensemble model using two models outlined data toy dataset real dataset lot groups years features particular interested approach work scikit learn compatible models,ensemble machine learning models scikit learn,ensemble machine learning models scikit learndataframe want use feature_ feature_ predict dependent_variable want construct two models first model want construct separate model group second model want use available data cases data years used training used testing construct ensemble model using two models outlined data toy dataset real dataset lot groups years features particular interested approach work scikit learn compatible models,"['ensemble', 'machine', 'learning', 'models', 'scikit', 'learndataframe', 'want', 'use', 'feature_', 'feature_', 'predict', 'dependent_variable', 'want', 'construct', 'two', 'models', 'first', 'model', 'want', 'construct', 'separate', 'model', 'group', 'second', 'model', 'want', 'use', 'available', 'data', 'cases', 'data', 'years', 'used', 'training', 'used', 'testing', 'construct', 'ensemble', 'model', 'using', 'two', 'models', 'outlined', 'data', 'toy', 'dataset', 'real', 'dataset', 'lot', 'groups', 'years', 'features', 'particular', 'interested', 'approach', 'work', 'scikit', 'learn', 'compatible', 'models']","['ensembl', 'machin', 'learn', 'model', 'scikit', 'learndatafram', 'want', 'use', 'feature_', 'feature_', 'predict', 'dependent_vari', 'want', 'construct', 'two', 'model', 'first', 'model', 'want', 'construct', 'separ', 'model', 'group', 'second', 'model', 'want', 'use', 'avail', 'data', 'case', 'data', 'year', 'use', 'train', 'use', 'test', 'construct', 'ensembl', 'model', 'use', 'two', 'model', 'outlin', 'data', 'toy', 'dataset', 'real', 'dataset', 'lot', 'group', 'year', 'featur', 'particular', 'interest', 'approach', 'work', 'scikit', 'learn', 'compat', 'model']"
126,138,138,4029467,73071394,Designing data pipeline using in-memory database,"<p>I'm designing a data pipeline that feeds batches of records into a machine learning model. The data pipeline reads data from a local database (or files in local storage), collates into batches and feeds into the ML model. Refer to the graphic below. Since the training step uses a GPU, I'd like that step to be the bottleneck and not the data loading process.</p>
<p><a href=""https://i.stack.imgur.com/ursmw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ursmw.jpg"" alt=""Data Pipeline Design"" /></a></p>
<p>In my case, the data loading process is much slower than the training step and I'm trying to use parallelism such that for every train step, enough batches are generated that the train step is never waiting. This improves the GPU utilization. The reason my data loading process is slower because each batch is ~150-200 MB.</p>
<p>I tried using Pytorch's <code>DataLoader</code> where it launches multiple workers using <code>multiprocessing.Process</code> to create batches and feed to the network. It works as long as the size of the batch (memory footprint) is not big. But there is no performance improvement for my case with more workers. I attribute this to the serialization / deserialization mechanism using pickling in python's <code>multiprocessing</code>. You can refer to this <a href=""https://github.com/pytorch/pytorch/issues/81412"" rel=""nofollow noreferrer"">github issue</a> for more context.</p>
<p><strong>Question 1</strong>:
How can I design such a data pipeline which can parallelize the batch generation where each batch is large (150-200 MB)? Multiprocessing only works if the batch size is small.</p>
<p>It is a multi-producer, single-consumer pattern. I tried creating a shared queue where each worker adds to the queue. However, because of the data size, there is NO improvement using <code>multiprocessing</code>.</p>
<p><strong>Possible Solution</strong></p>
<p>I'm thinking of creating a in-memory database to maintain a buffer. The database would allow parallel inserts where each worker would independently keep adding to the buffer. With enough workers in parallel, there would be a point where average time for production of a batch equals the consumption of a batch. In this example, it would be 10 workers since each worker takes 0.5 seconds and the train step takes 0.05 seconds.</p>
<p><strong>Question 2:</strong>
Is Redis a good option to maintain the buffer as cache? Does it allow parallel inserts without hurting read performance?</p>
<p><strong>Question 3:</strong>
Are there other alternatives for in-memory databases that would take advantage of parallelization of inserts?</p>
",23,0,0,4,python;redis;multiprocessing;in-memory-database,2022-07-21 21:57:59,2022-07-21 21:57:59,2022-07-21 21:57:59,i m designing a data pipeline that feeds batches of records into a machine learning model  the data pipeline reads data from a local database  or files in local storage   collates into batches and feeds into the ml model  refer to the graphic below  since the training step uses a gpu  i d like that step to be the bottleneck and not the data loading process   in my case  the data loading process is much slower than the training step and i m trying to use parallelism such that for every train step  enough batches are generated that the train step is never waiting  this improves the gpu utilization  the reason my data loading process is slower because each batch is    mb  i tried using pytorch s dataloader where it launches multiple workers using multiprocessing process to create batches and feed to the network  it works as long as the size of the batch  memory footprint  is not big  but there is no performance improvement for my case with more workers  i attribute this to the serialization   deserialization mechanism using pickling in python s multiprocessing  you can refer to this  for more context  it is a multi producer  single consumer pattern  i tried creating a shared queue where each worker adds to the queue  however  because of the data size  there is no improvement using multiprocessing  possible solution i m thinking of creating a in memory database to maintain a buffer  the database would allow parallel inserts where each worker would independently keep adding to the buffer  with enough workers in parallel  there would be a point where average time for production of a batch equals the consumption of a batch  in this example  it would be  workers since each worker takes   seconds and the train step takes   seconds ,designing data pipeline using in memory database,designing data pipeline feeds batches records machine learning model data pipeline reads data local database files local storage collates batches feeds ml model refer graphic since training step uses gpu like step bottleneck data loading process case data loading process much slower training step trying use parallelism every train step enough batches generated train step never waiting improves gpu utilization reason data loading process slower batch mb tried using pytorch dataloader launches multiple workers using multiprocessing process create batches feed network works long size batch memory footprint big performance improvement case workers attribute serialization deserialization mechanism using pickling python multiprocessing refer context multi producer single consumer pattern tried creating shared queue worker adds queue however data size improvement using multiprocessing possible solution thinking creating memory database maintain buffer database would allow parallel inserts worker would independently keep adding buffer enough workers parallel would point average time production batch equals consumption batch example would workers since worker takes seconds train step takes seconds,designing data pipeline using memory database,designing data pipeline using memory databasedesigning data pipeline feeds batches records machine learning model data pipeline reads data local database files local storage collates batches feeds ml model refer graphic since training step uses gpu like step bottleneck data loading process case data loading process much slower training step trying use parallelism every train step enough batches generated train step never waiting improves gpu utilization reason data loading process slower batch mb tried using pytorch dataloader launches multiple workers using multiprocessing process create batches feed network works long size batch memory footprint big performance improvement case workers attribute serialization deserialization mechanism using pickling python multiprocessing refer context multi producer single consumer pattern tried creating shared queue worker adds queue however data size improvement using multiprocessing possible solution thinking creating memory database maintain buffer database would allow parallel inserts worker would independently keep adding buffer enough workers parallel would point average time production batch equals consumption batch example would workers since worker takes seconds train step takes seconds,"['designing', 'data', 'pipeline', 'using', 'memory', 'databasedesigning', 'data', 'pipeline', 'feeds', 'batches', 'records', 'machine', 'learning', 'model', 'data', 'pipeline', 'reads', 'data', 'local', 'database', 'files', 'local', 'storage', 'collates', 'batches', 'feeds', 'ml', 'model', 'refer', 'graphic', 'since', 'training', 'step', 'uses', 'gpu', 'like', 'step', 'bottleneck', 'data', 'loading', 'process', 'case', 'data', 'loading', 'process', 'much', 'slower', 'training', 'step', 'trying', 'use', 'parallelism', 'every', 'train', 'step', 'enough', 'batches', 'generated', 'train', 'step', 'never', 'waiting', 'improves', 'gpu', 'utilization', 'reason', 'data', 'loading', 'process', 'slower', 'batch', 'mb', 'tried', 'using', 'pytorch', 'dataloader', 'launches', 'multiple', 'workers', 'using', 'multiprocessing', 'process', 'create', 'batches', 'feed', 'network', 'works', 'long', 'size', 'batch', 'memory', 'footprint', 'big', 'performance', 'improvement', 'case', 'workers', 'attribute', 'serialization', 'deserialization', 'mechanism', 'using', 'pickling', 'python', 'multiprocessing', 'refer', 'context', 'multi', 'producer', 'single', 'consumer', 'pattern', 'tried', 'creating', 'shared', 'queue', 'worker', 'adds', 'queue', 'however', 'data', 'size', 'improvement', 'using', 'multiprocessing', 'possible', 'solution', 'thinking', 'creating', 'memory', 'database', 'maintain', 'buffer', 'database', 'would', 'allow', 'parallel', 'inserts', 'worker', 'would', 'independently', 'keep', 'adding', 'buffer', 'enough', 'workers', 'parallel', 'would', 'point', 'average', 'time', 'production', 'batch', 'equals', 'consumption', 'batch', 'example', 'would', 'workers', 'since', 'worker', 'takes', 'seconds', 'train', 'step', 'takes', 'seconds']","['design', 'data', 'pipelin', 'use', 'memori', 'databasedesign', 'data', 'pipelin', 'feed', 'batch', 'record', 'machin', 'learn', 'model', 'data', 'pipelin', 'read', 'data', 'local', 'databas', 'file', 'local', 'storag', 'collat', 'batch', 'feed', 'ml', 'model', 'refer', 'graphic', 'sinc', 'train', 'step', 'use', 'gpu', 'like', 'step', 'bottleneck', 'data', 'load', 'process', 'case', 'data', 'load', 'process', 'much', 'slower', 'train', 'step', 'tri', 'use', 'parallel', 'everi', 'train', 'step', 'enough', 'batch', 'gener', 'train', 'step', 'never', 'wait', 'improv', 'gpu', 'util', 'reason', 'data', 'load', 'process', 'slower', 'batch', 'mb', 'tri', 'use', 'pytorch', 'dataload', 'launch', 'multipl', 'worker', 'use', 'multiprocess', 'process', 'creat', 'batch', 'feed', 'network', 'work', 'long', 'size', 'batch', 'memori', 'footprint', 'big', 'perform', 'improv', 'case', 'worker', 'attribut', 'serial', 'deseri', 'mechan', 'use', 'pickl', 'python', 'multiprocess', 'refer', 'context', 'multi', 'produc', 'singl', 'consum', 'pattern', 'tri', 'creat', 'share', 'queue', 'worker', 'add', 'queue', 'howev', 'data', 'size', 'improv', 'use', 'multiprocess', 'possibl', 'solut', 'think', 'creat', 'memori', 'databas', 'maintain', 'buffer', 'databas', 'would', 'allow', 'parallel', 'insert', 'worker', 'would', 'independ', 'keep', 'ad', 'buffer', 'enough', 'worker', 'parallel', 'would', 'point', 'averag', 'time', 'product', 'batch', 'equal', 'consumpt', 'batch', 'exampl', 'would', 'worker', 'sinc', 'worker', 'take', 'second', 'train', 'step', 'take', 'second']"
127,139,139,11872120,57319673,How to fix System.Data.SQLite not found when deploying a Visual Studio app with advanced installer?,"<p>(I apologize for my bad English, I'm a Mexican student)
I'm trying to deploy an app (using windows forms) which has an installer (Advanced installer ) and using SQLite, when running on Visual Studio works without problem, but when installed I get an &quot;System.Data.SqLite not found&quot;  error, How to fix it to work, explained for a new persons that is learning programing?</p>
<p>I have tried all the solutions and recommendations of similar questions, but the error keeps popping. I tried to solve it in a W10 64-bit pc.
Previously I tried with this question <a href=""https://stackoverflow.com/questions/1278929/could-not-load-file-or-assembly-system-data-sqlite"">Could not load file or assembly &#39;System.Data.SQLite&#39;</a>
The main difference is that the problem is showed when running in VS, solved by installing through Nu-get or changing some configurations, my problem is only when compiling an .msi installer and installing the app, and that changes doesn't work</p>
<p>here is the code error, if is not enough tell me and I can share more info</p>
<pre><code>System.IO.FileNotFoundException: No se puede cargar el archivo o ensamblado 'System.Data.SQLite, Version=1.0.111.0, Culture=neutral, PublicKeyToken=db937bc2d44ff139' ni una de sus dependencias. El sistema no puede encontrar el archivo especificado.
Nombre de archivo: 'System.Data.SQLite, Version=1.0.111.0, Culture=neutral, PublicKeyToken=db937bc2d44ff139'
   en Half_Heart_IDE.db_driver.Start_up(String Adress)
   en Half_Heart_IDE.Form1.Crear_todo(Object sender, EventArgs e)
   en System.Windows.Forms.Control.OnClick(EventArgs e)
   en System.Windows.Forms.Button.OnClick(EventArgs e)
   en System.Windows.Forms.Button.OnMouseUp(MouseEventArgs mevent)
   en System.Windows.Forms.Control.WmMouseUp(Message&amp; m, MouseButtons button, Int32 clicks)
   en System.Windows.Forms.Control.WndProc(Message&amp; m)
   en System.Windows.Forms.ButtonBase.WndProc(Message&amp; m)
   en System.Windows.Forms.Button.WndProc(Message&amp; m)
   en System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message&amp; m)
   en System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message&amp; m)
   en System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)
</code></pre>
<p>I want to fix it in order to be capable of using SQLite in 64/32bit machines (or creating specific installers) when the app is installed using the .msi generated by Advanced installer 16.1 in VS Community 2019 16.2.0</p>
",454,1,0,4,c#;visual-studio;sqlite;advanced-installer,2019-08-02 06:49:57,2019-08-02 06:49:57,2022-07-21 20:51:04,here is the code error  if is not enough tell me and i can share more info i want to fix it in order to be capable of using sqlite in  bit machines  or creating specific installers  when the app is installed using the  msi generated by advanced installer   in vs community    ,how to fix system data sqlite not found when deploying a visual studio app with advanced installer ,code error enough tell share info want fix order capable using sqlite bit machines creating specific installers app installed using msi generated advanced installer vs community,fix system data sqlite found deploying visual studio app advanced installer,fix system data sqlite found deploying visual studio app advanced installercode error enough tell share info want fix order capable using sqlite bit machines creating specific installers app installed using msi generated advanced installer vs community,"['fix', 'system', 'data', 'sqlite', 'found', 'deploying', 'visual', 'studio', 'app', 'advanced', 'installercode', 'error', 'enough', 'tell', 'share', 'info', 'want', 'fix', 'order', 'capable', 'using', 'sqlite', 'bit', 'machines', 'creating', 'specific', 'installers', 'app', 'installed', 'using', 'msi', 'generated', 'advanced', 'installer', 'vs', 'community']","['fix', 'system', 'data', 'sqlite', 'found', 'deploy', 'visual', 'studio', 'app', 'advanc', 'installercod', 'error', 'enough', 'tell', 'share', 'info', 'want', 'fix', 'order', 'capabl', 'use', 'sqlite', 'bit', 'machin', 'creat', 'specif', 'instal', 'app', 'instal', 'use', 'msi', 'gener', 'advanc', 'instal', 'vs', 'commun']"
128,140,140,18089331,72914328,Compare similarity of two names and identify duplicates with neural network,"<p>I have a dataset which contains pairs of names, it looks like this:</p>
<pre><code>ID; name1; name2
1; Mike Miller; Mike Miler
2; John Doe; Pete McGillen
3; Sara Johnson; Edita Johnson
4; John Lemond-Lee Peter; John LL. Peter
5; Marta Sunz; Martha Sund
6; John Peter; Johanna Petera
7; Joanna Nemzik; Joanna Niemczik
</code></pre>
<p>I have some cases, which are labelled. So I check them manually and decide if these are duplicates or not. The manual judgement in these cases would be:</p>
<pre><code>1: Is a duplicate
2: Is not a duplicate
3: Is not a duplicate
4: Is a duplicate
5: Is not a duplicate
6: Is not a duplicate
7: Is a duplicate
</code></pre>
<p>(The 7th case is a specific case, because here phonetics come into the game too. However, this is not the main problem, I am ok with ignoring phonetics.)</p>
<p>A first approach would be to calculate the Levenshtein-distance for each pair and mark those as a duplicate, where the Levenshtein-distance is for example less or equal than 2. This would lead to the following output:</p>
<pre><code>1: Levenshtein distance: 2 =&gt; duplicate
2: Levenshtein distance: 11 =&gt; not a duplicate
3: Levenshtein distance: 4 =&gt; not a duplicate
4: Levenshtein distance: 8 =&gt; not a duplicate
5: Levenshtein distance: 2 =&gt; duplicate
6: Levenshtein distance: 4 =&gt; not a duplicate
7: Levenshtein distance: 2 =&gt; duplicate
</code></pre>
<p>This would be an approach which uses a &quot;fixed&quot; algorithm based on the Levinshtein distance.</p>
<p>Now, I would like to do this task with using a neural network / machine learning:</p>
<p>I do not need the neural network to detect semantic similarity, like &quot;hospital&quot; and &quot;clininc&quot;. However, I would like to avoid the Levenshtein-distance, as I would like the ML algorithm to be able to detect &quot;John Lemond-Lee Peter&quot; and &quot;John LL. Peter&quot; as a potential duplicate, also not with a 100% certainty. The Levenshtein distance would lead to a relative high number in this case (8), as there are quite some characters to be added. In a case like &quot;John Peter&quot; and &quot;Johanna Petera&quot; the Levenshtein-distance would lead to a smaller number (4), however this is in fact no duplicate and for this case I would hope that the ML algorithm would be able to detect that this is likely not a duplicate. So I need the ML algorithm to &quot;learn the way I need the duplicates to be checked&quot;. With my labelling I would give as an input I would give the ML algorithm the direction, of what I want.</p>
<p>I actually thought that this should be an easy task for a ML algorithm / neural network, but I am not sure.</p>
<p><strong>How can I implement a neural network to compare the pairs of names and identify duplicates without using an explicit distance metric (like the Levenshtein distance, euclidean etc.)?</strong></p>
<p>I thought that it would be possible to convert the strings to numbers and a neural network can work with this and learn to detect duplicates according to my labelling style. So without having to specify a distance metric. <strong>I thought about an human: I would give this task to a person and this person would judge and make a decision. This person has no clue about a Levenshtein-distance or any other mathematical concept. So I just want to train the neural network to learn to do what the human is doing.</strong> Of course, every human is different and it also depends on my labelling.</p>
<p>(<em>Edit</em>: The ML/neural network solutions I have seen so far (like <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">this</a>) use a metric like levenshtein as a feature input. But as I said I thought it should be possible to teach the neural network the &quot;human judgement&quot; without making use of such a distance measure? Regarding my specific case with having pairs of names: What would the benefit be a of a ML approach using levenshtein distance as a feature? Because it will just detect those pairs of names as a duplicate that have a low levenshtein distance. So I could use a simple algorithm to mark a pair as duplicate if the levenshtein distance between the two names is less than x. Why use a ML instead, what would be the additional benefit?)</p>
",163,4,3,5,python;tensorflow;machine-learning;neural-network;levenshtein-distance,2022-07-08 19:25:04,2022-07-08 19:25:04,2022-07-21 18:38:56,i have a dataset which contains pairs of names  it looks like this  i have some cases  which are labelled  so i check them manually and decide if these are duplicates or not  the manual judgement in these cases would be   the th case is a specific case  because here phonetics come into the game too  however  this is not the main problem  i am ok with ignoring phonetics   a first approach would be to calculate the levenshtein distance for each pair and mark those as a duplicate  where the levenshtein distance is for example less or equal than   this would lead to the following output  this would be an approach which uses a  fixed  algorithm based on the levinshtein distance  now  i would like to do this task with using a neural network   machine learning  i do not need the neural network to detect semantic similarity  like  hospital  and  clininc   however  i would like to avoid the levenshtein distance  as i would like the ml algorithm to be able to detect  john lemond lee peter  and  john ll  peter  as a potential duplicate  also not with a   certainty  the levenshtein distance would lead to a relative high number in this case     as there are quite some characters to be added  in a case like  john peter  and  johanna petera  the levenshtein distance would lead to a smaller number     however this is in fact no duplicate and for this case i would hope that the ml algorithm would be able to detect that this is likely not a duplicate  so i need the ml algorithm to  learn the way i need the duplicates to be checked   with my labelling i would give as an input i would give the ml algorithm the direction  of what i want  i actually thought that this should be an easy task for a ml algorithm   neural network  but i am not sure  how can i implement a neural network to compare the pairs of names and identify duplicates without using an explicit distance metric  like the levenshtein distance  euclidean etc    i thought that it would be possible to convert the strings to numbers and a neural network can work with this and learn to detect duplicates according to my labelling style  so without having to specify a distance metric  i thought about an human  i would give this task to a person and this person would judge and make a decision  this person has no clue about a levenshtein distance or any other mathematical concept  so i just want to train the neural network to learn to do what the human is doing  of course  every human is different and it also depends on my labelling   edit  the ml neural network solutions i have seen so far  like   use a metric like levenshtein as a feature input  but as i said i thought it should be possible to teach the neural network the  human judgement  without making use of such a distance measure  regarding my specific case with having pairs of names  what would the benefit be a of a ml approach using levenshtein distance as a feature  because it will just detect those pairs of names as a duplicate that have a low levenshtein distance  so i could use a simple algorithm to mark a pair as duplicate if the levenshtein distance between the two names is less than x  why use a ml instead  what would be the additional benefit  ,compare similarity of two names and identify duplicates with neural network,dataset contains pairs names looks like cases labelled check manually decide duplicates manual judgement cases would th case specific case phonetics come game however main problem ok ignoring phonetics first approach would calculate levenshtein distance pair mark duplicate levenshtein distance example less equal would lead following output would approach uses fixed algorithm based levinshtein distance would like task using neural network machine learning need neural network detect semantic similarity like hospital clininc however would like avoid levenshtein distance would like ml algorithm able detect john lemond lee peter john peter potential duplicate also certainty levenshtein distance would lead relative high number case quite characters added case like john peter johanna petera levenshtein distance would lead smaller number however fact duplicate case would hope ml algorithm would able detect likely duplicate need ml algorithm learn way need duplicates checked labelling would give input would give ml algorithm direction want actually thought easy task ml algorithm neural network sure implement neural network compare pairs names identify duplicates without using explicit distance metric like levenshtein distance euclidean etc thought would possible convert strings numbers neural network work learn detect duplicates according labelling style without specify distance metric thought human would give task person person would judge make decision person clue levenshtein distance mathematical concept want train neural network learn human course every human different also depends labelling edit ml neural network solutions seen far like use metric like levenshtein feature input said thought possible teach neural network human judgement without making use distance measure regarding specific case pairs names would benefit ml approach using levenshtein distance feature detect pairs names duplicate low levenshtein distance could use simple algorithm mark pair duplicate levenshtein distance two names less x use ml instead would additional benefit,compare similarity two names identify duplicates neural network,compare similarity two names identify duplicates neural networkdataset contains pairs names looks like cases labelled check manually decide duplicates manual judgement cases would th case specific case phonetics come game however main problem ok ignoring phonetics first approach would calculate levenshtein distance pair mark duplicate levenshtein distance example less equal would lead following output would approach uses fixed algorithm based levinshtein distance would like task using neural network machine learning need neural network detect semantic similarity like hospital clininc however would like avoid levenshtein distance would like ml algorithm able detect john lemond lee peter john peter potential duplicate also certainty levenshtein distance would lead relative high number case quite characters added case like john peter johanna petera levenshtein distance would lead smaller number however fact duplicate case would hope ml algorithm would able detect likely duplicate need ml algorithm learn way need duplicates checked labelling would give input would give ml algorithm direction want actually thought easy task ml algorithm neural network sure implement neural network compare pairs names identify duplicates without using explicit distance metric like levenshtein distance euclidean etc thought would possible convert strings numbers neural network work learn detect duplicates according labelling style without specify distance metric thought human would give task person person would judge make decision person clue levenshtein distance mathematical concept want train neural network learn human course every human different also depends labelling edit ml neural network solutions seen far like use metric like levenshtein feature input said thought possible teach neural network human judgement without making use distance measure regarding specific case pairs names would benefit ml approach using levenshtein distance feature detect pairs names duplicate low levenshtein distance could use simple algorithm mark pair duplicate levenshtein distance two names less x use ml instead would additional benefit,"['compare', 'similarity', 'two', 'names', 'identify', 'duplicates', 'neural', 'networkdataset', 'contains', 'pairs', 'names', 'looks', 'like', 'cases', 'labelled', 'check', 'manually', 'decide', 'duplicates', 'manual', 'judgement', 'cases', 'would', 'th', 'case', 'specific', 'case', 'phonetics', 'come', 'game', 'however', 'main', 'problem', 'ok', 'ignoring', 'phonetics', 'first', 'approach', 'would', 'calculate', 'levenshtein', 'distance', 'pair', 'mark', 'duplicate', 'levenshtein', 'distance', 'example', 'less', 'equal', 'would', 'lead', 'following', 'output', 'would', 'approach', 'uses', 'fixed', 'algorithm', 'based', 'levinshtein', 'distance', 'would', 'like', 'task', 'using', 'neural', 'network', 'machine', 'learning', 'need', 'neural', 'network', 'detect', 'semantic', 'similarity', 'like', 'hospital', 'clininc', 'however', 'would', 'like', 'avoid', 'levenshtein', 'distance', 'would', 'like', 'ml', 'algorithm', 'able', 'detect', 'john', 'lemond', 'lee', 'peter', 'john', 'peter', 'potential', 'duplicate', 'also', 'certainty', 'levenshtein', 'distance', 'would', 'lead', 'relative', 'high', 'number', 'case', 'quite', 'characters', 'added', 'case', 'like', 'john', 'peter', 'johanna', 'petera', 'levenshtein', 'distance', 'would', 'lead', 'smaller', 'number', 'however', 'fact', 'duplicate', 'case', 'would', 'hope', 'ml', 'algorithm', 'would', 'able', 'detect', 'likely', 'duplicate', 'need', 'ml', 'algorithm', 'learn', 'way', 'need', 'duplicates', 'checked', 'labelling', 'would', 'give', 'input', 'would', 'give', 'ml', 'algorithm', 'direction', 'want', 'actually', 'thought', 'easy', 'task', 'ml', 'algorithm', 'neural', 'network', 'sure', 'implement', 'neural', 'network', 'compare', 'pairs', 'names', 'identify', 'duplicates', 'without', 'using', 'explicit', 'distance', 'metric', 'like', 'levenshtein', 'distance', 'euclidean', 'etc', 'thought', 'would', 'possible', 'convert', 'strings', 'numbers', 'neural', 'network', 'work', 'learn', 'detect', 'duplicates', 'according', 'labelling', 'style', 'without', 'specify', 'distance', 'metric', 'thought', 'human', 'would', 'give', 'task', 'person', 'person', 'would', 'judge', 'make', 'decision', 'person', 'clue', 'levenshtein', 'distance', 'mathematical', 'concept', 'want', 'train', 'neural', 'network', 'learn', 'human', 'course', 'every', 'human', 'different', 'also', 'depends', 'labelling', 'edit', 'ml', 'neural', 'network', 'solutions', 'seen', 'far', 'like', 'use', 'metric', 'like', 'levenshtein', 'feature', 'input', 'said', 'thought', 'possible', 'teach', 'neural', 'network', 'human', 'judgement', 'without', 'making', 'use', 'distance', 'measure', 'regarding', 'specific', 'case', 'pairs', 'names', 'would', 'benefit', 'ml', 'approach', 'using', 'levenshtein', 'distance', 'feature', 'detect', 'pairs', 'names', 'duplicate', 'low', 'levenshtein', 'distance', 'could', 'use', 'simple', 'algorithm', 'mark', 'pair', 'duplicate', 'levenshtein', 'distance', 'two', 'names', 'less', 'x', 'use', 'ml', 'instead', 'would', 'additional', 'benefit']","['compar', 'similar', 'two', 'name', 'identifi', 'duplic', 'neural', 'networkdataset', 'contain', 'pair', 'name', 'look', 'like', 'case', 'label', 'check', 'manual', 'decid', 'duplic', 'manual', 'judgement', 'case', 'would', 'th', 'case', 'specif', 'case', 'phonet', 'come', 'game', 'howev', 'main', 'problem', 'ok', 'ignor', 'phonet', 'first', 'approach', 'would', 'calcul', 'levenshtein', 'distanc', 'pair', 'mark', 'duplic', 'levenshtein', 'distanc', 'exampl', 'less', 'equal', 'would', 'lead', 'follow', 'output', 'would', 'approach', 'use', 'fix', 'algorithm', 'base', 'levinshtein', 'distanc', 'would', 'like', 'task', 'use', 'neural', 'network', 'machin', 'learn', 'need', 'neural', 'network', 'detect', 'semant', 'similar', 'like', 'hospit', 'clininc', 'howev', 'would', 'like', 'avoid', 'levenshtein', 'distanc', 'would', 'like', 'ml', 'algorithm', 'abl', 'detect', 'john', 'lemond', 'lee', 'peter', 'john', 'peter', 'potenti', 'duplic', 'also', 'certainti', 'levenshtein', 'distanc', 'would', 'lead', 'rel', 'high', 'number', 'case', 'quit', 'charact', 'ad', 'case', 'like', 'john', 'peter', 'johanna', 'petera', 'levenshtein', 'distanc', 'would', 'lead', 'smaller', 'number', 'howev', 'fact', 'duplic', 'case', 'would', 'hope', 'ml', 'algorithm', 'would', 'abl', 'detect', 'like', 'duplic', 'need', 'ml', 'algorithm', 'learn', 'way', 'need', 'duplic', 'check', 'label', 'would', 'give', 'input', 'would', 'give', 'ml', 'algorithm', 'direct', 'want', 'actual', 'thought', 'easi', 'task', 'ml', 'algorithm', 'neural', 'network', 'sure', 'implement', 'neural', 'network', 'compar', 'pair', 'name', 'identifi', 'duplic', 'without', 'use', 'explicit', 'distanc', 'metric', 'like', 'levenshtein', 'distanc', 'euclidean', 'etc', 'thought', 'would', 'possibl', 'convert', 'string', 'number', 'neural', 'network', 'work', 'learn', 'detect', 'duplic', 'accord', 'label', 'style', 'without', 'specifi', 'distanc', 'metric', 'thought', 'human', 'would', 'give', 'task', 'person', 'person', 'would', 'judg', 'make', 'decis', 'person', 'clue', 'levenshtein', 'distanc', 'mathemat', 'concept', 'want', 'train', 'neural', 'network', 'learn', 'human', 'cours', 'everi', 'human', 'differ', 'also', 'depend', 'label', 'edit', 'ml', 'neural', 'network', 'solut', 'seen', 'far', 'like', 'use', 'metric', 'like', 'levenshtein', 'featur', 'input', 'said', 'thought', 'possibl', 'teach', 'neural', 'network', 'human', 'judgement', 'without', 'make', 'use', 'distanc', 'measur', 'regard', 'specif', 'case', 'pair', 'name', 'would', 'benefit', 'ml', 'approach', 'use', 'levenshtein', 'distanc', 'featur', 'detect', 'pair', 'name', 'duplic', 'low', 'levenshtein', 'distanc', 'could', 'use', 'simpl', 'algorithm', 'mark', 'pair', 'duplic', 'levenshtein', 'distanc', 'two', 'name', 'less', 'x', 'use', 'ml', 'instead', 'would', 'addit', 'benefit']"
129,141,141,978769,23450534,How to call a Python function from Node.js,"<p>I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?</p>
",266096,11,309,3,python;node.js;express,2014-05-04 01:35:13,2014-05-04 01:35:13,2022-07-21 18:31:24,i have an express node js application  but i also have a machine learning algorithm to use in python  is there a way i can call python functions from my node js application to make use of the power of machine learning libraries ,how to call a python function from node js,express node js application also machine learning algorithm use python way call python functions node js application make use power machine learning libraries,call python function node js,call python function node jsexpress node js application also machine learning algorithm use python way call python functions node js application make use power machine learning libraries,"['call', 'python', 'function', 'node', 'jsexpress', 'node', 'js', 'application', 'also', 'machine', 'learning', 'algorithm', 'use', 'python', 'way', 'call', 'python', 'functions', 'node', 'js', 'application', 'make', 'use', 'power', 'machine', 'learning', 'libraries']","['call', 'python', 'function', 'node', 'jsexpress', 'node', 'js', 'applic', 'also', 'machin', 'learn', 'algorithm', 'use', 'python', 'way', 'call', 'python', 'function', 'node', 'js', 'applic', 'make', 'use', 'power', 'machin', 'learn', 'librari']"
130,142,142,9885747,70187921,How can I register a specific version of a Delta Table in Azure Machine Learning Studio from Azure ADLS Gen 1?,"<p>I created a Delta Table in ADLS Gen 1 with the following code in Databricks:</p>
<pre><code>df.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;db.my_tbl&quot;, path ='adl://organisation.azuredatalakestore.net/folder_name/my_data')
</code></pre>
<p>Sometimes, I re-run the code above to generate a new version of the <code>my_tbl</code> table. As usual with delta tables, a history is build and it must regulary be optimized and vaccumed. Now, I am often retraining a ML Model in Azure Machine Learning Studio and am wondering if it possible to register a specific version of the delta table?</p>
<p>Currently, even after vaccuming, all my delta files (including older versions) are registered in Azure ML Studio when reading the parquet files from <code>my_data</code> folder! That is because I can not lower the retention period of the delta table below 168h except turning of <code>spark.databricks.delta.retentionDurationCheck.enabled</code>. I do not want to turn it off.</p>
<p>I register my dataset through the ML Studio Interface as a File Dataset (not Tabular Dataset). This registration looks like this:</p>
<p><a href=""https://i.stack.imgur.com/iVd0U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iVd0U.png"" alt=""enter image description here"" /></a></p>
<p>Right now I see only the option to create a copy of <code>my_data</code> and reading it instead. Is there another way? Do you know if I can specify something in the path to point to the &quot;right&quot; .parquet files (belonging to a specific delta table version)?</p>
",381,1,4,5,python;azure;apache-spark;registration;retention,2021-12-01 18:34:20,2021-12-01 18:34:20,2022-07-21 16:39:04,i created a delta table in adls gen  with the following code in databricks  sometimes  i re run the code above to generate a new version of the my_tbl table  as usual with delta tables  a history is build and it must regulary be optimized and vaccumed  now  i am often retraining a ml model in azure machine learning studio and am wondering if it possible to register a specific version of the delta table  currently  even after vaccuming  all my delta files  including older versions  are registered in azure ml studio when reading the parquet files from my_data folder  that is because i can not lower the retention period of the delta table below h except turning of spark databricks delta retentiondurationcheck enabled  i do not want to turn it off  i register my dataset through the ml studio interface as a file dataset  not tabular dataset   this registration looks like this   right now i see only the option to create a copy of my_data and reading it instead  is there another way  do you know if i can specify something in the path to point to the  right   parquet files  belonging to a specific delta table version  ,how can i register a specific version of a delta table in azure machine learning studio from azure adls gen  ,created delta table adls gen following code databricks sometimes run code generate version my_tbl table usual delta tables history build must regulary optimized vaccumed often retraining ml model azure machine learning studio wondering possible register specific version delta table currently even vaccuming delta files including older versions registered azure ml studio reading parquet files my_data folder lower retention period delta table h except turning spark databricks delta retentiondurationcheck enabled want turn register dataset ml studio interface file dataset tabular dataset registration looks like right see option create copy my_data reading instead another way know specify something path point right parquet files belonging specific delta table version,register specific version delta table azure machine learning studio azure adls gen,register specific version delta table azure machine learning studio azure adls gencreated delta table adls gen following code databricks sometimes run code generate version my_tbl table usual delta tables history build must regulary optimized vaccumed often retraining ml model azure machine learning studio wondering possible register specific version delta table currently even vaccuming delta files including older versions registered azure ml studio reading parquet files my_data folder lower retention period delta table h except turning spark databricks delta retentiondurationcheck enabled want turn register dataset ml studio interface file dataset tabular dataset registration looks like right see option create copy my_data reading instead another way know specify something path point right parquet files belonging specific delta table version,"['register', 'specific', 'version', 'delta', 'table', 'azure', 'machine', 'learning', 'studio', 'azure', 'adls', 'gencreated', 'delta', 'table', 'adls', 'gen', 'following', 'code', 'databricks', 'sometimes', 'run', 'code', 'generate', 'version', 'my_tbl', 'table', 'usual', 'delta', 'tables', 'history', 'build', 'must', 'regulary', 'optimized', 'vaccumed', 'often', 'retraining', 'ml', 'model', 'azure', 'machine', 'learning', 'studio', 'wondering', 'possible', 'register', 'specific', 'version', 'delta', 'table', 'currently', 'even', 'vaccuming', 'delta', 'files', 'including', 'older', 'versions', 'registered', 'azure', 'ml', 'studio', 'reading', 'parquet', 'files', 'my_data', 'folder', 'lower', 'retention', 'period', 'delta', 'table', 'h', 'except', 'turning', 'spark', 'databricks', 'delta', 'retentiondurationcheck', 'enabled', 'want', 'turn', 'register', 'dataset', 'ml', 'studio', 'interface', 'file', 'dataset', 'tabular', 'dataset', 'registration', 'looks', 'like', 'right', 'see', 'option', 'create', 'copy', 'my_data', 'reading', 'instead', 'another', 'way', 'know', 'specify', 'something', 'path', 'point', 'right', 'parquet', 'files', 'belonging', 'specific', 'delta', 'table', 'version']","['regist', 'specif', 'version', 'delta', 'tabl', 'azur', 'machin', 'learn', 'studio', 'azur', 'adl', 'gencreat', 'delta', 'tabl', 'adl', 'gen', 'follow', 'code', 'databrick', 'sometim', 'run', 'code', 'gener', 'version', 'my_tbl', 'tabl', 'usual', 'delta', 'tabl', 'histori', 'build', 'must', 'regulari', 'optim', 'vaccum', 'often', 'retrain', 'ml', 'model', 'azur', 'machin', 'learn', 'studio', 'wonder', 'possibl', 'regist', 'specif', 'version', 'delta', 'tabl', 'current', 'even', 'vaccum', 'delta', 'file', 'includ', 'older', 'version', 'regist', 'azur', 'ml', 'studio', 'read', 'parquet', 'file', 'my_data', 'folder', 'lower', 'retent', 'period', 'delta', 'tabl', 'h', 'except', 'turn', 'spark', 'databrick', 'delta', 'retentiondurationcheck', 'enabl', 'want', 'turn', 'regist', 'dataset', 'ml', 'studio', 'interfac', 'file', 'dataset', 'tabular', 'dataset', 'registr', 'look', 'like', 'right', 'see', 'option', 'creat', 'copi', 'my_data', 'read', 'instead', 'anoth', 'way', 'know', 'specifi', 'someth', 'path', 'point', 'right', 'parquet', 'file', 'belong', 'specif', 'delta', 'tabl', 'version']"
131,143,143,5413413,73065642,Is there a list of all embedding techniques used in all applications of Machine learning?,"<p>I started to learn embedding techniques used in machine learning and allied fields. There are word embeddings, graph embeddings, and network embeddings. I was overwhelmed by the various embedding techniques as I googled. Is there a good repository or book or resource that starts with the need for embedding and slowly moves onto different advanced embedding techniques in NLP, Computer Vision, Knowledge Graphs etc.</p>
<p>I need an organic development of the subject so that I can think in different directions and to b able to come up with my own techniques in the embedding space.</p>
",15,0,0,5,nlp;computer-vision;embedding;word-embedding;knowledge-graph,2022-07-21 14:48:45,2022-07-21 14:48:45,2022-07-21 14:48:45,i started to learn embedding techniques used in machine learning and allied fields  there are word embeddings  graph embeddings  and network embeddings  i was overwhelmed by the various embedding techniques as i googled  is there a good repository or book or resource that starts with the need for embedding and slowly moves onto different advanced embedding techniques in nlp  computer vision  knowledge graphs etc  i need an organic development of the subject so that i can think in different directions and to b able to come up with my own techniques in the embedding space ,is there a list of all embedding techniques used in all applications of machine learning ,started learn embedding techniques used machine learning allied fields word embeddings graph embeddings network embeddings overwhelmed various embedding techniques googled good repository book resource starts need embedding slowly moves onto different advanced embedding techniques nlp computer vision knowledge graphs etc need organic development subject think different directions b able come techniques embedding space,embedding techniques used applications machine learning,embedding techniques used applications machine learningstarted learn embedding techniques used machine learning allied fields word embeddings graph embeddings network embeddings overwhelmed various embedding techniques googled good repository book resource starts need embedding slowly moves onto different advanced embedding techniques nlp computer vision knowledge graphs etc need organic development subject think different directions b able come techniques embedding space,"['embedding', 'techniques', 'used', 'applications', 'machine', 'learningstarted', 'learn', 'embedding', 'techniques', 'used', 'machine', 'learning', 'allied', 'fields', 'word', 'embeddings', 'graph', 'embeddings', 'network', 'embeddings', 'overwhelmed', 'various', 'embedding', 'techniques', 'googled', 'good', 'repository', 'book', 'resource', 'starts', 'need', 'embedding', 'slowly', 'moves', 'onto', 'different', 'advanced', 'embedding', 'techniques', 'nlp', 'computer', 'vision', 'knowledge', 'graphs', 'etc', 'need', 'organic', 'development', 'subject', 'think', 'different', 'directions', 'b', 'able', 'come', 'techniques', 'embedding', 'space']","['embed', 'techniqu', 'use', 'applic', 'machin', 'learningstart', 'learn', 'embed', 'techniqu', 'use', 'machin', 'learn', 'alli', 'field', 'word', 'embed', 'graph', 'embed', 'network', 'embed', 'overwhelm', 'variou', 'embed', 'techniqu', 'googl', 'good', 'repositori', 'book', 'resourc', 'start', 'need', 'embed', 'slowli', 'move', 'onto', 'differ', 'advanc', 'embed', 'techniqu', 'nlp', 'comput', 'vision', 'knowledg', 'graph', 'etc', 'need', 'organ', 'develop', 'subject', 'think', 'differ', 'direct', 'b', 'abl', 'come', 'techniqu', 'embed', 'space']"
132,144,144,15440045,73049155,sklearn.model_selection.cross_val_score has different results from a manual calculation done on a confusion matrix,"<p>TL;DR When I calculate precision, recall, and f1 through CV <code>cross_val_score()</code>, it gives me different results than when I calculate through the confusion matrix. Why does it give different precision, recall, and f1 scores?</p>
<p>I'm learning SVM in machine learning and I wanted to compare the result returned by <code>cross_val_score</code> and the result I get from manually calculating the metrics from the confusion matrix. However, I have different result.</p>
<p>To start, I have written the code below using <code>cross_val_score</code>.</p>
<pre class=""lang-py prettyprint-override""><code>clf = svm.SVC()
kfold = KFold(n_splits = 10)

accuracy = metrics.make_scorer(metrics.accuracy_score)
precision = metrics.make_scorer(metrics.precision_score, average = 'macro')
recall = metrics.make_scorer(metrics.recall_score, average = 'macro')
f1 = metrics.make_scorer(metrics.f1_score, average = 'macro')

accuracy_score = cross_val_score(clf, X, y, scoring = accuracy, cv = kfold)
precision_score = cross_val_score(clf, X, y, scoring = precision, cv = kfold)
recall_score = cross_val_score(clf, X, y, scoring = recall, cv = kfold)
f1_score = cross_val_score(clf, X, y, scoring = f1, cv = kfold)

print(&quot;accuracy score:&quot;, accuracy_score.mean())
print(&quot;precision score:&quot;, precision_score.mean())
print(&quot;recall score:&quot;,recall_score.mean())
print(&quot;f1 score:&quot;, f1_score.mean())
</code></pre>
<p>The result for each metric is shown below:</p>
<pre class=""lang-py prettyprint-override""><code>accuracy score: 0.97
precision score: 0.96
recall score: 0.97
f1 score: 0.96
</code></pre>
<p>In addition, I created a Confusion Matrix so that I can manually calculate the accuracy, precision, recall, and f1 score based on the values on the matrix. I manually created the Confusion Matrix because I am using K-Fold Cross Validation. To do that, I have to get the actual classes and predicted classes for each iteration of the Cross Validation and so I have this code:</p>
<pre class=""lang-py prettyprint-override""><code>def cross_val_predict(model, kfold : KFold, X : np.array, y : np.array) -&gt; Tuple[np.array, np.array]:
    
    model_ = cp.deepcopy(model)
    
    # gets the number of classes in the column/attribute
    no_of_classes = len(np.unique(y))
    
    # initializing empty numpy arrays to be returned
    actual_classes = np.empty([0], dtype = int)
    predicted_classes = np.empty([0], dtype = int)

    for train_index, test_index in kfold.split(X):

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        # append the actual classes for this iteration
        actual_classes = np.append(actual_classes, y_test)
        
        # fit the model
        model_.fit(X_train, y_train)
        
        # predict
        predicted_classes = np.append(predicted_classes, model_.predict(X_test))
        
    return actual_classes, predicted_classes

</code></pre>
<p>Afterwards, I created my confusion matrix after calling the above function.</p>
<pre class=""lang-py prettyprint-override""><code>actual_classes, predicted_classes = cross_val_predict(clf, kfold, X, y)
cm = metrics.confusion_matrix(y_true = actual_classes, y_pred = predicted_classes)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [2,4])
cm_display.plot()
</code></pre>
<p>Now, my confusion matrix looks like the below:</p>
<pre><code>where: col is the predicted label, and row is the true label.

    |------|------|
 2  | 431  |  13  |
    |------|------|
 4  |  9   | 230  |
    |------|------|
       2      4
</code></pre>
<p>If I manually calcuate the accuracy, precision, recall, and f1 score from that matrix, I have the ff:</p>
<pre><code>confusion matrix accuracy: 0.97
confusion matrix precision: 0.95
confusion matrix recall: 0.96
confusion matrix f1 score: 0.95
</code></pre>
<p>My question is that why did I get different result from manually calculating the metrics from the confusion matrix and the result from calling <code>cross_val_score</code> while specifying which scorer to use, i.e., [accuracy, precision, recall, fscore].</p>
<p>I hope you guys can help me understand why. Thank you very much for your responses!</p>
",72,1,2,5,python;scikit-learn;cross-validation;confusion-matrix;k-fold,2022-07-20 12:32:16,2022-07-20 12:32:16,2022-07-21 14:21:45,tl dr when i calculate precision  recall  and f through cv cross_val_score    it gives me different results than when i calculate through the confusion matrix  why does it give different precision  recall  and f scores  i m learning svm in machine learning and i wanted to compare the result returned by cross_val_score and the result i get from manually calculating the metrics from the confusion matrix  however  i have different result  to start  i have written the code below using cross_val_score  the result for each metric is shown below  in addition  i created a confusion matrix so that i can manually calculate the accuracy  precision  recall  and f score based on the values on the matrix  i manually created the confusion matrix because i am using k fold cross validation  to do that  i have to get the actual classes and predicted classes for each iteration of the cross validation and so i have this code  afterwards  i created my confusion matrix after calling the above function  now  my confusion matrix looks like the below  if i manually calcuate the accuracy  precision  recall  and f score from that matrix  i have the ff  my question is that why did i get different result from manually calculating the metrics from the confusion matrix and the result from calling cross_val_score while specifying which scorer to use  i e    accuracy  precision  recall  fscore   i hope you guys can help me understand why  thank you very much for your responses ,sklearn model_selection cross_val_score has different results from a manual calculation done on a confusion matrix,tl dr calculate precision recall f cv cross_val_score gives different results calculate confusion matrix give different precision recall f scores learning svm machine learning wanted compare result returned cross_val_score result get manually calculating metrics confusion matrix however different result start written code using cross_val_score result metric shown addition created confusion matrix manually calculate accuracy precision recall f score based values matrix manually created confusion matrix using k fold cross validation get actual classes predicted classes iteration cross validation code afterwards created confusion matrix calling function confusion matrix looks like manually calcuate accuracy precision recall f score matrix ff question get different result manually calculating metrics confusion matrix result calling cross_val_score specifying scorer use e accuracy precision recall fscore hope guys help understand thank much responses,sklearn model_selection cross_val_score different results manual calculation done confusion matrix,sklearn model_selection cross_val_score different results manual calculation done confusion matrixtl dr calculate precision recall f cv cross_val_score gives different results calculate confusion matrix give different precision recall f scores learning svm machine learning wanted compare result returned cross_val_score result get manually calculating metrics confusion matrix however different result start written code using cross_val_score result metric shown addition created confusion matrix manually calculate accuracy precision recall f score based values matrix manually created confusion matrix using k fold cross validation get actual classes predicted classes iteration cross validation code afterwards created confusion matrix calling function confusion matrix looks like manually calcuate accuracy precision recall f score matrix ff question get different result manually calculating metrics confusion matrix result calling cross_val_score specifying scorer use e accuracy precision recall fscore hope guys help understand thank much responses,"['sklearn', 'model_selection', 'cross_val_score', 'different', 'results', 'manual', 'calculation', 'done', 'confusion', 'matrixtl', 'dr', 'calculate', 'precision', 'recall', 'f', 'cv', 'cross_val_score', 'gives', 'different', 'results', 'calculate', 'confusion', 'matrix', 'give', 'different', 'precision', 'recall', 'f', 'scores', 'learning', 'svm', 'machine', 'learning', 'wanted', 'compare', 'result', 'returned', 'cross_val_score', 'result', 'get', 'manually', 'calculating', 'metrics', 'confusion', 'matrix', 'however', 'different', 'result', 'start', 'written', 'code', 'using', 'cross_val_score', 'result', 'metric', 'shown', 'addition', 'created', 'confusion', 'matrix', 'manually', 'calculate', 'accuracy', 'precision', 'recall', 'f', 'score', 'based', 'values', 'matrix', 'manually', 'created', 'confusion', 'matrix', 'using', 'k', 'fold', 'cross', 'validation', 'get', 'actual', 'classes', 'predicted', 'classes', 'iteration', 'cross', 'validation', 'code', 'afterwards', 'created', 'confusion', 'matrix', 'calling', 'function', 'confusion', 'matrix', 'looks', 'like', 'manually', 'calcuate', 'accuracy', 'precision', 'recall', 'f', 'score', 'matrix', 'ff', 'question', 'get', 'different', 'result', 'manually', 'calculating', 'metrics', 'confusion', 'matrix', 'result', 'calling', 'cross_val_score', 'specifying', 'scorer', 'use', 'e', 'accuracy', 'precision', 'recall', 'fscore', 'hope', 'guys', 'help', 'understand', 'thank', 'much', 'responses']","['sklearn', 'model_select', 'cross_val_scor', 'differ', 'result', 'manual', 'calcul', 'done', 'confus', 'matrixtl', 'dr', 'calcul', 'precis', 'recal', 'f', 'cv', 'cross_val_scor', 'give', 'differ', 'result', 'calcul', 'confus', 'matrix', 'give', 'differ', 'precis', 'recal', 'f', 'score', 'learn', 'svm', 'machin', 'learn', 'want', 'compar', 'result', 'return', 'cross_val_scor', 'result', 'get', 'manual', 'calcul', 'metric', 'confus', 'matrix', 'howev', 'differ', 'result', 'start', 'written', 'code', 'use', 'cross_val_scor', 'result', 'metric', 'shown', 'addit', 'creat', 'confus', 'matrix', 'manual', 'calcul', 'accuraci', 'precis', 'recal', 'f', 'score', 'base', 'valu', 'matrix', 'manual', 'creat', 'confus', 'matrix', 'use', 'k', 'fold', 'cross', 'valid', 'get', 'actual', 'class', 'predict', 'class', 'iter', 'cross', 'valid', 'code', 'afterward', 'creat', 'confus', 'matrix', 'call', 'function', 'confus', 'matrix', 'look', 'like', 'manual', 'calcuat', 'accuraci', 'precis', 'recal', 'f', 'score', 'matrix', 'ff', 'question', 'get', 'differ', 'result', 'manual', 'calcul', 'metric', 'confus', 'matrix', 'result', 'call', 'cross_val_scor', 'specifi', 'scorer', 'use', 'e', 'accuraci', 'precis', 'recal', 'fscore', 'hope', 'guy', 'help', 'understand', 'thank', 'much', 'respons']"
133,145,145,19017538,72090829,"Using XState, how can I access name of current state in an action?","<p>I'm playing around learning XState and wanted to include an action in a machine that would just log the current state to console.</p>
<p>Defining a simple example machine like so, how would I go about this? Also note the questions in the comments in the code.</p>
<pre><code>import { createMachine, interpret } from &quot;xstate&quot;

const sm = createMachine({
    initial: 'foo',
    states: {
        foo: {
            entry: 'logState', // Can I only reference an action by string?
                               // Or can I add arguments here somehow?
            on: {
                TOGGLE: {target: 'bar'}
            }
        },
        bar: {
            entry: 'logState',
            on: {
                TOGGLE: {target: 'foo'}
            }
        }
    }
},
{
    actions: {
        logState(/* What arguments can go here? */) =&gt; {
            // What do I do here?
        }
    }
});

</code></pre>
<p>I know that actions are called with <code>context</code> and <code>event</code> as arguments but I don't see a way to get the current state from either of those. Am I missing something here?</p>
",142,2,1,2,javascript;xstate,2022-05-02 21:23:44,2022-05-02 21:23:44,2022-07-21 12:50:24,i m playing around learning xstate and wanted to include an action in a machine that would just log the current state to console  defining a simple example machine like so  how would i go about this  also note the questions in the comments in the code  i know that actions are called with context and event as arguments but i don t see a way to get the current state from either of those  am i missing something here ,using xstate  how can i access name of current state in an action ,playing around learning xstate wanted include action machine would log current state console defining simple example machine like would go also note questions comments code know actions called context event arguments see way get current state either missing something,using xstate access name current state action,using xstate access name current state actionplaying around learning xstate wanted include action machine would log current state console defining simple example machine like would go also note questions comments code know actions called context event arguments see way get current state either missing something,"['using', 'xstate', 'access', 'name', 'current', 'state', 'actionplaying', 'around', 'learning', 'xstate', 'wanted', 'include', 'action', 'machine', 'would', 'log', 'current', 'state', 'console', 'defining', 'simple', 'example', 'machine', 'like', 'would', 'go', 'also', 'note', 'questions', 'comments', 'code', 'know', 'actions', 'called', 'context', 'event', 'arguments', 'see', 'way', 'get', 'current', 'state', 'either', 'missing', 'something']","['use', 'xstate', 'access', 'name', 'current', 'state', 'actionplay', 'around', 'learn', 'xstate', 'want', 'includ', 'action', 'machin', 'would', 'log', 'current', 'state', 'consol', 'defin', 'simpl', 'exampl', 'machin', 'like', 'would', 'go', 'also', 'note', 'question', 'comment', 'code', 'know', 'action', 'call', 'context', 'event', 'argument', 'see', 'way', 'get', 'current', 'state', 'either', 'miss', 'someth']"
134,146,146,13103715,73022079,Turning Python Keras Machine Learning model into repeatable function that can take as input multiple X and y data sets,"<p>I am currently building various machine learning models, each of the models takes in X and Y data that represent different stock prices e.g. there's an X and y data frame for each stock e.g. Apple, Microsoft.</p>
<p>I am trying to produce these models so that they are repeatable, modular, functions that I can quickly call for each of my X and y data sets.</p>
<p>I have tried these models as standalone lines of code, or as functions that don't take in parameters and they work as intended, however whenever I try to pass my X and y data sets as parameters they don't work!</p>
<p>Currently I have:</p>
<pre><code>def LSTM_regressor(X_train, X_test, y_train, y_test):
    convert_X_y_to_numpy_and_reshape(X_train, X_test, y_train, y_test)
    model = Sequential()
    model.add(Dense(1, input_dim=(X_train.shape[1]), kernel_initializer='normal', activation='sigmoid'))
    model.compile(loss='mse', optimizer='adam', metrics=['mse'])
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=20)
    print(model.summary())

final_model = KerasRegressor(build_fn=LSTM_regressor(X_train_reg_aapl, X_test_reg_aapl, y_train_reg_aapl, y_test_reg_aapl),batch_size=20, epochs=50, verbose=1)
kfold = KFold(n_splits=10) # random_state=seed)
results = cross_val_score(final_model, X_train_reg_aapl, y_train_reg_aapl, cv=kfold, n_jobs=1)
print(&quot;Results: %.2f (%.2f) MSE&quot; % (results.mean(),results.std()))
</code></pre>
<p>I am trying to pass the below into the function:</p>
<pre><code>X_train_reg_aapl, X_test_reg_aapl, y_train_reg_aapl, y_test_reg_aapl
</code></pre>
<p>but I get the error message:</p>
<pre><code>AttributeError: 'KerasRegressor' object has no attribute '__call__'
</code></pre>
<p>I have tried making a nested function and calling that, but it still doesn't work.</p>
<p>Also, the below is a function that I have created that I wanted to use to transform the parameters entered into the machine learning model into a data format suitable for the model type.</p>
<pre><code> convert_X_y_to_numpy_and_reshape(X_train, X_test, y_train, y_test)
</code></pre>
<p>It's full code is this:</p>
<pre><code>def convert_X_y_to_numpy_and_reshape(X_train,X_test,y_train,y_test):
    X_train = X_train.to_numpy()
    X_test = X_test.to_numpy()
    y_train = y_train.to_numpy()
    y_test = y_test.to_numpy()
    y_train = y_train.reshape(-1)
    y_test = y_test.reshape(-1)
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
</code></pre>
<p>Any help on this would really be appreciated!</p>
",47,2,0,5,python;function;tensorflow;machine-learning;keras,2022-07-18 15:07:18,2022-07-18 15:07:18,2022-07-21 11:54:09,i am currently building various machine learning models  each of the models takes in x and y data that represent different stock prices e g  there s an x and y data frame for each stock e g  apple  microsoft  i am trying to produce these models so that they are repeatable  modular  functions that i can quickly call for each of my x and y data sets  i have tried these models as standalone lines of code  or as functions that don t take in parameters and they work as intended  however whenever i try to pass my x and y data sets as parameters they don t work  currently i have  i am trying to pass the below into the function  but i get the error message  i have tried making a nested function and calling that  but it still doesn t work  also  the below is a function that i have created that i wanted to use to transform the parameters entered into the machine learning model into a data format suitable for the model type  it s full code is this  any help on this would really be appreciated ,turning python keras machine learning model into repeatable function that can take as input multiple x and y data sets,currently building various machine learning models models takes x data represent different stock prices e g x data frame stock e g apple microsoft trying produce models repeatable modular functions quickly call x data sets tried models standalone lines code functions take parameters work intended however whenever try pass x data sets parameters work currently trying pass function get error message tried making nested function calling still work also function created wanted use transform parameters entered machine learning model data format suitable model type full code help would really appreciated,turning python keras machine learning model repeatable function take input multiple x data sets,turning python keras machine learning model repeatable function take input multiple x data setscurrently building various machine learning models models takes x data represent different stock prices e g x data frame stock e g apple microsoft trying produce models repeatable modular functions quickly call x data sets tried models standalone lines code functions take parameters work intended however whenever try pass x data sets parameters work currently trying pass function get error message tried making nested function calling still work also function created wanted use transform parameters entered machine learning model data format suitable model type full code help would really appreciated,"['turning', 'python', 'keras', 'machine', 'learning', 'model', 'repeatable', 'function', 'take', 'input', 'multiple', 'x', 'data', 'setscurrently', 'building', 'various', 'machine', 'learning', 'models', 'models', 'takes', 'x', 'data', 'represent', 'different', 'stock', 'prices', 'e', 'g', 'x', 'data', 'frame', 'stock', 'e', 'g', 'apple', 'microsoft', 'trying', 'produce', 'models', 'repeatable', 'modular', 'functions', 'quickly', 'call', 'x', 'data', 'sets', 'tried', 'models', 'standalone', 'lines', 'code', 'functions', 'take', 'parameters', 'work', 'intended', 'however', 'whenever', 'try', 'pass', 'x', 'data', 'sets', 'parameters', 'work', 'currently', 'trying', 'pass', 'function', 'get', 'error', 'message', 'tried', 'making', 'nested', 'function', 'calling', 'still', 'work', 'also', 'function', 'created', 'wanted', 'use', 'transform', 'parameters', 'entered', 'machine', 'learning', 'model', 'data', 'format', 'suitable', 'model', 'type', 'full', 'code', 'help', 'would', 'really', 'appreciated']","['turn', 'python', 'kera', 'machin', 'learn', 'model', 'repeat', 'function', 'take', 'input', 'multipl', 'x', 'data', 'setscurr', 'build', 'variou', 'machin', 'learn', 'model', 'model', 'take', 'x', 'data', 'repres', 'differ', 'stock', 'price', 'e', 'g', 'x', 'data', 'frame', 'stock', 'e', 'g', 'appl', 'microsoft', 'tri', 'produc', 'model', 'repeat', 'modular', 'function', 'quickli', 'call', 'x', 'data', 'set', 'tri', 'model', 'standalon', 'line', 'code', 'function', 'take', 'paramet', 'work', 'intend', 'howev', 'whenev', 'tri', 'pass', 'x', 'data', 'set', 'paramet', 'work', 'current', 'tri', 'pass', 'function', 'get', 'error', 'messag', 'tri', 'make', 'nest', 'function', 'call', 'still', 'work', 'also', 'function', 'creat', 'want', 'use', 'transform', 'paramet', 'enter', 'machin', 'learn', 'model', 'data', 'format', 'suitabl', 'model', 'type', 'full', 'code', 'help', 'would', 'realli', 'appreci']"
135,147,147,4764604,73063009,&#39;torch&#39; is not defined while calling it inside its own library,"<p>I have a module that calculates data inference with a machine learning model:</p>
<pre><code>import torch

x_test, y_pred, listInferError = ntbk4Seat2dBbxXplique.computeInference(aiModel,dictParams)
</code></pre>
<p>But it returns:</p>
<pre><code>   .inference
     - /mnt/confiance/common/UC_Data_Folder/UC_Seat_Scene_understanding/Dataset_Woodscape/Data/rgb_images/00000_FV.png
torch.Size([1, 3, 288, 544])
Output exceeds the size limit. Open the full output data in a text editor
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
/home/yourservitor/kachou/UCmodeles/UC_Seat2dBbx/Seat2dBbxXplique.ipynb Cell 31 in &lt;cell line: 3&gt;()
      1 import torch
----&gt; 3 x_test, y_pred, listInferError = ntbk4Seat2dBbxXplique.computeInference(aiModel,dictParams)

File ~/kachou/UCmodeles/UC_Seat2dBbx/ntbk4Seat2dBbxXplique.py:175, in computeInference(pAiModel, pDictParams)
    173 # inference on data
    174 pAiModel.setInferenceCible(True)
--&gt; 175 prediction=pAiModel(testTorch)
    177 # save prediction (inference) with check inference vs truth
    178 inferAccuracy=pAiModel.savePrediction(pDataPathList[i],prediction,resultSavedOn,pClasses,pSauvegarde)

File ~/kachou/UCmodeles/UC_Seat2dBbx/Seat2dBbx.py:159, in Model.__call__(self, pData)
    157 with torch.no_grad():
    158     features = self.encoder(pData)
--&gt; 159     outputs = self.decoder(features, img_dim=[self.feed_width, self.feed_height])
    160 outputs = non_max_suppression(
    161     outputs[&quot;yolo_outputs&quot;],
    162     conf_thres=self.args['detection_conf_thres'],
    163     nms_thres=self.args['detection_nms_thres'])
    165 resultatDBG = []

File ~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102, in Module._call_impl(self, *input, **kwargs)
   1098 # If we don't have any hooks, we want to skip the rest of the logic in
...
--&gt; 210 x1_in = torch.cat([x1_in, x1], 1)
    211 out1, out1_branch = _branch(self.embedding1, x1_in)
    213 #  yolo branch 2

NameError: name 'torch' is not defined
</code></pre>
<p>So I was only expecting an image and not a thing that I gave me <code>Output exceeds the size limit</code>. Then I don't know why it tells me that <code>name 'torch' is not defined</code> in its own while it is own library: <code>/torch/nn/modules/module.py</code>. For being sure, I imported it in the first line</p>
",14,0,0,3,python;module;torch,2022-07-21 11:31:36,2022-07-21 11:31:36,2022-07-21 11:31:36,i have a module that calculates data inference with a machine learning model  but it returns  so i was only expecting an image and not a thing that i gave me output exceeds the size limit  then i don t know why it tells me that name  torch  is not defined in its own while it is own library   torch nn modules module py  for being sure  i imported it in the first line,   torch    is not defined while calling it inside its own library,module calculates data inference machine learning model returns expecting image thing gave output exceeds size limit know tells name torch defined library torch nn modules module py sure imported first line,torch defined calling inside library,torch defined calling inside librarymodule calculates data inference machine learning model returns expecting image thing gave output exceeds size limit know tells name torch defined library torch nn modules module py sure imported first line,"['torch', 'defined', 'calling', 'inside', 'librarymodule', 'calculates', 'data', 'inference', 'machine', 'learning', 'model', 'returns', 'expecting', 'image', 'thing', 'gave', 'output', 'exceeds', 'size', 'limit', 'know', 'tells', 'name', 'torch', 'defined', 'library', 'torch', 'nn', 'modules', 'module', 'py', 'sure', 'imported', 'first', 'line']","['torch', 'defin', 'call', 'insid', 'librarymodul', 'calcul', 'data', 'infer', 'machin', 'learn', 'model', 'return', 'expect', 'imag', 'thing', 'gave', 'output', 'exce', 'size', 'limit', 'know', 'tell', 'name', 'torch', 'defin', 'librari', 'torch', 'nn', 'modul', 'modul', 'py', 'sure', 'import', 'first', 'line']"
136,148,148,14364795,73061188,Estimating a cuboid&#39;s rotation from its parallel projection,"<p>I paint in my spare time, and that means I have a truly massive collection of reference images. Folders full of buildings, people, animals, cars, etc. It's gotten to the point where it'd be great to tag the objects by their pose, so I can find the right object at the right angle. CVAT, an image annotating tool for machine learning, allows you to mark images with cuboids, as you can see in this picture.</p>
<p><a href=""https://i.stack.imgur.com/6f1Sg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6f1Sg.png"" alt=""A cuboid annotation in CVAT"" /></a></p>
<p>But suddenly I'm wondering... is it even possible for a computer to estimate the rotation of a cuboid based on a single image, when all I can feed it are the eight (x,y) pairs that define the image of said cuboid?</p>
<p>My thinking is that I need to somehow invert the transformation matrix so that this cuboid looks like a rectangle. That would mean that we're looking at it &quot;on-axis&quot;, and I'm imagining that this inversion could furnish me with those XYZ rotations I'm looking for.</p>
<p>My best lead right now is OpenCv's <a href=""https://theailearner.com/tag/cv2-getperspectivetransform/"" rel=""nofollow noreferrer"">getPerspectiveTransform</a> function, which can create a matrix that will warp an image, but that transformation seems to be purely two-dimensional.</p>
<p>Wikipedia does mention the idea of using an <a href=""https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix"" rel=""nofollow noreferrer"">&quot;augmented matrix&quot;</a> to perform transformations in an extra dimension, which seems apropos here, since I want to go from a 2D representation to a 3d.</p>
<p>A couple constraints &amp; advantages that might clarify the feasibility, here:</p>
<ul>
<li>The cuboids are rendered in a parallel projection. They don't match the perspective of the image, and that's okay! Just need a rough sense of their pose -- a margin of error of 10 degrees on any given axis of rotation is fine by me, in case there are some inexact solutions that could work.</li>
<li>In the case of multiple cuboids in the scene, I don't care at all about their interrelations -- each case can be treated separately.</li>
<li>I always have a sense of the &quot;rear wall&quot; of the cuboid, because I'm careful in how I make these annotations, in case that symmetry-breaking helps.</li>
<li>The lengths of edges are irrelevant, I'm not trying to measure the &quot;aspect ratio&quot; of these  bounding cuboids.</li>
</ul>
<p>Thank you for any advice or hints!</p>
",12,0,0,2,computer-vision;cvat,2022-07-21 08:39:44,2022-07-21 08:39:44,2022-07-21 08:39:44,i paint in my spare time  and that means i have a truly massive collection of reference images  folders full of buildings  people  animals  cars  etc  it s gotten to the point where it d be great to tag the objects by their pose  so i can find the right object at the right angle  cvat  an image annotating tool for machine learning  allows you to mark images with cuboids  as you can see in this picture   but suddenly i m wondering    is it even possible for a computer to estimate the rotation of a cuboid based on a single image  when all i can feed it are the eight  x y  pairs that define the image of said cuboid  my thinking is that i need to somehow invert the transformation matrix so that this cuboid looks like a rectangle  that would mean that we re looking at it  on axis   and i m imagining that this inversion could furnish me with those xyz rotations i m looking for  my best lead right now is opencv s  function  which can create a matrix that will warp an image  but that transformation seems to be purely two dimensional  wikipedia does mention the idea of using an  to perform transformations in an extra dimension  which seems apropos here  since i want to go from a d representation to a d  a couple constraints  amp  advantages that might clarify the feasibility  here  thank you for any advice or hints ,estimating a cuboid   s rotation from its parallel projection,paint spare time means truly massive collection reference images folders full buildings people animals cars etc gotten point great tag objects pose find right object right angle cvat image annotating tool machine learning allows mark images cuboids see picture suddenly wondering even possible computer estimate rotation cuboid based single image feed eight x pairs define image said cuboid thinking need somehow invert transformation matrix cuboid looks like rectangle would mean looking axis imagining inversion could furnish xyz rotations looking best lead right opencv function create matrix warp image transformation seems purely two dimensional wikipedia mention idea using perform transformations extra dimension seems apropos since want go representation couple constraints amp advantages might clarify feasibility thank advice hints,estimating cuboid rotation parallel projection,estimating cuboid rotation parallel projectionpaint spare time means truly massive collection reference images folders full buildings people animals cars etc gotten point great tag objects pose find right object right angle cvat image annotating tool machine learning allows mark images cuboids see picture suddenly wondering even possible computer estimate rotation cuboid based single image feed eight x pairs define image said cuboid thinking need somehow invert transformation matrix cuboid looks like rectangle would mean looking axis imagining inversion could furnish xyz rotations looking best lead right opencv function create matrix warp image transformation seems purely two dimensional wikipedia mention idea using perform transformations extra dimension seems apropos since want go representation couple constraints amp advantages might clarify feasibility thank advice hints,"['estimating', 'cuboid', 'rotation', 'parallel', 'projectionpaint', 'spare', 'time', 'means', 'truly', 'massive', 'collection', 'reference', 'images', 'folders', 'full', 'buildings', 'people', 'animals', 'cars', 'etc', 'gotten', 'point', 'great', 'tag', 'objects', 'pose', 'find', 'right', 'object', 'right', 'angle', 'cvat', 'image', 'annotating', 'tool', 'machine', 'learning', 'allows', 'mark', 'images', 'cuboids', 'see', 'picture', 'suddenly', 'wondering', 'even', 'possible', 'computer', 'estimate', 'rotation', 'cuboid', 'based', 'single', 'image', 'feed', 'eight', 'x', 'pairs', 'define', 'image', 'said', 'cuboid', 'thinking', 'need', 'somehow', 'invert', 'transformation', 'matrix', 'cuboid', 'looks', 'like', 'rectangle', 'would', 'mean', 'looking', 'axis', 'imagining', 'inversion', 'could', 'furnish', 'xyz', 'rotations', 'looking', 'best', 'lead', 'right', 'opencv', 'function', 'create', 'matrix', 'warp', 'image', 'transformation', 'seems', 'purely', 'two', 'dimensional', 'wikipedia', 'mention', 'idea', 'using', 'perform', 'transformations', 'extra', 'dimension', 'seems', 'apropos', 'since', 'want', 'go', 'representation', 'couple', 'constraints', 'amp', 'advantages', 'might', 'clarify', 'feasibility', 'thank', 'advice', 'hints']","['estim', 'cuboid', 'rotat', 'parallel', 'projectionpaint', 'spare', 'time', 'mean', 'truli', 'massiv', 'collect', 'refer', 'imag', 'folder', 'full', 'build', 'peopl', 'anim', 'car', 'etc', 'gotten', 'point', 'great', 'tag', 'object', 'pose', 'find', 'right', 'object', 'right', 'angl', 'cvat', 'imag', 'annot', 'tool', 'machin', 'learn', 'allow', 'mark', 'imag', 'cuboid', 'see', 'pictur', 'suddenli', 'wonder', 'even', 'possibl', 'comput', 'estim', 'rotat', 'cuboid', 'base', 'singl', 'imag', 'feed', 'eight', 'x', 'pair', 'defin', 'imag', 'said', 'cuboid', 'think', 'need', 'somehow', 'invert', 'transform', 'matrix', 'cuboid', 'look', 'like', 'rectangl', 'would', 'mean', 'look', 'axi', 'imagin', 'invers', 'could', 'furnish', 'xyz', 'rotat', 'look', 'best', 'lead', 'right', 'opencv', 'function', 'creat', 'matrix', 'warp', 'imag', 'transform', 'seem', 'pure', 'two', 'dimension', 'wikipedia', 'mention', 'idea', 'use', 'perform', 'transform', 'extra', 'dimens', 'seem', 'apropo', 'sinc', 'want', 'go', 'represent', 'coupl', 'constraint', 'amp', 'advantag', 'might', 'clarifi', 'feasibl', 'thank', 'advic', 'hint']"
137,149,149,14050808,73025365,Position: fixed not working as intended for modal window in React (Next.js),"<p>I'm trying to create a modal window in React, but for some reason, using <code>position: fixed</code> sets the position of the modal window relative to the page as opposed to the browser window. This is what it looks like (note: the black boxes were edited in for privacy).
<a href=""https://i.stack.imgur.com/FZp2p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FZp2p.png"" alt=""enter image description here"" /></a></p>
<p>Here's the react for this page particularly, including the modal window component, which is at the top.</p>
<pre><code>import React, { useState } from 'react';
import Head from 'next/head';
import utilStyles from '../styles/util.module.css';
import styles from '../styles/education.module.css';
import courseData from '../public/courses.json' assert {type: 'json'};
import { FontAwesomeIcon } from '@fortawesome/react-fontawesome';
import { faCalendar } from '@fortawesome/free-regular-svg-icons';
import { faXmark } from '@fortawesome/free-solid-svg-icons';

function Modal({ isShow, handleClick, longDescription, children }) {
    return (isShow ? 
        &lt;div className=&quot;modal-outer&quot; onClick={handleClick}&gt;
            &lt;div className=&quot;modal-inner&quot;&gt;
                {children}
                &lt;button className=&quot;exit&quot;&gt;&lt;FontAwesomeIcon className=&quot;exit-icon&quot; icon={faXmark}/&gt;&lt;/button&gt;
                &lt;p className=&quot;course-description-long&quot;&gt;{longDescription}&lt;/p&gt;
            &lt;/div&gt;
        &lt;/div&gt;
     : &quot;&quot;);
}

function CourseContent({ courseName, shortDescription }) {
    return (
        &lt;&gt;
            &lt;h3 className=&quot;course-name&quot;&gt;{courseName}&lt;/h3&gt;
            &lt;p className=&quot;course-description-short&quot;&gt;{shortDescription}&lt;/p&gt;
        &lt;/&gt;
    )
}

function CourseCard({ courseName, shortDescription, longDescription}){
  const [isShow, setIsShow] = useState(false);

    function handleClick () {
        setIsShow(!isShow);
    }

  return (
    &lt;&gt;
        &lt;li key={courseName} className=&quot;course-card&quot; onClick={handleClick}&gt;
            &lt;CourseContent courseName={courseName} shortDescription={shortDescription}/&gt;
        &lt;/li&gt;
        &lt;Modal isShow={isShow} 
        longDescription=&quot;this is the long description&quot;
        handleClick={handleClick}&gt;
            &lt;CourseContent courseName={courseName} shortDescription={shortDescription}/&gt;
        &lt;/Modal&gt;
        
     &lt;/&gt;
  )
}

function DegreeInfo(props) {
  const { universityName, degreeType, degreeName, startYear, endYear, courses } = props;
  return (
    &lt;li&gt;
      &lt;p&gt;{universityName}&lt;/p&gt;
      &lt;p&gt;{degreeType}.&amp;nbsp;{degreeName}&lt;/p&gt;
      &lt;div className=&quot;date-container&quot;&gt;
        &lt;FontAwesomeIcon icon={faCalendar}/&gt;
        &lt;time dateTime={`${startYear}/${endYear}`}&gt;{startYear}&amp;ndash;{endYear}&lt;/time&gt;
      &lt;/div&gt;
      &lt;ul className=&quot;course-cards-list&quot;&gt;
        {courses.map(course =&gt; 
          &lt;CourseCard key={course.courseName} courseName={course.courseName} shortDescription={course.shortDescription} /&gt;
        )}
      &lt;/ul&gt;
    &lt;/li&gt;
  )
}

export default function EducationPage() {
  const yearDecoder = {1: &quot;First-year&quot;, 2: &quot;Second-year&quot;, 3: &quot;Third-year&quot;};
  return (
    &lt;&gt;
      &lt;Head&gt;
        &lt;title&gt;Education | Al Pouroullis&lt;/title&gt;
      &lt;/Head&gt;
      &lt;div  className={`${utilStyles.container} ${utilStyles['float-in']}`}&gt;
        
        &lt;h1&gt;Education&lt;/h1&gt;
        &lt;section className=&quot;university&quot;&gt;
            &lt;h2&gt;Tertiary&lt;/h2&gt;
            &lt;ul className={`${styles['main-list']}`}&gt;
                    {courseData.degrees.map(degree =&gt; {
                    const courses = degree.courses;
                    courses.sort(sortByYear);
                    return (&lt;DegreeInfo key={degree.name} universityName={degree.universityName}
                                degreeType={degree.type} degreeName={degree.degreeName}
                                startYear={degree.startYear} endYear={degree.endYear}
                                courses={courses}/&gt;)
                    })}
                &lt;/ul&gt;
        &lt;/section&gt;
        &lt;section className=&quot;courses&quot;&gt;
          &lt;h2&gt;Courses&lt;/h2&gt;
          &lt;ul className=&quot;main-list&quot;&gt;
            &lt;li className=&quot;main-list-item&quot;&gt;
              &lt;span className=&quot;main-list-text&quot;&gt;Mathematics for Machine Learning Specialization&lt;/span&gt;
              &lt;ul className=&quot;sub-list&quot;&gt;
                &lt;li className=&quot;sub-list-item&quot;&gt;&lt;span className=&quot;sub-list-text&quot;&gt;Linear Algebra&lt;/span&gt;&lt;/li&gt;
                &lt;li className=&quot;sub-list-item&quot;&gt;&lt;span className=&quot;sub-list-text&quot;&gt;Multivariable Calculus&lt;/span&gt;&lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;/ul&gt;
        &lt;/section&gt;
      &lt;/div&gt;
    &lt;/&gt;
  );
}

function sortByYear(a, b) {
  if (a.year &lt; b.year) {
    return -1;
  } else if (a.year &gt; b.year) {
    return 1;
  } else {
    return 0;
  }
}
</code></pre>
<p>And here's the styling for the modal window:</p>
<pre><code>.modal-outer {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, .5);
  display: flex;
  justify-content: center;
  align-items: center; 
}
</code></pre>
<p>I'm rather new to React, so excuse this if it's a complete rookie mistake.</p>
<p>Thanks for the help.</p>
",40,2,0,5,javascript;html;css;reactjs;next.js,2022-07-18 19:05:43,2022-07-18 19:05:43,2022-07-21 08:33:28,here s the react for this page particularly  including the modal window component  which is at the top  and here s the styling for the modal window  i m rather new to react  so excuse this if it s a complete rookie mistake  thanks for the help ,position  fixed not working as intended for modal window in react  next js ,react page particularly including modal window component top styling modal window rather react excuse complete rookie mistake thanks help,position fixed working intended modal window react next js,position fixed working intended modal window react next jsreact page particularly including modal window component top styling modal window rather react excuse complete rookie mistake thanks help,"['position', 'fixed', 'working', 'intended', 'modal', 'window', 'react', 'next', 'jsreact', 'page', 'particularly', 'including', 'modal', 'window', 'component', 'top', 'styling', 'modal', 'window', 'rather', 'react', 'excuse', 'complete', 'rookie', 'mistake', 'thanks', 'help']","['posit', 'fix', 'work', 'intend', 'modal', 'window', 'react', 'next', 'jsreact', 'page', 'particularli', 'includ', 'modal', 'window', 'compon', 'top', 'style', 'modal', 'window', 'rather', 'react', 'excus', 'complet', 'rooki', 'mistak', 'thank', 'help']"
138,150,150,19301271,72550803,Visual Studio for Mac 2022 - Where is GIT and Version Control?,"<p>I am bit new to this Visual Studio for Mac 2022 as it came out just recently for Mac OS.
I am learning by following a video course, which is done on Windows. And there is supposed to be an option to &quot;Add to Source Control&quot; or from the official docs for Visual Studio for Mac =&gt; <a href=""https://docs.microsoft.com/en-us/visualstudio/mac/working-with-git?view=vsmac-2019"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/visualstudio/mac/working-with-git?view=vsmac-2019</a> there is supposed to be a tab &quot;Version Control&quot; to push my project to my Github.</p>
<p>But there is not. If i try to go like this:</p>
<ol>
<li>Select Solution</li>
<li>Commit or Stash</li>
<li>Push
i get error &quot;Create an initial commit first&quot;</li>
</ol>
<p>But i can not commit as this i the only way to commit. It is bit frustrating.
Also there is no way to connect my Github account to the VSforMac 2022 even if I use the &quot;Help&quot; and put in Git or Github this it the only option that it finds.</p>
<p>Did they just forgot to add Git integration to the new Visual Studio code for 2022 ? Or I am missing something in settings or possible a package?</p>
<p>See screenshot <a href=""https://ibb.co/N2nPMbg"" rel=""nofollow noreferrer"">https://ibb.co/N2nPMbg</a>
PS: I have Git installed on my machine</p>
<p>Thanks!</p>
",363,1,0,2,c#;visual-studio-mac,2022-06-08 21:51:26,2022-06-08 21:51:26,2022-07-21 08:33:13,but there is not  if i try to go like this  did they just forgot to add git integration to the new visual studio code for    or i am missing something in settings or possible a package  thanks ,visual studio for mac    where is git and version control ,try go like forgot git integration visual studio code missing something settings possible package thanks,visual studio mac git version control,visual studio mac git version controltry go like forgot git integration visual studio code missing something settings possible package thanks,"['visual', 'studio', 'mac', 'git', 'version', 'controltry', 'go', 'like', 'forgot', 'git', 'integration', 'visual', 'studio', 'code', 'missing', 'something', 'settings', 'possible', 'package', 'thanks']","['visual', 'studio', 'mac', 'git', 'version', 'controltri', 'go', 'like', 'forgot', 'git', 'integr', 'visual', 'studio', 'code', 'miss', 'someth', 'set', 'possibl', 'packag', 'thank']"
139,151,151,16962809,73040570,How to share models in a multitenant enviroment with Mlflow?,"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/manage-model-lifecycle/multiple-workspaces"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/manage-model-lifecycle/multiple-workspaces</a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?</p>
",27,1,0,4,azure;azure-blob-storage;databricks;mlflow,2022-07-19 19:33:46,2022-07-19 19:33:46,2022-07-21 07:21:53,the company i work for are using databricks with azure as a storage service  my group is trying to create a centralized model registry that allows us to push and pull models into different instances of databricks  we are aware that we can share models within the same subscription    however we have multiple subscriptions so this wont work for us  from what i ve read there are two solutions for this  use azure blob storage or an sql solution  unfortunately i cant find much info online  anyone have any idea how i can implement this ,how to share models in a multitenant enviroment with mlflow ,company work using databricks azure storage service group trying create centralized model registry allows us push pull models different instances databricks aware share models within subscription however multiple subscriptions wont work us read two solutions use azure blob storage sql solution unfortunately cant find much info online anyone idea implement,share models multitenant enviroment mlflow,share models multitenant enviroment mlflowcompany work using databricks azure storage service group trying create centralized model registry allows us push pull models different instances databricks aware share models within subscription however multiple subscriptions wont work us read two solutions use azure blob storage sql solution unfortunately cant find much info online anyone idea implement,"['share', 'models', 'multitenant', 'enviroment', 'mlflowcompany', 'work', 'using', 'databricks', 'azure', 'storage', 'service', 'group', 'trying', 'create', 'centralized', 'model', 'registry', 'allows', 'us', 'push', 'pull', 'models', 'different', 'instances', 'databricks', 'aware', 'share', 'models', 'within', 'subscription', 'however', 'multiple', 'subscriptions', 'wont', 'work', 'us', 'read', 'two', 'solutions', 'use', 'azure', 'blob', 'storage', 'sql', 'solution', 'unfortunately', 'cant', 'find', 'much', 'info', 'online', 'anyone', 'idea', 'implement']","['share', 'model', 'multiten', 'enviro', 'mlflowcompani', 'work', 'use', 'databrick', 'azur', 'storag', 'servic', 'group', 'tri', 'creat', 'central', 'model', 'registri', 'allow', 'us', 'push', 'pull', 'model', 'differ', 'instanc', 'databrick', 'awar', 'share', 'model', 'within', 'subscript', 'howev', 'multipl', 'subscript', 'wont', 'work', 'us', 'read', 'two', 'solut', 'use', 'azur', 'blob', 'storag', 'sql', 'solut', 'unfortun', 'cant', 'find', 'much', 'info', 'onlin', 'anyon', 'idea', 'implement']"
140,152,152,19345011,72879170,Deployment of Machine Learning models on KAFKA,"<p>How can I deploying ML models on Kafka?</p>
",21,1,-1,2,machine-learning;apache-kafka,2022-07-06 10:02:31,2022-07-06 10:02:31,2022-07-21 02:39:24,how can i deploying ml models on kafka ,deployment of machine learning models on kafka,deploying ml models kafka,deployment machine learning models kafka,deployment machine learning models kafkadeploying ml models kafka,"['deployment', 'machine', 'learning', 'models', 'kafkadeploying', 'ml', 'models', 'kafka']","['deploy', 'machin', 'learn', 'model', 'kafkadeploy', 'ml', 'model', 'kafka']"
141,153,153,19587005,73051708,How to Install JupyterHub in Remote server (Client Node) and use web browser from local machines to connect and run parallely?,"<p>I am new to production environment server setup. I want to install JupyterHub in my company's remote server which we use as client Node, and allow developers to connect from their local machine though their web browser and build their code. Is there any documentation where I can refer to for installation and creating Login IDs for developers to connect from their browser and also how to allocate resources to each developer?</p>
<p>The Client node has 256 GB of ram, while there maybe up to 30 parallel users running their data analysis/machine learning codes. Could anyone guide with their expertise?</p>
",10,0,0,1,jupyterhub,2022-07-20 15:31:41,2022-07-20 15:31:41,2022-07-20 18:19:37,i am new to production environment server setup  i want to install jupyterhub in my company s remote server which we use as client node  and allow developers to connect from their local machine though their web browser and build their code  is there any documentation where i can refer to for installation and creating login ids for developers to connect from their browser and also how to allocate resources to each developer  the client node has  gb of ram  while there maybe up to  parallel users running their data analysis machine learning codes  could anyone guide with their expertise ,how to install jupyterhub in remote server  client node  and use web browser from local machines to connect and run parallely ,production environment server setup want install jupyterhub company remote server use client node allow developers connect local machine though web browser build code documentation refer installation creating login ids developers connect browser also allocate resources developer client node gb ram maybe parallel users running data analysis machine learning codes could anyone guide expertise,install jupyterhub remote server client node use web browser local machines connect run parallely,install jupyterhub remote server client node use web browser local machines connect run parallelyproduction environment server setup want install jupyterhub company remote server use client node allow developers connect local machine though web browser build code documentation refer installation creating login ids developers connect browser also allocate resources developer client node gb ram maybe parallel users running data analysis machine learning codes could anyone guide expertise,"['install', 'jupyterhub', 'remote', 'server', 'client', 'node', 'use', 'web', 'browser', 'local', 'machines', 'connect', 'run', 'parallelyproduction', 'environment', 'server', 'setup', 'want', 'install', 'jupyterhub', 'company', 'remote', 'server', 'use', 'client', 'node', 'allow', 'developers', 'connect', 'local', 'machine', 'though', 'web', 'browser', 'build', 'code', 'documentation', 'refer', 'installation', 'creating', 'login', 'ids', 'developers', 'connect', 'browser', 'also', 'allocate', 'resources', 'developer', 'client', 'node', 'gb', 'ram', 'maybe', 'parallel', 'users', 'running', 'data', 'analysis', 'machine', 'learning', 'codes', 'could', 'anyone', 'guide', 'expertise']","['instal', 'jupyterhub', 'remot', 'server', 'client', 'node', 'use', 'web', 'browser', 'local', 'machin', 'connect', 'run', 'parallelyproduct', 'environ', 'server', 'setup', 'want', 'instal', 'jupyterhub', 'compani', 'remot', 'server', 'use', 'client', 'node', 'allow', 'develop', 'connect', 'local', 'machin', 'though', 'web', 'browser', 'build', 'code', 'document', 'refer', 'instal', 'creat', 'login', 'id', 'develop', 'connect', 'browser', 'also', 'alloc', 'resourc', 'develop', 'client', 'node', 'gb', 'ram', 'mayb', 'parallel', 'user', 'run', 'data', 'analysi', 'machin', 'learn', 'code', 'could', 'anyon', 'guid', 'expertis']"
142,154,154,19246617,73053195,Machine Learning (python) Saving a scaler in a model,"<p>I'm using the following scaler:</p>
<pre><code>scaler = preprocessing.MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(dataset_train), 
                              columns=dataset_train.columns, 
                              index=dataset_train.index)

X_train.sample(frac=1)

X_test = pd.DataFrame(scaler.transform(dataset_test), 
                             columns=dataset_test.columns, 
                             index=dataset_test.index)
</code></pre>
<p>The model that I'm using for anomaly detection is sequential().</p>
<p>I know how to save the model, but my question is how can I save the scaler in the model, so I can simply apply the model in a new df.</p>
<p>Thank you.</p>
",38,1,0,3,python;machine-learning;model,2022-07-20 17:09:04,2022-07-20 17:09:04,2022-07-20 17:31:20,i m using the following scaler  the model that i m using for anomaly detection is sequential    i know how to save the model  but my question is how can i save the scaler in the model  so i can simply apply the model in a new df  thank you ,machine learning  python  saving a scaler in a model,using following scaler model using anomaly detection sequential know save model question save scaler model simply apply model df thank,machine learning python saving scaler model,machine learning python saving scaler modelusing following scaler model using anomaly detection sequential know save model question save scaler model simply apply model df thank,"['machine', 'learning', 'python', 'saving', 'scaler', 'modelusing', 'following', 'scaler', 'model', 'using', 'anomaly', 'detection', 'sequential', 'know', 'save', 'model', 'question', 'save', 'scaler', 'model', 'simply', 'apply', 'model', 'df', 'thank']","['machin', 'learn', 'python', 'save', 'scaler', 'modelus', 'follow', 'scaler', 'model', 'use', 'anomali', 'detect', 'sequenti', 'know', 'save', 'model', 'question', 'save', 'scaler', 'model', 'simpli', 'appli', 'model', 'df', 'thank']"
143,155,155,13409338,72995451,How to convert the vectorized text into embedding matrix using Longformer transformer,"<p>Everyone knows about converting the text into vectors and that vectors into a matrix which helps to feed the machine learning models like LightGBM as features.</p>
<pre class=""lang-py prettyprint-override""><code>import transformers
from transformers import LongformerTokenizer,LongformerForSequenceClassification,Trainer, TrainingArguments, LongformerConfig,LongformerTokenizerFast
import tensorflow as tf

#tokenizer=LongformerTokenizer.from_pretrained(&quot;hf-internal-testing/tiny-random-longformer&quot;)
#model=TFLongformerForSequenceClassification.from_pretrained(&quot;hf-internal-testing/tiny-random-longformer&quot;)
from torch.utils.data import Dataset, DataLoader
config=LongformerConfig()
test=pd.read_csv('../input/feedback-prize-effectiveness/test.csv')
train=pd.read_csv('../input/feedback-prize-effectiveness/train.csv')
# load model and tokenizer and define length of the text sequence
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',gradient_checkpointing=False,attention_window = 512)
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)
#inputs = tokenizer(&quot;Hello, my dog is cute killer and bad&quot;)
#print(inputs.input_ids)
k=[]
for i in train['discourse_text']:
    inputs=tokenizer(i)
    m=inputs.input_ids
    k.append(m)
train['long_tokens']=k
</code></pre>
<p>The above code uses the tokenization method from longformer to encode the sentences in the dataset. So, after doing that the dataset is going to look like below
<a href=""https://i.stack.imgur.com/W0dpr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W0dpr.png"" alt=""embeddings of the data"" /></a></p>
<p>So, the feature &quot;long_tokens&quot; should serve as a feature for the machine learning model[LightGBM].</p>
<p>My question is how can we transform those features to input the model?</p>
<p>The datatype of the &quot;long_tokens&quot; is tensor.</p>
<p>Please answer the question
Thanks &amp; Regards
Satwik Sunnam</p>
",35,1,0,5,tensorflow;machine-learning;deep-learning;numpy-ndarray;huggingface-transformers,2022-07-15 17:20:19,2022-07-15 17:20:19,2022-07-20 14:01:34,everyone knows about converting the text into vectors and that vectors into a matrix which helps to feed the machine learning models like lightgbm as features  so  the feature  long_tokens  should serve as a feature for the machine learning model lightgbm   my question is how can we transform those features to input the model  the datatype of the  long_tokens  is tensor ,how to convert the vectorized text into embedding matrix using longformer transformer,everyone knows converting text vectors vectors matrix helps feed machine learning models like lightgbm features feature long_tokens serve feature machine learning model lightgbm question transform features input model datatype long_tokens tensor,convert vectorized text embedding matrix using longformer transformer,convert vectorized text embedding matrix using longformer transformereveryone knows converting text vectors vectors matrix helps feed machine learning models like lightgbm features feature long_tokens serve feature machine learning model lightgbm question transform features input model datatype long_tokens tensor,"['convert', 'vectorized', 'text', 'embedding', 'matrix', 'using', 'longformer', 'transformereveryone', 'knows', 'converting', 'text', 'vectors', 'vectors', 'matrix', 'helps', 'feed', 'machine', 'learning', 'models', 'like', 'lightgbm', 'features', 'feature', 'long_tokens', 'serve', 'feature', 'machine', 'learning', 'model', 'lightgbm', 'question', 'transform', 'features', 'input', 'model', 'datatype', 'long_tokens', 'tensor']","['convert', 'vector', 'text', 'embed', 'matrix', 'use', 'longform', 'transformereveryon', 'know', 'convert', 'text', 'vector', 'vector', 'matrix', 'help', 'feed', 'machin', 'learn', 'model', 'like', 'lightgbm', 'featur', 'featur', 'long_token', 'serv', 'featur', 'machin', 'learn', 'model', 'lightgbm', 'question', 'transform', 'featur', 'input', 'model', 'datatyp', 'long_token', 'tensor']"
144,156,156,8610564,70336597,C# VS Object detection with Microsoft.ML and Tensorflow Faster_rcnn_Resnet50,"<p>I have been trying to understand how the C# MicrosoftML library works with VS.
I have run the demo with ONNX and the pretrained model: <a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/object-detection-onnx"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/object-detection-onnx</a></p>
<p>That works fine. But I have a trained model in Tensorflow, Faster_rcnn_Resnet50 and on the COCO dataset. Its not in ONNX format. So can I get ML to read the file I have without transferring to ONNX.</p>
<p>If I have to transfer, how do I do? I have installed the <a href=""https://github.com/onnx/tensorflow-onnx"" rel=""nofollow noreferrer"">https://github.com/onnx/tensorflow-onnx</a> and tried to convert a model without luck.</p>
<p>to be honest I am quite in the dark and starting to understand all the acronyms. If anyone can point to a tutorial or guide on how to use tensorflow model based on COCO in ML C# I would be very happy.</p>
<p>thanks in advance.. If I miss out on somethings its because I am learning how all comes together...</p>
",264,0,0,5,c#;tensorflow;transfer-learning;resnet;microsoft.ml,2021-12-13 16:54:49,2021-12-13 16:54:49,2022-07-20 12:21:58,that works fine  but i have a trained model in tensorflow  faster_rcnn_resnet and on the coco dataset  its not in onnx format  so can i get ml to read the file i have without transferring to onnx  if i have to transfer  how do i do  i have installed the  and tried to convert a model without luck  to be honest i am quite in the dark and starting to understand all the acronyms  if anyone can point to a tutorial or guide on how to use tensorflow model based on coco in ml c  i would be very happy  thanks in advance   if i miss out on somethings its because i am learning how all comes together   ,c  vs object detection with microsoft ml and tensorflow faster_rcnn_resnet,works fine trained model tensorflow faster_rcnn_resnet coco dataset onnx format get ml read file without transferring onnx transfer installed tried convert model without luck honest quite dark starting understand acronyms anyone point tutorial guide use tensorflow model based coco ml c would happy thanks advance miss somethings learning comes together,c vs object detection microsoft ml tensorflow faster_rcnn_resnet,c vs object detection microsoft ml tensorflow faster_rcnn_resnetworks fine trained model tensorflow faster_rcnn_resnet coco dataset onnx format get ml read file without transferring onnx transfer installed tried convert model without luck honest quite dark starting understand acronyms anyone point tutorial guide use tensorflow model based coco ml c would happy thanks advance miss somethings learning comes together,"['c', 'vs', 'object', 'detection', 'microsoft', 'ml', 'tensorflow', 'faster_rcnn_resnetworks', 'fine', 'trained', 'model', 'tensorflow', 'faster_rcnn_resnet', 'coco', 'dataset', 'onnx', 'format', 'get', 'ml', 'read', 'file', 'without', 'transferring', 'onnx', 'transfer', 'installed', 'tried', 'convert', 'model', 'without', 'luck', 'honest', 'quite', 'dark', 'starting', 'understand', 'acronyms', 'anyone', 'point', 'tutorial', 'guide', 'use', 'tensorflow', 'model', 'based', 'coco', 'ml', 'c', 'would', 'happy', 'thanks', 'advance', 'miss', 'somethings', 'learning', 'comes', 'together']","['c', 'vs', 'object', 'detect', 'microsoft', 'ml', 'tensorflow', 'faster_rcnn_resnetwork', 'fine', 'train', 'model', 'tensorflow', 'faster_rcnn_resnet', 'coco', 'dataset', 'onnx', 'format', 'get', 'ml', 'read', 'file', 'without', 'transfer', 'onnx', 'transfer', 'instal', 'tri', 'convert', 'model', 'without', 'luck', 'honest', 'quit', 'dark', 'start', 'understand', 'acronym', 'anyon', 'point', 'tutori', 'guid', 'use', 'tensorflow', 'model', 'base', 'coco', 'ml', 'c', 'would', 'happi', 'thank', 'advanc', 'miss', 'someth', 'learn', 'come', 'togeth']"
145,157,157,10252776,73044828,What does this error mean? And how can I fix it?,"<p>I'm using TensorFlow to create an autoencoder model, but I am facing an error and I don't know what it means nor how to fix it. The only clue that I get from this error message is that there are incompatible shapes &quot;[32,14110,32] vs. [25,14110,32]&quot;. Where is that coming from given that I'm using the same variable/data as my input and target variable because I'm training the data using an autoencoder model. I inputted data with the shape (2425, 28220) after splitting it into testing, training, and validation. I figured it might be pointless to split the data to have a testing set simply because I'm training the model using the same data as input and output. I don't know let me know what you all thoughts are. Again, I'm new to machine learning and tensorflow, so try to bear with me. Here is my code:</p>
<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import pandas as pd

from tensorflow.keras.models import Model
from tensorflow.keras import datasets, layers, models


from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler



genotype_file = 'genotype_full_1_2.txt'
phenotype_file = 'phenotype.csv'


genotype_data = pd.read_csv(genotype_file, sep='\t', index_col=0)


#default values
dropout_percentage = .25
max_epochs = 1000
min_epochs = 2
batch_size = 32
convfilter_size = 5
maxpooling_filtersize = 2
upsampling_filtersize = 2
numb_kernels = 32


training_percentage = .65
validation_percentage = .15
testing_percentage = .20



scaler = MinMaxScaler() 
genotypedata_scaled = scaler.fit_transform(genotype_data.dropna())



#splitting the data
train_data, testing_data = train_test_split(genotypedata_scaled, test_size=testing_percentage, train_size=training_percentage)
train_data, valid_data = train_test_split(train_data, train_size=1-validation_percentage)


#creating autoencoder(based on SCDA model)
#///////////////////////////////ENCODER
model = models.Sequential()
model.add(layers.InputLayer(input_shape=(genotype_data.shape[1],1), batch_size=batch_size))
model.add(layers.Conv1D(filters=numb_kernels, kernel_size=convfilter_size * 2, strides=1, padding='same', activation='relu'))#CONV1D
model.add(layers.MaxPooling1D(pool_size=maxpooling_filtersize))#MAXPOOLING
model.add(layers.Dropout(dropout_percentage))#DROPOUT
model.add(layers.Conv1D(filters=numb_kernels * 2, kernel_size=convfilter_size, strides=1, padding='same', activation='relu'))#CONV1D
model.add(layers.MaxPooling1D(pool_size=maxpooling_filtersize))#MAXPOOLING
model.add(layers.Dropout(dropout_percentage))#DROPOUT
model.add(layers.Conv1D(filters=numb_kernels * 4, kernel_size=convfilter_size, strides=1, padding='same', activation='relu'))#CONV1D


#///////////////////////////////DECODER
model.add(layers.Conv1D(filters=numb_kernels * 4, kernel_size=convfilter_size, strides=1, padding='same', activation='relu'))#CONV1D
model.add(layers.UpSampling1D(size=upsampling_filtersize))#UPSAMPLING ***CHECK
model.add(layers.Dropout(dropout_percentage))#DROPOUT
model.add(layers.Conv1D(filters=numb_kernels * 2, kernel_size=convfilter_size, strides=1, padding='same', activation='relu'))#CONV1D
model.add(layers.UpSampling1D(size=upsampling_filtersize))#UPSAMPLING ***CHECK
model.add(layers.Dropout(dropout_percentage))#DROPOUT
model.add(layers.Conv1D(filters=1, kernel_size=convfilter_size, strides=1, padding='same', activation='relu'))#CONV1D



#summary of model
model.compile(loss='mse', optimizer='adam', metrics=['mse'])
model.summary()

#training the model
scda_train = model.fit(train_data, train_data, batch_size=batch_size,epochs=1,verbose=1, validation_data=(valid_data, valid_data))

</code></pre>
<p>Here is the summary of my autoencoder model:</p>
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d_12 (Conv1D)          (32, 28220, 32)           352       
                                                                 
 max_pooling1d_4 (MaxPooling  (32, 14110, 32)          0         
 1D)                                                             
                                                                 
 dropout_4 (Dropout)         (32, 14110, 32)           0         
                                                                 
 conv1d_13 (Conv1D)          (32, 14110, 64)           10304     
                                                                 
 max_pooling1d_5 (MaxPooling  (32, 7055, 64)           0         
 1D)                                                             
                                                                 
 dropout_5 (Dropout)         (32, 7055, 64)            0         
                                                                 
 conv1d_14 (Conv1D)          (32, 7055, 128)           41088     
                                                                 
 conv1d_15 (Conv1D)          (32, 7055, 128)           82048     
                                                                 
 up_sampling1d_4 (UpSampling  (32, 14110, 128)         0         
 1D)                                                             
                                                                 
 dropout_6 (Dropout)         (32, 14110, 128)          0         
                                                                 
 conv1d_16 (Conv1D)          (32, 14110, 64)           41024     
                                                                 
 up_sampling1d_5 (UpSampling  (32, 28220, 64)          0         
 1D)                                                             
                                                                 
 dropout_7 (Dropout)         (32, 28220, 64)           0         
                                                                 
 conv1d_17 (Conv1D)          (32, 28220, 1)            321       
                                                                 
=================================================================
Total params: 175,137
Trainable params: 175,137
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>And the error message:</p>
<pre><code>(2425, 28220)
75/76 [============================&gt;.] - ETA: 9s - loss: 0.0283 - mse: 0.0283 
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-3-d30d9ac2067a&gt; in &lt;module&gt;()
    105 #training the model
    106 print(train_data.shape)
--&gt; 107 scda_train = model.fit(train_data, train_data, batch_size=batch_size,epochs=1,verbose=1, validation_data=(valid_data, valid_data))
    108 #print(train_data.shape)
    109 

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'sequential_2/dropout_4/dropout/Mul_1' defined at (most recent call last):
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
      &quot;__main__&quot;, mod_spec)
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
      exec(code, run_globals)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py&quot;, line 16, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py&quot;, line 846, in launch_instance
      app.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py&quot;, line 499, in start
      self.io_loop.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 132, in start
      self.asyncio_loop.run_forever()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 541, in run_forever
      self._run_once()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 1786, in _run_once
      handle._run()
    File &quot;/usr/lib/python3.7/asyncio/events.py&quot;, line 88, in _run
      self._context.run(self._callback, *self._args)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 122, in _handle_events
      handler_func(fileobj, events)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 577, in _handle_events
      self._handle_recv()
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 606, in _handle_recv
      self._run_callback(callback, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 556, in _run_callback
      callback(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 283, in dispatcher
      return self.dispatch_shell(stream, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 233, in dispatch_shell
      handler(stream, idents, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 399, in execute_request
      user_expressions, allow_stdin)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py&quot;, line 208, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py&quot;, line 537, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2718, in run_cell
      interactivity=interactivity, compiler=compiler, result=result)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2822, in run_ast_nodes
      if self.run_code(code, result):
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2882, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;&lt;ipython-input-3-d30d9ac2067a&gt;&quot;, line 107, in &lt;module&gt;
      scda_train = model.fit(train_data, train_data, batch_size=batch_size,epochs=1,verbose=1, validation_data=(valid_data, valid_data))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1384, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1021, in train_function
      return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1010, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1000, in run_step
      outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 859, in train_step
      y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py&quot;, line 374, in call
      return super(Sequential, self).call(inputs, training=training, mask=mask)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 452, in call
      inputs, training=training, mask=mask)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 589, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py&quot;, line 112, in call
      lambda: tf.identity(inputs))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/control_flow_util.py&quot;, line 106, in smart_cond
      pred, true_fn=true_fn, false_fn=false_fn, name=name)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py&quot;, line 109, in dropped_inputs
      inputs, self.rate, noise_shape=self._get_noise_shape(inputs))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/backend.py&quot;, line 1941, in dropout
      seed=self.make_legacy_seed())
Node: 'sequential_2/dropout_4/dropout/Mul_1'
Incompatible shapes: [32,14110,32] vs. [25,14110,32]
     [[{{node sequential_2/dropout_4/dropout/Mul_1}}]] [Op:__inference_train_function_829911]
</code></pre>
",51,0,0,4,python;tensorflow;machine-learning;autoencoder,2022-07-20 03:41:20,2022-07-20 03:41:20,2022-07-20 11:15:37,i m using tensorflow to create an autoencoder model  but i am facing an error and i don t know what it means nor how to fix it  the only clue that i get from this error message is that there are incompatible shapes       vs         where is that coming from given that i m using the same variable data as my input and target variable because i m training the data using an autoencoder model  i inputted data with the shape      after splitting it into testing  training  and validation  i figured it might be pointless to split the data to have a testing set simply because i m training the model using the same data as input and output  i don t know let me know what you all thoughts are  again  i m new to machine learning and tensorflow  so try to bear with me  here is my code  here is the summary of my autoencoder model  and the error message ,what does this error mean  and how can i fix it ,using tensorflow create autoencoder model facing error know means fix clue get error message incompatible shapes vs coming given using variable data input target variable training data using autoencoder model inputted data shape splitting testing training validation figured might pointless split data testing set simply training model using data input output know let know thoughts machine learning tensorflow try bear code summary autoencoder model error message,error mean fix,error mean fixusing tensorflow create autoencoder model facing error know means fix clue get error message incompatible shapes vs coming given using variable data input target variable training data using autoencoder model inputted data shape splitting testing training validation figured might pointless split data testing set simply training model using data input output know let know thoughts machine learning tensorflow try bear code summary autoencoder model error message,"['error', 'mean', 'fixusing', 'tensorflow', 'create', 'autoencoder', 'model', 'facing', 'error', 'know', 'means', 'fix', 'clue', 'get', 'error', 'message', 'incompatible', 'shapes', 'vs', 'coming', 'given', 'using', 'variable', 'data', 'input', 'target', 'variable', 'training', 'data', 'using', 'autoencoder', 'model', 'inputted', 'data', 'shape', 'splitting', 'testing', 'training', 'validation', 'figured', 'might', 'pointless', 'split', 'data', 'testing', 'set', 'simply', 'training', 'model', 'using', 'data', 'input', 'output', 'know', 'let', 'know', 'thoughts', 'machine', 'learning', 'tensorflow', 'try', 'bear', 'code', 'summary', 'autoencoder', 'model', 'error', 'message']","['error', 'mean', 'fixus', 'tensorflow', 'creat', 'autoencod', 'model', 'face', 'error', 'know', 'mean', 'fix', 'clue', 'get', 'error', 'messag', 'incompat', 'shape', 'vs', 'come', 'given', 'use', 'variabl', 'data', 'input', 'target', 'variabl', 'train', 'data', 'use', 'autoencod', 'model', 'input', 'data', 'shape', 'split', 'test', 'train', 'valid', 'figur', 'might', 'pointless', 'split', 'data', 'test', 'set', 'simpli', 'train', 'model', 'use', 'data', 'input', 'output', 'know', 'let', 'know', 'thought', 'machin', 'learn', 'tensorflow', 'tri', 'bear', 'code', 'summari', 'autoencod', 'model', 'error', 'messag']"
146,158,158,18857317,73044946,How to write SVR_Loss function in python?,"<p>I encountered a problem when I was doing quantum support vector machine experiments: when I was doing quantum kernel alignment, I saw that the example given by qiskit was SVC_Loss: <a href=""https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.utils.loss_functions.SVCLoss.html#qiskit_machine_learning.utils.loss_functions.SVCLoss"" rel=""nofollow noreferrer"">https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.utils.loss_functions.SVCLoss.html#qiskit_machine_learning.utils.loss_functions.SVCLoss</a></p>
<p>Its core code is: SVCLoss = \sum_{i} a_i - 0.5 \sum_{i,j} a_i a_j y_{i} y_{j} K_(x_i, x_j)</p>
<pre><code>class SVCLoss(KernelLoss):
r&quot;&quot;&quot;
.. math::

    \text{This class provides a kernel loss function for classification tasks by
    fitting an ``SVC`` model from sklearn. Given training samples, x_{i}, with binary
    labels, y_{i}, and a kernel, K_{}, parameterized by values, , the loss is defined as:}

    SVCLoss = \sum_{i} a_i - 0.5 \sum_{i,j} a_i a_j y_{i} y_{j} K_(x_i, x_j)

    \text{where a_i are the optimal Lagrange multipliers found by solving the standard
    SVM quadratic program. Note that the hyper-parameter C for the soft-margin penalty can
    be specified through the keyword args.}

Minimizing this loss over the parameters, , of the kernel is equivalent to maximizing a
weighted kernel alignment, which in turn yields the smallest upper bound to the SVM
generalization error for a given parameterization.

See https://arxiv.org/abs/2105.03406 for further details.
&quot;&quot;&quot;

def __init__(self, **kwargs):
    &quot;&quot;&quot;
    Args:
        **kwargs: Arbitrary keyword arguments to pass to SVC constructor within
                  SVCLoss evaluation.
    &quot;&quot;&quot;
    self.kwargs = kwargs

def evaluate(
    self,
    parameter_values: Sequence[float],
    quantum_kernel: QuantumKernel,
    data: np.ndarray,
    labels: np.ndarray,
) -&gt; float:
    # Bind training parameters
    quantum_kernel.assign_user_parameters(parameter_values)

    # Get estimated kernel matrix
    kmatrix = quantum_kernel.evaluate(np.array(data))

    # Train a quantum support vector classifier
    svc = SVR(kernel=&quot;precomputed&quot;, **self.kwargs)
    svc.fit(kmatrix, labels)

    # Get dual coefficients
    dual_coefs = svc.dual_coef_[0]

    # Get support vectors
    support_vecs = svc.support_

    # Prune kernel matrix of non-support-vector entries
    kmatrix = kmatrix[support_vecs, :][:, support_vecs]

    # Calculate loss
    loss = np.sum(np.abs(dual_coefs)) - (0.5 * (dual_coefs.T @ kmatrix @ dual_coefs))

   

    return loss
</code></pre>
<p><code>svc_loss = np.sum(np.abs(dual_coefs)) - (0.5 * (dual_coefs.T @ kmatrix @ dual_coefs))</code></p>
<p>I wonder how the loss function of svr looks like SVRLoss = \sum_{i} a_i + 0.5 \sum_{i,j} a_i a_j y_{i} y_{j} K_(x_i, x_j)</p>
<p><code>svr_loss = np.sum(np.abs(dual_coefs)) + (0.5 * (dual_coefs.T @ kmatrix @ dual_coefs))</code></p>
<p>is that so</p>
",25,0,-1,4,python;machine-learning;svm;loss-function,2022-07-20 04:05:25,2022-07-20 04:05:25,2022-07-20 11:14:19,i encountered a problem when i was doing quantum support vector machine experiments  when i was doing quantum kernel alignment  i saw that the example given by qiskit was svc_loss   its core code is  svcloss    sum_ i  a_i      sum_ i j  a_i a_j y_ i  y_ j  k_ x_i  x_j  svc_loss   np sum np abs dual_coefs           dual_coefs t   kmatrix   dual_coefs   i wonder how the loss function of svr looks like  svrloss    sum_ i  a_i      sum_ i j  a_i a_j y_ i  y_ j  k_ x_i  x_j  svr_loss   np sum np abs dual_coefs           dual_coefs t   kmatrix   dual_coefs   is that so ,how to write svr_loss function in python ,encountered problem quantum support vector machine experiments quantum kernel alignment saw example given qiskit svc_loss core code svcloss sum_ a_i sum_ j a_i a_j y_ y_ j k_ x_i x_j svc_loss np sum np abs dual_coefs dual_coefs kmatrix dual_coefs wonder loss function svr looks like svrloss sum_ a_i sum_ j a_i a_j y_ y_ j k_ x_i x_j svr_loss np sum np abs dual_coefs dual_coefs kmatrix dual_coefs,write svr_loss function python,write svr_loss function pythonencountered problem quantum support vector machine experiments quantum kernel alignment saw example given qiskit svc_loss core code svcloss sum_ a_i sum_ j a_i a_j y_ y_ j k_ x_i x_j svc_loss np sum np abs dual_coefs dual_coefs kmatrix dual_coefs wonder loss function svr looks like svrloss sum_ a_i sum_ j a_i a_j y_ y_ j k_ x_i x_j svr_loss np sum np abs dual_coefs dual_coefs kmatrix dual_coefs,"['write', 'svr_loss', 'function', 'pythonencountered', 'problem', 'quantum', 'support', 'vector', 'machine', 'experiments', 'quantum', 'kernel', 'alignment', 'saw', 'example', 'given', 'qiskit', 'svc_loss', 'core', 'code', 'svcloss', 'sum_', 'a_i', 'sum_', 'j', 'a_i', 'a_j', 'y_', 'y_', 'j', 'k_', 'x_i', 'x_j', 'svc_loss', 'np', 'sum', 'np', 'abs', 'dual_coefs', 'dual_coefs', 'kmatrix', 'dual_coefs', 'wonder', 'loss', 'function', 'svr', 'looks', 'like', 'svrloss', 'sum_', 'a_i', 'sum_', 'j', 'a_i', 'a_j', 'y_', 'y_', 'j', 'k_', 'x_i', 'x_j', 'svr_loss', 'np', 'sum', 'np', 'abs', 'dual_coefs', 'dual_coefs', 'kmatrix', 'dual_coefs']","['write', 'svr_loss', 'function', 'pythonencount', 'problem', 'quantum', 'support', 'vector', 'machin', 'experi', 'quantum', 'kernel', 'align', 'saw', 'exampl', 'given', 'qiskit', 'svc_loss', 'core', 'code', 'svcloss', 'sum_', 'a_i', 'sum_', 'j', 'a_i', 'a_j', 'y_', 'y_', 'j', 'k_', 'x_i', 'x_j', 'svc_loss', 'np', 'sum', 'np', 'ab', 'dual_coef', 'dual_coef', 'kmatrix', 'dual_coef', 'wonder', 'loss', 'function', 'svr', 'look', 'like', 'svrloss', 'sum_', 'a_i', 'sum_', 'j', 'a_i', 'a_j', 'y_', 'y_', 'j', 'k_', 'x_i', 'x_j', 'svr_loss', 'np', 'sum', 'np', 'ab', 'dual_coef', 'dual_coef', 'kmatrix', 'dual_coef']"
147,160,160,14978092,65663428,jinja2.exceptions.UndefinedError: &#39;message&#39; is undefined,"<p>I have made the machine learning model which removes the background of image.I am trying to pass the message variable in index.html file but it throws an error. When I am printing the file name of the variable it is giving the name of it but when I render it to the index.html file, it throws an error.</p>
<pre class=""lang-py prettyprint-override""><code>import argparse
import os
import tqdm
import logging
from libs.strings import *
from libs.networks import model_detect
import libs.preprocessing as preprocessing
import libs.postprocessing as postprocessing

# Flask utils
from flask import Flask, redirect, url_for, request, render_template
from werkzeug.utils import secure_filename
from gevent.pywsgi import WSGIServer
import torch

# Define a flask app
app = Flask(__name__)



def __save_image_file__(img, file_name, output_path, wmode):
    &quot;&quot;&quot;
    Saves the PIL image to a file
    :param img: PIL image
    :param file_name: File name
    :param output_path: Output path
    :param wmode: Work mode
    &quot;&quot;&quot;
    # create output directory if it doesn't exist
    folder = os.path.dirname(output_path)
    if folder != '':
        os.makedirs(folder, exist_ok=True)
    if wmode == &quot;file&quot;:
        file_name_out = os.path.basename(output_path)
        if file_name_out == '':
            # Change file extension to png
            file_name = os.path.splitext(file_name)[0] + '.png'
            # Save image
            img.save(os.path.join(output_path, file_name))
            print(&quot;file name 1&quot;,file_name)
            # return render_template('index.html',message=file_name)
        else:
            try:
                # Save image
                img.save(output_path)
                print(&quot;file name 2&quot;,file_name)
                return render_template(&quot;index.html&quot;,message=file_name)

            except OSError as e:
                if str(e) == &quot;cannot write mode RGBA as JPEG&quot;:
                    raise OSError(&quot;Error! &quot;
                                    &quot;Please indicate the correct extension of the final file, for example: .png&quot;)
                else:
                    raise e
    else:
        # Change file extension to png
        file_name = os.path.splitext(file_name)[0] + '.png'
        # Save image
        img.save(os.path.join(output_path, file_name))
        # return render_template(&quot;index.html&quot;,message=file_name)




def cli(inp,oup):
    &quot;&quot;&quot;CLI&quot;&quot;&quot;
    
    parser = argparse.ArgumentParser(description=DESCRIPTION, usage=ARGS_HELP)
    parser.add_argument('-i',help=&quot;Path to input file or dir.&quot;, action=&quot;store&quot;,dest=&quot;input_path&quot;, default=inp)
    parser.add_argument('-o',help=&quot;Path to output file or dir.&quot;, action=&quot;store&quot;,dest=&quot;output_path&quot;, default=oup)
    parser.add_argument('-m', required=False,
                        help=&quot;Model name. Can be {} . U2NET is better to use.&quot;.format(MODELS_NAMES),
                        action=&quot;store&quot;, dest=&quot;model_name&quot;, default=&quot;u2net&quot;)
    parser.add_argument('-prep', required=False,
                        help=&quot;Preprocessing method. Can be {} . `bbd-fastrcnn` is better to use.&quot;
                        .format(PREPROCESS_METHODS),
                        action=&quot;store&quot;, dest=&quot;preprocessing_method_name&quot;, default=&quot;bbd-fastrcnn&quot;)
    parser.add_argument('-postp', required=False,
                        help=&quot;Postprocessing method. Can be {} .&quot;
                                &quot; `rtb-bnb` is better to use.&quot;.format(POSTPROCESS_METHODS),
                        action=&quot;store&quot;, dest=&quot;postprocessing_method_name&quot;, default=&quot;rtb-bnb&quot;)
    args = parser.parse_args()
    # Parse arguments
    input_path = args.input_path
    output_path = args.output_path
    model_name = args.model_name
    preprocessing_method_name = args.preprocessing_method_name
    postprocessing_method_name = args.postprocessing_method_name

    if model_name == &quot;test&quot;:
        print(input_path, output_path, model_name, preprocessing_method_name, postprocessing_method_name)
    else:
        process(input_path, output_path, model_name, preprocessing_method_name, postprocessing_method_name)


if __name__ == &quot;__main__&quot;:
    
    
    @app.route('/', methods=['GET'])
    def index():
        return render_template('index.html')
        
    @app.route('/input', methods=['GET','POST'])
    def result():
        if request.method == 'POST':
            file = request.files['file']
            basepath = os.path.dirname(__file__)
            print(file.filename)
            input_p = file.save(os.path.join(basepath,'uploads\input',secure_filename(file.filename)))
            #output_p = file.save(os.path.join(basepath,'uploads\output',secure_filename(file.filename)))
            cli((os.path.join(basepath,'uploads\input',secure_filename(file.filename))),
                                                                (os.path.join(basepath,'static\images',secure_filename(file.filename))) )
            userImage=os.path.join(basepath,'static\images',secure_filename(file.filename))
            print(userImage)
            return 'ok'

    app.run(debug=True)
</code></pre>
<p>In HTML file ,I get that message variable using below</p>
<pre class=""lang-html prettyprint-override""><code>&lt;img src=&quot;{{ url_for('static', filename= '/images/'+ message ) }}&quot; /&gt;   
</code></pre>
",186,2,-1,2,python;flask,2021-01-11 10:19:55,2021-01-11 10:19:55,2022-07-20 07:32:03,i have made the machine learning model which removes the background of image i am trying to pass the message variable in index html file but it throws an error  when i am printing the file name of the variable it is giving the name of it but when i render it to the index html file  it throws an error  in html file  i get that message variable using below,jinja exceptions undefinederror     message    is undefined,made machine learning model removes background image trying pass message variable index html file throws error printing file name variable giving name render index html file throws error html file get message variable using,jinja exceptions undefinederror message undefined,jinja exceptions undefinederror message undefinedmade machine learning model removes background image trying pass message variable index html file throws error printing file name variable giving name render index html file throws error html file get message variable using,"['jinja', 'exceptions', 'undefinederror', 'message', 'undefinedmade', 'machine', 'learning', 'model', 'removes', 'background', 'image', 'trying', 'pass', 'message', 'variable', 'index', 'html', 'file', 'throws', 'error', 'printing', 'file', 'name', 'variable', 'giving', 'name', 'render', 'index', 'html', 'file', 'throws', 'error', 'html', 'file', 'get', 'message', 'variable', 'using']","['jinja', 'except', 'undefinederror', 'messag', 'undefinedmad', 'machin', 'learn', 'model', 'remov', 'background', 'imag', 'tri', 'pass', 'messag', 'variabl', 'index', 'html', 'file', 'throw', 'error', 'print', 'file', 'name', 'variabl', 'give', 'name', 'render', 'index', 'html', 'file', 'throw', 'error', 'html', 'file', 'get', 'messag', 'variabl', 'use']"
148,161,161,17313581,73044648,Trying to predict the next number in cyphertext using tensorflow,"<p>I am experimenting with machine learning and I wanted to see how difficult it would be to predict a number given a series of other numbers. I have seen it accomplished with people making vectors such as 1-10. However, I wanted to try to do something more difficult. I wanted to do it based on the ciphertext. Here is what I have tried so far:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
#from sklearn.linear_model import LinearRegression
from tensorflow.keras import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.layers import Lambda, SimpleRNN
from tensorflow.keras import backend as K
from numpy.polynomial import polynomial as poly
from sklearn.feature_extraction import DictVectorizer

import Pyfhel

def generateInput(x, length):
    return np.append(x, [0 for i in range(length)], axis=0)

def main():

    HE = Pyfhel.Pyfhel()
    HE.contextGen(scheme='BFV', n=2048,  q=34, t=34, t_bits=35, sec=128)
    HE.keyGen()
    a = &quot;Hello&quot;
    a = np.asarray(bytearray(a, &quot;utf-8&quot;))
    a = HE.encode(a)
    ct = HE.encrypt(a).to_bytes('none')
    ct = np.asarray([c for c in ct])

    length = 100 # How many records to take into account
    batch_size = 1
    n_features = 1
    epochs = 1
    generator = TimeseriesGenerator(ct, ct, stride=length, length=length, batch_size=batch_size)
    model = Sequential()
    model.add(SimpleRNN(100, activation='leaky_relu', input_shape=(length, n_features)))
    model.add(Dense(100, activation='leaky_relu', input_shape=(length, n_features)))
    model.add(Dense(256, activation='softmax'))
    model.compile(optimizer='adam', loss=&quot;sparse_categorical_crossentropy&quot;, metrics=['accuracy'])
    history = model.fit(generator, epochs=epochs)


    for i in range(1, length):
        try:
            x_input = np.asarray(generateInput(ct[:i], length-len(ct[:i]))).reshape((1, length))
            yhat = model.predict(x_input).tolist()
            yhat_normalized = [float(i)/sum(yhat[0]) for i in yhat[0]]
            yhat_max = max(yhat_normalized)
            yhat_index = yhat_normalized.index(yhat_max)

            print(&quot;based on {} actual {} predicted {}&quot;.format(ct[:i], ct[i], yhat_index))
        except Exception as e:
            print(&quot;Error {}&quot;.format(e))
if __name__==&quot;__main__&quot;:
    main()
</code></pre>
<p>Now the problem is that all of my predictions are 0. Can anyone explain to me why this is happening? How can I fix this?</p>
<p>Here's what my current output looks like:</p>
<pre><code>based on [94] actual 161 predicted 0
based on [ 94 161] actual 16 predicted 0
based on [ 94 161  16] actual 3 predicted 0
based on [ 94 161  16   3] actual 7 predicted 0
based on [ 94 161  16   3   7] actual 0 predicted 0
based on [ 94 161  16   3   7   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0] actual 105 predicted 0
based on [ 94 161  16   3   7   0   0   0 105] actual 128 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0] actual 78 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78] actual 6 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6] actual 78 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78] actual 65 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65] actual 45 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45] actual 23 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23] actual 12 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12] actual 234 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234] actual 155 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155] actual 45 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45] actual 217 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217] actual 42 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42] actual 230 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230] actual 122 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122] actual 64 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64] actual 99 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99] actual 53 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53] actual 143 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143] actual 104 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104] actual 96 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96] actual 158 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158] actual 146 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0] actual 99 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99] actual 122 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122] actual 217 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217] actual 34 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34] actual 140 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140] actual 238 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238] actual 76 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76] actual 135 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135] actual 237 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0] actual 2 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0] actual 8 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0] actual 1 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0] actual 240 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240] actual 63 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63] actual 94 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94] actual 161 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161] actual 16 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16] actual 3 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3] actual 7 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0] actual 24 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24] actual 128 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0   0   0   0] actual 0 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0   0   0   0   0] actual 16 predicted 0
based on [ 94 161  16   3   7   0   0   0 105 128   0   0   0   0   0   0  78   6
  78  65  45  23  12 234 155  45 217  42 230 122  64  99  53 143 104  96
 158 146   0  99 122 217  34 140 238  76 135 237   0   2   0   0   0   0
   0   0   0   0   8   0   0   0   0   0   0   1   0   0   0   0   0   0
   0   0   0   0   0   0   0 240  63  94 161  16   3   7   0   0   0  24
 128   0   0   0   0   0   0   0  16] actual 0 predicted 0
</code></pre>
",25,0,0,3,python;tensorflow;keras,2022-07-20 03:05:01,2022-07-20 03:05:01,2022-07-20 04:12:17,i am experimenting with machine learning and i wanted to see how difficult it would be to predict a number given a series of other numbers  i have seen it accomplished with people making vectors such as    however  i wanted to try to do something more difficult  i wanted to do it based on the ciphertext  here is what i have tried so far  now the problem is that all of my predictions are   can anyone explain to me why this is happening  how can i fix this  here s what my current output looks like ,trying to predict the next number in cyphertext using tensorflow,experimenting machine learning wanted see difficult would predict number given series numbers seen accomplished people making vectors however wanted try something difficult wanted based ciphertext tried far problem predictions anyone explain happening fix current output looks like,trying predict next number cyphertext using tensorflow,trying predict next number cyphertext using tensorflowexperimenting machine learning wanted see difficult would predict number given series numbers seen accomplished people making vectors however wanted try something difficult wanted based ciphertext tried far problem predictions anyone explain happening fix current output looks like,"['trying', 'predict', 'next', 'number', 'cyphertext', 'using', 'tensorflowexperimenting', 'machine', 'learning', 'wanted', 'see', 'difficult', 'would', 'predict', 'number', 'given', 'series', 'numbers', 'seen', 'accomplished', 'people', 'making', 'vectors', 'however', 'wanted', 'try', 'something', 'difficult', 'wanted', 'based', 'ciphertext', 'tried', 'far', 'problem', 'predictions', 'anyone', 'explain', 'happening', 'fix', 'current', 'output', 'looks', 'like']","['tri', 'predict', 'next', 'number', 'cyphertext', 'use', 'tensorflowexperi', 'machin', 'learn', 'want', 'see', 'difficult', 'would', 'predict', 'number', 'given', 'seri', 'number', 'seen', 'accomplish', 'peopl', 'make', 'vector', 'howev', 'want', 'tri', 'someth', 'difficult', 'want', 'base', 'ciphertext', 'tri', 'far', 'problem', 'predict', 'anyon', 'explain', 'happen', 'fix', 'current', 'output', 'look', 'like']"
149,162,162,14826821,73042923,Docker [Errno 13] Permission denied error when running with --u=$(id -u $USER):$(id -g $USER),"<p>I try to run a docker container with a tkinter gui app in python and x11 forwarding for a machine learning application. When I run the image with</p>
<pre><code>sudo docker run --runtime=nvidia --gpus='all' --net=host --env=&quot;DISPLAY&quot; --volume=&quot;$HOME/.Xauthority:/root/.Xauthority:rw&quot; -i ganspace
</code></pre>
<p>I got the following display connection error, but the app seams to run</p>
<pre><code>Downloading https://drive.google.com/uc?export=download&amp;id=1FJRwzAkV-XWbxgTwxEmEACvuqF5DsBiV
Not cached
[19.07 19:59] Computing stylegan2-ffhq_style_ipca_c80_n1000000_w.npz
Reusing InstrumentedModel instance
Using W latent space
Feature shape: torch.Size([1, 512])
B=10000, N=1000000, dims=512, N/dims=1953.1
Sampling latents: 100%|| 101/101 [00:08&lt;00:00, 11.80it/s]
Fitting batches (NB=10000): 100%|##########| 100/100 [00:19&lt;00:00,  5.11it/s]Authorization required, but no authorization protocol specified
Authorization required, but no authorization protocol specified

Total time: 0:00:28.239383
Loaded components for ffhq from /ganspace/cache/components/stylegan2-ffhq_style_ipca_c80_n1000000_w.npz
Traceback (most recent call last):
  File &quot;interactive.py&quot;, line 645, in &lt;module&gt;
    setup_ui()
  File &quot;interactive.py&quot;, line 214, in setup_ui
    root = tk.Tk()
  File &quot;/opt/conda/envs/ganspace/lib/python3.7/tkinter/__init__.py&quot;, line 2023, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display &quot;:1&quot;
</code></pre>
<p>If I run the image in the following way with</p>
<pre><code>sudo docker run -u=$(id -u $USER):$(id -g $USER) -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw --rm -it --runtime=nvidia --gpus='all' ganspace
</code></pre>
<p>the display connection is working (I tested it with another tkinter image), but I got the following permission error due to the user I think</p>
<pre><code>Traceback (most recent call last):
  File &quot;interactive.py&quot;, line 644, in &lt;module&gt;
    setup_model()
  File &quot;interactive.py&quot;, line 143, in setup_model
    inst = get_instrumented_model(model_name, class_name, layer_name, torch.device('cuda'), use_w=args.use_w)
  File &quot;/opt/conda/envs/ganspace/lib/python3.7/functools.py&quot;, line 840, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File &quot;/ganspace/models/wrappers.py&quot;, line 695, in get_instrumented_model
    model = get_model(name, output_class, device, **kwargs)
  File &quot;/opt/conda/envs/ganspace/lib/python3.7/functools.py&quot;, line 840, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File &quot;/ganspace/models/wrappers.py&quot;, line 680, in get_model
    model = StyleGAN2(device, class_name=output_class)
  File &quot;/ganspace/models/wrappers.py&quot;, line 125, in __init__
    self.load_model()
  File &quot;/ganspace/models/wrappers.py&quot;, line 160, in load_model
    os.makedirs(checkpoint.parent, exist_ok=True)
  File &quot;/opt/conda/envs/ganspace/lib/python3.7/os.py&quot;, line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File &quot;/opt/conda/envs/ganspace/lib/python3.7/os.py&quot;, line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/ganspace/models/checkpoints'
ERROR conda.cli.main_run:execute(49): `conda run python interactive.py --model=StyleGAN2 --class=ffhq --layer=style --use_w -n=1_000_000 -b=10_000` failed. (See above for error)
</code></pre>
<p>Any ideas how to fix this or another solution to have the right permission and get the connection to the display correctly? thanks in advance!</p>
",27,1,-1,5,docker;tkinter;x11;display;permission-denied,2022-07-19 23:08:03,2022-07-19 23:08:03,2022-07-19 23:47:34,i try to run a docker container with a tkinter gui app in python and x forwarding for a machine learning application  when i run the image with i got the following display connection error  but the app seams to run if i run the image in the following way with the display connection is working  i tested it with another tkinter image   but i got the following permission error due to the user i think any ideas how to fix this or another solution to have the right permission and get the connection to the display correctly  thanks in advance ,docker  errno   permission denied error when running with   u   id  u  user    id  g  user ,try run docker container tkinter gui app python x forwarding machine learning application run image got following display connection error app seams run run image following way display connection working tested another tkinter image got following permission error due user think ideas fix another solution right permission get connection display correctly thanks advance,docker errno permission denied error running u id u user id g user,docker errno permission denied error running u id u user id g usertry run docker container tkinter gui app python x forwarding machine learning application run image got following display connection error app seams run run image following way display connection working tested another tkinter image got following permission error due user think ideas fix another solution right permission get connection display correctly thanks advance,"['docker', 'errno', 'permission', 'denied', 'error', 'running', 'u', 'id', 'u', 'user', 'id', 'g', 'usertry', 'run', 'docker', 'container', 'tkinter', 'gui', 'app', 'python', 'x', 'forwarding', 'machine', 'learning', 'application', 'run', 'image', 'got', 'following', 'display', 'connection', 'error', 'app', 'seams', 'run', 'run', 'image', 'following', 'way', 'display', 'connection', 'working', 'tested', 'another', 'tkinter', 'image', 'got', 'following', 'permission', 'error', 'due', 'user', 'think', 'ideas', 'fix', 'another', 'solution', 'right', 'permission', 'get', 'connection', 'display', 'correctly', 'thanks', 'advance']","['docker', 'errno', 'permiss', 'deni', 'error', 'run', 'u', 'id', 'u', 'user', 'id', 'g', 'usertri', 'run', 'docker', 'contain', 'tkinter', 'gui', 'app', 'python', 'x', 'forward', 'machin', 'learn', 'applic', 'run', 'imag', 'got', 'follow', 'display', 'connect', 'error', 'app', 'seam', 'run', 'run', 'imag', 'follow', 'way', 'display', 'connect', 'work', 'test', 'anoth', 'tkinter', 'imag', 'got', 'follow', 'permiss', 'error', 'due', 'user', 'think', 'idea', 'fix', 'anoth', 'solut', 'right', 'permiss', 'get', 'connect', 'display', 'correctli', 'thank', 'advanc']"
150,163,163,16538259,72714460,What is Undecidability and its practical application?,"<p>I have a vague understanding of undecidability. I get it that undecidability is that for a problem there exists no algorithm or the Turing machine will never halt. But I cant just visualize it. Can someone explain in a better way. and I don't get why we are learning about it and its applications. Can someone explain in this topic?</p>
",19,1,-1,1,computation-theory,2022-06-22 14:11:04,2022-06-22 14:11:04,2022-07-19 21:19:46,i have a vague understanding of undecidability  i get it that undecidability is that for a problem there exists no algorithm or the turing machine will never halt  but i cant just visualize it  can someone explain in a better way  and i don t get why we are learning about it and its applications  can someone explain in this topic ,what is undecidability and its practical application ,vague understanding undecidability get undecidability problem exists algorithm turing machine never halt cant visualize someone explain better way get learning applications someone explain topic,undecidability practical application,undecidability practical applicationvague understanding undecidability get undecidability problem exists algorithm turing machine never halt cant visualize someone explain better way get learning applications someone explain topic,"['undecidability', 'practical', 'applicationvague', 'understanding', 'undecidability', 'get', 'undecidability', 'problem', 'exists', 'algorithm', 'turing', 'machine', 'never', 'halt', 'cant', 'visualize', 'someone', 'explain', 'better', 'way', 'get', 'learning', 'applications', 'someone', 'explain', 'topic']","['undecid', 'practic', 'applicationvagu', 'understand', 'undecid', 'get', 'undecid', 'problem', 'exist', 'algorithm', 'ture', 'machin', 'never', 'halt', 'cant', 'visual', 'someon', 'explain', 'better', 'way', 'get', 'learn', 'applic', 'someon', 'explain', 'topic']"
151,164,164,15267044,73040021,I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error,"<pre><code>    2022-07-19 18:41:58.081967: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-07-19 18:41:58.082145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-07-19 18:42:01.316963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-07-19 18:42:01.317546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2022-07-19 18:42:01.318073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2022-07-19 18:42:01.318783: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2022-07-19 18:42:01.319328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found
2022-07-19 18:42:01.319857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found
2022-07-19 18:42:01.320492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2022-07-19 18:42:01.321261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2022-07-19 18:42:01.321404: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-07-19 18:42:01.321943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
 --------- Training --------- 
Weight for class fire : 0.84
Weight for class No_fire : 1.24
Found 8617 files belonging to 2 classes.
Using 6894 files for training.
Found 8617 files belonging to 2 classes.
Using 1723 files for validation.
Traceback (most recent call last):
  File &quot;C:\Users\Lenovo\Desktop\Hobiler\PythonDers\Programlar\SaDe\main.py&quot;, line 80, in &lt;module&gt;
    train_keras()
  File &quot;C:\Users\Lenovo\Desktop\Hobiler\PythonDers\Programlar\SaDe\training.py&quot;, line 114, in train_keras
    keras.utils.plot_model(model, show_shapes=True)
  File &quot;C:\Users\Lenovo\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\utils\vis_utils.py&quot;, line 421, in plot_model
    dot = model_to_dot(
  File &quot;C:\Users\Lenovo\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\utils\vis_utils.py&quot;, line 163, in model_to_dot
    raise ImportError(message)
ImportError: You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.

Process finished with exit code 1
</code></pre>
<p>I tried using</p>
<pre><code>pip install pydot
pip install pydotplus
pip install graphviz
</code></pre>
<p>commands. But still got this error.I'm new to machine learning and I would appreciate it if you could help me a bit.</p>
<p><a href=""https://github.com/AlirezaShamsoshoara/Fire-Detection-UAV-Aerial-Image-Classification-Segmentation-UnmannedAerialVehicle"" rel=""nofollow noreferrer"">Here</a> is the GitHub of the code.</p>
",32,1,0,3,matplotlib;keras;pydot,2022-07-19 18:49:59,2022-07-19 18:49:59,2022-07-19 20:35:44,i tried using commands  but still got this error i m new to machine learning and i would appreciate it if you could help me a bit   is the github of the code ,i   m getting this issue when trying to run the code i found on github  pydot and graphivz are installed but still getting this error,tried using commands still got error machine learning would appreciate could help bit github code,getting issue trying run code found github pydot graphivz installed still getting error,getting issue trying run code found github pydot graphivz installed still getting errortried using commands still got error machine learning would appreciate could help bit github code,"['getting', 'issue', 'trying', 'run', 'code', 'found', 'github', 'pydot', 'graphivz', 'installed', 'still', 'getting', 'errortried', 'using', 'commands', 'still', 'got', 'error', 'machine', 'learning', 'would', 'appreciate', 'could', 'help', 'bit', 'github', 'code']","['get', 'issu', 'tri', 'run', 'code', 'found', 'github', 'pydot', 'graphivz', 'instal', 'still', 'get', 'errortri', 'use', 'command', 'still', 'got', 'error', 'machin', 'learn', 'would', 'appreci', 'could', 'help', 'bit', 'github', 'code']"
152,165,165,13978354,73040793,"Insurance Dataset Linear Regression, PyTorch, loss is massive","<p>I am trying to build a Linear Regression method from scratch by following a tutorial called Zero to Gans.</p>
<p>I am using the Medical Cost dataset <a href=""https://www.kaggle.com/datasets/mirichoi0218/insurance"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/mirichoi0218/insurance</a></p>
<p>I first started off by changing all the non-numerical values to numerical ones.</p>
<pre><code>df[&quot;region&quot;].replace([&quot;northeast&quot;, &quot;southeast&quot;, &quot;southwest&quot;, &quot;northwest&quot;], [0,1,2,3], inplace = True)
</code></pre>
<p>I then split the data into input and output</p>
<pre><code>inputdf = df.iloc[:, 0:6]
outputdf = df.iloc[:, 6]
</code></pre>
<p>Converting all of these to Numpy Arrays and then to Tesnsors</p>
<pre><code>inputvalues = inputdf.to_numpy()
outputvalues = outputdf.to_numpy()
outputvalues = np.reshape(outputvalues, (1338, 1))
inputvalues = torch.from_numpy(inputvalues)
outputvalues = torch.from_numpy(outputvalues)
</code></pre>
<p>Created the weights and bias matrix and vector.</p>
<pre><code>weights = torch.randn(1,6, dtype=torch.float64, requires_grad = True)
bias = torch.randn(1, dtype=torch.float64, requires_grad=True)
</code></pre>
<p>Then created the actual model</p>
<pre><code>def linearegression(x, bias):
    return x @ weights.t() + bias
</code></pre>
<p>The the issue comes around here.</p>
<pre><code>def lossfunc(y, y_):
    delta = y - y_
    return torch.sum(delta * delta) / delta.numel()

loss = lossfunc(predictions, outputvalues)
print(loss)
</code></pre>
<p>When calculating the loss, I get something to 10^8 which is stupidly high for the loss. I am trying to do this by scratch, as I am new to machine learning and I want to knuckle down the basics. I fully understand that there are functions built into the PyTorch library which can create this model within 10 lines of code but I am trying to follow the first part of this Jovian Notebook <a href=""https://jovian.ai/aakashns/02-linear-regression"" rel=""nofollow noreferrer"">https://jovian.ai/aakashns/02-linear-regression</a> (however he using Linear Regression to calculate crop yield).</p>
<p>Does anyone know why my loss is so stupidly large?</p>
",24,0,1,3,python;pytorch;linear-regression,2022-07-19 19:52:42,2022-07-19 19:52:42,2022-07-19 19:52:42,i am trying to build a linear regression method from scratch by following a tutorial called zero to gans  i am using the medical cost dataset  i first started off by changing all the non numerical values to numerical ones  i then split the data into input and output converting all of these to numpy arrays and then to tesnsors created the weights and bias matrix and vector  then created the actual model the the issue comes around here  when calculating the loss  i get something to   which is stupidly high for the loss  i am trying to do this by scratch  as i am new to machine learning and i want to knuckle down the basics  i fully understand that there are functions built into the pytorch library which can create this model within  lines of code but i am trying to follow the first part of this jovian notebook   however he using linear regression to calculate crop yield   does anyone know why my loss is so stupidly large ,insurance dataset linear regression  pytorch  loss is massive,trying build linear regression method scratch following tutorial called zero gans using medical cost dataset first started changing non numerical values numerical ones split data input output converting numpy arrays tesnsors created weights bias matrix vector created actual model issue comes around calculating loss get something stupidly high loss trying scratch machine learning want knuckle basics fully understand functions built pytorch library create model within lines code trying follow first part jovian notebook however using linear regression calculate crop yield anyone know loss stupidly large,insurance dataset linear regression pytorch loss massive,insurance dataset linear regression pytorch loss massivetrying build linear regression method scratch following tutorial called zero gans using medical cost dataset first started changing non numerical values numerical ones split data input output converting numpy arrays tesnsors created weights bias matrix vector created actual model issue comes around calculating loss get something stupidly high loss trying scratch machine learning want knuckle basics fully understand functions built pytorch library create model within lines code trying follow first part jovian notebook however using linear regression calculate crop yield anyone know loss stupidly large,"['insurance', 'dataset', 'linear', 'regression', 'pytorch', 'loss', 'massivetrying', 'build', 'linear', 'regression', 'method', 'scratch', 'following', 'tutorial', 'called', 'zero', 'gans', 'using', 'medical', 'cost', 'dataset', 'first', 'started', 'changing', 'non', 'numerical', 'values', 'numerical', 'ones', 'split', 'data', 'input', 'output', 'converting', 'numpy', 'arrays', 'tesnsors', 'created', 'weights', 'bias', 'matrix', 'vector', 'created', 'actual', 'model', 'issue', 'comes', 'around', 'calculating', 'loss', 'get', 'something', 'stupidly', 'high', 'loss', 'trying', 'scratch', 'machine', 'learning', 'want', 'knuckle', 'basics', 'fully', 'understand', 'functions', 'built', 'pytorch', 'library', 'create', 'model', 'within', 'lines', 'code', 'trying', 'follow', 'first', 'part', 'jovian', 'notebook', 'however', 'using', 'linear', 'regression', 'calculate', 'crop', 'yield', 'anyone', 'know', 'loss', 'stupidly', 'large']","['insur', 'dataset', 'linear', 'regress', 'pytorch', 'loss', 'massivetri', 'build', 'linear', 'regress', 'method', 'scratch', 'follow', 'tutori', 'call', 'zero', 'gan', 'use', 'medic', 'cost', 'dataset', 'first', 'start', 'chang', 'non', 'numer', 'valu', 'numer', 'one', 'split', 'data', 'input', 'output', 'convert', 'numpi', 'array', 'tesnsor', 'creat', 'weight', 'bia', 'matrix', 'vector', 'creat', 'actual', 'model', 'issu', 'come', 'around', 'calcul', 'loss', 'get', 'someth', 'stupidli', 'high', 'loss', 'tri', 'scratch', 'machin', 'learn', 'want', 'knuckl', 'basic', 'fulli', 'understand', 'function', 'built', 'pytorch', 'librari', 'creat', 'model', 'within', 'line', 'code', 'tri', 'follow', 'first', 'part', 'jovian', 'notebook', 'howev', 'use', 'linear', 'regress', 'calcul', 'crop', 'yield', 'anyon', 'know', 'loss', 'stupidli', 'larg']"
153,166,166,1510267,73038901,Creating a Conda Environment taking too long,"<p>I have the latest version of Anaconda installed on my machine, 64bit. I have run the following command via VSCode:</p>
<pre class=""lang-py prettyprint-override""><code>conda env create -f 000_environment_setup/01_conda_environment.yml
</code></pre>
<p>Here is the <code>yaml</code> file contents:</p>
<pre class=""lang-yaml prettyprint-override""><code># DS4B 101-P: PYTHON FOR DATA SCIENCE AUTOMATION ----
# Environment Setup ----

# Conda Environment Setup Instructions ----
#  - Using an environment.yml file with conda
#  - Key Terminal Commands:
#
#    conda env create -f 000_environment_setup/01_conda_environment.yml
#    conda env update --file 000_environment_setup/01_conda_environment.yml --prune
#    conda env export --name ds4b_101p &gt; envname.yml
#    conda env remove --name ds4b_101p
#    conda env list

name: ds4b_101p
channels:
  - anaconda
  - conda-forge
  - defaults
dependencies:
  - python=3.7.1
  - pip
  - pip:
      # Core Data
      #- numpy==1.20.2
      - numpy
      - pandas==1.2.2

      # R Data
      - plydata==0.4.3
      - siuba==0.0.24
      - datatable

      # Visualization
      - matplotlib==3.3.4
      - plotnine==0.7.1
      - mizani==0.7.2
      - plotly==4.14.3
      - altair==4.1.0

      # EDA
      - pandas-profiling
      - ppscore==1.2.0
      - pyjanitor==0.20.14

      # Modeling &amp; Machine Learning
      - statsmodels
      - nltk==3.6.6
      - h2o==3.32.0.3
      - pycaret==2.3.0
      - scikit-learn==0.23.2
      - xgboost==0.90
      - lightgbm==3.1.1
      - catboost==0.24.4
      - sklearn-pandas==2.0.4
      - scikit-misc==0.1.3

      # Time Series
      - sktime==0.5.3
      - pmdarima==1.8.1
      - tsfresh==0.17.0

      # Scalability &amp; Automation
      - dask==2.30.0
      - dask-ml==1.8.0
      - dask-xgboost==0.1.11
      - zict==1.0.0
      - joblib==1.0.1

      # API
      - fastapi==0.65.2
      - uvicorn==0.13.4

      # Database
      - sqlalchemy==1.4.7

      # Excel
      - xlsxwriter==1.3.7
      - openpyxl

      # Jupyter
      - jupyterlab==3.0.17
      - jupyterlab-server==2.4.0
      - ipywidgets==7.6.3
      - ipympl==0.7.0
      - jupytext
      - papermill==2.3.3

      # Apps
      - streamlit==0.80.0

      # Terminal Formatting
      - rich

      # Extending Pandas
      - pandas_flavor

      # R users
      - radian
      - jedi==0.17.2

      # misc
      - bottleneck==1.3.2
</code></pre>
<p>In light of this seeming to never finish, is there something else I should be checking on?</p>
",24,0,0,3,python;anaconda;conda,2022-07-19 17:37:11,2022-07-19 17:37:11,2022-07-19 19:31:53,i have the latest version of anaconda installed on my machine  bit  i have run the following command via vscode  here is the yaml file contents  in light of this seeming to never finish  is there something else i should be checking on ,creating a conda environment taking too long,latest version anaconda installed machine bit run following command via vscode yaml file contents light seeming never finish something else checking,creating conda environment taking long,creating conda environment taking longlatest version anaconda installed machine bit run following command via vscode yaml file contents light seeming never finish something else checking,"['creating', 'conda', 'environment', 'taking', 'longlatest', 'version', 'anaconda', 'installed', 'machine', 'bit', 'run', 'following', 'command', 'via', 'vscode', 'yaml', 'file', 'contents', 'light', 'seeming', 'never', 'finish', 'something', 'else', 'checking']","['creat', 'conda', 'environ', 'take', 'longlatest', 'version', 'anaconda', 'instal', 'machin', 'bit', 'run', 'follow', 'command', 'via', 'vscode', 'yaml', 'file', 'content', 'light', 'seem', 'never', 'finish', 'someth', 'els', 'check']"
154,167,167,4833129,43530761,Cross validation with grid search returns worse results than default,"<p>I'm using scikitlearn in Python to run some basic machine learning models. Using the built in GridSearchCV() function, I determined the ""best"" parameters for different techniques, yet many of these perform worse than the defaults. I include the default parameters as an option, so I'm surprised this would happen.</p>

<p>For example:</p>

<pre><code>from sklearn import svm, grid_search
from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(verbose=1)
parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 1],  
              'min_samples_split':[2,5,10,20], 
              'max_depth':[2,3,5,10]}
clf = grid_search.GridSearchCV(gbc, parameters)
t0 = time()
clf.fit(X_crossval, labels)
print ""Gridsearch time:"", round(time() - t0, 3), ""s""
print clf.best_params_
# The output is: {'min_samples_split': 2, 'learning_rate': 0.01, 'max_depth': 2}
</code></pre>

<p>This is the same as the defaults, except max_depth is 3. When I use these parameters, I get an accuracy of 72%, compared to 78% from the default.</p>

<p>One thing I did, that I will admit is suspicious, is that I used my entire dataset for the cross validation. Then after obtaining the parameters, I ran it using the same dataset, split into 75-25 training/testing.</p>

<p>Is there a reason my grid search overlooked the ""superior"" defaults?</p>
",6994,2,15,5,python;machine-learning;scikit-learn;cross-validation;grid-search,2017-04-21 01:11:23,2017-04-21 01:11:23,2022-07-19 19:29:57,i m using scikitlearn in python to run some basic machine learning models  using the built in gridsearchcv   function  i determined the best parameters for different techniques  yet many of these perform worse than the defaults  i include the default parameters as an option  so i m surprised this would happen  for example  this is the same as the defaults  except max_depth is   when i use these parameters  i get an accuracy of    compared to   from the default  one thing i did  that i will admit is suspicious  is that i used my entire dataset for the cross validation  then after obtaining the parameters  i ran it using the same dataset  split into   training testing  is there a reason my grid search overlooked the superior defaults ,cross validation with grid search returns worse results than default,using scikitlearn python run basic machine learning models using built gridsearchcv function determined best parameters different techniques yet many perform worse defaults include default parameters option surprised would happen example defaults except max_depth use parameters get accuracy compared default one thing admit suspicious used entire dataset cross validation obtaining parameters ran using dataset split training testing reason grid search overlooked superior defaults,cross validation grid search returns worse results default,cross validation grid search returns worse results defaultusing scikitlearn python run basic machine learning models using built gridsearchcv function determined best parameters different techniques yet many perform worse defaults include default parameters option surprised would happen example defaults except max_depth use parameters get accuracy compared default one thing admit suspicious used entire dataset cross validation obtaining parameters ran using dataset split training testing reason grid search overlooked superior defaults,"['cross', 'validation', 'grid', 'search', 'returns', 'worse', 'results', 'defaultusing', 'scikitlearn', 'python', 'run', 'basic', 'machine', 'learning', 'models', 'using', 'built', 'gridsearchcv', 'function', 'determined', 'best', 'parameters', 'different', 'techniques', 'yet', 'many', 'perform', 'worse', 'defaults', 'include', 'default', 'parameters', 'option', 'surprised', 'would', 'happen', 'example', 'defaults', 'except', 'max_depth', 'use', 'parameters', 'get', 'accuracy', 'compared', 'default', 'one', 'thing', 'admit', 'suspicious', 'used', 'entire', 'dataset', 'cross', 'validation', 'obtaining', 'parameters', 'ran', 'using', 'dataset', 'split', 'training', 'testing', 'reason', 'grid', 'search', 'overlooked', 'superior', 'defaults']","['cross', 'valid', 'grid', 'search', 'return', 'wors', 'result', 'defaultus', 'scikitlearn', 'python', 'run', 'basic', 'machin', 'learn', 'model', 'use', 'built', 'gridsearchcv', 'function', 'determin', 'best', 'paramet', 'differ', 'techniqu', 'yet', 'mani', 'perform', 'wors', 'default', 'includ', 'default', 'paramet', 'option', 'surpris', 'would', 'happen', 'exampl', 'default', 'except', 'max_depth', 'use', 'paramet', 'get', 'accuraci', 'compar', 'default', 'one', 'thing', 'admit', 'suspici', 'use', 'entir', 'dataset', 'cross', 'valid', 'obtain', 'paramet', 'ran', 'use', 'dataset', 'split', 'train', 'test', 'reason', 'grid', 'search', 'overlook', 'superior', 'default']"
155,168,168,19572165,73039563,Implementing My Flask POST METHOD API into my site,"<p>After 4 days of learning FLASK and POST, I have finally got my virtual machine to accept POST requests with the following code.</p>
<pre><code>from flask import Flask, jsonify, request

app = Flask(__name__)

accounts = [
    {'name': &quot;Billy&quot;, 'balance': 450.0},
    {'name': &quot;Kelly&quot;, 'balance': 250.0}
     ]


@app.route(&quot;/accounts&quot;, methods=[&quot;GET&quot;])
def get_accounts():
    return jsonify(accounts)
    
    
@app.route(&quot;/account/&lt;id&gt;&quot;, methods=[&quot;GET&quot;])
def get_account(id):
    id = int(id) - 1
    return jsonify(accounts[id])
    
@app.route(&quot;/account&quot;, methods=[&quot;POST&quot;])
def add_account():
    name = request.json['name']
    balance = request.json['balance']
    data = {'name': name, 'balance': balance}
    accounts.append(data)
    
    return jsonify(data)    
    
    
if __name__ == '__main__':
    app.run(port=8080)
</code></pre>
<p>I am running a tor_service onion website on my ubuntu server, so i have replicated the steps i took to get the above code working and now the pyrest folder is within my tor_service folder.</p>
<p>I am using request to pull these variables from my index.html via html form</p>
<pre><code>&lt;?php $product=$_REQUEST['product']; echo $product ?&gt;
&lt;?php $name=$_REQUEST['name']; echo $name ?&gt;                
&lt;?php $name=$_REQUEST['address']; echo $name ?&gt; 
&lt;?php $name=$_REQUEST['price']; echo file_get_contents(&quot;https://blockchain.info/tobtc?currency=USD&amp;value=&quot;.$name);?&gt; 
</code></pre>
<p>I am guessing i can just include ALL files from the Pyrest folder and adjust the variables within the api.py file which is the bulk code above.</p>
<p>My page where the data is/will be displayed is pay.php so my guess is this is what i need.</p>
<pre><code>from flask import Flask, jsonify, request

app = Flask(__name__)
        
@app.route(&quot;/pay.php&quot;, methods=[&quot;POST&quot;])
def add_account():
    product = request.json['product']
    name = request.json['name']
    address = request.json['address']
    price = request.json['price']


    data = {'product': product, 'name': name, 'address': address, 'price': price}
    accounts.append(data)
    
    return jsonify(data)    
    
    
if __name__ == '__main__':
    app.run(port=8080)
</code></pre>
",18,0,0,5,python;python-3.x;ubuntu;curl;server,2022-07-19 18:18:43,2022-07-19 18:18:43,2022-07-19 18:18:43,after  days of learning flask and post  i have finally got my virtual machine to accept post requests with the following code  i am running a tor_service onion website on my ubuntu server  so i have replicated the steps i took to get the above code working and now the pyrest folder is within my tor_service folder  i am using request to pull these variables from my index html via html form i am guessing i can just include all files from the pyrest folder and adjust the variables within the api py file which is the bulk code above  my page where the data is will be displayed is pay php so my guess is this is what i need ,implementing my flask post method api into my site,days learning flask post finally got virtual machine accept post requests following code running tor_service onion website ubuntu server replicated steps took get code working pyrest folder within tor_service folder using request pull variables index html via html form guessing include files pyrest folder adjust variables within api py file bulk code page data displayed pay php guess need,implementing flask post method api site,implementing flask post method api sitedays learning flask post finally got virtual machine accept post requests following code running tor_service onion website ubuntu server replicated steps took get code working pyrest folder within tor_service folder using request pull variables index html via html form guessing include files pyrest folder adjust variables within api py file bulk code page data displayed pay php guess need,"['implementing', 'flask', 'post', 'method', 'api', 'sitedays', 'learning', 'flask', 'post', 'finally', 'got', 'virtual', 'machine', 'accept', 'post', 'requests', 'following', 'code', 'running', 'tor_service', 'onion', 'website', 'ubuntu', 'server', 'replicated', 'steps', 'took', 'get', 'code', 'working', 'pyrest', 'folder', 'within', 'tor_service', 'folder', 'using', 'request', 'pull', 'variables', 'index', 'html', 'via', 'html', 'form', 'guessing', 'include', 'files', 'pyrest', 'folder', 'adjust', 'variables', 'within', 'api', 'py', 'file', 'bulk', 'code', 'page', 'data', 'displayed', 'pay', 'php', 'guess', 'need']","['implement', 'flask', 'post', 'method', 'api', 'siteday', 'learn', 'flask', 'post', 'final', 'got', 'virtual', 'machin', 'accept', 'post', 'request', 'follow', 'code', 'run', 'tor_servic', 'onion', 'websit', 'ubuntu', 'server', 'replic', 'step', 'took', 'get', 'code', 'work', 'pyrest', 'folder', 'within', 'tor_servic', 'folder', 'use', 'request', 'pull', 'variabl', 'index', 'html', 'via', 'html', 'form', 'guess', 'includ', 'file', 'pyrest', 'folder', 'adjust', 'variabl', 'within', 'api', 'py', 'file', 'bulk', 'code', 'page', 'data', 'display', 'pay', 'php', 'guess', 'need']"
156,169,169,1367124,70851048,Does it make sense to use Conda + Poetry?,"<p>Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:</p>
<p>As far as I understand, <strong>Conda</strong> and <strong>Poetry</strong> have different purposes but are largely redundant:</p>
<ul>
<li>Conda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.</li>
<li>Poetry is primarily a Python package manager (say, an upgrade of <strong>pip</strong>), but it can also create and manage Python environments (say, an upgrade of <strong>Pyenv</strong>).</li>
</ul>
<p>My idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.</p>
<p>I've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like <code>poetry shell</code> or <code>poetry run</code>, only <code>poetry init</code>, <code>poetry install</code> etc (after activating the Conda environment).</p>
<p>For full disclosure, my <em>environment.yml</em> file (for Conda) looks like this:</p>
<pre><code>name: N

channels:
  - defaults
  - conda-forge

dependencies:
  - python=3.9
  - cudatoolkit
  - cudnn
</code></pre>
<p>and my <em>poetry.toml</em> file looks like that:</p>
<pre><code>[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>To be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.</p>
<p>Does this project design look reasonable to you?</p>
",10144,1,37,5,python;machine-learning;package;conda;python-poetry,2022-01-25 17:09:43,2022-01-25 17:09:43,2022-07-19 18:12:40,does it make sense to use conda   poetry for a machine learning project  allow me to share my  novice  understanding and please correct or enlighten me  as far as i understand  conda and poetry have different purposes but are largely redundant  my idea is to use both and compartmentalize their roles  let conda be the environment manager and poetry the package manager  my reasoning is that  it sounds like  conda is best for managing environments and can be used for compiling and installing non python packages  especially cuda drivers  for gpu capability   while poetry is more powerful than conda as a python package manager  i ve managed to make this work fairly easily by using poetry within a conda environment  the trick is to not use poetry to manage the python environment  i m not using commands like poetry shell or poetry run  only poetry init  poetry install etc  after activating the conda environment   for full disclosure  my environment yml file  for conda  looks like this  and my poetry toml file looks like that  to be honest  one of the reasons i proceeded this way is that i was struggling to install cuda  for gpu support  without conda  does this project design look reasonable to you ,does it make sense to use conda   poetry ,make sense use conda poetry machine learning project allow share novice understanding please correct enlighten far understand conda poetry different purposes largely redundant idea use compartmentalize roles let conda environment manager poetry package manager reasoning sounds like conda best managing environments used compiling installing non python packages especially cuda drivers gpu capability poetry powerful conda python package manager managed make work fairly easily using poetry within conda environment trick use poetry manage python environment using commands like poetry shell poetry run poetry init poetry install etc activating conda environment full disclosure environment yml file conda looks like poetry toml file looks like honest one reasons proceeded way struggling install cuda gpu support without conda project design look reasonable,make sense use conda poetry,make sense use conda poetrymake sense use conda poetry machine learning project allow share novice understanding please correct enlighten far understand conda poetry different purposes largely redundant idea use compartmentalize roles let conda environment manager poetry package manager reasoning sounds like conda best managing environments used compiling installing non python packages especially cuda drivers gpu capability poetry powerful conda python package manager managed make work fairly easily using poetry within conda environment trick use poetry manage python environment using commands like poetry shell poetry run poetry init poetry install etc activating conda environment full disclosure environment yml file conda looks like poetry toml file looks like honest one reasons proceeded way struggling install cuda gpu support without conda project design look reasonable,"['make', 'sense', 'use', 'conda', 'poetrymake', 'sense', 'use', 'conda', 'poetry', 'machine', 'learning', 'project', 'allow', 'share', 'novice', 'understanding', 'please', 'correct', 'enlighten', 'far', 'understand', 'conda', 'poetry', 'different', 'purposes', 'largely', 'redundant', 'idea', 'use', 'compartmentalize', 'roles', 'let', 'conda', 'environment', 'manager', 'poetry', 'package', 'manager', 'reasoning', 'sounds', 'like', 'conda', 'best', 'managing', 'environments', 'used', 'compiling', 'installing', 'non', 'python', 'packages', 'especially', 'cuda', 'drivers', 'gpu', 'capability', 'poetry', 'powerful', 'conda', 'python', 'package', 'manager', 'managed', 'make', 'work', 'fairly', 'easily', 'using', 'poetry', 'within', 'conda', 'environment', 'trick', 'use', 'poetry', 'manage', 'python', 'environment', 'using', 'commands', 'like', 'poetry', 'shell', 'poetry', 'run', 'poetry', 'init', 'poetry', 'install', 'etc', 'activating', 'conda', 'environment', 'full', 'disclosure', 'environment', 'yml', 'file', 'conda', 'looks', 'like', 'poetry', 'toml', 'file', 'looks', 'like', 'honest', 'one', 'reasons', 'proceeded', 'way', 'struggling', 'install', 'cuda', 'gpu', 'support', 'without', 'conda', 'project', 'design', 'look', 'reasonable']","['make', 'sens', 'use', 'conda', 'poetrymak', 'sens', 'use', 'conda', 'poetri', 'machin', 'learn', 'project', 'allow', 'share', 'novic', 'understand', 'pleas', 'correct', 'enlighten', 'far', 'understand', 'conda', 'poetri', 'differ', 'purpos', 'larg', 'redund', 'idea', 'use', 'compartment', 'role', 'let', 'conda', 'environ', 'manag', 'poetri', 'packag', 'manag', 'reason', 'sound', 'like', 'conda', 'best', 'manag', 'environ', 'use', 'compil', 'instal', 'non', 'python', 'packag', 'especi', 'cuda', 'driver', 'gpu', 'capabl', 'poetri', 'power', 'conda', 'python', 'packag', 'manag', 'manag', 'make', 'work', 'fairli', 'easili', 'use', 'poetri', 'within', 'conda', 'environ', 'trick', 'use', 'poetri', 'manag', 'python', 'environ', 'use', 'command', 'like', 'poetri', 'shell', 'poetri', 'run', 'poetri', 'init', 'poetri', 'instal', 'etc', 'activ', 'conda', 'environ', 'full', 'disclosur', 'environ', 'yml', 'file', 'conda', 'look', 'like', 'poetri', 'toml', 'file', 'look', 'like', 'honest', 'one', 'reason', 'proceed', 'way', 'struggl', 'instal', 'cuda', 'gpu', 'support', 'without', 'conda', 'project', 'design', 'look', 'reason']"
157,170,170,19579794,73038348,Recoding ShuffleSplit from sklearn,"<p>I am fairly new to Machine Learning and currently I am going through the book 'Hands-on
Machine Learning with Scikit-Learn, Keras &amp; TensorFlow' from O'Reilly.
In exercise 8 of chapter 6 (<a href=""https://i.stack.imgur.com/YBVHD.png"" rel=""nofollow noreferrer"">here is the full exercise</a>) the goal is to fit a random forest classifier on the training set of the moons dataset. For that it is asked to generate 1,000 subsets of the training, each containing 100 instances selected randomly. This can be done by means of the ShuffleSplit method from scikit learn. When I do so I also get nice results. However, I tried to implement the ShuffleSplit method myself but something goes wrong and I can't figure out what it is.</p>
<p>First I do a small Grid Search to find a good parameter setting for fitting one single decision tree:</p>
<pre><code>from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier 
import numpy as np

np.random.seed(1234)
X, y = make_moons(n_samples=10000, noise=0.4)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)


param_dic = {'max_leaf_nodes': list(range(2, 100)),
            'min_samples_split': [2,3,4]}


dtc = DecisionTreeClassifier()
dtc_grid = GridSearchCV(dtc, param_dic, cv=5, scoring='accuracy')
dtc_grid.fit(X_train, y_train)
best_params = dtc_grid.best_params_
dtc.set_params(**best_params)
dtc.fit(X_train, y_train)
predictions = dtc.predict(X_train)
print(f'Accuracy Score on Train Set: {accuracy_score(y_train, predictions)}')
predictions = dtc.predict(X_test)
print(f'Accuracy Score on Test Set: {accuracy_score(y_test, predictions)}')
# Accuracy Score on Train Set: 0.855875
# Accuracy Score on Test Set: 0.8525
</code></pre>
<p>Now I want to create 1000 random subsets of the training set of size 100. For that I create the following class:</p>
<pre><code>from sklearn.base import TransformerMixin, BaseEstimator
import random 

class CreateSubsets(TransformerMixin, BaseEstimator):
    def __init__(self, n_sets, set_size):
        self.n_sets = n_sets
        self.set_size = set_size
        
    def fit(self, X, y):
        return self
    
    def transform(self, X):
        self.new_sets = {}
        for set_i in range(self.n_sets):
            idx = random.choices(range(len(X)), k=self.set_size) 
            X_new, y_new= X[idx], y[idx]
            self.new_sets[set_i] = (X_new, y_new)
        return self.new_sets    
 
n_sets, set_size = 1000, 100
cs = CreateSubsets(n_sets, set_size)
new_sets = cs.fit_transform(X_train,y_train)  
</code></pre>
<p>Next I fit the decision tree classifier with the optimal hyperparameter setting found by GridSearchCV on each of the small random datasets and predict labels of the test set observations. Those predictions are saved in <code>tree_predictions</code> for later purpose. Finally, the accuracy score is predicted for each fit and those values are averaged.</p>
<pre><code>dtc = DecisionTreeClassifier()
dtc.set_params(**best_params)
tree_predictions = np.zeros((n_sets, len(X_test)))
accuracy_scores = []
for set_i in range(n_sets):
    X_train_new, y_train_new = new_sets[set_i][0], new_sets[set_i][1]
    dtc.fit(X_train_new, y_train_new)
    predictions = dtc.predict(X_test)
    tree_predictions[set_i,:] = predictions
    accuracy_scores.append(accuracy_score(y_test, predictions))

print(f' Average performance of Decision Tree Classifier with optimal Hyperparameter Setting,\n fitted on smaller subsets: {np.mean(accuracy_scores)}')    
# Average performance of Decision Tree Classifier with optimal Hyperparameter Settings, fitted on smaller subsets: 0.4941225
</code></pre>
<p>As a final step, for each instance of the test set I do a majority vote over the predictions (by taking the median over the predictions per test observation). Computing the accuracy score on those majority predictions should be higher than the average prediction score prior.</p>
<pre><code>from scipy.stats import mode
predictions = mode(tree_predictions, axis=0)
accuracy_score(y_test, predictions[0].reshape(-1))
#0.403
</code></pre>
<p>Something certainly goes wrong with the class CreateSubsets, but I can't figure out what it is. If I use ShuffleSplit from sklearn like this instead, the random forest classifier performs well:</p>
<pre><code>from sklearn.model_selection import ShuffleSplit

n_sets, set_size = 1000, 100
new_sets = []

rs = ShuffleSplit(n_splits=n_sets, test_size=len(X_train) - set_size)
for mini_train_index, mini_test_index in rs.split(X_train):
    X_mini_train = X_train[mini_train_index]
    y_mini_train = y_train[mini_train_index]
    new_sets.append((X_mini_train, y_mini_train))
</code></pre>
",23,0,0,5,python;class;machine-learning;scikit-learn;random-forest,2022-07-19 17:00:23,2022-07-19 17:00:23,2022-07-19 17:04:17,first i do a small grid search to find a good parameter setting for fitting one single decision tree  now i want to create  random subsets of the training set of size   for that i create the following class  next i fit the decision tree classifier with the optimal hyperparameter setting found by gridsearchcv on each of the small random datasets and predict labels of the test set observations  those predictions are saved in tree_predictions for later purpose  finally  the accuracy score is predicted for each fit and those values are averaged  as a final step  for each instance of the test set i do a majority vote over the predictions  by taking the median over the predictions per test observation   computing the accuracy score on those majority predictions should be higher than the average prediction score prior  something certainly goes wrong with the class createsubsets  but i can t figure out what it is  if i use shufflesplit from sklearn like this instead  the random forest classifier performs well ,recoding shufflesplit from sklearn,first small grid search find good parameter setting fitting one single decision tree want create random subsets training set size create following class next fit decision tree classifier optimal hyperparameter setting found gridsearchcv small random datasets predict labels test set observations predictions saved tree_predictions later purpose finally accuracy score predicted fit values averaged final step instance test set majority vote predictions taking median predictions per test observation computing accuracy score majority predictions higher average prediction score prior something certainly goes wrong class createsubsets figure use shufflesplit sklearn like instead random forest classifier performs well,recoding shufflesplit sklearn,recoding shufflesplit sklearnfirst small grid search find good parameter setting fitting one single decision tree want create random subsets training set size create following class next fit decision tree classifier optimal hyperparameter setting found gridsearchcv small random datasets predict labels test set observations predictions saved tree_predictions later purpose finally accuracy score predicted fit values averaged final step instance test set majority vote predictions taking median predictions per test observation computing accuracy score majority predictions higher average prediction score prior something certainly goes wrong class createsubsets figure use shufflesplit sklearn like instead random forest classifier performs well,"['recoding', 'shufflesplit', 'sklearnfirst', 'small', 'grid', 'search', 'find', 'good', 'parameter', 'setting', 'fitting', 'one', 'single', 'decision', 'tree', 'want', 'create', 'random', 'subsets', 'training', 'set', 'size', 'create', 'following', 'class', 'next', 'fit', 'decision', 'tree', 'classifier', 'optimal', 'hyperparameter', 'setting', 'found', 'gridsearchcv', 'small', 'random', 'datasets', 'predict', 'labels', 'test', 'set', 'observations', 'predictions', 'saved', 'tree_predictions', 'later', 'purpose', 'finally', 'accuracy', 'score', 'predicted', 'fit', 'values', 'averaged', 'final', 'step', 'instance', 'test', 'set', 'majority', 'vote', 'predictions', 'taking', 'median', 'predictions', 'per', 'test', 'observation', 'computing', 'accuracy', 'score', 'majority', 'predictions', 'higher', 'average', 'prediction', 'score', 'prior', 'something', 'certainly', 'goes', 'wrong', 'class', 'createsubsets', 'figure', 'use', 'shufflesplit', 'sklearn', 'like', 'instead', 'random', 'forest', 'classifier', 'performs', 'well']","['recod', 'shufflesplit', 'sklearnfirst', 'small', 'grid', 'search', 'find', 'good', 'paramet', 'set', 'fit', 'one', 'singl', 'decis', 'tree', 'want', 'creat', 'random', 'subset', 'train', 'set', 'size', 'creat', 'follow', 'class', 'next', 'fit', 'decis', 'tree', 'classifi', 'optim', 'hyperparamet', 'set', 'found', 'gridsearchcv', 'small', 'random', 'dataset', 'predict', 'label', 'test', 'set', 'observ', 'predict', 'save', 'tree_predict', 'later', 'purpos', 'final', 'accuraci', 'score', 'predict', 'fit', 'valu', 'averag', 'final', 'step', 'instanc', 'test', 'set', 'major', 'vote', 'predict', 'take', 'median', 'predict', 'per', 'test', 'observ', 'comput', 'accuraci', 'score', 'major', 'predict', 'higher', 'averag', 'predict', 'score', 'prior', 'someth', 'certainli', 'goe', 'wrong', 'class', 'createsubset', 'figur', 'use', 'shufflesplit', 'sklearn', 'like', 'instead', 'random', 'forest', 'classifi', 'perform', 'well']"
158,171,171,14315579,73035682,Tensorforce - Agent Training with Custom Environment,"<p>I am currently working on a university reinforcement learning project with Tensorforce.</p>
<p><strong>The setting</strong> is as follows: In a production line are 8 machines and we have a total of 40 measures that can be implemented to optimize those machines (5 measures per machine). Each measure improves the quality factor of a specific machine and has a certain cost. The goal is to select 10 measures out of the 40 that maximize the formula &quot;<em>Quality factor Machine 1 * Quality Factor Machine 2 * ... * Quality Factor Machine 8 - Total Costs of the 10 implemented measures</em>&quot; --&gt; this is also going to be the reward function.</p>
<p><strong>The goal</strong> of the reinforcement agent is to learn which <strong>sequence of measures</strong> optimizes the abovementioned function. The order here is important as the quality factors on that machine are being overriden, eg. first measure chosen updates the qualityfactor of machine 1 to 0.98 and the second measure updates it to 0.97, then the reward will be lower! Hence, the agent has to learn to implement worse measures first before updating to the best one. For 8 machines and 10 decisions, there will be machines that have to be updated twice!
However, when the same measure is chosen twice, a reward of -1 is given.</p>
<p>I created a custom environment that looks like this below. The state is defined by the 8 quality factors of each machine and the actions are the 40 measures to choose from.</p>
<pre><code>class SimulationEnvironment(Environment):
    def __init__(self):
        super().__init__()
        self.SimulationModel = SimulationModel() # 8 machines, 40 measures, 10 decisions
        self.NUM_ACTIONS = len(self.SimulationModel.actions)
        self.finished = False
        self.episode_end = False
        self.STATES_SIZE = len(self.SimulationModel.state)
        self.max_step_per_episode = 10
    

    def states(self):
        return dict(type=&quot;float&quot;, shape=(self.STATES_SIZE,))

    def actions(self):
        return {
            &quot;measure&quot;: dict(type=&quot;int&quot;, num_values=self.NUM_ACTIONS),
        }

    def max_episode_timesteps(self):
     return 10

    def close(self):
        super().close()

    def reset(self):
        self.SimulationModel = SimulationModel()
        state = np.array([0.8,0.88,0.88,0.88,0.88,0.88,0.88,0.88]) # initial quality factors of each machine
        return state

    def execute(self, actions):
        reward = 0
        next_state, terminal, reward = self.SimulationModel.get_nextState(actions)
        return next_state, terminal, reward
</code></pre>
<p>The SimulationModel (where the function get_nextState is defined looks like this:</p>
<pre><code>class SimulationModel:
def __init__(self):
    &quot;&quot;&quot;
    Constants
    &quot;&quot;&quot;
    self.num_machines = 8 #number of machines
    self.num_measures = 40 #number of measures
    self.num_decisions = 10 #number of decisions to be made
    self.initial_costs = np.array([[0],[0],[0],[0],[0],[0],[0],[0]]) # Initial Costs
    self.initial_quality = np.array([[0.8],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88]]) # Initial Quality Vector
    self.costs = np.array([[10],[3],[50],[22],[10], # new costs for each measure, machine 1
                            [5],[1.5],[37.5],[20],[7], # machine 2
                            [5],[1.5],[37.5],[20],[6], # machine 3
                            [7],[2],[20],[19],[8], # machine 4
                            [2],[23],[19],[12],[5], # machine 5
                            [6],[15],[15],[35],[4], # machine 6
                            [2],[37.5],[36],[12],[5], # machine 7
                            [6],[15],[20],[37.5],[4]]) # machine 8
    self.quality = np.array([[0.85],[0.92],[0.97],[0.98],[0.99], # new costs for each measure, machine 1
                            [0.9],[0.92],[0.97],[0.98],[0.99], # machine 2
                            [0.9],[0.92],[0.97],[0.98],[0.99], # machine 3
                            [0.9],[0.92],[0.96],[0.98],[0.99], # machine 4
                            [0.9],[0.96],[0.98],[0.99],[0.99], # machine 5
                            [0.9],[0.92],[0.94],[0.96],[0.99], # machine 6
                            [0.9],[0.97],[0.98],[0.99],[0.99], # machine 7
                            [0.9],[0.92],[0.94],[0.97],[0.99]]) # machine 8

    &quot;&quot;&quot;
    Variables
    &quot;&quot;&quot;
    self.decisions_made = 0 #Number of decisions that have been made, has to be smaller or 
equal than num_decisions

    &quot;&quot;&quot;
    DYNAMICS
    Machine Configurations and OEE(REWARD) will evolve at every timestep.
    &quot;&quot;&quot;
    self.configuration = np.append(self.initial_costs, self.initial_quality, axis=1)
    self.OEE_previous = 32.69 #starting OEE
    
   
    &quot;&quot;&quot;
    ACTIONS:
    Action vec for RL Process Times and OEE
    &quot;&quot;&quot;
    self.timestep = 0 # init timestep
    self.timestep_max = self.num_decisions # Max number of timesteps per episode is number of 
 decisions to be taken

    # Represent the actions : Choose one measure
    self.actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,
                    21,22,23,24,25,26,27,28,28,30,31,32,33,34,35,36,37,38,39]
    
    &quot;&quot;&quot;
    Observations
    States vec for RL. Initially, no tasks are chosen
    &quot;&quot;&quot;
    self.measurestaken = [0,0,0,0,0,0,0,0,0,0, # eg. when measure 3 is selected, index 3 will 
                          0,0,0,0,0,0,0,0,0,0, # be set to 1
                          0,0,0,0,0,0,0,0,0,0,
                          0,0,0,0,0,0,0,0,0,0] 
    self.state = np.array([[0.8],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88]])

'''
Get_initial_state: Reset the environment state for the new batch of measures
'''   
def get_initial_state(self):
    state = np.array([[0.8],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88],[0.88]]) 
    reward = 0
    return state, reward

'''
    This function defines the logic behind changing the states and deciding rewards based on 
the input provided by agent in terms of action.
'''
def get_nextState(self, action):
    done = False

    #case 1, if same task selected then penalize the agent
    if self.decisions_made &lt; self.num_decisions:
        if self.measurestaken[action['measure']] == 1:
            reward = -1
            self.decisions_made = self.decisions_made + 1
            self.OEE_previous = reward
            return self.state, done, reward
        else:
            self.measurestaken[action['measure']] = 1
    else: # 10 measures already chosen
        reward = self.OEE_previous
        done = True
        return self.state, done, reward
  
    # if measure is between 0 and 4, machine 1 has to be updated and so on
    # costs are being added every time, quality factors is being updated
    if action['measure'] &lt; 5:
        self.configuration[0][0] = self.configuration[0][0] + self.costs[action['measure']]
        self.configuration[0][1] = self.quality[action['measure']]
    elif action['measure'] &gt;= 5 and action['measure'] &lt; 10:
        self.configuration[1][0] = self.configuration[1][0] + self.costs[action['measure']]
        self.configuration[1][1] = self.quality[action['measure']] 
    elif action['measure'] &gt;= 10 and action['measure'] &lt; 15:
        self.configuration[2][0] = self.configuration[2][0] + self.costs[action['measure']]
        self.configuration[2][1] = self.quality[action['measure']] 
    elif action['measure'] &gt;= 15 and action['measure'] &lt; 20:
        self.configuration[3][0] = self.configuration[3][0] + self.costs[action['measure']]
        self.configuration[3][1] = self.quality[action['measure']] 
    elif action['measure'] &gt;= 20 and action['measure'] &lt; 25:
        self.configuration[4][0] = self.configuration[4][0] + self.costs[action['measure']]
        self.configuration[4][1] = self.quality[action['measure']] 
    elif action['measure'] &gt;= 25 and action['measure'] &lt; 30:
        self.configuration[5][0] = self.configuration[5][0] + self.costs[action['measure']]
        self.configuration[5][1] = self.quality[action['measure']] 
    elif action['measure'] &gt;= 30 and action['measure'] &lt; 35:
        self.configuration[6][0] = self.configuration[6][0] + self.costs[action['measure']]
        self.configuration[6][1] = self.quality[action['measure']] 
    else:
        self.configuration[7][0] = self.configuration[7][0] + self.costs[action['measure']]
        self.configuration[7][1] = self.quality[action['measure']]

    # compute the reward (OEE)
    reward =  self.compute_OEE(self.configuration)
    self.OEE_previous = reward
    self.decisions_made = self.decisions_made + 1
    self.state = self.configuration[:,1] #new state are the 8 quality factor values         
    return self.state,done,reward

'''
 Return the OEE (reward)
'''        
def compute_OEE(self, configuration):
    OEE = 100 * (configuration[0][1] * configuration[1][1] * configuration[2][1] * 
configuration[3][1] * 
                configuration[4][1] * configuration[5][1] * configuration[6][1] * 
configuration[7][1]) - 0.1 * (configuration[0][0] + configuration[1][0] + configuration[2][0] 
+ 
configuration[3][0] + 
                configuration[4][0] + configuration[5][0] + configuration[6][0] + 
configuration[7][0])
    return OEE
</code></pre>
<p>Now, here is the main function where the problem lies:</p>
<pre><code>from env.SimulationModel import SimulationModel
from env.SimulationEnv import SimulationEnvironment
from tensorforce import Agent

def main():
    # Instantiate our environment &amp; Tensorforce Agent
    environment = SimulationEnvironment()
    
    agent = Agent.create(agent='tensorforce', environment=environment,update=64, 
optimizer=dict(optimizer='adam', learning_rate=1e-3),objective='policy_gradient', 
reward_estimation=dict(horizon=1))

max_reward = 0
best_state =[]
best_episode = 0
 # Train for 100 episodes
for episode in range(100):

    # Episode using act and observe
    states = environment.reset()
    terminal = False
    while not terminal:
        actions = agent.act(states=states)
        states, terminal, reward = environment.execute(actions=actions)
        agent.observe(terminal=terminal, reward=reward)
    if reward &gt; max_reward:
        max_reward = reward
        best_state = states
        best_episode = episode
    print('Episode {}: reward={} state={}'.format(episode, reward, states))

print(&quot;Best Episode: &quot; + str(best_episode))
print(&quot;Best Reward: &quot; + str(max_reward))
print(&quot;Best state: &quot; + str(best_state))
print(&quot;XXXXXXXXXXXXXXXXXXXXXX&quot;)
print(&quot;XXXXXXXXXXXXXXXXXXXXXX&quot;)
print(&quot;Evaluation starts now!&quot;)
print(&quot;XXXXXXXXXXXXXXXXXXXXXX&quot;)
print(&quot;XXXXXXXXXXXXXXXXXXXXXX&quot;)
# Evaluate for 100 episodes
sum_negativerewards = 0
for evaluation in range(100):
    states = environment.reset()
    internals = agent.initial_internals()
    terminal = False
    while not terminal:
        actions, internals = agent.act(
            states=states, internals=internals, independent=True)
        states, terminal, reward_evaluation = environment.execute(actions=actions)
        #print(&quot;Reward: &quot; + str(reward_evaluation))
    if reward_evaluation == -1:
        sum_negativerewards += 1
    print('Evaluation {}: reward={} state={}'.format(evaluation, reward_evaluation, states))
print('Number of times with reward -100:', sum_negativerewards)

# Close agent and environment   
agent.close()
environment.close()


if __name__ == &quot;__main__&quot;:
        main()
</code></pre>
<p>For training, I get results here that look like this:</p>
<pre><code>Episode 95: reward=-1 state=[0.85 0.88 0.99 0.98 0.88 0.88 0.88 0.88]
Episode 96: reward=46.544582225592315 state=[0.99 0.88 0.88 0.88 0.88 0.99 0.98 0.94]
Episode 97: reward=34.4495764099072 state=[0.97 0.88 0.88 0.88 0.98 0.88 0.9  0.88]
Episode 98: reward=35.074430548377606 state=[0.8  0.92 0.99 0.88 0.88 0.94 0.88 0.88]
Episode 99: reward=37.04327327653888 state=[0.97 0.97 0.88 0.88 0.96 0.88 0.97 0.88]
Best Episode: 54
Best Reward: 52.52461319503872
Best state: [0.99 0.88 0.88 0.98 0.88 0.88 0.99 0.99]
</code></pre>
<p>[Training Output][1]</p>
<p>However, the evaluation part has this output, clearly showing that the same measure is being chosen over and over again:</p>
<pre><code>Evaluation 1: reward=-1 state=[0.99 0.88 0.88 0.88 0.88 0.88 0.88 0.88]
Reward: 39.45888404062209
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Evaluation 2: reward=-1 state=[0.99 0.88 0.88 0.88 0.88 0.88 0.88 0.88]
Reward: 39.45888404062209
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Reward: -1
Evaluation 3: reward=-1 state=[0.99 0.88 0.88 0.88 0.88 0.88 0.88 0.88]
</code></pre>
<p>[Evaluation Output][2]</p>
<p>I fear that I use the wrong agent, however if I use a ppo agent for example I get this error:</p>
<pre><code>Traceback (most recent call last):
File &quot;c:\Users\Sara\Documents\Uni\Master\Seminar Data Mining in der Produktion\Reinforcement Learning\main.py&quot;, line 92, in &lt;module&gt;
    main()
  File &quot;c:\Users\Sara\Documents\Uni\Master\Seminar Data Mining in der Produktion\Reinforcement Learning\main.py&quot;, line 47, in main
    actions = agent.act(states=states)
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\agent.py&quot;, line 415, in act
    return super().act(
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\recorder.py&quot;, line 262, in act
    actions, internals = self.fn_act(
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\agent.py&quot;, line 462, in fn_act
    actions, timesteps = self.model.act(
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\module.py&quot;, line 136, in decorated
    output_args = function_graphs[str(graph_params)](*graph_args)
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;, line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\execute.py&quot;, 
line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node 'agent/TensorScatterUpdate_2' defined at (most recent call last):
    File &quot;c:\Users\Sara\Documents\Uni\Master\Seminar Data Mining in der Produktion\Reinforcement Learning\main.py&quot;, 
line 92, in &lt;module&gt;
      main()
    File &quot;c:\Users\Sara\Documents\Uni\Master\Seminar Data Mining in der Produktion\Reinforcement Learning\main.py&quot;, 
line 47, in main
      actions = agent.act(states=states)
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\agent.py&quot;, line 
415, in act
      return super().act(
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\recorder.py&quot;, line 262, in act
      actions, internals = self.fn_act(
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\agents\agent.py&quot;, line 
462, in fn_act
      actions, timesteps = self.model.act(
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\module.py&quot;, line 136, in decorated
      output_args = function_graphs[str(graph_params)](*graph_args)
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\module.py&quot;, line 107, in function_graph
      args = function(self, **kwargs.to_kwargs(), **params_kwargs)
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\models\model.py&quot;, 
line 609, in act
      actions, internals = self.core_act(
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\module.py&quot;, line 136, in decorated
      output_args = function_graphs[str(graph_params)](*graph_args)
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\module.py&quot;, line 107, in function_graph
      args = function(self, **kwargs.to_kwargs(), **params_kwargs)
    File &quot;C:\Users\Sara\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorforce\core\models\tensorforce.py&quot;, line 1311, in core_act
      value = tf.tensor_scatter_nd_update(tensor=buffer, indices=indices, updates=action)
Node: 'agent/TensorScatterUpdate_2'
indices[0] = [0, 5] does not index into shape [1,5]
         [[{{node agent/TensorScatterUpdate_2}}]] [Op:__inference_act_1114]
</code></pre>
<p>I honestly really do not know what to do - it is my first time implementing Reinforcement Learning  as a student and would really appreciate any kind of help!
[1]: <a href=""https://i.stack.imgur.com/tVP5N.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/tVP5N.png</a>
[2]: <a href=""https://i.stack.imgur.com/2yxR9.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/2yxR9.png</a></p>
",35,0,1,3,python;reinforcement-learning;agent,2022-07-19 13:59:16,2022-07-19 13:59:16,2022-07-19 14:08:34,i am currently working on a university reinforcement learning project with tensorforce  the setting is as follows  in a production line are  machines and we have a total of  measures that can be implemented to optimize those machines   measures per machine   each measure improves the quality factor of a specific machine and has a certain cost  the goal is to select  measures out of the  that maximize the formula  quality factor machine    quality factor machine          quality factor machine    total costs of the  implemented measures     gt  this is also going to be the reward function  i created a custom environment that looks like this below  the state is defined by the  quality factors of each machine and the actions are the  measures to choose from  the simulationmodel  where the function get_nextstate is defined looks like this  now  here is the main function where the problem lies  for training  i get results here that look like this   training output    however  the evaluation part has this output  clearly showing that the same measure is being chosen over and over again   evaluation output    i fear that i use the wrong agent  however if i use a ppo agent for example i get this error ,tensorforce   agent training with custom environment,currently working university reinforcement learning project tensorforce setting follows production line machines total measures implemented optimize machines measures per machine measure improves quality factor specific machine certain cost goal select measures maximize formula quality factor machine quality factor machine quality factor machine total costs implemented measures gt also going reward function created environment looks like state defined quality factors machine actions measures choose simulationmodel function get_nextstate defined looks like main function problem lies training get results look like training output however evaluation part output clearly showing measure chosen evaluation output fear use wrong agent however use ppo agent example get error,tensorforce agent training environment,tensorforce agent training environmentcurrently working university reinforcement learning project tensorforce setting follows production line machines total measures implemented optimize machines measures per machine measure improves quality factor specific machine certain cost goal select measures maximize formula quality factor machine quality factor machine quality factor machine total costs implemented measures gt also going reward function created environment looks like state defined quality factors machine actions measures choose simulationmodel function get_nextstate defined looks like main function problem lies training get results look like training output however evaluation part output clearly showing measure chosen evaluation output fear use wrong agent however use ppo agent example get error,"['tensorforce', 'agent', 'training', 'environmentcurrently', 'working', 'university', 'reinforcement', 'learning', 'project', 'tensorforce', 'setting', 'follows', 'production', 'line', 'machines', 'total', 'measures', 'implemented', 'optimize', 'machines', 'measures', 'per', 'machine', 'measure', 'improves', 'quality', 'factor', 'specific', 'machine', 'certain', 'cost', 'goal', 'select', 'measures', 'maximize', 'formula', 'quality', 'factor', 'machine', 'quality', 'factor', 'machine', 'quality', 'factor', 'machine', 'total', 'costs', 'implemented', 'measures', 'gt', 'also', 'going', 'reward', 'function', 'created', 'environment', 'looks', 'like', 'state', 'defined', 'quality', 'factors', 'machine', 'actions', 'measures', 'choose', 'simulationmodel', 'function', 'get_nextstate', 'defined', 'looks', 'like', 'main', 'function', 'problem', 'lies', 'training', 'get', 'results', 'look', 'like', 'training', 'output', 'however', 'evaluation', 'part', 'output', 'clearly', 'showing', 'measure', 'chosen', 'evaluation', 'output', 'fear', 'use', 'wrong', 'agent', 'however', 'use', 'ppo', 'agent', 'example', 'get', 'error']","['tensorforc', 'agent', 'train', 'environmentcurr', 'work', 'univers', 'reinforc', 'learn', 'project', 'tensorforc', 'set', 'follow', 'product', 'line', 'machin', 'total', 'measur', 'implement', 'optim', 'machin', 'measur', 'per', 'machin', 'measur', 'improv', 'qualiti', 'factor', 'specif', 'machin', 'certain', 'cost', 'goal', 'select', 'measur', 'maxim', 'formula', 'qualiti', 'factor', 'machin', 'qualiti', 'factor', 'machin', 'qualiti', 'factor', 'machin', 'total', 'cost', 'implement', 'measur', 'gt', 'also', 'go', 'reward', 'function', 'creat', 'environ', 'look', 'like', 'state', 'defin', 'qualiti', 'factor', 'machin', 'action', 'measur', 'choos', 'simulationmodel', 'function', 'get_nextst', 'defin', 'look', 'like', 'main', 'function', 'problem', 'lie', 'train', 'get', 'result', 'look', 'like', 'train', 'output', 'howev', 'evalu', 'part', 'output', 'clearli', 'show', 'measur', 'chosen', 'evalu', 'output', 'fear', 'use', 'wrong', 'agent', 'howev', 'use', 'ppo', 'agent', 'exampl', 'get', 'error']"
159,172,172,19254089,73035702,How to resolve this Imbalance Data set issue?,"<p>I am not getting any links on the Internet about this. In a machine learning model with a highly imbalanced dataset with Target variable (1,0) and 0 being 6% (count of 0 is approx. 44000), I find that Smote, Random Undersampling and Oversampling are not giving results. I manually prepared the data with 1 as 55000 and 0 as 44000 (actual number). This way I am able to get satisfactory results in terms of precision, recall and accuracy. The data used to prepare the model (which of course splits in training and test data sets) is from 1st Jan 2020 to Apr 2022.</p>
<p>However, when I run the model on May 2022 data, I am getting very poor results. Am I making a mistake by manually preparing and balancing the data? How can I make the model give good results on fresh data?</p>
",16,0,-1,3,machine-learning;imbalanced-data;smote,2022-07-19 14:00:44,2022-07-19 14:00:44,2022-07-19 14:00:44,i am not getting any links on the internet about this  in a machine learning model with a highly imbalanced dataset with target variable     and  being    count of  is approx     i find that smote  random undersampling and oversampling are not giving results  i manually prepared the data with  as  and  as   actual number   this way i am able to get satisfactory results in terms of precision  recall and accuracy  the data used to prepare the model  which of course splits in training and test data sets  is from st jan  to apr   however  when i run the model on may  data  i am getting very poor results  am i making a mistake by manually preparing and balancing the data  how can i make the model give good results on fresh data ,how to resolve this imbalance data set issue ,getting links internet machine learning model highly imbalanced dataset target variable count approx find smote random undersampling oversampling giving results manually prepared data actual number way able get satisfactory results terms precision recall accuracy data used prepare model course splits training test data sets st jan apr however run model may data getting poor results making mistake manually preparing balancing data make model give good results fresh data,resolve imbalance data set issue,resolve imbalance data set issuegetting links internet machine learning model highly imbalanced dataset target variable count approx find smote random undersampling oversampling giving results manually prepared data actual number way able get satisfactory results terms precision recall accuracy data used prepare model course splits training test data sets st jan apr however run model may data getting poor results making mistake manually preparing balancing data make model give good results fresh data,"['resolve', 'imbalance', 'data', 'set', 'issuegetting', 'links', 'internet', 'machine', 'learning', 'model', 'highly', 'imbalanced', 'dataset', 'target', 'variable', 'count', 'approx', 'find', 'smote', 'random', 'undersampling', 'oversampling', 'giving', 'results', 'manually', 'prepared', 'data', 'actual', 'number', 'way', 'able', 'get', 'satisfactory', 'results', 'terms', 'precision', 'recall', 'accuracy', 'data', 'used', 'prepare', 'model', 'course', 'splits', 'training', 'test', 'data', 'sets', 'st', 'jan', 'apr', 'however', 'run', 'model', 'may', 'data', 'getting', 'poor', 'results', 'making', 'mistake', 'manually', 'preparing', 'balancing', 'data', 'make', 'model', 'give', 'good', 'results', 'fresh', 'data']","['resolv', 'imbal', 'data', 'set', 'issueget', 'link', 'internet', 'machin', 'learn', 'model', 'highli', 'imbalanc', 'dataset', 'target', 'variabl', 'count', 'approx', 'find', 'smote', 'random', 'undersampl', 'oversampl', 'give', 'result', 'manual', 'prepar', 'data', 'actual', 'number', 'way', 'abl', 'get', 'satisfactori', 'result', 'term', 'precis', 'recal', 'accuraci', 'data', 'use', 'prepar', 'model', 'cours', 'split', 'train', 'test', 'data', 'set', 'st', 'jan', 'apr', 'howev', 'run', 'model', 'may', 'data', 'get', 'poor', 'result', 'make', 'mistak', 'manual', 'prepar', 'balanc', 'data', 'make', 'model', 'give', 'good', 'result', 'fresh', 'data']"
160,173,173,314763,73030972,Azure Machine Learning Service writing to AzureDataLakeGen2Datastore,"<p>I registered an azure storage account gen2 where datalake filesystem enabled as a datastore as</p>
<pre><code>    _ = Datastore.register_azure_data_lake_gen2(workspace='xxx',
                                                datastore_name='store-identifier',
                                                filesystem= 'container',
                                                account_name= 'xxx',
                                                subscription_id= 'xxx',
                                                resource_group= 'xxx',
                                                overwrite=1)
</code></pre>
<p>I tried uploading files to this using</p>
<pre><code> destination_store = Datastore.get(workspace, 'store-identifier');
 destination_store.upload(
     src='data/', target_path=&quot;data_science/&quot;, overwrite=True
 )
</code></pre>
<blockquote>
<p>AzureDataLakeGen2Datastore' object has no attribute 'upload'</p>
</blockquote>
<p>When i opened the underlying <code>AzureDataLakeGen2Datastore</code> it mentions that the class does not support an upload and asks users to use a <code>Dataset</code> instance to upload data. However  I tried both <code>Dataset.Tabular.upload</code> and <code>Dataset.Files.upload</code> however neither of them support support this functionality</p>
<p>How should i proceed further. Is this functionality not yet supported via azure machine learning ?</p>
",479,0,1,2,azure-machine-learning-service;azureml-python-sdk,2022-07-19 07:00:07,2022-07-19 07:00:07,2022-07-19 13:18:08,i registered an azure storage account gen where datalake filesystem enabled as a datastore as i tried uploading files to this using azuredatalakegendatastore  object has no attribute  upload  when i opened the underlying azuredatalakegendatastore it mentions that the class does not support an upload and asks users to use a dataset instance to upload data  however  i tried both dataset tabular upload and dataset files upload however neither of them support support this functionality how should i proceed further  is this functionality not yet supported via azure machine learning  ,azure machine learning service writing to azuredatalakegendatastore,registered azure storage account gen datalake filesystem enabled datastore tried uploading files using azuredatalakegendatastore object attribute upload opened underlying azuredatalakegendatastore mentions class support upload asks users use dataset instance upload data however tried dataset tabular upload dataset files upload however neither support support functionality proceed functionality yet supported via azure machine learning,azure machine learning service writing azuredatalakegendatastore,azure machine learning service writing azuredatalakegendatastoreregistered azure storage account gen datalake filesystem enabled datastore tried uploading files using azuredatalakegendatastore object attribute upload opened underlying azuredatalakegendatastore mentions class support upload asks users use dataset instance upload data however tried dataset tabular upload dataset files upload however neither support support functionality proceed functionality yet supported via azure machine learning,"['azure', 'machine', 'learning', 'service', 'writing', 'azuredatalakegendatastoreregistered', 'azure', 'storage', 'account', 'gen', 'datalake', 'filesystem', 'enabled', 'datastore', 'tried', 'uploading', 'files', 'using', 'azuredatalakegendatastore', 'object', 'attribute', 'upload', 'opened', 'underlying', 'azuredatalakegendatastore', 'mentions', 'class', 'support', 'upload', 'asks', 'users', 'use', 'dataset', 'instance', 'upload', 'data', 'however', 'tried', 'dataset', 'tabular', 'upload', 'dataset', 'files', 'upload', 'however', 'neither', 'support', 'support', 'functionality', 'proceed', 'functionality', 'yet', 'supported', 'via', 'azure', 'machine', 'learning']","['azur', 'machin', 'learn', 'servic', 'write', 'azuredatalakegendatastoreregist', 'azur', 'storag', 'account', 'gen', 'datalak', 'filesystem', 'enabl', 'datastor', 'tri', 'upload', 'file', 'use', 'azuredatalakegendatastor', 'object', 'attribut', 'upload', 'open', 'underli', 'azuredatalakegendatastor', 'mention', 'class', 'support', 'upload', 'ask', 'user', 'use', 'dataset', 'instanc', 'upload', 'data', 'howev', 'tri', 'dataset', 'tabular', 'upload', 'dataset', 'file', 'upload', 'howev', 'neither', 'support', 'support', 'function', 'proceed', 'function', 'yet', 'support', 'via', 'azur', 'machin', 'learn']"
161,174,174,19253809,73033517,Common Keyword/ Substring in a List of Company Name,"<p>Suppose I have several lists of company names which are all operating in the steel and property industries, respectively. How to guess a company industry based on the current list?
example:</p>
<pre><code>steel=['Steelson Inc', 'First Steel Inc', 'North Steel Inc', 'Hard Hammer Inc', 'Mythril Forge Ltd'] 
propert=['New Land Inc', ' Green Meadow Inc', ' Golden Land Inc', 'City Lander Inc'] 

name='World Steel Inc'
print(find_industry(name))
</code></pre>
<p>Above code should print 'steel'.</p>
<p>I do not know how to best approach this problem, from Machine Learning or traditional string search methods. Please note that this problem might be different from longest common substring problem, because the 'key string' (or in above example 'steel') does not necessarily have to be appeared in all of the string in the list. Any help is appreciated.</p>
<p><a href=""https://en.wikipedia.org/wiki/Longest_common_substring_problem"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Longest_common_substring_problem</a></p>
",28,0,0,4,python;string;machine-learning;string-search,2022-07-19 11:21:51,2022-07-19 11:21:51,2022-07-19 11:23:32,above code should print  steel   i do not know how to best approach this problem  from machine learning or traditional string search methods  please note that this problem might be different from longest common substring problem  because the  key string   or in above example  steel   does not necessarily have to be appeared in all of the string in the list  any help is appreciated  ,common keyword  substring in a list of company name,code print steel know best approach problem machine learning traditional string search methods please note problem might different longest common substring problem key string example steel necessarily appeared string help appreciated,common keyword substring company name,common keyword substring company namecode print steel know best approach problem machine learning traditional string search methods please note problem might different longest common substring problem key string example steel necessarily appeared string help appreciated,"['common', 'keyword', 'substring', 'company', 'namecode', 'print', 'steel', 'know', 'best', 'approach', 'problem', 'machine', 'learning', 'traditional', 'string', 'search', 'methods', 'please', 'note', 'problem', 'might', 'different', 'longest', 'common', 'substring', 'problem', 'key', 'string', 'example', 'steel', 'necessarily', 'appeared', 'string', 'help', 'appreciated']","['common', 'keyword', 'substr', 'compani', 'namecod', 'print', 'steel', 'know', 'best', 'approach', 'problem', 'machin', 'learn', 'tradit', 'string', 'search', 'method', 'pleas', 'note', 'problem', 'might', 'differ', 'longest', 'common', 'substr', 'problem', 'key', 'string', 'exampl', 'steel', 'necessarili', 'appear', 'string', 'help', 'appreci']"
162,175,175,19540718,73032858,Load trained model in c# tensorflow,"<p>I want to load a trained model (.pb file) in tensorflow c# for image classification. I have found several tutorials regarding this topic like this (<a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/image-classification"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/image-classification</a>), but I dont want to train the model. I want to load the model and do a single image classification.</p>
<p>Thanks for any help!</p>
",31,0,0,2,c#;tensorflow,2022-07-19 10:35:05,2022-07-19 10:35:05,2022-07-19 10:56:22,i want to load a trained model   pb file  in tensorflow c  for image classification  i have found several tutorials regarding this topic like this     but i don t want to train the model  i want to load the model and do a single image classification  thanks for any help ,load trained model in c  tensorflow,want load trained model pb file tensorflow c image classification found several tutorials regarding topic like want train model want load model single image classification thanks help,load trained model c tensorflow,load trained model c tensorflowwant load trained model pb file tensorflow c image classification found several tutorials regarding topic like want train model want load model single image classification thanks help,"['load', 'trained', 'model', 'c', 'tensorflowwant', 'load', 'trained', 'model', 'pb', 'file', 'tensorflow', 'c', 'image', 'classification', 'found', 'several', 'tutorials', 'regarding', 'topic', 'like', 'want', 'train', 'model', 'want', 'load', 'model', 'single', 'image', 'classification', 'thanks', 'help']","['load', 'train', 'model', 'c', 'tensorfloww', 'load', 'train', 'model', 'pb', 'file', 'tensorflow', 'c', 'imag', 'classif', 'found', 'sever', 'tutori', 'regard', 'topic', 'like', 'want', 'train', 'model', 'want', 'load', 'model', 'singl', 'imag', 'classif', 'thank', 'help']"
163,176,176,9485286,73032760,Multiindex and multiheader dataframe,"<p>I want to replicate the following structure in a dataframe.</p>
<p><a href=""https://i.stack.imgur.com/lylhs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lylhs.png"" alt=""enter image description here"" /></a></p>
<p>Here is what I want to achieve: I am testing 3 machine learning models with their names defined as Model 1 - Model 3 in the table above as a second header row. I will be adding more as my code develops. For all models, I am calculating some metrics like their accuracy, precision, training time, memory etc, appearing as a top header row. The performance of these models are a function of the number of samples and the number of Epochs (Epochs column) used to train them. I presume these columns can be defined as indices in a MultiIndex dataframe. There are also 2 other columns at the end of the table which depend only on the number of samples and are not relevant to the models. As you may notice there is a grouping that I also want to achieve, namely my tests run for all epochs for each of the samples values. Eventually, I want to be able to plot these results in different configurations, for example compare the accuracy of all models as a function of the number of epochs for a specific sample value or the accuracy of all models for a certain epoch for all sample values, etc.</p>
<p>Is this dataframe possible?</p>
",35,0,0,4,python;pandas;dataframe;multi-index,2022-07-19 10:28:25,2022-07-19 10:28:25,2022-07-19 10:28:25,i want to replicate the following structure in a dataframe   here is what i want to achieve  i am testing  machine learning models with their names defined as model    model  in the table above as a second header row  i will be adding more as my code develops  for all models  i am calculating some metrics like their accuracy  precision  training time  memory etc  appearing as a top header row  the performance of these models are a function of the number of samples and the number of epochs  epochs column  used to train them  i presume these columns can be defined as indices in a multiindex dataframe  there are also  other columns at the end of the table which depend only on the number of samples and are not relevant to the models  as you may notice there is a grouping that i also want to achieve  namely my tests run for all epochs for each of the samples values  eventually  i want to be able to plot these results in different configurations  for example compare the accuracy of all models as a function of the number of epochs for a specific sample value or the accuracy of all models for a certain epoch for all sample values  etc  is this dataframe possible ,multiindex and multiheader dataframe,want replicate following structure dataframe want achieve testing machine learning models names defined model model table second header row adding code develops models calculating metrics like accuracy precision training time memory etc appearing top header row performance models function number samples number epochs epochs column used train presume columns defined indices multiindex dataframe also columns end table depend number samples relevant models may notice grouping also want achieve namely tests run epochs samples values eventually want able plot results different configurations example compare accuracy models function number epochs specific sample value accuracy models certain epoch sample values etc dataframe possible,multiindex multiheader dataframe,multiindex multiheader dataframewant replicate following structure dataframe want achieve testing machine learning models names defined model model table second header row adding code develops models calculating metrics like accuracy precision training time memory etc appearing top header row performance models function number samples number epochs epochs column used train presume columns defined indices multiindex dataframe also columns end table depend number samples relevant models may notice grouping also want achieve namely tests run epochs samples values eventually want able plot results different configurations example compare accuracy models function number epochs specific sample value accuracy models certain epoch sample values etc dataframe possible,"['multiindex', 'multiheader', 'dataframewant', 'replicate', 'following', 'structure', 'dataframe', 'want', 'achieve', 'testing', 'machine', 'learning', 'models', 'names', 'defined', 'model', 'model', 'table', 'second', 'header', 'row', 'adding', 'code', 'develops', 'models', 'calculating', 'metrics', 'like', 'accuracy', 'precision', 'training', 'time', 'memory', 'etc', 'appearing', 'top', 'header', 'row', 'performance', 'models', 'function', 'number', 'samples', 'number', 'epochs', 'epochs', 'column', 'used', 'train', 'presume', 'columns', 'defined', 'indices', 'multiindex', 'dataframe', 'also', 'columns', 'end', 'table', 'depend', 'number', 'samples', 'relevant', 'models', 'may', 'notice', 'grouping', 'also', 'want', 'achieve', 'namely', 'tests', 'run', 'epochs', 'samples', 'values', 'eventually', 'want', 'able', 'plot', 'results', 'different', 'configurations', 'example', 'compare', 'accuracy', 'models', 'function', 'number', 'epochs', 'specific', 'sample', 'value', 'accuracy', 'models', 'certain', 'epoch', 'sample', 'values', 'etc', 'dataframe', 'possible']","['multiindex', 'multihead', 'dataframew', 'replic', 'follow', 'structur', 'datafram', 'want', 'achiev', 'test', 'machin', 'learn', 'model', 'name', 'defin', 'model', 'model', 'tabl', 'second', 'header', 'row', 'ad', 'code', 'develop', 'model', 'calcul', 'metric', 'like', 'accuraci', 'precis', 'train', 'time', 'memori', 'etc', 'appear', 'top', 'header', 'row', 'perform', 'model', 'function', 'number', 'sampl', 'number', 'epoch', 'epoch', 'column', 'use', 'train', 'presum', 'column', 'defin', 'indic', 'multiindex', 'datafram', 'also', 'column', 'end', 'tabl', 'depend', 'number', 'sampl', 'relev', 'model', 'may', 'notic', 'group', 'also', 'want', 'achiev', 'name', 'test', 'run', 'epoch', 'sampl', 'valu', 'eventu', 'want', 'abl', 'plot', 'result', 'differ', 'configur', 'exampl', 'compar', 'accuraci', 'model', 'function', 'number', 'epoch', 'specif', 'sampl', 'valu', 'accuraci', 'model', 'certain', 'epoch', 'sampl', 'valu', 'etc', 'datafram', 'possibl']"
164,177,177,10206779,73031395,Accuracy Ratio (Gini coef) computation in Python by Definition and ROC Method,"<p>Why do the following methods of computing the accuracy ratio give different results?</p>
<p><strong>Approach 1</strong>: Cumulative Accuracy Profile (CAP) curve</p>
<p>The accuracy ratio is computed from definition as the difference between the area under curve of the CAP of the trained model and that of a random model, divided by the difference between the area under curve of the CAP of a perfect model and that of a random model.</p>
<p><strong>Approach 2</strong>: Receiver Operating Characteristic (ROC) curve.
We compute the area under the ROC curve, and use the statistic</p>
<p>AR = Gini = 2*(Area Under ROC Curve) - 1</p>
<p>For a derivation of the statistics, please refer to the paper &quot;Measuring the Discriminative Power of Rating Systems&quot; (<a href=""https://www.bundesbank.de/resource/blob/704150/b9fa10a16dfff3c98842581253f6d141/mL/2003-10-01-dkp-01-data.pdf"" rel=""nofollow noreferrer"">https://www.bundesbank.de/resource/blob/704150/b9fa10a16dfff3c98842581253f6d141/mL/2003-10-01-dkp-01-data.pdf</a>)</p>
<p>For the source of the code I'm using and more examples, please refer to the article &quot;Machine Learning Classifier Evaluation using ROC&quot; (<a href=""https://towardsdatascience.com/machine-learning-classifier-evaluation-using-roc-and-cap-curves-7db60fe6b716"" rel=""nofollow noreferrer"">https://towardsdatascience.com/machine-learning-classifier-evaluation-using-roc-and-cap-curves-7db60fe6b716</a>)</p>
<p>In what follows, we have a vector <code>y_test</code> of actual test labels, with a vector <code>test_pred_probs</code> of predicted probabilities of the positive class (label=1) that is output from some predictive model. We compute the accuracy ratio of this prediction.</p>
<pre><code>from sklearn.metrics import roc_curve, auc, roc_auc_score
import numpy as np
y_test = [0,1,1,0,1,1,1,0,0,1]
test_pred_probs = [0.2,0.4,0.1,0,0,1,0.9,0.3,0.2,0.8]
total = len(y_test)
one_count = np.sum(y_test)
zero_count = total - one_count
lm = [y for _,y in sorted(zip(test_pred_probs,y_test),reverse=True)]
x = np.arange(0,total+1)
y = np.append([0],np.cumsum(lm))
a = auc([0,total],[0,one_count])
aP = auc([0,one_count,total],[0,one_count,one_count]) - a
aR = auc(x,y) - a
print(&quot;Acc ratio:&quot;,aR/aP) #returns 0.5
print(&quot;Acc ratio from ROC curve:&quot;,2*roc_auc_score(y_test,test_pred_probs)-1) #returns 0.458
</code></pre>
<p>In some cases, these two approaches give drastically different results - the former gives an AR of about 0.7, and the latter using ROC method gives an AR of 0.12.</p>
",59,0,0,5,python-3.x;machine-learning;scikit-learn;statistics;gini,2022-07-19 08:09:47,2022-07-19 08:09:47,2022-07-19 09:12:04,why do the following methods of computing the accuracy ratio give different results  approach   cumulative accuracy profile  cap  curve the accuracy ratio is computed from definition as the difference between the area under curve of the cap of the trained model and that of a random model  divided by the difference between the area under curve of the cap of a perfect model and that of a random model  ar   gini     area under roc curve     for a derivation of the statistics  please refer to the paper  measuring the discriminative power of rating systems     for the source of the code i m using and more examples  please refer to the article  machine learning classifier evaluation using roc     in what follows  we have a vector y_test of actual test labels  with a vector test_pred_probs of predicted probabilities of the positive class  label   that is output from some predictive model  we compute the accuracy ratio of this prediction  in some cases  these two approaches give drastically different results   the former gives an ar of about    and the latter using roc method gives an ar of   ,accuracy ratio  gini coef  computation in python by definition and roc method,following methods computing accuracy ratio give different results approach cumulative accuracy profile cap curve accuracy ratio computed definition difference area curve cap trained model random model divided difference area curve cap perfect model random model ar gini area roc curve derivation statistics please refer paper measuring discriminative power rating systems source code using examples please refer article machine learning classifier evaluation using roc follows vector y_test actual test labels vector test_pred_probs predicted probabilities positive class label output predictive model compute accuracy ratio prediction cases two approaches give drastically different results former gives ar latter using roc method gives ar,accuracy ratio gini coef computation python definition roc method,accuracy ratio gini coef computation python definition roc methodfollowing methods computing accuracy ratio give different results approach cumulative accuracy profile cap curve accuracy ratio computed definition difference area curve cap trained model random model divided difference area curve cap perfect model random model ar gini area roc curve derivation statistics please refer paper measuring discriminative power rating systems source code using examples please refer article machine learning classifier evaluation using roc follows vector y_test actual test labels vector test_pred_probs predicted probabilities positive class label output predictive model compute accuracy ratio prediction cases two approaches give drastically different results former gives ar latter using roc method gives ar,"['accuracy', 'ratio', 'gini', 'coef', 'computation', 'python', 'definition', 'roc', 'methodfollowing', 'methods', 'computing', 'accuracy', 'ratio', 'give', 'different', 'results', 'approach', 'cumulative', 'accuracy', 'profile', 'cap', 'curve', 'accuracy', 'ratio', 'computed', 'definition', 'difference', 'area', 'curve', 'cap', 'trained', 'model', 'random', 'model', 'divided', 'difference', 'area', 'curve', 'cap', 'perfect', 'model', 'random', 'model', 'ar', 'gini', 'area', 'roc', 'curve', 'derivation', 'statistics', 'please', 'refer', 'paper', 'measuring', 'discriminative', 'power', 'rating', 'systems', 'source', 'code', 'using', 'examples', 'please', 'refer', 'article', 'machine', 'learning', 'classifier', 'evaluation', 'using', 'roc', 'follows', 'vector', 'y_test', 'actual', 'test', 'labels', 'vector', 'test_pred_probs', 'predicted', 'probabilities', 'positive', 'class', 'label', 'output', 'predictive', 'model', 'compute', 'accuracy', 'ratio', 'prediction', 'cases', 'two', 'approaches', 'give', 'drastically', 'different', 'results', 'former', 'gives', 'ar', 'latter', 'using', 'roc', 'method', 'gives', 'ar']","['accuraci', 'ratio', 'gini', 'coef', 'comput', 'python', 'definit', 'roc', 'methodfollow', 'method', 'comput', 'accuraci', 'ratio', 'give', 'differ', 'result', 'approach', 'cumul', 'accuraci', 'profil', 'cap', 'curv', 'accuraci', 'ratio', 'comput', 'definit', 'differ', 'area', 'curv', 'cap', 'train', 'model', 'random', 'model', 'divid', 'differ', 'area', 'curv', 'cap', 'perfect', 'model', 'random', 'model', 'ar', 'gini', 'area', 'roc', 'curv', 'deriv', 'statist', 'pleas', 'refer', 'paper', 'measur', 'discrimin', 'power', 'rate', 'system', 'sourc', 'code', 'use', 'exampl', 'pleas', 'refer', 'articl', 'machin', 'learn', 'classifi', 'evalu', 'use', 'roc', 'follow', 'vector', 'y_test', 'actual', 'test', 'label', 'vector', 'test_pred_prob', 'predict', 'probabl', 'posit', 'class', 'label', 'output', 'predict', 'model', 'comput', 'accuraci', 'ratio', 'predict', 'case', 'two', 'approach', 'give', 'drastic', 'differ', 'result', 'former', 'give', 'ar', 'latter', 'use', 'roc', 'method', 'give', 'ar']"
165,178,178,19576065,73030486,I have a question about who we should do testing phase in transformers for time series forecasting,"<p>In this article <a href=""https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04"" rel=""nofollow noreferrer"">https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</a> the author used an empty sequence with start of the token as first element for feeding to the decoder in inference phase. For empty sequence I considered a sequence with the start token as first element and zeros as remaining elements. For instance if the output window is in size of 3, I considered a sequence with length of three ( the first element is start of the token and the remaining is zero) then these zero will be replaced with new token generated in each step. It will be continued till our sequence fed to the decoder has been filled with all new tokens. I want to know if I am in a right path? The empty sequence that I considered is a right thing ?</p>
",10,0,0,1,transformer,2022-07-19 05:31:05,2022-07-19 05:31:05,2022-07-19 05:31:05,in this article  the author used an empty sequence with start of the token as first element for feeding to the decoder in inference phase  for empty sequence i considered a sequence with the start token as first element and zeros as remaining elements  for instance if the output window is in size of   i considered a sequence with length of three   the first element is start of the token and the remaining is zero  then these zero will be replaced with new token generated in each step  it will be continued till our sequence fed to the decoder has been filled with all new tokens  i want to know if i am in a right path  the empty sequence that i considered is a right thing  ,i have a question about who we should do testing phase in transformers for time series forecasting,article author used empty sequence start token first element feeding decoder inference phase empty sequence considered sequence start token first element zeros remaining elements instance output window size considered sequence length three first element start token remaining zero zero replaced token generated step continued till sequence fed decoder filled tokens want know right path empty sequence considered right thing,question testing phase transformers time series forecasting,question testing phase transformers time series forecastingarticle author used empty sequence start token first element feeding decoder inference phase empty sequence considered sequence start token first element zeros remaining elements instance output window size considered sequence length three first element start token remaining zero zero replaced token generated step continued till sequence fed decoder filled tokens want know right path empty sequence considered right thing,"['question', 'testing', 'phase', 'transformers', 'time', 'series', 'forecastingarticle', 'author', 'used', 'empty', 'sequence', 'start', 'token', 'first', 'element', 'feeding', 'decoder', 'inference', 'phase', 'empty', 'sequence', 'considered', 'sequence', 'start', 'token', 'first', 'element', 'zeros', 'remaining', 'elements', 'instance', 'output', 'window', 'size', 'considered', 'sequence', 'length', 'three', 'first', 'element', 'start', 'token', 'remaining', 'zero', 'zero', 'replaced', 'token', 'generated', 'step', 'continued', 'till', 'sequence', 'fed', 'decoder', 'filled', 'tokens', 'want', 'know', 'right', 'path', 'empty', 'sequence', 'considered', 'right', 'thing']","['question', 'test', 'phase', 'transform', 'time', 'seri', 'forecastingarticl', 'author', 'use', 'empti', 'sequenc', 'start', 'token', 'first', 'element', 'feed', 'decod', 'infer', 'phase', 'empti', 'sequenc', 'consid', 'sequenc', 'start', 'token', 'first', 'element', 'zero', 'remain', 'element', 'instanc', 'output', 'window', 'size', 'consid', 'sequenc', 'length', 'three', 'first', 'element', 'start', 'token', 'remain', 'zero', 'zero', 'replac', 'token', 'gener', 'step', 'continu', 'till', 'sequenc', 'fed', 'decod', 'fill', 'token', 'want', 'know', 'right', 'path', 'empti', 'sequenc', 'consid', 'right', 'thing']"
166,179,179,10320475,73030101,How to handle outliers in this dataset?,"<p>I am doing a project in machine learning using the 'Pump sensor dataset'. In this, while using the IQR method, I got 894997 outliers in our dataset. But with Isolation Forest, I got around 28642 outliers, which is much lesser than the IQR method. The code for Isolation forest is given below.</p>
<pre><code>`from sklearn.ensemble import IsolationForest  
 model= IsolationForest(contamination=0.13, n_estimators=1000)
 model.fit(X.values)
 pred= model.predict(X.values)
 X['anomalies']= pred
 outliers= X.loc[X['anomalies']==-1]
 outlier_index= list(outliers.index)`
</code></pre>
<p>My question is which is the correct method to handle these outliers.
Replacing all outliers found through the IQR method with the median of each feature?
or
Deleting the entire rows that were found through Isolation Forest? If so how to do that</p>
",23,0,0,5,pandas;scikit-learn;data-science;sklearn-pandas;isolation-forest,2022-07-19 04:12:22,2022-07-19 04:12:22,2022-07-19 04:12:22,i am doing a project in machine learning using the  pump sensor dataset   in this  while using the iqr method  i got  outliers in our dataset  but with isolation forest  i got around  outliers  which is much lesser than the iqr method  the code for isolation forest is given below ,how to handle outliers in this dataset ,project machine learning using pump sensor dataset using iqr method got outliers dataset isolation forest got around outliers much lesser iqr method code isolation forest given,handle outliers dataset,handle outliers datasetproject machine learning using pump sensor dataset using iqr method got outliers dataset isolation forest got around outliers much lesser iqr method code isolation forest given,"['handle', 'outliers', 'datasetproject', 'machine', 'learning', 'using', 'pump', 'sensor', 'dataset', 'using', 'iqr', 'method', 'got', 'outliers', 'dataset', 'isolation', 'forest', 'got', 'around', 'outliers', 'much', 'lesser', 'iqr', 'method', 'code', 'isolation', 'forest', 'given']","['handl', 'outlier', 'datasetproject', 'machin', 'learn', 'use', 'pump', 'sensor', 'dataset', 'use', 'iqr', 'method', 'got', 'outlier', 'dataset', 'isol', 'forest', 'got', 'around', 'outlier', 'much', 'lesser', 'iqr', 'method', 'code', 'isol', 'forest', 'given']"
167,180,180,12431997,73028414,How can i load a ML model in memory so i can reduce the request time (Lambda function),"<p>I am using a machine learning model in a lambda function that usually takes 10s to load the predictive model and then do the prediction. I am trying to load this model in memory (something like Redis) so i can just do the prediction (no joblib.load()) and reduce the execution time of my script.</p>
<p>Is there any way to achieve that? I tried using Redis but i can only save the model with joblib.dumps or pickle.dumps and this does not help.</p>
<p>Probably the best way could be creating an API and using containers in AWS but i need to use a cheapear option.</p>
",19,0,0,5,python;machine-learning;redis;pickle;joblib,2022-07-18 23:43:38,2022-07-18 23:43:38,2022-07-18 23:43:38,i am using a machine learning model in a lambda function that usually takes s to load the predictive model and then do the prediction  i am trying to load this model in memory  something like redis  so i can just do the prediction  no joblib load    and reduce the execution time of my script  is there any way to achieve that  i tried using redis but i can only save the model with joblib dumps or pickle dumps and this does not help  probably the best way could be creating an api and using containers in aws but i need to use a cheapear option ,how can i load a ml model in memory so i can reduce the request time  lambda function ,using machine learning model lambda function usually takes load predictive model prediction trying load model memory something like redis prediction joblib load reduce execution time script way achieve tried using redis save model joblib dumps pickle dumps help probably best way could creating api using containers aws need use cheapear option,load ml model memory reduce request time lambda function,load ml model memory reduce request time lambda functionusing machine learning model lambda function usually takes load predictive model prediction trying load model memory something like redis prediction joblib load reduce execution time script way achieve tried using redis save model joblib dumps pickle dumps help probably best way could creating api using containers aws need use cheapear option,"['load', 'ml', 'model', 'memory', 'reduce', 'request', 'time', 'lambda', 'functionusing', 'machine', 'learning', 'model', 'lambda', 'function', 'usually', 'takes', 'load', 'predictive', 'model', 'prediction', 'trying', 'load', 'model', 'memory', 'something', 'like', 'redis', 'prediction', 'joblib', 'load', 'reduce', 'execution', 'time', 'script', 'way', 'achieve', 'tried', 'using', 'redis', 'save', 'model', 'joblib', 'dumps', 'pickle', 'dumps', 'help', 'probably', 'best', 'way', 'could', 'creating', 'api', 'using', 'containers', 'aws', 'need', 'use', 'cheapear', 'option']","['load', 'ml', 'model', 'memori', 'reduc', 'request', 'time', 'lambda', 'functionus', 'machin', 'learn', 'model', 'lambda', 'function', 'usual', 'take', 'load', 'predict', 'model', 'predict', 'tri', 'load', 'model', 'memori', 'someth', 'like', 'redi', 'predict', 'joblib', 'load', 'reduc', 'execut', 'time', 'script', 'way', 'achiev', 'tri', 'use', 'redi', 'save', 'model', 'joblib', 'dump', 'pickl', 'dump', 'help', 'probabl', 'best', 'way', 'could', 'creat', 'api', 'use', 'contain', 'aw', 'need', 'use', 'cheapear', 'option']"
168,181,181,9285142,73027898,Bubble sort works on local host but not on webserver,"<p>Pretty much the title.  I wrote a script for a game my friends and I play.  It parses information from the game and it gives us some useful data.  I used a simple bubble sort as there is never going to be more than a few hundred lines of data.  The exact same code I upload to my machine to a free webhost server does not display the sort properly.  It works on my localhost fine.  Is there anything I can do?</p>
<p>Here is the bubble sort.  I did not feel like learning PHP OOP so I just used 3 arrays to hold the 3 values I needed.  I sorted all 3 arrays by the values held in one of the arrays.</p>
<pre><code>for($i = 0 ; $i &lt; count($provinces); $i++){
        for ($j = 0; $j &lt; count($provinces) -$i - 1; $j++) {
            if ($killed[$j] &lt; $killed[$j+1]) {
                $tempProv = $provinces[$j];
                $provinces[$j] = $provinces[$j+1];
                $provinces[$j+1] = $tempProv;
                $tempCount = $count[$j];
                $count[$j] = $count[$j+1];
                $count[$j + 1] = $tempCount;
                $tempKilled = $killed[$j];
                $killed[$j] = $killed[$j + 1];
                $killed[$j + 1] = $tempKilled;
            }
        }
    }
</code></pre>
<p>The data is a series of strings I parse and grab key points of data.  I do not think there is any issue with parsing the strings as I grab all the data fine.</p>
<p>Here is a sample String:
January 2 of YR2    14 - Respect my authoritah (3:10) invaded 21 - Shredstar (5:5) and killed 589 people.</p>
<p>There are several hundred like that.</p>
<p>My output for a sample is:</p>
<p><a href=""https://i.stack.imgur.com/jo5yN.png"" rel=""nofollow noreferrer"">Website Output</a></p>
<p>Notice the last entries are not in order.  When I run the exact same php file and same data set on my local host they are in order.</p>
<p><a href=""https://i.stack.imgur.com/xh5z8.png"" rel=""nofollow noreferrer"">Local Host Output</a></p>
",40,0,-1,2,php;sorting,2022-07-18 22:54:58,2022-07-18 22:54:58,2022-07-18 23:38:40,pretty much the title   i wrote a script for a game my friends and i play   it parses information from the game and it gives us some useful data   i used a simple bubble sort as there is never going to be more than a few hundred lines of data   the exact same code i upload to my machine to a free webhost server does not display the sort properly   it works on my localhost fine   is there anything i can do  here is the bubble sort   i did not feel like learning php oop so i just used  arrays to hold the  values i needed   i sorted all  arrays by the values held in one of the arrays  the data is a series of strings i parse and grab key points of data   i do not think there is any issue with parsing the strings as i grab all the data fine  there are several hundred like that  my output for a sample is   notice the last entries are not in order   when i run the exact same php file and same data set on my local host they are in order  ,bubble sort works on local host but not on webserver,pretty much title wrote script game friends play parses information game gives us useful data used simple bubble sort never going hundred lines data exact code upload machine free webhost server display sort properly works localhost fine anything bubble sort feel like learning php oop used arrays hold values needed sorted arrays values held one arrays data series strings parse grab key points data think issue parsing strings grab data fine several hundred like output sample notice last entries order run exact php file data set local host order,bubble sort works local host webserver,bubble sort works local host webserverpretty much title wrote script game friends play parses information game gives us useful data used simple bubble sort never going hundred lines data exact code upload machine free webhost server display sort properly works localhost fine anything bubble sort feel like learning php oop used arrays hold values needed sorted arrays values held one arrays data series strings parse grab key points data think issue parsing strings grab data fine several hundred like output sample notice last entries order run exact php file data set local host order,"['bubble', 'sort', 'works', 'local', 'host', 'webserverpretty', 'much', 'title', 'wrote', 'script', 'game', 'friends', 'play', 'parses', 'information', 'game', 'gives', 'us', 'useful', 'data', 'used', 'simple', 'bubble', 'sort', 'never', 'going', 'hundred', 'lines', 'data', 'exact', 'code', 'upload', 'machine', 'free', 'webhost', 'server', 'display', 'sort', 'properly', 'works', 'localhost', 'fine', 'anything', 'bubble', 'sort', 'feel', 'like', 'learning', 'php', 'oop', 'used', 'arrays', 'hold', 'values', 'needed', 'sorted', 'arrays', 'values', 'held', 'one', 'arrays', 'data', 'series', 'strings', 'parse', 'grab', 'key', 'points', 'data', 'think', 'issue', 'parsing', 'strings', 'grab', 'data', 'fine', 'several', 'hundred', 'like', 'output', 'sample', 'notice', 'last', 'entries', 'order', 'run', 'exact', 'php', 'file', 'data', 'set', 'local', 'host', 'order']","['bubbl', 'sort', 'work', 'local', 'host', 'webserverpretti', 'much', 'titl', 'wrote', 'script', 'game', 'friend', 'play', 'pars', 'inform', 'game', 'give', 'us', 'use', 'data', 'use', 'simpl', 'bubbl', 'sort', 'never', 'go', 'hundr', 'line', 'data', 'exact', 'code', 'upload', 'machin', 'free', 'webhost', 'server', 'display', 'sort', 'properli', 'work', 'localhost', 'fine', 'anyth', 'bubbl', 'sort', 'feel', 'like', 'learn', 'php', 'oop', 'use', 'array', 'hold', 'valu', 'need', 'sort', 'array', 'valu', 'held', 'one', 'array', 'data', 'seri', 'string', 'pars', 'grab', 'key', 'point', 'data', 'think', 'issu', 'pars', 'string', 'grab', 'data', 'fine', 'sever', 'hundr', 'like', 'output', 'sampl', 'notic', 'last', 'entri', 'order', 'run', 'exact', 'php', 'file', 'data', 'set', 'local', 'host', 'order']"
169,182,182,16518807,73027314,What machine learning method can be used to predict next element in a sequence?(101001000100001...),"<p>This problem is looking hard to solve with classical machine learning methods, even for deep neural networks</p>
<p>so, the idea is to predict next element in the following sequence
<code>1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0x6 1 0x7 1 ... 1 0xN</code></p>
<p>What is the suitable method(if it exists so far) to solve this problem?</p>
",39,1,-2,5,machine-learning;deep-learning;neural-network;artificial-intelligence;sequence,2022-07-18 21:58:57,2022-07-18 21:58:57,2022-07-18 22:07:55,this problem is looking hard to solve with classical machine learning methods  even for deep neural networks what is the suitable method if it exists so far  to solve this problem ,what machine learning method can be used to predict next element in a sequence      ,problem looking hard solve classical machine learning methods even deep neural networks suitable method exists far solve problem,machine learning method used predict next element sequence,machine learning method used predict next element sequenceproblem looking hard solve classical machine learning methods even deep neural networks suitable method exists far solve problem,"['machine', 'learning', 'method', 'used', 'predict', 'next', 'element', 'sequenceproblem', 'looking', 'hard', 'solve', 'classical', 'machine', 'learning', 'methods', 'even', 'deep', 'neural', 'networks', 'suitable', 'method', 'exists', 'far', 'solve', 'problem']","['machin', 'learn', 'method', 'use', 'predict', 'next', 'element', 'sequenceproblem', 'look', 'hard', 'solv', 'classic', 'machin', 'learn', 'method', 'even', 'deep', 'neural', 'network', 'suitabl', 'method', 'exist', 'far', 'solv', 'problem']"
170,183,183,19519627,72926799,How to use the rear camera instead of the front camera on a teachable machine,"
<br>




    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image
<pre><code>// the link to your model provided by Teachable Machine export panel
const URL = &quot;./my_model/&quot;;

let model, webcam, labelContainer, maxPredictions;

// Load the image model and setup the webcam
async function init() {
    const modelURL = URL + &quot;model.json&quot;;
    const metadataURL = URL + &quot;metadata.json&quot;;
   

    // load the model and metadata
    // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
    // or files from your local hard drive
    // Note: the pose library adds &quot;tmImage&quot; object to your window (window.tmImage)
    model = await tmImage.load(modelURL, metadataURL);
    maxPredictions = model.getTotalClasses();
    
    // Convenience function to setup a webcam

    const size = 350;
    const flip = true; // whether to flip the webcam
    webcam = new tmImage.Webcam(size, size, flip); // width, height, flip
    await webcam.setup(); // request access to the webcam
    await webcam.play();
    window.requestAnimationFrame(loop);

    // append elements to the DOM
    document.getElementById(&quot;webcam-container&quot;).appendChild(webcam.canvas);
    labelContainer = document.getElementById(&quot;label-container&quot;);
    for (let i = 0; i &lt; maxPredictions; i++) { // and class labels
        labelContainer.appendChild(document.createElement(&quot;div&quot;));
    }
}

async function loop() {
    webcam.update(); // update the webcam frame
    await predict();
    window.requestAnimationFrame(loop);
}
</code></pre>
<p>Currently, it is producing a program that can produce food-related information by filming food with a mobile phone rear camera using a learning machine.</p>
<p>I want to use the back camera in this part, but only the front camera keeps coming out. I'd appreciate it if you could help me :)</p>
",31,1,0,4,javascript;camera;webcam;teachable-machine,2022-07-10 10:21:46,2022-07-10 10:21:46,2022-07-18 21:49:25,currently  it is producing a program that can produce food related information by filming food with a mobile phone rear camera using a learning machine  i want to use the back camera in this part  but only the front camera keeps coming out  i d appreciate it if you could help me   ,how to use the rear camera instead of the front camera on a teachable machine,currently producing program produce food related information filming food mobile phone rear camera using learning machine want use back camera part front camera keeps coming appreciate could help,use rear camera instead front camera teachable machine,use rear camera instead front camera teachable machinecurrently producing program produce food related information filming food mobile phone rear camera using learning machine want use back camera part front camera keeps coming appreciate could help,"['use', 'rear', 'camera', 'instead', 'front', 'camera', 'teachable', 'machinecurrently', 'producing', 'program', 'produce', 'food', 'related', 'information', 'filming', 'food', 'mobile', 'phone', 'rear', 'camera', 'using', 'learning', 'machine', 'want', 'use', 'back', 'camera', 'part', 'front', 'camera', 'keeps', 'coming', 'appreciate', 'could', 'help']","['use', 'rear', 'camera', 'instead', 'front', 'camera', 'teachabl', 'machinecurr', 'produc', 'program', 'produc', 'food', 'relat', 'inform', 'film', 'food', 'mobil', 'phone', 'rear', 'camera', 'use', 'learn', 'machin', 'want', 'use', 'back', 'camera', 'part', 'front', 'camera', 'keep', 'come', 'appreci', 'could', 'help']"
171,184,184,19381301,73026841,concatenate tfidf vector arrays to feed machine learning model,"<p>I am working on an active learning project for text classification with a human in the loop. So, after training a model on tfidf vectorised texts and binarised labels and testing the model on unseen data, a human helps the system  by labelling uncertain instances.</p>
<p>After getting  human labels, a new dataset is generated which I need to concatenate to the original train dataset in order to retrain the model and check if the results improved. However, when I do tfidf transformation in  the newly labelled dataset, I get an numpy array that differs in size from the original training dataset. What would be the solution?</p>
<p>Below is the function I am using to preprocess the datasets:</p>
<pre><code>def preprocess(data, X, y):
  global Xfeatures
  global y_train
  multilabel=MultiLabelBinarizer()
  y_train=multilabel.fit_transform(data[y])
  data[X].apply(lambda x:nt.TextFrame(x).noise_scan())
  data[X].apply(lambda x:nt.TextExtractor(x).extract_stopwords())
  corpus_train = data[X].apply(nfx.remove_stopwords)
  tfidf = TfidfVectorizer()
  Xfeatures = tfidf.fit_transform(corpus_train).toarray()

preprocess(df1, 'corpus', 'labels')

   def train_test_split(X, y):
      &quot;&quot;&quot; splitting the dataset into train, test and pool&quot;&quot;&quot;
      global Xfeatures_train
      global y_train
      global X_test
      global y_test
      global Xfeatures_pool
      global y_pool
      Xfeatures_train=X[:200]
      y_train=y[:200]
      print(f'the model will be trained in {Xfeatures_train.shape[0]} text instances 
      and {y_train.shape[0]} label instances')
      X_test=X[201:300]
      y_test=y[201:300]
      print(f'the model will be tested in {X_test.shape[0]} instances text instances 
      and {y_test.shape[0]} label instances')
      Xfeatures_pool= X[300:]
      y_pool=y[300:]
      print(f'the pool of unlabeled texts contains {Xfeatures_pool.shape[0]} 
      instances and {y_pool.shape[0]} label instances')

    train_test_split(Xfeatures, y_train)
</code></pre>
<p>the size of my X_train is (200, 14848)</p>
<p>the size  of the new vectorised dataset I need to concatenate to X_train is (40, 563)</p>
<p>So, they differ in number of columns. What would be the best approach in this case?</p>
",15,0,0,5,python;numpy;tensorflow;numpy-ndarray;tf-idf,2022-07-18 21:16:33,2022-07-18 21:16:33,2022-07-18 21:16:33,i am working on an active learning project for text classification with a human in the loop  so  after training a model on tfidf vectorised texts and binarised labels and testing the model on unseen data  a human helps the system  by labelling uncertain instances  after getting  human labels  a new dataset is generated which i need to concatenate to the original train dataset in order to retrain the model and check if the results improved  however  when i do tfidf transformation in  the newly labelled dataset  i get an numpy array that differs in size from the original training dataset  what would be the solution  below is the function i am using to preprocess the datasets  the size of my x_train is      the size  of the new vectorised dataset i need to concatenate to x_train is      so  they differ in number of columns  what would be the best approach in this case ,concatenate tfidf vector arrays to feed machine learning model,working active learning project text classification human loop training model tfidf vectorised texts binarised labels testing model unseen data human helps system labelling uncertain instances getting human labels dataset generated need concatenate original train dataset order retrain model check results improved however tfidf transformation newly labelled dataset get numpy array differs size original training dataset would solution function using preprocess datasets size x_train size vectorised dataset need concatenate x_train differ number columns would best approach case,concatenate tfidf vector arrays feed machine learning model,concatenate tfidf vector arrays feed machine learning modelworking active learning project text classification human loop training model tfidf vectorised texts binarised labels testing model unseen data human helps system labelling uncertain instances getting human labels dataset generated need concatenate original train dataset order retrain model check results improved however tfidf transformation newly labelled dataset get numpy array differs size original training dataset would solution function using preprocess datasets size x_train size vectorised dataset need concatenate x_train differ number columns would best approach case,"['concatenate', 'tfidf', 'vector', 'arrays', 'feed', 'machine', 'learning', 'modelworking', 'active', 'learning', 'project', 'text', 'classification', 'human', 'loop', 'training', 'model', 'tfidf', 'vectorised', 'texts', 'binarised', 'labels', 'testing', 'model', 'unseen', 'data', 'human', 'helps', 'system', 'labelling', 'uncertain', 'instances', 'getting', 'human', 'labels', 'dataset', 'generated', 'need', 'concatenate', 'original', 'train', 'dataset', 'order', 'retrain', 'model', 'check', 'results', 'improved', 'however', 'tfidf', 'transformation', 'newly', 'labelled', 'dataset', 'get', 'numpy', 'array', 'differs', 'size', 'original', 'training', 'dataset', 'would', 'solution', 'function', 'using', 'preprocess', 'datasets', 'size', 'x_train', 'size', 'vectorised', 'dataset', 'need', 'concatenate', 'x_train', 'differ', 'number', 'columns', 'would', 'best', 'approach', 'case']","['concaten', 'tfidf', 'vector', 'array', 'feed', 'machin', 'learn', 'modelwork', 'activ', 'learn', 'project', 'text', 'classif', 'human', 'loop', 'train', 'model', 'tfidf', 'vectoris', 'text', 'binaris', 'label', 'test', 'model', 'unseen', 'data', 'human', 'help', 'system', 'label', 'uncertain', 'instanc', 'get', 'human', 'label', 'dataset', 'gener', 'need', 'concaten', 'origin', 'train', 'dataset', 'order', 'retrain', 'model', 'check', 'result', 'improv', 'howev', 'tfidf', 'transform', 'newli', 'label', 'dataset', 'get', 'numpi', 'array', 'differ', 'size', 'origin', 'train', 'dataset', 'would', 'solut', 'function', 'use', 'preprocess', 'dataset', 'size', 'x_train', 'size', 'vectoris', 'dataset', 'need', 'concaten', 'x_train', 'differ', 'number', 'column', 'would', 'best', 'approach', 'case']"
172,185,185,19572829,73024262,Error running R code from project (&quot;Error in cat == &quot;INS&quot; : comparison (1) is possible only for atomic and list types&quot;),"<p>I'm trying to recreate code used in a Stanford paper on machine learning for environmental monitoring. The paper is <a href=""https://static1.squarespace.com/static/5bf34064c3c16a648f15d85b/t/5bf3d37503ce64eaeba7bab2/1542706045258/Hino+Benami+Brooks+2018+Machine+learning+for+environmental+monitoring.pdf"" rel=""nofollow noreferrer"">here</a> and the code is <a href=""https://purl.stanford.edu/hr919hp5420"" rel=""nofollow noreferrer"">here</a>. Obviously, it was working for the authors but in trying to make it work, I've stumbled on the error:</p>
<blockquote>
<p>Error in cat == &quot;INS&quot; :    comparison (1) is possible only for atomic
and list types</p>
</blockquote>
<p>When looking at the original code the offending line is line 481. I've excerpted the lines at issue below. Any guidance is appreciated.</p>
<pre><code>df.marker &lt;- 1 #this is added to the output label
      
      #both of these need to be split into inspected/notinspected
        test &lt;- as.data.frame(test.random[test$cat == &quot;INS&quot;])
        train &lt;- as.data.frame(train.random[cat == &quot;INS&quot;])
</code></pre>
",22,0,0,3,r;machine-learning;project,2022-07-18 17:47:46,2022-07-18 17:47:46,2022-07-18 19:57:22,i m trying to recreate code used in a stanford paper on machine learning for environmental monitoring  the paper is  and the code is   obviously  it was working for the authors but in trying to make it work  i ve stumbled on the error  when looking at the original code the offending line is line   i ve excerpted the lines at issue below  any guidance is appreciated ,error running r code from project   error in cat     ins    comparison    is possible only for atomic and list types  ,trying recreate code used stanford paper machine learning environmental monitoring paper code obviously working authors trying make work stumbled error looking original code offending line line excerpted lines issue guidance appreciated,error running r code project error cat ins comparison possible atomic types,error running r code project error cat ins comparison possible atomic typestrying recreate code used stanford paper machine learning environmental monitoring paper code obviously working authors trying make work stumbled error looking original code offending line line excerpted lines issue guidance appreciated,"['error', 'running', 'r', 'code', 'project', 'error', 'cat', 'ins', 'comparison', 'possible', 'atomic', 'typestrying', 'recreate', 'code', 'used', 'stanford', 'paper', 'machine', 'learning', 'environmental', 'monitoring', 'paper', 'code', 'obviously', 'working', 'authors', 'trying', 'make', 'work', 'stumbled', 'error', 'looking', 'original', 'code', 'offending', 'line', 'line', 'excerpted', 'lines', 'issue', 'guidance', 'appreciated']","['error', 'run', 'r', 'code', 'project', 'error', 'cat', 'in', 'comparison', 'possibl', 'atom', 'typestri', 'recreat', 'code', 'use', 'stanford', 'paper', 'machin', 'learn', 'environment', 'monitor', 'paper', 'code', 'obvious', 'work', 'author', 'tri', 'make', 'work', 'stumbl', 'error', 'look', 'origin', 'code', 'offend', 'line', 'line', 'excerpt', 'line', 'issu', 'guidanc', 'appreci']"
173,186,186,18609891,73024833,Is this the correct way of using SVM model in my case?,"<p>So, I started a project to construct a real face out of  a painted , first step was applying a face detector to get only the face and then apply a 68 dlib facial landmarks.</p>
<p>second step was based on this particular landmarks points obtained I computed Euclidean distances between one specific landmark point which is the N27 landmark with  the rest of the landmarks (as shown in [1]), which is about 63 distances and  then i stored this results for this image in a vector.
[ https://i.stack.imgur.com/N34cJ.png]      <strong>[1]</strong></p>
<p>step 3 was just doing what I did in step 1 and 2 but for a large dataset of  faces and stored them this time in a dataframe which resulted in giving me a dataframe of (2000 x 63) where 2099 is the number of faces in the dataset folder and 63 is the number of distances I calculated for each face, the dataframe looks like this :</p>
<pre><code>      print(df)

               Faces  ...          Orientation_V
     1        face 1  ...  -0.018532666634075004
     2        face 2  ...  -0.028847144005773918
     3        face 3  ...     0.0441854315476044
     4        face 4  ...   0.018626215579301213
     5        face 5  ...    0.08838870341944553
     ...         ...  ...                    ...
    2095  face 2095  ...   0.012677967883353333
    2096  face 2096  ...    0.10165486101866342
    2097  face 2097  ...    0.08712556551986858
    2098  face 2098  ...    0.09420965696835078
    2099  face 2099  ...    0.06901196982857326


    [2099 rows x 63 columns]
</code></pre>
<p>step 4 , is where I'm stuck I'm supposed to apply a machine learning model to find the most similar face in that dataset for my painting and I'm kind of confused of how to do it, any help or documentation is welcome</p>
<p>what i did to try is like this :</p>
<pre><code>x=df.drop('Faces',axis=1)
y=df[&quot;Faces&quot;]
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=0)
from sklearn.svm import SVC  
model=SVC()
model.fit(x_train,y_train)
predictions=model.predict(x_test)
</code></pre>
<p>the problem here is the y_train and y_test label are not the same i can't find a way of dealing with this issue</p>
",25,0,0,5,python;opencv;machine-learning;computer-vision;data-science,2022-07-18 18:27:06,2022-07-18 18:27:06,2022-07-18 18:27:06,so  i started a project to construct a real face out of  a painted   first step was applying a face detector to get only the face and then apply a  dlib facial landmarks  step  was just doing what i did in step  and  but for a large dataset of  faces and stored them this time in a dataframe which resulted in giving me a dataframe of   x   where  is the number of faces in the dataset folder and  is the number of distances i calculated for each face  the dataframe looks like this   step    is where i m stuck i m supposed to apply a machine learning model to find the most similar face in that dataset for my painting and i m kind of confused of how to do it  any help or documentation is welcome what i did to try is like this   the problem here is the y_train and y_test label are not the same i can t find a way of dealing with this issue,is this the correct way of using svm model in my case ,started project construct real face painted first step applying face detector get face apply dlib facial landmarks step step large dataset faces stored time dataframe resulted giving dataframe x number faces dataset folder number distances calculated face dataframe looks like step stuck supposed apply machine learning model find similar face dataset painting kind confused help documentation welcome try like problem y_train y_test label find way dealing issue,correct way using svm model case,correct way using svm model casestarted project construct real face painted first step applying face detector get face apply dlib facial landmarks step step large dataset faces stored time dataframe resulted giving dataframe x number faces dataset folder number distances calculated face dataframe looks like step stuck supposed apply machine learning model find similar face dataset painting kind confused help documentation welcome try like problem y_train y_test label find way dealing issue,"['correct', 'way', 'using', 'svm', 'model', 'casestarted', 'project', 'construct', 'real', 'face', 'painted', 'first', 'step', 'applying', 'face', 'detector', 'get', 'face', 'apply', 'dlib', 'facial', 'landmarks', 'step', 'step', 'large', 'dataset', 'faces', 'stored', 'time', 'dataframe', 'resulted', 'giving', 'dataframe', 'x', 'number', 'faces', 'dataset', 'folder', 'number', 'distances', 'calculated', 'face', 'dataframe', 'looks', 'like', 'step', 'stuck', 'supposed', 'apply', 'machine', 'learning', 'model', 'find', 'similar', 'face', 'dataset', 'painting', 'kind', 'confused', 'help', 'documentation', 'welcome', 'try', 'like', 'problem', 'y_train', 'y_test', 'label', 'find', 'way', 'dealing', 'issue']","['correct', 'way', 'use', 'svm', 'model', 'casestart', 'project', 'construct', 'real', 'face', 'paint', 'first', 'step', 'appli', 'face', 'detector', 'get', 'face', 'appli', 'dlib', 'facial', 'landmark', 'step', 'step', 'larg', 'dataset', 'face', 'store', 'time', 'datafram', 'result', 'give', 'datafram', 'x', 'number', 'face', 'dataset', 'folder', 'number', 'distanc', 'calcul', 'face', 'datafram', 'look', 'like', 'step', 'stuck', 'suppos', 'appli', 'machin', 'learn', 'model', 'find', 'similar', 'face', 'dataset', 'paint', 'kind', 'confus', 'help', 'document', 'welcom', 'tri', 'like', 'problem', 'y_train', 'y_test', 'label', 'find', 'way', 'deal', 'issu']"
174,187,187,19520875,73024052,URLError: &lt;urlopen error [Errno -3] Temporary failure in name resolution&gt; error on Kaggle,"<p>I want to pre-train the pytorch <code>inceptionv4</code> machine learning model using <code>imagenet</code>. My code raised <code>URLError: &lt;urlopen error [Errno -3] Temporary failure in name resolution&gt;</code> when I run it on Kaggle.</p>
<p>Train with imagenet</p>
<pre><code>pretrained_settings = {
    'inceptionv4': {
        'imagenet': {
            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',
            'input_space': 'RGB',
            'input_size': [3, 299, 299],
            'input_range': [0, 1],
            'mean': [0.5, 0.5, 0.5],
            'std': [0.5, 0.5, 0.5],
            'num_classes': 1000
        },
        'imagenet+background': {
            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',
            'input_space': 'RGB',
            'input_size': [3, 299, 299],
            'input_range': [0, 1],
            'mean': [0.5, 0.5, 0.5],
            'std': [0.5, 0.5, 0.5],
            'num_classes': 1001
        }
    }
}
</code></pre>
<p>Inceptionv4 function</p>
<pre><code>def inceptionv4(num_classes=1000, pretrained='imagenet'):
    if pretrained:
        settings = pretrained_settings['inceptionv4'][pretrained]
        assert num_classes == settings['num_classes'], \
            &quot;num_classes should be {}, but is {}&quot;.format(settings['num_classes'], num_classes)

        # both 'imagenet'&amp;'imagenet+background' are loaded from same parameters
        model = InceptionV4(num_classes=1001)
        model.load_state_dict(model_zoo.load_url(settings['url']))

        if pretrained == 'imagenet':
            new_last_linear = nn.Linear(1536, 1000)
            new_last_linear.weight.data = model.last_linear.weight.data[1:]
            new_last_linear.bias.data = model.last_linear.bias.data[1:]
            model.last_linear = new_last_linear

        model.input_space = settings['input_space']
        model.input_size = settings['input_size']
        model.input_range = settings['input_range']
        model.mean = settings['mean']
        model.std = settings['std']
    else:
        model = InceptionV4(num_classes=num_classes)
    return model
</code></pre>
<p>Call the model</p>
<pre><code>models = [
    inceptionv4(num_classes=1000, pretrained=&quot;imagenet&quot;)
]
for model in models:
    model.to(device)
</code></pre>
<p>Traceback:</p>
<pre><code>Downloading: &quot;http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth&quot; to /root/.cache/torch/hub/checkpoints/inceptionv4-8e4777a0.pth
---------------------------------------------------------------------------
gaierror                                  Traceback (most recent call last)
/opt/conda/lib/python3.7/urllib/request.py in do_open(self, http_class, req, **http_conn_args)
   1349                 h.request(req.get_method(), req.selector, req.data, headers,
-&gt; 1350                           encode_chunked=req.has_header('Transfer-encoding'))
   1351             except OSError as err: # timeout error

/opt/conda/lib/python3.7/http/client.py in request(self, method, url, body, headers, encode_chunked)
   1280         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;
-&gt; 1281         self._send_request(method, url, body, headers, encode_chunked)
   1282 

/opt/conda/lib/python3.7/http/client.py in _send_request(self, method, url, body, headers, encode_chunked)
   1326             body = _encode(body, 'body')
-&gt; 1327         self.endheaders(body, encode_chunked=encode_chunked)
   1328 

/opt/conda/lib/python3.7/http/client.py in endheaders(self, message_body, encode_chunked)
   1275             raise CannotSendHeader()
-&gt; 1276         self._send_output(message_body, encode_chunked=encode_chunked)
   1277 

/opt/conda/lib/python3.7/http/client.py in _send_output(self, message_body, encode_chunked)
   1035         del self._buffer[:]
-&gt; 1036         self.send(msg)
   1037 

/opt/conda/lib/python3.7/http/client.py in send(self, data)
    975             if self.auto_open:
--&gt; 976                 self.connect()
    977             else:

/opt/conda/lib/python3.7/http/client.py in connect(self)
    947         self.sock = self._create_connection(
--&gt; 948             (self.host,self.port), self.timeout, self.source_address)
    949         self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

/opt/conda/lib/python3.7/socket.py in create_connection(address, timeout, source_address)
    706     err = None
--&gt; 707     for res in getaddrinfo(host, port, 0, SOCK_STREAM):
    708         af, socktype, proto, canonname, sa = res

/opt/conda/lib/python3.7/socket.py in getaddrinfo(host, port, family, type, proto, flags)
    751     addrlist = []
--&gt; 752     for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
    753         af, socktype, proto, canonname, sa = res

gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

URLError                                  Traceback (most recent call last)
/tmp/ipykernel_33/2322807599.py in &lt;module&gt;
      1 models = [
----&gt; 2     inceptionv4(num_classes=1000, pretrained=&quot;imagenet&quot;),
      3 
      4 #     inceptionresnetv2(),
      5 ]

/kaggle/input/models/inceptionv4.py in inceptionv4(num_classes, pretrained)
    320         # both 'imagenet'&amp;'imagenet+background' are loaded from same parameters
    321         model = InceptionV4(num_classes=1001)
--&gt; 322         model.load_state_dict(model_zoo.load_url(settings['url']))
    323 
    324         if pretrained == 'imagenet':

/opt/conda/lib/python3.7/site-packages/torch/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress, check_hash, file_name)
    589             r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]
    590             hash_prefix = r.group(1) if r else None
--&gt; 591         download_url_to_file(url, cached_file, hash_prefix, progress=progress)
    592 
    593     if _is_legacy_zip_format(cached_file):

/opt/conda/lib/python3.7/site-packages/torch/hub.py in download_url_to_file(url, dst, hash_prefix, progress)
    455     file_size = None
    456     req = Request(url, headers={&quot;User-Agent&quot;: &quot;torch.hub&quot;})
--&gt; 457     u = urlopen(req)
    458     meta = u.info()
    459     if hasattr(meta, 'getheaders'):

/opt/conda/lib/python3.7/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    220     else:
    221         opener = _opener
--&gt; 222     return opener.open(url, data, timeout)
    223 
    224 def install_opener(opener):

/opt/conda/lib/python3.7/urllib/request.py in open(self, fullurl, data, timeout)
    523             req = meth(req)
    524 
--&gt; 525         response = self._open(req, data)
    526 
    527         # post-process response

/opt/conda/lib/python3.7/urllib/request.py in _open(self, req, data)
    541         protocol = req.type
    542         result = self._call_chain(self.handle_open, protocol, protocol +
--&gt; 543                                   '_open', req)
    544         if result:
    545             return result

/opt/conda/lib/python3.7/urllib/request.py in _call_chain(self, chain, kind, meth_name, *args)
    501         for handler in handlers:
    502             func = getattr(handler, meth_name)
--&gt; 503             result = func(*args)
    504             if result is not None:
    505                 return result

/opt/conda/lib/python3.7/urllib/request.py in http_open(self, req)
   1376 
   1377     def http_open(self, req):
-&gt; 1378         return self.do_open(http.client.HTTPConnection, req)
   1379 
   1380     http_request = AbstractHTTPHandler.do_request_

/opt/conda/lib/python3.7/urllib/request.py in do_open(self, http_class, req, **http_conn_args)
   1350                           encode_chunked=req.has_header('Transfer-encoding'))
   1351             except OSError as err: # timeout error
-&gt; 1352                 raise URLError(err)
   1353             r = h.getresponse()
   1354         except:

URLError: &lt;urlopen error [Errno -3] Temporary failure in name resolution&gt;
</code></pre>
",7,0,0,5,python;machine-learning;pytorch;client;kaggle,2022-07-18 17:32:47,2022-07-18 17:32:47,2022-07-18 17:32:47,i want to pre train the pytorch inceptionv machine learning model using imagenet  my code raised urlerror   lt urlopen error  errno    temporary failure in name resolution gt  when i run it on kaggle  train with imagenet inceptionv function call the model traceback ,urlerror   lt urlopen error  errno    temporary failure in name resolution gt  error on kaggle,want pre train pytorch inceptionv machine learning model using imagenet code raised urlerror lt urlopen error errno temporary failure name resolution gt run kaggle train imagenet inceptionv function call model traceback,urlerror lt urlopen error errno temporary failure name resolution gt error kaggle,urlerror lt urlopen error errno temporary failure name resolution gt error kagglewant pre train pytorch inceptionv machine learning model using imagenet code raised urlerror lt urlopen error errno temporary failure name resolution gt run kaggle train imagenet inceptionv function call model traceback,"['urlerror', 'lt', 'urlopen', 'error', 'errno', 'temporary', 'failure', 'name', 'resolution', 'gt', 'error', 'kagglewant', 'pre', 'train', 'pytorch', 'inceptionv', 'machine', 'learning', 'model', 'using', 'imagenet', 'code', 'raised', 'urlerror', 'lt', 'urlopen', 'error', 'errno', 'temporary', 'failure', 'name', 'resolution', 'gt', 'run', 'kaggle', 'train', 'imagenet', 'inceptionv', 'function', 'call', 'model', 'traceback']","['urlerror', 'lt', 'urlopen', 'error', 'errno', 'temporari', 'failur', 'name', 'resolut', 'gt', 'error', 'kagglew', 'pre', 'train', 'pytorch', 'inceptionv', 'machin', 'learn', 'model', 'use', 'imagenet', 'code', 'rais', 'urlerror', 'lt', 'urlopen', 'error', 'errno', 'temporari', 'failur', 'name', 'resolut', 'gt', 'run', 'kaggl', 'train', 'imagenet', 'inceptionv', 'function', 'call', 'model', 'traceback']"
175,188,188,80353,47595396,"Using machine learning to parse excel file and extract table data with no named tables involved? If so, how do I get started?","<p>I have read several articles on machine learning.</p>

<p>An example is this <a href=""https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1"" rel=""nofollow noreferrer"">https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1</a> and was wondering if it's possible to modify the code to do the use case below.</p>

<p>Let's say the machine is given an excel file (assuming .xlsx) that has only 1 sheet.</p>

<p>Inside the sheet there will be a table (defined as having a header row and at least 2 data rows and will be agreed as a table by common sense humans who read the file. Not a named table or object as defined by MS Excel)</p>

<p>The following is unknown before hand:</p>

<ul>
<li>the position of the table</li>
<li>the number of rows and columns</li>
<li>the data type in the table</li>
</ul>

<p>This table in the excel file is also <strong>NOT</strong> <a href=""http://www.contextures.com/xlExcelTable01.html#rename"" rel=""nofollow noreferrer"">named or identified beforehand</a> in any way. <strong>There are no objects in this excel file. No named tables.</strong> </p>

<p>If I were to attempt to use VBA to list objects, I will get this error message</p>

<p><a href=""https://i.stack.imgur.com/YQr2D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQr2D.png"" alt=""enter image description here""></a></p>

<p>There will be at least 1 other cell outside of this table that will contain some text or value.</p>

<p>How or what algorithm using what software library can I train a machine to identify where does the table start and end?</p>

<p>E.g.</p>

<p>The table first cell (identified as the top left most cell in the header row) is A4 and the table last cell (identified as the bottom right most cell) is G12</p>

<p>The machine is able to read the excel file and say that the table is at A4:G12 even if there are other cells outside the table that has data.</p>

<p>My various google search involving ""excel and machine learning"" or ""identify tables in excel"" tends to give me articles on how to feed data to machine learning software using excel files or how to find/name data tables that are named in the Excel. <strong>I want to emphasize the table data is NOT named.</strong></p>

<p>Also come across <a href=""https://stackoverflow.com/questions/28532770/extract-identify-tables-from-pdf-python"">answers</a> talking about extracting such table data from PDFs. But my focus is on Excel.</p>

<p>I have read that machine learning can be used to read images. So I imagine trying to ""read"" an excel file which is a highly structured file just to find where a table is should be possible.</p>

<p>Here are some sample screenshots</p>

<p><a href=""https://i.stack.imgur.com/ng6g1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ng6g1.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/LwWy8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LwWy8.png"" alt=""enter image description here""></a></p>

<p>Here are the links to the Excel files</p>

<p><a href=""https://www.dropbox.com/s/l3vjjsgunp0zu23/A4toG12.xlsx?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/l3vjjsgunp0zu23/A4toG12.xlsx?dl=0</a></p>

<p><a href=""https://www.dropbox.com/s/nwzw0211ruhwvf0/G7toN19.xlsx?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/nwzw0211ruhwvf0/G7toN19.xlsx?dl=0</a></p>

<p>Please note that these are dummy files to illustrate my point about identifying the table data.</p>

<ul>
<li>The first one should be identified as A4 to G12</li>
<li>The second one should be identified as G7 to N19</li>
</ul>

<p>The comment by <a href=""https://stackoverflow.com/users/5457259/janlauge"">JanLauGe</a> about <code>ctrl</code> + <code>a</code> is interesting See <a href=""https://stackoverflow.com/questions/47595396/using-machine-learning-to-parse-excel-file-and-extract-table-data-with-no-named?noredirect=1#comment82149511_47595541"">Using machine learning to parse excel file and extract table data with no named tables involved? If so, how do I get started?</a></p>

<p>I have googled for the equivalent in VBA but to no avail.</p>

<h2>What I have tried</h2>

<p>I wrote a script as suggested by Scott Craner to find all the cells with values I get this.</p>

<p>Thanks to his comments, I got it to work.</p>

<pre><code>Public Sub LookForCells()
    For Each block In ActiveSheet.Columns(""A:Z"").SpecialCells(xlCellTypeConstants, 23).Areas
       MsgBox block.Count
    Next block
End Sub
</code></pre>

<p>And you should see the following</p>

<p><a href=""https://i.stack.imgur.com/434Vk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/434Vk.png"" alt=""enter image description here""></a></p>
",3779,2,2,3,excel;machine-learning;deep-learning,2017-12-01 16:21:16,2017-12-01 16:21:16,2022-07-18 15:36:27,i have read several articles on machine learning  an example is this  and was wondering if it s possible to modify the code to do the use case below  let s say the machine is given an excel file  assuming  xlsx  that has only  sheet  inside the sheet there will be a table  defined as having a header row and at least  data rows and will be agreed as a table by common sense humans who read the file  not a named table or object as defined by ms excel  the following is unknown before hand  this table in the excel file is also not  in any way  there are no objects in this excel file  no named tables   if i were to attempt to use vba to list objects  i will get this error message  there will be at least  other cell outside of this table that will contain some text or value  how or what algorithm using what software library can i train a machine to identify where does the table start and end  e g  the table first cell  identified as the top left most cell in the header row  is a and the table last cell  identified as the bottom right most cell  is g the machine is able to read the excel file and say that the table is at a g even if there are other cells outside the table that has data  my various google search involving excel and machine learning or identify tables in excel tends to give me articles on how to feed data to machine learning software using excel files or how to find name data tables that are named in the excel  i want to emphasize the table data is not named  also come across  talking about extracting such table data from pdfs  but my focus is on excel  i have read that machine learning can be used to read images  so i imagine trying to read an excel file which is a highly structured file just to find where a table is should be possible  here are some sample screenshots   here are the links to the excel files   please note that these are dummy files to illustrate my point about identifying the table data  the comment by  about ctrl   a is interesting see  i have googled for the equivalent in vba but to no avail  i wrote a script as suggested by scott craner to find all the cells with values i get this  thanks to his comments  i got it to work  and you should see the following ,using machine learning to parse excel file and extract table data with no named tables involved  if so  how do i get started ,read several articles machine learning example wondering possible modify code use case let say machine given excel file assuming xlsx sheet inside sheet table defined header row least data rows agreed table common sense humans read file named table object defined ms excel following unknown hand table excel file also way objects excel file named tables attempt use vba objects get error message least cell outside table contain text value algorithm using software library train machine identify table start end e g table first cell identified top left cell header row table last cell identified bottom right cell g machine able read excel file say table g even cells outside table data various google search involving excel machine learning identify tables excel tends give articles feed data machine learning software using excel files find name data tables named excel want emphasize table data named also come across talking extracting table data pdfs focus excel read machine learning used read images imagine trying read excel file highly structured file find table possible sample screenshots links excel files please note dummy files illustrate point identifying table data comment ctrl interesting see googled equivalent vba avail wrote script suggested scott craner find cells values get thanks comments got work see following,using machine learning parse excel file extract table data named tables involved get started,using machine learning parse excel file extract table data named tables involved get startedread several articles machine learning example wondering possible modify code use case let say machine given excel file assuming xlsx sheet inside sheet table defined header row least data rows agreed table common sense humans read file named table object defined ms excel following unknown hand table excel file also way objects excel file named tables attempt use vba objects get error message least cell outside table contain text value algorithm using software library train machine identify table start end e g table first cell identified top left cell header row table last cell identified bottom right cell g machine able read excel file say table g even cells outside table data various google search involving excel machine learning identify tables excel tends give articles feed data machine learning software using excel files find name data tables named excel want emphasize table data named also come across talking extracting table data pdfs focus excel read machine learning used read images imagine trying read excel file highly structured file find table possible sample screenshots links excel files please note dummy files illustrate point identifying table data comment ctrl interesting see googled equivalent vba avail wrote script suggested scott craner find cells values get thanks comments got work see following,"['using', 'machine', 'learning', 'parse', 'excel', 'file', 'extract', 'table', 'data', 'named', 'tables', 'involved', 'get', 'startedread', 'several', 'articles', 'machine', 'learning', 'example', 'wondering', 'possible', 'modify', 'code', 'use', 'case', 'let', 'say', 'machine', 'given', 'excel', 'file', 'assuming', 'xlsx', 'sheet', 'inside', 'sheet', 'table', 'defined', 'header', 'row', 'least', 'data', 'rows', 'agreed', 'table', 'common', 'sense', 'humans', 'read', 'file', 'named', 'table', 'object', 'defined', 'ms', 'excel', 'following', 'unknown', 'hand', 'table', 'excel', 'file', 'also', 'way', 'objects', 'excel', 'file', 'named', 'tables', 'attempt', 'use', 'vba', 'objects', 'get', 'error', 'message', 'least', 'cell', 'outside', 'table', 'contain', 'text', 'value', 'algorithm', 'using', 'software', 'library', 'train', 'machine', 'identify', 'table', 'start', 'end', 'e', 'g', 'table', 'first', 'cell', 'identified', 'top', 'left', 'cell', 'header', 'row', 'table', 'last', 'cell', 'identified', 'bottom', 'right', 'cell', 'g', 'machine', 'able', 'read', 'excel', 'file', 'say', 'table', 'g', 'even', 'cells', 'outside', 'table', 'data', 'various', 'google', 'search', 'involving', 'excel', 'machine', 'learning', 'identify', 'tables', 'excel', 'tends', 'give', 'articles', 'feed', 'data', 'machine', 'learning', 'software', 'using', 'excel', 'files', 'find', 'name', 'data', 'tables', 'named', 'excel', 'want', 'emphasize', 'table', 'data', 'named', 'also', 'come', 'across', 'talking', 'extracting', 'table', 'data', 'pdfs', 'focus', 'excel', 'read', 'machine', 'learning', 'used', 'read', 'images', 'imagine', 'trying', 'read', 'excel', 'file', 'highly', 'structured', 'file', 'find', 'table', 'possible', 'sample', 'screenshots', 'links', 'excel', 'files', 'please', 'note', 'dummy', 'files', 'illustrate', 'point', 'identifying', 'table', 'data', 'comment', 'ctrl', 'interesting', 'see', 'googled', 'equivalent', 'vba', 'avail', 'wrote', 'script', 'suggested', 'scott', 'craner', 'find', 'cells', 'values', 'get', 'thanks', 'comments', 'got', 'work', 'see', 'following']","['use', 'machin', 'learn', 'pars', 'excel', 'file', 'extract', 'tabl', 'data', 'name', 'tabl', 'involv', 'get', 'startedread', 'sever', 'articl', 'machin', 'learn', 'exampl', 'wonder', 'possibl', 'modifi', 'code', 'use', 'case', 'let', 'say', 'machin', 'given', 'excel', 'file', 'assum', 'xlsx', 'sheet', 'insid', 'sheet', 'tabl', 'defin', 'header', 'row', 'least', 'data', 'row', 'agre', 'tabl', 'common', 'sens', 'human', 'read', 'file', 'name', 'tabl', 'object', 'defin', 'ms', 'excel', 'follow', 'unknown', 'hand', 'tabl', 'excel', 'file', 'also', 'way', 'object', 'excel', 'file', 'name', 'tabl', 'attempt', 'use', 'vba', 'object', 'get', 'error', 'messag', 'least', 'cell', 'outsid', 'tabl', 'contain', 'text', 'valu', 'algorithm', 'use', 'softwar', 'librari', 'train', 'machin', 'identifi', 'tabl', 'start', 'end', 'e', 'g', 'tabl', 'first', 'cell', 'identifi', 'top', 'left', 'cell', 'header', 'row', 'tabl', 'last', 'cell', 'identifi', 'bottom', 'right', 'cell', 'g', 'machin', 'abl', 'read', 'excel', 'file', 'say', 'tabl', 'g', 'even', 'cell', 'outsid', 'tabl', 'data', 'variou', 'googl', 'search', 'involv', 'excel', 'machin', 'learn', 'identifi', 'tabl', 'excel', 'tend', 'give', 'articl', 'feed', 'data', 'machin', 'learn', 'softwar', 'use', 'excel', 'file', 'find', 'name', 'data', 'tabl', 'name', 'excel', 'want', 'emphas', 'tabl', 'data', 'name', 'also', 'come', 'across', 'talk', 'extract', 'tabl', 'data', 'pdf', 'focu', 'excel', 'read', 'machin', 'learn', 'use', 'read', 'imag', 'imagin', 'tri', 'read', 'excel', 'file', 'highli', 'structur', 'file', 'find', 'tabl', 'possibl', 'sampl', 'screenshot', 'link', 'excel', 'file', 'pleas', 'note', 'dummi', 'file', 'illustr', 'point', 'identifi', 'tabl', 'data', 'comment', 'ctrl', 'interest', 'see', 'googl', 'equival', 'vba', 'avail', 'wrote', 'script', 'suggest', 'scott', 'craner', 'find', 'cell', 'valu', 'get', 'thank', 'comment', 'got', 'work', 'see', 'follow']"
176,189,189,16841993,73018863,How can we reduce weak hit records using Machine Learning in Elasticsearch?,"<p>I just learn the Elasticsearch app instance. I build the query to search data in Elasticsearch but it seems doesn't match really well. I want to use Machine Language to reduce the weak hit records.</p>
",24,0,-2,4,java;kotlin;elasticsearch;elasticsearch-5,2022-07-18 10:46:58,2022-07-18 10:46:58,2022-07-18 13:10:44,i just learn the elasticsearch app instance  i build the query to search data in elasticsearch but it seems doesn t match really well  i want to use machine language to reduce the weak hit records ,how can we reduce weak hit records using machine learning in elasticsearch ,learn elasticsearch app instance build query search data elasticsearch seems match really well want use machine language reduce weak hit records,reduce weak hit records using machine learning elasticsearch,reduce weak hit records using machine learning elasticsearchlearn elasticsearch app instance build query search data elasticsearch seems match really well want use machine language reduce weak hit records,"['reduce', 'weak', 'hit', 'records', 'using', 'machine', 'learning', 'elasticsearchlearn', 'elasticsearch', 'app', 'instance', 'build', 'query', 'search', 'data', 'elasticsearch', 'seems', 'match', 'really', 'well', 'want', 'use', 'machine', 'language', 'reduce', 'weak', 'hit', 'records']","['reduc', 'weak', 'hit', 'record', 'use', 'machin', 'learn', 'elasticsearchlearn', 'elasticsearch', 'app', 'instanc', 'build', 'queri', 'search', 'data', 'elasticsearch', 'seem', 'match', 'realli', 'well', 'want', 'use', 'machin', 'languag', 'reduc', 'weak', 'hit', 'record']"
177,191,191,19562893,73006451,How can I work on my funtion to make it work and transfer 1 dimensinal to n dimensional?,"<p>I need to create a simulated data set.</p>
<p>I have an idea to my theory and would share it with you all :
I have an idea that if i have random value of 0.8(True probability) is generated that mean it goes to class 1(Prob 0-1) and now as the data set contains of some featurevalues .Now I want to find the values of the featurevalues and the algorithm for that and  I need the value of feature values also similar to The Value of my random value to insure that my Machine Learning Model is working and it gives me a right class Label ( Expected Class 1) in the correct manner.</p>
<pre><code>f(x) = a*featurevalue1+ b*featurevalue2 + ... + n*featurevalue(n)
</code></pre>
<p>where x = (For example = 0.8)</p>
<p>Info About my Model to understand my problem better:</p>
<p>My Machine Learning Framework gives me probablistic Probability Interval (Low and high bound).
I want to check that if the True Probability (In the interval of values from the model)and the true posibility of each row in above described and created data set are equal .</p>
<p>I have an idea given by my professor and i need help understanding :</p>
<blockquote>
<p>Normally, however, one specifies priors P(y) and class conditionals P(x|y), and then computes/samples P(x,y) = P(y)P(x|y). The probabilities are then obtained through P(y|x) \propto P(x|y)*P(y). For example P(x|y) could be Gaussian</p>
</blockquote>
",34,0,-1,4,python;dataframe;machine-learning;data-science,2022-07-16 20:47:41,2022-07-16 20:47:41,2022-07-18 10:51:41,i need to create a simulated data set  where x    for example      info about my model to understand my problem better  i have an idea given by my professor and i need help understanding   normally  however  one specifies priors p y  and class conditionals p x y   and then computes samples p x y    p y p x y   the probabilities are then obtained through p y x   propto p x y  p y   for example p x y  could be gaussian,how can i work on my funtion to make it work and transfer  dimensinal to n dimensional ,need create simulated data set x example info model understand problem better idea given professor need help understanding normally however one specifies priors p class conditionals p x computes samples p x p p x probabilities obtained p x propto p x p example p x could gaussian,work funtion make work transfer dimensinal n dimensional,work funtion make work transfer dimensinal n dimensionalneed create simulated data set x example info model understand problem better idea given professor need help understanding normally however one specifies priors p class conditionals p x computes samples p x p p x probabilities obtained p x propto p x p example p x could gaussian,"['work', 'funtion', 'make', 'work', 'transfer', 'dimensinal', 'n', 'dimensionalneed', 'create', 'simulated', 'data', 'set', 'x', 'example', 'info', 'model', 'understand', 'problem', 'better', 'idea', 'given', 'professor', 'need', 'help', 'understanding', 'normally', 'however', 'one', 'specifies', 'priors', 'p', 'class', 'conditionals', 'p', 'x', 'computes', 'samples', 'p', 'x', 'p', 'p', 'x', 'probabilities', 'obtained', 'p', 'x', 'propto', 'p', 'x', 'p', 'example', 'p', 'x', 'could', 'gaussian']","['work', 'funtion', 'make', 'work', 'transfer', 'dimensin', 'n', 'dimensionalne', 'creat', 'simul', 'data', 'set', 'x', 'exampl', 'info', 'model', 'understand', 'problem', 'better', 'idea', 'given', 'professor', 'need', 'help', 'understand', 'normal', 'howev', 'one', 'specifi', 'prior', 'p', 'class', 'condit', 'p', 'x', 'comput', 'sampl', 'p', 'x', 'p', 'p', 'x', 'probabl', 'obtain', 'p', 'x', 'propto', 'p', 'x', 'p', 'exampl', 'p', 'x', 'could', 'gaussian']"
178,192,192,9165423,73017157,Most common transformations in an sklearn pipeline?,"<p>I'm wondering if there is a list anywhere of the most commonly used transformations that people use in a machine learning pipeline - particularly in Python and scikit-learn.</p>
<p>Additionally, are there any resources online that are dedicated to what transformations in the pipeline typically go together? For instance, I've seen in a few areas that standardization often comes before PCA, and it would definitely be useful to me to see what pairs or triplets of transformations might commonly be used together.</p>
",16,0,-1,4,python;machine-learning;scikit-learn;pipeline,2022-07-18 06:56:31,2022-07-18 06:56:31,2022-07-18 06:56:31,i m wondering if there is a list anywhere of the most commonly used transformations that people use in a machine learning pipeline   particularly in python and scikit learn  additionally  are there any resources online that are dedicated to what transformations in the pipeline typically go together  for instance  i ve seen in a few areas that standardization often comes before pca  and it would definitely be useful to me to see what pairs or triplets of transformations might commonly be used together ,most common transformations in an sklearn pipeline ,wondering anywhere commonly used transformations people use machine learning pipeline particularly python scikit learn additionally resources online dedicated transformations pipeline typically go together instance seen areas standardization often comes pca would definitely useful see pairs triplets transformations might commonly used together,common transformations sklearn pipeline,common transformations sklearn pipelinewondering anywhere commonly used transformations people use machine learning pipeline particularly python scikit learn additionally resources online dedicated transformations pipeline typically go together instance seen areas standardization often comes pca would definitely useful see pairs triplets transformations might commonly used together,"['common', 'transformations', 'sklearn', 'pipelinewondering', 'anywhere', 'commonly', 'used', 'transformations', 'people', 'use', 'machine', 'learning', 'pipeline', 'particularly', 'python', 'scikit', 'learn', 'additionally', 'resources', 'online', 'dedicated', 'transformations', 'pipeline', 'typically', 'go', 'together', 'instance', 'seen', 'areas', 'standardization', 'often', 'comes', 'pca', 'would', 'definitely', 'useful', 'see', 'pairs', 'triplets', 'transformations', 'might', 'commonly', 'used', 'together']","['common', 'transform', 'sklearn', 'pipelinewond', 'anywher', 'commonli', 'use', 'transform', 'peopl', 'use', 'machin', 'learn', 'pipelin', 'particularli', 'python', 'scikit', 'learn', 'addit', 'resourc', 'onlin', 'dedic', 'transform', 'pipelin', 'typic', 'go', 'togeth', 'instanc', 'seen', 'area', 'standard', 'often', 'come', 'pca', 'would', 'definit', 'use', 'see', 'pair', 'triplet', 'transform', 'might', 'commonli', 'use', 'togeth']"
179,193,193,10444642,52870022,Inverse_transform method (LabelEncoder),"<p>You can find below the code I found on the internet to build a simple neural network. Everything works fine. I encoded the y labels and these are the predictions I get:</p>
<p>2 0 1 2 1 2 2 0 2 1 0 0 0 1 1 1 1 1 1 1 2 1 2 1 0 1 0 1 0 2</p>
<p>So now I need to convert it back to the original Iris class (Iris-Virginica, Setosa, Versicolor). I need to use the <code>inverse_transform</code> method. Can you help out?</p>
<pre class=""lang-py prettyprint-override""><code>    import pandas as pd
    from sklearn import preprocessing
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import classification_report, confusion_matrix 
    
    
    # Location of dataset
    url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;
    
    # Assign colum names to the dataset
    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
    
    # Read dataset to pandas dataframe
    irisdata = pd.read_csv(url, names=names)  
    
    irisdata.head()
    #head_tableau=irisdata.head()
    #print(head_tableau)
    
    # Assign data from first four columns to X variable
    X = irisdata.iloc[:, 0:4]
    
    # Assign data from first fifth columns to y variable
    y = irisdata.select_dtypes(include=[object])  
    
    y.head()
    #afficher_y=y.head()
    #print(afficher_y)
    
    y.Class.unique()
    #affiche=y.Class.unique()
    #print(affiche)
    
    le = preprocessing.LabelEncoder()
    
    y = y.apply(le.fit_transform)  
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
    
    mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)  
    mlp.fit(X_train, y_train.values.ravel())
    
    predictions = mlp.predict(X_test)
    print(predictions)

</code></pre>
",8870,1,3,3,python;scikit-learn;neural-network,2018-10-18 11:33:40,2018-10-18 11:33:40,2022-07-18 01:13:10,you can find below the code i found on the internet to build a simple neural network  everything works fine  i encoded the y labels and these are the predictions i get                                so now i need to convert it back to the original iris class  iris virginica  setosa  versicolor   i need to use the inverse_transform method  can you help out ,inverse_transform method  labelencoder ,find code found internet build simple neural network everything works fine encoded labels predictions get need convert back original iris class iris virginica setosa versicolor need use inverse_transform method help,inverse_transform method labelencoder,inverse_transform method labelencoderfind code found internet build simple neural network everything works fine encoded labels predictions get need convert back original iris class iris virginica setosa versicolor need use inverse_transform method help,"['inverse_transform', 'method', 'labelencoderfind', 'code', 'found', 'internet', 'build', 'simple', 'neural', 'network', 'everything', 'works', 'fine', 'encoded', 'labels', 'predictions', 'get', 'need', 'convert', 'back', 'original', 'iris', 'class', 'iris', 'virginica', 'setosa', 'versicolor', 'need', 'use', 'inverse_transform', 'method', 'help']","['inverse_transform', 'method', 'labelencoderfind', 'code', 'found', 'internet', 'build', 'simpl', 'neural', 'network', 'everyth', 'work', 'fine', 'encod', 'label', 'predict', 'get', 'need', 'convert', 'back', 'origin', 'iri', 'class', 'iri', 'virginica', 'setosa', 'versicolor', 'need', 'use', 'inverse_transform', 'method', 'help']"
180,194,194,19567904,73014928,How to do transfer learning from word2vec to time series prediction?,"<p>(cross-posted from datascience stackexchange, given lack of views there)</p>
<p>The problem goes as follows. There are economic data consisting of output level for each industry at time t. But the data are multiple-period data, so this is time series data as well.</p>
<p>I want to utilize machine learning technology to study the data. My initial thought is to first focus on relations between industries. So my guess is that something like word2vec would work - guess one industry's output level using output level of other industries. After learning the relations between industries, I want to transfer this learning to time series predictions, utilizing something of a transformer.</p>
<p>Conceptually, this sounds easy to do. Yet actual implementations turn out to be quite difficult. The main blocking issue is how one can transfer learning of a word2vec model to fit into a time series prediction. People speak of transfer learning as straightforward, yet I am lost at how I should go implementing. Can anyone provide some help on how to proceed?</p>
<p>Either TensorFlow (Keras) or PyTorch implementation is fine.</p>
",19,0,-1,4,machine-learning;neural-network;word2vec;transfer-learning,2022-07-17 22:54:05,2022-07-17 22:54:05,2022-07-17 22:54:05, cross posted from datascience stackexchange  given lack of views there  the problem goes as follows  there are economic data consisting of output level for each industry at time t  but the data are multiple period data  so this is time series data as well  i want to utilize machine learning technology to study the data  my initial thought is to first focus on relations between industries  so my guess is that something like wordvec would work   guess one industry s output level using output level of other industries  after learning the relations between industries  i want to transfer this learning to time series predictions  utilizing something of a transformer  conceptually  this sounds easy to do  yet actual implementations turn out to be quite difficult  the main blocking issue is how one can transfer learning of a wordvec model to fit into a time series prediction  people speak of transfer learning as straightforward  yet i am lost at how i should go implementing  can anyone provide some help on how to proceed  either tensorflow  keras  or pytorch implementation is fine ,how to do transfer learning from wordvec to time series prediction ,cross posted datascience stackexchange given lack views problem goes follows economic data consisting output level industry time data multiple period data time series data well want utilize machine learning technology study data initial thought first focus relations industries guess something like wordvec would work guess one industry output level using output level industries learning relations industries want transfer learning time series predictions utilizing something transformer conceptually sounds easy yet actual implementations turn quite difficult main blocking issue one transfer learning wordvec model fit time series prediction people speak transfer learning straightforward yet lost go implementing anyone provide help proceed either tensorflow keras pytorch implementation fine,transfer learning wordvec time series prediction,transfer learning wordvec time series predictioncross posted datascience stackexchange given lack views problem goes follows economic data consisting output level industry time data multiple period data time series data well want utilize machine learning technology study data initial thought first focus relations industries guess something like wordvec would work guess one industry output level using output level industries learning relations industries want transfer learning time series predictions utilizing something transformer conceptually sounds easy yet actual implementations turn quite difficult main blocking issue one transfer learning wordvec model fit time series prediction people speak transfer learning straightforward yet lost go implementing anyone provide help proceed either tensorflow keras pytorch implementation fine,"['transfer', 'learning', 'wordvec', 'time', 'series', 'predictioncross', 'posted', 'datascience', 'stackexchange', 'given', 'lack', 'views', 'problem', 'goes', 'follows', 'economic', 'data', 'consisting', 'output', 'level', 'industry', 'time', 'data', 'multiple', 'period', 'data', 'time', 'series', 'data', 'well', 'want', 'utilize', 'machine', 'learning', 'technology', 'study', 'data', 'initial', 'thought', 'first', 'focus', 'relations', 'industries', 'guess', 'something', 'like', 'wordvec', 'would', 'work', 'guess', 'one', 'industry', 'output', 'level', 'using', 'output', 'level', 'industries', 'learning', 'relations', 'industries', 'want', 'transfer', 'learning', 'time', 'series', 'predictions', 'utilizing', 'something', 'transformer', 'conceptually', 'sounds', 'easy', 'yet', 'actual', 'implementations', 'turn', 'quite', 'difficult', 'main', 'blocking', 'issue', 'one', 'transfer', 'learning', 'wordvec', 'model', 'fit', 'time', 'series', 'prediction', 'people', 'speak', 'transfer', 'learning', 'straightforward', 'yet', 'lost', 'go', 'implementing', 'anyone', 'provide', 'help', 'proceed', 'either', 'tensorflow', 'keras', 'pytorch', 'implementation', 'fine']","['transfer', 'learn', 'wordvec', 'time', 'seri', 'predictioncross', 'post', 'datasci', 'stackexchang', 'given', 'lack', 'view', 'problem', 'goe', 'follow', 'econom', 'data', 'consist', 'output', 'level', 'industri', 'time', 'data', 'multipl', 'period', 'data', 'time', 'seri', 'data', 'well', 'want', 'util', 'machin', 'learn', 'technolog', 'studi', 'data', 'initi', 'thought', 'first', 'focu', 'relat', 'industri', 'guess', 'someth', 'like', 'wordvec', 'would', 'work', 'guess', 'one', 'industri', 'output', 'level', 'use', 'output', 'level', 'industri', 'learn', 'relat', 'industri', 'want', 'transfer', 'learn', 'time', 'seri', 'predict', 'util', 'someth', 'transform', 'conceptu', 'sound', 'easi', 'yet', 'actual', 'implement', 'turn', 'quit', 'difficult', 'main', 'block', 'issu', 'one', 'transfer', 'learn', 'wordvec', 'model', 'fit', 'time', 'seri', 'predict', 'peopl', 'speak', 'transfer', 'learn', 'straightforward', 'yet', 'lost', 'go', 'implement', 'anyon', 'provid', 'help', 'proceed', 'either', 'tensorflow', 'kera', 'pytorch', 'implement', 'fine']"
181,195,195,11644347,73006936,Is it more beneficial to read many small files or fewer large files of the exact same data?,"<p>I am working on a project where I am combining 300,000 small files together to form a dataset to be used for training a machine learning model. Because each of these files do not represent a single sample, but rather a variable number of samples, the dataset I require can only be formed by iterating through each of these files and concatenating/appending them to a single, unified array. With this being said, I unfortunately cannot avoid having to iterate through such files in order to form the dataset I require. As such, the process of data loading prior to model training is very slow.</p>
<p>Therefore my question is this: would it be better to merge these small files together into relatively larger files, e.g., reducing the 300,000 files to 300 (merged) files? I assume that iterating through less (but larger) files would be faster than iterating through many (but smaller) files. Can someone confirm if this is actually the case?</p>
<p>For context, my programs are written in Python and I am using PyTorch as the ML framework.</p>
<p>Thanks!</p>
",45,1,0,5,python;machine-learning;pytorch;iteration;dataloader,2022-07-16 22:02:03,2022-07-16 22:02:03,2022-07-17 22:52:06,i am working on a project where i am combining   small files together to form a dataset to be used for training a machine learning model  because each of these files do not represent a single sample  but rather a variable number of samples  the dataset i require can only be formed by iterating through each of these files and concatenating appending them to a single  unified array  with this being said  i unfortunately cannot avoid having to iterate through such files in order to form the dataset i require  as such  the process of data loading prior to model training is very slow  therefore my question is this  would it be better to merge these small files together into relatively larger files  e g   reducing the   files to   merged  files  i assume that iterating through less  but larger  files would be faster than iterating through many  but smaller  files  can someone confirm if this is actually the case  for context  my programs are written in python and i am using pytorch as the ml framework  thanks ,is it more beneficial to read many small files or fewer large files of the exact same data ,working project combining small files together form dataset used training machine learning model files represent single sample rather variable number samples dataset require formed iterating files concatenating appending single unified array said unfortunately cannot avoid iterate files order form dataset require process data loading prior model training slow therefore question would better merge small files together relatively larger files e g reducing files merged files assume iterating less larger files would faster iterating many smaller files someone confirm actually case context programs written python using pytorch ml framework thanks,beneficial read many small files fewer large files exact data,beneficial read many small files fewer large files exact dataworking project combining small files together form dataset used training machine learning model files represent single sample rather variable number samples dataset require formed iterating files concatenating appending single unified array said unfortunately cannot avoid iterate files order form dataset require process data loading prior model training slow therefore question would better merge small files together relatively larger files e g reducing files merged files assume iterating less larger files would faster iterating many smaller files someone confirm actually case context programs written python using pytorch ml framework thanks,"['beneficial', 'read', 'many', 'small', 'files', 'fewer', 'large', 'files', 'exact', 'dataworking', 'project', 'combining', 'small', 'files', 'together', 'form', 'dataset', 'used', 'training', 'machine', 'learning', 'model', 'files', 'represent', 'single', 'sample', 'rather', 'variable', 'number', 'samples', 'dataset', 'require', 'formed', 'iterating', 'files', 'concatenating', 'appending', 'single', 'unified', 'array', 'said', 'unfortunately', 'can', 'not', 'avoid', 'iterate', 'files', 'order', 'form', 'dataset', 'require', 'process', 'data', 'loading', 'prior', 'model', 'training', 'slow', 'therefore', 'question', 'would', 'better', 'merge', 'small', 'files', 'together', 'relatively', 'larger', 'files', 'e', 'g', 'reducing', 'files', 'merged', 'files', 'assume', 'iterating', 'less', 'larger', 'files', 'would', 'faster', 'iterating', 'many', 'smaller', 'files', 'someone', 'confirm', 'actually', 'case', 'context', 'programs', 'written', 'python', 'using', 'pytorch', 'ml', 'framework', 'thanks']","['benefici', 'read', 'mani', 'small', 'file', 'fewer', 'larg', 'file', 'exact', 'datawork', 'project', 'combin', 'small', 'file', 'togeth', 'form', 'dataset', 'use', 'train', 'machin', 'learn', 'model', 'file', 'repres', 'singl', 'sampl', 'rather', 'variabl', 'number', 'sampl', 'dataset', 'requir', 'form', 'iter', 'file', 'concaten', 'append', 'singl', 'unifi', 'array', 'said', 'unfortun', 'can', 'not', 'avoid', 'iter', 'file', 'order', 'form', 'dataset', 'requir', 'process', 'data', 'load', 'prior', 'model', 'train', 'slow', 'therefor', 'question', 'would', 'better', 'merg', 'small', 'file', 'togeth', 'rel', 'larger', 'file', 'e', 'g', 'reduc', 'file', 'merg', 'file', 'assum', 'iter', 'less', 'larger', 'file', 'would', 'faster', 'iter', 'mani', 'smaller', 'file', 'someon', 'confirm', 'actual', 'case', 'context', 'program', 'written', 'python', 'use', 'pytorch', 'ml', 'framework', 'thank']"
182,196,196,5428238,43067511,Anomaly Detection - Correlated Variables,"<p>I am working on an 'Anomaly' detection assignment in R. My dataset has around 30,000 records of which around 200 are anomalous. It has around 30 columns &amp; all are quantitative. Some of the variables are highly correlated (~0.9). By anomaly I mean some of the records have unusual (high/low) values for some column(s) while some have correlated variables not behaving as expected. The below example will give some idea.</p>

<p><em>Suppose vehicle speed &amp; heart rate are highly positively correlated. Usually vehicle speed varies between 40 &amp; 60 while heart rate between 55-70.</em></p>

<pre><code>time_s  steering    vehicle.speed running.distance heart_rate
0       -0.011734953    40         0.251867414      58
0.01    -0.011734953    50         0.251936555      61
0.02    -0.011734953    60         0.252005577      62
0.03    -0.011734953    60         0.252074778      90
0.04    -0.011734953    40         0.252074778      65
</code></pre>

<p>Here we have two types of anomalies. 4th record has exceptionally high value for heart_rate while 5th record seems okay if we look individual columns. But as we can see that heart_rate increases with speed, we expected a lower heart rate for 5th record while we have a higher value.</p>

<p>I could identify the column level anomalies using box plots etc but find it hard to identify the second type. Somewhere I read about PCA based anomaly detection but I couldn't find it's implementation in R. </p>

<p>Will you please help me with PCA based anomaly detection in R for this scenario. My google search was throwing mainly time series related stuff which is not something I am looking for.</p>

<p>Note: There is a similar implementation in Microsoft Azure Machine Learning - 'PCA Based Anomaly Detection for Credit Risk' which does the job but I wan't to know the logic behind it &amp; replicate the same in R.</p>
",441,0,0,3,r;pca;anomaly-detection,2017-03-28 13:46:36,2017-03-28 13:46:36,2022-07-17 21:52:01,i am working on an  anomaly  detection assignment in r  my dataset has around   records of which around  are anomalous  it has around  columns  amp  all are quantitative  some of the variables are highly correlated       by anomaly i mean some of the records have unusual  high low  values for some column s  while some have correlated variables not behaving as expected  the below example will give some idea  suppose vehicle speed  amp  heart rate are highly positively correlated  usually vehicle speed varies between   amp   while heart rate between    here we have two types of anomalies  th record has exceptionally high value for heart_rate while th record seems okay if we look individual columns  but as we can see that heart_rate increases with speed  we expected a lower heart rate for th record while we have a higher value  i could identify the column level anomalies using box plots etc but find it hard to identify the second type  somewhere i read about pca based anomaly detection but i couldn t find it s implementation in r   will you please help me with pca based anomaly detection in r for this scenario  my google search was throwing mainly time series related stuff which is not something i am looking for  note  there is a similar implementation in microsoft azure machine learning    pca based anomaly detection for credit risk  which does the job but i wan t to know the logic behind it  amp  replicate the same in r ,anomaly detection   correlated variables,working anomaly detection assignment r dataset around records around anomalous around columns amp quantitative variables highly correlated anomaly mean records unusual high low values column correlated variables behaving expected example give idea suppose vehicle speed amp heart rate highly positively correlated usually vehicle speed varies amp heart rate two types anomalies th record exceptionally high value heart_rate th record seems okay look individual columns see heart_rate increases speed expected lower heart rate th record higher value could identify column level anomalies using box plots etc find hard identify second type somewhere read pca based anomaly detection find implementation r please help pca based anomaly detection r scenario google search throwing mainly time series related stuff something looking note similar implementation microsoft azure machine learning pca based anomaly detection credit risk job wan know logic behind amp replicate r,anomaly detection correlated variables,anomaly detection correlated variablesworking anomaly detection assignment r dataset around records around anomalous around columns amp quantitative variables highly correlated anomaly mean records unusual high low values column correlated variables behaving expected example give idea suppose vehicle speed amp heart rate highly positively correlated usually vehicle speed varies amp heart rate two types anomalies th record exceptionally high value heart_rate th record seems okay look individual columns see heart_rate increases speed expected lower heart rate th record higher value could identify column level anomalies using box plots etc find hard identify second type somewhere read pca based anomaly detection find implementation r please help pca based anomaly detection r scenario google search throwing mainly time series related stuff something looking note similar implementation microsoft azure machine learning pca based anomaly detection credit risk job wan know logic behind amp replicate r,"['anomaly', 'detection', 'correlated', 'variablesworking', 'anomaly', 'detection', 'assignment', 'r', 'dataset', 'around', 'records', 'around', 'anomalous', 'around', 'columns', 'amp', 'quantitative', 'variables', 'highly', 'correlated', 'anomaly', 'mean', 'records', 'unusual', 'high', 'low', 'values', 'column', 'correlated', 'variables', 'behaving', 'expected', 'example', 'give', 'idea', 'suppose', 'vehicle', 'speed', 'amp', 'heart', 'rate', 'highly', 'positively', 'correlated', 'usually', 'vehicle', 'speed', 'varies', 'amp', 'heart', 'rate', 'two', 'types', 'anomalies', 'th', 'record', 'exceptionally', 'high', 'value', 'heart_rate', 'th', 'record', 'seems', 'okay', 'look', 'individual', 'columns', 'see', 'heart_rate', 'increases', 'speed', 'expected', 'lower', 'heart', 'rate', 'th', 'record', 'higher', 'value', 'could', 'identify', 'column', 'level', 'anomalies', 'using', 'box', 'plots', 'etc', 'find', 'hard', 'identify', 'second', 'type', 'somewhere', 'read', 'pca', 'based', 'anomaly', 'detection', 'find', 'implementation', 'r', 'please', 'help', 'pca', 'based', 'anomaly', 'detection', 'r', 'scenario', 'google', 'search', 'throwing', 'mainly', 'time', 'series', 'related', 'stuff', 'something', 'looking', 'note', 'similar', 'implementation', 'microsoft', 'azure', 'machine', 'learning', 'pca', 'based', 'anomaly', 'detection', 'credit', 'risk', 'job', 'wan', 'know', 'logic', 'behind', 'amp', 'replicate', 'r']","['anomali', 'detect', 'correl', 'variableswork', 'anomali', 'detect', 'assign', 'r', 'dataset', 'around', 'record', 'around', 'anomal', 'around', 'column', 'amp', 'quantit', 'variabl', 'highli', 'correl', 'anomali', 'mean', 'record', 'unusu', 'high', 'low', 'valu', 'column', 'correl', 'variabl', 'behav', 'expect', 'exampl', 'give', 'idea', 'suppos', 'vehicl', 'speed', 'amp', 'heart', 'rate', 'highli', 'posit', 'correl', 'usual', 'vehicl', 'speed', 'vari', 'amp', 'heart', 'rate', 'two', 'type', 'anomali', 'th', 'record', 'except', 'high', 'valu', 'heart_rat', 'th', 'record', 'seem', 'okay', 'look', 'individu', 'column', 'see', 'heart_rat', 'increas', 'speed', 'expect', 'lower', 'heart', 'rate', 'th', 'record', 'higher', 'valu', 'could', 'identifi', 'column', 'level', 'anomali', 'use', 'box', 'plot', 'etc', 'find', 'hard', 'identifi', 'second', 'type', 'somewher', 'read', 'pca', 'base', 'anomali', 'detect', 'find', 'implement', 'r', 'pleas', 'help', 'pca', 'base', 'anomali', 'detect', 'r', 'scenario', 'googl', 'search', 'throw', 'mainli', 'time', 'seri', 'relat', 'stuff', 'someth', 'look', 'note', 'similar', 'implement', 'microsoft', 'azur', 'machin', 'learn', 'pca', 'base', 'anomali', 'detect', 'credit', 'risk', 'job', 'wan', 'know', 'logic', 'behind', 'amp', 'replic', 'r']"
183,197,197,19564591,73009289,"When I hot encode a categorical variable using OneHotEncoder, do I need to remove the original column before I train a machine learning model?","<p>I used OneHotEncoder to convert a zipcode before feeding into a Random Forest Model:</p>
<pre><code>from sklearn.preprocessing import OneHotEncoder
one_hot = OneHotEncoder()
encoded = one_hot.fit_transform(df[['zipcode']])
df[one_hot.categories_[0]] = encoded.toarray()
</code></pre>
<p>Should I drop the orignal &quot;zipcode&quot; column from the independent variables? Or does sklearn account for that?</p>
<p>I ask mainly because &quot;zipcode&quot; is showing up as the second most important feature. Is that an aggregate of the importance of the all the hot encoded features?</p>
",23,1,0,4,python;scikit-learn;random-forest;one-hot-encoding,2022-07-17 07:39:08,2022-07-17 07:39:08,2022-07-17 13:27:50,i used onehotencoder to convert a zipcode before feeding into a random forest model  should i drop the orignal  zipcode  column from the independent variables  or does sklearn account for that  i ask mainly because  zipcode  is showing up as the second most important feature  is that an aggregate of the importance of the all the hot encoded features ,when i hot encode a categorical variable using onehotencoder  do i need to remove the original column before i train a machine learning model ,used onehotencoder convert zipcode feeding random forest model drop orignal zipcode column independent variables sklearn account ask mainly zipcode showing second important feature aggregate importance hot encoded features,hot encode categorical variable using onehotencoder need remove original column train machine learning model,hot encode categorical variable using onehotencoder need remove original column train machine learning modelused onehotencoder convert zipcode feeding random forest model drop orignal zipcode column independent variables sklearn account ask mainly zipcode showing second important feature aggregate importance hot encoded features,"['hot', 'encode', 'categorical', 'variable', 'using', 'onehotencoder', 'need', 'remove', 'original', 'column', 'train', 'machine', 'learning', 'modelused', 'onehotencoder', 'convert', 'zipcode', 'feeding', 'random', 'forest', 'model', 'drop', 'orignal', 'zipcode', 'column', 'independent', 'variables', 'sklearn', 'account', 'ask', 'mainly', 'zipcode', 'showing', 'second', 'important', 'feature', 'aggregate', 'importance', 'hot', 'encoded', 'features']","['hot', 'encod', 'categor', 'variabl', 'use', 'onehotencod', 'need', 'remov', 'origin', 'column', 'train', 'machin', 'learn', 'modelus', 'onehotencod', 'convert', 'zipcod', 'feed', 'random', 'forest', 'model', 'drop', 'orign', 'zipcod', 'column', 'independ', 'variabl', 'sklearn', 'account', 'ask', 'mainli', 'zipcod', 'show', 'second', 'import', 'featur', 'aggreg', 'import', 'hot', 'encod', 'featur']"
184,198,198,0,73010563,"If I compile a C/C++ program on a linux machine, does it automatically have rwx perms","<p>I am currently learning penetration testing as part of a cybersecurity career path. I was working on a vulnhub machine that required me writing some malware to exploit a buffer overflow bug. I decided to write it in C for the sake of practicing OpSec. I used code::blocks on my machine to write the code and used tcp to transfer the compiled app to the machine. Once I tried to run it, I found that it did not have execute permissions, and the compromised user account did not have permission to use chmod, however it did have permission to use GCC. I was wondering if I uploaded the code directly to the machine and compiled it natively, would the app have execute permissions?</p>
",41,1,0,4,c;linux;security;penetration-testing,2022-07-17 12:12:04,2022-07-17 12:12:04,2022-07-17 12:43:39,i am currently learning penetration testing as part of a cybersecurity career path  i was working on a vulnhub machine that required me writing some malware to exploit a buffer overflow bug  i decided to write it in c for the sake of practicing opsec  i used code  blocks on my machine to write the code and used tcp to transfer the compiled app to the machine  once i tried to run it  i found that it did not have execute permissions  and the compromised user account did not have permission to use chmod  however it did have permission to use gcc  i was wondering if i uploaded the code directly to the machine and compiled it natively  would the app have execute permissions ,if i compile a c c   program on a linux machine  does it automatically have rwx perms,currently learning penetration testing part cybersecurity career path working vulnhub machine required writing malware exploit buffer overflow bug decided write c sake practicing opsec used code blocks machine write code used tcp transfer compiled app machine tried run found execute permissions compromised user account permission use chmod however permission use gcc wondering uploaded code directly machine compiled natively would app execute permissions,compile c c program linux machine automatically rwx perms,compile c c program linux machine automatically rwx permscurrently learning penetration testing part cybersecurity career path working vulnhub machine required writing malware exploit buffer overflow bug decided write c sake practicing opsec used code blocks machine write code used tcp transfer compiled app machine tried run found execute permissions compromised user account permission use chmod however permission use gcc wondering uploaded code directly machine compiled natively would app execute permissions,"['compile', 'c', 'c', 'program', 'linux', 'machine', 'automatically', 'rwx', 'permscurrently', 'learning', 'penetration', 'testing', 'part', 'cybersecurity', 'career', 'path', 'working', 'vulnhub', 'machine', 'required', 'writing', 'malware', 'exploit', 'buffer', 'overflow', 'bug', 'decided', 'write', 'c', 'sake', 'practicing', 'opsec', 'used', 'code', 'blocks', 'machine', 'write', 'code', 'used', 'tcp', 'transfer', 'compiled', 'app', 'machine', 'tried', 'run', 'found', 'execute', 'permissions', 'compromised', 'user', 'account', 'permission', 'use', 'chmod', 'however', 'permission', 'use', 'gcc', 'wondering', 'uploaded', 'code', 'directly', 'machine', 'compiled', 'natively', 'would', 'app', 'execute', 'permissions']","['compil', 'c', 'c', 'program', 'linux', 'machin', 'automat', 'rwx', 'permscurr', 'learn', 'penetr', 'test', 'part', 'cybersecur', 'career', 'path', 'work', 'vulnhub', 'machin', 'requir', 'write', 'malwar', 'exploit', 'buffer', 'overflow', 'bug', 'decid', 'write', 'c', 'sake', 'practic', 'opsec', 'use', 'code', 'block', 'machin', 'write', 'code', 'use', 'tcp', 'transfer', 'compil', 'app', 'machin', 'tri', 'run', 'found', 'execut', 'permiss', 'compromis', 'user', 'account', 'permiss', 'use', 'chmod', 'howev', 'permiss', 'use', 'gcc', 'wonder', 'upload', 'code', 'directli', 'machin', 'compil', 'nativ', 'would', 'app', 'execut', 'permiss']"
185,199,199,3441597,36013063,What is the purpose of meshgrid in Python / NumPy?,"<p>Can someone explain to me what is the purpose of <code>meshgrid</code> function in Numpy? I know it creates some kind of grid of coordinates for plotting, but I can't really see the direct benefit of it.</p>

<p>I am studying ""Python Machine Learning"" from Sebastian Raschka, and he is using it for plotting the decision borders. See input 11 <a href=""https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb"" rel=""noreferrer"">here</a>.</p>

<p>I have also tried this code from official documentation, but, again, the output doesn't really make sense to me.</p>

<pre><code>x = np.arange(-5, 5, 1)
y = np.arange(-5, 5, 1)
xx, yy = np.meshgrid(x, y, sparse=True)
z = np.sin(xx**2 + yy**2) / (xx**2 + yy**2)
h = plt.contourf(x,y,z)
</code></pre>

<p>Please, if possible, also show me a lot of real-world examples.</p>
",292614,9,459,5,python;numpy;multidimensional-array;mesh;numpy-ndarray,2016-03-15 15:43:43,2016-03-15 15:43:43,2022-07-17 11:54:41,can someone explain to me what is the purpose of meshgrid function in numpy  i know it creates some kind of grid of coordinates for plotting  but i can t really see the direct benefit of it  i am studying python machine learning from sebastian raschka  and he is using it for plotting the decision borders  see input    i have also tried this code from official documentation  but  again  the output doesn t really make sense to me  please  if possible  also show me a lot of real world examples ,what is the purpose of meshgrid in python   numpy ,someone explain purpose meshgrid function numpy know creates kind grid coordinates plotting really see direct benefit studying python machine learning sebastian raschka using plotting decision borders see input also tried code official documentation output really make sense please possible also show lot real world examples,purpose meshgrid python numpy,purpose meshgrid python numpysomeone explain purpose meshgrid function numpy know creates kind grid coordinates plotting really see direct benefit studying python machine learning sebastian raschka using plotting decision borders see input also tried code official documentation output really make sense please possible also show lot real world examples,"['purpose', 'meshgrid', 'python', 'numpysomeone', 'explain', 'purpose', 'meshgrid', 'function', 'numpy', 'know', 'creates', 'kind', 'grid', 'coordinates', 'plotting', 'really', 'see', 'direct', 'benefit', 'studying', 'python', 'machine', 'learning', 'sebastian', 'raschka', 'using', 'plotting', 'decision', 'borders', 'see', 'input', 'also', 'tried', 'code', 'official', 'documentation', 'output', 'really', 'make', 'sense', 'please', 'possible', 'also', 'show', 'lot', 'real', 'world', 'examples']","['purpos', 'meshgrid', 'python', 'numpysomeon', 'explain', 'purpos', 'meshgrid', 'function', 'numpi', 'know', 'creat', 'kind', 'grid', 'coordin', 'plot', 'realli', 'see', 'direct', 'benefit', 'studi', 'python', 'machin', 'learn', 'sebastian', 'raschka', 'use', 'plot', 'decis', 'border', 'see', 'input', 'also', 'tri', 'code', 'offici', 'document', 'output', 'realli', 'make', 'sens', 'pleas', 'possibl', 'also', 'show', 'lot', 'real', 'world', 'exampl']"
186,200,200,12697245,73010316,Can I get this Python code snippet translated for usage with Axios+Vue?,"<p>The code snippet is simply about performing a post request to a machine learning model endpoint and logging out the response if successful.</p>
<pre><code>import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

# Request data goes here
# The example below assumes JSON formatting which may be updated
# depending on the format your endpoint expects.
# More information can be found here:
# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script
data =  {
  &quot;Inputs&quot;: {
    &quot;data&quot;: [
      {
        &quot;test_date&quot;: &quot;2000-01-01T00:00:00.000Z&quot;,
        &quot;cough&quot;: false,
        &quot;fever&quot;: 0,
        &quot;sore_throat&quot;: false,
        &quot;shortness_of_breath&quot;: 0,
        &quot;head_ache&quot;: 0,
        &quot;age_60_and_above&quot;: &quot;example_value&quot;,
        &quot;gender&quot;: &quot;example_value&quot;
      }
    ]
  },
  &quot;GlobalParameters&quot;: {
    &quot;method&quot;: &quot;predict&quot;
  }
}

body = str.encode(json.dumps(data))

url = 'http://8daf0a82-3a30-4581-96c3-5d4374473502.southafricanorth.azurecontainer.io/score'
api_key = '' # Replace this with the API key for the web service

# The azureml-model-deployment header will force the request to go to a specific deployment.
# Remove this header to have the request observe the endpoint traffic rules
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print(&quot;The request failed with status code: &quot; + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(error.read().decode(&quot;utf8&quot;, 'ignore'))

</code></pre>
<p>I have tried this, but I keep getting</p>
<p>&quot;Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at <a href=""http://8daf0a82-3a30-4581-96c3-5d4374473502.southafricanorth.azurecontainer.io/score"" rel=""nofollow noreferrer"">http://8daf0a82-3a30-4581-96c3-5d4374473502.southafricanorth.azurecontainer.io/score</a>. (Reason: CORS request did not succeed). Status code: (null).&quot;</p>
<pre><code>function predict() {
  axios
    .post(
      &quot;http://8daf0a82-3a30-4581-96c3-5d4374473502.southafricanorth.azurecontainer.io/score&quot;,
      {
        Inputs: {
          data: [
            {
              cough: S1Val.value,
              fever: S2Val.value,
              sore_throat: S3Val.value,
              shortness_of_breath: S4Val.value,
              head_ache: S5Val.value,
              age_60_and_above: a1.value,
              gender: a2.value,
            },
          ],
        },
        GlobalParameters: {
          method: &quot;predict&quot;,
        },
      },
      {
        headers: {
          &quot;Access-Control-Allow-Orgin&quot;: &quot;*&quot;,
          &quot;Content-Type&quot;: &quot;application/json&quot;,
        },
      }
    )
    .then((response) =&gt; {
      console.log(response);
    })
    .catch((error) =&gt; {
      console.log(error);
    });
}
</code></pre>
<p>Also note that this endpoint doesn't require any authentication that's why I didn't append an API key to the Bearer when performing the request.</p>
",15,0,0,5,python;vue.js;axios;python-requests;translate,2022-07-17 11:33:04,2022-07-17 11:33:04,2022-07-17 11:37:31,the code snippet is simply about performing a post request to a machine learning model endpoint and logging out the response if successful  i have tried this  but i keep getting  cross origin request blocked  the same origin policy disallows reading the remote resource at    reason  cors request did not succeed   status code   null    also note that this endpoint doesn t require any authentication that s why i didn t append an api key to the bearer when performing the request ,can i get this python code snippet translated for usage with axios vue ,code snippet simply performing post request machine learning model endpoint logging response successful tried keep getting cross origin request blocked origin policy disallows reading remote resource reason cors request succeed status code null also note endpoint require authentication append api key bearer performing request,get python code snippet translated usage axios vue,get python code snippet translated usage axios vuecode snippet simply performing post request machine learning model endpoint logging response successful tried keep getting cross origin request blocked origin policy disallows reading remote resource reason cors request succeed status code null also note endpoint require authentication append api key bearer performing request,"['get', 'python', 'code', 'snippet', 'translated', 'usage', 'axios', 'vuecode', 'snippet', 'simply', 'performing', 'post', 'request', 'machine', 'learning', 'model', 'endpoint', 'logging', 'response', 'successful', 'tried', 'keep', 'getting', 'cross', 'origin', 'request', 'blocked', 'origin', 'policy', 'disallows', 'reading', 'remote', 'resource', 'reason', 'cors', 'request', 'succeed', 'status', 'code', 'null', 'also', 'note', 'endpoint', 'require', 'authentication', 'append', 'api', 'key', 'bearer', 'performing', 'request']","['get', 'python', 'code', 'snippet', 'translat', 'usag', 'axio', 'vuecod', 'snippet', 'simpli', 'perform', 'post', 'request', 'machin', 'learn', 'model', 'endpoint', 'log', 'respons', 'success', 'tri', 'keep', 'get', 'cross', 'origin', 'request', 'block', 'origin', 'polici', 'disallow', 'read', 'remot', 'resourc', 'reason', 'cor', 'request', 'succeed', 'statu', 'code', 'null', 'also', 'note', 'endpoint', 'requir', 'authent', 'append', 'api', 'key', 'bearer', 'perform', 'request']"
187,201,201,3723090,73009870,"Neural Network Predictions in R - Creating a Column with ,1 in the name","<p>I am learning about Neural Networks on my own for a business analytics class I am taking. We don't have to learn it, but we were asked to discuss other machine learning algorithms we could use R to process and I went down the rabbit hole of trying it out as I think its pretty useful to know long term. I found a few resources that talk through the process but my outputs aren't making sense and it's causing me to pull hair out I don't have.</p>
<p>My Code:</p>
<pre><code>library(neuralnet)

### Prepare the Dataframe for use. These items not needed for the network
nn1 &lt;- df %&gt;% ungroup() %&gt;% 
  select(store, week, high_med_rev, high_med_gp, high_med_gpm)

### Prepare the Dataframe for use. These items are needed for the network
nn2 %&lt;&gt;% ungroup() %&gt;% 
  select(high_med_units, size, region, 
         promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

### The region column needs to be a number so the neural network is able to work properly
nn2$region %&lt;&gt;% as.numeric()

# Check that &quot;positive&quot; is last for `my_confusion_matrix()` to work 
# contrasts(nn2$high_med_units) ## NOT APPLICABLE FOR THIS MODEL

set.seed(77)

## Create a training and testing data partition
partition_nn &lt;- caret::createDataPartition(y=nn2$high_med_units, p=.75, list=FALSE)
data_train_nn &lt;- nn2[partition, ]
data_test_nn &lt;- nn2[-partition, ]

## Create the Neural Network where high_med_units it the dependent outcome, the rest is independent
nn &lt;- neuralnet(high_med_units ~ ., data=data_train_nn, hidden=3, threshold = 0.01,
                linear.output = FALSE)

##Plot the Neural Network
plot(nn)

##predict the network outcomes on the testing data
predict_nn &lt;- compute(nn, data_test_nn)

# Put the prediction back into the test data
data_test_nn$nn &lt;- predict_nn$net.result

# Create a variable that shows if the prediction was correct
data_test_nn %&lt;&gt;% 
  mutate(correct_nn = if_else(nn == high_med_units, 'correct', 'WRONG!'))

# Add back the variables we took out at the beginning to the test data
temp1 &lt;- nn1[-partition, ]
full_nn &lt;- bind_cols(temp1, data_test_nn)

# For viewing in class
full_nn %&gt;% 
  select(store, week, high_med_units, tree, correct_tree, correct_nn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)

</code></pre>
<p>What I find is when I display the output of predict_nn$net.result it gives me a columns of data per row similar to this when I output it to the console:</p>
<p><a href=""https://www.datacamp.com/tutorial/neural-network-models-r"" rel=""nofollow noreferrer"">https://www.datacamp.com/tutorial/neural-network-models-r</a></p>
<p>But when I add it to the data_test_nn dataframe above, the column name ends up being nn[,1] so the rest of the code fails.</p>
<p>The network does generate here:</p>
<p><a href=""https://i.stack.imgur.com/Is4Rn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Is4Rn.png"" alt=""enter image description here"" /></a></p>
<p>Another interesting thing is that all the prediction values are the same:</p>
<p><a href=""https://i.stack.imgur.com/tXk9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tXk9u.png"" alt=""enter image description here"" /></a></p>
<p>I'm probably doing something very wrong since this is all new to me, any pointers/help would be awesome.</p>
",21,0,0,4,r;machine-learning;deep-learning;neural-network,2022-07-17 10:16:39,2022-07-17 10:16:39,2022-07-17 10:16:39,i am learning about neural networks on my own for a business analytics class i am taking  we don t have to learn it  but we were asked to discuss other machine learning algorithms we could use r to process and i went down the rabbit hole of trying it out as i think its pretty useful to know long term  i found a few resources that talk through the process but my outputs aren t making sense and it s causing me to pull hair out i don t have  my code  what i find is when i display the output of predict_nn net result it gives me a columns of data per row similar to this when i output it to the console   but when i add it to the data_test_nn dataframe above  the column name ends up being nn    so the rest of the code fails  the network does generate here   another interesting thing is that all the prediction values are the same   i m probably doing something very wrong since this is all new to me  any pointers help would be awesome ,neural network predictions in r   creating a column with   in the name,learning neural networks business analytics class taking learn asked discuss machine learning algorithms could use r process went rabbit hole trying think pretty useful know long term found resources talk process outputs making sense causing pull hair code find display output predict_nn net result gives columns data per row similar output console data_test_nn dataframe column name ends nn rest code fails network generate another interesting thing prediction values probably something wrong since pointers help would awesome,neural network predictions r creating column name,neural network predictions r creating column namelearning neural networks business analytics class taking learn asked discuss machine learning algorithms could use r process went rabbit hole trying think pretty useful know long term found resources talk process outputs making sense causing pull hair code find display output predict_nn net result gives columns data per row similar output console data_test_nn dataframe column name ends nn rest code fails network generate another interesting thing prediction values probably something wrong since pointers help would awesome,"['neural', 'network', 'predictions', 'r', 'creating', 'column', 'namelearning', 'neural', 'networks', 'business', 'analytics', 'class', 'taking', 'learn', 'asked', 'discuss', 'machine', 'learning', 'algorithms', 'could', 'use', 'r', 'process', 'went', 'rabbit', 'hole', 'trying', 'think', 'pretty', 'useful', 'know', 'long', 'term', 'found', 'resources', 'talk', 'process', 'outputs', 'making', 'sense', 'causing', 'pull', 'hair', 'code', 'find', 'display', 'output', 'predict_nn', 'net', 'result', 'gives', 'columns', 'data', 'per', 'row', 'similar', 'output', 'console', 'data_test_nn', 'dataframe', 'column', 'name', 'ends', 'nn', 'rest', 'code', 'fails', 'network', 'generate', 'another', 'interesting', 'thing', 'prediction', 'values', 'probably', 'something', 'wrong', 'since', 'pointers', 'help', 'would', 'awesome']","['neural', 'network', 'predict', 'r', 'creat', 'column', 'namelearn', 'neural', 'network', 'busi', 'analyt', 'class', 'take', 'learn', 'ask', 'discuss', 'machin', 'learn', 'algorithm', 'could', 'use', 'r', 'process', 'went', 'rabbit', 'hole', 'tri', 'think', 'pretti', 'use', 'know', 'long', 'term', 'found', 'resourc', 'talk', 'process', 'output', 'make', 'sens', 'caus', 'pull', 'hair', 'code', 'find', 'display', 'output', 'predict_nn', 'net', 'result', 'give', 'column', 'data', 'per', 'row', 'similar', 'output', 'consol', 'data_test_nn', 'datafram', 'column', 'name', 'end', 'nn', 'rest', 'code', 'fail', 'network', 'gener', 'anoth', 'interest', 'thing', 'predict', 'valu', 'probabl', 'someth', 'wrong', 'sinc', 'pointer', 'help', 'would', 'awesom']"
188,202,202,16469159,72963263,Python read from csv with condition for TimeSeriesGenerator,"<p>I have a .csv file with many entries that looks like this:</p>
<pre><code>observation1, observation2, tag
observation1, observation2, tag
...
b r e a k
observation1, observation2, tag
...
b r e a k
</code></pre>
<p>whereas the observations are some numbers and the tag the ground truth true/false.</p>
<p>the <code>break</code> part comes with the data and symbolizes the end of a file and the end of an observation chain. Datapoints within two <code>break</code> entries belong together. (All those datapoints are merged from multiple files into one huge csv).</p>
<p>With this data I am supposed to do some machine learning using the tensorflow TimeSeriesGenerator.</p>
<p>I found out however, that TSG uses a fixed time series chain length, which means I have to do some cutting/filtering of my data given.</p>
<p>Condition one, is that if a <code>true</code> appears in the chain, it has to be the last value. Condition two, that all chains consist of the same amount of entries.</p>
<p>This means, if say my chain length would be 3, then the following chains are allowed:</p>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, false
observation1, observation2, true
b r e a k
</code></pre>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, false
observation1, observation2, false
b r e a k
</code></pre>
<p>but not</p>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, true
observation1, observation2, false
b r e a k
</code></pre>
<p>A chain like this would also be allowed</p>
<pre><code>observation1, observation2, false
observation1, observation2, false
observation1, observation2, false
observation1, observation2, true
</code></pre>
<p>as I could simply throw the first line away to get a length of 3.</p>
<p>But not a chain like this:</p>
<pre><code>observation1, observation2, false
b r e a k
observation1, observation2, false
observation1, observation2, true
b r e a k
</code></pre>
<p>This means I need some way (my guess would be pandas) to filter the .csv file and find all occurences, where between to <code>b r e a k</code> lines there are at least x amount of <code>false</code> datapoints followed by a <code>true</code> or another <code>false</code>.</p>
<p>What would be a good way of achieving this filtering?</p>
",48,2,1,4,python;csv;tensorflow;filter,2022-07-13 11:40:11,2022-07-13 11:40:11,2022-07-17 07:35:12,i have a  csv file with many entries that looks like this  whereas the observations are some numbers and the tag the ground truth true false  the break part comes with the data and symbolizes the end of a file and the end of an observation chain  datapoints within two break entries belong together   all those datapoints are merged from multiple files into one huge csv   with this data i am supposed to do some machine learning using the tensorflow timeseriesgenerator  i found out however  that tsg uses a fixed time series chain length  which means i have to do some cutting filtering of my data given  condition one  is that if a true appears in the chain  it has to be the last value  condition two  that all chains consist of the same amount of entries  this means  if say my chain length would be   then the following chains are allowed  but not a chain like this would also be allowed as i could simply throw the first line away to get a length of   but not a chain like this  this means i need some way  my guess would be pandas  to filter the  csv file and find all occurences  where between to b r e a k lines there are at least x amount of false datapoints followed by a true or another false  what would be a good way of achieving this filtering ,python read from csv with condition for timeseriesgenerator,csv file many entries looks like whereas observations numbers tag ground truth true false break part comes data symbolizes end file end observation chain datapoints within two break entries belong together datapoints merged multiple files one huge csv data supposed machine learning using tensorflow timeseriesgenerator found however tsg uses fixed time series chain length means cutting filtering data given condition one true appears chain last value condition two chains consist amount entries means say chain length would following chains allowed chain like would also allowed could simply throw first line away get length chain like means need way guess would pandas filter csv file find occurences b r e k lines least x amount false datapoints followed true another false would good way achieving filtering,python read csv condition timeseriesgenerator,python read csv condition timeseriesgeneratorcsv file many entries looks like whereas observations numbers tag ground truth true false break part comes data symbolizes end file end observation chain datapoints within two break entries belong together datapoints merged multiple files one huge csv data supposed machine learning using tensorflow timeseriesgenerator found however tsg uses fixed time series chain length means cutting filtering data given condition one true appears chain last value condition two chains consist amount entries means say chain length would following chains allowed chain like would also allowed could simply throw first line away get length chain like means need way guess would pandas filter csv file find occurences b r e k lines least x amount false datapoints followed true another false would good way achieving filtering,"['python', 'read', 'csv', 'condition', 'timeseriesgeneratorcsv', 'file', 'many', 'entries', 'looks', 'like', 'whereas', 'observations', 'numbers', 'tag', 'ground', 'truth', 'true', 'false', 'break', 'part', 'comes', 'data', 'symbolizes', 'end', 'file', 'end', 'observation', 'chain', 'datapoints', 'within', 'two', 'break', 'entries', 'belong', 'together', 'datapoints', 'merged', 'multiple', 'files', 'one', 'huge', 'csv', 'data', 'supposed', 'machine', 'learning', 'using', 'tensorflow', 'timeseriesgenerator', 'found', 'however', 'tsg', 'uses', 'fixed', 'time', 'series', 'chain', 'length', 'means', 'cutting', 'filtering', 'data', 'given', 'condition', 'one', 'true', 'appears', 'chain', 'last', 'value', 'condition', 'two', 'chains', 'consist', 'amount', 'entries', 'means', 'say', 'chain', 'length', 'would', 'following', 'chains', 'allowed', 'chain', 'like', 'would', 'also', 'allowed', 'could', 'simply', 'throw', 'first', 'line', 'away', 'get', 'length', 'chain', 'like', 'means', 'need', 'way', 'guess', 'would', 'pandas', 'filter', 'csv', 'file', 'find', 'occurences', 'b', 'r', 'e', 'k', 'lines', 'least', 'x', 'amount', 'false', 'datapoints', 'followed', 'true', 'another', 'false', 'would', 'good', 'way', 'achieving', 'filtering']","['python', 'read', 'csv', 'condit', 'timeseriesgeneratorcsv', 'file', 'mani', 'entri', 'look', 'like', 'wherea', 'observ', 'number', 'tag', 'ground', 'truth', 'true', 'fals', 'break', 'part', 'come', 'data', 'symbol', 'end', 'file', 'end', 'observ', 'chain', 'datapoint', 'within', 'two', 'break', 'entri', 'belong', 'togeth', 'datapoint', 'merg', 'multipl', 'file', 'one', 'huge', 'csv', 'data', 'suppos', 'machin', 'learn', 'use', 'tensorflow', 'timeseriesgener', 'found', 'howev', 'tsg', 'use', 'fix', 'time', 'seri', 'chain', 'length', 'mean', 'cut', 'filter', 'data', 'given', 'condit', 'one', 'true', 'appear', 'chain', 'last', 'valu', 'condit', 'two', 'chain', 'consist', 'amount', 'entri', 'mean', 'say', 'chain', 'length', 'would', 'follow', 'chain', 'allow', 'chain', 'like', 'would', 'also', 'allow', 'could', 'simpli', 'throw', 'first', 'line', 'away', 'get', 'length', 'chain', 'like', 'mean', 'need', 'way', 'guess', 'would', 'panda', 'filter', 'csv', 'file', 'find', 'occur', 'b', 'r', 'e', 'k', 'line', 'least', 'x', 'amount', 'fals', 'datapoint', 'follow', 'true', 'anoth', 'fals', 'would', 'good', 'way', 'achiev', 'filter']"
189,203,203,15510425,67931185,Airflow XCOM communication from BashOperator to SSHOperator,"<p>I just began learning Airflow, but it is quite difficult to grasp the concept of Xcom. Therefore I wrote a dag like this:</p>
<pre class=""lang-py prettyprint-override""><code>from airflow import DAG
from airflow.utils.edgemodifier import Label

from datetime import datetime
from datetime import timedelta

from airflow.operators.bash import BashOperator
from airflow.contrib.operators.ssh_operator import SSHOperator
from airflow.contrib.hooks.ssh_hook import SSHHook

#For more default argument for a task (or creating templates), please check this website
#https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/index.html#airflow.models.BaseOperator

default_args = {
    'owner': '...',
    'email': ['...'],
    'email_on_retry': False,
    'email_on_failure': True,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2021, 6, 10, 23, 0, 0, 0),
    
}

hook = SSHHook(
    remote_host='...',
    username='...',
    password='...## Heading ##',
    port=22,
)

with DAG(
    'test_dag',
    description='This is my first DAG to learn BASH operation, SSH connection, and transfer data among jobs',
    default_args=default_args,
    start_date=datetime(2021, 6, 10, 23, 0, 0, 0),
    schedule_interval=&quot;0 * * * *&quot;,
    tags = ['Testing', 'Tutorial'],
) as dag:
    # Declare Tasks
    Read_my_IP = BashOperator(
        # Task ID has to be the combination of alphanumeric chars, dashes, dots, and underscores 
        task_id='Read_my_IP',
        # The last line will be pushed to next task
        bash_command=&quot;hostname -i | awk '{print $1}'&quot;,
    )

    Read_remote_IP = SSHOperator(
        task_id='Read_remote_IP',
        ssh_hook=hook,
        environment={
            'Pi_IP': Read_my_IP.xcom_pull('Read_my_IP'),
        },
        command=&quot;echo {{Pi_IP}}&quot;,
    )

    # Declare Relationship between tasks
    Read_my_IP &gt;&gt; Label(&quot;PI's IP address&quot;) &gt;&gt; Read_remote_IP
</code></pre>
<p>The first task ran successfully, but I could not obtain the XCom return_value from task <code>Read_my_IP</code>, which is the IP address of the local machine. This might be very basic, but the documentation does not mention how to declare <code>task_instance</code>.</p>
<p>Please help to complete the Xcom flow and pass the IP address from the local machine to the remote machine for further procedure.</p>
",2451,1,4,3,python;airflow;airflow-2.x,2021-06-11 07:11:28,2021-06-11 07:11:28,2022-07-16 23:28:11,i just began learning airflow  but it is quite difficult to grasp the concept of xcom  therefore i wrote a dag like this  the first task ran successfully  but i could not obtain the xcom return_value from task read_my_ip  which is the ip address of the local machine  this might be very basic  but the documentation does not mention how to declare task_instance  please help to complete the xcom flow and pass the ip address from the local machine to the remote machine for further procedure ,airflow xcom communication from bashoperator to sshoperator,began learning airflow quite difficult grasp concept xcom therefore wrote dag like first task ran successfully could obtain xcom return_value task read_my_ip ip address local machine might basic documentation mention declare task_instance please help complete xcom flow pass ip address local machine remote machine procedure,airflow xcom communication bashoperator sshoperator,airflow xcom communication bashoperator sshoperatorbegan learning airflow quite difficult grasp concept xcom therefore wrote dag like first task ran successfully could obtain xcom return_value task read_my_ip ip address local machine might basic documentation mention declare task_instance please help complete xcom flow pass ip address local machine remote machine procedure,"['airflow', 'xcom', 'communication', 'bashoperator', 'sshoperatorbegan', 'learning', 'airflow', 'quite', 'difficult', 'grasp', 'concept', 'xcom', 'therefore', 'wrote', 'dag', 'like', 'first', 'task', 'ran', 'successfully', 'could', 'obtain', 'xcom', 'return_value', 'task', 'read_my_ip', 'ip', 'address', 'local', 'machine', 'might', 'basic', 'documentation', 'mention', 'declare', 'task_instance', 'please', 'help', 'complete', 'xcom', 'flow', 'pass', 'ip', 'address', 'local', 'machine', 'remote', 'machine', 'procedure']","['airflow', 'xcom', 'commun', 'bashoper', 'sshoperatorbegan', 'learn', 'airflow', 'quit', 'difficult', 'grasp', 'concept', 'xcom', 'therefor', 'wrote', 'dag', 'like', 'first', 'task', 'ran', 'success', 'could', 'obtain', 'xcom', 'return_valu', 'task', 'read_my_ip', 'ip', 'address', 'local', 'machin', 'might', 'basic', 'document', 'mention', 'declar', 'task_inst', 'pleas', 'help', 'complet', 'xcom', 'flow', 'pass', 'ip', 'address', 'local', 'machin', 'remot', 'machin', 'procedur']"
190,204,204,11837574,73004617,Calculated column using machine learning predictions,"<p>I would like to have a column that will mention weather an item in the row is a Phone, laptop, watch or a tablet based on a few different column specs, is this possible using machine learning in python?</p>
",13,0,-2,4,python;data-science;tensor;flow,2022-07-16 16:31:22,2022-07-16 16:31:22,2022-07-16 16:31:22,i would like to have a column that will mention weather an item in the row is a phone  laptop  watch or a tablet based on a few different column specs  is this possible using machine learning in python ,calculated column using machine learning predictions,would like column mention weather item row phone laptop watch tablet based different column specs possible using machine learning python,calculated column using machine learning predictions,calculated column using machine learning predictionswould like column mention weather item row phone laptop watch tablet based different column specs possible using machine learning python,"['calculated', 'column', 'using', 'machine', 'learning', 'predictionswould', 'like', 'column', 'mention', 'weather', 'item', 'row', 'phone', 'laptop', 'watch', 'tablet', 'based', 'different', 'column', 'specs', 'possible', 'using', 'machine', 'learning', 'python']","['calcul', 'column', 'use', 'machin', 'learn', 'predictionswould', 'like', 'column', 'mention', 'weather', 'item', 'row', 'phone', 'laptop', 'watch', 'tablet', 'base', 'differ', 'column', 'spec', 'possibl', 'use', 'machin', 'learn', 'python']"
191,205,205,7347020,64947723,vue application best practice/pattern to display data with people-friendly label?,"<p>A high-level vue application question:</p>
<p>A little background:</p>
<ul>
<li>small team, still learning vue</li>
<li>creating a new large reasonably complex vue app from scratch</li>
<li>vue-cli scaffolded (vue 2, veux, router, i118n, bootstrapVue)</li>
<li>there are many different application entities  e.g. &quot;user&quot;, product, etc.</li>
<li>there are many of each item, and each has a data record (from a db) that includes a number of boolean fields such as &quot;active&quot;, enabled,  is_sensitive.</li>
<li>item data displays in many different contexts, including: search results, table lists (e.g. browse), individual landing views for each (an item detail page), as well as ancillary lists (e.g. A related item:</li>
</ul>
<p><strong>The key problem:<br />
In every display situation, the boolean values should to be translated from machine-friendly to people-friendly terminology.</strong></p>
<p>The appropriate human-friendly term depends on the field itself (so there is no single translation for all fields).</p>
<p>Example:
Machine-friendly data:</p>
<pre><code>[
    {
        name: Megaphone,
        is_embarrassing: false,
        active: false,
    },
    {
        name: Wankel Rotary Engine,
        is_embarrassing: true,
        active: true,
    },
]
</code></pre>
<p>Human-friendly list:</p>
<pre class=""lang-none prettyprint-override""><code>+----------------------+----------+---------------------+
| Name                 | Active?  | Embarrassing?       |
+----------------------+----------+---------------------+
| Megaphone            | Inactive | Not embarassing     |
| Wankel Rotary Engine | Active   | Item is Embarassing |
+----------------------+----------+---------------------+
</code></pre>
<p><strong>The key question:<br />
What is the best way to solve this in a scalable, efficient, elegant, sensible way?</strong></p>
<p>I have thought of a couple of optionsneither of these feel scalable nor elegant,
and are brittle.</p>
<p>(1) sequence of in-line v-if conditions within the component view template</p>
<pre class=""lang-html prettyprint-override""><code>    &lt;p v-if=item.property.is_embarrassing&quot;&gt;
        Item is Embarrassing
    &lt;/p&gt;
    &lt;p v-else&gt;
        Not embarassing
    &lt;/p&gt;
</code></pre>
<p>(2) computed properties in the component</p>
<pre class=""lang-html prettyprint-override""><code>    &lt;p&gt;
        {{ detailsPropertyEmbarrassing }}
    &lt;/p&gt;
</code></pre>
<pre class=""lang-js prettyprint-override""><code>    detailsPropertyEmbarrassing() {
        return item.property.is_embarrassing ? Item is Embarrassing : Not Embarrassing;
    },
</code></pre>
<p>I have also been noodling over the idea of some sort of Map that is imported along with the data and used to get the right labels, but I havent completely worked that out yet.</p>
<p>What is a solution for transforming data fields to people-friendly labels across an entire application, for a variety of different display situations?</p>
<p>(Side note: I may also need to transform the field in other ways, such as truncating length)</p>
<p>And is there a way to establish this globally in the app in a manner that is scalable, both technically and organizationally,
so that new components can display as desired, near-automatically?</p>
<p>This seems like a basic fundamental need in any app of size, not just vue,
so I feel like this has been/has to have been solved before, but either I cannot find the right research keywords or am missing something obvious.</p>
<p>Thanks in advance!</p>
",201,1,0,2,vue.js;design-patterns,2020-11-21 22:26:05,2020-11-21 22:26:05,2022-07-16 08:06:39,a high level vue application question  a little background  the appropriate human friendly term depends on the field itself  so there is no single translation for all fields   human friendly list     sequence of in line v if conditions within the component view template    computed properties in the component i have also been noodling over the idea of some sort of map that is imported along with the data and used to get the right labels  but i haven t completely worked that out yet  what is a solution for transforming data fields to people friendly labels across an entire application  for a variety of different display situations   side note  i may also need to transform the field in other ways  such as truncating length   thanks in advance ,vue application best practice pattern to display data with people friendly label ,high level vue application question little background appropriate human friendly term depends field single translation fields human friendly sequence line v conditions within component view template computed properties component also noodling idea sort map imported along data used get right labels completely worked yet solution transforming data fields people friendly labels across entire application variety different display situations side note may also need transform field ways truncating length thanks advance,vue application best practice pattern display data people friendly label,vue application best practice pattern display data people friendly labelhigh level vue application question little background appropriate human friendly term depends field single translation fields human friendly sequence line v conditions within component view template computed properties component also noodling idea sort map imported along data used get right labels completely worked yet solution transforming data fields people friendly labels across entire application variety different display situations side note may also need transform field ways truncating length thanks advance,"['vue', 'application', 'best', 'practice', 'pattern', 'display', 'data', 'people', 'friendly', 'labelhigh', 'level', 'vue', 'application', 'question', 'little', 'background', 'appropriate', 'human', 'friendly', 'term', 'depends', 'field', 'single', 'translation', 'fields', 'human', 'friendly', 'sequence', 'line', 'v', 'conditions', 'within', 'component', 'view', 'template', 'computed', 'properties', 'component', 'also', 'noodling', 'idea', 'sort', 'map', 'imported', 'along', 'data', 'used', 'get', 'right', 'labels', 'completely', 'worked', 'yet', 'solution', 'transforming', 'data', 'fields', 'people', 'friendly', 'labels', 'across', 'entire', 'application', 'variety', 'different', 'display', 'situations', 'side', 'note', 'may', 'also', 'need', 'transform', 'field', 'ways', 'truncating', 'length', 'thanks', 'advance']","['vue', 'applic', 'best', 'practic', 'pattern', 'display', 'data', 'peopl', 'friendli', 'labelhigh', 'level', 'vue', 'applic', 'question', 'littl', 'background', 'appropri', 'human', 'friendli', 'term', 'depend', 'field', 'singl', 'translat', 'field', 'human', 'friendli', 'sequenc', 'line', 'v', 'condit', 'within', 'compon', 'view', 'templat', 'comput', 'properti', 'compon', 'also', 'noodl', 'idea', 'sort', 'map', 'import', 'along', 'data', 'use', 'get', 'right', 'label', 'complet', 'work', 'yet', 'solut', 'transform', 'data', 'field', 'peopl', 'friendli', 'label', 'across', 'entir', 'applic', 'varieti', 'differ', 'display', 'situat', 'side', 'note', 'may', 'also', 'need', 'transform', 'field', 'way', 'truncat', 'length', 'thank', 'advanc']"
192,206,206,19541582,72999914,How to remove square brackets from a csv file,"<p>I am currently working on a machine learning prediction using TensorFlow. To capture the results of the prediction, I have had to use the function 'tolist()' to put them in a column suited for submission. Here is a small sample of the submission file.</p>
<pre><code>    ,target_r,target_g,target_b
0,[0.5584068298339844],[0.583054780960083],[0.5836431384086609]

1,[0.5065091848373413],[0.522176206111908],[0.5133420825004578] 

2,[0.48487409949302673],[0.49142706394195557],[0.4913524091243744]

3,[0.4954725503921509],[0.5046894550323486],[0.49923211336135864]

4,[0.4939081370830536],[0.4992391765117645],[0.4986460208892822]
</code></pre>
<p>As you can see, I need to remove the square brackets. How can I do this using python? Please advise; thanks</p>
",43,1,0,1,python,2022-07-16 00:51:54,2022-07-16 00:51:54,2022-07-16 01:15:06,i am currently working on a machine learning prediction using tensorflow  to capture the results of the prediction  i have had to use the function  tolist    to put them in a column suited for submission  here is a small sample of the submission file  as you can see  i need to remove the square brackets  how can i do this using python  please advise  thanks,how to remove square brackets from a csv file,currently working machine learning prediction using tensorflow capture results prediction use function tolist put column suited submission small sample submission file see need remove square brackets using python please advise thanks,remove square brackets csv file,remove square brackets csv filecurrently working machine learning prediction using tensorflow capture results prediction use function tolist put column suited submission small sample submission file see need remove square brackets using python please advise thanks,"['remove', 'square', 'brackets', 'csv', 'filecurrently', 'working', 'machine', 'learning', 'prediction', 'using', 'tensorflow', 'capture', 'results', 'prediction', 'use', 'function', 'tolist', 'put', 'column', 'suited', 'submission', 'small', 'sample', 'submission', 'file', 'see', 'need', 'remove', 'square', 'brackets', 'using', 'python', 'please', 'advise', 'thanks']","['remov', 'squar', 'bracket', 'csv', 'filecurr', 'work', 'machin', 'learn', 'predict', 'use', 'tensorflow', 'captur', 'result', 'predict', 'use', 'function', 'tolist', 'put', 'column', 'suit', 'submiss', 'small', 'sampl', 'submiss', 'file', 'see', 'need', 'remov', 'squar', 'bracket', 'use', 'python', 'pleas', 'advis', 'thank']"
193,207,207,10747001,54263894,How do I allow pip inside anaconda3 venv when pip set to require virtualenv?,"<p>I've just rebuilt my mac environment using the tutorials here:</p>

<p><a href=""https://hackercodex.com/guide/mac-development-configuration/"" rel=""nofollow noreferrer"">https://hackercodex.com/guide/mac-development-configuration/</a> &amp; here: <a href=""https://hackercodex.com/guide/python-development-environment-on-mac-osx/"" rel=""nofollow noreferrer"">https://hackercodex.com/guide/python-development-environment-on-mac-osx/</a></p>

<p>I want to require a virtualenv for pip, and have set that by opening:</p>

<p><code>vim ~/Library/Application\ Support/pip/pip.conf</code></p>

<p>and adding:</p>

<pre><code>[install]
require-virtualenv = true

[uninstall]
require-virtualenv = true
</code></pre>

<p>Then, I followed a guide to set up jupyter notebooks w/tensorflow, because I am trying to follow a udemy course on machine learning that requires both: <a href=""https://medium.com/@margaretmz/anaconda-jupyter-notebook-tensorflow-and-keras-b91f381405f8"" rel=""nofollow noreferrer"">https://medium.com/@margaretmz/anaconda-jupyter-notebook-tensorflow-and-keras-b91f381405f8</a></p>

<p>During this tutorial, it mentions that you should use pip install instead of conda install for tensorflow, because the conda package isn't officially supported.</p>

<p>I can install pip on conda just fine by running:</p>

<pre><code>conda install pip
</code></pre>

<p>But when I try to run:</p>

<pre><code>pip3 install tensorflow
</code></pre>

<p>I get the error:</p>

<p>""Could not find an activated virtualenv (required).""</p>

<p>I know why I'm getting this error, I just don't know how to change my code to ALSO accept use of pip &amp; pip3 inside anaconda venvs.</p>

<p>My anaconda3 folder is inside my Virtualenvs folder, along with all of my other virtual environments.</p>

<p>I've tried temporarily turning off the restriction by defining a new function in ~/.bashrc:</p>

<pre><code>cpip(){
PIP_REQUIRE_VIRTUALENV=""0"" pip3 ""$@""
}
</code></pre>

<p>and using that instead, with no luck, not surprisingly. </p>

<p>I think the problem may be here, inside my bash_profile:</p>

<pre><code># How to Set Up Mac For Dev:
# https://hackercodex.com/guide/mac-development-configuration/
# Ensure user-installed binaries take precedence
export PATH=/usr/local/bin:$PATH
# Load .bashrc if it exists
test -f ~/.bashrc &amp;&amp; source ~/.bashrc


# Activate Bash Completion:
if [ -f $(brew --prefix)/etc/bash_completion ]; then
    source $(brew --prefix)/etc/bash_completion
fi


# Toggle for installing global packages:
gpip(){
   PIP_REQUIRE_VIRTUALENV=""0"" pip3 ""$@""
}
# Toggle for installing conda packages:
cpip(){
   PIP_REQUIRE_VIRTUALENV=""0"" pip3 ""$@""
}
# Be sure to run ""source ~/.bash_profile after toggle for changes to
take effect.
# Run ""gpip install"" (i.e. ""gpip install --upgrade pip setuptools
wheel virtualenv"")


# added by Anaconda3 2018.12 installer
# &gt;&gt;&gt; conda init &gt;&gt;&gt;
# !! Contents within this block are managed by 'conda init' !!
__conda_setup=""$(CONDA_REPORT_ERRORS=false
'/Users/erikhayton/Virtualenvs/anaconda3/bin/conda' shell.bash hook
2&gt; /dev/null)""
if [ $? -eq 0 ]; then
    \eval ""$__conda_setup""
else
    if [ -f
""/Users/erikhayton/Virtualenvs/anaconda3/etc/profile.d/conda.sh"" ];
then
        .
""/Users/erikhayton/Virtualenvs/anaconda3/etc/profile.d/conda.sh""
        CONDA_CHANGEPS1=false conda activate base
    else
        \export
PATH=""/Users/erikhayton/Virtualenvs/anaconda3/bin:$PATH""
    fi
fi
unset __conda_setup
# &lt;&lt;&lt; conda init &lt;&lt;&lt;
</code></pre>

<p>I want to be able to use pip (&amp; pip3, pip2) in both (&amp; only in) anaconda3's activated 'env's
and virtualenvs.</p>
",970,3,2,5,python;python-3.x;anaconda;virtualenv;conda,2019-01-19 05:46:12,2019-01-19 05:46:12,2022-07-15 23:21:56,i ve just rebuilt my mac environment using the tutorials here    amp  here   i want to require a virtualenv for pip  and have set that by opening  vim   library application  support pip pip conf and adding  then  i followed a guide to set up jupyter notebooks w tensorflow  because i am trying to follow a udemy course on machine learning that requires both   during this tutorial  it mentions that you should use pip install instead of conda install for tensorflow  because the conda package isn t officially supported  i can install pip on conda just fine by running  but when i try to run  i get the error  could not find an activated virtualenv  required   i know why i m getting this error  i just don t know how to change my code to also accept use of pip  amp  pip inside anaconda venvs  my anaconda folder is inside my virtualenvs folder  along with all of my other virtual environments  i ve tried temporarily turning off the restriction by defining a new function in    bashrc  and using that instead  with no luck  not surprisingly   i think the problem may be here  inside my bash_profile ,how do i allow pip inside anaconda venv when pip set to require virtualenv ,rebuilt mac environment using tutorials amp want require virtualenv pip set opening vim library application support pip pip conf adding followed guide set jupyter notebooks w tensorflow trying follow udemy course machine learning requires tutorial mentions use pip install instead conda install tensorflow conda package officially supported install pip conda fine running try run get error could find activated virtualenv required know getting error know change code also accept use pip amp pip inside anaconda venvs anaconda folder inside virtualenvs folder along virtual environments tried temporarily turning restriction defining function bashrc using instead luck surprisingly think problem may inside bash_profile,allow pip inside anaconda venv pip set require virtualenv,allow pip inside anaconda venv pip set require virtualenvrebuilt mac environment using tutorials amp want require virtualenv pip set opening vim library application support pip pip conf adding followed guide set jupyter notebooks w tensorflow trying follow udemy course machine learning requires tutorial mentions use pip install instead conda install tensorflow conda package officially supported install pip conda fine running try run get error could find activated virtualenv required know getting error know change code also accept use pip amp pip inside anaconda venvs anaconda folder inside virtualenvs folder along virtual environments tried temporarily turning restriction defining function bashrc using instead luck surprisingly think problem may inside bash_profile,"['allow', 'pip', 'inside', 'anaconda', 'venv', 'pip', 'set', 'require', 'virtualenvrebuilt', 'mac', 'environment', 'using', 'tutorials', 'amp', 'want', 'require', 'virtualenv', 'pip', 'set', 'opening', 'vim', 'library', 'application', 'support', 'pip', 'pip', 'conf', 'adding', 'followed', 'guide', 'set', 'jupyter', 'notebooks', 'w', 'tensorflow', 'trying', 'follow', 'udemy', 'course', 'machine', 'learning', 'requires', 'tutorial', 'mentions', 'use', 'pip', 'install', 'instead', 'conda', 'install', 'tensorflow', 'conda', 'package', 'officially', 'supported', 'install', 'pip', 'conda', 'fine', 'running', 'try', 'run', 'get', 'error', 'could', 'find', 'activated', 'virtualenv', 'required', 'know', 'getting', 'error', 'know', 'change', 'code', 'also', 'accept', 'use', 'pip', 'amp', 'pip', 'inside', 'anaconda', 'venvs', 'anaconda', 'folder', 'inside', 'virtualenvs', 'folder', 'along', 'virtual', 'environments', 'tried', 'temporarily', 'turning', 'restriction', 'defining', 'function', 'bashrc', 'using', 'instead', 'luck', 'surprisingly', 'think', 'problem', 'may', 'inside', 'bash_profile']","['allow', 'pip', 'insid', 'anaconda', 'venv', 'pip', 'set', 'requir', 'virtualenvrebuilt', 'mac', 'environ', 'use', 'tutori', 'amp', 'want', 'requir', 'virtualenv', 'pip', 'set', 'open', 'vim', 'librari', 'applic', 'support', 'pip', 'pip', 'conf', 'ad', 'follow', 'guid', 'set', 'jupyt', 'notebook', 'w', 'tensorflow', 'tri', 'follow', 'udemi', 'cours', 'machin', 'learn', 'requir', 'tutori', 'mention', 'use', 'pip', 'instal', 'instead', 'conda', 'instal', 'tensorflow', 'conda', 'packag', 'offici', 'support', 'instal', 'pip', 'conda', 'fine', 'run', 'tri', 'run', 'get', 'error', 'could', 'find', 'activ', 'virtualenv', 'requir', 'know', 'get', 'error', 'know', 'chang', 'code', 'also', 'accept', 'use', 'pip', 'amp', 'pip', 'insid', 'anaconda', 'venv', 'anaconda', 'folder', 'insid', 'virtualenv', 'folder', 'along', 'virtual', 'environ', 'tri', 'temporarili', 'turn', 'restrict', 'defin', 'function', 'bashrc', 'use', 'instead', 'luck', 'surprisingli', 'think', 'problem', 'may', 'insid', 'bash_profil']"
194,208,208,19535796,72998807,"SciKit Python ValueError: Found input variables with inconsistent numbers of samples: [2, 59158]","<p>I'm getting the error in the question title while trying to use the SciKit machine learning library. My code is shown here:</p>
<pre><code>def ml():
    col_names = ['Tweet ID', 'entity', 'sentiment', 'Tweet content']
    data = pd.read_csv('twitter_training.csv', names=col_names).drop(columns=['Tweet ID']).dropna()
    x = data.drop(columns=['sentiment'])
    y = data.drop(columns=['entity', 'Tweet content'])
    print(x.shape, y.shape)

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

    model = Pipeline([
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer()),
        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),
    ])

    model.fit(x_train, y_train)
</code></pre>
<p>The outputs of those print statements are as follows:</p>
<pre><code>(73948, 2) (73948, 1)
(59158, 2) (59158, 1) (14790, 2) (14790, 1)
</code></pre>
<p>The print statements show that x_train and y_train are the same length, yet I'm still getting this error. Also, it tells me the error is coming from the model.fit call at the bottom, not the train_test_split call. Any ideas as to what's happening? I'm at a dead stop and there's a ton of people with this same error online and every single solution I've found I already did correctly.</p>
",22,0,0,5,python;pandas;dataframe;machine-learning;scikit-learn,2022-07-15 22:29:11,2022-07-15 22:29:11,2022-07-15 22:29:11,i m getting the error in the question title while trying to use the scikit machine learning library  my code is shown here  the outputs of those print statements are as follows  the print statements show that x_train and y_train are the same length  yet i m still getting this error  also  it tells me the error is coming from the model fit call at the bottom  not the train_test_split call  any ideas as to what s happening  i m at a dead stop and there s a ton of people with this same error online and every single solution i ve found i already did correctly ,scikit python valueerror  found input variables with inconsistent numbers of samples      ,getting error question title trying use scikit machine learning library code shown outputs print statements follows print statements show x_train y_train length yet still getting error also tells error coming model fit call bottom train_test_split call ideas happening dead stop ton people error online every single solution found already correctly,scikit python valueerror found input variables inconsistent numbers samples,scikit python valueerror found input variables inconsistent numbers samplesgetting error question title trying use scikit machine learning library code shown outputs print statements follows print statements show x_train y_train length yet still getting error also tells error coming model fit call bottom train_test_split call ideas happening dead stop ton people error online every single solution found already correctly,"['scikit', 'python', 'valueerror', 'found', 'input', 'variables', 'inconsistent', 'numbers', 'samplesgetting', 'error', 'question', 'title', 'trying', 'use', 'scikit', 'machine', 'learning', 'library', 'code', 'shown', 'outputs', 'print', 'statements', 'follows', 'print', 'statements', 'show', 'x_train', 'y_train', 'length', 'yet', 'still', 'getting', 'error', 'also', 'tells', 'error', 'coming', 'model', 'fit', 'call', 'bottom', 'train_test_split', 'call', 'ideas', 'happening', 'dead', 'stop', 'ton', 'people', 'error', 'online', 'every', 'single', 'solution', 'found', 'already', 'correctly']","['scikit', 'python', 'valueerror', 'found', 'input', 'variabl', 'inconsist', 'number', 'samplesget', 'error', 'question', 'titl', 'tri', 'use', 'scikit', 'machin', 'learn', 'librari', 'code', 'shown', 'output', 'print', 'statement', 'follow', 'print', 'statement', 'show', 'x_train', 'y_train', 'length', 'yet', 'still', 'get', 'error', 'also', 'tell', 'error', 'come', 'model', 'fit', 'call', 'bottom', 'train_test_split', 'call', 'idea', 'happen', 'dead', 'stop', 'ton', 'peopl', 'error', 'onlin', 'everi', 'singl', 'solut', 'found', 'alreadi', 'correctli']"
195,209,209,14890200,72997845,How do I use predict() on CompactClassificationNeuralNetwork and CompactClassificationEnsemble objects in 32 bit Matlab 2015b?,"<p>I'm trying to use existing machine learning models to make predicitions based on sensor data in real time. Interfacing between the sensor and computer requires a 32 bit ADC, so the computer can use at best R2015b. I'm trying to find a way to perform the equivalent of <code>predict(model,data)</code>
in R2015b. I thought to generate C/C++ code similarly to the &quot;Generate C/C++ for Prediction&quot; example in the <a href=""https://www.mathworks.com/help/stats/loadlearnerforcoder.html"" rel=""nofollow noreferrer"">LoadLearnerForCoder() documentation</a>. However, since the C/C++ code must be generated on a 64 bit Matlab version, the mex file is 64 bit as well and can't be run in R2015b. Is there a way to tell LoadLearnerForCoder() to create a 32 bit mex file? Or is there a different approach I should take to getting an equivalent to predict() working in R2015b?</p>
<p>Note: I have also posted this question on the Matlab community forum <a href=""https://www.mathworks.com/matlabcentral/answers/1761120-how-do-i-use-predict-on-compactclassificationneuralnetwork-and-compactclassificationensemble-objec"" rel=""nofollow noreferrer"">here</a></p>
",18,0,-1,5,c++;matlab;machine-learning;code-generation;predict,2022-07-15 20:40:21,2022-07-15 20:40:21,2022-07-15 20:40:21,note  i have also posted this question on the matlab community forum ,how do i use predict   on compactcla ssificatio nneuralnet work and compactcla ssificatio nensemble objects in  bit matlab b ,note also posted question matlab community forum,use predict compactcla ssificatio nneuralnet work compactcla ssificatio nensemble objects bit matlab b,use predict compactcla ssificatio nneuralnet work compactcla ssificatio nensemble objects bit matlab bnote also posted question matlab community forum,"['use', 'predict', 'compactcla', 'ssificatio', 'nneuralnet', 'work', 'compactcla', 'ssificatio', 'nensemble', 'objects', 'bit', 'matlab', 'bnote', 'also', 'posted', 'question', 'matlab', 'community', 'forum']","['use', 'predict', 'compactcla', 'ssificatio', 'nneuralnet', 'work', 'compactcla', 'ssificatio', 'nensembl', 'object', 'bit', 'matlab', 'bnote', 'also', 'post', 'question', 'matlab', 'commun', 'forum']"
196,210,210,19555440,72992237,How do you load binary files compressed in gzip format?,"<p>To begin with, let me preface this by saying I am not a Computer Scientist or Software Engineer. I am a physics student, so I apologize if this question is dumb.</p>
<p>I have been messing with machine learning models on PyTorch and learning how to use them, how they work, etc. However, the issue is the datasets. I have used MNIST digits and fashion and the other datasets in PyTorch, but I want to learn how to actually download and use the other datasets out there (ImageNet, Animals with Attributes, SUN, to name a few).</p>
<p>When I download these files (to be specific, the SUN labels with attributes and the corresponding images) they all are in .tar.gz format. As I understand it, .gz stands for gzip, which is a compression algorithm. However, I have several questions:</p>
<ol>
<li>What even is a .tar file and why is it that most datasets come in this format?</li>
<li>How do you actually uncompress those gzip files in code (and more importantly, what do each of the lines of code actually do)?</li>
<li>I have tried to uncompress the MNIST files from the source using some tutorials on gzip, but they seem to be in &quot;binary.&quot; What exactly does it mean for them to be in binary once uncompressed, and how do you go from binary to arrays?</li>
</ol>
<p>I have seen some tutorials and code on HOW to do some of these things, but in all honesty all that I do is just copy code and sometimes it works, sometimes it doesn't, but regardless I have no idea why, or what the code even does. I have by now a fairly good grasp on how to train and test models on PyTorch, but my issue is more on the computer side and what those files even are, and what the lines of code even do when transforming those files into usable arrays. Explain the steps to me like I have no idea how anything computer works, if possible.</p>
",25,1,0,4,python;gzip;binaryfiles;tar,2022-07-15 12:56:25,2022-07-15 12:56:25,2022-07-15 19:40:13,to begin with  let me preface this by saying i am not a computer scientist or software engineer  i am a physics student  so i apologize if this question is dumb  i have been messing with machine learning models on pytorch and learning how to use them  how they work  etc  however  the issue is the datasets  i have used mnist digits and fashion and the other datasets in pytorch  but i want to learn how to actually download and use the other datasets out there  imagenet  animals with attributes  sun  to name a few   when i download these files  to be specific  the sun labels with attributes and the corresponding images  they all are in  tar gz format  as i understand it   gz stands for gzip  which is a compression algorithm  however  i have several questions  i have seen some tutorials and code on how to do some of these things  but in all honesty all that i do is just copy code and sometimes it works  sometimes it doesn t  but regardless i have no idea why  or what the code even does  i have by now a fairly good grasp on how to train and test models on pytorch  but my issue is more on the computer side and what those files even are  and what the lines of code even do when transforming those files into usable arrays  explain the steps to me like i have no idea how anything computer works  if possible ,how do you load binary files compressed in gzip format ,begin let preface saying computer scientist software engineer physics student apologize question dumb messing machine learning models pytorch learning use work etc however issue datasets used mnist digits fashion datasets pytorch want learn actually download use datasets imagenet animals attributes sun name download files specific sun labels attributes corresponding images tar gz format understand gz stands gzip compression algorithm however several questions seen tutorials code things honesty copy code sometimes works sometimes regardless idea code even fairly good grasp train test models pytorch issue computer side files even lines code even transforming files usable arrays explain steps like idea anything computer works possible,load binary files compressed gzip format,load binary files compressed gzip formatbegin let preface saying computer scientist software engineer physics student apologize question dumb messing machine learning models pytorch learning use work etc however issue datasets used mnist digits fashion datasets pytorch want learn actually download use datasets imagenet animals attributes sun name download files specific sun labels attributes corresponding images tar gz format understand gz stands gzip compression algorithm however several questions seen tutorials code things honesty copy code sometimes works sometimes regardless idea code even fairly good grasp train test models pytorch issue computer side files even lines code even transforming files usable arrays explain steps like idea anything computer works possible,"['load', 'binary', 'files', 'compressed', 'gzip', 'formatbegin', 'let', 'preface', 'saying', 'computer', 'scientist', 'software', 'engineer', 'physics', 'student', 'apologize', 'question', 'dumb', 'messing', 'machine', 'learning', 'models', 'pytorch', 'learning', 'use', 'work', 'etc', 'however', 'issue', 'datasets', 'used', 'mnist', 'digits', 'fashion', 'datasets', 'pytorch', 'want', 'learn', 'actually', 'download', 'use', 'datasets', 'imagenet', 'animals', 'attributes', 'sun', 'name', 'download', 'files', 'specific', 'sun', 'labels', 'attributes', 'corresponding', 'images', 'tar', 'gz', 'format', 'understand', 'gz', 'stands', 'gzip', 'compression', 'algorithm', 'however', 'several', 'questions', 'seen', 'tutorials', 'code', 'things', 'honesty', 'copy', 'code', 'sometimes', 'works', 'sometimes', 'regardless', 'idea', 'code', 'even', 'fairly', 'good', 'grasp', 'train', 'test', 'models', 'pytorch', 'issue', 'computer', 'side', 'files', 'even', 'lines', 'code', 'even', 'transforming', 'files', 'usable', 'arrays', 'explain', 'steps', 'like', 'idea', 'anything', 'computer', 'works', 'possible']","['load', 'binari', 'file', 'compress', 'gzip', 'formatbegin', 'let', 'prefac', 'say', 'comput', 'scientist', 'softwar', 'engin', 'physic', 'student', 'apolog', 'question', 'dumb', 'mess', 'machin', 'learn', 'model', 'pytorch', 'learn', 'use', 'work', 'etc', 'howev', 'issu', 'dataset', 'use', 'mnist', 'digit', 'fashion', 'dataset', 'pytorch', 'want', 'learn', 'actual', 'download', 'use', 'dataset', 'imagenet', 'anim', 'attribut', 'sun', 'name', 'download', 'file', 'specif', 'sun', 'label', 'attribut', 'correspond', 'imag', 'tar', 'gz', 'format', 'understand', 'gz', 'stand', 'gzip', 'compress', 'algorithm', 'howev', 'sever', 'question', 'seen', 'tutori', 'code', 'thing', 'honesti', 'copi', 'code', 'sometim', 'work', 'sometim', 'regardless', 'idea', 'code', 'even', 'fairli', 'good', 'grasp', 'train', 'test', 'model', 'pytorch', 'issu', 'comput', 'side', 'file', 'even', 'line', 'code', 'even', 'transform', 'file', 'usabl', 'array', 'explain', 'step', 'like', 'idea', 'anyth', 'comput', 'work', 'possibl']"
197,211,211,17119410,72985935,How to run Machine Learning algorithms in GPU,"<p>I have used backward elimination algorithm to reduce features. But the point is with large amount of features and samples, it run on CPU and pretty slow.</p>
<p>How could i run it multithreading on GPU like i train Deep Learning. This is my code</p>
<pre><code>import pandas as pd
data = pd.read_csv('data.csv')
X = data.drop(['Path','id','label'], axis=1)
y = data['label']

from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from sklearn.linear_model import LinearRegression
lreg = LinearRegression()
new_sfs = sfs(lreg, k_features=1600, forward=False, verbose=1, scoring='neg_mean_squared_error')
new_sfs = new_sfs.fit(X, y)
feat_names = list(sfs1.k_feature_names_)
</code></pre>
",38,1,0,5,python;machine-learning;time;process;gpu,2022-07-14 22:49:29,2022-07-14 22:49:29,2022-07-15 19:21:11,i have used backward elimination algorithm to reduce features  but the point is with large amount of features and samples  it run on cpu and pretty slow  how could i run it multithreading on gpu like i train deep learning  this is my code,how to run machine learning algorithms in gpu,used backward elimination algorithm reduce features point large amount features samples run cpu pretty slow could run multithreading gpu like train deep learning code,run machine learning algorithms gpu,run machine learning algorithms gpuused backward elimination algorithm reduce features point large amount features samples run cpu pretty slow could run multithreading gpu like train deep learning code,"['run', 'machine', 'learning', 'algorithms', 'gpuused', 'backward', 'elimination', 'algorithm', 'reduce', 'features', 'point', 'large', 'amount', 'features', 'samples', 'run', 'cpu', 'pretty', 'slow', 'could', 'run', 'multithreading', 'gpu', 'like', 'train', 'deep', 'learning', 'code']","['run', 'machin', 'learn', 'algorithm', 'gpuus', 'backward', 'elimin', 'algorithm', 'reduc', 'featur', 'point', 'larg', 'amount', 'featur', 'sampl', 'run', 'cpu', 'pretti', 'slow', 'could', 'run', 'multithread', 'gpu', 'like', 'train', 'deep', 'learn', 'code']"
198,212,212,17126954,72964814,How to calibrate probabilities in R?,"<p>I am trying to calibrate probabilities that I get with the predict function in the R package.
I have in my case two classes and mutiple predictors. I used the iris dataset as an example for you to try and help me out.</p>
<pre><code>my_data  &lt;- iris %&gt;% #reducing the data to have two classes only
  dplyr::filter((Species ==&quot;virginica&quot; | Species == &quot;versicolor&quot;) ) %&gt;% dplyr::select(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species)

my_data &lt;- droplevels(my_data)

index &lt;- createDataPartition(y=my_data$Species,p=0.6,list=FALSE) 
#creating train and test set for machine learning
Train &lt;- my_data[index,]
Test &lt;-  my_data[-index,]

#machine learning based on Train data partition with glmnet method
classCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10,repeats=5,classProbs =  TRUE,savePredictions = &quot;final&quot;)
set.seed(355)
glmnet_ML &lt;- train(Species~., Train, method= &quot;glmnet&quot;,  trControl=classCtrl)
glmnet_ML

#probabilities to assign each row of data to one class or the other on Test
predTestprob &lt;- predict(glmnet_ML,Test,type=&quot;prob&quot;)
pred 


#trying out calibration following &quot;Applied predictive modeling&quot; book from Max Kuhn p266-273
predTrainprob &lt;- predict(glmnet_ML,Train,type=&quot;prob&quot;)
predTest &lt;- predict(glmnet_ML,Test)
predTestprob &lt;- predict(glmnet_ML,Test,type=&quot;prob&quot;)

Test$PredProb &lt;- predTestprob[,&quot;versicolor&quot;]
Test$Pred &lt;- predTest
Train$PredProb &lt;- predTrainprob[,&quot;versicolor&quot;]

#logistic regression to calibrate
sigmoidalCal &lt;- glm(relevel(Species, ref= &quot;virginica&quot;) ~ PredProb,data = Train,family = binomial)
coef(summary(sigmoidalCal))

#predicting calibrated scores
sigmoidProbs &lt;- predict(sigmoidalCal,newdata = Test[,&quot;PredProb&quot;, drop = FALSE],type = &quot;response&quot;)
Test$CalProb &lt;- sigmoidProbs

#plotting to see if it works
calCurve2 &lt;- calibration(Species ~ PredProb +  CalProb, data = Test)
xyplot(calCurve2,auto.key = list(columns = 2))
</code></pre>
<p>According to me, the result given by the plot is not good which indicates a mistake in the calibration, the Calprob curve should follow the diagonal but it doe not.</p>
<p>Has anyone done anything similar ?</p>
",31,0,0,4,r;probability;caret;calibration,2022-07-13 13:33:02,2022-07-13 13:33:02,2022-07-15 19:20:20,according to me  the result given by the plot is not good which indicates a mistake in the calibration  the calprob curve should follow the diagonal but it doe not  has anyone done anything similar  ,how to calibrate probabilities in r ,according result given plot good indicates mistake calibration calprob curve follow diagonal doe anyone done anything similar,calibrate probabilities r,calibrate probabilities raccording result given plot good indicates mistake calibration calprob curve follow diagonal doe anyone done anything similar,"['calibrate', 'probabilities', 'raccording', 'result', 'given', 'plot', 'good', 'indicates', 'mistake', 'calibration', 'calprob', 'curve', 'follow', 'diagonal', 'doe', 'anyone', 'done', 'anything', 'similar']","['calibr', 'probabl', 'raccord', 'result', 'given', 'plot', 'good', 'indic', 'mistak', 'calibr', 'calprob', 'curv', 'follow', 'diagon', 'doe', 'anyon', 'done', 'anyth', 'similar']"
199,213,213,4332837,72993997,Scikit Learn Perceptron always returns same threshold for 1D vector and converges &lt;10 iter,"<p>I am trying to run a simulation where the true population is 2 classes of normal distribution, both mean 0, standard deviation 4000.  I am attempting to use a perceptron to determine the relation of sample size to degree of overfitting.  However, the perceptron always converges after 6 iterations with a threshold of 0 despite a sample size of 10 per each class which you can clearly see should not have a threshold of 0.  Why is the threshold always 0?  Also, is there a better way to output the threshold than my code below?  I'm using perceptron because I want the simplest possible classifier - is there a simpler, easier classifier to use?  Note, Logistic Regression does seem to have thresholds other than 0 when used in exactly the same way.</p>
<pre><code>import numpy as np
mu, sigma = 0, 4000 # mean and standard deviation
pop_size=int(1e4)
p1 = (np.random.normal(mu, sigma, pop_size)) #1 million, pinky
p2 = (np.random.normal(mu, sigma, pop_size))

#take 10 samples of each group and plot on the same plot
def sample_pop(n):
    s1 = np.random.choice(p1, size=n, replace=False)
    s2 = np.random.choice(p2, size=n, replace=False)
    plt.subplot(211)
    count, bins, ignored = plt.hist(p1, 50, density=False, color='green', range=[-15000, 15000], histtype='bar', ec='black')
    plt.ylabel(&quot;n with Rebel Alliance&quot;)
    ymax=plt.gca().get_ylim()
    plt.plot(s1,[ymax]*n,'o',color='green')
    plt.subplot(212)
    count, bins, ignored = plt.hist(p2, 50, density=False, color = &quot;red&quot;, range=[-15000, 15000], histtype='bar', ec='black')
    plt.xlabel(&quot;Midi Clorian Rate (The Force)&quot;)
    plt.ylabel(&quot;n with Dark Side&quot;)
    ymax=plt.gca().get_ylim()[1]
    plt.plot(s2,[ymax]*n,'x',color='red')
    plt.show()
    return s1,s2
n=10
s1,s2=sample_pop(n)

from sklearn.linear_model import Perceptron
clf = Perceptron()
s_all=np.hstack((s1,s2)).reshape(-1, 1)
y=np.hstack( ( [0]*len(s1), [1]*len(s2) ) )
clf.fit(s_all, y)

def plot1D(X, y, model,show=True):

    # adapted from https://github.com/tirthajyoti/Machine-Learning-with #Python/blob/master/Utilities/ML-Python-utils.py
    
    # Step size of the mesh. Decrease to increase the quality of the VQ.
    h = 0.2    # point in the mesh [x_min, m_max]x[y_min, y_max].    

    # Plot the decision boundary. For that, we will assign a color to each
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = -.1,.1# X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))
    # Predictions to obtain the classification results
    #Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    Z = model.predict(np.arange(x_min, x_max, 0.1).reshape(-1, 1))
    dZ=np.diff(Z)
    print(Z[np.where(abs(dZ)&gt;0)[0]]) #this is the threshold
    

    # Plotting
    if show:
        plt.figure(figsize=(6,6))
    plt.contourf(xx, yy, np.vstack((Z,Z)), alpha=0.4)
    plt.scatter(X[:, 0], np.array( [-.05]*len(X) ), c=y, alpha=0.8, edgecolor=&quot;k&quot;)
    plt.ylim(-.1,0)
    plt.gca().get_yaxis().set_ticks([]) #set_visible(False)
    plt.xlabel('Midichlorian Rate (The Force)')
    if show:
        plt.show()
        
plot1D(s_all,y,clf)

from sklearn.metrics import accuracy_score
acc=accuracy_score(y, clf.predict(s_all))
acc
clf.n_iter_
</code></pre>
<p>PS - adding this image to answer Chris' comment below:
<a href=""https://i.stack.imgur.com/SFOHx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SFOHx.png"" alt=""1000 draws of sample size &quot;n&quot; vs. distribution of accuracies using logistic regression (perceptron results are similar)"" /></a></p>
",26,1,0,3,python;scikit-learn;perceptron,2022-07-15 15:27:19,2022-07-15 15:27:19,2022-07-15 18:38:25,i am trying to run a simulation where the true population is  classes of normal distribution  both mean   standard deviation    i am attempting to use a perceptron to determine the relation of sample size to degree of overfitting   however  the perceptron always converges after  iterations with a threshold of  despite a sample size of  per each class which you can clearly see should not have a threshold of    why is the threshold always    also  is there a better way to output the threshold than my code below   i m using perceptron because i want the simplest possible classifier   is there a simpler  easier classifier to use   note  logistic regression does seem to have thresholds other than  when used in exactly the same way ,scikit learn perceptron always returns same threshold for d vector and converges  lt  iter,trying run simulation true population classes normal distribution mean standard deviation attempting use perceptron determine relation sample size degree overfitting however perceptron always converges iterations threshold despite sample size per class clearly see threshold threshold always also better way output threshold code using perceptron want simplest possible classifier simpler easier classifier use note logistic regression seem thresholds used exactly way,scikit learn perceptron always returns threshold vector converges lt iter,scikit learn perceptron always returns threshold vector converges lt itertrying run simulation true population classes normal distribution mean standard deviation attempting use perceptron determine relation sample size degree overfitting however perceptron always converges iterations threshold despite sample size per class clearly see threshold threshold always also better way output threshold code using perceptron want simplest possible classifier simpler easier classifier use note logistic regression seem thresholds used exactly way,"['scikit', 'learn', 'perceptron', 'always', 'returns', 'threshold', 'vector', 'converges', 'lt', 'itertrying', 'run', 'simulation', 'true', 'population', 'classes', 'normal', 'distribution', 'mean', 'standard', 'deviation', 'attempting', 'use', 'perceptron', 'determine', 'relation', 'sample', 'size', 'degree', 'overfitting', 'however', 'perceptron', 'always', 'converges', 'iterations', 'threshold', 'despite', 'sample', 'size', 'per', 'class', 'clearly', 'see', 'threshold', 'threshold', 'always', 'also', 'better', 'way', 'output', 'threshold', 'code', 'using', 'perceptron', 'want', 'simplest', 'possible', 'classifier', 'simpler', 'easier', 'classifier', 'use', 'note', 'logistic', 'regression', 'seem', 'thresholds', 'used', 'exactly', 'way']","['scikit', 'learn', 'perceptron', 'alway', 'return', 'threshold', 'vector', 'converg', 'lt', 'itertri', 'run', 'simul', 'true', 'popul', 'class', 'normal', 'distribut', 'mean', 'standard', 'deviat', 'attempt', 'use', 'perceptron', 'determin', 'relat', 'sampl', 'size', 'degre', 'overfit', 'howev', 'perceptron', 'alway', 'converg', 'iter', 'threshold', 'despit', 'sampl', 'size', 'per', 'class', 'clearli', 'see', 'threshold', 'threshold', 'alway', 'also', 'better', 'way', 'output', 'threshold', 'code', 'use', 'perceptron', 'want', 'simplest', 'possibl', 'classifi', 'simpler', 'easier', 'classifi', 'use', 'note', 'logist', 'regress', 'seem', 'threshold', 'use', 'exactli', 'way']"
200,214,214,19381301,72994757,loop through the values of multiple dictionaries to perform an operation,"<p>I have three python dictionaries each created to store the predictions of three different machine learning model. The keys of the dictionaries are the instances (texts) indices and the values are lists of predictions from three different machine learning models.</p>
<pre><code>d1={348: [0,0,0,0.5,0,0,0.49], 349: [0,0,0.3,0.4,0.7,0.55,0.7], 350 [0,0,0.3,0.45,0.0,0.52,0.8]}

d2={348: [0,0,0,0.3,0,0.2,0.8], 349: [0,0,0.3,0.4,0.9,0.5,0.9], 350 [0,0,0.3,0.45,0.0,0.52,0.8]}

d3={348: [0,0,0,0.0,0,0,0.6], 349: [0,0,0.3,0.8,0.4,0.5,0.6], 350 [0,0,0.0,0.4,0.0,0.5,0.7]}
</code></pre>
<p>I want to calculate the agreement between the three models when assigning labels to the texts of the test set. I know that I can do this with lists of lists. For example:</p>
<pre><code>import krippendorff

preds_model1=[[0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0], [0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0]]
preds_model2=[[0,0,1,0,0,0,0], [0,1,1,1,0,1,0], [0,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0], [0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0]]
preds_model3=[[0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0], [0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0]]
preds_model4=[[0,0,1,0,0,0,0], [0,1,1,1,0,1,0], [0,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0], [0,0,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], [1,1,1,1,0,1,0], 
[1,1,1,1,0,1,0]]

for x in zip(preds_model1, preds_model2, preds_model3, preds_model4):
  kappa = krippendorff.alpha(x)
  print(kappa)
</code></pre>
<p>However, in my case, I want to perform the same but considering the values of the three dictionaries I have. My desired output is something like:</p>
<pre><code>preds_agreement={348: 0.55, 349: 0.8, 350: 0.4, 351: 0.6}
</code></pre>
<p>where the keys are still the text indices and the values are the agreement scores calculated using the function krippendorff.alpha()</p>
<p>Any ideas how I can achieve this result? Thanks in advance.</p>
",17,0,0,3,python;dictionary;kappa,2022-07-15 16:26:57,2022-07-15 16:26:57,2022-07-15 16:26:57,i have three python dictionaries each created to store the predictions of three different machine learning model  the keys of the dictionaries are the instances  texts  indices and the values are lists of predictions from three different machine learning models  i want to calculate the agreement between the three models when assigning labels to the texts of the test set  i know that i can do this with lists of lists  for example  however  in my case  i want to perform the same but considering the values of the three dictionaries i have  my desired output is something like  where the keys are still the text indices and the values are the agreement scores calculated using the function krippendorff alpha   any ideas how i can achieve this result  thanks in advance ,loop through the values of multiple dictionaries to perform an operation,three python dictionaries created store predictions three different machine learning model keys dictionaries instances texts indices values lists predictions three different machine learning models want calculate agreement three models assigning labels texts test set know lists lists example however case want perform considering values three dictionaries desired output something like keys still text indices values agreement scores calculated using function krippendorff alpha ideas achieve result thanks advance,loop values multiple dictionaries perform operation,loop values multiple dictionaries perform operationthree python dictionaries created store predictions three different machine learning model keys dictionaries instances texts indices values lists predictions three different machine learning models want calculate agreement three models assigning labels texts test set know lists lists example however case want perform considering values three dictionaries desired output something like keys still text indices values agreement scores calculated using function krippendorff alpha ideas achieve result thanks advance,"['loop', 'values', 'multiple', 'dictionaries', 'perform', 'operationthree', 'python', 'dictionaries', 'created', 'store', 'predictions', 'three', 'different', 'machine', 'learning', 'model', 'keys', 'dictionaries', 'instances', 'texts', 'indices', 'values', 'lists', 'predictions', 'three', 'different', 'machine', 'learning', 'models', 'want', 'calculate', 'agreement', 'three', 'models', 'assigning', 'labels', 'texts', 'test', 'set', 'know', 'lists', 'lists', 'example', 'however', 'case', 'want', 'perform', 'considering', 'values', 'three', 'dictionaries', 'desired', 'output', 'something', 'like', 'keys', 'still', 'text', 'indices', 'values', 'agreement', 'scores', 'calculated', 'using', 'function', 'krippendorff', 'alpha', 'ideas', 'achieve', 'result', 'thanks', 'advance']","['loop', 'valu', 'multipl', 'dictionari', 'perform', 'operationthre', 'python', 'dictionari', 'creat', 'store', 'predict', 'three', 'differ', 'machin', 'learn', 'model', 'key', 'dictionari', 'instanc', 'text', 'indic', 'valu', 'list', 'predict', 'three', 'differ', 'machin', 'learn', 'model', 'want', 'calcul', 'agreement', 'three', 'model', 'assign', 'label', 'text', 'test', 'set', 'know', 'list', 'list', 'exampl', 'howev', 'case', 'want', 'perform', 'consid', 'valu', 'three', 'dictionari', 'desir', 'output', 'someth', 'like', 'key', 'still', 'text', 'indic', 'valu', 'agreement', 'score', 'calcul', 'use', 'function', 'krippendorff', 'alpha', 'idea', 'achiev', 'result', 'thank', 'advanc']"
201,215,215,19527291,72991442,How does tensorflow pipeline work with data that does not fit in memory?,"<p>Hi I'm new to machine learning and I had a problem loading a big image dataset.
I saw some video about pipeline in tensorflow and I think I quite understand the concept behind: you give some data, you work on it (map, filter, ecc...) and you get new data to give to the model.</p>
<p>My question was about how can this system handle a very large dataset?</p>
<pre class=""lang-py prettyprint-override""><code>images_filenames = tf.constant(image_list)
masks_filenames = tf.constant(mask_list)

dataset = tf.data.Dataset.from_tensor_slices((images_filenames,
                                              masks_filenames))

def process_path(image_path,mask_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_png(img,channels=3)
    img = tf.image.convert_image_dtype(img,tf.float32) #this do the same as dividing by 255 to set the values between 0 and 1 (normalization)
    mask = tf.io.read_file(mask_path)
    mask = tf.image.decode_png(mask,channels=3)
    mask = tf.math.reduce_max(mask,axis=-1,keepdims=True)
    return img , mask

def preprocess(image,mask): 
    input_image = tf.image.resize(image,(96,128),method='nearest')
    input_mask = tf.image.resize(mask,(96,128),method='nearest')
    
    return input_image , input_mask

image_ds = dataset.map(process_path) # apply the preprocces_path function to our dataset
processed_image_ds = image_ds.map(preprocess) # apply the preprocess function to our dataset
</code></pre>
<p>In this case if I map my dataset I will load every image into the dataset. So what differences is it from a normal pandas dataframe?</p>
",39,0,1,4,python;tensorflow;deep-learning;tensorflow-datasets,2022-07-15 11:51:28,2022-07-15 11:51:28,2022-07-15 13:26:13,my question was about how can this system handle a very large dataset  in this case if i map my dataset i will load every image into the dataset  so what differences is it from a normal pandas dataframe ,how does tensorflow pipeline work with data that does not fit in memory ,question system handle large dataset case map dataset load every image dataset differences normal pandas dataframe,tensorflow pipeline work data fit memory,tensorflow pipeline work data fit memoryquestion system handle large dataset case map dataset load every image dataset differences normal pandas dataframe,"['tensorflow', 'pipeline', 'work', 'data', 'fit', 'memoryquestion', 'system', 'handle', 'large', 'dataset', 'case', 'map', 'dataset', 'load', 'every', 'image', 'dataset', 'differences', 'normal', 'pandas', 'dataframe']","['tensorflow', 'pipelin', 'work', 'data', 'fit', 'memoryquest', 'system', 'handl', 'larg', 'dataset', 'case', 'map', 'dataset', 'load', 'everi', 'imag', 'dataset', 'differ', 'normal', 'panda', 'datafram']"
202,216,216,13562070,72991723,Are there advantages of domain specific auto machine learning applications?,"<p>I am currently working in my phd position in production technology. As we are analyzing a lot data, machine learning is quite interesting. I know there are AutoML tools in the market to automate the tasks of an ML Engineer, like preprocessing, model selection or hyperparameter optimization. I would like to know, if there is a sense in developing an AutoML tool specified for a subdomain (Like production technology). I am thinking about the question if certain models and certain hyperparameters work well in a subdomain, which could be proved by experiments, and consequently added to an AutoML application, which does not include other models then.</p>
<p>Do you know any AutoML tools specified for a subdomain? In a lot of papers, they talk about the issue that AutoML needs to adapt to subdomains.</p>
",22,0,-2,2,machine-learning;automl,2022-07-15 12:15:33,2022-07-15 12:15:33,2022-07-15 12:56:57,i am currently working in my phd position in production technology  as we are analyzing a lot data  machine learning is quite interesting  i know there are automl tools in the market to automate the tasks of an ml engineer  like preprocessing  model selection or hyperparameter optimization  i would like to know  if there is a sense in developing an automl tool specified for a subdomain  like production technology   i am thinking about the question if certain models and certain hyperparameters work well in a subdomain  which could be proved by experiments  and consequently added to an automl application  which does not include other models then  do you know any automl tools specified for a subdomain  in a lot of papers  they talk about the issue that automl needs to adapt to subdomains ,are there advantages of domain specific auto machine learning applications ,currently working phd position production technology analyzing lot data machine learning quite interesting know automl tools market automate tasks ml engineer like preprocessing model selection hyperparameter optimization would like know sense developing automl tool specified subdomain like production technology thinking question certain models certain hyperparameters work well subdomain could proved experiments consequently added automl application include models know automl tools specified subdomain lot papers talk issue automl needs adapt subdomains,advantages domain specific auto machine learning applications,advantages domain specific auto machine learning applicationscurrently working phd position production technology analyzing lot data machine learning quite interesting know automl tools market automate tasks ml engineer like preprocessing model selection hyperparameter optimization would like know sense developing automl tool specified subdomain like production technology thinking question certain models certain hyperparameters work well subdomain could proved experiments consequently added automl application include models know automl tools specified subdomain lot papers talk issue automl needs adapt subdomains,"['advantages', 'domain', 'specific', 'auto', 'machine', 'learning', 'applicationscurrently', 'working', 'phd', 'position', 'production', 'technology', 'analyzing', 'lot', 'data', 'machine', 'learning', 'quite', 'interesting', 'know', 'automl', 'tools', 'market', 'automate', 'tasks', 'ml', 'engineer', 'like', 'preprocessing', 'model', 'selection', 'hyperparameter', 'optimization', 'would', 'like', 'know', 'sense', 'developing', 'automl', 'tool', 'specified', 'subdomain', 'like', 'production', 'technology', 'thinking', 'question', 'certain', 'models', 'certain', 'hyperparameters', 'work', 'well', 'subdomain', 'could', 'proved', 'experiments', 'consequently', 'added', 'automl', 'application', 'include', 'models', 'know', 'automl', 'tools', 'specified', 'subdomain', 'lot', 'papers', 'talk', 'issue', 'automl', 'needs', 'adapt', 'subdomains']","['advantag', 'domain', 'specif', 'auto', 'machin', 'learn', 'applicationscurr', 'work', 'phd', 'posit', 'product', 'technolog', 'analyz', 'lot', 'data', 'machin', 'learn', 'quit', 'interest', 'know', 'automl', 'tool', 'market', 'autom', 'task', 'ml', 'engin', 'like', 'preprocess', 'model', 'select', 'hyperparamet', 'optim', 'would', 'like', 'know', 'sens', 'develop', 'automl', 'tool', 'specifi', 'subdomain', 'like', 'product', 'technolog', 'think', 'question', 'certain', 'model', 'certain', 'hyperparamet', 'work', 'well', 'subdomain', 'could', 'prove', 'experi', 'consequ', 'ad', 'automl', 'applic', 'includ', 'model', 'know', 'automl', 'tool', 'specifi', 'subdomain', 'lot', 'paper', 'talk', 'issu', 'automl', 'need', 'adapt', 'subdomain']"
203,217,217,19166370,72991909,How to download Python modules on VS Code,"<p>I'm kinds new in the world of programming and ever since I started learning I got really comfortable working with VS Code due to its environment which is very flexible.</p>
<p>I'm in the process of doing Harvard's CS50 course and I need to download different modules for my code to work. For example I need Pygame for the Minesweeper project. I did download Pygame snippets from the extension tab. However when I try to run the code, it says &quot;No module named 'Pygame'&quot;</p>
<p>I'm also doing the Machine Learning with Python course on Sololearn, which requires Sk-learn.</p>
<p>I also tried using the &quot;pip install&quot; for both but I get the following error:
{
pip : The term 'pip' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the
path is correct and try again.
At line:1 char:1</p>
<ul>
<li>pip install pygame</li>
<li>
<pre><code>  + CategoryInfo          : ObjectNotFound: (pip:String) [], CommandNotFoundException
  + FullyQualifiedErrorId : CommandNotFoundException
</code></pre>
</li>
</ul>
<p>}</p>
<p>I tried everything that I knew but still I couldn't pass this obstacle.</p>
<p>If someone could help me with this, I'd be really thankful.</p>
",44,1,0,2,python;scikit-learn,2022-07-15 12:31:51,2022-07-15 12:31:51,2022-07-15 12:45:06,i m kinds new in the world of programming and ever since i started learning i got really comfortable working with vs code due to its environment which is very flexible  i m in the process of doing harvard s cs course and i need to download different modules for my code to work  for example i need pygame for the minesweeper project  i did download pygame snippets from the extension tab  however when i try to run the code  it says  no module named  pygame   i m also doing the machine learning with python course on sololearn  which requires sk learn    i tried everything that i knew but still i couldn t pass this obstacle  if someone could help me with this  i d be really thankful ,how to download python modules on vs code,kinds world programming ever since started learning got really comfortable working vs code due environment flexible process harvard cs course need download different modules code work example need pygame minesweeper project download pygame snippets extension tab however try run code says module named pygame also machine learning python course sololearn requires sk learn tried everything knew still pass obstacle someone could help really thankful,download python modules vs code,download python modules vs codekinds world programming ever since started learning got really comfortable working vs code due environment flexible process harvard cs course need download different modules code work example need pygame minesweeper project download pygame snippets extension tab however try run code says module named pygame also machine learning python course sololearn requires sk learn tried everything knew still pass obstacle someone could help really thankful,"['download', 'python', 'modules', 'vs', 'codekinds', 'world', 'programming', 'ever', 'since', 'started', 'learning', 'got', 'really', 'comfortable', 'working', 'vs', 'code', 'due', 'environment', 'flexible', 'process', 'harvard', 'cs', 'course', 'need', 'download', 'different', 'modules', 'code', 'work', 'example', 'need', 'pygame', 'minesweeper', 'project', 'download', 'pygame', 'snippets', 'extension', 'tab', 'however', 'try', 'run', 'code', 'says', 'module', 'named', 'pygame', 'also', 'machine', 'learning', 'python', 'course', 'sololearn', 'requires', 'sk', 'learn', 'tried', 'everything', 'knew', 'still', 'pass', 'obstacle', 'someone', 'could', 'help', 'really', 'thankful']","['download', 'python', 'modul', 'vs', 'codekind', 'world', 'program', 'ever', 'sinc', 'start', 'learn', 'got', 'realli', 'comfort', 'work', 'vs', 'code', 'due', 'environ', 'flexibl', 'process', 'harvard', 'cs', 'cours', 'need', 'download', 'differ', 'modul', 'code', 'work', 'exampl', 'need', 'pygam', 'minesweep', 'project', 'download', 'pygam', 'snippet', 'extens', 'tab', 'howev', 'tri', 'run', 'code', 'say', 'modul', 'name', 'pygam', 'also', 'machin', 'learn', 'python', 'cours', 'sololearn', 'requir', 'sk', 'learn', 'tri', 'everyth', 'knew', 'still', 'pass', 'obstacl', 'someon', 'could', 'help', 'realli', 'thank']"
204,218,218,16812115,72991824,Does JShell compiles statements or directly executes them?,"<p>Reading &quot;<a href=""https://www.oreilly.com/library/view/learning-java-5th/9781492056263/"" rel=""nofollow noreferrer"">Learning Java</a>&quot; by O'Reilly and getting familiar with how does the JDK work, I have come upon JShell. However, I'm not quite sure I understand how this <a href=""https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"" rel=""nofollow noreferrer"">REPL</a> works deep down.</p>
<p>Does it compile each statement into bytecode and thereafter executes it with the help of the Java Virtual Machine (the way java source code is built and executed)? Or does it somehow uses some interpreter direclty to execute the statements, avoiding the compiling step?</p>
",25,0,0,2,java;jshell,2022-07-15 12:24:51,2022-07-15 12:24:51,2022-07-15 12:24:51,reading    by o reilly and getting familiar with how does the jdk work  i have come upon jshell  however  i m not quite sure i understand how this  works deep down  does it compile each statement into bytecode and thereafter executes it with the help of the java virtual machine  the way java source code is built and executed   or does it somehow uses some interpreter direclty to execute the statements  avoiding the compiling step ,does jshell compiles statements or directly executes them ,reading reilly getting familiar jdk work come upon jshell however quite sure understand works deep compile statement bytecode thereafter executes help java virtual machine way java source code built executed somehow uses interpreter direclty execute statements avoiding compiling step,jshell compiles statements directly executes,jshell compiles statements directly executesreading reilly getting familiar jdk work come upon jshell however quite sure understand works deep compile statement bytecode thereafter executes help java virtual machine way java source code built executed somehow uses interpreter direclty execute statements avoiding compiling step,"['jshell', 'compiles', 'statements', 'directly', 'executesreading', 'reilly', 'getting', 'familiar', 'jdk', 'work', 'come', 'upon', 'jshell', 'however', 'quite', 'sure', 'understand', 'works', 'deep', 'compile', 'statement', 'bytecode', 'thereafter', 'executes', 'help', 'java', 'virtual', 'machine', 'way', 'java', 'source', 'code', 'built', 'executed', 'somehow', 'uses', 'interpreter', 'direclty', 'execute', 'statements', 'avoiding', 'compiling', 'step']","['jshell', 'compil', 'statement', 'directli', 'executesread', 'reilli', 'get', 'familiar', 'jdk', 'work', 'come', 'upon', 'jshell', 'howev', 'quit', 'sure', 'understand', 'work', 'deep', 'compil', 'statement', 'bytecod', 'thereaft', 'execut', 'help', 'java', 'virtual', 'machin', 'way', 'java', 'sourc', 'code', 'built', 'execut', 'somehow', 'use', 'interpret', 'direclti', 'execut', 'statement', 'avoid', 'compil', 'step']"
205,219,219,16746844,72991609,How can i extract specific information on formal or informal text using machine learning?,"<p>I want to create a program that extract specific information on classified ad</p>
<p>here is an example:</p>
<pre><code>I'm selling my Xiaomi Mi 9T Black
128GB and 6GB of RAM
Snapdragon 855 (you'll run all the games without any worries)
Amoled screen
+1234597812345
</code></pre>
<p>and in output i want this:</p>
<pre><code>Xiaomi: Brand
Mi: Serie
9T: Model
Black: Color
128: internal memory in GB
6: RAM in GB
Snapdragon 805: CPU
Amoled: Screen
1234597812345: phone number
...
</code></pre>
<p>I've already trying a named entity recognition but every tutorial i saw tell me to remove all digits in and special but if i do this, there won't be much information left of the classified ad.</p>
",17,0,-2,3,python;machine-learning;nlp,2022-07-15 12:05:20,2022-07-15 12:05:20,2022-07-15 12:05:20,i want to create a program that extract specific information on classified ad here is an example  and in output i want this  i ve already trying a named entity recognition but every tutorial i saw tell me to remove all digits in and special but if i do this  there won t be much information left of the classified ad ,how can i extract specific information on formal or informal text using machine learning ,want create program extract specific information classified ad example output want already trying named entity recognition every tutorial saw tell remove digits special much information left classified ad,extract specific information formal informal text using machine learning,extract specific information formal informal text using machine learningwant create program extract specific information classified ad example output want already trying named entity recognition every tutorial saw tell remove digits special much information left classified ad,"['extract', 'specific', 'information', 'formal', 'informal', 'text', 'using', 'machine', 'learningwant', 'create', 'program', 'extract', 'specific', 'information', 'classified', 'ad', 'example', 'output', 'want', 'already', 'trying', 'named', 'entity', 'recognition', 'every', 'tutorial', 'saw', 'tell', 'remove', 'digits', 'special', 'much', 'information', 'left', 'classified', 'ad']","['extract', 'specif', 'inform', 'formal', 'inform', 'text', 'use', 'machin', 'learningw', 'creat', 'program', 'extract', 'specif', 'inform', 'classifi', 'ad', 'exampl', 'output', 'want', 'alreadi', 'tri', 'name', 'entiti', 'recognit', 'everi', 'tutori', 'saw', 'tell', 'remov', 'digit', 'special', 'much', 'inform', 'left', 'classifi', 'ad']"
206,220,220,19552165,72986505,How to display the fit method list of parameters?,"<p>I'm taking a course (introduction to machine learning with python), and im using PyCharm for coding. One of the exercises im working on it is the &quot;Iris example&quot; with the KNeighborsClassifer.</p>
<pre><code>In[26]:
knn.fit(X_train, y_train) 

Out[26]:
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
       metric_params=None, n_jobs=1, n_neighbors=1, p=2,
       weights='uniform')***
</code></pre>
<p>the problem here is that I cannot get the same output( the list of parameters) for fit_method() when typing the syntax in [26]. I know that the book is using a different IDE than Pycharm but I get nothing when I run it. Any advice?</p>
",39,1,0,3,python;scikit-learn;knn,2022-07-14 23:48:39,2022-07-14 23:48:39,2022-07-15 11:25:19,i m taking a course  introduction to machine learning with python   and im using pycharm for coding  one of the exercises im working on it is the  iris example  with the kneighborsclassifer  the problem here is that i cannot get the same output  the list of parameters  for fit_method   when typing the syntax in     i know that the book is using a different ide than pycharm but i get nothing when i run it  any advice ,how to display the fit method list of parameters ,taking course introduction machine learning python im using pycharm coding one exercises im working iris example kneighborsclassifer problem cannot get output parameters fit_method typing syntax know book using different ide pycharm get nothing run advice,display fit method parameters,display fit method parameterstaking course introduction machine learning python im using pycharm coding one exercises im working iris example kneighborsclassifer problem cannot get output parameters fit_method typing syntax know book using different ide pycharm get nothing run advice,"['display', 'fit', 'method', 'parameterstaking', 'course', 'introduction', 'machine', 'learning', 'python', 'im', 'using', 'pycharm', 'coding', 'one', 'exercises', 'im', 'working', 'iris', 'example', 'kneighborsclassifer', 'problem', 'can', 'not', 'get', 'output', 'parameters', 'fit_method', 'typing', 'syntax', 'know', 'book', 'using', 'different', 'ide', 'pycharm', 'get', 'nothing', 'run', 'advice']","['display', 'fit', 'method', 'parameterstak', 'cours', 'introduct', 'machin', 'learn', 'python', 'im', 'use', 'pycharm', 'code', 'one', 'exercis', 'im', 'work', 'iri', 'exampl', 'kneighborsclassif', 'problem', 'can', 'not', 'get', 'output', 'paramet', 'fit_method', 'type', 'syntax', 'know', 'book', 'use', 'differ', 'ide', 'pycharm', 'get', 'noth', 'run', 'advic']"
207,221,221,11267281,72612782,Python SDK v2 for Azure Machine Learning SDK (preview) - how to retrieve workspace from WorkspaceOperations,"<p>Im following <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk"" rel=""nofollow noreferrer"">this article</a> to create ML pipelines with the new SDK.</p>
<p>So I started by loading the first class</p>
<pre><code>from azure.ai.ml import MLClient
</code></pre>
<p>and then I used it to authenticated on my workspace</p>
<pre><code>ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group_name,
    workspace_name=&quot; mmAmlsWksp01&quot;,
)
</code></pre>
<p>However, I cant understand how I can retrieve the objects it refers to. For example, it contains a workspaces member, but if I run</p>
<pre><code>ml_client.workspaces[&quot;mmAmlsWksp01&quot;] 
</code></pre>
<p>, I get the error <em><strong>'WorkspaceOperations' object is not subscriptable</strong></em>.</p>
<p>So I tried to run</p>
<pre><code>for w in ml_client.workspaces.list():
    print(w)
</code></pre>
<p>and it returns the workspace details (name, displayName, id) for a SINGLE workspace, but not the workspace object.</p>
<p>In fact, the ml_client.workspaces object is a</p>
<p>&lt;azure.ai.ml._operations.workspace_operations.WorkspaceOperations at 0x7f3526e45d60&gt;</p>
<p>, but I dont want a WorkspaceOperation, I want the Workspace itself. How can I retrieve it?</p>
",78,0,0,3,python-3.x;azure;azure-machine-learning-service,2022-06-14 10:05:44,2022-06-14 10:05:44,2022-07-15 11:14:55,i m following  to create ml pipelines with the new sdk  so i started by loading the first class and then i used it to authenticated on my workspace however  i can t understand how i can retrieve the objects it refers to  for example  it contains a  workspaces  member  but if i run   i get the error   workspaceoperations  object is not subscriptable   so i tried to run and it returns the workspace details  name  displayname  id   for a single workspace  but not the workspace object  in fact  the ml_client workspaces object is a  lt azure ai ml _operations workspace_operations workspaceoperations at xfed gt    but i don t want a workspaceoperation  i want the workspace itself  how can i retrieve it ,python sdk v for azure machine learning sdk  preview    how to retrieve workspace from workspaceoperations,following create ml pipelines sdk started loading first class used authenticated workspace however understand retrieve objects refers example contains workspaces member run get error workspaceoperations object subscriptable tried run returns workspace details name displayname id single workspace workspace object fact ml_client workspaces object lt azure ai ml _operations workspace_operations workspaceoperations xfed gt want workspaceoperation want workspace retrieve,python sdk v azure machine learning sdk preview retrieve workspace workspaceoperations,python sdk v azure machine learning sdk preview retrieve workspace workspaceoperationsfollowing create ml pipelines sdk started loading first class used authenticated workspace however understand retrieve objects refers example contains workspaces member run get error workspaceoperations object subscriptable tried run returns workspace details name displayname id single workspace workspace object fact ml_client workspaces object lt azure ai ml _operations workspace_operations workspaceoperations xfed gt want workspaceoperation want workspace retrieve,"['python', 'sdk', 'v', 'azure', 'machine', 'learning', 'sdk', 'preview', 'retrieve', 'workspace', 'workspaceoperationsfollowing', 'create', 'ml', 'pipelines', 'sdk', 'started', 'loading', 'first', 'class', 'used', 'authenticated', 'workspace', 'however', 'understand', 'retrieve', 'objects', 'refers', 'example', 'contains', 'workspaces', 'member', 'run', 'get', 'error', 'workspaceoperations', 'object', 'subscriptable', 'tried', 'run', 'returns', 'workspace', 'details', 'name', 'displayname', 'id', 'single', 'workspace', 'workspace', 'object', 'fact', 'ml_client', 'workspaces', 'object', 'lt', 'azure', 'ai', 'ml', '_operations', 'workspace_operations', 'workspaceoperations', 'xfed', 'gt', 'want', 'workspaceoperation', 'want', 'workspace', 'retrieve']","['python', 'sdk', 'v', 'azur', 'machin', 'learn', 'sdk', 'preview', 'retriev', 'workspac', 'workspaceoperationsfollow', 'creat', 'ml', 'pipelin', 'sdk', 'start', 'load', 'first', 'class', 'use', 'authent', 'workspac', 'howev', 'understand', 'retriev', 'object', 'refer', 'exampl', 'contain', 'workspac', 'member', 'run', 'get', 'error', 'workspaceoper', 'object', 'subscript', 'tri', 'run', 'return', 'workspac', 'detail', 'name', 'displaynam', 'id', 'singl', 'workspac', 'workspac', 'object', 'fact', 'ml_client', 'workspac', 'object', 'lt', 'azur', 'ai', 'ml', '_oper', 'workspace_oper', 'workspaceoper', 'xfed', 'gt', 'want', 'workspaceoper', 'want', 'workspac', 'retriev']"
208,222,222,3879858,43049545,Python: Check if dataframe column contain string type,"<p>I want check if columns in a dataframe consists of strings so I can label them with numbers for machine learning purposes. Some columns consists of numbers, I dont want to change them. Columns example can be seen below:</p>

<pre><code>TRAIN FEATURES
  Age              Level  
  32.0              Silver      
  61.0              Silver  
  66.0              Silver      
  36.0              Gold      
  20.0              Silver     
  29.0              Silver     
  46.0              Silver  
  27.0              Silver      
</code></pre>

<p>Thank you=)</p>
",54036,8,25,2,python;dataframe,2017-03-27 17:44:56,2017-03-27 17:44:56,2022-07-15 08:07:24,i want check if columns in a dataframe consists of strings so i can label them with numbers for machine learning purposes  some columns consists of numbers  i dont want to change them  columns example can be seen below  thank you  ,python  check if dataframe column contain string type,want check columns dataframe consists strings label numbers machine learning purposes columns consists numbers dont want change columns example seen thank,python check dataframe column contain string type,python check dataframe column contain string typewant check columns dataframe consists strings label numbers machine learning purposes columns consists numbers dont want change columns example seen thank,"['python', 'check', 'dataframe', 'column', 'contain', 'string', 'typewant', 'check', 'columns', 'dataframe', 'consists', 'strings', 'label', 'numbers', 'machine', 'learning', 'purposes', 'columns', 'consists', 'numbers', 'dont', 'want', 'change', 'columns', 'example', 'seen', 'thank']","['python', 'check', 'datafram', 'column', 'contain', 'string', 'typew', 'check', 'column', 'datafram', 'consist', 'string', 'label', 'number', 'machin', 'learn', 'purpos', 'column', 'consist', 'number', 'dont', 'want', 'chang', 'column', 'exampl', 'seen', 'thank']"
209,223,223,3098629,72982696,How to train data of different lengths in machine learning?,"<p>I am analyzing the text of some literary works and I want to look at the distance between certain words in the text. Specifically, I am looking for parallelism.</p>
<p>Since I cant know the specific number of tokens in a text I cant simply put all words in the text in the training data because it would not be uniform across all training data.</p>
<p>For example, the text:</p>
<p>I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.&quot;</p>
<p>Is not the same text length as</p>
<p>&quot;My fellow Americans, ask not what your country can do for you, ask what you can do for your country.&quot;</p>
<p>So therefore I could not columns out of each word and then assign the distance in a row because the lengths would be different.</p>
<p>How could I go about representing this in training data?  I was under the assumption that training data had to be the same type and length.</p>
",24,1,0,2,data-science;training-data,2022-07-14 18:03:22,2022-07-14 18:03:22,2022-07-15 03:35:54,i am analyzing the text of some literary works and i want to look at the distance between certain words in the text  specifically  i am looking for parallelism  since i can t know the specific number of tokens in a text i can t simply put all words in the text in the training data because it would not be uniform across all training data  for example  the text   i have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character  i have a dream today   is not the same text length as  my fellow americans  ask not what your country can do for you  ask what you can do for your country   so therefore i could not columns out of each word and then assign the distance in a row because the lengths would be different  how could i go about representing this in training data   i was under the assumption that training data had to be the same type and length ,how to train data of different lengths in machine learning ,analyzing text literary works want look distance certain text specifically looking parallelism since know specific number tokens text simply put text training data would uniform across training data example text dream four little children one day live nation judged color skin content character dream today text length fellow americans ask country ask country therefore could columns word assign distance row lengths would different could go representing training data assumption training data type length,train data different lengths machine learning,train data different lengths machine learninganalyzing text literary works want look distance certain text specifically looking parallelism since know specific number tokens text simply put text training data would uniform across training data example text dream four little children one day live nation judged color skin content character dream today text length fellow americans ask country ask country therefore could columns word assign distance row lengths would different could go representing training data assumption training data type length,"['train', 'data', 'different', 'lengths', 'machine', 'learninganalyzing', 'text', 'literary', 'works', 'want', 'look', 'distance', 'certain', 'text', 'specifically', 'looking', 'parallelism', 'since', 'know', 'specific', 'number', 'tokens', 'text', 'simply', 'put', 'text', 'training', 'data', 'would', 'uniform', 'across', 'training', 'data', 'example', 'text', 'dream', 'four', 'little', 'children', 'one', 'day', 'live', 'nation', 'judged', 'color', 'skin', 'content', 'character', 'dream', 'today', 'text', 'length', 'fellow', 'americans', 'ask', 'country', 'ask', 'country', 'therefore', 'could', 'columns', 'word', 'assign', 'distance', 'row', 'lengths', 'would', 'different', 'could', 'go', 'representing', 'training', 'data', 'assumption', 'training', 'data', 'type', 'length']","['train', 'data', 'differ', 'length', 'machin', 'learninganalyz', 'text', 'literari', 'work', 'want', 'look', 'distanc', 'certain', 'text', 'specif', 'look', 'parallel', 'sinc', 'know', 'specif', 'number', 'token', 'text', 'simpli', 'put', 'text', 'train', 'data', 'would', 'uniform', 'across', 'train', 'data', 'exampl', 'text', 'dream', 'four', 'littl', 'children', 'one', 'day', 'live', 'nation', 'judg', 'color', 'skin', 'content', 'charact', 'dream', 'today', 'text', 'length', 'fellow', 'american', 'ask', 'countri', 'ask', 'countri', 'therefor', 'could', 'column', 'word', 'assign', 'distanc', 'row', 'length', 'would', 'differ', 'could', 'go', 'repres', 'train', 'data', 'assumpt', 'train', 'data', 'type', 'length']"
210,224,224,15637303,72987647,Warning: File_get_contents(Http://127.0.0.1:5000/): Failed To Open Stream: HTTP Request Failed! HTTP/1.0 405 METHOD NOT ALLOWED,"<p>I m trying to call a python flask api in a php website so I used the following code and got the warning mentioned in the title as a result</p>
<pre><code>function bot_msg($user_msg){
    $url = 'http://127.0.0.1:5000/';

    $data = array('msg' =&gt; $user_msg);
    $options = array(
        'http' =&gt; array(
            'header'  =&gt; &quot;Content-type: application/x-www-form-urlencoded\r\n&quot;,
            'method'  =&gt; 'POST',
            'content' =&gt; http_build_query($data)
        )
    );
      $context  = stream_context_create($options);
      $result = file_get_contents($url, false, $context);  $result = json_decode($result, true);
    return $result; }
</code></pre>
<p>python code</p>
<pre><code>@app.route(&quot;/&quot;)
def get_bot_response():
    userText = request.args.get('msg')
    #chatbot_resp=chatbot_response(userText)
    return chatbot_response(userText)
</code></pre>
<p>in chatbot_response I used a machine learning to predict a response</p>
",29,0,-3,2,python;flask,2022-07-15 02:32:25,2022-07-15 02:32:25,2022-07-15 02:45:00,i m trying to call a python flask api in a php website so i used the following code and got the warning mentioned in the title as a result python code in chatbot_response i used a machine learning to predict a response,warning  file_get_contents http           failed to open stream  http request failed  http    method not allowed,trying call python flask api php website used following code got warning mentioned title result python code chatbot_response used machine learning predict response,warning file_get_contents http failed open stream http request failed http method allowed,warning file_get_contents http failed open stream http request failed http method allowedtrying call python flask api php website used following code got warning mentioned title result python code chatbot_response used machine learning predict response,"['warning', 'file_get_contents', 'http', 'failed', 'open', 'stream', 'http', 'request', 'failed', 'http', 'method', 'allowedtrying', 'call', 'python', 'flask', 'api', 'php', 'website', 'used', 'following', 'code', 'got', 'warning', 'mentioned', 'title', 'result', 'python', 'code', 'chatbot_response', 'used', 'machine', 'learning', 'predict', 'response']","['warn', 'file_get_cont', 'http', 'fail', 'open', 'stream', 'http', 'request', 'fail', 'http', 'method', 'allowedtri', 'call', 'python', 'flask', 'api', 'php', 'websit', 'use', 'follow', 'code', 'got', 'warn', 'mention', 'titl', 'result', 'python', 'code', 'chatbot_respons', 'use', 'machin', 'learn', 'predict', 'respons']"
211,225,225,19549708,72981396,I am getting many errors including tcl error in my GUI code in python,"<p>I am working on creating a GUI for digit recognizer. I had completed all the machine learning part successfully however i am getting many errors in my GUI code for it. Here is my code for GUI of digit recognizer:</p>
<pre><code>from keras.models import load_model
from tkinter import *
import tkinter  as tk
import win32gui
from PIL import ImageGrab, Image
import numpy as np
model = load_model('mnist.h5')
def predict_digit(img):
    #resize image to 28x28 pixels
    img = img.resize((28,28))
    #convert rgb to grayscale
    img = img.convert('L')
    img = np.array(img)
    #reshaping for model normalization
    img = img.reshape(1,28,28,1)
    img = img/255.0
    #predicting the class
    res = model.predict([img])[0]
    return np.argmax(res), max(res)
class App(tk.Tk):
    def __init__(self):
        tk.Tk.__init__(self)
        self.x = self.y = 0
        # Creating elements
        self.canvas = tk.Canvas(self, width=200, height=200, bg = &quot;black&quot;, cursor=&quot;cross&quot;)
        self.label = tk.Label(self, text=&quot;Analyzing..&quot;, font=(&quot;Helvetica&quot;, 48))
        self.classify_btn = tk.Button(self, text = &quot;Searched&quot;, command = self.classify_handwriting) 
        self.button_clear = tk.Button(self, text = &quot;Dlt&quot;, command = self.clear_all)
        # Grid structure
        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )
        self.label.grid(row=0, column=1,pady=2, padx=2)
        self.classify_btn.grid(row=1, column=1, pady=2, padx=2)
        self.button_clear.grid(row=1, column=0, pady=2)
        #self.canvas.bind(&quot;&quot;, self.start_pos)
        self.canvas.bind(&quot;&quot;, self.draw_lines)
    def clear_all(self):
        self.canvas.delete(&quot;all&quot;)
    def classify_handwriting(self):
        Hd = self.canvas.winfo_id() # to fetch the handle of the canvas
        rect = win32gui.GetWindowRect(Hd) # to fetch the edges of the canvas
        im = ImageGrab.grab(rect)
        digit, acc = predict_digit(im)
        self.label.configure(text= str(digit)+', '+ str(int(acc*100))+'%')
    def draw_lines(slf, event):
        slf.x = event.x
        slf.y = event.y
        r=8
        slf.canvas.create_oval(slf.x-r, slf.y-r, slf.x + r, slf.y + r, fill='black')
app = App()
mainloop()
</code></pre>
<p>list of errors in my code:</p>
<p><a href=""https://i.stack.imgur.com/o8FuM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8FuM.png"" alt=""enter image description here"" /></a></p>
<p>Please provide me the right code.</p>
",31,0,-2,3,python;machine-learning;tkinter,2022-07-14 16:32:14,2022-07-14 16:32:14,2022-07-15 02:14:04,i am working on creating a gui for digit recognizer  i had completed all the machine learning part successfully however i am getting many errors in my gui code for it  here is my code for gui of digit recognizer  list of errors in my code   please provide me the right code ,i am getting many errors including tcl error in my gui code in python,working creating gui digit recognizer completed machine learning part successfully however getting many errors gui code code gui digit recognizer errors code please provide right code,getting many errors including tcl error gui code python,getting many errors including tcl error gui code pythonworking creating gui digit recognizer completed machine learning part successfully however getting many errors gui code code gui digit recognizer errors code please provide right code,"['getting', 'many', 'errors', 'including', 'tcl', 'error', 'gui', 'code', 'pythonworking', 'creating', 'gui', 'digit', 'recognizer', 'completed', 'machine', 'learning', 'part', 'successfully', 'however', 'getting', 'many', 'errors', 'gui', 'code', 'code', 'gui', 'digit', 'recognizer', 'errors', 'code', 'please', 'provide', 'right', 'code']","['get', 'mani', 'error', 'includ', 'tcl', 'error', 'gui', 'code', 'pythonwork', 'creat', 'gui', 'digit', 'recogn', 'complet', 'machin', 'learn', 'part', 'success', 'howev', 'get', 'mani', 'error', 'gui', 'code', 'code', 'gui', 'digit', 'recogn', 'error', 'code', 'pleas', 'provid', 'right', 'code']"
212,226,226,7798822,72987174,How to loop through a dictionary values and get all numbers between two numbers?,"<p>I have a dictionary storing machine learning prediction probabilities. The keys of the dictionary are the indices of predicted instances and the values are lists containing class probabilities. I want to create a new dictionary to store all class probabilities that are between two numbers (0.48 and 0.55). With the code below I did not get the expected result as it seems my for loop does not iterate through all the values of my dictionary.</p>
<pre><code>d={348: [0,0,0,0.5,0,0,0.49], 349: [0,0,0.3,0.48,0.49,0.55,0.9], 350: 
[0,0,0.3,0.45,0.0,0.52,0.8]} 

dt={}
for i in d:
  for index, value in enumerate(d[i]):
    if value &gt;= 0.45 and value &lt;= 0.55:
      dt={i: {(str(index + 1)): [value]} }
print(dt) 
</code></pre>
<p>My output is now is only:</p>
<pre><code>{350: {'6': 0.52}}. 
</code></pre>
<p>However, I want to get all the numbers from all the values list between 0.48 and 0.55 and get the keys associated with the numbers selected as well as the indices of the numbers in the values list. My desired output is below:</p>
<pre><code>{348: {'4': 0.5, '7': 0,49}, 349: {'4': 0.48, '5': 0.49, '6', 0.55}, 350: {'4': 0.45, 
'6': 0.52}
</code></pre>
",42,3,1,3,python;list;dictionary,2022-07-15 01:16:00,2022-07-15 01:16:00,2022-07-15 01:53:40,i have a dictionary storing machine learning prediction probabilities  the keys of the dictionary are the indices of predicted instances and the values are lists containing class probabilities  i want to create a new dictionary to store all class probabilities that are between two numbers    and     with the code below i did not get the expected result as it seems my for loop does not iterate through all the values of my dictionary  my output is now is only  however  i want to get all the numbers from all the values list between   and   and get the keys associated with the numbers selected as well as the indices of the numbers in the values list  my desired output is below ,how to loop through a dictionary values and get all numbers between two numbers ,dictionary storing machine learning prediction probabilities keys dictionary indices predicted instances values lists containing class probabilities want create dictionary store class probabilities two numbers code get expected result seems loop iterate values dictionary output however want get numbers values get keys associated numbers selected well indices numbers values desired output,loop dictionary values get numbers two numbers,loop dictionary values get numbers two numbersdictionary storing machine learning prediction probabilities keys dictionary indices predicted instances values lists containing class probabilities want create dictionary store class probabilities two numbers code get expected result seems loop iterate values dictionary output however want get numbers values get keys associated numbers selected well indices numbers values desired output,"['loop', 'dictionary', 'values', 'get', 'numbers', 'two', 'numbersdictionary', 'storing', 'machine', 'learning', 'prediction', 'probabilities', 'keys', 'dictionary', 'indices', 'predicted', 'instances', 'values', 'lists', 'containing', 'class', 'probabilities', 'want', 'create', 'dictionary', 'store', 'class', 'probabilities', 'two', 'numbers', 'code', 'get', 'expected', 'result', 'seems', 'loop', 'iterate', 'values', 'dictionary', 'output', 'however', 'want', 'get', 'numbers', 'values', 'get', 'keys', 'associated', 'numbers', 'selected', 'well', 'indices', 'numbers', 'values', 'desired', 'output']","['loop', 'dictionari', 'valu', 'get', 'number', 'two', 'numbersdictionari', 'store', 'machin', 'learn', 'predict', 'probabl', 'key', 'dictionari', 'indic', 'predict', 'instanc', 'valu', 'list', 'contain', 'class', 'probabl', 'want', 'creat', 'dictionari', 'store', 'class', 'probabl', 'two', 'number', 'code', 'get', 'expect', 'result', 'seem', 'loop', 'iter', 'valu', 'dictionari', 'output', 'howev', 'want', 'get', 'number', 'valu', 'get', 'key', 'associ', 'number', 'select', 'well', 'indic', 'number', 'valu', 'desir', 'output']"
213,227,227,19528421,72982603,Problem with conditional rendering with React,"<p>I don't really have an error but I'm not getting the desired outcome. I have a Flask API as my backend. It returns JSON data, this data is then passed to the React frontend to be displayed after certain actions such as a button click.</p>
<p>The Flask backend is working fine, the data is being sent to the React frontend and I can use <code>console.log(data)</code> to see it in the console.</p>
<p>The issue is I tried doing something to make it so that while the data is being fetched from the API, I want a message such as &quot;Loading...&quot; to be displayed.</p>
<p>Here's what I did in that regard.</p>
<pre class=""lang-jsx prettyprint-override""><code>const [data, setData] = useState([{}])

useEffect(() =&gt;{
  fetch(&quot;/details&quot;).then(
    res =&gt; res.json()
  ).then(
    data =&gt; {
      setData(data)
      console.log(data)
    }
  )
}, [])
</code></pre>
<p>That <code>console.log(data)</code> does show my the JSON response in the console.</p>
<p>Then I do  this for the display.</p>
<pre class=""lang-jsx prettyprint-override""><code>return (
  &lt;div&gt;
    ...
    &lt;div&gt;
      {(typeof data.members === 'undefined') ? (
        &lt;p&gt;Loading...&lt;/p&gt;
      ) : (
        data.members.map((member, i) =&gt; (
          &lt;p key={i}&gt;{member}&lt;/p&gt;
        ))
      )}
    &lt;/div&gt;
  &lt;/div&gt;
);
</code></pre>
<p>This is supposed to display &quot;Loading...&quot; while the data is being fetched and then display the data. But it keeps displaying &quot;Loading...&quot; even though my data was fetched and it's never displayed.</p>
<p>I tried what emrich said and got this error <code>TypeError: Cannot read properties of undefined (reading 'map')</code></p>
<pre><code>App
D:/React/Candetect/src/App.jsx:32
  29 | &lt;FileList files={files} removeFile={removeFile} /&gt;
  30 | &lt;div&gt;
  31 | {(data.length === 0) ? (
&gt; 32 |   &lt;p&gt;Loading...&lt;/p&gt;
     | ^  33 | ) : (
  34 |   data.members.map((member, i) =&gt; (
  35 |     &lt;p key={i}&gt;{member}&lt;/p&gt;

D:/React/Candetect/src/App.jsx:15
  12 |   res =&gt; res.json()
  13 | ).then(
  14 |   data =&gt; {
&gt; 15 |     setData(data)
     | ^  16 |     console.log(data)
  17 |   }
  18 | )
</code></pre>
<p>This is the object from the backend</p>
<pre><code>{
    &quot;Information&quot;: [
        {
            &quot;college_name&quot;: null,
            &quot;company_names&quot;: [
                &quot;Marathwada Mitra Mandals College of Engineering&quot;
            ],
            &quot;degree&quot;: [
                &quot;B.E. IN COMPUTER ENGINEERING&quot;
            ],
            &quot;designation&quot;: [
                &quot;Machine Learning&quot;,
                &quot;TECHNICAL CONTENT WRITER&quot;,
                &quot;Schlumberger\nDATA ENGINEER&quot;
            ],
            &quot;email&quot;: &quot;omkarpathak27@gmail.com&quot;,
            &quot;experience&quot;: [
                &quot;Schlumberger&quot;,
                &quot;DATA ENGINEER&quot;,
                &quot;July 2018 - Present&quot;,
                &quot; Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;brought migration time to 1.5 hours from 9 hours without any manual intervention&quot;,
                &quot; Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business&quot;,
                &quot;units&quot;,
                &quot; Working with L3 developer team to ensure the discussed Scrum PBIs are delivered on time for data ingestions&quot;,
                &quot; Planning and Executing QA and Production Release Cycle activities&quot;,
                &quot;Truso&quot;,
                &quot;FULL STACK DEVELOPER INTERN&quot;,
                &quot; Created RESTful apis&quot;,
                &quot; Tried my hands on Angular 5/6&quot;,
                &quot; Was responsible for Django backend development&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;June 2018 - July 2018&quot;,
                &quot;Propeluss&quot;,
                &quot;DATA ENGINEERING INTERN&quot;,
                &quot; Wrote various automation scripts to scrape data from various websites.&quot;,
                &quot; Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;October 2017 - January 2018&quot;,
                &quot;extraction algorithms and applying Machine Learning to classify these articles.&quot;,
                &quot; Also applied KNN with LSA for extracting relevant tags for various startups based on their works.&quot;,
                &quot;GeeksForGeeks&quot;,
                &quot;TECHNICAL CONTENT WRITER&quot;,
                &quot; Published 4 articles for the topics such as Data Structures and Algorithms and Python&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;July 2017 - September 2017&quot;,
                &quot;Softtestlab Technologies&quot;,
                &quot;WEB DEVELOPER INTERN&quot;,
                &quot; Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes&quot;,
                &quot; Worked on a live project for creating closure reports using PHP and Excel&quot;
            ],
            &quot;mobile_number&quot;: &quot;8087996634&quot;,
            &quot;name&quot;: &quot;Omkar Pathak&quot;,
            &quot;no_of_pages&quot;: 3,
            &quot;skills&quot;: [
                &quot;Python&quot;,
                &quot;Cloud&quot;,
                &quot;Github&quot;,
                &quot;Django&quot;,
                &quot;Writing&quot;,
                &quot;Unix&quot;,
                &quot;Algorithms&quot;,
                &quot;C&quot;,
                &quot;Windows&quot;,
                &quot;Training&quot;,
                &quot;C++&quot;,
                &quot;Flask&quot;,
                &quot;Scrum&quot;,
                &quot;Testing&quot;,
                &quot;Reports&quot;,
                &quot;Programming&quot;,
                &quot;Operating systems&quot;,
                &quot;Automation&quot;,
                &quot;Engineering&quot;,
                &quot;Html&quot;,
                &quot;Css&quot;,
                &quot;Analytics&quot;,
                &quot;Opencv&quot;,
                &quot;Content&quot;,
                &quot;Excel&quot;,
                &quot;Mysql&quot;,
                &quot;Migration&quot;,
                &quot;Api&quot;,
                &quot;Parser&quot;,
                &quot;Machine learning&quot;,
                &quot;System&quot;,
                &quot;Php&quot;,
                &quot;Apis&quot;,
                &quot;Auditing&quot;,
                &quot;Technical&quot;,
                &quot;Photography&quot;,
                &quot;Shell&quot;,
                &quot;Linux&quot;,
                &quot;Security&quot;,
                &quot;Website&quot;,
                &quot;Javascript&quot;
            ],
            &quot;total_experience&quot;: 4.5
        }
    ]
} 
</code></pre>
",52,2,0,2,javascript;reactjs,2022-07-14 17:56:50,2022-07-14 17:56:50,2022-07-15 01:36:56,i don t really have an error but i m not getting the desired outcome  i have a flask api as my backend  it returns json data  this data is then passed to the react frontend to be displayed after certain actions such as a button click  the flask backend is working fine  the data is being sent to the react frontend and i can use console log data  to see it in the console  the issue is i tried doing something to make it so that while the data is being fetched from the api  i want a message such as  loading     to be displayed  here s what i did in that regard  that console log data  does show my the json response in the console  then i do  this for the display  this is supposed to display  loading     while the data is being fetched and then display the data  but it keeps displaying  loading     even though my data was fetched and it s never displayed  i tried what emrich said and got this error typeerror  cannot read properties of undefined  reading  map   this is the object from the backend,problem with conditional rendering with react,really error getting desired outcome flask api backend returns json data data passed react frontend displayed certain actions button click flask backend working fine data sent react frontend use console log data see console issue tried something make data fetched api want message loading displayed regard console log data show json response console display supposed display loading data fetched display data keeps displaying loading even though data fetched never displayed tried emrich said got error typeerror cannot read properties undefined reading map object backend,problem conditional rendering react,problem conditional rendering reactreally error getting desired outcome flask api backend returns json data data passed react frontend displayed certain actions button click flask backend working fine data sent react frontend use console log data see console issue tried something make data fetched api want message loading displayed regard console log data show json response console display supposed display loading data fetched display data keeps displaying loading even though data fetched never displayed tried emrich said got error typeerror cannot read properties undefined reading map object backend,"['problem', 'conditional', 'rendering', 'reactreally', 'error', 'getting', 'desired', 'outcome', 'flask', 'api', 'backend', 'returns', 'json', 'data', 'data', 'passed', 'react', 'frontend', 'displayed', 'certain', 'actions', 'button', 'click', 'flask', 'backend', 'working', 'fine', 'data', 'sent', 'react', 'frontend', 'use', 'console', 'log', 'data', 'see', 'console', 'issue', 'tried', 'something', 'make', 'data', 'fetched', 'api', 'want', 'message', 'loading', 'displayed', 'regard', 'console', 'log', 'data', 'show', 'json', 'response', 'console', 'display', 'supposed', 'display', 'loading', 'data', 'fetched', 'display', 'data', 'keeps', 'displaying', 'loading', 'even', 'though', 'data', 'fetched', 'never', 'displayed', 'tried', 'emrich', 'said', 'got', 'error', 'typeerror', 'can', 'not', 'read', 'properties', 'undefined', 'reading', 'map', 'object', 'backend']","['problem', 'condit', 'render', 'reactreal', 'error', 'get', 'desir', 'outcom', 'flask', 'api', 'backend', 'return', 'json', 'data', 'data', 'pass', 'react', 'frontend', 'display', 'certain', 'action', 'button', 'click', 'flask', 'backend', 'work', 'fine', 'data', 'sent', 'react', 'frontend', 'use', 'consol', 'log', 'data', 'see', 'consol', 'issu', 'tri', 'someth', 'make', 'data', 'fetch', 'api', 'want', 'messag', 'load', 'display', 'regard', 'consol', 'log', 'data', 'show', 'json', 'respons', 'consol', 'display', 'suppos', 'display', 'load', 'data', 'fetch', 'display', 'data', 'keep', 'display', 'load', 'even', 'though', 'data', 'fetch', 'never', 'display', 'tri', 'emrich', 'said', 'got', 'error', 'typeerror', 'can', 'not', 'read', 'properti', 'undefin', 'read', 'map', 'object', 'backend']"
214,228,228,19550109,72982291,Machine Learning - How does a Single Perceptron learn?,"<p>How does a Perceptron learn?</p>
<p>To be more specific:
In university we had following exercise:
<a href=""https://i.stack.imgur.com/gZKE2.png"" rel=""nofollow noreferrer"">Perceptron exercicse</a></p>
<p>The solution was kind of easy:
After the first Data-Point the weights were (0, -4, -3, 6) after the second Data-Point (1,-2, -5, 3) and so on. The algorithm we used to update the weights was (in Pseudocode):</p>
<p>If Act.Fct(f(x)) != y:</p>
<p>w_new = w_old + y * x</p>
<p>Else: Do nothing.</p>
<p>[Act.Fct = Activation Function]</p>
<p>What I dont understand at this point is: What is with the &quot;Error-Function&quot; (or Loss-Function)? Does a single layer Perceptron dont use this function to update the weights? Because in this algorith we havent calculatet any Error?</p>
<p>I know that in Neuronal-Networks (that are just a bunch of chained Perceptrons?) the Error-Function is used for backpropagation. Im just confused by this exercise, that it is not used for the single layer perceptron as well?</p>
",29,0,0,2,machine-learning;perceptron,2022-07-14 17:35:45,2022-07-14 17:35:45,2022-07-14 23:01:12,how does a perceptron learn  if act fct f x      y  w_new   w_old   y   x else  do nothing   act fct   activation function  what i dont understand at this point is  what is with the  error function   or loss function   does a single layer perceptron dont use this function to update the weights  because in this algorith we havent calculatet any error  i know that in neuronal networks  that are just a bunch of chained perceptrons   the error function is used for backpropagation  im just confused by this exercise  that it is not used for the single layer perceptron as well ,machine learning   how does a single perceptron learn ,perceptron learn act fct f x w_new w_old x else nothing act fct activation function dont understand point error function loss function single layer perceptron dont use function update weights algorith havent calculatet error know neuronal networks bunch chained perceptrons error function used backpropagation im confused exercise used single layer perceptron well,machine learning single perceptron learn,machine learning single perceptron learnperceptron learn act fct f x w_new w_old x else nothing act fct activation function dont understand point error function loss function single layer perceptron dont use function update weights algorith havent calculatet error know neuronal networks bunch chained perceptrons error function used backpropagation im confused exercise used single layer perceptron well,"['machine', 'learning', 'single', 'perceptron', 'learnperceptron', 'learn', 'act', 'fct', 'f', 'x', 'w_new', 'w_old', 'x', 'else', 'nothing', 'act', 'fct', 'activation', 'function', 'dont', 'understand', 'point', 'error', 'function', 'loss', 'function', 'single', 'layer', 'perceptron', 'dont', 'use', 'function', 'update', 'weights', 'algorith', 'havent', 'calculatet', 'error', 'know', 'neuronal', 'networks', 'bunch', 'chained', 'perceptrons', 'error', 'function', 'used', 'backpropagation', 'im', 'confused', 'exercise', 'used', 'single', 'layer', 'perceptron', 'well']","['machin', 'learn', 'singl', 'perceptron', 'learnperceptron', 'learn', 'act', 'fct', 'f', 'x', 'w_new', 'w_old', 'x', 'els', 'noth', 'act', 'fct', 'activ', 'function', 'dont', 'understand', 'point', 'error', 'function', 'loss', 'function', 'singl', 'layer', 'perceptron', 'dont', 'use', 'function', 'updat', 'weight', 'algorith', 'havent', 'calculatet', 'error', 'know', 'neuron', 'network', 'bunch', 'chain', 'perceptron', 'error', 'function', 'use', 'backpropag', 'im', 'confus', 'exercis', 'use', 'singl', 'layer', 'perceptron', 'well']"
215,229,229,18670887,72985526,Loading ONNX model asynchronously runs forever or throws Null Reference,"<p>I am currently building a Xamarin app for UWP. I am integrating the Windows.AI.MachineLearning/Microsoft.ML.Tensorflow packages so that I can use an .onnx format TensorFlow model trained in Python within my C# Xamarin app. I have verified using the ONNX python package that my .onnx file is a valid model, so that is not an issue.</p>
<p>The function is question is called LoadModelAsync(). I get two errors depending on how I try to fix it. Either I get a null reference exception,
<strong>System.NullReferenceException: 'Object reference not set to an instance of an object.'</strong>, or the function runs forever and never returns. I know this because I have a string that should update on my XAML page once the function completes, and it never shows up.</p>
<p>I am following closely this guide: <a href=""https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-started-uwp"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-started-uwp</a>
and this code: <a href=""https://github.com/Azure-Samples/cognitive-services-onnx-customvision-sample/blob/master/MainPage.xaml.cs"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/cognitive-services-onnx-customvision-sample/blob/master/MainPage.xaml.cs</a></p>
<p>Here is the function where the error occurs, LoadModelAsync. I am calling an .onnx model to use it to make a prediction. Here is how the model, called <em>modelGen</em>, is initialize and later called. The function It is used exactly the same as the code in the first link:</p>
<pre><code>private TempModel modelGen = null;

...

private async Task LoadModelAsync() {
        // Load a machine learning model
        StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx:///Assets/model.onnx&quot;));
        modelGen = await TempModel.CreateFromStreamAsync(modelFile as IRandomAccessStreamReference);
    }

...

async Task StartInventoryAsync() {
        ...

        _activitystatus = &quot;Before load model...&quot;; RaisePropertyChanged(() =&gt; activitystatus);

        if (_model == null) { // Load the model here
            // two options here, the Task.Run or just running the function
            // await Task.Run(async () =&gt; await LoadModelAsync()); // tried this, never runs
            LoadModelAsync();  // gives the null reference exception
        }

        // this string will update on my Xamarin UI if the previous conditional finishes
        _activitystatus = &quot;After load model.&quot;; RaisePropertyChanged(() =&gt; activitystatus);

        ...
    }
</code></pre>
<p>I am not sure why the code runs forever. I believe it to be an issue with the asynchronous nature of the code, but as far as I can tell the amount of awaits and asyncs are the same as the MNIST example. I've read the answers to <a href=""https://stackoverflow.com/questions/9895048/async-call-with-await-in-httpclient-never-returns"">this question</a> and <a href=""https://stackoverflow.com/questions/10343632/httpclient-getasync-never-returns-when-using-await-async/10351400#10351400"">this question</a>, but I cannot seem to draw a solution from them regarding which async/await uses I should get rid of.</p>
<p>For reference, here is my TempModel class:</p>
<pre><code>public sealed class TempModel {
        private LearningModel model;
        private LearningModelSession session;
        private LearningModelBinding binding;
        public static async Task&lt;TempModel&gt; CreateFromStreamAsync(IRandomAccessStreamReference stream) {
            TempModel _model = new TempModel();
            _model.model   = await LearningModel.LoadFromStreamAsync(stream);
            _model.session = new LearningModelSession(_model.model);
            _model.binding = new LearningModelBinding(_model.session);
            return _model;
        }
        public async Task&lt;List&lt;float&gt;&gt; EvaluateAsync(TempInput input) {
            binding.Bind(&quot;lstm_12_input&quot;, input.Input3);
            var result = await session.EvaluateAsync(binding, string.Empty);
            var resultTensor = result.Outputs[&quot;dense_13&quot;] as TensorFloat;
            var resultVector = resultTensor.GetAsVectorView();
            List&lt;float&gt; op = new List&lt;float&gt;(resultVector);
            return op;
        }
    }

    private async Task LoadModelAsync() {  // Load a machine learning model
        StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx://newmodel.onnx&quot;));
        _model = await TempModel.CreateFromStreamAsync(modelFile as IRandomAccessStreamReference);
        _activitystatus = &quot;Finished LoadModelAsync&quot;; RaisePropertyChanged(() =&gt; activitystatus);
    }
</code></pre>
",44,0,0,5,c#;.net;tensorflow;uwp;onnx,2022-07-14 22:06:45,2022-07-14 22:06:45,2022-07-14 22:06:45,i am currently building a xamarin app for uwp  i am integrating the windows ai machinelearning microsoft ml tensorflow packages so that i can use an  onnx format tensorflow model trained in python within my c  xamarin app  i have verified using the onnx python package that my  onnx file is a valid model  so that is not an issue  here is the function where the error occurs  loadmodelasync  i am calling an  onnx model to use it to make a prediction  here is how the model  called modelgen  is initialize and later called  the function it is used exactly the same as the code in the first link  i am not sure why the code runs forever  i believe it to be an issue with the asynchronous nature of the code  but as far as i can tell the amount of awaits and asyncs are the same as the mnist example  i ve read the answers to  and   but i cannot seem to draw a solution from them regarding which async await uses i should get rid of  for reference  here is my tempmodel class ,loading onnx model asynchronously runs forever or throws null reference,currently building xamarin app uwp integrating windows ai machinelearning microsoft ml tensorflow packages use onnx format tensorflow model trained python within c xamarin app verified using onnx python package onnx file valid model issue function error occurs loadmodelasync calling onnx model use make prediction model called modelgen initialize later called function used exactly code first link sure code runs forever believe issue asynchronous nature code far tell amount awaits asyncs mnist example read answers cannot seem draw solution regarding async await uses get rid reference tempmodel class,loading onnx model asynchronously runs forever throws null reference,loading onnx model asynchronously runs forever throws null referencecurrently building xamarin app uwp integrating windows ai machinelearning microsoft ml tensorflow packages use onnx format tensorflow model trained python within c xamarin app verified using onnx python package onnx file valid model issue function error occurs loadmodelasync calling onnx model use make prediction model called modelgen initialize later called function used exactly code first link sure code runs forever believe issue asynchronous nature code far tell amount awaits asyncs mnist example read answers cannot seem draw solution regarding async await uses get rid reference tempmodel class,"['loading', 'onnx', 'model', 'asynchronously', 'runs', 'forever', 'throws', 'null', 'referencecurrently', 'building', 'xamarin', 'app', 'uwp', 'integrating', 'windows', 'ai', 'machinelearning', 'microsoft', 'ml', 'tensorflow', 'packages', 'use', 'onnx', 'format', 'tensorflow', 'model', 'trained', 'python', 'within', 'c', 'xamarin', 'app', 'verified', 'using', 'onnx', 'python', 'package', 'onnx', 'file', 'valid', 'model', 'issue', 'function', 'error', 'occurs', 'loadmodelasync', 'calling', 'onnx', 'model', 'use', 'make', 'prediction', 'model', 'called', 'modelgen', 'initialize', 'later', 'called', 'function', 'used', 'exactly', 'code', 'first', 'link', 'sure', 'code', 'runs', 'forever', 'believe', 'issue', 'asynchronous', 'nature', 'code', 'far', 'tell', 'amount', 'awaits', 'asyncs', 'mnist', 'example', 'read', 'answers', 'can', 'not', 'seem', 'draw', 'solution', 'regarding', 'async', 'await', 'uses', 'get', 'rid', 'reference', 'tempmodel', 'class']","['load', 'onnx', 'model', 'asynchron', 'run', 'forev', 'throw', 'null', 'referencecurr', 'build', 'xamarin', 'app', 'uwp', 'integr', 'window', 'ai', 'machinelearn', 'microsoft', 'ml', 'tensorflow', 'packag', 'use', 'onnx', 'format', 'tensorflow', 'model', 'train', 'python', 'within', 'c', 'xamarin', 'app', 'verifi', 'use', 'onnx', 'python', 'packag', 'onnx', 'file', 'valid', 'model', 'issu', 'function', 'error', 'occur', 'loadmodelasync', 'call', 'onnx', 'model', 'use', 'make', 'predict', 'model', 'call', 'modelgen', 'initi', 'later', 'call', 'function', 'use', 'exactli', 'code', 'first', 'link', 'sure', 'code', 'run', 'forev', 'believ', 'issu', 'asynchron', 'natur', 'code', 'far', 'tell', 'amount', 'await', 'async', 'mnist', 'exampl', 'read', 'answer', 'can', 'not', 'seem', 'draw', 'solut', 'regard', 'async', 'await', 'use', 'get', 'rid', 'refer', 'tempmodel', 'class']"
216,230,230,8973620,72978955,Time periods to evenly-spaced time series,"<p>I need to prepare data with time periods for machine learning in the way that I get equal spacing between timestamps. For example, for 3 hours spacing, I would like to have the following timestamps: 00:00, 03:00, 6:00, 9:00, 12:00, 15:00... For example:</p>
<pre><code>df = pd.DataFrame({'Start': ['2022-07-01 11:30', '2022-07-01 22:30'], 'End': ['2022-07-01 18:30', '2022-07-02 3:30'], 'Val': ['a', 'b']})
for col in ['Start', 'End']:
    df[col] = df[col].apply(pd.to_datetime)
print(df)
</code></pre>
<p>Output:</p>
<pre><code>                Start                 End Val
0 2022-07-01 11:30:00 2022-07-01 18:30:00   a
1 2022-07-01 22:30:00 2022-07-02 03:30:00   b
</code></pre>
<p>I try to get timestamps:</p>
<pre><code>df['Datetime'] = df.apply(lambda x: pd.date_range(x['Start'], x['End'], freq='3H'), axis=1)
df = df.explode('Datetime').drop(['Start', 'End'], axis=1)
df['Datetime'] = df['Datetime'].dt.round('H')
print(df[['Datetime', 'Val']])
</code></pre>
<p>Output:</p>
<pre><code>             Datetime Val
0 2022-07-01 12:00:00   a
0 2022-07-01 14:00:00   a
0 2022-07-01 18:00:00   a
1 2022-07-01 22:00:00   b
1 2022-07-02 02:00:00   b
</code></pre>
<p>As you can see, those timestamps are not equally spaced. My expected result:</p>
<pre><code>            Datetime  Val
4 2022-07-01 12:00:00    a
5 2022-07-01 15:00:00    a
6 2022-07-01 18:00:00    a
7 2022-07-01 21:00:00  NaN
8 2022-07-02 00:00:00    b
9 2022-07-02 03:00:00    b
</code></pre>
",70,4,2,3,python;pandas;time-series,2022-07-14 13:21:06,2022-07-14 13:21:06,2022-07-14 20:05:28,i need to prepare data with time periods for machine learning in the way that i get equal spacing between timestamps  for example  for  hours spacing  i would like to have the following timestamps                      for example  output  i try to get timestamps  output  as you can see  those timestamps are not equally spaced  my expected result ,time periods to evenly spaced time series,need prepare data time periods machine learning way get equal spacing timestamps example hours spacing would like following timestamps example output try get timestamps output see timestamps equally spaced expected result,time periods evenly spaced time series,time periods evenly spaced time seriesneed prepare data time periods machine learning way get equal spacing timestamps example hours spacing would like following timestamps example output try get timestamps output see timestamps equally spaced expected result,"['time', 'periods', 'evenly', 'spaced', 'time', 'seriesneed', 'prepare', 'data', 'time', 'periods', 'machine', 'learning', 'way', 'get', 'equal', 'spacing', 'timestamps', 'example', 'hours', 'spacing', 'would', 'like', 'following', 'timestamps', 'example', 'output', 'try', 'get', 'timestamps', 'output', 'see', 'timestamps', 'equally', 'spaced', 'expected', 'result']","['time', 'period', 'evenli', 'space', 'time', 'seriesne', 'prepar', 'data', 'time', 'period', 'machin', 'learn', 'way', 'get', 'equal', 'space', 'timestamp', 'exampl', 'hour', 'space', 'would', 'like', 'follow', 'timestamp', 'exampl', 'output', 'tri', 'get', 'timestamp', 'output', 'see', 'timestamp', 'equal', 'space', 'expect', 'result']"
217,232,232,7241279,60628482,How to fix the error &quot;Singleton array cannot be considered a valid collection&quot;,"<p>I'm starting my first machine learning code with python. But, I encountered an error while developing the confusion matrix for my multiclass model.</p>

<pre class=""lang-py prettyprint-override""><code>#Defining the model 

model = Sequential()

model.add(Dense(32,input_shape=(22,),activation='tanh'))
model.add(Dense(16,activation='tanh'))
model.add(Dense(6,activation='tanh'))
model.add(Dense(5,activation='softmax'))

model.compile(Adam(lr=0.004),'sparse_categorical_crossentropy',metrics=['accuracy'])

#fitting the model and predicting 

model.fit(X_train,Y_train,epochs=1)

Y_pred = model.predict(X_test)

Y_pred = Y_pred.astype(int)

Y_test_class = np.argmax(Y_test, axis=0)
Y_pred_class = np.argmax(Y_pred, axis=0)

#Accuracy of the predicted values

print(metrics.classification_report(Y_test_class,Y_pred_class))
print(metrics.confusion_matrix(Y_test_class,Y_pred_class))
</code></pre>

<p>I'm getting this error:</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Singleton array 3045 cannot be considered a valid collection.
</code></pre>

<p>Test data details
X_test[:5]</p>

<pre class=""lang-py prettyprint-override""><code>[['0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '1' '0' '0']
 ['1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '1' '1' '0']]
</code></pre>

<p>Y_test[:5]</p>

<pre class=""lang-py prettyprint-override""><code>['1' '2' '2' '2' '2']
</code></pre>

<p>The shape of </p>

<p><code>Y_test_class</code> ==> <code>()</code><br>
<code>Y_pred_class</code> ==> <code>(5,)</code></p>
",3042,1,1,4,python;machine-learning;confusion-matrix;argmax,2020-03-11 04:34:41,2020-03-11 04:34:41,2022-07-14 11:06:42,i m starting my first machine learning code with python  but  i encountered an error while developing the confusion matrix for my multiclass model  i m getting this error  y_test    the shape of ,how to fix the error  singleton array cannot be considered a valid collection ,starting first machine learning code python encountered error developing confusion matrix multiclass model getting error y_test shape,fix error singleton array cannot considered valid collection,fix error singleton array cannot considered valid collectionstarting first machine learning code python encountered error developing confusion matrix multiclass model getting error y_test shape,"['fix', 'error', 'singleton', 'array', 'can', 'not', 'considered', 'valid', 'collectionstarting', 'first', 'machine', 'learning', 'code', 'python', 'encountered', 'error', 'developing', 'confusion', 'matrix', 'multiclass', 'model', 'getting', 'error', 'y_test', 'shape']","['fix', 'error', 'singleton', 'array', 'can', 'not', 'consid', 'valid', 'collectionstart', 'first', 'machin', 'learn', 'code', 'python', 'encount', 'error', 'develop', 'confus', 'matrix', 'multiclass', 'model', 'get', 'error', 'y_test', 'shape']"
218,233,233,19500560,72976376,How to train a spacy model by using streaming data?,"<p>I have created a spacy model. But I need to retrain it until it reaches it maximum level. I need to train this model and retrain the model using the streaming data. I have seen that we can train some machine learning model using stream data. Is it possible to do the same to NLP models?</p>
",23,1,-1,5,python;machine-learning;nlp;stream;spacy,2022-07-14 09:53:32,2022-07-14 09:53:32,2022-07-14 10:14:05,i have created a spacy model  but i need to retrain it until it reaches it maximum level  i need to train this model and retrain the model using the streaming data  i have seen that we can train some machine learning model using stream data  is it possible to do the same to nlp models ,how to train a spacy model by using streaming data ,created spacy model need retrain reaches maximum level need train model retrain model using streaming data seen train machine learning model using stream data possible nlp models,train spacy model using streaming data,train spacy model using streaming datacreated spacy model need retrain reaches maximum level need train model retrain model using streaming data seen train machine learning model using stream data possible nlp models,"['train', 'spacy', 'model', 'using', 'streaming', 'datacreated', 'spacy', 'model', 'need', 'retrain', 'reaches', 'maximum', 'level', 'need', 'train', 'model', 'retrain', 'model', 'using', 'streaming', 'data', 'seen', 'train', 'machine', 'learning', 'model', 'using', 'stream', 'data', 'possible', 'nlp', 'models']","['train', 'spaci', 'model', 'use', 'stream', 'datacr', 'spaci', 'model', 'need', 'retrain', 'reach', 'maximum', 'level', 'need', 'train', 'model', 'retrain', 'model', 'use', 'stream', 'data', 'seen', 'train', 'machin', 'learn', 'model', 'use', 'stream', 'data', 'possibl', 'nlp', 'model']"
219,234,234,8389618,60319321,Enhancing the resolution of an image,"<p>I have images that are having very low quality and these images I have to use for person identification but with this quality it's difficult to detect. I want to enhance the quality of the images using deep learning/machine learning techniques. I have studied about SRCNN, perceptual Loss, SRResNet, SRGAN but most of the super image resolution techniques require original images for improving the quality of the images. So my question is there any deep learning techniques that can be used for the improving the quality of the images without using the original images.</p>

<p><a href=""https://i.stack.imgur.com/Lwv4T.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lwv4T.jpg"" alt=""image with low quality""></a></p>
",155,2,2,5,image;machine-learning;image-processing;deep-learning;computer-vision,2020-02-20 13:52:30,2020-02-20 13:52:30,2022-07-14 08:46:37,i have images that are having very low quality and these images i have to use for person identification but with this quality it s difficult to detect  i want to enhance the quality of the images using deep learning machine learning techniques  i have studied about srcnn  perceptual loss  srresnet  srgan but most of the super image resolution techniques require original images for improving the quality of the images  so my question is there any deep learning techniques that can be used for the improving the quality of the images without using the original images  ,enhancing the resolution of an image,images low quality images use person identification quality difficult detect want enhance quality images using deep learning machine learning techniques studied srcnn perceptual loss srresnet srgan super image resolution techniques require original images improving quality images question deep learning techniques used improving quality images without using original images,enhancing resolution image,enhancing resolution imageimages low quality images use person identification quality difficult detect want enhance quality images using deep learning machine learning techniques studied srcnn perceptual loss srresnet srgan super image resolution techniques require original images improving quality images question deep learning techniques used improving quality images without using original images,"['enhancing', 'resolution', 'imageimages', 'low', 'quality', 'images', 'use', 'person', 'identification', 'quality', 'difficult', 'detect', 'want', 'enhance', 'quality', 'images', 'using', 'deep', 'learning', 'machine', 'learning', 'techniques', 'studied', 'srcnn', 'perceptual', 'loss', 'srresnet', 'srgan', 'super', 'image', 'resolution', 'techniques', 'require', 'original', 'images', 'improving', 'quality', 'images', 'question', 'deep', 'learning', 'techniques', 'used', 'improving', 'quality', 'images', 'without', 'using', 'original', 'images']","['enhanc', 'resolut', 'imageimag', 'low', 'qualiti', 'imag', 'use', 'person', 'identif', 'qualiti', 'difficult', 'detect', 'want', 'enhanc', 'qualiti', 'imag', 'use', 'deep', 'learn', 'machin', 'learn', 'techniqu', 'studi', 'srcnn', 'perceptu', 'loss', 'srresnet', 'srgan', 'super', 'imag', 'resolut', 'techniqu', 'requir', 'origin', 'imag', 'improv', 'qualiti', 'imag', 'question', 'deep', 'learn', 'techniqu', 'use', 'improv', 'qualiti', 'imag', 'without', 'use', 'origin', 'imag']"
220,235,235,19543504,72969730,Getting error while checking hosts IP via Ansible Server machine,"<p>I'm getting an error prompt while checking the node's IP in hosts/inventory from my server machines, all nodes are connected.</p>
<p>Error Prompt:</p>
<pre><code>[WARNING]: Unable to parse /etc/ansible/hosts #poll_interval  = 15 as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'
[WARNING]: Could not match supplied host pattern, ignoring: demo 
</code></pre>
<p>SSH connections are already established with my nodes and can access from the server machine as well</p>
<p>Also, in the <em>hosts</em> file inside the ansible server a group is created with the node's private IPs:</p>
<pre class=""lang-ini prettyprint-override""><code># Ex 1: Ungrouped hosts, specify before any group headers.

[demo]
172.31.31.15
172.31.31.128
</code></pre>
<p>Also, inside the ansible server config file:</p>
<pre><code>inventory = /etc/ansible/hosts
sudo_user = root
</code></pre>
<p>Both these are uncommented as well. I'm learning ansible and new with this configuration tool.</p>
<p>Please help as I'm unable to fetch details of nodes with the above-said command not by group which I created as <code>demo</code>.</p>
",49,0,0,1,ansible,2022-07-13 19:36:53,2022-07-13 19:36:53,2022-07-14 08:18:05,i m getting an error prompt while checking the node s ip in hosts inventory from my server machines  all nodes are connected  error prompt  ssh connections are already established with my nodes and can access from the server machine as well also  in the hosts file inside the ansible server a group is created with the node s private ips  also  inside the ansible server config file  both these are uncommented as well  i m learning ansible and new with this configuration tool  please help as i m unable to fetch details of nodes with the above said command not by group which i created as demo ,getting error while checking hosts ip via ansible server machine,getting error prompt checking node ip hosts inventory server machines nodes connected error prompt ssh connections already established nodes access server machine well also hosts file inside ansible server group created node private ips also inside ansible server config file uncommented well learning ansible configuration tool please help unable fetch details nodes said command group created demo,getting error checking hosts ip via ansible server machine,getting error checking hosts ip via ansible server machinegetting error prompt checking node ip hosts inventory server machines nodes connected error prompt ssh connections already established nodes access server machine well also hosts file inside ansible server group created node private ips also inside ansible server config file uncommented well learning ansible configuration tool please help unable fetch details nodes said command group created demo,"['getting', 'error', 'checking', 'hosts', 'ip', 'via', 'ansible', 'server', 'machinegetting', 'error', 'prompt', 'checking', 'node', 'ip', 'hosts', 'inventory', 'server', 'machines', 'nodes', 'connected', 'error', 'prompt', 'ssh', 'connections', 'already', 'established', 'nodes', 'access', 'server', 'machine', 'well', 'also', 'hosts', 'file', 'inside', 'ansible', 'server', 'group', 'created', 'node', 'private', 'ips', 'also', 'inside', 'ansible', 'server', 'config', 'file', 'uncommented', 'well', 'learning', 'ansible', 'configuration', 'tool', 'please', 'help', 'unable', 'fetch', 'details', 'nodes', 'said', 'command', 'group', 'created', 'demo']","['get', 'error', 'check', 'host', 'ip', 'via', 'ansibl', 'server', 'machineget', 'error', 'prompt', 'check', 'node', 'ip', 'host', 'inventori', 'server', 'machin', 'node', 'connect', 'error', 'prompt', 'ssh', 'connect', 'alreadi', 'establish', 'node', 'access', 'server', 'machin', 'well', 'also', 'host', 'file', 'insid', 'ansibl', 'server', 'group', 'creat', 'node', 'privat', 'ip', 'also', 'insid', 'ansibl', 'server', 'config', 'file', 'uncom', 'well', 'learn', 'ansibl', 'configur', 'tool', 'pleas', 'help', 'unabl', 'fetch', 'detail', 'node', 'said', 'command', 'group', 'creat', 'demo']"
221,237,237,0,72107817,I Cannot Install SQL Server 2019 Express. It gets stuck on Offline Installation of Microsoft Machine Learning Server Components,"<p>I'm trying to install SQL Server 2019 Express on my laptop. I initially click on custom to start and everything seems to go smooth up to the point where it's at the Offline Installation of Microsoft Machine Learning Server Components section.</p>
<p>When I get there I get a screenshot that looks like this:<br>
<p><img src=""https://bobgatto.com/images/sql_install.jpg""></p>
<p>From this point on I cannot figure out what to do next. I tried creating a directory, downloading all of the files listed to that directory, and enter that directory in the Install Path line. But when I do that the Next button still is disabled.</p>
<p>So what is the next step?</p>
<p>Thanks for any help</p> 
",4860,3,5,1,sql-server,2022-05-04 07:07:37,2022-05-04 07:07:37,2022-07-14 05:53:19,i m trying to install sql server  express on my laptop  i initially click on custom to start and everything seems to go smooth up to the point where it s at the offline installation of microsoft machine learning server components section   from this point on i cannot figure out what to do next  i tried creating a directory  downloading all of the files listed to that directory  and enter that directory in the install path line  but when i do that the next button still is disabled  so what is the next step  thanks for any help,i cannot install sql server  express  it gets stuck on offline installation of microsoft machine learning server components,trying install sql server express laptop initially click start everything seems go smooth point offline installation microsoft machine learning server components section point cannot figure next tried creating directory downloading files listed directory enter directory install path line next button still disabled next step thanks help,cannot install sql server express gets stuck offline installation microsoft machine learning server components,cannot install sql server express gets stuck offline installation microsoft machine learning server componentstrying install sql server express laptop initially click start everything seems go smooth point offline installation microsoft machine learning server components section point cannot figure next tried creating directory downloading files listed directory enter directory install path line next button still disabled next step thanks help,"['can', 'not', 'install', 'sql', 'server', 'express', 'gets', 'stuck', 'offline', 'installation', 'microsoft', 'machine', 'learning', 'server', 'componentstrying', 'install', 'sql', 'server', 'express', 'laptop', 'initially', 'click', 'start', 'everything', 'seems', 'go', 'smooth', 'point', 'offline', 'installation', 'microsoft', 'machine', 'learning', 'server', 'components', 'section', 'point', 'can', 'not', 'figure', 'next', 'tried', 'creating', 'directory', 'downloading', 'files', 'listed', 'directory', 'enter', 'directory', 'install', 'path', 'line', 'next', 'button', 'still', 'disabled', 'next', 'step', 'thanks', 'help']","['can', 'not', 'instal', 'sql', 'server', 'express', 'get', 'stuck', 'offlin', 'instal', 'microsoft', 'machin', 'learn', 'server', 'componentstri', 'instal', 'sql', 'server', 'express', 'laptop', 'initi', 'click', 'start', 'everyth', 'seem', 'go', 'smooth', 'point', 'offlin', 'instal', 'microsoft', 'machin', 'learn', 'server', 'compon', 'section', 'point', 'can', 'not', 'figur', 'next', 'tri', 'creat', 'directori', 'download', 'file', 'list', 'directori', 'enter', 'directori', 'instal', 'path', 'line', 'next', 'button', 'still', 'disabl', 'next', 'step', 'thank', 'help']"
222,238,238,6237651,55079999,Adding an image to a toolbar in a Vue + Vuetify Single File Compnonent,"<p>I'm essentially just remixing the code available here for a side project: <a href=""https://github.com/aws-samples/aws-ai-qna-bot.git"" rel=""nofollow noreferrer"">https://github.com/aws-samples/aws-ai-qna-bot.git</a></p>
<p><strong>Problem</strong>: I'm trying to insert a centered logo in the toolbar between the app drawer and the logout button.  Typically I could accomplish this pretty easily with vanilla HTML and CSS, but this project is leveraging Vue.js and Vuetify, which I'm doing my best to get myself up to speed with.</p>
<p>I've referenced the following documents, including the <code>README.md</code> in the git repo:</p>
<p><a href=""https://vuetifyjs.com/en/components/images"" rel=""nofollow noreferrer"">https://vuetifyjs.com/en/components/images</a></p>
<p><a href=""https://v2.vuejs.org/v2/guide/single-file-components.html"" rel=""nofollow noreferrer"">https://v2.vuejs.org/v2/guide/single-file-components.html</a></p>
<p><strong>File path</strong>: <code>qna-bot-template/website/js/admin.vue</code></p>
<pre><code>&lt;template lang=&quot;pug&quot;&gt;
  v-app
    v-navigation-drawer(temporary v-model=&quot;drawer&quot; app)
      v-toolbar(flat)
        v-list
          v-list-tile
            v-list-tile-title.title Tools
      v-divider
      v-list(dense three-line subheader)
        v-list-tile(v-for=&quot;(page,key) in pages&quot; :key=&quot;key&quot;
          @click=&quot;drawer=false&quot; 
          :href=&quot;page.href&quot;
          :id=&quot;'page-link-'+page.id&quot;
          :target=&quot;page.target || '_self'&quot;) 
          v-list-tile-avatar
            v-icon(color=&quot;primary&quot;) {{page.icon}}
          v-list-tile-content
            v-list-tile-title {{page.title}}
            v-list-tile-sub-title {{page.subTitle}}
        v-list-group( prepend-icon=&quot;info&quot; value=&quot;true&quot; color=&quot;primary&quot;)
          v-list-tile(slot=&quot;activator&quot;) 
            v-list-tile-title QnABot Help
          v-list-tile
            v-list-tile-content 
              v-list-tile-title Version: {{Version}}
              v-list-tile-title BuildDate: {{BuildDate}}
          v-list-tile
            v-list-tile-content
              v-list-tile-title 
                a(href=&quot;https://amazon.com/qnabot&quot; target=&quot;_blank&quot;) General Instructions / QnABot Blog Post
              v-list-tile-title
                a(href=&quot;https://aws.amazon.com/blogs/machine-learning/creating-virtual-guided-navigation-using-a-question-and-answer-bot-with-amazon-lex-and-amazon-alexa/&quot; target=&quot;_blank&quot;) Guided Navigation using QnABot
              v-list-tile-title
                a(href=&quot;https://aws.amazon.com/blogs/machine-learning/create-a-questionnaire-bot-with-amazon-lex-and-amazon-alexa/&quot; target=&quot;_blank&quot;) Create a questionnaire using QnABot
    v-toolbar(app fixed)
      v-toolbar-side-icon.primary--text(
        id=&quot;nav-open&quot;
        @click.stop=&quot;drawer = !drawer&quot;
      )
      v-toolbar-title 
        v-breadcrumbs
          v-breadcrumbs-item(href='#/edit') {{$store.state.info.StackName}}:{{$store.state.user.name}}
          v-breadcrumbs-item {{page}}
      v-spacer
      v-toolbar-items
        v-btn.primary--text(flat
          id=&quot;logout-button&quot;
          @click=&quot;logout&quot;
          v-if=&quot;login&quot;) LogOut
    v-container(fluid id=&quot;workspace&quot;)
      v-layout(column)
        v-flex
          router-view
    v-footer
&lt;/template&gt;
</code></pre>
<p>So far I've tried following the following syntax, which I added right after the <code>v-spacer</code> toward the bottom of the wrapping template tags.</p>
<pre><code>v-container
  v-img(:src=&quot;/abc/xyz&quot;)
</code></pre>
<p>and this doesn't seem to be working.</p>
<p>Lastly I'll add that since this environment is deployed to an EC2 instance (don't think you can deploy it locally to prototype via <code>vue serve</code> or at least I haven't been able to), I'm having to do this very roundabout way of prototyping by deploying this S3 bucket where the webpages are built to, then I <code>make</code> this webpack listener which will see whenever I modify a file.  Then I can refresh the index.html that is built in the S3 bucket to see my changes.  Extremely clunky workflow, I know, but I've never worked in an environment like this so I'm not sure if there's a better way, plus the readme provided in the github repo is very light on details for how to modify the default layout.</p>
<p>Any help/pointers would be greatly appreciated.</p>
",5760,1,0,3,amazon-web-services;vue.js;vuetify.js,2019-03-09 19:22:23,2019-03-09 19:22:23,2022-07-14 04:44:12,i m essentially just remixing the code available here for a side project   problem  i m trying to insert a centered logo in the toolbar between the app drawer and the logout button   typically i could accomplish this pretty easily with vanilla html and css  but this project is leveraging vue js and vuetify  which i m doing my best to get myself up to speed with  i ve referenced the following documents  including the readme md in the git repo    file path  qna bot template website js admin vue so far i ve tried following the following syntax  which i added right after the v spacer toward the bottom of the wrapping template tags  and this doesn t seem to be working  lastly i ll add that since this environment is deployed to an ec instance  don t think you can deploy it locally to prototype via vue serve or at least i haven t been able to   i m having to do this very roundabout way of prototyping by deploying this s bucket where the webpages are built to  then i make this webpack listener which will see whenever i modify a file   then i can refresh the index html that is built in the s bucket to see my changes   extremely clunky workflow  i know  but i ve never worked in an environment like this so i m not sure if there s a better way  plus the readme provided in the github repo is very light on details for how to modify the default layout  any help pointers would be greatly appreciated ,adding an image to a toolbar in a vue   vuetify single file compnonent,essentially remixing code available side project problem trying insert centered logo toolbar app drawer logout button typically could accomplish pretty easily vanilla html css project leveraging vue js vuetify best get speed referenced following documents including readme md git repo file path qna bot template website js admin vue far tried following following syntax added right v spacer toward bottom wrapping template tags seem working lastly since environment deployed ec instance think deploy locally prototype via vue serve least able roundabout way prototyping deploying bucket webpages built make webpack listener see whenever modify file refresh index html built bucket see changes extremely clunky workflow know never worked environment like sure better way plus readme provided github repo light details modify default layout help pointers would greatly appreciated,adding image toolbar vue vuetify single file compnonent,adding image toolbar vue vuetify single file compnonentessentially remixing code available side project problem trying insert centered logo toolbar app drawer logout button typically could accomplish pretty easily vanilla html css project leveraging vue js vuetify best get speed referenced following documents including readme md git repo file path qna bot template website js admin vue far tried following following syntax added right v spacer toward bottom wrapping template tags seem working lastly since environment deployed ec instance think deploy locally prototype via vue serve least able roundabout way prototyping deploying bucket webpages built make webpack listener see whenever modify file refresh index html built bucket see changes extremely clunky workflow know never worked environment like sure better way plus readme provided github repo light details modify default layout help pointers would greatly appreciated,"['adding', 'image', 'toolbar', 'vue', 'vuetify', 'single', 'file', 'compnonentessentially', 'remixing', 'code', 'available', 'side', 'project', 'problem', 'trying', 'insert', 'centered', 'logo', 'toolbar', 'app', 'drawer', 'logout', 'button', 'typically', 'could', 'accomplish', 'pretty', 'easily', 'vanilla', 'html', 'css', 'project', 'leveraging', 'vue', 'js', 'vuetify', 'best', 'get', 'speed', 'referenced', 'following', 'documents', 'including', 'readme', 'md', 'git', 'repo', 'file', 'path', 'qna', 'bot', 'template', 'website', 'js', 'admin', 'vue', 'far', 'tried', 'following', 'following', 'syntax', 'added', 'right', 'v', 'spacer', 'toward', 'bottom', 'wrapping', 'template', 'tags', 'seem', 'working', 'lastly', 'since', 'environment', 'deployed', 'ec', 'instance', 'think', 'deploy', 'locally', 'prototype', 'via', 'vue', 'serve', 'least', 'able', 'roundabout', 'way', 'prototyping', 'deploying', 'bucket', 'webpages', 'built', 'make', 'webpack', 'listener', 'see', 'whenever', 'modify', 'file', 'refresh', 'index', 'html', 'built', 'bucket', 'see', 'changes', 'extremely', 'clunky', 'workflow', 'know', 'never', 'worked', 'environment', 'like', 'sure', 'better', 'way', 'plus', 'readme', 'provided', 'github', 'repo', 'light', 'details', 'modify', 'default', 'layout', 'help', 'pointers', 'would', 'greatly', 'appreciated']","['ad', 'imag', 'toolbar', 'vue', 'vuetifi', 'singl', 'file', 'compnonentessenti', 'remix', 'code', 'avail', 'side', 'project', 'problem', 'tri', 'insert', 'center', 'logo', 'toolbar', 'app', 'drawer', 'logout', 'button', 'typic', 'could', 'accomplish', 'pretti', 'easili', 'vanilla', 'html', 'css', 'project', 'leverag', 'vue', 'js', 'vuetifi', 'best', 'get', 'speed', 'referenc', 'follow', 'document', 'includ', 'readm', 'md', 'git', 'repo', 'file', 'path', 'qna', 'bot', 'templat', 'websit', 'js', 'admin', 'vue', 'far', 'tri', 'follow', 'follow', 'syntax', 'ad', 'right', 'v', 'spacer', 'toward', 'bottom', 'wrap', 'templat', 'tag', 'seem', 'work', 'lastli', 'sinc', 'environ', 'deploy', 'ec', 'instanc', 'think', 'deploy', 'local', 'prototyp', 'via', 'vue', 'serv', 'least', 'abl', 'roundabout', 'way', 'prototyp', 'deploy', 'bucket', 'webpag', 'built', 'make', 'webpack', 'listen', 'see', 'whenev', 'modifi', 'file', 'refresh', 'index', 'html', 'built', 'bucket', 'see', 'chang', 'extrem', 'clunki', 'workflow', 'know', 'never', 'work', 'environ', 'like', 'sure', 'better', 'way', 'plu', 'readm', 'provid', 'github', 'repo', 'light', 'detail', 'modifi', 'default', 'layout', 'help', 'pointer', 'would', 'greatli', 'appreci']"
223,239,239,19520507,72973487,Which Random Forest for stock market analysis?,"<p>Wanting to use machine learning for stock market analysis as a summer project (new to compter sci). Want to use random forest but am unsure if i need randomfroestclassifier or randomforestregression and am also unsure about the difference between the two. I want to also find out advantaged of this over LSTM models. THANKS</p>
",13,0,2,1,lstm,2022-07-14 01:54:13,2022-07-14 01:54:13,2022-07-14 01:54:13,wanting to use machine learning for stock market analysis as a summer project  new to compter sci   want to use random forest but am unsure if i need randomfroestclassifier or randomforestregression and am also unsure about the difference between the two  i want to also find out advantaged of this over lstm models  thanks,which random forest for stock market analysis ,wanting use machine learning stock market analysis summer project compter sci want use random forest unsure need randomfroestclassifier randomforestregression also unsure difference two want also find advantaged lstm models thanks,random forest stock market analysis,random forest stock market analysiswanting use machine learning stock market analysis summer project compter sci want use random forest unsure need randomfroestclassifier randomforestregression also unsure difference two want also find advantaged lstm models thanks,"['random', 'forest', 'stock', 'market', 'analysiswanting', 'use', 'machine', 'learning', 'stock', 'market', 'analysis', 'summer', 'project', 'compter', 'sci', 'want', 'use', 'random', 'forest', 'unsure', 'need', 'randomfroestclassifier', 'randomforestregression', 'also', 'unsure', 'difference', 'two', 'want', 'also', 'find', 'advantaged', 'lstm', 'models', 'thanks']","['random', 'forest', 'stock', 'market', 'analysisw', 'use', 'machin', 'learn', 'stock', 'market', 'analysi', 'summer', 'project', 'compter', 'sci', 'want', 'use', 'random', 'forest', 'unsur', 'need', 'randomfroestclassifi', 'randomforestregress', 'also', 'unsur', 'differ', 'two', 'want', 'also', 'find', 'advantag', 'lstm', 'model', 'thank']"
224,240,240,19295532,72969484,Non-Linear Machine Learning Algorithms,"<p>I am working on a dataset using python I have 17 variables that need to be used to predict one thing (Thing being a %) is there any non-linear machine learning algorithms anyone knows that could achieve this. I have. already implemented a multiple linear regression to it but the data is not very linear so I would want a better fit. All the other algorithms I've tried such as polynomial regression need the same values X=Y but in this case that is not possible as I have 17 variables all needed to predict one thing.</p>
",30,1,-1,3,python;regression;non-linear,2022-07-13 19:16:37,2022-07-13 19:16:37,2022-07-13 19:35:30,i am working on a dataset using python i have  variables that need to be used to predict one thing  thing being a    is there any non linear machine learning algorithms anyone knows that could achieve this  i have  already implemented a multiple linear regression to it but the data is not very linear so i would want a better fit  all the other algorithms i ve tried such as polynomial regression need the same values x y but in this case that is not possible as i have  variables all needed to predict one thing ,non linear machine learning algorithms,working dataset using python variables need used predict one thing thing non linear machine learning algorithms anyone knows could achieve already implemented multiple linear regression data linear would want better fit algorithms tried polynomial regression need values x case possible variables needed predict one thing,non linear machine learning algorithms,non linear machine learning algorithmsworking dataset using python variables need used predict one thing thing non linear machine learning algorithms anyone knows could achieve already implemented multiple linear regression data linear would want better fit algorithms tried polynomial regression need values x case possible variables needed predict one thing,"['non', 'linear', 'machine', 'learning', 'algorithmsworking', 'dataset', 'using', 'python', 'variables', 'need', 'used', 'predict', 'one', 'thing', 'thing', 'non', 'linear', 'machine', 'learning', 'algorithms', 'anyone', 'knows', 'could', 'achieve', 'already', 'implemented', 'multiple', 'linear', 'regression', 'data', 'linear', 'would', 'want', 'better', 'fit', 'algorithms', 'tried', 'polynomial', 'regression', 'need', 'values', 'x', 'case', 'possible', 'variables', 'needed', 'predict', 'one', 'thing']","['non', 'linear', 'machin', 'learn', 'algorithmswork', 'dataset', 'use', 'python', 'variabl', 'need', 'use', 'predict', 'one', 'thing', 'thing', 'non', 'linear', 'machin', 'learn', 'algorithm', 'anyon', 'know', 'could', 'achiev', 'alreadi', 'implement', 'multipl', 'linear', 'regress', 'data', 'linear', 'would', 'want', 'better', 'fit', 'algorithm', 'tri', 'polynomi', 'regress', 'need', 'valu', 'x', 'case', 'possibl', 'variabl', 'need', 'predict', 'one', 'thing']"
225,241,241,8993562,47445310,Python : (Titanic) debugging error in imputing fare,"<p>I will thank very much for your help.I have a problem with the classic Kaggle Titanic Tutorial for machine learning. My problem is when using a pivotal table to imput means in a dataframe (or tupple):</p>

<pre><code>fare_means = df.pivot_table(""Fare"", index= ""Pclass"", aggfunc=""mean"")

fare_means.info()    
fare_class 'pandas.core.frame.DataFrame'&gt;    
Int64   
Index: 3 entries, 1 to 3    
Data columns (total 1 columns):  Fare    3 non-null float64  
dtypes: float64(1)   
memory usage: 48.0 bytes

fare_means   
Out[46]:   
Fare ---Pclass     
1     ------- 84.154687  
2      -------20.662183   
3      -------13.675550

df_test['Fare'] = df_test[['Fare', 'Pclass']].apply(lambda x:

fare_means[x['Pclass']] if pd.isnull(x['Fare'])

else x['Fare'], axis=1)
</code></pre>

<h1>KeyError: (3, u'occurred at index 152')</h1>

<pre><code>df_test.iloc[150:155, 0: ]   
  Index-- Pclass    ----    Fare       
  150 ------ 1      --------83.1583   
  151 ------ 3      --------7.8958    
  152 ------ 3      --------NaN    
  153 ------ 3      --------12.1833  
  154 ------ 3      --------31.3875 
</code></pre>
",146,1,0,3,python;python-2.7;debugging,2017-11-23 01:18:24,2017-11-23 01:18:24,2022-07-13 18:40:50,i will thank very much for your help i have a problem with the classic kaggle titanic tutorial for machine learning  my problem is when using a pivotal table to imput means in a dataframe  or tupple  ,python    titanic  debugging error in imputing fare,thank much help problem classic kaggle titanic tutorial machine learning problem using pivotal table imput means dataframe tupple,python titanic debugging error imputing fare,python titanic debugging error imputing farethank much help problem classic kaggle titanic tutorial machine learning problem using pivotal table imput means dataframe tupple,"['python', 'titanic', 'debugging', 'error', 'imputing', 'farethank', 'much', 'help', 'problem', 'classic', 'kaggle', 'titanic', 'tutorial', 'machine', 'learning', 'problem', 'using', 'pivotal', 'table', 'imput', 'means', 'dataframe', 'tupple']","['python', 'titan', 'debug', 'error', 'imput', 'farethank', 'much', 'help', 'problem', 'classic', 'kaggl', 'titan', 'tutori', 'machin', 'learn', 'problem', 'use', 'pivot', 'tabl', 'imput', 'mean', 'datafram', 'tuppl']"
226,242,242,18355714,72955247,GPU question about multithreading and machine learning,"<p>I have a machine learning model that uses nearly 3GB of my GPU. I would like to multithread this. In my head, this is how it should happen:</p>
<p>For each thread, I create I would need to generate a duplicate neural network to put into the GPU. As having all threads using the same model could be dangerous? and I don't want to wrap the model in a critical lock section.</p>
<p>Therefore to have 2 threads running I would need 6GB of GPU space?</p>
",28,0,-1,4,multithreading;machine-learning;computer-vision;gpu,2022-07-12 19:04:15,2022-07-12 19:04:15,2022-07-13 11:37:34,i have a machine learning model that uses nearly gb of my gpu  i would like to multithread this  in my head  this is how it should happen  for each thread  i create i would need to generate a duplicate neural network to put into the gpu  as having all threads using the same model could be dangerous  and i don t want to wrap the model in a critical lock section  therefore to have  threads running i would need gb of gpu space ,gpu question about multithreading and machine learning,machine learning model uses nearly gb gpu would like multithread head happen thread create would need generate duplicate neural network put gpu threads using model could dangerous want wrap model critical lock section therefore threads running would need gb gpu space,gpu question multithreading machine learning,gpu question multithreading machine learningmachine learning model uses nearly gb gpu would like multithread head happen thread create would need generate duplicate neural network put gpu threads using model could dangerous want wrap model critical lock section therefore threads running would need gb gpu space,"['gpu', 'question', 'multithreading', 'machine', 'learningmachine', 'learning', 'model', 'uses', 'nearly', 'gb', 'gpu', 'would', 'like', 'multithread', 'head', 'happen', 'thread', 'create', 'would', 'need', 'generate', 'duplicate', 'neural', 'network', 'put', 'gpu', 'threads', 'using', 'model', 'could', 'dangerous', 'want', 'wrap', 'model', 'critical', 'lock', 'section', 'therefore', 'threads', 'running', 'would', 'need', 'gb', 'gpu', 'space']","['gpu', 'question', 'multithread', 'machin', 'learningmachin', 'learn', 'model', 'use', 'nearli', 'gb', 'gpu', 'would', 'like', 'multithread', 'head', 'happen', 'thread', 'creat', 'would', 'need', 'gener', 'duplic', 'neural', 'network', 'put', 'gpu', 'thread', 'use', 'model', 'could', 'danger', 'want', 'wrap', 'model', 'critic', 'lock', 'section', 'therefor', 'thread', 'run', 'would', 'need', 'gb', 'gpu', 'space']"
227,243,243,19538261,72960178,how to insert or update a new data into existing table using mysql in python flask (data from machine learning),"<p><strong>My problem when i using query insert or update the result of value from naive bayes algorithm into my existing table and column</strong></p>
<p>example of problem 1 that occurs when I use the query insert into my table data_uji3 at column prob which is the column that is used as the result value from the naive bayes algorithm, the data stored in the data_uji3 table in the &quot;prob&quot; column does not start from the top or the first row, but it start from the bottom row and makes as many rows as the amount of data available.
i have tried to insert it to another table for the result of algorithm and it works, but i need this value from algorithm must be place at table data_uji3 at column prob, the code for insert describe at below</p>
<pre><code>akurasi=pd.read_sql_query('SELECT*FROM data_uji3',cur )
        x1 = akurasi.iloc[:,[4,5,6,7,8]].values
        y_pred1=NBmodel.predict(x1)
        print(y_pred1)
        pt= y_pred1.reshape(250,1)
        if request.method == &quot;POST&quot;:
            
            cur1 =mysql.connection.cursor()
            for i in pt:
             cur1.execute(&quot;INSERT INTO data_uji3 (prob) VALUES (%s)&quot;,i)
             print(i)       
             mysql.connection.commit()
</code></pre>
<p>and the result at my table at this image
<a href=""https://i.stack.imgur.com/pFJjn.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/pFJjn.png</a></p>
<p>example of next problem is when i used the update query for that case, the data updated from result of algorithm just write by one, the code for update describe at below</p>
<pre><code>akurasi=pd.read_sql_query('SELECT*FROM data_uji3',cur )
        x1 = akurasi.iloc[:,[4,5,6,7,8]].values
        y_pred1=NBmodel.predict(x1)
        print(y_pred1)
        pt= y_pred1.reshape(250,1)
        if request.method == &quot;POST&quot;:
            
            cur1 =mysql.connection.cursor()
            for i in pt:
             cur1.execute(&quot;UPDATE data_uji3 SET prob=%s&quot;,i)
             print(i)       
             mysql.connection.commit()
</code></pre>
<p>and the result at my table at this image
<a href=""https://i.stack.imgur.com/tqa2I.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/tqa2I.png</a></p>
<p>on those my case, is there any unused code in my program or is there any solution for my case, please some one to help me</p>
",25,0,-1,3,python;mysql;pandas,2022-07-13 05:13:33,2022-07-13 05:13:33,2022-07-13 06:12:59,my problem when i using query insert or update the result of value from naive bayes algorithm into my existing table and column example of next problem is when i used the update query for that case  the data updated from result of algorithm just write by one  the code for update describe at below on those my case  is there any unused code in my program or is there any solution for my case  please some one to help me,how to insert or update a new data into existing table using mysql in python flask  data from machine learning ,problem using query insert update result value naive bayes algorithm existing table column example next problem used update query case data updated result algorithm write one code update describe case unused code program solution case please one help,insert update data existing table using mysql python flask data machine learning,insert update data existing table using mysql python flask data machine learningproblem using query insert update result value naive bayes algorithm existing table column example next problem used update query case data updated result algorithm write one code update describe case unused code program solution case please one help,"['insert', 'update', 'data', 'existing', 'table', 'using', 'mysql', 'python', 'flask', 'data', 'machine', 'learningproblem', 'using', 'query', 'insert', 'update', 'result', 'value', 'naive', 'bayes', 'algorithm', 'existing', 'table', 'column', 'example', 'next', 'problem', 'used', 'update', 'query', 'case', 'data', 'updated', 'result', 'algorithm', 'write', 'one', 'code', 'update', 'describe', 'case', 'unused', 'code', 'program', 'solution', 'case', 'please', 'one', 'help']","['insert', 'updat', 'data', 'exist', 'tabl', 'use', 'mysql', 'python', 'flask', 'data', 'machin', 'learningproblem', 'use', 'queri', 'insert', 'updat', 'result', 'valu', 'naiv', 'bay', 'algorithm', 'exist', 'tabl', 'column', 'exampl', 'next', 'problem', 'use', 'updat', 'queri', 'case', 'data', 'updat', 'result', 'algorithm', 'write', 'one', 'code', 'updat', 'describ', 'case', 'unus', 'code', 'program', 'solut', 'case', 'pleas', 'one', 'help']"
228,244,244,13555760,72958457,custom neural network model that shares the same fixed weights across a variable number of inputs,"<p>I have a special machine learning problem and I've come up with a weird solution that I'm not entirely sure how to implement in tensorflow for python (and it has to be tensorflow for python). The problem is essentially just picking the best of 5 options, where each option is represented by a vector of length 10. The training data is organized into input vectors of length 50 (5 options * 10 option length) and labeled with one-hot vectors of length 5.</p>
<p>Here's the wacky part: it might not be 5. The model needs to be able to choose from any number of options. So, just having a dense model with 50 inputs and 5 outputs isn't going to work.</p>
<p>The best solution I can think of is to just evaluate individual options rather than a full set. Make a model with 10 inputs and 1 output, where the output is just the model's &quot;confidence&quot; in how good the input vector is. Pass all 5 (or however many) inputs through the model, put each confidence value into an array, and pass that array through a softmax function to get an output.</p>
<p>Now, I can't just train a model with 10 inputs and 1 output and do the combination and softmax stuff afterwards in numpy, because my training data isn't 10 values mapped to a confidence value. It's 30, 40, 50, or 10*n values mapped to a one-hot vector of size 3, 4, 5, or n. It needs to train in sets, so the model itself needs to be able to handle sets. I have no clue how to do this in tensorflow. As far as I can tell, Keras forces you to build models with a fixed input and output size. Even though it has a nice functional API, it won't let me do variable input/output sizes.</p>
<p>If this were libtorch for C++, I would just create a forward() function when defining my model and it would let me do whatever input and output sizes I want, but I can't find how to make the equivalent of this in tensorflow for python. Is this even possible?</p>
<p>Any tips are appreciated, thanks.</p>
",27,0,0,3,python;tensorflow;keras,2022-07-13 00:14:11,2022-07-13 00:14:11,2022-07-13 01:56:19,i have a special machine learning problem and i ve come up with a weird solution that i m not entirely sure how to implement in tensorflow for python  and it has to be tensorflow for python   the problem is essentially just picking the best of  options  where each option is represented by a vector of length   the training data is organized into input vectors of length    options    option length  and labeled with one hot vectors of length   here s the wacky part  it might not be   the model needs to be able to choose from any number of options  so  just having a dense model with  inputs and  outputs isn t going to work  the best solution i can think of is to just evaluate individual options rather than a full set  make a model with  inputs and  output  where the output is just the model s  confidence  in how good the input vector is  pass all   or however many  inputs through the model  put each confidence value into an array  and pass that array through a softmax function to get an output  now  i can t just train a model with  inputs and  output and do the combination and softmax stuff afterwards in numpy  because my training data isn t  values mapped to a confidence value  it s       or  n values mapped to a one hot vector of size       or n  it needs to train in sets  so the model itself needs to be able to handle sets  i have no clue how to do this in tensorflow  as far as i can tell  keras forces you to build models with a fixed input and output size  even though it has a nice functional api  it won t let me do variable input output sizes  if this were libtorch for c    i would just create a forward   function when defining my model and it would let me do whatever input and output sizes i want  but i can t find how to make the equivalent of this in tensorflow for python  is this even possible  any tips are appreciated  thanks ,custom neural network model that shares the same fixed weights across a variable number of inputs,special machine learning problem come weird solution entirely sure implement tensorflow python tensorflow python problem essentially picking best options option represented vector length training data organized input vectors length options option length labeled one hot vectors length wacky part might model needs able choose number options dense model inputs outputs going work best solution think evaluate individual options rather full set make model inputs output output model confidence good input vector pass however many inputs model put confidence value array pass array softmax function get output train model inputs output combination softmax stuff afterwards numpy training data values mapped confidence value n values mapped one hot vector size n needs train sets model needs able handle sets clue tensorflow far tell keras forces build models fixed input output size even though nice functional api let variable input output sizes libtorch c would create forward function defining model would let whatever input output sizes want find make equivalent tensorflow python even possible tips appreciated thanks,neural network model shares fixed weights across variable number inputs,neural network model shares fixed weights across variable number inputsspecial machine learning problem come weird solution entirely sure implement tensorflow python tensorflow python problem essentially picking best options option represented vector length training data organized input vectors length options option length labeled one hot vectors length wacky part might model needs able choose number options dense model inputs outputs going work best solution think evaluate individual options rather full set make model inputs output output model confidence good input vector pass however many inputs model put confidence value array pass array softmax function get output train model inputs output combination softmax stuff afterwards numpy training data values mapped confidence value n values mapped one hot vector size n needs train sets model needs able handle sets clue tensorflow far tell keras forces build models fixed input output size even though nice functional api let variable input output sizes libtorch c would create forward function defining model would let whatever input output sizes want find make equivalent tensorflow python even possible tips appreciated thanks,"['neural', 'network', 'model', 'shares', 'fixed', 'weights', 'across', 'variable', 'number', 'inputsspecial', 'machine', 'learning', 'problem', 'come', 'weird', 'solution', 'entirely', 'sure', 'implement', 'tensorflow', 'python', 'tensorflow', 'python', 'problem', 'essentially', 'picking', 'best', 'options', 'option', 'represented', 'vector', 'length', 'training', 'data', 'organized', 'input', 'vectors', 'length', 'options', 'option', 'length', 'labeled', 'one', 'hot', 'vectors', 'length', 'wacky', 'part', 'might', 'model', 'needs', 'able', 'choose', 'number', 'options', 'dense', 'model', 'inputs', 'outputs', 'going', 'work', 'best', 'solution', 'think', 'evaluate', 'individual', 'options', 'rather', 'full', 'set', 'make', 'model', 'inputs', 'output', 'output', 'model', 'confidence', 'good', 'input', 'vector', 'pass', 'however', 'many', 'inputs', 'model', 'put', 'confidence', 'value', 'array', 'pass', 'array', 'softmax', 'function', 'get', 'output', 'train', 'model', 'inputs', 'output', 'combination', 'softmax', 'stuff', 'afterwards', 'numpy', 'training', 'data', 'values', 'mapped', 'confidence', 'value', 'n', 'values', 'mapped', 'one', 'hot', 'vector', 'size', 'n', 'needs', 'train', 'sets', 'model', 'needs', 'able', 'handle', 'sets', 'clue', 'tensorflow', 'far', 'tell', 'keras', 'forces', 'build', 'models', 'fixed', 'input', 'output', 'size', 'even', 'though', 'nice', 'functional', 'api', 'let', 'variable', 'input', 'output', 'sizes', 'libtorch', 'c', 'would', 'create', 'forward', 'function', 'defining', 'model', 'would', 'let', 'whatever', 'input', 'output', 'sizes', 'want', 'find', 'make', 'equivalent', 'tensorflow', 'python', 'even', 'possible', 'tips', 'appreciated', 'thanks']","['neural', 'network', 'model', 'share', 'fix', 'weight', 'across', 'variabl', 'number', 'inputsspeci', 'machin', 'learn', 'problem', 'come', 'weird', 'solut', 'entir', 'sure', 'implement', 'tensorflow', 'python', 'tensorflow', 'python', 'problem', 'essenti', 'pick', 'best', 'option', 'option', 'repres', 'vector', 'length', 'train', 'data', 'organ', 'input', 'vector', 'length', 'option', 'option', 'length', 'label', 'one', 'hot', 'vector', 'length', 'wacki', 'part', 'might', 'model', 'need', 'abl', 'choos', 'number', 'option', 'dens', 'model', 'input', 'output', 'go', 'work', 'best', 'solut', 'think', 'evalu', 'individu', 'option', 'rather', 'full', 'set', 'make', 'model', 'input', 'output', 'output', 'model', 'confid', 'good', 'input', 'vector', 'pass', 'howev', 'mani', 'input', 'model', 'put', 'confid', 'valu', 'array', 'pass', 'array', 'softmax', 'function', 'get', 'output', 'train', 'model', 'input', 'output', 'combin', 'softmax', 'stuff', 'afterward', 'numpi', 'train', 'data', 'valu', 'map', 'confid', 'valu', 'n', 'valu', 'map', 'one', 'hot', 'vector', 'size', 'n', 'need', 'train', 'set', 'model', 'need', 'abl', 'handl', 'set', 'clue', 'tensorflow', 'far', 'tell', 'kera', 'forc', 'build', 'model', 'fix', 'input', 'output', 'size', 'even', 'though', 'nice', 'function', 'api', 'let', 'variabl', 'input', 'output', 'size', 'libtorch', 'c', 'would', 'creat', 'forward', 'function', 'defin', 'model', 'would', 'let', 'whatev', 'input', 'output', 'size', 'want', 'find', 'make', 'equival', 'tensorflow', 'python', 'even', 'possibl', 'tip', 'appreci', 'thank']"
229,245,245,19480081,72880854,How to balance a dataset using SCUT and the scutr-package in R,"<p>I was given a machine learning project in R by a colleague who can no longer work on it. I am currently trying to balance the used dataset with the SCUT function in the scutr package and I keep running into the following Problem:</p>
<p>The project I am working with contains the base dataset, formatted as a standard dataframe that contains different information on different YouTube channels (URL, name, description, etc.) and also a classification of 4 classes (hkgeschlecht). The classification is numerical, some of the other information as well, but the channel description for example is a text:</p>
<pre><code>'data.frame':   199 obs. of  6 variables:
 $ ctitle       : chr  &quot;Gaming Kati&quot; &quot;EinfallsReich&quot; &quot;Frank / Generation - E&quot; &quot;Gladiator Glubschi&quot; ...
 $ cdescr       : chr  &quot;Dieser Kanal ist einfach ein Kanal von einem Mdel, welches einfach im Animal Crossing hype ist &lt;U+0001F61D&gt;&lt;U+&quot;| __truncated__ &quot;Kurze und EinfallsReiche Fakten Videos mit folgenden Themen:\n\n                                               &quot;| __truncated__ &quot;Ich bin Frank aus Hamburg...\n\n...glcklicher Ehemann und Vater von zwei fantastischen Jungs. \n\nZu meinen gr&quot;| __truncated__ &quot;Gladiator Glubschi\n- Ein Glubschi\n- Zwei krasse Kanle\n- Drei Unterhaltung!\n\nUnd damit erstmal danke frs &quot;| __truncated__ ...
 $ cthumbnailurl: chr  &quot;https://yt3.ggpht.com/a/AATXAJwsWCPoVZ6g-uk_9UbMU3NqOU-QuoQyunPoYg=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJxunaT5qD2CbS7AQodCYq-HDOVee87NYBnRnw=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJzaeY6aZJuWpCsa8ul1CXHmQ1bC6reTWk9mTw=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJx0pmglui0v3YZblGuT1yOdNTm33qVP7mLXxQ=s240-c-k-c0xffffffff-no-rj-mo&quot; ...
 $ cviews       : int  1348087 2764 229744 15556 1884 1077314 158044 113570 25495 2364116 ...
 $ csubscriber  : int  13000 0 1140 320 0 7940 623 823 406 34700 ...
 $ hkgeschlecht : num  2 99 1 1 1 2 1 1 1 2 ...
</code></pre>
<p>The project uses a Naive Bayes Classifier and thus the channel description (sdescr) in the dataframe is transformed into a document feature matrix dfm which then is split into a training dataset and test dataset. This all works out fine and the model gives me decent predictions.</p>
<p>However the main dataset is unbalanced as one class is much more dominant than the others. I now want to balance this dataset using the SCUT-method so that the prediction of the minority classes improves. I had planned on using the <a href=""https://mran.microsoft.com/web/packages/scutr/scutr.pdf"" rel=""nofollow noreferrer"">scutr package</a> and the SCUT function in it since it is seems fairly straight forward.</p>
<p>Now my problem is, if I apply the function to main dataset like this:</p>
<pre><code>ret &lt;- SCUT(mldata, &quot;hkgeschlecht&quot;, oversample = oversample_smote, undersample = undersample_hclust,)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error in get.knnx(data, query, k, algorithm) : Data non-numeric</p>
</blockquote>
<p>I assume that is due to the differently formated variables in the dataframe.</p>
<p>But if I try to apply it only to the training dataset like this:</p>
<pre><code>ret &lt;- SCUT(testdfm1, testdfm1@docvars$docvars, oversample = oversample_smote, undersample = undersample_hclust,)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error in validate_dataset(data, cls_col) :
Column not found in data: 22211299112111312111122333311133211111111</p>
</blockquote>
<p>Which I assume is due to the SCUT function needing a dataframe format and not a document feature matrix.</p>
<p>My question thus is: How I can apply the SCUT method in this case? Is there a way to make the function work with a document feature matrix, say to get it to recognize the column with the classification? Would that even make sense? Or do I have to go about it in a completely different way?</p>
",22,0,0,4,r;dataframe;machine-learning;matrix,2022-07-06 12:16:41,2022-07-06 12:16:41,2022-07-12 23:06:23,i was given a machine learning project in r by a colleague who can no longer work on it  i am currently trying to balance the used dataset with the scut function in the scutr package and i keep running into the following problem  the project i am working with contains the base dataset  formatted as a standard dataframe that contains different information on different youtube channels  url  name  description  etc   and also a classification of  classes  hkgeschlecht   the classification is numerical  some of the other information as well  but the channel description for example is a text  the project uses a naive bayes classifier and thus the channel description  sdescr  in the dataframe is transformed into a document feature matrix dfm which then is split into a training dataset and test dataset  this all works out fine and the model gives me decent predictions  however the main dataset is unbalanced as one class is much more dominant than the others  i now want to balance this dataset using the scut method so that the prediction of the minority classes improves  i had planned on using the  and the scut function in it since it is seems fairly straight forward  now my problem is  if i apply the function to main dataset like this  i get this error  error in get knnx data  query  k  algorithm    data non numeric i assume that is due to the differently formated variables in the dataframe  but if i try to apply it only to the training dataset like this  i get this error  which i assume is due to the scut function needing a dataframe format and not a document feature matrix  my question thus is  how i can apply the scut method in this case  is there a way to make the function work with a document feature matrix  say to get it to recognize the column with the classification  would that even make sense  or do i have to go about it in a completely different way ,how to balance a dataset using scut and the scutr package in r,given machine learning project r colleague longer work currently trying balance used dataset scut function scutr package keep running following problem project working contains base dataset formatted standard dataframe contains different information different youtube channels url name description etc also classification classes hkgeschlecht classification numerical information well channel description example text project uses naive bayes classifier thus channel description sdescr dataframe transformed document feature matrix dfm split training dataset test dataset works fine model gives decent predictions however main dataset unbalanced one class much dominant others want balance dataset using scut method prediction minority classes improves planned using scut function since seems fairly straight forward problem apply function main dataset like get error error get knnx data query k algorithm data non numeric assume due differently formated variables dataframe try apply training dataset like get error assume due scut function needing dataframe format document feature matrix question thus apply scut method case way make function work document feature matrix say get recognize column classification would even make sense go completely different way,balance dataset using scut scutr package r,balance dataset using scut scutr package rgiven machine learning project r colleague longer work currently trying balance used dataset scut function scutr package keep running following problem project working contains base dataset formatted standard dataframe contains different information different youtube channels url name description etc also classification classes hkgeschlecht classification numerical information well channel description example text project uses naive bayes classifier thus channel description sdescr dataframe transformed document feature matrix dfm split training dataset test dataset works fine model gives decent predictions however main dataset unbalanced one class much dominant others want balance dataset using scut method prediction minority classes improves planned using scut function since seems fairly straight forward problem apply function main dataset like get error error get knnx data query k algorithm data non numeric assume due differently formated variables dataframe try apply training dataset like get error assume due scut function needing dataframe format document feature matrix question thus apply scut method case way make function work document feature matrix say get recognize column classification would even make sense go completely different way,"['balance', 'dataset', 'using', 'scut', 'scutr', 'package', 'rgiven', 'machine', 'learning', 'project', 'r', 'colleague', 'longer', 'work', 'currently', 'trying', 'balance', 'used', 'dataset', 'scut', 'function', 'scutr', 'package', 'keep', 'running', 'following', 'problem', 'project', 'working', 'contains', 'base', 'dataset', 'formatted', 'standard', 'dataframe', 'contains', 'different', 'information', 'different', 'youtube', 'channels', 'url', 'name', 'description', 'etc', 'also', 'classification', 'classes', 'hkgeschlecht', 'classification', 'numerical', 'information', 'well', 'channel', 'description', 'example', 'text', 'project', 'uses', 'naive', 'bayes', 'classifier', 'thus', 'channel', 'description', 'sdescr', 'dataframe', 'transformed', 'document', 'feature', 'matrix', 'dfm', 'split', 'training', 'dataset', 'test', 'dataset', 'works', 'fine', 'model', 'gives', 'decent', 'predictions', 'however', 'main', 'dataset', 'unbalanced', 'one', 'class', 'much', 'dominant', 'others', 'want', 'balance', 'dataset', 'using', 'scut', 'method', 'prediction', 'minority', 'classes', 'improves', 'planned', 'using', 'scut', 'function', 'since', 'seems', 'fairly', 'straight', 'forward', 'problem', 'apply', 'function', 'main', 'dataset', 'like', 'get', 'error', 'error', 'get', 'knnx', 'data', 'query', 'k', 'algorithm', 'data', 'non', 'numeric', 'assume', 'due', 'differently', 'formated', 'variables', 'dataframe', 'try', 'apply', 'training', 'dataset', 'like', 'get', 'error', 'assume', 'due', 'scut', 'function', 'needing', 'dataframe', 'format', 'document', 'feature', 'matrix', 'question', 'thus', 'apply', 'scut', 'method', 'case', 'way', 'make', 'function', 'work', 'document', 'feature', 'matrix', 'say', 'get', 'recognize', 'column', 'classification', 'would', 'even', 'make', 'sense', 'go', 'completely', 'different', 'way']","['balanc', 'dataset', 'use', 'scut', 'scutr', 'packag', 'rgiven', 'machin', 'learn', 'project', 'r', 'colleagu', 'longer', 'work', 'current', 'tri', 'balanc', 'use', 'dataset', 'scut', 'function', 'scutr', 'packag', 'keep', 'run', 'follow', 'problem', 'project', 'work', 'contain', 'base', 'dataset', 'format', 'standard', 'datafram', 'contain', 'differ', 'inform', 'differ', 'youtub', 'channel', 'url', 'name', 'descript', 'etc', 'also', 'classif', 'class', 'hkgeschlecht', 'classif', 'numer', 'inform', 'well', 'channel', 'descript', 'exampl', 'text', 'project', 'use', 'naiv', 'bay', 'classifi', 'thu', 'channel', 'descript', 'sdescr', 'datafram', 'transform', 'document', 'featur', 'matrix', 'dfm', 'split', 'train', 'dataset', 'test', 'dataset', 'work', 'fine', 'model', 'give', 'decent', 'predict', 'howev', 'main', 'dataset', 'unbalanc', 'one', 'class', 'much', 'domin', 'other', 'want', 'balanc', 'dataset', 'use', 'scut', 'method', 'predict', 'minor', 'class', 'improv', 'plan', 'use', 'scut', 'function', 'sinc', 'seem', 'fairli', 'straight', 'forward', 'problem', 'appli', 'function', 'main', 'dataset', 'like', 'get', 'error', 'error', 'get', 'knnx', 'data', 'queri', 'k', 'algorithm', 'data', 'non', 'numer', 'assum', 'due', 'differ', 'format', 'variabl', 'datafram', 'tri', 'appli', 'train', 'dataset', 'like', 'get', 'error', 'assum', 'due', 'scut', 'function', 'need', 'datafram', 'format', 'document', 'featur', 'matrix', 'question', 'thu', 'appli', 'scut', 'method', 'case', 'way', 'make', 'function', 'work', 'document', 'featur', 'matrix', 'say', 'get', 'recogn', 'column', 'classif', 'would', 'even', 'make', 'sens', 'go', 'complet', 'differ', 'way']"
230,247,247,18543927,72937452,ImportError: dlopen(...): Library not loaded: @rpath/_pywrap_tensorflow_internal.so,"<p>I am a beginner at machine learning. I try to use LSTM algorism but when I write</p>
<p><code>from keras.models import Sequential</code></p>
<p>it shows error as below:</p>
<pre><code>ImportError: dlopen(/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so, 2): Library not loaded: @rpath/_pywrap_tensorflow_internal.so
  Referenced from: /Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
  Reason: image not found
</code></pre>
<p>How can I fix this? Thank you so much!</p>
<p>full error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/wangzifan/Desktop/machine/LSTM.py&quot;, line 39, in &lt;module&gt;
    from keras.models import Sequential
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/keras/__init__.py&quot;, line 21, in &lt;module&gt;
    from tensorflow.python import tf2
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.eager import context
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 33, in &lt;module&gt;
    from tensorflow.python import pywrap_tfe
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/pywrap_tfe.py&quot;, line 25, in &lt;module&gt;
    from tensorflow.python._pywrap_tfe import *
ImportError: dlopen(/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so, 2): Library not loaded: @rpath/_pywrap_tensorflow_internal.so
  Referenced from: /Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
  Reason: image not found

</code></pre>
",27,1,0,5,python;tensorflow;keras;pip;lstm,2022-07-11 13:56:50,2022-07-11 13:56:50,2022-07-12 19:37:14,i am a beginner at machine learning  i try to use lstm algorism but when i write from keras models import sequential it shows error as below  how can i fix this  thank you so much  full error message ,importerror  dlopen       library not loaded   rpath _pywrap_tensorflow_internal so,beginner machine learning try use lstm algorism write keras models import sequential shows error fix thank much full error message,importerror dlopen library loaded rpath _pywrap_tensorflow_internal,importerror dlopen library loaded rpath _pywrap_tensorflow_internalbeginner machine learning try use lstm algorism write keras models import sequential shows error fix thank much full error message,"['importerror', 'dlopen', 'library', 'loaded', 'rpath', '_pywrap_tensorflow_internalbeginner', 'machine', 'learning', 'try', 'use', 'lstm', 'algorism', 'write', 'keras', 'models', 'import', 'sequential', 'shows', 'error', 'fix', 'thank', 'much', 'full', 'error', 'message']","['importerror', 'dlopen', 'librari', 'load', 'rpath', '_pywrap_tensorflow_internalbeginn', 'machin', 'learn', 'tri', 'use', 'lstm', 'algor', 'write', 'kera', 'model', 'import', 'sequenti', 'show', 'error', 'fix', 'thank', 'much', 'full', 'error', 'messag']"
231,248,248,17126954,72936240,identifying miss classified values in confusion matrix in R,"<p>I am using the caret package along with the confusionMatrix function and I would like to know if it is possible to know which are the exact values that were not clasified properly.</p>
<p>Here is a subset of my train data</p>
<pre><code>train_sub &lt;- structure(
  list(
    corr = c(
      0.629922866893549,
      0.632354159559817,
      0.656112138936032,
      0.4469719807955,
      0.598136079870775,
      0.314461239093862,
      0.379065842199838,
      0.347331370037428,
      0.310270891798492,
      0.361064451331448,
      0.335628455451358
    ),
    rdist = c(
      0.775733824285612,
      0.834148208687529,
      0.884167982488944,
      0.633989717138057,
      0.850225777237626,
      0.626197919283803,
      0.649597055761598,
      0.680382136363523,
      0.627828985862852,
      0.713674404108905,
      0.646094473468118
    ),
    CCF2 = c(
      0.634465565134314,
      0.722096802135009,
      0.792385621105087,
      0.46497582143802,
      0.739612023831014,
      0.470724554509749,
      0.505961260826622,
      0.527876803999064,
      0.461724328071479,
      0.564117580569802,
      0.490084457081904
    ),
    Wcorr = c(
      0.629,
      0.613,
      0.812,
      0.424,
      0.593,
      0.36,
      0.346,
      0.286,
      0.333,
      0.381,
      0.333
    ),
    Wcorr2 = c(
      0.735,
      0.743,
      0.802,
      0.588,
      0.691,
      0.632,
      0.61,
      0.599,
      0.599,
      0.632,
      0.613
    ),
    Wcorr3 = c(
      0.21,
      0.301,
      0.421,
      -0.052,
      0.169,
      -0.032,
      -0.042,-0.048,
      -0.035,
      0.006,
      -0.004
    ),
    Var = c(&quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;,
            &quot;W&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;)
  ),
  row.names = c(1L, 2L,
                3L, 5L, 7L, 214L, 215L, 216L, 217L, 218L, 221L),
  class = &quot;data.frame&quot;
)
</code></pre>
<p>and here is a subset of my test data</p>
<pre><code>test_sub &lt;- structure(
  list(
    corr = c(
      0.636658204667785,
      0.5637857758104,
      0.540558984461647,
      0.392647603023863,
      0.561801911406989,
      0.297187412065481,
      0.278864501603015,
      0.505277007007347,
      0.403811785308709,
      0.510158398354856,
      0.459607853624603
    ),
    rdist = c(
      0.887270722679019,
      0.843656768956754,
      0.815806338767273,
      0.732093571145576,
      0.832944903081762,
      0.485497073465096,
      0.454461718498521,
      0.69094669881886,
      0.627667080657035,
      0.705558894672344,
      0.620838398507191
    ),
    CCF2 = c(
      0.802017782695131,
      0.731763898271157,
      0.689402284804853,
      0.577932997250877,
      0.715111899030751,
      0.324826043263382,
      0.298456267077388,
      0.544808216945995,
      0.458148923874818,
      0.551160266327893,
      0.461228649848996
    ),
    Wcorr = c(
      0.655,
      0.536,
      0.677,
      0.556,
      0.571,
      0.29,
      0.25,
      0.484,
      0.25,
      0.515,
      0.314
    ),
    Wcorr2 = c(
      0.779,
      0.682,
      0.734,
      0.675,
      0.736,
      0.5,
      0.529,
      0.611,
      0.555,
      0.639,
      0.572
    ),
    Wcorr3 = c(
      0.368,
      0.154,
      0.266,
      0.103,
      0.224,
      -0.204,
      -0.16,
      -0.026,
      -0.149,
      0.032,
      -0.097
    ),
    Var = c(&quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;,
            &quot;B&quot;)
  ),
  row.names = c(4L, 6L, 8L, 13L, 15L, 321L, 322L, 329L,
                334L, 341L, 344L),
  class = &quot;data.frame&quot;
)
</code></pre>
<p>When I use this line,</p>
<pre><code>confusionMatrix(reference=as.factor(test$Var),data=fittedTL,mode = &quot;everything&quot;)
</code></pre>
<p>With this I compute some machine learning using glmnet method (it gives the best accuracy ini my case)</p>
<pre><code>classCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10,repeats=5,classProbs =  TRUE,savePredictions = &quot;final&quot;)
set.seed(355)
glmnetTL &lt;- train(Var~., train_sub, method= &quot;glmnet&quot;,   trControl=classCtrl)
glmnetTL
</code></pre>
<p>And finally I compute the confusion matrix on my test set:</p>
<pre><code>predict_glmnet &lt;- predict(glmnetTL,test_sub)
predict_glmnet

CM_glmnet &lt;- confusionMatrix(reference=as.factor(test_sub$Var),data=predict_glmnet,mode = &quot;everything&quot;)
CM_glmnet
</code></pre>
<p>The output of the confusion matrix is a table like so</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: center;"">B</th>
<th style=""text-align: right;"">W</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">W</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">5</td>
</tr>
</tbody>
</table>
</div>
<p>So here I have two predictions/classifications that are not good.</p>
<p>Is there any way I can traceback to which row of my test set it corresponds ?</p>
",27,0,0,3,r;r-caret;confusion-matrix,2022-07-11 12:20:10,2022-07-11 12:20:10,2022-07-12 17:24:26,i am using the caret package along with the confusionmatrix function and i would like to know if it is possible to know which are the exact values that were not clasified properly  here is a subset of my train data and here is a subset of my test data when i use this line  with this i compute some machine learning using glmnet method  it gives the best accuracy ini my case  and finally i compute the confusion matrix on my test set  the output of the confusion matrix is a table like so so here i have two predictions classifications that are not good  is there any way i can traceback to which row of my test set it corresponds  ,identifying miss classified values in confusion matrix in r,using caret package along confusionmatrix function would like know possible know exact values clasified properly subset train data subset test data use line compute machine learning using glmnet method gives best accuracy ini case finally compute confusion matrix test set output confusion matrix table like two predictions classifications good way traceback row test set corresponds,identifying miss classified values confusion matrix r,identifying miss classified values confusion matrix rusing caret package along confusionmatrix function would like know possible know exact values clasified properly subset train data subset test data use line compute machine learning using glmnet method gives best accuracy ini case finally compute confusion matrix test set output confusion matrix table like two predictions classifications good way traceback row test set corresponds,"['identifying', 'miss', 'classified', 'values', 'confusion', 'matrix', 'rusing', 'caret', 'package', 'along', 'confusionmatrix', 'function', 'would', 'like', 'know', 'possible', 'know', 'exact', 'values', 'clasified', 'properly', 'subset', 'train', 'data', 'subset', 'test', 'data', 'use', 'line', 'compute', 'machine', 'learning', 'using', 'glmnet', 'method', 'gives', 'best', 'accuracy', 'ini', 'case', 'finally', 'compute', 'confusion', 'matrix', 'test', 'set', 'output', 'confusion', 'matrix', 'table', 'like', 'two', 'predictions', 'classifications', 'good', 'way', 'traceback', 'row', 'test', 'set', 'corresponds']","['identifi', 'miss', 'classifi', 'valu', 'confus', 'matrix', 'ruse', 'caret', 'packag', 'along', 'confusionmatrix', 'function', 'would', 'like', 'know', 'possibl', 'know', 'exact', 'valu', 'clasifi', 'properli', 'subset', 'train', 'data', 'subset', 'test', 'data', 'use', 'line', 'comput', 'machin', 'learn', 'use', 'glmnet', 'method', 'give', 'best', 'accuraci', 'ini', 'case', 'final', 'comput', 'confus', 'matrix', 'test', 'set', 'output', 'confus', 'matrix', 'tabl', 'like', 'two', 'predict', 'classif', 'good', 'way', 'traceback', 'row', 'test', 'set', 'correspond']"
232,249,249,19534112,72952963,Neural network doesn&#39;t classify sleep EEG recordings,"<p>We have a problem with the code that I'm running in an academic course and I'd appreciate getting some help from someone on the forum.
We're trying to train a CNN model to classify EEG sleep recordings as male or female. It's something that was done in some papers but we use a different dataset in our course. The problem is that the network doesn't learn no matter what we do - we tried changing the number of the layers, the size of each layer, the learning rate, the number of epochs, the batch sizem the optimizer and also adding data.
We also tried using an RNN with a GRU (Gated Recurrent Unit) instead of a CNN but it didn't help.
Here are some examples of the network not learning:</p>
<p><a href=""https://i.stack.imgur.com/kr8yD.png"" rel=""nofollow noreferrer"">Example 1</a>
<a href=""https://i.stack.imgur.com/WYUuG.png"" rel=""nofollow noreferrer"">Example 2</a></p>
<p>Note that the &quot;test&quot; dataset is actually validation.</p>
<p>We can't find any problem with the machine learning part of our code. We think that maybe the problem is with the data processing part but we're not sure so we wanted someone to check if he/she can find a problem with the machine learning part.
Before I should say that we have 200 8-hour recordings of sleep EEG obtained from 200 patients. These are 100 Hz recording. The shape of each sample is 4X2X1000 (4 batches X 2 EEG channels for each recording X 1000 voltage values representing 10 seconds of recording).</p>
<p>Here's the machine learning part (I post a lot of code in case someone says that more code is needed...):</p>
<pre><code>import _pickle
import contextlib
import io
import torch
import os
import numpy as np
from aux_eegproj_funcs_simplified import *
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import torchviz
import mne
</code></pre>
<pre><code># DATASET
table_name = 'sleep_1_10sec.csv' # name of the table from which we take the beginning and end time of each sample (subrecording)
eeg_ds = EEGDataset(EEGTransform, exp_table=build_experiment_tbl(table_name), filtered=0)  # EEGDataSet and EEGTransform are a class
batch_sz = 4 # batch size

# splitting the data to train, validation and test
dl_train, dl_val, dl_test = tt_split_by_pid_mf(dataset=eeg_ds, batch_size=batch_sz, train_rt=.8,  num_workers=0, verbose=1)
nF = sum(eeg_ds.table['sex (F=1)'][dl_train.sampler.indices] == 1)  # number of females
nM = sum(eeg_ds.table['sex (F=1)'][dl_train.sampler.indices] == 2)  # number of males
wF = nM / (nF + nM)  # females percentage
wM = nF / (nF + nM)  # males percentage
</code></pre>
<p>The model:</p>
<pre><code>class eegSexNet(nn.Module):  # Define a network to classify Sex
    def __init__(self, input_shape):
        &quot;&quot;&quot;
        :param input_shape: input tensor shape - every batch size will be ok as it is used to compute the FCs input size.
        &quot;&quot;&quot;
        super().__init__()
        # Define the CNN layers in a nn.Sequential.
        # Remember to use the number of input channels as the first layer input shape.
        self.CNN = nn.Sequential(
            nn.Conv1d(in_channels=input_shape[1], out_channels=8, kernel_size=5, stride=1, padding=0, dilation=2),
            # TODO try changing the kernel sizes they were 3
            nn.ReLU(),
            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=2),
            nn.ReLU(),
            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0, dilation=2),
            nn.ReLU(),
            Residual(in_channels=32)
        )

        # Compute the CNN output size here to use as the input size for the fully-connected part.
        CNN_forward = self.CNN(torch.zeros(input_shape))

        self.FCs = nn.Sequential(
            nn.Linear(CNN_forward.shape[1] * CNN_forward.shape[2], 10),
            nn.ReLU(),
            nn.Linear(10, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # ------Your code------#
        # Forward through the CNN by passing x, flatten and then forward through the linears.
        features = self.CNN(x)
        features = features.view(features.size(0), -1)  # reshape/flatten
        scores = self.FCs(features)
        # ------^^^^^^^^^------#
        return torch.squeeze(scores)
</code></pre>
<p>Residual block used in the class of the model:</p>
<pre><code>class Residual(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # Define self.direct_path by adding the layers into a nn.Sequential. Use nn.Conv1d and nn.Relu.
        # You can use padding to avoid reducing L size, to allow the skip-connection adding.
        self.direct_path = nn.Sequential(
            nn.Conv1d(in_channels=in_channels, out_channels=16, kernel_size=7, padding=3),
            nn.ReLU(),
            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=7, padding=3)
        )
        # You should use convolution layer with a kernel size of 1 to consider the case where the input and output shapes mismatch.
        skip_layers = []
        if in_channels != 32:  # HOW DOES THIS PART WORK? When are you adding the layers
            skip_layers.append(
                nn.Conv1d(in_channels=in_channels, out_channels=32, kernel_size=1, stride=1, padding=0, dilation=1,
                          bias=False)
            )
        else:
            self.skip_path = nn.Sequential(*skip_layers)

    def forward(self, x):
        # Compute the two paths and add the results to each other, then use ReLU (torch.relu) to activate the output.
        direct_output = self.direct_path(x)
        skip_output = self.skip_path(x)
        activated_output = torch.relu(direct_output + skip_output)
        return activated_output
</code></pre>
<p>Training loop:</p>
<pre><code>def Train_Sex_Net(epochs=n_epochs, fn='None', optimizer=opt_sex, loss_function=bce):  # Training SEXNET
    global sex_net
    gpu_0 = torch.device(1)
    label = 0  # select the sex label
    train_loss_vec = []
    test_loss_vec = []
    train_acc_vec = []
    test_acc_vec = []
    for i_epoch in range(epochs):
        train_loss = 0
        test_loss = 0
        # Train set
        train_loss, y_true_train, y_pred_train = forward_epoch(sex_net, dl_train, loss_function, optimizer,
                                                                wM, train_loss,
                                                               to_train=True, desc='Train', device=gpu_0, label=label)
        # Test set
        test_loss, y_true_test, y_pred_test = forward_epoch(sex_net, dl_test, loss_function, optimizer, wM, test_loss,
                                                            to_train=False, desc='Test', device=gpu_0, label=label)

        # Metrics:
        train_loss = train_loss / len(dl_train)  # we want to get the mean over batches.
        test_loss = test_loss / len(dl_test)
        train_loss_vec.append(train_loss)
        test_loss_vec.append(test_loss)

        train_accuracy = accuracy_score(y_true_train.cpu(),
                                        (y_pred_train.cpu().detach() &gt; 0.5) * 1)
        test_accuracy = accuracy_score(y_true_test.cpu(),
                                       (y_pred_test.cpu().detach() &gt; 0.5) * 1)

        train_acc_vec.append(train_accuracy)
        test_acc_vec.append(test_accuracy)

        print('\n')
        print(f'train_loss={round(train_loss, 3)}; train_accuracy={round(train_accuracy, 3)} \
              test_loss={round(test_loss, 3)}; test_accuracy={round(test_accuracy, 3)}')

    return (train_loss_vec, train_acc_vec), (test_loss_vec, test_acc_vec) # (val_loss_vec, val_acc_vec)

</code></pre>
<p>Called by the training loop:</p>
<pre><code>def forward_epoch(model, dl, loss_function, optimizer, weight, total_loss=0,
                  to_train=False, desc=None, device=torch.device('cpu'), label=0): # Training loop
    # label =0 is for sex
    # label = 1 is for Age
    # total loss is over the entire epoch
    # y_trues is by patient for the entire epoch; can get last batch with [-batch_size]
    # y_preds is by patient for the entire epoch
    #
    with tqdm(total=len(dl), desc=desc, ncols=100) as pbar:
        model = model.double().to(device)  # solving runtime memory issue

        y_trues = torch.empty(0).type(torch.int).to(device)
        y_preds = torch.empty(0).type(torch.int).to(device)
        for i_batch, (X, y) in enumerate(dl):
            X = X.to(device)
            X = X.type(torch.double)
            y = y[label].to(device)  # added index because of get label returning sex, age
            y_pred = model(X)  # Forward
            y_true = y.type(torch.double)  # Loss:
            y_true_copy = torch.clone(y_true)
            loss = loss_function(y_pred, y_true)  # loss of one batch
            total_loss += loss.item()

            y_trues = torch.cat((y_trues, y_true))
            y_preds = torch.cat((y_preds, y_pred))
            if to_train:
                optimizer.zero_grad()  #  Backward:zero the gradients to not accumulate their changes.
                loss.backward()  # get gradients
                optimizer.step()  # Optimization step: use gradients
            pbar.update(1)  # Progress bar

    return total_loss, y_trues, y_preds
</code></pre>
<p>Calling all the functions:</p>
<pre><code>sex_net = eegSexNet(torch.Size([4, 2, 1000]))  # Instantiate the network

learning_rate = 0.0001
opt_sex = torch.optim.Adam(params=sex_net.parameters(), lr=learning_rate)  # Optimizer for eegnet
bce = nn.BCELoss()
n_epochs = 6
f0 = 'sex_k7'  # file format '.pickle' added automatically
train_res, test_res = Train_Sex_Net(epochs=n_epochs, fn=f0)
</code></pre>
<p>Does anyone see anything that might be wrong in the code? Or maybe the problem is in the data processing and choosing part that I didn't show?</p>
",55,1,0,4,python;machine-learning;neural-network;pytorch,2022-07-12 16:17:11,2022-07-12 16:17:11,2022-07-12 17:00:34,note that the  test  dataset is actually validation  here s the machine learning part  i post a lot of code in case someone says that more code is needed      the model  residual block used in the class of the model  training loop  called by the training loop  calling all the functions  does anyone see anything that might be wrong in the code  or maybe the problem is in the data processing and choosing part that i didn t show ,neural network doesn   t classify sleep eeg recordings,note test dataset actually validation machine learning part post lot code case someone says code needed model residual block used class model training loop called training loop calling functions anyone see anything might wrong code maybe problem data processing choosing part show,neural network classify sleep eeg recordings,neural network classify sleep eeg recordingsnote test dataset actually validation machine learning part post lot code case someone says code needed model residual block used class model training loop called training loop calling functions anyone see anything might wrong code maybe problem data processing choosing part show,"['neural', 'network', 'classify', 'sleep', 'eeg', 'recordingsnote', 'test', 'dataset', 'actually', 'validation', 'machine', 'learning', 'part', 'post', 'lot', 'code', 'case', 'someone', 'says', 'code', 'needed', 'model', 'residual', 'block', 'used', 'class', 'model', 'training', 'loop', 'called', 'training', 'loop', 'calling', 'functions', 'anyone', 'see', 'anything', 'might', 'wrong', 'code', 'maybe', 'problem', 'data', 'processing', 'choosing', 'part', 'show']","['neural', 'network', 'classifi', 'sleep', 'eeg', 'recordingsnot', 'test', 'dataset', 'actual', 'valid', 'machin', 'learn', 'part', 'post', 'lot', 'code', 'case', 'someon', 'say', 'code', 'need', 'model', 'residu', 'block', 'use', 'class', 'model', 'train', 'loop', 'call', 'train', 'loop', 'call', 'function', 'anyon', 'see', 'anyth', 'might', 'wrong', 'code', 'mayb', 'problem', 'data', 'process', 'choos', 'part', 'show']"
233,250,250,19520661,72933415,ROOT&#39;s TMVA user guide,"<p>I am using ROOT's TMVA (developed by CERN), the version of the ROOT is 6.24.
the user manual i have is for TMVA version 4.3.0 (for ROOT &gt;= 6.12/00 on May 26, 2020)
but the manual seems to be a little bit different from my current version (for example, the options available for a particular machine learning model).</p>
<p>is there any updated user manual, or portals that provide guides on the options available for a particular machine learning model.</p>
",24,1,0,1,root-framework,2022-07-11 06:30:43,2022-07-11 06:30:43,2022-07-12 16:51:37,is there any updated user manual  or portals that provide guides on the options available for a particular machine learning model ,root   s tmva user guide,updated user manual portals provide guides options available particular machine learning model,root tmva user guide,root tmva user guideupdated user manual portals provide guides options available particular machine learning model,"['root', 'tmva', 'user', 'guideupdated', 'user', 'manual', 'portals', 'provide', 'guides', 'options', 'available', 'particular', 'machine', 'learning', 'model']","['root', 'tmva', 'user', 'guideupd', 'user', 'manual', 'portal', 'provid', 'guid', 'option', 'avail', 'particular', 'machin', 'learn', 'model']"
234,251,251,5654564,38380795,"pandas read_json: &quot;If using all scalar values, you must pass an index&quot;","<p>I have some difficulty in importing a JSON file with pandas.</p>

<pre><code>import pandas as pd
map_index_to_word = pd.read_json('people_wiki_map_index_to_word.json')
</code></pre>

<p>This is the  error that I get: </p>

<pre><code>ValueError: If using all scalar values, you must pass an index
</code></pre>

<p>The file structure is simplified like this:</p>

<pre><code>{""biennials"": 522004, ""lb915"": 116290, ""shatzky"": 127647, ""woode"": 174106, ""damfunk"": 133206, ""nualart"": 153444, ""hatefillot"": 164111, ""missionborn"": 261765, ""yeardescribed"": 161075, ""theoryhe"": 521685}
</code></pre>

<p>It is from the machine learning course of University of Washington on Coursera. You can find the file <a href=""https://www.coursera.org/learn/ml-clustering-and-retrieval/supplement/gJSfc/choosing-features-and-metrics-for-nearest-neighbor-search"" rel=""noreferrer"">here</a>.</p>
",82030,7,56,3,python;json;pandas,2016-07-14 20:41:10,2016-07-14 20:41:10,2022-07-12 12:10:53,i have some difficulty in importing a json file with pandas  this is the  error that i get   the file structure is simplified like this  it is from the machine learning course of university of washington on coursera  you can find the file  ,pandas read_json   if using all scalar values  you must pass an index ,difficulty importing json file pandas error get file structure simplified like machine learning course university washington coursera find file,pandas read_json using scalar values must pass index,pandas read_json using scalar values must pass indexdifficulty importing json file pandas error get file structure simplified like machine learning course university washington coursera find file,"['pandas', 'read_json', 'using', 'scalar', 'values', 'must', 'pass', 'indexdifficulty', 'importing', 'json', 'file', 'pandas', 'error', 'get', 'file', 'structure', 'simplified', 'like', 'machine', 'learning', 'course', 'university', 'washington', 'coursera', 'find', 'file']","['panda', 'read_json', 'use', 'scalar', 'valu', 'must', 'pass', 'indexdifficulti', 'import', 'json', 'file', 'panda', 'error', 'get', 'file', 'structur', 'simplifi', 'like', 'machin', 'learn', 'cours', 'univers', 'washington', 'coursera', 'find', 'file']"
235,252,252,11053795,72945697,how to get pycharm to do GraphViz,"<p>The data for this comes from the UCI machine learning repository
Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.</p>
<p>This code runs without any technical error but whether I run it with None for out_file or give it a png file name it doesnt make the decision tree visual, instead it just prints it like this:</p>
<pre><code>digraph Tree {
node [shape=box, style=&quot;filled&quot;, color=&quot;black&quot;, fontname=&quot;helvetica&quot;] ;
edge [fontname=&quot;helvetica&quot;] ;
0 [label=&quot;income_ &gt;50K &lt;= 0.5\ngini = 0.366\nsamples = 32561\nvalue = [24720, 
7841]\nclass = i&quot;, fillcolor=&quot;#eda978&quot;] ;
1 [label=&quot;gini = 0.0\nsamples = 24720\nvalue = [24720, 0]\nclass = i&quot;, 
fillcolor=&quot;#e58139&quot;] ;
0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;
2 [label=&quot;gini = 0.0\nsamples = 7841\nvalue = [0, 7841]\nclass = n&quot;, 
fillcolor=&quot;#399de5&quot;] ;
0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;
}
</code></pre>
<p><strong>here is my code</strong></p>
<pre><code>csv_file = age, workclass, fnlwgt, education, educationnum, maritalstatus, occupation, 
relationship, race, sex, capitalgain, capitalloss, hoursperweek, nativecountry, income
39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, 
Male, 2174, 0, 40, United-States, &lt;=50K
50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, 
Husband, White, Male, 0, 0, 13, United-States, &lt;=50K
38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, 
Male, 0, 0, 40, United-States, &lt;=50K
53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, 
Male, 0, 0, 40, United-States, &lt;=50K


########## alternative 2 ##########
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
import graphviz

# basic dataframe
dataframe = pd.read_csv(csv_file)

# cleaned up dataframe
dataframe.columns = dataframe.columns.str.strip()

# one hot encoded dataframe
ohe = pd.get_dummies(dataframe, columns=dataframe.columns)

# decision tree classifier
clf = tree.DecisionTreeClassifier(max_leaf_nodes=3)
clf = clf.fit(ohe, ohe['income_ &gt;50K'])

# display for user
dot_data = tree.export_graphviz(clf, out_file=None,
                            feature_names=ohe.columns, class_names='income_ &gt;50K', 
filled=True)

graph = graphviz.Source(dot_data, format='png')

print(graph)
</code></pre>
<p>I am unsure how to proceed...</p>
",29,0,1,4,python-3.x;scikit-learn;pycharm;graphviz,2022-07-12 02:49:55,2022-07-12 02:49:55,2022-07-12 04:07:07,this code runs without any technical error but whether i run it with none for out_file or give it a png file name it doesnt make the decision tree visual  instead it just prints it like this  here is my code i am unsure how to proceed   ,how to get pycharm to do graphviz,code runs without technical error whether run none out_file give png file name doesnt make decision tree visual instead prints like code unsure proceed,get pycharm graphviz,get pycharm graphvizcode runs without technical error whether run none out_file give png file name doesnt make decision tree visual instead prints like code unsure proceed,"['get', 'pycharm', 'graphvizcode', 'runs', 'without', 'technical', 'error', 'whether', 'run', 'none', 'out_file', 'give', 'png', 'file', 'name', 'doesnt', 'make', 'decision', 'tree', 'visual', 'instead', 'prints', 'like', 'code', 'unsure', 'proceed']","['get', 'pycharm', 'graphvizcod', 'run', 'without', 'technic', 'error', 'whether', 'run', 'none', 'out_fil', 'give', 'png', 'file', 'name', 'doesnt', 'make', 'decis', 'tree', 'visual', 'instead', 'print', 'like', 'code', 'unsur', 'proceed']"
236,253,253,78259,648246,At what point does a config file become a programming language?,"<p>I have been mulling over config files and their relationship to code for a while now and depending on the day and direction of the wind my opinions seem to change.  More and more though I keep coming back to the realization I first had while learning Lisp: there is little difference between data and code.  This seems doubly true for config files.  When looked at in the right light a Perl script is little more than a config file for perl.  This tends to have fairly heavy consequences for tasks such as QA and divisions of labor like who should be responsible for changing config files.</p>

<p>The creep from config file to full fledged language is generally slow and seems to be driven by the desire to have a generic system.  Most projects seem to start out small with a few config items like where to write logs, where to look for data, user names and passwords, etc.  But then they start to grow: features start to be able to be turned on or off, the timings and order of operations start to be controlled, and, inevitably, someone wants to start adding logic to it (e.g. use 10 if the machine is X and 15 if the machine is Y).  At a certain point the config file becomes a domain specific language, and a poorly written one at that.</p>

<p>Now that I have rambled on to set the stage, here are my questions:</p>

<ol>
<li>What is the true purpose of a config
file?</li>
<li>Should an attempt be made to keep
config files simple?</li>
<li>Who should be responsible for making
changes to them (developers, users,
admins, etc.)?</li>
<li>Should they be source controlled
(see question 3)?</li>
</ol>

<p>As I said earlier my answers to these questions shift constantly, but right now I am thinking:</p>

<ol>
<li>to allow a non-programmers to change
large chunks of behaviour quickly</li>
<li>yes, anything that is not coarsely
grained should be in code</li>
<li>users should be responsible for
config files and programmers should
be responsible for a configuration
layer between config files and code
that gives more fine grained control
of the application</li>
<li>no, but the finer grained middle layer should be</li>
</ol>
",17212,19,102,4,configuration;programming-languages;configuration-files;config,2009-03-15 20:07:14,2009-03-15 20:07:14,2022-07-12 03:00:09,i have been mulling over config files and their relationship to code for a while now and depending on the day and direction of the wind my opinions seem to change   more and more though i keep coming back to the realization i first had while learning lisp  there is little difference between data and code   this seems doubly true for config files   when looked at in the right light a perl script is little more than a config file for perl   this tends to have fairly heavy consequences for tasks such as qa and divisions of labor like who should be responsible for changing config files  the creep from config file to full fledged language is generally slow and seems to be driven by the desire to have a generic system   most projects seem to start out small with a few config items like where to write logs  where to look for data  user names and passwords  etc   but then they start to grow  features start to be able to be turned on or off  the timings and order of operations start to be controlled  and  inevitably  someone wants to start adding logic to it  e g  use  if the machine is x and  if the machine is y    at a certain point the config file becomes a domain specific language  and a poorly written one at that  now that i have rambled on to set the stage  here are my questions  as i said earlier my answers to these questions shift constantly  but right now i am thinking ,at what point does a config file become a programming language ,mulling config files relationship code depending day direction wind opinions seem change though keep coming back realization first learning lisp little difference data code seems doubly true config files looked right light perl script little config file perl tends fairly heavy consequences tasks qa divisions labor like responsible changing config files creep config file full fledged language generally slow seems driven desire generic system projects seem start small config items like write logs look data user names passwords etc start grow features start able turned timings order operations start controlled inevitably someone wants start adding logic e g use machine x machine certain point config file becomes domain specific language poorly written one rambled set stage questions said earlier answers questions shift constantly right thinking,point config file become programming language,point config file become programming languagemulling config files relationship code depending day direction wind opinions seem change though keep coming back realization first learning lisp little difference data code seems doubly true config files looked right light perl script little config file perl tends fairly heavy consequences tasks qa divisions labor like responsible changing config files creep config file full fledged language generally slow seems driven desire generic system projects seem start small config items like write logs look data user names passwords etc start grow features start able turned timings order operations start controlled inevitably someone wants start adding logic e g use machine x machine certain point config file becomes domain specific language poorly written one rambled set stage questions said earlier answers questions shift constantly right thinking,"['point', 'config', 'file', 'become', 'programming', 'languagemulling', 'config', 'files', 'relationship', 'code', 'depending', 'day', 'direction', 'wind', 'opinions', 'seem', 'change', 'though', 'keep', 'coming', 'back', 'realization', 'first', 'learning', 'lisp', 'little', 'difference', 'data', 'code', 'seems', 'doubly', 'true', 'config', 'files', 'looked', 'right', 'light', 'perl', 'script', 'little', 'config', 'file', 'perl', 'tends', 'fairly', 'heavy', 'consequences', 'tasks', 'qa', 'divisions', 'labor', 'like', 'responsible', 'changing', 'config', 'files', 'creep', 'config', 'file', 'full', 'fledged', 'language', 'generally', 'slow', 'seems', 'driven', 'desire', 'generic', 'system', 'projects', 'seem', 'start', 'small', 'config', 'items', 'like', 'write', 'logs', 'look', 'data', 'user', 'names', 'passwords', 'etc', 'start', 'grow', 'features', 'start', 'able', 'turned', 'timings', 'order', 'operations', 'start', 'controlled', 'inevitably', 'someone', 'wants', 'start', 'adding', 'logic', 'e', 'g', 'use', 'machine', 'x', 'machine', 'certain', 'point', 'config', 'file', 'becomes', 'domain', 'specific', 'language', 'poorly', 'written', 'one', 'rambled', 'set', 'stage', 'questions', 'said', 'earlier', 'answers', 'questions', 'shift', 'constantly', 'right', 'thinking']","['point', 'config', 'file', 'becom', 'program', 'languagemul', 'config', 'file', 'relationship', 'code', 'depend', 'day', 'direct', 'wind', 'opinion', 'seem', 'chang', 'though', 'keep', 'come', 'back', 'realiz', 'first', 'learn', 'lisp', 'littl', 'differ', 'data', 'code', 'seem', 'doubli', 'true', 'config', 'file', 'look', 'right', 'light', 'perl', 'script', 'littl', 'config', 'file', 'perl', 'tend', 'fairli', 'heavi', 'consequ', 'task', 'qa', 'divis', 'labor', 'like', 'respons', 'chang', 'config', 'file', 'creep', 'config', 'file', 'full', 'fledg', 'languag', 'gener', 'slow', 'seem', 'driven', 'desir', 'gener', 'system', 'project', 'seem', 'start', 'small', 'config', 'item', 'like', 'write', 'log', 'look', 'data', 'user', 'name', 'password', 'etc', 'start', 'grow', 'featur', 'start', 'abl', 'turn', 'time', 'order', 'oper', 'start', 'control', 'inevit', 'someon', 'want', 'start', 'ad', 'logic', 'e', 'g', 'use', 'machin', 'x', 'machin', 'certain', 'point', 'config', 'file', 'becom', 'domain', 'specif', 'languag', 'poorli', 'written', 'one', 'rambl', 'set', 'stage', 'question', 'said', 'earlier', 'answer', 'question', 'shift', 'constantli', 'right', 'think']"
237,254,254,2451456,72409120,"Unity&#39;s ml-agents assets throw warnings and errors [PushBlockWithInput, Actuator, Barracuda]","<h2>The Problem</h2>
<p>I'm trying to work with <a href=""https://unity.com/products/machine-learning-agents"" rel=""nofollow noreferrer"">Unity Machine Learning Agents</a> and encountered problems during the setup. When I try to import the assets from <a href=""https://github.com/Unity-Technologies/ml-agents"" rel=""nofollow noreferrer"">Unity's ml-agents git</a> into Unity, I get many warnings and errors inside Unity. For the purpose of context, I'm at the very beginning of learning Unity, so I don't know if the errors are due to the ml-agents package or user error from my side in how to set everything up.</p>
<h2>The errors and warnings</h2>
<p>Instructions to create a first test scene with assets from Unity's ml-agents git suggest making a new 3D project in Unity and drag and drop the folder <code>projects/assets/ml-agents</code> into the project's assets. At this point, Unity is showing many errors and warnings in the Terminal. It still has the examples in the assets but every element in the scene is full of warnings.</p>
<p>according to these tutorials from 2020 by dragging and dropping the assets into Unity <a href=""https://www.youtube.com/watch?v=GhS6-vvhOy8&amp;t=469s"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://www.youtube.com/watch?v=_9aPZH6pyA8&amp;t=86s"" rel=""nofollow noreferrer"">[2]</a>, I subsequently
<a href=""https://i.stack.imgur.com/FIVsQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FIVsQ.jpg"" alt=""enter image description here"" /></a></p>
<p>In my case the majority of the warnings seem to come from <code>PushBlockWithInput</code>, <code>PushblockActions</code> and <code>PushBlockWithInputPlayerController</code> missing <code>UnityEngine.InputSystem</code> and <code>Unity.MLAgents.Extensions.Input</code> with the note &quot;(are you missing a using directive or an assembly reference?)&quot;. This; however, did not happen in the aforementioned tutorials.</p>
<p>Although they make the majority of errors, they are not exclusively about assembly references. Other errors, which may or may not be about assembly references, are</p>
<ul>
<li>error CS0115: 'Match3Board.GetCurrentBoardSize()': no suitable method found to override</li>
<li>error CS0535: 'SensorBase' does not implement interface member 'ISensor.GetCompressionType()'</li>
</ul>
<p><a href=""https://i.stack.imgur.com/dH4Gj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dH4Gj.jpg"" alt=""enter image description here"" /></a></p>
<h2>The things I've tried</h2>
<h3>Python</h3>
<p>I've followed the <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a> from Unity's ml-agents git and was successful in installing a Python 3.7 environment with Anaconda, PyTorch with Cuda, and the Unity's ml-agents python package via pip. When trying to verify the ml-agents python package works with <code>mlagents-learn --help</code>, I first got an exception but could resolve that by updating <code>protobuf==3.20.1</code> as per <a href=""https://discuss.streamlit.io/t/typeerror-descriptors-cannot-not-be-created-directly/25639/3"" rel=""nofollow noreferrer"">suggestion from a forum</a> (just mentioning this in case it is relevant).</p>
<h3>Unity</h3>
<p>I downloaded the C# package from Unity's package manager and tried it for several versions (<code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.1.0 (Preview) -- lastest</code>). After which I'm able to select ML-Agents from the 'Add Component' menu in the Inspector.</p>
<p>I've also tried to create a new Unity project with the <code>ml-agent package 1.9.1 (Preview)</code> with the right Barracuda version, and the release 19 branch of Unity's ml-agents git, without success (now it's 53 warnings and 70 errors). Now also the Actuators are not found, which seems to be a more common problem on its own.</p>
<h3>VS Code</h3>
<p>I'm using VS Code as opposed to VS as was recommended <a href=""https://stackoverflow.com/a/63301180/2451456"">here</a>. I downloaded .Net version 6.0.301 and checked it was installed with 'dotnet --info'. In the VS Code's extension manager, I installed the extensions <a href=""https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp"" rel=""nofollow noreferrer"">C#</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner"" rel=""nofollow noreferrer"">Code Runner</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=Tobiah.unity-tools"" rel=""nofollow noreferrer"">Unity Tools
</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug"" rel=""nofollow noreferrer"">Debugger for Unity</a>.</p>
<h3>Git-Repository</h3>
<p>I have also switched from Unity's ml-agents git's main branch to the <a href=""https://github.com/Unity-Technologies/ml-agents/tree/release_19_branch"" rel=""nofollow noreferrer"">release 19 branch</a> and also tried other versions of the Barracuda package, e.g. <code>Version 3.0.0</code>, which seems to remove the warnings, but not the errors and instead gives these notifications:</p>
<p><a href=""https://i.stack.imgur.com/gQZqW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gQZqW.jpg"" alt=""enter image description here"" /></a></p>
<p>However, warnings still show up in the assets' settings:</p>
<p><a href=""https://i.stack.imgur.com/rQWDb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rQWDb.jpg"" alt=""enter image description here"" /></a></p>
<h2>Course: ML-Agents: Hummingbirds</h2>
<p>I successfully completed the <a href=""https://learn.unity.com/course/ml-agents-hummingbirds?uv=2019.3"" rel=""nofollow noreferrer"">ML-Agents: Hummingbirds</a>. This course doesn't use any of the assets from the ML-agents Git repository. Although, assets for this course are downloaded and added the same way, without issues. This lets me assume that the general setup for ML-agents is working but I specifically can't import the assets.</p>
<h2>My setup</h2>
<ul>
<li>I'm working on a machine with Windows 11</li>
<li><code>Unity Version is 2020.3.32f1 Personal &lt;DX11&gt;</code></li>
<li>The Unity <code>ml-agent package</code> was tried with <code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.0.2 (Preview)</code></li>
<li>The Unity <code>ML Agents Extensions</code> package 0.6.1 (preview)</li>
<li>Python Version is, as per <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a>, <code>3.7 with an Anaconda virtual environment</code></li>
<li>Unity's ml-agents git is <code>main</code>, as well as <code>release_19_branch</code></li>
<li>C# editor would be <code>Visual Studio Code 1.67.2</code></li>
<li>DotNet Version: 6.0.301</li>
</ul>
<h2>Things I found out so far</h2>
<p>This problem seems to be somewhat common, I've found several variations of similar problems over a couple of years, some more specific to the <a href=""https://stackoverflow.com/questions/64085348/unity-ml-agents-package-manager-is-not-importing-actuator-script"">Actuators</a> missing, some more <a href=""https://github.com/Unity-Technologies/ml-agents/issues/3027"" rel=""nofollow noreferrer"">general</a>. Some are posting about <a href=""https://forum.unity.com/threads/cannot-find-unityengine-inputsystem.807645/"" rel=""nofollow noreferrer"">problems with the InputSystem</a> as well, but seemingly different solutions and mixed reactions to the solutions.</p>
<p>There are various suggestions, about version changes for Unity, the ml-agents package, and Visual Studio Code. Other solutions involve downloading additional packages in Visual Studio or Unity. Some suggest editing scripts within the cloned git repository. Although most of these threads are from the past 2 years. I've spent two days trying to set this up and fix this and am just about to give up on the ml-agents package. A shame the installation process for a seemingly great resource seems so infeasible. I'd appreciate further suggestions or directions on additional resources on how to set up this package.</p>
",173,1,2,4,python;c#;unity3d;ml-agent,2022-05-27 20:32:29,2022-05-27 20:32:29,2022-07-11 19:58:00,i m trying to work with  and encountered problems during the setup  when i try to import the assets from  into unity  i get many warnings and errors inside unity  for the purpose of context  i m at the very beginning of learning unity  so i don t know if the errors are due to the ml agents package or user error from my side in how to set everything up  instructions to create a first test scene with assets from unity s ml agents git suggest making a new d project in unity and drag and drop the folder projects assets ml agents into the project s assets  at this point  unity is showing many errors and warnings in the terminal  it still has the examples in the assets but every element in the scene is full of warnings  in my case the majority of the warnings seem to come from pushblockwithinput  pushblockactions and pushblockwithinputplayercontroller missing unityengine inputsystem and unity mlagents extensions input with the note   are you missing a using directive or an assembly reference     this  however  did not happen in the aforementioned tutorials  although they make the majority of errors  they are not exclusively about assembly references  other errors  which may or may not be about assembly references  are  i ve followed the  from unity s ml agents git and was successful in installing a python   environment with anaconda  pytorch with cuda  and the unity s ml agents python package via pip  when trying to verify the ml agents python package works with mlagents learn   help  i first got an exception but could resolve that by updating protobuf     as per   just mentioning this in case it is relevant   i downloaded the c  package from unity s package manager and tried it for several versions      verified       preview       and     preview     lastest   after which i m able to select ml agents from the  add component  menu in the inspector  i ve also tried to create a new unity project with the ml agent package     preview  with the right barracuda version  and the release  branch of unity s ml agents git  without success  now it s  warnings and  errors   now also the actuators are not found  which seems to be a more common problem on its own  i have also switched from unity s ml agents git s main branch to the  and also tried other versions of the barracuda package  e g  version     which seems to remove the warnings  but not the errors and instead gives these notifications   however  warnings still show up in the assets  settings   i successfully completed the   this course doesn t use any of the assets from the ml agents git repository  although  assets for this course are downloaded and added the same way  without issues  this lets me assume that the general setup for ml agents is working but i specifically can t import the assets  this problem seems to be somewhat common  i ve found several variations of similar problems over a couple of years  some more specific to the  missing  some more   some are posting about  as well  but seemingly different solutions and mixed reactions to the solutions  there are various suggestions  about version changes for unity  the ml agents package  and visual studio code  other solutions involve downloading additional packages in visual studio or unity  some suggest editing scripts within the cloned git repository  although most of these threads are from the past  years  i ve spent two days trying to set this up and fix this and am just about to give up on the ml agents package  a shame the installation process for a seemingly great resource seems so infeasible  i d appreciate further suggestions or directions on additional resources on how to set up this package ,unity   s ml agents assets throw warnings and errors  pushblockwithinput  actuator  barracuda ,trying work encountered problems setup try import assets unity get many warnings errors inside unity purpose context beginning learning unity know errors due ml agents package user error side set everything instructions create first test scene assets unity ml agents git suggest making project unity drag drop folder projects assets ml agents project assets point unity showing many errors warnings terminal still examples assets every element scene full warnings case majority warnings seem come pushblockwithinput pushblockactions pushblockwithinputplayercontroller missing unityengine inputsystem unity mlagents extensions input note missing using directive assembly reference however happen aforementioned tutorials although make majority errors exclusively assembly references errors may may assembly references followed unity ml agents git successful installing python environment anaconda pytorch cuda unity ml agents python package via pip trying verify ml agents python package works mlagents learn help first got exception could resolve updating protobuf per mentioning case relevant downloaded c package unity package manager tried several versions verified preview preview lastest able select ml agents component menu inspector also tried create unity project ml agent package preview right barracuda version release branch unity ml agents git without success warnings errors also actuators found seems common problem also switched unity ml agents git main branch also tried versions barracuda package e g version seems remove warnings errors instead gives notifications however warnings still show assets settings successfully completed course use assets ml agents git repository although assets course downloaded added way without issues lets assume general setup ml agents working specifically import assets problem seems somewhat common found several variations similar problems couple years specific missing posting well seemingly different solutions mixed reactions solutions various suggestions version changes unity ml agents package visual studio code solutions involve downloading additional packages visual studio unity suggest editing scripts within cloned git repository although threads past years spent two days trying set fix give ml agents package shame installation process seemingly great resource seems infeasible appreciate suggestions directions additional resources set package,unity ml agents assets throw warnings errors pushblockwithinput actuator barracuda,unity ml agents assets throw warnings errors pushblockwithinput actuator barracudatrying work encountered problems setup try import assets unity get many warnings errors inside unity purpose context beginning learning unity know errors due ml agents package user error side set everything instructions create first test scene assets unity ml agents git suggest making project unity drag drop folder projects assets ml agents project assets point unity showing many errors warnings terminal still examples assets every element scene full warnings case majority warnings seem come pushblockwithinput pushblockactions pushblockwithinputplayercontroller missing unityengine inputsystem unity mlagents extensions input note missing using directive assembly reference however happen aforementioned tutorials although make majority errors exclusively assembly references errors may may assembly references followed unity ml agents git successful installing python environment anaconda pytorch cuda unity ml agents python package via pip trying verify ml agents python package works mlagents learn help first got exception could resolve updating protobuf per mentioning case relevant downloaded c package unity package manager tried several versions verified preview preview lastest able select ml agents component menu inspector also tried create unity project ml agent package preview right barracuda version release branch unity ml agents git without success warnings errors also actuators found seems common problem also switched unity ml agents git main branch also tried versions barracuda package e g version seems remove warnings errors instead gives notifications however warnings still show assets settings successfully completed course use assets ml agents git repository although assets course downloaded added way without issues lets assume general setup ml agents working specifically import assets problem seems somewhat common found several variations similar problems couple years specific missing posting well seemingly different solutions mixed reactions solutions various suggestions version changes unity ml agents package visual studio code solutions involve downloading additional packages visual studio unity suggest editing scripts within cloned git repository although threads past years spent two days trying set fix give ml agents package shame installation process seemingly great resource seems infeasible appreciate suggestions directions additional resources set package,"['unity', 'ml', 'agents', 'assets', 'throw', 'warnings', 'errors', 'pushblockwithinput', 'actuator', 'barracudatrying', 'work', 'encountered', 'problems', 'setup', 'try', 'import', 'assets', 'unity', 'get', 'many', 'warnings', 'errors', 'inside', 'unity', 'purpose', 'context', 'beginning', 'learning', 'unity', 'know', 'errors', 'due', 'ml', 'agents', 'package', 'user', 'error', 'side', 'set', 'everything', 'instructions', 'create', 'first', 'test', 'scene', 'assets', 'unity', 'ml', 'agents', 'git', 'suggest', 'making', 'project', 'unity', 'drag', 'drop', 'folder', 'projects', 'assets', 'ml', 'agents', 'project', 'assets', 'point', 'unity', 'showing', 'many', 'errors', 'warnings', 'terminal', 'still', 'examples', 'assets', 'every', 'element', 'scene', 'full', 'warnings', 'case', 'majority', 'warnings', 'seem', 'come', 'pushblockwithinput', 'pushblockactions', 'pushblockwithinputplayercontroller', 'missing', 'unityengine', 'inputsystem', 'unity', 'mlagents', 'extensions', 'input', 'note', 'missing', 'using', 'directive', 'assembly', 'reference', 'however', 'happen', 'aforementioned', 'tutorials', 'although', 'make', 'majority', 'errors', 'exclusively', 'assembly', 'references', 'errors', 'may', 'may', 'assembly', 'references', 'followed', 'unity', 'ml', 'agents', 'git', 'successful', 'installing', 'python', 'environment', 'anaconda', 'pytorch', 'cuda', 'unity', 'ml', 'agents', 'python', 'package', 'via', 'pip', 'trying', 'verify', 'ml', 'agents', 'python', 'package', 'works', 'mlagents', 'learn', 'help', 'first', 'got', 'exception', 'could', 'resolve', 'updating', 'protobuf', 'per', 'mentioning', 'case', 'relevant', 'downloaded', 'c', 'package', 'unity', 'package', 'manager', 'tried', 'several', 'versions', 'verified', 'preview', 'preview', 'lastest', 'able', 'select', 'ml', 'agents', 'component', 'menu', 'inspector', 'also', 'tried', 'create', 'unity', 'project', 'ml', 'agent', 'package', 'preview', 'right', 'barracuda', 'version', 'release', 'branch', 'unity', 'ml', 'agents', 'git', 'without', 'success', 'warnings', 'errors', 'also', 'actuators', 'found', 'seems', 'common', 'problem', 'also', 'switched', 'unity', 'ml', 'agents', 'git', 'main', 'branch', 'also', 'tried', 'versions', 'barracuda', 'package', 'e', 'g', 'version', 'seems', 'remove', 'warnings', 'errors', 'instead', 'gives', 'notifications', 'however', 'warnings', 'still', 'show', 'assets', 'settings', 'successfully', 'completed', 'course', 'use', 'assets', 'ml', 'agents', 'git', 'repository', 'although', 'assets', 'course', 'downloaded', 'added', 'way', 'without', 'issues', 'lets', 'assume', 'general', 'setup', 'ml', 'agents', 'working', 'specifically', 'import', 'assets', 'problem', 'seems', 'somewhat', 'common', 'found', 'several', 'variations', 'similar', 'problems', 'couple', 'years', 'specific', 'missing', 'posting', 'well', 'seemingly', 'different', 'solutions', 'mixed', 'reactions', 'solutions', 'various', 'suggestions', 'version', 'changes', 'unity', 'ml', 'agents', 'package', 'visual', 'studio', 'code', 'solutions', 'involve', 'downloading', 'additional', 'packages', 'visual', 'studio', 'unity', 'suggest', 'editing', 'scripts', 'within', 'cloned', 'git', 'repository', 'although', 'threads', 'past', 'years', 'spent', 'two', 'days', 'trying', 'set', 'fix', 'give', 'ml', 'agents', 'package', 'shame', 'installation', 'process', 'seemingly', 'great', 'resource', 'seems', 'infeasible', 'appreciate', 'suggestions', 'directions', 'additional', 'resources', 'set', 'package']","['uniti', 'ml', 'agent', 'asset', 'throw', 'warn', 'error', 'pushblockwithinput', 'actuat', 'barracudatri', 'work', 'encount', 'problem', 'setup', 'tri', 'import', 'asset', 'uniti', 'get', 'mani', 'warn', 'error', 'insid', 'uniti', 'purpos', 'context', 'begin', 'learn', 'uniti', 'know', 'error', 'due', 'ml', 'agent', 'packag', 'user', 'error', 'side', 'set', 'everyth', 'instruct', 'creat', 'first', 'test', 'scene', 'asset', 'uniti', 'ml', 'agent', 'git', 'suggest', 'make', 'project', 'uniti', 'drag', 'drop', 'folder', 'project', 'asset', 'ml', 'agent', 'project', 'asset', 'point', 'uniti', 'show', 'mani', 'error', 'warn', 'termin', 'still', 'exampl', 'asset', 'everi', 'element', 'scene', 'full', 'warn', 'case', 'major', 'warn', 'seem', 'come', 'pushblockwithinput', 'pushblockact', 'pushblockwithinputplayercontrol', 'miss', 'unityengin', 'inputsystem', 'uniti', 'mlagent', 'extens', 'input', 'note', 'miss', 'use', 'direct', 'assembl', 'refer', 'howev', 'happen', 'aforement', 'tutori', 'although', 'make', 'major', 'error', 'exclus', 'assembl', 'refer', 'error', 'may', 'may', 'assembl', 'refer', 'follow', 'uniti', 'ml', 'agent', 'git', 'success', 'instal', 'python', 'environ', 'anaconda', 'pytorch', 'cuda', 'uniti', 'ml', 'agent', 'python', 'packag', 'via', 'pip', 'tri', 'verifi', 'ml', 'agent', 'python', 'packag', 'work', 'mlagent', 'learn', 'help', 'first', 'got', 'except', 'could', 'resolv', 'updat', 'protobuf', 'per', 'mention', 'case', 'relev', 'download', 'c', 'packag', 'uniti', 'packag', 'manag', 'tri', 'sever', 'version', 'verifi', 'preview', 'preview', 'lastest', 'abl', 'select', 'ml', 'agent', 'compon', 'menu', 'inspector', 'also', 'tri', 'creat', 'uniti', 'project', 'ml', 'agent', 'packag', 'preview', 'right', 'barracuda', 'version', 'releas', 'branch', 'uniti', 'ml', 'agent', 'git', 'without', 'success', 'warn', 'error', 'also', 'actuat', 'found', 'seem', 'common', 'problem', 'also', 'switch', 'uniti', 'ml', 'agent', 'git', 'main', 'branch', 'also', 'tri', 'version', 'barracuda', 'packag', 'e', 'g', 'version', 'seem', 'remov', 'warn', 'error', 'instead', 'give', 'notif', 'howev', 'warn', 'still', 'show', 'asset', 'set', 'success', 'complet', 'cours', 'use', 'asset', 'ml', 'agent', 'git', 'repositori', 'although', 'asset', 'cours', 'download', 'ad', 'way', 'without', 'issu', 'let', 'assum', 'gener', 'setup', 'ml', 'agent', 'work', 'specif', 'import', 'asset', 'problem', 'seem', 'somewhat', 'common', 'found', 'sever', 'variat', 'similar', 'problem', 'coupl', 'year', 'specif', 'miss', 'post', 'well', 'seemingli', 'differ', 'solut', 'mix', 'reaction', 'solut', 'variou', 'suggest', 'version', 'chang', 'uniti', 'ml', 'agent', 'packag', 'visual', 'studio', 'code', 'solut', 'involv', 'download', 'addit', 'packag', 'visual', 'studio', 'uniti', 'suggest', 'edit', 'script', 'within', 'clone', 'git', 'repositori', 'although', 'thread', 'past', 'year', 'spent', 'two', 'day', 'tri', 'set', 'fix', 'give', 'ml', 'agent', 'packag', 'shame', 'instal', 'process', 'seemingli', 'great', 'resourc', 'seem', 'infeas', 'appreci', 'suggest', 'direct', 'addit', 'resourc', 'set', 'packag']"
238,255,255,345660,7044808,"Using R to download gzipped data file, extract, and import data","<p>A follow up to <a href=""https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data"">this question</a>: How can I download and uncompress a gzipped file using R?  For example (from <a href=""http://archive.ics.uci.edu/ml/datasets"" rel=""noreferrer"">the UCI Machine Learning Repository</a>), I have a <a href=""http://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29"" rel=""noreferrer"">file of insurance data</a>.  How can I download it using R?</p>

<p>Here is the data url: <code>http://archive.ics.uci.edu/ml/databases/tic/tic.tar.gz</code>.</p>
",8762,4,11,3,r;zip;connection,2011-08-12 21:40:45,2011-08-12 21:40:45,2022-07-11 19:06:21,a follow up to   how can i download and uncompress a gzipped file using r   for example  from    i have a    how can i download it using r  here is the data url  http   archive ics uci edu ml databases tic tic tar gz ,using r to download gzipped data file  extract  and import data,follow download uncompress gzipped file using r example download using r data url http archive ics uci edu ml databases tic tic tar gz,using r download gzipped data file extract import data,using r download gzipped data file extract import datafollow download uncompress gzipped file using r example download using r data url http archive ics uci edu ml databases tic tic tar gz,"['using', 'r', 'download', 'gzipped', 'data', 'file', 'extract', 'import', 'datafollow', 'download', 'uncompress', 'gzipped', 'file', 'using', 'r', 'example', 'download', 'using', 'r', 'data', 'url', 'http', 'archive', 'ics', 'uci', 'edu', 'ml', 'databases', 'tic', 'tic', 'tar', 'gz']","['use', 'r', 'download', 'gzip', 'data', 'file', 'extract', 'import', 'datafollow', 'download', 'uncompress', 'gzip', 'file', 'use', 'r', 'exampl', 'download', 'use', 'r', 'data', 'url', 'http', 'archiv', 'ic', 'uci', 'edu', 'ml', 'databas', 'tic', 'tic', 'tar', 'gz']"
239,256,256,17957100,72938442,How do I return all pages from a rest API request without manually specifying the page number?,"<p>I'm retrieving data from a paginated API and converting it to .JSON format and I'd like to retrieve all pages in the response, without having to specify the page number in the URL. The API accepts page number and results per page (max. 250) as inputs.</p>
<p>I understand that the typical solution is to loop through pages using a key that specifies the address of the next page. However, it appears as though this API doesn't include a next page parameter in the output (see example response below). I can only think that the last page (i.e. total pages) parameter could be useful here? How can I scrape all of the pages without specifying the page number?</p>
<p><strong>My script:</strong></p>
<pre><code>  import requests
  import json

  url = &quot;https://api-v2.pitchbook.com/deals/search?keywords=machine learning accelerator&amp;perPage=250&quot;

  payload={}
  headers = {
      'Authorization': 'PB-Token 1234567'
   }

  response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload)

  data = response.json()

  print(data)                                                                                                                                                                                 
</code></pre>
<p><strong>Example response</strong></p>
<p>{'stats': {'total': 2, 'perPage': 250, 'page': 1, 'lastPage': 1}, 'items': [{'dealId': '98982-
28T', 'companyId': '162120-79', 'companyName': 'companyA'}, {'dealId': '112532-05T',
'companyId': '233527-87', 'companyName': 'companyB'}]}</p>
",32,1,0,5,python;json;loops;rest;pagination,2022-07-11 15:16:07,2022-07-11 15:16:07,2022-07-11 16:35:55,i m retrieving data from a paginated api and converting it to  json format and i d like to retrieve all pages in the response  without having to specify the page number in the url  the api accepts page number and results per page  max    as inputs  i understand that the typical solution is to loop through pages using a key that specifies the address of the next page  however  it appears as though this api doesn t include a next page parameter in the output  see example response below   i can only think that the last page  i e  total pages  parameter could be useful here  how can i scrape all of the pages without specifying the page number  my script  example response,how do i return all pages from a rest api request without manually specifying the page number ,retrieving data paginated api converting json format like retrieve pages response without specify page number url api accepts page number results per page max inputs understand typical solution loop pages using key specifies address next page however appears though api include next page parameter output see example response think last page e total pages parameter could useful scrape pages without specifying page number script example response,return pages rest api request without manually specifying page number,return pages rest api request without manually specifying page numberretrieving data paginated api converting json format like retrieve pages response without specify page number url api accepts page number results per page max inputs understand typical solution loop pages using key specifies address next page however appears though api include next page parameter output see example response think last page e total pages parameter could useful scrape pages without specifying page number script example response,"['return', 'pages', 'rest', 'api', 'request', 'without', 'manually', 'specifying', 'page', 'numberretrieving', 'data', 'paginated', 'api', 'converting', 'json', 'format', 'like', 'retrieve', 'pages', 'response', 'without', 'specify', 'page', 'number', 'url', 'api', 'accepts', 'page', 'number', 'results', 'per', 'page', 'max', 'inputs', 'understand', 'typical', 'solution', 'loop', 'pages', 'using', 'key', 'specifies', 'address', 'next', 'page', 'however', 'appears', 'though', 'api', 'include', 'next', 'page', 'parameter', 'output', 'see', 'example', 'response', 'think', 'last', 'page', 'e', 'total', 'pages', 'parameter', 'could', 'useful', 'scrape', 'pages', 'without', 'specifying', 'page', 'number', 'script', 'example', 'response']","['return', 'page', 'rest', 'api', 'request', 'without', 'manual', 'specifi', 'page', 'numberretriev', 'data', 'pagin', 'api', 'convert', 'json', 'format', 'like', 'retriev', 'page', 'respons', 'without', 'specifi', 'page', 'number', 'url', 'api', 'accept', 'page', 'number', 'result', 'per', 'page', 'max', 'input', 'understand', 'typic', 'solut', 'loop', 'page', 'use', 'key', 'specifi', 'address', 'next', 'page', 'howev', 'appear', 'though', 'api', 'includ', 'next', 'page', 'paramet', 'output', 'see', 'exampl', 'respons', 'think', 'last', 'page', 'e', 'total', 'page', 'paramet', 'could', 'use', 'scrape', 'page', 'without', 'specifi', 'page', 'number', 'script', 'exampl', 'respons']"
240,257,257,19436129,72939406,SQL Server Vector Index,"<p>I would like to create a vector index for a database table in SQL Server. In my case, I have 256-dimensional vectors (numerical representations of the text contents of each row, derived from some NLP methods). However, SQL Server unfortunately does not seem to include array functionality (numerical vector data type).</p>
<p>Some potential solutions are offered here: <a href=""https://www.sqlshack.com/implement-array-like-functionality-sql-server/"" rel=""nofollow noreferrer"">https://www.sqlshack.com/implement-array-like-functionality-sql-server/</a>. For instance, I could create a table with one column for each entry of the vector. Is there a more elegant (performant) way?</p>
<hr />
<p>Background: I am implementing a semantic search feature for my database using MS Machine Learning Services (Python), that is, I want to be able to compare vector representations of query texts to the index (using cosine similarity). I know that there is also a full-text search feature for SQL server. Currently, I do not use it because it provides too little control over the pre-processing of the rather messy text fields in my database.</p>
",29,0,0,4,sql-server;python-3.x;indexing;full-text-search,2022-07-11 16:31:21,2022-07-11 16:31:21,2022-07-11 16:31:21,i would like to create a vector index for a database table in sql server  in my case  i have  dimensional vectors  numerical representations of the text contents of each row  derived from some nlp methods   however  sql server unfortunately does not seem to include array functionality  numerical vector data type   some potential solutions are offered here    for instance  i could create a table with one column for each entry of the vector  is there a more elegant  performant  way  background  i am implementing a semantic search feature for my database using ms machine learning services  python   that is  i want to be able to compare vector representations of query texts to the index  using cosine similarity   i know that there is also a full text search feature for sql server  currently  i do not use it because it provides too little control over the pre processing of the rather messy text fields in my database ,sql server vector index,would like create vector index database table sql server case dimensional vectors numerical representations text contents row derived nlp methods however sql server unfortunately seem include array functionality numerical vector data type potential solutions offered instance could create table one column entry vector elegant performant way background implementing semantic search feature database using ms machine learning services python want able compare vector representations query texts index using cosine similarity know also full text search feature sql server currently use provides little control pre processing rather messy text fields database,sql server vector index,sql server vector indexwould like create vector index database table sql server case dimensional vectors numerical representations text contents row derived nlp methods however sql server unfortunately seem include array functionality numerical vector data type potential solutions offered instance could create table one column entry vector elegant performant way background implementing semantic search feature database using ms machine learning services python want able compare vector representations query texts index using cosine similarity know also full text search feature sql server currently use provides little control pre processing rather messy text fields database,"['sql', 'server', 'vector', 'indexwould', 'like', 'create', 'vector', 'index', 'database', 'table', 'sql', 'server', 'case', 'dimensional', 'vectors', 'numerical', 'representations', 'text', 'contents', 'row', 'derived', 'nlp', 'methods', 'however', 'sql', 'server', 'unfortunately', 'seem', 'include', 'array', 'functionality', 'numerical', 'vector', 'data', 'type', 'potential', 'solutions', 'offered', 'instance', 'could', 'create', 'table', 'one', 'column', 'entry', 'vector', 'elegant', 'performant', 'way', 'background', 'implementing', 'semantic', 'search', 'feature', 'database', 'using', 'ms', 'machine', 'learning', 'services', 'python', 'want', 'able', 'compare', 'vector', 'representations', 'query', 'texts', 'index', 'using', 'cosine', 'similarity', 'know', 'also', 'full', 'text', 'search', 'feature', 'sql', 'server', 'currently', 'use', 'provides', 'little', 'control', 'pre', 'processing', 'rather', 'messy', 'text', 'fields', 'database']","['sql', 'server', 'vector', 'indexwould', 'like', 'creat', 'vector', 'index', 'databas', 'tabl', 'sql', 'server', 'case', 'dimension', 'vector', 'numer', 'represent', 'text', 'content', 'row', 'deriv', 'nlp', 'method', 'howev', 'sql', 'server', 'unfortun', 'seem', 'includ', 'array', 'function', 'numer', 'vector', 'data', 'type', 'potenti', 'solut', 'offer', 'instanc', 'could', 'creat', 'tabl', 'one', 'column', 'entri', 'vector', 'eleg', 'perform', 'way', 'background', 'implement', 'semant', 'search', 'featur', 'databas', 'use', 'ms', 'machin', 'learn', 'servic', 'python', 'want', 'abl', 'compar', 'vector', 'represent', 'queri', 'text', 'index', 'use', 'cosin', 'similar', 'know', 'also', 'full', 'text', 'search', 'featur', 'sql', 'server', 'current', 'use', 'provid', 'littl', 'control', 'pre', 'process', 'rather', 'messi', 'text', 'field', 'databas']"
241,258,258,17668281,72938908,MS Azure Machine Learning - Notebook extremely slow in comparison to GC,"<p>I work with a jupyter notebook on different platforms.</p>
<p>The notebook includes a detectron2 algorithm.</p>
<p>The runtime for the training process on different platforms differs significantl (Same Code, Same parameters, same data)</p>
<p><strong>Google Colab</strong></p>
<ul>
<li>K80 GPU (Successfully used while training)</li>
<li>Data stored on Google Drive</li>
<li>Torch: 1.9.0+cu111</li>
<li><strong>Runtime: 4,5h</strong></li>
</ul>
<hr />
<p><strong>Microsoft Azure - Machine Learning</strong></p>
<ul>
<li>K80 GPU (Successfully used while training)</li>
<li>STANDARD_NC6</li>
<li>Data stored an Azure Files</li>
<li>Torch: 1.9.0+cu111</li>
<li><strong>Runtime: 15h</strong></li>
</ul>
<p>Why is the expensive Azure ML three times slower than free Google Colab and is there a possibility to improve that?</p>
",20,0,0,5,azure;deep-learning;jupyter-notebook;google-colaboratory;object-detection,2022-07-11 15:51:35,2022-07-11 15:51:35,2022-07-11 15:51:35,i work with a jupyter notebook on different platforms  the notebook includes a detectron algorithm  the runtime for the training process on different platforms differs significantl  same code  same parameters  same data  google colab microsoft azure   machine learning why is the expensive azure ml three times slower than free google colab and is there a possibility to improve that ,ms azure machine learning   notebook extremely slow in comparison to gc,work jupyter notebook different platforms notebook includes detectron algorithm runtime training process different platforms differs significantl code parameters data google colab microsoft azure machine learning expensive azure ml three times slower free google colab possibility improve,ms azure machine learning notebook extremely slow comparison gc,ms azure machine learning notebook extremely slow comparison gcwork jupyter notebook different platforms notebook includes detectron algorithm runtime training process different platforms differs significantl code parameters data google colab microsoft azure machine learning expensive azure ml three times slower free google colab possibility improve,"['ms', 'azure', 'machine', 'learning', 'notebook', 'extremely', 'slow', 'comparison', 'gcwork', 'jupyter', 'notebook', 'different', 'platforms', 'notebook', 'includes', 'detectron', 'algorithm', 'runtime', 'training', 'process', 'different', 'platforms', 'differs', 'significantl', 'code', 'parameters', 'data', 'google', 'colab', 'microsoft', 'azure', 'machine', 'learning', 'expensive', 'azure', 'ml', 'three', 'times', 'slower', 'free', 'google', 'colab', 'possibility', 'improve']","['ms', 'azur', 'machin', 'learn', 'notebook', 'extrem', 'slow', 'comparison', 'gcwork', 'jupyt', 'notebook', 'differ', 'platform', 'notebook', 'includ', 'detectron', 'algorithm', 'runtim', 'train', 'process', 'differ', 'platform', 'differ', 'significantl', 'code', 'paramet', 'data', 'googl', 'colab', 'microsoft', 'azur', 'machin', 'learn', 'expens', 'azur', 'ml', 'three', 'time', 'slower', 'free', 'googl', 'colab', 'possibl', 'improv']"
242,259,259,6151951,72911756,GCP Pipeline for ML Images in Cloud Storage,"<p>I am setting up a Machine Learning training instance on the GKE. My images live in Cloud Storage. How can I access those images without first downloading them onto the GKE? They are considered private (medical data), and so I do not want to risk them being exposed by transferring them to a GKE instance first.</p>
<p>Is there a recommended way to do this?</p>
",36,0,-1,3,machine-learning;google-cloud-storage;google-kubernetes-engine,2022-07-08 15:55:28,2022-07-08 15:55:28,2022-07-11 15:00:20,i am setting up a machine learning training instance on the gke  my images live in cloud storage  how can i access those images without first downloading them onto the gke  they are considered private  medical data   and so i do not want to risk them being exposed by transferring them to a gke instance first  is there a recommended way to do this ,gcp pipeline for ml images in cloud storage,setting machine learning training instance gke images live cloud storage access images without first downloading onto gke considered private medical data want risk exposed transferring gke instance first recommended way,gcp pipeline ml images cloud storage,gcp pipeline ml images cloud storagesetting machine learning training instance gke images live cloud storage access images without first downloading onto gke considered private medical data want risk exposed transferring gke instance first recommended way,"['gcp', 'pipeline', 'ml', 'images', 'cloud', 'storagesetting', 'machine', 'learning', 'training', 'instance', 'gke', 'images', 'live', 'cloud', 'storage', 'access', 'images', 'without', 'first', 'downloading', 'onto', 'gke', 'considered', 'private', 'medical', 'data', 'want', 'risk', 'exposed', 'transferring', 'gke', 'instance', 'first', 'recommended', 'way']","['gcp', 'pipelin', 'ml', 'imag', 'cloud', 'storageset', 'machin', 'learn', 'train', 'instanc', 'gke', 'imag', 'live', 'cloud', 'storag', 'access', 'imag', 'without', 'first', 'download', 'onto', 'gke', 'consid', 'privat', 'medic', 'data', 'want', 'risk', 'expos', 'transfer', 'gke', 'instanc', 'first', 'recommend', 'way']"
243,260,260,11143347,72935277,upload a pre-trained model locally into databricks,"<p>Is it possible to upload a pre-trained machine learning model that was trained on a different environment on databricks, and serve it? Or is it impossible on Databricks ?</p>
",44,1,0,3,machine-learning;databricks;mlflow,2022-07-11 10:50:31,2022-07-11 10:50:31,2022-07-11 14:58:05,is it possible to upload a pre trained machine learning model that was trained on a different environment on databricks  and serve it  or is it impossible on databricks  ,upload a pre trained model locally into databricks,possible upload pre trained machine learning model trained different environment databricks serve impossible databricks,upload pre trained model locally databricks,upload pre trained model locally databrickspossible upload pre trained machine learning model trained different environment databricks serve impossible databricks,"['upload', 'pre', 'trained', 'model', 'locally', 'databrickspossible', 'upload', 'pre', 'trained', 'machine', 'learning', 'model', 'trained', 'different', 'environment', 'databricks', 'serve', 'impossible', 'databricks']","['upload', 'pre', 'train', 'model', 'local', 'databricksposs', 'upload', 'pre', 'train', 'machin', 'learn', 'model', 'train', 'differ', 'environ', 'databrick', 'serv', 'imposs', 'databrick']"
244,261,261,8614508,72937870,"The method is not allowed for the requested URL - flask, model deployment","<p>I was trying to build a web application for my machine learning model deployment.</p>
<p>First I was testing the connection using the below code:</p>
<pre><code>import pickle  ## read .bin file
from distutils.log import debug
from pyexpat import model
from urllib import response
from flask import Flask, request, jsonify
from model_file.ml_model import predict_mpg

app = Flask(&quot;mpg_prediction&quot;)

@app.route('/', methods=['GET'])  ## test if the app is working
def ping():
    return &quot;Pinging Model Application!!&quot;
</code></pre>
<p>The above worked fine when I went to the server and the web app reads <code>Pinging Model Application!!</code></p>
<p>The problem was when I tried to &quot;fit&quot; my machine learning model to the app. I've pickled my trained model into the model.bin file and I used the below codes:</p>
<pre><code>@app.route('/', methods=['POST'])
def predict():
    vehicle_config = request.get_json()

    with open('./model_file/model.bin', 'rb') as f_in:
        model = pickle.load(f_in)
        f_in.close()

        predictions = predict_mpg(vehicle_config, model)

        response = {
            'mpg_predictions': list(predictions)
        }
        return jsonify()
</code></pre>
<p>After I ran the above codes, and tested the web app, I came across the error message:
<code>The method is not allowed for the requested URL</code></p>
<p>This is my first time using Flask, so I've no idea what went wrong.</p>
<p>I'd appreciate any help on this. Thanks in advance.</p>
",23,0,-1,4,python;flask;deployment;pickle,2022-07-11 14:30:42,2022-07-11 14:30:42,2022-07-11 14:30:42,i was trying to build a web application for my machine learning model deployment  first i was testing the connection using the below code  the above worked fine when i went to the server and the web app reads pinging model application   the problem was when i tried to  fit  my machine learning model to the app  i ve pickled my trained model into the model bin file and i used the below codes  this is my first time using flask  so i ve no idea what went wrong  i d appreciate any help on this  thanks in advance ,the method is not allowed for the requested url   flask  model deployment,trying build web application machine learning model deployment first testing connection using code worked fine went server web app reads pinging model application problem tried fit machine learning model app pickled trained model model bin file used codes first time using flask idea went wrong appreciate help thanks advance,method allowed requested url flask model deployment,method allowed requested url flask model deploymenttrying build web application machine learning model deployment first testing connection using code worked fine went server web app reads pinging model application problem tried fit machine learning model app pickled trained model model bin file used codes first time using flask idea went wrong appreciate help thanks advance,"['method', 'allowed', 'requested', 'url', 'flask', 'model', 'deploymenttrying', 'build', 'web', 'application', 'machine', 'learning', 'model', 'deployment', 'first', 'testing', 'connection', 'using', 'code', 'worked', 'fine', 'went', 'server', 'web', 'app', 'reads', 'pinging', 'model', 'application', 'problem', 'tried', 'fit', 'machine', 'learning', 'model', 'app', 'pickled', 'trained', 'model', 'model', 'bin', 'file', 'used', 'codes', 'first', 'time', 'using', 'flask', 'idea', 'went', 'wrong', 'appreciate', 'help', 'thanks', 'advance']","['method', 'allow', 'request', 'url', 'flask', 'model', 'deploymenttri', 'build', 'web', 'applic', 'machin', 'learn', 'model', 'deploy', 'first', 'test', 'connect', 'use', 'code', 'work', 'fine', 'went', 'server', 'web', 'app', 'read', 'ping', 'model', 'applic', 'problem', 'tri', 'fit', 'machin', 'learn', 'model', 'app', 'pickl', 'train', 'model', 'model', 'bin', 'file', 'use', 'code', 'first', 'time', 'use', 'flask', 'idea', 'went', 'wrong', 'appreci', 'help', 'thank', 'advanc']"
245,262,262,11619848,72936419,Getting error 404 in azure machine learning service,"<p>I am new to the cloud and i've been trying to follow a tutorial. I am trying to create a simple pipeline and understand how the <strong>Execute python script</strong> component works. So far this is what i'm trying to execute.</p>
<p><a href=""https://i.stack.imgur.com/Kkn9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kkn9z.png"" alt=""Pipeline structure"" /></a></p>
<p>This is the code i have within the execute block:</p>
<pre><code>import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
    dataframe1['Dollar/HP'] = dataframe1.price / dataframe1.horsepower
    return dataframe1
</code></pre>
<p>No matter what i try i get the same error and i can't find anything about it on the internet.</p>
<p><strong>Job preparation failed: HTTP Error 404: Unable to find snapshot with id: ...</strong></p>
",15,0,0,1,azure-pipelines,2022-07-11 12:34:48,2022-07-11 12:34:48,2022-07-11 12:34:48,i am new to the cloud and i ve been trying to follow a tutorial  i am trying to create a simple pipeline and understand how the execute python script component works  so far this is what i m trying to execute   this is the code i have within the execute block  no matter what i try i get the same error and i can t find anything about it on the internet  job preparation failed  http error   unable to find snapshot with id     ,getting error  in azure machine learning service,cloud trying follow tutorial trying create simple pipeline understand execute python script component works far trying execute code within execute block matter try get error find anything internet job preparation failed http error unable find snapshot id,getting error azure machine learning service,getting error azure machine learning servicecloud trying follow tutorial trying create simple pipeline understand execute python script component works far trying execute code within execute block matter try get error find anything internet job preparation failed http error unable find snapshot id,"['getting', 'error', 'azure', 'machine', 'learning', 'servicecloud', 'trying', 'follow', 'tutorial', 'trying', 'create', 'simple', 'pipeline', 'understand', 'execute', 'python', 'script', 'component', 'works', 'far', 'trying', 'execute', 'code', 'within', 'execute', 'block', 'matter', 'try', 'get', 'error', 'find', 'anything', 'internet', 'job', 'preparation', 'failed', 'http', 'error', 'unable', 'find', 'snapshot', 'id']","['get', 'error', 'azur', 'machin', 'learn', 'servicecloud', 'tri', 'follow', 'tutori', 'tri', 'creat', 'simpl', 'pipelin', 'understand', 'execut', 'python', 'script', 'compon', 'work', 'far', 'tri', 'execut', 'code', 'within', 'execut', 'block', 'matter', 'tri', 'get', 'error', 'find', 'anyth', 'internet', 'job', 'prepar', 'fail', 'http', 'error', 'unabl', 'find', 'snapshot', 'id']"
246,263,263,1361737,72927724,Why do I get &#39;c50 code called exit with value 1&#39; in R?,"<p>I am using RStudio 2021.09.0 &quot;Ghost Orchid&quot; Release for macOS.</p>
<p>I am learning to use to C5.0 algorithm in R. For this I am following <em>'Machine Learning in R'</em> by Brett Lantz. The dataset I am using is a modified version of one relating to loans obtained from a credit agency in Germany.</p>
<p>The data has no missing values, and no empty factor levels (this has caused the same error in other posts I have viewed). I have split the data into training and test tibbles using the <code>initial_split()</code> function in <code>rsample</code> package. The structure of the data is:</p>
<pre><code>str(credit_train)

tibble [900  21] (S3: tbl_df/tbl/data.frame)
 $ checking_balance    : Factor w/ 4 levels &quot;&lt; 0 DM&quot;,&quot;&gt; 200 DM&quot;,..: 4 1 4 3 3 4 3 4 1 1 ...
 $ months_loan_duration: Factor w/ 33 levels &quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,..: 18 22 18 16 30 18 9 9 14 9 ...
 $ credit_history      : Factor w/ 5 levels &quot;critical&quot;,&quot;delayed&quot;,..: 1 1 1 5 4 2 5 5 5 5 ...
 $ purpose             : Factor w/ 10 levels &quot;business&quot;,&quot;car (new)&quot;,..: 8 3 3 1 1 1 8 1 2 2 ...
 $ amount              : num [1:900] 2611 6187 2197 2767 6416 ...
 $ savings_balance     : Factor w/ 5 levels &quot;&lt; 100 DM&quot;,&quot;&gt; 1000 DM&quot;,..: 1 3 5 3 1 1 1 1 3 1 ...
 $ employment_length   : Factor w/ 5 levels &quot;&gt; 7 yrs&quot;,&quot;0 - 1 yrs&quot;,..: 1 4 4 1 1 3 1 4 4 3 ...
 $ installment_rate    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 4 1 4 4 4 1 3 2 4 4 ...
 $ personal_status     : Factor w/ 4 levels &quot;divorced male&quot;,..: 3 3 4 1 2 4 3 4 4 2 ...
 $ other_debtors       : Factor w/ 3 levels &quot;co-applicant&quot;,..: 1 3 3 3 3 3 2 3 3 2 ...
 $ residence_history   : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 4 4 2 3 2 3 4 3 4 ...
 $ property            : Factor w/ 4 levels &quot;building society savings&quot;,..: 3 2 2 2 4 4 3 2 2 1 ...
 $ age                 : num [1:900] 46 24 43 61 59 32 40 36 30 29 ...
 $ installment_plan    : Factor w/ 3 levels &quot;bank&quot;,&quot;none&quot;,..: 2 2 2 1 2 2 1 2 2 2 ...
 $ housing             : Factor w/ 3 levels &quot;for free&quot;,&quot;own&quot;,..: 2 3 2 3 3 1 2 2 2 2 ...
 $ existing_credits    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 2 2 2 1 1 2 1 1 1 ...
 $ default             : Factor w/ 2 levels &quot;paid&quot;,&quot;default&quot;: 1 1 1 2 2 1 1 1 1 1 ...
 $ dependents          : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 2 1 1 1 1 1 2 1 ...
 $ telephone           : Factor w/ 2 levels &quot;none&quot;,&quot;yes&quot;: 1 1 2 1 1 1 1 2 2 2 ...
 $ foreign_worker      : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...
 $ job                 : Factor w/ 4 levels &quot;management self-employed&quot;,..: 2 2 2 4 2 2 4 2 1 2 ...

</code></pre>
<p>My issue is specifically when I try to fit a model using a cost matrix. Without this cost matrix, the model does <strong>not</strong> throw this error. This is how I have created the cost matrix:</p>
<pre><code>error_cost &lt;- matrix(nrow = 2, 
                     ncol = 2,
                     dimnames = list(c('predict_paid','predict_default'), #rows
                                     c('actual_paid','actual_default')), #columns
                     data = c(0, 1, 4, 0))  
</code></pre>
<p>I must also point out that I have tried several ways to create this matrix, including literally copying the exact method given in the Lantz book, and they all result in this same error.</p>
<p>Here is the code I am using to try and fit the model.</p>
<pre><code>c5_boostTree &lt;- C5.0(default ~.,
                     credit_train,
                     trials = 3,
                     costs = error_cost)
</code></pre>
<p>However, this also happens if I use the <code>x = credit_train %&gt;% select(-default), y = credit_train$default</code> rather than the formula approach, and any similar approaches I can find or think of. I am at a complete loss as to why I am getting this error.</p>
<pre><code>c50 code called exit with value 1
</code></pre>
<p>Anyone have any ideas???</p>
<p>====================================</p>
<p>In response to a request for <code>dput(credit_train</code>, here is the output for dput(head(credit_train)), it seems too large otherwise:</p>
<pre><code>structure(list(checking_balance = structure(c(4L, 1L, 4L, 3L, 
3L, 4L), .Label = c(&quot;&lt; 0 DM&quot;, &quot;&gt; 200 DM&quot;, &quot;1 - 200 DM&quot;, &quot;unknown&quot;
), class = &quot;factor&quot;), months_loan_duration = structure(c(18L, 
22L, 18L, 16L, 30L, 18L), .Label = c(&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, 
&quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;18&quot;, &quot;20&quot;, &quot;21&quot;, 
&quot;22&quot;, &quot;24&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;30&quot;, &quot;33&quot;, &quot;36&quot;, &quot;39&quot;, &quot;40&quot;, &quot;42&quot;, 
&quot;45&quot;, &quot;47&quot;, &quot;48&quot;, &quot;54&quot;, &quot;60&quot;, &quot;72&quot;), class = &quot;factor&quot;), credit_history = structure(c(1L, 
1L, 1L, 5L, 4L, 2L), .Label = c(&quot;critical&quot;, &quot;delayed&quot;, &quot;fully repaid&quot;, 
&quot;fully repaid this bank&quot;, &quot;repaid&quot;), class = &quot;factor&quot;), purpose = structure(c(8L, 
3L, 3L, 1L, 1L, 1L), .Label = c(&quot;business&quot;, &quot;car (new)&quot;, &quot;car (used)&quot;, 
&quot;domestic appliances&quot;, &quot;education&quot;, &quot;furniture&quot;, &quot;others&quot;, &quot;radio/tv&quot;, 
&quot;repairs&quot;, &quot;retraining&quot;), class = &quot;factor&quot;), amount = c(2611, 
6187, 2197, 2767, 6416, 3863), savings_balance = structure(c(1L, 
3L, 5L, 3L, 1L, 1L), .Label = c(&quot;&lt; 100 DM&quot;, &quot;&gt; 1000 DM&quot;, &quot;101 - 500 DM&quot;, 
&quot;501 - 1000 DM&quot;, &quot;unknown&quot;), class = &quot;factor&quot;), employment_length = structure(c(1L, 
4L, 4L, 1L, 1L, 3L), .Label = c(&quot;&gt; 7 yrs&quot;, &quot;0 - 1 yrs&quot;, &quot;1 - 4 yrs&quot;, 
&quot;4 - 7 yrs&quot;, &quot;unemployed&quot;), class = &quot;factor&quot;), installment_rate = structure(c(4L, 
1L, 4L, 4L, 4L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    personal_status = structure(c(3L, 3L, 4L, 1L, 2L, 4L), .Label = c(&quot;divorced male&quot;, 
    &quot;female&quot;, &quot;married male&quot;, &quot;single male&quot;), class = &quot;factor&quot;), 
    other_debtors = structure(c(1L, 3L, 3L, 3L, 3L, 3L), .Label = c(&quot;co-applicant&quot;, 
    &quot;guarantor&quot;, &quot;none&quot;), class = &quot;factor&quot;), residence_history = structure(c(3L, 
    4L, 4L, 2L, 3L, 2L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    property = structure(c(3L, 2L, 2L, 2L, 4L, 4L), .Label = c(&quot;building society savings&quot;, 
    &quot;other&quot;, &quot;real estate&quot;, &quot;unknown/none&quot;), class = &quot;factor&quot;), 
    age = c(46, 24, 43, 61, 59, 32), installment_plan = structure(c(2L, 
    2L, 2L, 1L, 2L, 2L), .Label = c(&quot;bank&quot;, &quot;none&quot;, &quot;stores&quot;), class = &quot;factor&quot;), 
    housing = structure(c(2L, 3L, 2L, 3L, 3L, 1L), .Label = c(&quot;for free&quot;, 
    &quot;own&quot;, &quot;rent&quot;), class = &quot;factor&quot;), existing_credits = structure(c(2L, 
    2L, 2L, 2L, 1L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    default = structure(c(1L, 1L, 1L, 2L, 2L, 1L), .Label = c(&quot;paid&quot;, 
    &quot;default&quot;), class = &quot;factor&quot;), dependents = structure(c(1L, 
    1L, 2L, 1L, 1L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;), class = &quot;factor&quot;), 
    telephone = structure(c(1L, 1L, 2L, 1L, 1L, 1L), .Label = c(&quot;none&quot;, 
    &quot;yes&quot;), class = &quot;factor&quot;), foreign_worker = structure(c(2L, 
    2L, 2L, 2L, 2L, 2L), .Label = c(&quot;no&quot;, &quot;yes&quot;), class = &quot;factor&quot;), 
    job = structure(c(2L, 2L, 2L, 4L, 2L, 2L), .Label = c(&quot;management self-employed&quot;, 
    &quot;skilled employee&quot;, &quot;unemployed non-resident&quot;, &quot;unskilled resident&quot;
    ), class = &quot;factor&quot;)), row.names = c(NA, -6L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
",43,1,0,4,r;machine-learning;classification;c5.0,2022-07-10 13:17:02,2022-07-10 13:17:02,2022-07-11 12:04:35,i am using rstudio     ghost orchid  release for macos  i am learning to use to c  algorithm in r  for this i am following  machine learning in r  by brett lantz  the dataset i am using is a modified version of one relating to loans obtained from a credit agency in germany  the data has no missing values  and no empty factor levels  this has caused the same error in other posts i have viewed   i have split the data into training and test tibbles using the initial_split   function in rsample package  the structure of the data is  my issue is specifically when i try to fit a model using a cost matrix  without this cost matrix  the model does not throw this error  this is how i have created the cost matrix  i must also point out that i have tried several ways to create this matrix  including literally copying the exact method given in the lantz book  and they all result in this same error  here is the code i am using to try and fit the model  however  this also happens if i use the x   credit_train   gt   select  default   y   credit_train default rather than the formula approach  and any similar approaches i can find or think of  i am at a complete loss as to why i am getting this error  anyone have any ideas                                         in response to a request for dput credit_train  here is the output for dput head credit_train    it seems too large otherwise ,why do i get    c code called exit with value     in r ,using rstudio ghost orchid release macos learning use c algorithm r following machine learning r brett lantz dataset using modified version one relating loans obtained credit agency germany data missing values empty factor levels caused error posts viewed split data training test tibbles using initial_split function rsample package structure data issue specifically try fit model using cost matrix without cost matrix model throw error created cost matrix must also point tried several ways create matrix including literally copying exact method given lantz book result error code using try fit model however also happens use x credit_train gt select default credit_train default rather formula approach similar approaches find think complete loss getting error anyone ideas response request dput credit_train output dput head credit_train seems large otherwise,get c code called exit value r,get c code called exit value rusing rstudio ghost orchid release macos learning use c algorithm r following machine learning r brett lantz dataset using modified version one relating loans obtained credit agency germany data missing values empty factor levels caused error posts viewed split data training test tibbles using initial_split function rsample package structure data issue specifically try fit model using cost matrix without cost matrix model throw error created cost matrix must also point tried several ways create matrix including literally copying exact method given lantz book result error code using try fit model however also happens use x credit_train gt select default credit_train default rather formula approach similar approaches find think complete loss getting error anyone ideas response request dput credit_train output dput head credit_train seems large otherwise,"['get', 'c', 'code', 'called', 'exit', 'value', 'rusing', 'rstudio', 'ghost', 'orchid', 'release', 'macos', 'learning', 'use', 'c', 'algorithm', 'r', 'following', 'machine', 'learning', 'r', 'brett', 'lantz', 'dataset', 'using', 'modified', 'version', 'one', 'relating', 'loans', 'obtained', 'credit', 'agency', 'germany', 'data', 'missing', 'values', 'empty', 'factor', 'levels', 'caused', 'error', 'posts', 'viewed', 'split', 'data', 'training', 'test', 'tibbles', 'using', 'initial_split', 'function', 'rsample', 'package', 'structure', 'data', 'issue', 'specifically', 'try', 'fit', 'model', 'using', 'cost', 'matrix', 'without', 'cost', 'matrix', 'model', 'throw', 'error', 'created', 'cost', 'matrix', 'must', 'also', 'point', 'tried', 'several', 'ways', 'create', 'matrix', 'including', 'literally', 'copying', 'exact', 'method', 'given', 'lantz', 'book', 'result', 'error', 'code', 'using', 'try', 'fit', 'model', 'however', 'also', 'happens', 'use', 'x', 'credit_train', 'gt', 'select', 'default', 'credit_train', 'default', 'rather', 'formula', 'approach', 'similar', 'approaches', 'find', 'think', 'complete', 'loss', 'getting', 'error', 'anyone', 'ideas', 'response', 'request', 'dput', 'credit_train', 'output', 'dput', 'head', 'credit_train', 'seems', 'large', 'otherwise']","['get', 'c', 'code', 'call', 'exit', 'valu', 'ruse', 'rstudio', 'ghost', 'orchid', 'releas', 'maco', 'learn', 'use', 'c', 'algorithm', 'r', 'follow', 'machin', 'learn', 'r', 'brett', 'lantz', 'dataset', 'use', 'modifi', 'version', 'one', 'relat', 'loan', 'obtain', 'credit', 'agenc', 'germani', 'data', 'miss', 'valu', 'empti', 'factor', 'level', 'caus', 'error', 'post', 'view', 'split', 'data', 'train', 'test', 'tibbl', 'use', 'initial_split', 'function', 'rsampl', 'packag', 'structur', 'data', 'issu', 'specif', 'tri', 'fit', 'model', 'use', 'cost', 'matrix', 'without', 'cost', 'matrix', 'model', 'throw', 'error', 'creat', 'cost', 'matrix', 'must', 'also', 'point', 'tri', 'sever', 'way', 'creat', 'matrix', 'includ', 'liter', 'copi', 'exact', 'method', 'given', 'lantz', 'book', 'result', 'error', 'code', 'use', 'tri', 'fit', 'model', 'howev', 'also', 'happen', 'use', 'x', 'credit_train', 'gt', 'select', 'default', 'credit_train', 'default', 'rather', 'formula', 'approach', 'similar', 'approach', 'find', 'think', 'complet', 'loss', 'get', 'error', 'anyon', 'idea', 'respons', 'request', 'dput', 'credit_train', 'output', 'dput', 'head', 'credit_train', 'seem', 'larg', 'otherwis']"
247,264,264,614157,31610971,Spark - repartition() vs coalesce(),"<p>According to Learning Spark</p>

<blockquote>
  <p>Keep in mind that repartitioning your data is a fairly expensive operation.
  Spark also has an optimized version of <code>repartition()</code> called <code>coalesce()</code> that allows avoiding data movement, but only if you are decreasing the number of RDD partitions.</p>
</blockquote>

<p>One difference I get is that with <code>repartition()</code> the number of partitions can be increased/decreased, but with <code>coalesce()</code> the number of partitions can only be decreased.</p>

<p>If the partitions are spread across multiple machines and <code>coalesce()</code> is run, how can it avoid data movement?</p>
",326210,20,379,3,apache-spark;distributed-computing;rdd,2015-07-24 15:49:19,2015-07-24 15:49:19,2022-07-11 09:23:03,according to learning spark one difference i get is that with repartition   the number of partitions can be increased decreased  but with coalesce   the number of partitions can only be decreased  if the partitions are spread across multiple machines and coalesce   is run  how can it avoid data movement ,spark   repartition   vs coalesce  ,according learning spark one difference get repartition number partitions increased decreased coalesce number partitions decreased partitions spread across multiple machines coalesce run avoid data movement,spark repartition vs coalesce,spark repartition vs coalesceaccording learning spark one difference get repartition number partitions increased decreased coalesce number partitions decreased partitions spread across multiple machines coalesce run avoid data movement,"['spark', 'repartition', 'vs', 'coalesceaccording', 'learning', 'spark', 'one', 'difference', 'get', 'repartition', 'number', 'partitions', 'increased', 'decreased', 'coalesce', 'number', 'partitions', 'decreased', 'partitions', 'spread', 'across', 'multiple', 'machines', 'coalesce', 'run', 'avoid', 'data', 'movement']","['spark', 'repartit', 'vs', 'coalesceaccord', 'learn', 'spark', 'one', 'differ', 'get', 'repartit', 'number', 'partit', 'increas', 'decreas', 'coalesc', 'number', 'partit', 'decreas', 'partit', 'spread', 'across', 'multipl', 'machin', 'coalesc', 'run', 'avoid', 'data', 'movement']"
248,265,265,14837550,65324610,Python Machine Learning - Rule based match,"<p>I am new to machine learning and need help on the best approach.</p>
<p>I have a master dataset with millions of rows with columns:</p>
<pre><code>Customer first name, 
last name, 
SSN , 
address,
Unique cust id 
</code></pre>
<p>Input is a new customer details with same columns.I want to create a machine learning model with following rules</p>
<pre><code>If new customer matches any customer on SSN then return cust ids of
    matching customers  
else if customer matches any customer on First +
    Last name + zip then return cust ids of matching customers  
else
    create new cust id
</code></pre>
<p>The other issue is that name and address could have spelling errors, so exact match is not an option</p>
<p>what is the best approach and what model will work</p>
",139,1,0,2,python;match,2020-12-16 15:51:58,2020-12-16 15:51:58,2022-07-11 06:35:43,i am new to machine learning and need help on the best approach  i have a master dataset with millions of rows with columns  input is a new customer details with same columns i want to create a machine learning model with following rules the other issue is that name and address could have spelling errors  so exact match is not an option what is the best approach and what model will work,python machine learning   rule based match,machine learning need help best approach master dataset millions rows columns input customer details columns want create machine learning model following rules issue name address could spelling errors exact match option best approach model work,python machine learning rule based match,python machine learning rule based matchmachine learning need help best approach master dataset millions rows columns input customer details columns want create machine learning model following rules issue name address could spelling errors exact match option best approach model work,"['python', 'machine', 'learning', 'rule', 'based', 'matchmachine', 'learning', 'need', 'help', 'best', 'approach', 'master', 'dataset', 'millions', 'rows', 'columns', 'input', 'customer', 'details', 'columns', 'want', 'create', 'machine', 'learning', 'model', 'following', 'rules', 'issue', 'name', 'address', 'could', 'spelling', 'errors', 'exact', 'match', 'option', 'best', 'approach', 'model', 'work']","['python', 'machin', 'learn', 'rule', 'base', 'matchmachin', 'learn', 'need', 'help', 'best', 'approach', 'master', 'dataset', 'million', 'row', 'column', 'input', 'custom', 'detail', 'column', 'want', 'creat', 'machin', 'learn', 'model', 'follow', 'rule', 'issu', 'name', 'address', 'could', 'spell', 'error', 'exact', 'match', 'option', 'best', 'approach', 'model', 'work']"
249,266,266,9744542,72932518,How to deal with a skewed Time series data,"<p>I have hourly data of no. of minutes spent online by people for 2 years. Hence the values are distributed between 0 and 60 and also most data is either 0 or 60. My goal is to predict the number of minutes the person will spend online in the future (next day/hour/month etc.). What kind of approach or machine learning model can I use to predict this data? Can this be modelled into a regression/forecasting problem in spite of the skewness?<a href=""https://i.stack.imgur.com/Vk93I.png"" rel=""nofollow noreferrer"">hourly data</a></p>
",29,1,-2,2,machine-learning;time-series,2022-07-11 02:29:36,2022-07-11 02:29:36,2022-07-11 04:22:05,i have hourly data of no  of minutes spent online by people for  years  hence the values are distributed between  and  and also most data is either  or   my goal is to predict the number of minutes the person will spend online in the future  next day hour month etc    what kind of approach or machine learning model can i use to predict this data  can this be modelled into a regression forecasting problem in spite of the skewness ,how to deal with a skewed time series data,hourly data minutes spent online people years hence values distributed also data either goal predict number minutes person spend online future next day hour month etc kind approach machine learning model use predict data modelled regression forecasting problem spite skewness,deal skewed time series data,deal skewed time series datahourly data minutes spent online people years hence values distributed also data either goal predict number minutes person spend online future next day hour month etc kind approach machine learning model use predict data modelled regression forecasting problem spite skewness,"['deal', 'skewed', 'time', 'series', 'datahourly', 'data', 'minutes', 'spent', 'online', 'people', 'years', 'hence', 'values', 'distributed', 'also', 'data', 'either', 'goal', 'predict', 'number', 'minutes', 'person', 'spend', 'online', 'future', 'next', 'day', 'hour', 'month', 'etc', 'kind', 'approach', 'machine', 'learning', 'model', 'use', 'predict', 'data', 'modelled', 'regression', 'forecasting', 'problem', 'spite', 'skewness']","['deal', 'skew', 'time', 'seri', 'datahourli', 'data', 'minut', 'spent', 'onlin', 'peopl', 'year', 'henc', 'valu', 'distribut', 'also', 'data', 'either', 'goal', 'predict', 'number', 'minut', 'person', 'spend', 'onlin', 'futur', 'next', 'day', 'hour', 'month', 'etc', 'kind', 'approach', 'machin', 'learn', 'model', 'use', 'predict', 'data', 'model', 'regress', 'forecast', 'problem', 'spite', 'skew']"
250,267,267,14515155,72931509,Symbol not found error while importing tensorflow in M1 Macbook Pro,"<p>I have installed tensorflow in my M1 Macbook Pro using these commands in a conda environment:</p>
<pre><code>conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
python -m pip install tensorflow-datasets
conda install jupyter pandas numpy matplotlib scikit-learn
</code></pre>
<p>I did it following this instruction - <a href=""https://github.com/mrdbourke/m1-machine-learning-test"" rel=""nofollow noreferrer"">https://github.com/mrdbourke/m1-machine-learning-test</a></p>
<p>But when I import the packages I installed it gives me an error message.</p>
<p>Here:</p>
<pre><code>import numpy as np
import pandas as pd
import sklearn
import tensorflow as tf
import matplotlib.pyplot as plt

# Check for TensorFlow GPU access
print(f&quot;TensorFlow has access to the following devices:\n{tf.config.list_physical_devices()}&quot;)

# See TensorFlow version
print(f&quot;TensorFlow version: {tf.__version__}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
Input In [1], in &lt;cell line: 4&gt;()
      2 import pandas as pd
      3 import sklearn
----&gt; 4 import tensorflow as tf
      5 import matplotlib.pyplot as plt
      7 # Check for TensorFlow GPU access

File ~/miniforge3/lib/python3.9/site-packages/tensorflow/__init__.py:443, in &lt;module&gt;
    441 _plugin_dir = _os.path.join(_s, 'tensorflow-plugins')
    442 if _os.path.exists(_plugin_dir):
--&gt; 443   _ll.load_library(_plugin_dir)
    444   # Load Pluggable Device Library
    445   _ll.load_pluggable_device_library(_plugin_dir)

File ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:151, in load_library(library_location)
    148     kernel_libraries = [library_location]
    150   for lib in kernel_libraries:
--&gt; 151     py_tf.TF_LoadLibrary(lib)
    153 else:
    154   raise OSError(
    155       errno.ENOENT,
    156       'The file or folder to load kernel libraries from does not exist.',
    157       library_location)

NotFoundError: dlopen(/Users/arannya/miniforge3/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: __ZNKSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEE3strEv
  Referenced from: /Users/arannya/miniforge3/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib (which was built for Mac OS X 12.3)
  Expected in: /usr/lib/libc++.1.dylib
</code></pre>
<p>Please help me solve this issue. Thank you.</p>
",37,0,0,3,python;tensorflow;apple-m1,2022-07-10 22:59:45,2022-07-10 22:59:45,2022-07-10 22:59:45,i have installed tensorflow in my m macbook pro using these commands in a conda environment  i did it following this instruction    but when i import the packages i installed it gives me an error message  here  error  please help me solve this issue  thank you ,symbol not found error while importing tensorflow in m macbook pro,installed tensorflow macbook pro using commands conda environment following instruction import packages installed gives error message error please help solve issue thank,symbol found error importing tensorflow macbook pro,symbol found error importing tensorflow macbook proinstalled tensorflow macbook pro using commands conda environment following instruction import packages installed gives error message error please help solve issue thank,"['symbol', 'found', 'error', 'importing', 'tensorflow', 'macbook', 'proinstalled', 'tensorflow', 'macbook', 'pro', 'using', 'commands', 'conda', 'environment', 'following', 'instruction', 'import', 'packages', 'installed', 'gives', 'error', 'message', 'error', 'please', 'help', 'solve', 'issue', 'thank']","['symbol', 'found', 'error', 'import', 'tensorflow', 'macbook', 'proinstal', 'tensorflow', 'macbook', 'pro', 'use', 'command', 'conda', 'environ', 'follow', 'instruct', 'import', 'packag', 'instal', 'give', 'error', 'messag', 'error', 'pleas', 'help', 'solv', 'issu', 'thank']"
251,268,268,13605647,72929325,nginx: [emerg] invalid number of arguments in &quot;include&quot; directive,"<p>I am learning how to deploy AWS for the first time. I am following this guide here: <a href=""https://www.youtube.com/watch?v=HtWgb_vbyvY"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=HtWgb_vbyvY</a>.</p>
<p>I am using windows machine while the person in the video uses mac.</p>
<p>I am getting the following error when running ngnix commands on the windows terminal: &quot;nginx: [emerg] CreateFile() &quot;C:/nginx-1.23.0/nginx-1.23.0/mime.types&quot; failed (3: The system cannot find the path specified) in C:\Users\Shi Jie\Downloads\nginx-1.23.0\nginx-1.23.0/conf/nginx.conf:12&quot;</p>
<p>i think it is how i write the path on my ngnix.conf which i write as such</p>
<pre><code>worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include \Users\Shi Jie\Downloads\nginx-1.23.0\nginx-1.23.0\mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                      '$status $body_bytes_sent &quot;$http_referer&quot; '
                      '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    client_body_buffer_size 100k;
    client_header_buffer_size 1k;
    client_max_body_size 100k;
    large_client_header_buffers 2 1k;
    client_body_timeout 10;
    client_header_timeout 10;
    keepalive_timeout 5 5;
    send_timeout 10;
    server_tokens off;
    #gzip  on; on;

    include /etc/nginx/conf.d/*.conf;
}


</code></pre>
<p>Can anyone point the way to teach me how to write the path properly? Thank you.</p>
",32,1,0,2,amazon-web-services;amazon-ec2,2022-07-10 17:42:01,2022-07-10 17:42:01,2022-07-10 20:38:04,i am learning how to deploy aws for the first time  i am following this guide here    i am using windows machine while the person in the video uses mac  i am getting the following error when running ngnix commands on the windows terminal   nginx   emerg  createfile    c  nginx    nginx    mime types  failed    the system cannot find the path specified  in c  users shi jie downloads nginx    nginx    conf nginx conf   i think it is how i write the path on my ngnix conf which i write as such can anyone point the way to teach me how to write the path properly  thank you ,nginx   emerg  invalid number of arguments in  include  directive,learning deploy aws first time following guide using windows machine person video uses mac getting following error running ngnix commands windows terminal nginx emerg createfile c nginx nginx mime types failed system cannot find path specified c users shi jie downloads nginx nginx conf nginx conf think write path ngnix conf write anyone point way teach write path properly thank,nginx emerg invalid number arguments include directive,nginx emerg invalid number arguments include directivelearning deploy aws first time following guide using windows machine person video uses mac getting following error running ngnix commands windows terminal nginx emerg createfile c nginx nginx mime types failed system cannot find path specified c users shi jie downloads nginx nginx conf nginx conf think write path ngnix conf write anyone point way teach write path properly thank,"['nginx', 'emerg', 'invalid', 'number', 'arguments', 'include', 'directivelearning', 'deploy', 'aws', 'first', 'time', 'following', 'guide', 'using', 'windows', 'machine', 'person', 'video', 'uses', 'mac', 'getting', 'following', 'error', 'running', 'ngnix', 'commands', 'windows', 'terminal', 'nginx', 'emerg', 'createfile', 'c', 'nginx', 'nginx', 'mime', 'types', 'failed', 'system', 'can', 'not', 'find', 'path', 'specified', 'c', 'users', 'shi', 'jie', 'downloads', 'nginx', 'nginx', 'conf', 'nginx', 'conf', 'think', 'write', 'path', 'ngnix', 'conf', 'write', 'anyone', 'point', 'way', 'teach', 'write', 'path', 'properly', 'thank']","['nginx', 'emerg', 'invalid', 'number', 'argument', 'includ', 'directivelearn', 'deploy', 'aw', 'first', 'time', 'follow', 'guid', 'use', 'window', 'machin', 'person', 'video', 'use', 'mac', 'get', 'follow', 'error', 'run', 'ngnix', 'command', 'window', 'termin', 'nginx', 'emerg', 'createfil', 'c', 'nginx', 'nginx', 'mime', 'type', 'fail', 'system', 'can', 'not', 'find', 'path', 'specifi', 'c', 'user', 'shi', 'jie', 'download', 'nginx', 'nginx', 'conf', 'nginx', 'conf', 'think', 'write', 'path', 'ngnix', 'conf', 'write', 'anyon', 'point', 'way', 'teach', 'write', 'path', 'properli', 'thank']"
252,269,269,9721314,72924904,Stratified Sampling in Python without scikit-learn,"<p>I have a vector which contains 10 values of sample 1 and 25 values of sample 2.</p>
<pre><code>Fact = np.array((2,2,2,2,1,2,1,1,2,2,2,1,2,2,2,1,2,2,2,1,2,2,1,1,2,1,2,2,2,2,2,2,1,2,2))
</code></pre>
<p>I want to create a stratified output vector where :</p>
<p>sample 1 is divided in 80% : 8 values of 1 and 20% : 2 values of 0.</p>
<p>sample 2 is divided in 80% : 20 values of 1 and 20% : 5 values of 0.</p>
<p>The expected output will be :</p>
<pre><code>Output = np.array((0,1,1,1,0,1,1,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1))
</code></pre>
<p>How can I automate this ? I cant use the sampling function from scikit-learn because it is not for a machine learning experience.</p>
",46,1,0,2,python;numpy,2022-07-10 01:05:59,2022-07-10 01:05:59,2022-07-10 04:47:43,i have a vector which contains  values of sample  and  values of sample   i want to create a stratified output vector where   sample  is divided in      values of  and      values of   sample  is divided in      values of  and      values of   the expected output will be   how can i automate this   i can t use the sampling function from scikit learn because it is not for a machine learning experience ,stratified sampling in python without scikit learn,vector contains values sample values sample want create stratified output vector sample divided values values sample divided values values expected output automate use sampling function scikit learn machine learning experience,stratified sampling python without scikit learn,stratified sampling python without scikit learnvector contains values sample values sample want create stratified output vector sample divided values values sample divided values values expected output automate use sampling function scikit learn machine learning experience,"['stratified', 'sampling', 'python', 'without', 'scikit', 'learnvector', 'contains', 'values', 'sample', 'values', 'sample', 'want', 'create', 'stratified', 'output', 'vector', 'sample', 'divided', 'values', 'values', 'sample', 'divided', 'values', 'values', 'expected', 'output', 'automate', 'use', 'sampling', 'function', 'scikit', 'learn', 'machine', 'learning', 'experience']","['stratifi', 'sampl', 'python', 'without', 'scikit', 'learnvector', 'contain', 'valu', 'sampl', 'valu', 'sampl', 'want', 'creat', 'stratifi', 'output', 'vector', 'sampl', 'divid', 'valu', 'valu', 'sampl', 'divid', 'valu', 'valu', 'expect', 'output', 'autom', 'use', 'sampl', 'function', 'scikit', 'learn', 'machin', 'learn', 'experi']"
