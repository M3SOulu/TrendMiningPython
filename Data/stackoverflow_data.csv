,Unnamed: 0,AuthorId,Q_id,Title,Abstract,Views,Answers,Cites,Tags_n,Tags,Date,CR_Date,LA_Date,Abstract_clean
0,0,15958905,72794914,Survey bot or Survey Chatbot with Flask and HTML,"<p>So now I'm working on a survey chatbot that requires users to answer my questions. After the said user answered the question, I will append their answers to a pandas dataframe and my machine learning model will predict the disease based on the symptoms. My machine learning model has been saved to json and now I'm really confused on how to make it work with Flask and HTML. I've watched so many chatbot tutorials but they usually just predict the user's input and response right away. I want my chatbot to ask questions and append user's inputs in a loop before predict it all. Here's my code snippet so far.</p>
<pre><code>from flask import Flask, render_template, request
from xgboost import XGBClassifier

app = Flask(__name__)

@app.route('/welcome')

def home():
    return render_template('home.html')

@app.route('/chatbot')
def get_user():
    name=request.args.get('name')
    gender=request.form['gender']
    age=request.form['age']
    hypertension=request.form['hypertension']
    heart_disease=request.form['heart_disease']
    ever_married=request.form['ever_married']
    work_type=request.form['work_type']
    residence_type=request.form['residence_type']
    weight=request.form['weight']
    height=request.form['height']
    smoking=request.form['smoking']
    return render_template('fortunecookie.html',name=name)

def predict():
    data=pd.DataFrame({'gender':[gender],
                             'age':[age],
                             'hypertension':[hypertension],
                             'heart_disease':[heart_disease],
                             'ever_married':[ever_married],
                             'work_type':[work_type],
                             'residence_type':[residence_type],
                             'bmi':[weight/(height/100)**2],
                             'smoking':[smoking]})
    model=XGBClassifier.load_model(&quot;xgbimbalance.json&quot;)
    predresult=model.predict(data.values)

if __name__==&quot;__main__&quot;:
    app.run()
</code></pre>
<p>Thank you in advance, I'm sorry if it's a bit confusing but the point is to design a survey bot with machine learning to predict the user's input after all the questions are answered.</p>
",17,0,-1,2,python;flask,2022-06-29 07:18:53,2022-06-29 07:18:53,2022-06-29 09:29:31,so now i m working on a survey chatbot that requires users to answer my questions  after the said user answered the question  i will append their answers to a pandas dataframe and my machine learning model will predict the disease based on the symptoms  my machine learning model has been saved to json and now i m really confused on how to make it work with flask and html  i ve watched so many chatbot tutorials but they usually just predict the user s input and response right away  i want my chatbot to ask questions and append user s inputs in a loop before predict it all  here s my code snippet so far  thank you in advance  i m sorry if it s a bit confusing but the point is to design a survey bot with machine learning to predict the user s input after all the questions are answered 
1,1,15535535,72794500,Missed Python package for machine learning and neural networks,"<p>I’m running a neural network codes downloaded from github (<a href=""https://github.com/okada39/pinn_wave"" rel=""nofollow noreferrer"">https://github.com/okada39/pinn_wave</a>) that solves the 1D wave equation via PINNs algorithm.
After running my executed neural network code.py by cmd I got:</p>
<pre><code>Traceback (most recent call last):
File &quot;file path&quot;, line 1, in &lt;module&gt;
import lib.tf_silent
ModuleNotFoundError: No module
named 'lib.tf_silent'
</code></pre>
<p>How to fix this on windows 10_64bit Enterprise LTSC.</p>
<p>PS: I can download tensorflow and other packages except this lib; and still get the same error.</p>
<p>Please help!</p>
",24,0,-2,2,python;importerror,2022-06-29 05:54:35,2022-06-29 05:54:35,2022-06-29 07:39:57,how to fix this on windows _bit enterprise ltsc  ps  i can download tensorflow and other packages except this lib  and still get the same error  please help 
2,2,19437661,72791482,"Not able to upload project on heroku,getting the below error","<pre><code>(carprediction) E:\Machine learning project( car price predictor)&gt;python app.py
C:\Users\sourav sharma\anaconda3\envs\carprediction\lib\site-packages\sklearn\base.py:338: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations
  UserWarning,
C:\Users\sourav sharma\anaconda3\envs\carprediction\lib\site-packages\sklearn\base.py:338: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations
</code></pre>
",22,0,-4,1,scikit-learn,2022-06-28 23:56:50,2022-06-28 23:56:50,2022-06-29 07:00:28,
3,3,15797456,72794761,Displaying an Image with Matplotlib in deeplearning,"<p>#loading test images
val_ds = tf.keras.preprocessing.image_dataset_from_directory(test_path,image_size=(img_height,img_width),
label_mode='int',
batch_size=batch_size)</p>
<p>import matplotlib.pyplot as plt</p>
<p>plt.figure(figsize=(12,12))
for img ,label in val_ds.take(1):
for i in range(12):
ax = plt.subplot(4,3,i + 1)
plt.imshow(img[i].numpy().astype('uint8'))
plt.title(class_name[label[i]])
plt.axis('off')</p>
<p>Then I got this error :</p>
<p>otFoundError: NewRandomAccessFile failed to Create/Open: D:\Machine Learning\DPL\Deep Learning for Computer Vision with TensorFlow 2[TutsNode.com] - Deep Learning for Computer Vision with TensorFlow 2\3. Convolutional Neural Networks\15.1 covid19\covid19\test\Covid\auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg : The system cannot find the path specified.
; No such process
[[{{node ReadFile}}]] [Op:IteratorGetNext]</p>
<p>Any help ??</p>
",13,0,0,4,python;tensorflow;keras;deep-learning,2022-06-29 06:52:37,2022-06-29 06:52:37,2022-06-29 06:52:37,import matplotlib pyplot as plt then i got this error   any help   
4,4,5983841,72789747,What is libpython3.9.a and what is used for?,"<p>I've searched the web and couldn't find an answer.</p>
<p>I know what <code>.a</code> and <code>.so</code> files and know how to build a shared library.</p>
<p>One day, when a colleague ran a python command in one of python environments of the production server, it gives a <code>libpython3.x.a</code> not found error and I had no idea what happens.</p>
<p>So these days I have been learning building python from source, reading the <code>make</code> log and <code>make install</code> log, so as to know more about the python installation process and file structures.</p>
<p>And finally I see <code>libpython3.x.a</code> is built with</p>
<pre><code>ar rcs libpython3.9.a Modules/getbuildinfo.o Parser/acceler.o Parser/grammar1.o Parser/listnode.o Parser/node.o Parser/parser.o Parser/token.o  Parser/pegen/pegen.o Parser/pegen/parse.o Parser/pegen/parse_string.o Parser/pegen/peg_api.o Parser/myreadline.o Parser/parsetok.o Parser/tokenizer.o Objects/abstract.o Objects/accu.o Objects/boolobject.o Objects/bytes_methods.o Objects/bytearrayobject.o Objects/bytesobject.o Objects/call.o Objects/capsule.o Objects/cellobject.o Objects/classobject.o Objects/codeobject.o Objects/complexobject.o Objects/descrobject.o Objects/enumobject.o Objects/exceptions.o Objects/genericaliasobject.o Objects/genobject.o Objects/fileobject.o Objects/floatobject.o Objects/frameobject.o Objects/funcobject.o Objects/interpreteridobject.o Objects/iterobject.o Objects/listobject.o Objects/longobject.o Objects/dictobject.o Objects/odictobject.o Objects/memoryobject.o Objects/methodobject.o Objects/moduleobject.o Objects/namespaceobject.o Objects/object.o Objects/obmalloc.o Objects/picklebufobject.o Objects/rangeobject.o Objects/setobject.o Objects/sliceobject.o Objects/structseq.o Objects/tupleobject.o Objects/typeobject.o Objects/unicodeobject.o Objects/unicodectype.o Objects/weakrefobject.o Python/_warnings.o Python/Python-ast.o Python/asdl.o Python/ast.o Python/ast_opt.o Python/ast_unparse.o Python/bltinmodule.o Python/ceval.o Python/codecs.o Python/compile.o Python/context.o Python/dynamic_annotations.o Python/errors.o Python/frozenmain.o Python/future.o Python/getargs.o Python/getcompiler.o Python/getcopyright.o Python/getplatform.o Python/getversion.o Python/graminit.o Python/hamt.o Python/hashtable.o Python/import.o Python/importdl.o Python/initconfig.o Python/marshal.o Python/modsupport.o Python/mysnprintf.o Python/mystrtoul.o Python/pathconfig.o Python/peephole.o Python/preconfig.o Python/pyarena.o Python/pyctype.o Python/pyfpe.o Python/pyhash.o Python/pylifecycle.o Python/pymath.o Python/pystate.o Python/pythonrun.o Python/pytime.o Python/bootstrap_hash.o Python/structmember.o Python/symtable.o Python/sysmodule.o Python/thread.o Python/traceback.o Python/getopt.o Python/pystrcmp.o Python/pystrtod.o Python/pystrhex.o Python/dtoa.o Python/formatter_unicode.o Python/fileutils.o Python/dynload_shlib.o    Modules/config.o Modules/getpath.o Modules/main.o Modules/gcmodule.o Modules/posixmodule.o  Modules/errnomodule.o  Modules/pwdmodule.o  Modules/_sre.o  Modules/_codecsmodule.o  Modules/_weakref.o  Modules/_functoolsmodule.o  Modules/_operator.o  Modules/_collectionsmodule.o  Modules/_abc.o  Modules/itertoolsmodule.o  Modules/atexitmodule.o  Modules/signalmodule.o  Modules/_stat.o  Modules/timemodule.o  Modules/_threadmodule.o  Modules/_localemodule.o  Modules/_iomodule.o Modules/iobase.o Modules/fileio.o Modules/bytesio.o Modules/bufferedio.o Modules/textio.o Modules/stringio.o  Modules/faulthandler.o  Modules/_tracemalloc.o  Modules/_peg_parser.o  Modules/symtablemodule.o  Modules/xxsubtype.o Python/frozen.o
</code></pre>
<p>I found similar header files in my self-built python <code>~/py3.9.13_new/include/python3.9</code> directory.</p>
<pre><code>tian@tian-B250M-Wind:~/py3.9.13_new/include/python3.9$ ls
abstract.h         cellobject.h     cpython                exports.h             graminit.h             longintrepr.h      node.h         patchlevel.h       pydtrace.h     pymacro.h   Python-ast.h   structmember.h  typeslots.h
asdl.h             ceval.h          datetime.h             fileobject.h          grammar.h              longobject.h       object.h       picklebufobject.h  pyerrors.h     pymath.h    Python.h       structseq.h     ucnhash.h
ast.h              classobject.h    descrobject.h          fileutils.h           import.h               marshal.h          objimpl.h      pyarena.h          pyexpat.h      pymem.h     pythonrun.h    symtable.h      unicodeob

...
</code></pre>
<p>And there is a <a href=""https://docs.python.org/3/using/configure.html#cmdoption-enable-shared"" rel=""nofollow noreferrer"">configuration option</a> on whether to build <code>.so</code> or <code>.a</code> :</p>
<pre><code>3.1.6. Linker options¶
--enable-shared
Enable building a shared Python library: libpython (default is no).
</code></pre>
<p>So my question is:</p>
<ol>
<li><p>What is <code>libpython3.x.a</code> or <code>libpython3.x.so</code>? For example my Ubuntu machine have a <code>libpython3.8</code> package (<a href=""https://packages.ubuntu.com/focal/amd64/libpython3.8/filelist"" rel=""nofollow noreferrer"">https://packages.ubuntu.com/focal/amd64/libpython3.8/filelist</a>) come along with the system. And what are they used for?</p>
</li>
<li><p>Well I also have little clue on what's inside <code>~/py3.9.13_new/lib/python3.9/lib-dynload</code>：</p>
</li>
</ol>
<pre><code>tian@tian-B250M-Wind:~/py3.9.13_new/lib/python3.9/lib-dynload$ ls
array.cpython-39-x86_64-linux-gnu.so            _curses.cpython-39-x86_64-linux-gnu.so           nis.cpython-39-x86_64-linux-gnu.so               _sqlite3.cpython-39-x86_64-linux-gnu.so
_asyncio.cpython-39-x86_64-linux-gnu.so         _curses_panel.cpython-39-x86_64-linux-gnu.so     _opcode.cpython-39-x86_64-linux-gnu.so           _ssl.cpython-39-x86_64-linux-gnu.so
audioop.cpython-39-x86_64-linux-gnu.so          _datetime.cpython-39-x86_64-linux-gnu.so         ossaudiodev.cpython-39-x86_64-linux-gnu.so       _statistics.cpython-39-x86_64-linux-gnu.so
binascii.cpython-39-x86_64-linux-gnu.so         _dbm.cpython-39-x86_64-linux-gnu.so              parser.cpython-39-x86_64-linux-gnu.so            _struct.cpython-39-x86_64-linux-gnu.so
_bisect.cpython-39-x86_64-linux-gnu.so          _decimal.cpython-39-x86_64-linux-gnu.so          _pickle.cpython-39-x86_64-linux-gnu.so           syslog.cpython-39-x86_64-linux-gnu.so
_blake2.cpython-39-x86_64-linux-gnu.so          _elementtree.cpython-39-x86_64-linux-gnu.so      _posixshmem.cpython-39-x86_64-linux-gnu.so       termios.cpython-39-x86_64-linux-gnu.so
_bz2.cpython-39-x86_64-linux-gnu.so             fcntl.cpython-39-x86_64-linux-gnu.so             _posixsubprocess.cpython-39-x86_64-linux-gnu.so  _testbuffer.cpython-39-x86_64-linux-gnu.so
...
</code></pre>
<p>are these <code>.so</code> also related to <code>libpython3.x.a</code> or <code>libpython3.x.so</code> ?</p>
<ol start=""3"">
<li>How should I fix my previous <code>libpython3.x.a</code> not found error?</li>
</ol>
",32,0,0,4,python;python-3.x;linux;cpython,2022-06-28 21:30:52,2022-06-28 21:30:52,2022-06-29 06:30:14,i ve searched the web and couldn t find an answer  i know what  a and  so files and know how to build a shared library  one day  when a colleague ran a python command in one of python environments of the production server  it gives a libpython x a not found error and i had no idea what happens  so these days i have been learning building python from source  reading the make log and make install log  so as to know more about the python installation process and file structures  and finally i see libpython x a is built with i found similar header files in my self built python   py  _new include python  directory  and there is a  on whether to build  so or  a   so my question is  what is libpython x a or libpython x so  for example my ubuntu machine have a libpython  package    come along with the system  and what are they used for  well i also have little clue on what s inside   py  _new lib python  lib dynload  are these  so also related to libpython x a or libpython x so  
5,5,15797456,72794225,Tensorboard Callback,"<p>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint</p>
<p>early_stopping =EarlyStopping(patience=5, monitor='val_loss', mode='min')
learning_rate_reduction = ReduceLROnPlateau(patience=4,monitor='val_loss', factor=0.1)</p>
<p>#path for saving the best weights
checkpoint_best_path =r'C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Model\Model best checkpoint'</p>
<p>check_bestpoint = ModelCheckpoint(filepath=checkpoint_best_path, save_weights_only=True, save_freq='epoch',
monitor='val_loss',save_best_only=True, verbose=1)</p>
<p>#tensorboard path
log_dir_path =r'C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Data\logs/'
log_dir = os.path.join(log_dir_path, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))</p>
<p>tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)</p>
<p>callbacks= [early_stopping, learning_rate_reduction,check_bestpoint,tensorboard_callback]</p>
<p>but I got an error stating :
NotFoundError: Failed to create a NewWriteableFile: C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Data\logs/20220628-231734\train/events.out.tfevents.1656501454.DESKTOP-1ULGF16.14344.2.v2 : The system cannot find the path specified.
; No such process
Creating writable file C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Data\logs/20220628-231734\train/events.out.tfevents.1656501454.DESKTOP-1ULGF16.14344.2.v2
Could not initialize events writer. [Op:CreateSummaryFileWriter]</p>
",17,1,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-06-29 05:02:40,2022-06-29 05:02:40,2022-06-29 05:49:36,from tensorflow keras callbacks import reducelronplateau  earlystopping  modelcheckpoint tensorboard_callback   tf keras callbacks tensorboard log_dir log_dir histogram_freq   callbacks   early_stopping  learning_rate_reduction check_bestpoint tensorboard_callback 
6,6,13251544,72794304,Trying to run a virtual machine on a desktop environment,"<p>I was using vagrant to use a virtual machine on a windows desktop environment. Since a few days ago, I am having trouble starting the virtual machine. I keep on getting the same error message when I tried to start it by typing - &quot;vagrant up&quot;. I uninstalled it, deleted all related files and reinstalled it but all in vain. As I am still learning on how to use vagrant, virtual machines, and generally computing, I really appreciate any help on this issue. Below is a snapshot of the error message.
<a href=""https://i.stack.imgur.com/x4BAg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x4BAg.png"" alt=""enter image description here"" /></a></p>
",8,0,0,1,vagrant-windows,2022-06-29 05:17:28,2022-06-29 05:17:28,2022-06-29 05:17:28,
7,7,19425906,72794079,Python (sklearn) train_test_split: choosing which data to train and which data to test,"<p>I want to use sklearn's train_test_split to manually split data into train and test categories. Specifically, in my .csv file, I want to use all the rows of data until the last row to train, and the last row to test. </p> The reason I'm doing this is because I need to launch a machine learning model but am incredibly short on time. I thought the best way would be to use predictions rather than deploying it using IBM Watson. I don't need it to be live. </p> My code so far looks like this:</p>
<pre><code>df=pd.read_csv('Book5.csv', names=['Amiability', 'Email'])

from sklearn.model_selection import train_test_split

df_x = df['Amiability']
df_y = df['Email']

x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=4) 
</code></pre>
<p>Then,</p>
<pre><code>len(df)
</code></pre>
<p>Produces</p>
<pre><code>331
</code></pre>
<p>I want to train with rows 0-330, and test with row 331. How can I do this?</p>
",24,1,0,4,python;pandas;machine-learning;scikit-learn,2022-06-29 04:36:45,2022-06-29 04:36:45,2022-06-29 05:12:46,i want to use sklearn s train_test_split to manually split data into train and test categories  specifically  in my  csv file  i want to use all the rows of data until the last row to train  and the last row to test   then  produces i want to train with rows    and test with row   how can i do this 
8,8,18204980,72767066,Stratified sampling getting odd proportion of factor levels,"<p>I'm experimenting with the Dry Bean dataset from UCI machine learning repo. I want to iterate through the dataset, repeatedly removing samples and running classifiers to see how accuracy changes as sample size decreases, but first I make the loop and check it works.</p>
<p>The dataset:</p>
<pre><code>&gt; summary(DryBean)
      Area          Perimeter      MajorAxisLength MinorAxisLength  AspectRation    Eccentricity      ConvexArea     EquivDiameter       Extent          Solidity     
 Min.   : 20420   Min.   : 524.7   Min.   :183.6   Min.   :122.5   Min.   :1.025   Min.   :0.2190   Min.   : 20684   Min.   :161.2   Min.   :0.5553   Min.   :0.9192  
 1st Qu.: 36328   1st Qu.: 703.5   1st Qu.:253.3   1st Qu.:175.8   1st Qu.:1.432   1st Qu.:0.7159   1st Qu.: 36715   1st Qu.:215.1   1st Qu.:0.7186   1st Qu.:0.9857  
 Median : 44652   Median : 794.9   Median :296.9   Median :192.4   Median :1.551   Median :0.7644   Median : 45178   Median :238.4   Median :0.7599   Median :0.9883  
 Mean   : 53048   Mean   : 855.3   Mean   :320.1   Mean   :202.3   Mean   :1.583   Mean   :0.7509   Mean   : 53768   Mean   :253.1   Mean   :0.7497   Mean   :0.9871  
 3rd Qu.: 61332   3rd Qu.: 977.2   3rd Qu.:376.5   3rd Qu.:217.0   3rd Qu.:1.707   3rd Qu.:0.8105   3rd Qu.: 62294   3rd Qu.:279.4   3rd Qu.:0.7869   3rd Qu.:0.9900  
 Max.   :254616   Max.   :1985.4   Max.   :738.9   Max.   :460.2   Max.   :2.430   Max.   :0.9114   Max.   :263261   Max.   :569.4   Max.   :0.8662   Max.   :0.9947  
                                                                                                                                                                      
   roundness       Compactness      ShapeFactor1       ShapeFactor2        ShapeFactor3     ShapeFactor4         Class     
 Min.   :0.4896   Min.   :0.6406   Min.   :0.002778   Min.   :0.0005642   Min.   :0.4103   Min.   :0.9477   BARBUNYA:1322  
 1st Qu.:0.8321   1st Qu.:0.7625   1st Qu.:0.005900   1st Qu.:0.0011535   1st Qu.:0.5814   1st Qu.:0.9937   BOMBAY  : 522  
 Median :0.8832   Median :0.8013   Median :0.006645   Median :0.0016935   Median :0.6420   Median :0.9964   CALI    :1630  
 Mean   :0.8733   Mean   :0.7999   Mean   :0.006564   Mean   :0.0017159   Mean   :0.6436   Mean   :0.9951   DERMASON:3546  
 3rd Qu.:0.9169   3rd Qu.:0.8343   3rd Qu.:0.007271   3rd Qu.:0.0021703   3rd Qu.:0.6960   3rd Qu.:0.9979   HOROZ   :1928  
 Max.   :0.9907   Max.   :0.9873   Max.   :0.010451   Max.   :0.0036650   Max.   :0.9748   Max.   :0.9997   SEKER   :2027  
                                                                                                            SIRA    :2636  
</code></pre>
<p>I set the Class variable to a factor, then found the proportion of levels:</p>
<pre><code>&gt; prop.table(table(DryBean$Class))

  BARBUNYA     BOMBAY       CALI   DERMASON      HOROZ      SEKER       SIRA 
0.09712732 0.03835133 0.11975608 0.26052458 0.14165014 0.14892366 0.19366689
</code></pre>
<p>I created a copy of the data then used a loop to remove samples in those proportions:</p>
<pre><code>while(nrow(beanclone) &gt; 100) {
  mysample &lt;- stratified(beanclone, &quot;Class&quot;, size = c(&quot;BARBUNYA&quot; = 0.09712732 , &quot;BOMBAY&quot; = 0.03835133 , &quot;CALI&quot; = 0.11975608 , &quot;DERMASON&quot; = 0.26052458 , &quot;HOROZ&quot; = 0.14165014 , &quot;SEKER&quot; = 0.14892366 , &quot;SIRA&quot; = 0.19366689 ), keep.rownames = TRUE)
  beanclone &lt;- beanclone[!seq_len(nrow(beanclone)) %in% mysample$rn,]
  print(table(beanclone$Class))
  print(nrow(beanclone))
}
</code></pre>
<p>However, the first few iterations of the loop had this output:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1271      459     1205     2859     1655     1830     2243 
[1] 11522

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1222      404      891     2305     1421     1652     1909 
[1] 9804

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1175      356      659     1859     1220     1492     1625 
[1] 8386
</code></pre>
<p>The Barbunya level is losing samples much more slowly than the rest, ultimately ending at this distribution:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
      80        4        1        2        3        5        3 
[1] 98
</code></pre>
<p>Testing with the size being set to different integers for each level showed that the loop was associating the correct size label with the level, i.e. setting &quot;Barbunya&quot; = 3 would result in barbunya losing 3 samples at each iteration, so why is it changing so slowly when going by proportion?</p>
<p>Edit:</p>
<p>Changing some of the proportion decimals has shown some unexpected results.
Barbunya 0.09712732 -&gt; 0.5 ended the loop at:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
     89        4        1        2        3        1        3 
[1] 103

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
     86        4        1        2        3        1        3 
[1] 100
</code></pre>
<p>Which is a negligible increase in the amount of Barbunya being removed at each iteration.</p>
",45,0,3,3,r;while-loop;sampling,2022-06-27 10:09:27,2022-06-27 10:09:27,2022-06-29 04:12:48,i m experimenting with the dry bean dataset from uci machine learning repo  i want to iterate through the dataset  repeatedly removing samples and running classifiers to see how accuracy changes as sample size decreases  but first i make the loop and check it works  the dataset  i set the class variable to a factor  then found the proportion of levels  i created a copy of the data then used a loop to remove samples in those proportions  however  the first few iterations of the loop had this output  the barbunya level is losing samples much more slowly than the rest  ultimately ending at this distribution  testing with the size being set to different integers for each level showed that the loop was associating the correct size label with the level  i e  setting  barbunya     would result in barbunya losing  samples at each iteration  so why is it changing so slowly when going by proportion  edit  which is a negligible increase in the amount of barbunya being removed at each iteration 
9,9,6480757,72793076,Feature Store tool that can serve live feature engineering use case,"<p>I am reviewing different feature store solutions to help me manage feature columns and feature functions. I have the following live feature engineering use case that I can't seem to find a solution for in any of the popular feature stores.</p>
<p><strong>Use case description</strong></p>
<p>I want to register a feature for my machine learning model to the feature store. That feature is generated by a python function. For example, I have a python function called <code>day_of_week</code>, and I can generate a feature column via <code>day_of_week(raw_data, day='Monday')</code>. This will return the <code>raw_data</code> dataframe with a one-hot encoded column named <code>Monday</code> attached to the original columns.</p>
<p>Now, when I do batch inference, I can just call my feature store client to get the feature columns for a model. Most feature store solutions provide an interface like <code>feature_store_client.get_feature(feature_name='Monday')</code>. This is simplified, usually you need to specify a feature view, maybe you need to provide an entity-identifying dataframe for the feature store to do the lookup, etc. This is fine, all feature store solutions do this.</p>
<p>When I do live feature engineering, however, I want to actually get the original python function that generated a specific feature column, not just the static, pre-computed features. In our case, we have an application where business users create scenarios live, so we need to re-engineer feature columns at run-time. So, we need the feature functions handy at inference-time to apply them to new data at run time, and I want the feature store to provide them to me through something like <code>feature_func = feature_store_client.get_feature_function(feature_name='Monday')</code>.</p>
<p><strong>Question</strong></p>
<p>Are you aware of any feature stores that provide such functionality?</p>
<p>I imagine that this type of use-case is widespread, and I'm surprised that I can't seem to find a solution that even attempts to provide such functionality. In fact, I thought the entire point of a feature store is that it helps you manage the python functions that generate the tables. Managing the tables themselves is great, but it doesn't help me with feature engineering, it only helps me with retrieval and cataloguing. What I want is a service that can help me name and retrieve functions, and keep track of what arrays/tensors/columns they generate.</p>
<p>PS, I've reviewed <code>Feast</code>, <code>Amazon SageMaker Feature Store</code> and <code>Databricks Feature Store</code>.</p>
",7,0,0,4,machine-learning;data-science;feature-engineering;feature-store,2022-06-29 02:28:50,2022-06-29 02:28:50,2022-06-29 02:28:50,i am reviewing different feature store solutions to help me manage feature columns and feature functions  i have the following live feature engineering use case that i can t seem to find a solution for in any of the popular feature stores  use case description i want to register a feature for my machine learning model to the feature store  that feature is generated by a python function  for example  i have a python function called day_of_week  and i can generate a feature column via day_of_week raw_data  day  monday    this will return the raw_data dataframe with a one hot encoded column named monday attached to the original columns  now  when i do batch inference  i can just call my feature store client to get the feature columns for a model  most feature store solutions provide an interface like feature_store_client get_feature feature_name  monday    this is simplified  usually you need to specify a feature view  maybe you need to provide an entity identifying dataframe for the feature store to do the lookup  etc  this is fine  all feature store solutions do this  when i do live feature engineering  however  i want to actually get the original python function that generated a specific feature column  not just the static  pre computed features  in our case  we have an application where business users create scenarios live  so we need to re engineer feature columns at run time  so  we need the feature functions handy at inference time to apply them to new data at run time  and i want the feature store to provide them to me through something like feature_func   feature_store_client get_feature_function feature_name  monday    question are you aware of any feature stores that provide such functionality  i imagine that this type of use case is widespread  and i m surprised that i can t seem to find a solution that even attempts to provide such functionality  in fact  i thought the entire point of a feature store is that it helps you manage the python functions that generate the tables  managing the tables themselves is great  but it doesn t help me with feature engineering  it only helps me with retrieval and cataloguing  what i want is a service that can help me name and retrieve functions  and keep track of what arrays tensors columns they generate  ps  i ve reviewed feast  amazon sagemaker feature store and databricks feature store 
10,10,19437952,72792101,Are there any models to extract specific data from pdf files?,"<p>For the purpose of my project, I am given large pdfs and need to manually extract one specific value (commission). I am looking for ay machine learning or AI model that would be able to automate this process. The structure of the pdfs vary, so ideally the model would be able to scan the pdf and return the commission percent for any type of pdf. For example the value can be provided in such ways:</p>
<ol>
<li><p>Commission Rate = 20%</p>
</li>
<li><p>The commission rate for this transaction is 20%.</p>
</li>
<li><p>Premium             Commission           Net</p>
<p>50000               20%                  40000</p>
</li>
</ol>
<p>Any feedback would be helpful.</p>
<p>Thank you!</p>
",15,1,-1,3,machine-learning;artificial-intelligence;textdecoder,2022-06-29 00:53:03,2022-06-29 00:53:03,2022-06-29 01:36:39,for the purpose of my project  i am given large pdfs and need to manually extract one specific value  commission   i am looking for ay machine learning or ai model that would be able to automate this process  the structure of the pdfs vary  so ideally the model would be able to scan the pdf and return the commission percent for any type of pdf  for example the value can be provided in such ways  commission rate     the commission rate for this transaction is    premium             commission           net                                    any feedback would be helpful  thank you 
11,11,19436129,72789256,Jupyter-notebook fails to start | PyForMLS,"<p>I am currently setting up a Python data-science client for SQL Server Machine Learning Services following this guide: <a href=""https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15</a></p>
<p>Unfortunately, running Jupyter notebooks for this distribution does not seem to work for me: Typing <code>.\Scripts\jupyter-notebook</code> in the distribution folder, or directly running jupyter-notebook.exe from the Scripts sub-folder does not start Jupyter. In the terminal, the command exits with no ouput.
Afterwards, https://localhost:8889/tree is not reachable as should be the case according to the tutorial above.</p>
<p>Any suggestions? (I already checked <a href=""https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html"" rel=""nofollow noreferrer"">https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html</a> for solutions). Thank you!</p>
",20,0,0,2,jupyter-notebook;azure-machine-learning-service,2022-06-28 20:56:50,2022-06-28 20:56:50,2022-06-28 20:56:50,i am currently setting up a python data science client for sql server machine learning services following this guide   any suggestions   i already checked  for solutions   thank you 
12,12,17230057,72789246,Combine numerical and categorical data within the same feature for machine learning,"<p>I want to predict concept labels given to specific records, but have trouble handling the variation these records have, in particularly the presence (or absence) of values for specific features because of mixed data.</p>
<p>I have a dataset in which we want to cluster data and label these clusters for easier retrieval and eventually further detailed processing. We therefore group by the parameter label provided which is a short description of the concept, but with a large variation. We therefore want to include summary statistics such as mean values and most frequent values to improve these as concept-specific words are often repeated in meta-records which are not important.</p>
<p>Unfortunately, the underlying data is sometimes numeric, sometimes categorical, resulting in mixed data for the aggregated features of most frequent values, and missing values for features such as the mean value. Imputing these missing values seems impossible as they should not have a mean to begin with. The most frequent value has mixed data with frequently large gaps between numerical values (e.g. parameters close to 0 - 10, but sometimes closer to 100 - 500, or even &gt; 20k) so binning seems impractical as well.</p>
<p>Example data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text_description</th>
<th>timestamp</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text example one describing data</td>
<td>0</td>
<td>80</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>2</td>
<td>70</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>3</td>
<td>2000</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>0</td>
<td>Home</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>1</td>
<td>Transit</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>2</td>
<td>Parents</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>3</td>
<td>Transit</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>0</td>
<td>1.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>1</td>
<td>0.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>2</td>
<td>1.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>3</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div>
<p>Example aggregated dataframe:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text_description</th>
<th>summary_mean</th>
<th>summary_most_frequent</th>
<th>target_concept_label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text example one describing data</td>
<td>1000</td>
<td>80</td>
<td>Label_One</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>NaN</td>
<td>Transit</td>
<td>Label_Two</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>0.8</td>
<td>1.0</td>
<td>Label_Three</td>
</tr>
</tbody>
</table>
</div>
<p>This lower table will be used as input to try to infer the target_concept_label. I've also thought of other approaches like separating the dataset and training three different models for each datatype, but that would mean losing a lot of training data for the text description parts. How would you approach this problem?</p>
<p>Kind regards,
Tadsz</p>
",7,0,-1,2,machine-learning;data-processing,2022-06-28 20:56:14,2022-06-28 20:56:14,2022-06-28 20:56:14,i want to predict concept labels given to specific records  but have trouble handling the variation these records have  in particularly the presence  or absence  of values for specific features because of mixed data  i have a dataset in which we want to cluster data and label these clusters for easier retrieval and eventually further detailed processing  we therefore group by the parameter label provided which is a short description of the concept  but with a large variation  we therefore want to include summary statistics such as mean values and most frequent values to improve these as concept specific words are often repeated in meta records which are not important  unfortunately  the underlying data is sometimes numeric  sometimes categorical  resulting in mixed data for the aggregated features of most frequent values  and missing values for features such as the mean value  imputing these missing values seems impossible as they should not have a mean to begin with  the most frequent value has mixed data with frequently large gaps between numerical values  e g  parameters close to      but sometimes closer to      or even  gt  k  so binning seems impractical as well  example data  example aggregated dataframe  this lower table will be used as input to try to infer the target_concept_label  i ve also thought of other approaches like separating the dataset and training three different models for each datatype  but that would mean losing a lot of training data for the text description parts  how would you approach this problem 
13,13,19435795,72789139,Is there a model to predict tensor from an image,"<p>I’m new to tensorflow and machine learning. I would like to predict a matrix that contains 101 points from an RGB image (9x101) created from sensor data. So, from this kind of images <a href=""https://i.stack.imgur.com/NoduF.jpg"" rel=""nofollow noreferrer"">enter image description here</a>, I would like to predict those data <a href=""https://i.stack.imgur.com/69jbg.png"" rel=""nofollow noreferrer"">enter image description here</a>.</p>
<p>Any help, guidance or links to associate the image with the output matrix in a dataset and which output layer I should use.</p>
<p>Best regards,</p>
<p>Simon</p>
",13,0,-2,3,python;tensorflow;keras,2022-06-28 20:49:16,2022-06-28 20:49:16,2022-06-28 20:49:16,i m new to tensorflow and machine learning  i would like to predict a matrix that contains  points from an rgb image  x  created from sensor data  so  from this kind of images   i would like to predict those data   any help  guidance or links to associate the image with the output matrix in a dataset and which output layer i should use  best regards  simon
14,14,19436200,72788900,Using TIMM for binary classification,"<p>I am new to machine learning and I have an application where I need to classify images of human vs animals. I have gathered a dataset of 150k images for both classes.</p>
<p>I would like to use the TIMM library to do a binary classification. I want to start with the ResNet-50 model and train it from scratch.</p>
<p>I am unsure of how I should use features of TIMM for my purpose. Is changing the num_classes to 2 enough or do I need to add a separate classifier as the last layer? How can I use the existing training script to change the last activation function to Sigmoid for example? What other parameters e.g. learning rate, weight decay etc. should I change?</p>
",15,0,-1,4,python;deep-learning;pytorch;classification,2022-06-28 20:34:50,2022-06-28 20:34:50,2022-06-28 20:34:50,i am new to machine learning and i have an application where i need to classify images of human vs animals  i have gathered a dataset of k images for both classes  i would like to use the timm library to do a binary classification  i want to start with the resnet  model and train it from scratch  i am unsure of how i should use features of timm for my purpose  is changing the num_classes to  enough or do i need to add a separate classifier as the last layer  how can i use the existing training script to change the last activation function to sigmoid for example  what other parameters e g  learning rate  weight decay etc  should i change 
15,15,18201044,72788138,Machine Learning - Train a model using imbalanced data,"<p>I have two classes in my data.</p>
<p>This is how class distribution looks like.</p>
<pre><code>0.0    169072
1.0     84944
</code></pre>
<p>In other words, I have 2:1 class distribution.</p>
<p>I believe I have two choices. Downsample the class <code>0.0</code> or upsample class <code>1.0</code>. If I go with option 1, I'm losing data. If i go with option 2, then I'm using non-real data.</p>
<p>Is there a way, I can train the model without upsample or downsample?</p>
<p>This is how my classification_report looks like.</p>
<pre><code>               precision    recall  f1-score   support

         0.0       0.68      1.00      0.81     51683
         1.0       1.00      0.00      0.00     24522

    accuracy                           0.68     76205
   macro avg       0.84      0.50      0.40     76205
weighted avg       0.78      0.68      0.55     76205  
</code></pre>
",15,2,0,4,scikit-learn;logistic-regression;xgboost;lightgbm,2022-06-28 19:48:30,2022-06-28 19:48:30,2022-06-28 20:09:49,i have two classes in my data  this is how class distribution looks like  in other words  i have   class distribution  i believe i have two choices  downsample the class   or upsample class    if i go with option   i m losing data  if i go with option   then i m using non real data  is there a way  i can train the model without upsample or downsample  this is how my classification_report looks like 
16,16,10029745,72788374,What is a ML model suitable for a nutricionist AI?,"<p>Say a Machine Learning engineer wants to make an AI that predicts the best diet for a person. It takes into account the goal of the diet, profile of the person, calorie intake and possible meals from a database of diets.</p>
<p>Is there an existing model that serves this purpose? Would you recomend using AI to solve this problem as opposed to another algorithm? If no, then why?</p>
<p>The context is I am new to AI and am trying to understand its use cases.</p>
",13,0,-1,3,machine-learning;artificial-intelligence;diet,2022-06-28 20:01:59,2022-06-28 20:01:59,2022-06-28 20:01:59,say a machine learning engineer wants to make an ai that predicts the best diet for a person  it takes into account the goal of the diet  profile of the person  calorie intake and possible meals from a database of diets  is there an existing model that serves this purpose  would you recomend using ai to solve this problem as opposed to another algorithm  if no  then why  the context is i am new to ai and am trying to understand its use cases 
17,17,10456623,72777047,How to use data without leakage,"<p>I am developing a deep learning model, but the data is sensitive and can only be stored on a remote server. However, coding and debugging on that server is inconvenient.  So, is it possible to use the data in my local machine without data leakage? (make sure I can't download the real data from my side otherwise the person who manages the data won't allow me to do it.)</p>
",15,0,-1,3,deep-learning;privacy;data-security,2022-06-28 00:20:10,2022-06-28 00:20:10,2022-06-28 19:56:09,i am developing a deep learning model  but the data is sensitive and can only be stored on a remote server  however  coding and debugging on that server is inconvenient   so  is it possible to use the data in my local machine without data leakage   make sure i can t download the real data from my side otherwise the person who manages the data won t allow me to do it  
18,18,9222360,72786131,Scrape large scale data from Wikipedia,"<p>I am training a large machine learning model and need to scrape a lot of data for the same. I want to train my model on domain specific tasks and hence, given a domain, I will need to scrape Wikipedia pages and pages from the web.</p>
<p>For example, if the domain name is &quot;finance&quot;, I will need to crawl Wikipedia pages that are related to finance and then store the text present in them. Wikipedia Rest API has a limit of 200 requests per second. I will probably need more data than that.</p>
<p>There are wikipedia data dumps but the size of the dumps are too large.</p>
<p>Is there any other way of doing this or working with the API?</p>
<p>On a similar note, if I want to search for pages on the web related to some query, how will I do it?</p>
",16,0,-1,3,web-scraping;web-crawler;mediawiki,2022-06-28 17:36:52,2022-06-28 17:36:52,2022-06-28 18:30:17,i am training a large machine learning model and need to scrape a lot of data for the same  i want to train my model on domain specific tasks and hence  given a domain  i will need to scrape wikipedia pages and pages from the web  for example  if the domain name is  finance   i will need to crawl wikipedia pages that are related to finance and then store the text present in them  wikipedia rest api has a limit of  requests per second  i will probably need more data than that  there are wikipedia data dumps but the size of the dumps are too large  is there any other way of doing this or working with the api  on a similar note  if i want to search for pages on the web related to some query  how will i do it 
19,19,19160479,72700574,Compute the loss of a moving dataset,"<p>I’m new to the world of machine learning, so it could be that my question is trivial or incorrectly posed.</p>
<p>I am using a moving dataset that I have forwarded to an STN network (Spatial Tranformation Network). To the STN I forward each image individually, then restack the whole images together in a tuple.
My problem lies in the loss calculation. My target has a torch.tensor with this size [2,1,64,64]
and my prediction that I want to implement has a torch.tensor [2,1,10,64,64], which means that the prediction and the target are not the same.
Could someone explain an idea to me. The only idea I have is to return the last STN output meaning something like this [2,1,1,64,64] to my prediction and then squeezed to be [2,1,64,64] and then calculate the loss.</p>
<p>Thank you in advance</p>
",36,0,0,5,python;machine-learning;pytorch;mnist;spatial-transformer-network,2022-06-21 17:42:31,2022-06-21 17:42:31,2022-06-28 18:19:39,i m new to the world of machine learning  so it could be that my question is trivial or incorrectly posed  thank you in advance
20,20,11719827,72786082,Mixing video and data streams on raspberry pi for machine learning,"<p>What is best way to mix data and video for machine learning on single board computer like raspberry pi?</p>
<p>It should be very common problem. I found there is GPMF and KLV extensions for video formats. But I fail to find someone to implment that for raspberry pi, opencv, etc.</p>
<p>It seems I can solve the problem by mixing 2 data streams in single file and then read that file with my own code but I wonder what is canonical solution for that kind of problem.</p>
",22,0,2,3,machine-learning;raspberry-pi;klvdata,2022-06-28 17:33:13,2022-06-28 17:33:13,2022-06-28 17:33:13,what is best way to mix data and video for machine learning on single board computer like raspberry pi  it should be very common problem  i found there is gpmf and klv extensions for video formats  but i fail to find someone to implment that for raspberry pi  opencv  etc  it seems i can solve the problem by mixing  data streams in single file and then read that file with my own code but i wonder what is canonical solution for that kind of problem 
21,21,18390926,72782753,"How can I resize a image to shape=(None, 321, 321, 3) with name name=None and dtype=tf.float32","<p><strong>Problem</strong>: I am trying to reshape a image to size (None,321,321,3) and also set the name of image to None. I want to match the image dimension requirements to train a machine learning model of specs,</p>
<pre><code>TensorSpec(shape=(None, 321, 321, 3), dtype=tf.float32, name=None)
</code></pre>
<p><strong>What I have done:</strong> I am using <code>PIL</code> to reshape the image. I can convert the image to the required size and RGB band, but have no idea how to make the first argument in the size as <code>None</code> and also how to set the name of image to <code>None</code>. Please help me find a solution to this problem.</p>
",37,1,2,5,python;tensorflow;image-processing;python-imaging-library;image-resizing,2022-06-28 13:31:46,2022-06-28 13:31:46,2022-06-28 14:50:59,problem  i am trying to reshape a image to size  none     and also set the name of image to none  i want to match the image dimension requirements to train a machine learning model of specs  what i have done  i am using pil to reshape the image  i can convert the image to the required size and rgb band  but have no idea how to make the first argument in the size as none and also how to set the name of image to none  please help me find a solution to this problem 
22,22,15494110,72783059,creating a document classifier via Quantum Machine learning and the classification isnt correct,"<p>I am very new to quantum machine learning
this is the code,
i am getting an error at qml.templates.lself.layer = qml.QNode(_circuit, self.dev, interface=&quot;tf&quot;)ayers.StronglyEntanglingLayers(parameters, wires=list(range(self.n_qubits)))
as syntax error</p>
<pre><code>import tensorflow_hub as hub
import tensorflow as tf
import tensorflow_quantum as qml

import cirq
import sympy
import numpy as np

embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)
embeddings = embed([
    &quot;The quick brown fox jumps over the lazy dog.&quot;,
    &quot;I am a sentence for which I would like to get its embedding&quot;])

def call(self, inputs):
   x = tf.matmul(inputs, self.W1) + self.b1
   x = tf.math.tanh(x)
   return x

def _circuit(inputs, parameters):
   qml.templates.embeddings.AngleEmbedding(inputs, wires=list(range(self.n_qubits)))
   qml.templates.lself.layer = qml.QNode(_circuit, self.dev, interface=&quot;tf&quot;)ayers.StronglyEntanglingLayers(parameters, wires=list(range(self.n_qubits)))
   return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]

def make_model_quantum(n_categories, n_qubits=4, n_layers=2, embedding_dim=512):

    text_in = keras.Input( shape=(embedding_dim,), dtype=tf.float64, name='text_in')

    x = layers.Dense(n_qubits, activation='tanh', dtype=tf.float64)(text_in)

    x = VariationalQuantumCircuit(
            n_categories=n_categories, 
            n_qubits=n_qubits, 
            n_layers=n_layers)(x)
    
    assert x.shape[-1] == n_qubits

    x_out = layers.Dense(n_categories, activation='softmax', dtype=tf.float64)(x)

    return keras.Model(inputs=text_in, outputs=x_out, name=&quot;QuantumPreprintClassifier&quot;)

make_model_quantum(embeddings)
</code></pre>
",16,0,-1,5,tensorflow;machine-learning;classification;text-classification;quantum-computing,2022-06-28 13:53:00,2022-06-28 13:53:00,2022-06-28 13:54:27,
23,23,9690492,62669501,Accessing Saved Pickle Model in github,"<p>So I saved my trained machine learning model as pickle in github, but I can't find a way  to access it yet? Anyone have a suggestion?</p>
",282,1,0,5,python;machine-learning;github;pickle;github-pages,2020-07-01 09:33:14,2020-07-01 09:33:14,2022-06-28 13:42:32,so i saved my trained machine learning model as pickle in github  but i can t find a way  to access it yet  anyone have a suggestion 
24,24,11088453,72781392,Unable to get functionality of python application build in flask on locla host,"<p>I am trying to build a machine learning model and implement it in flask.</p>
<p>However, I am getting an error:</p>
<p><strong>Internal Server Error The Server Encountered an internal error aand was unable to complete your request.</strong></p>
<p><a href=""https://i.stack.imgur.com/Gb5uF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gb5uF.png"" alt=""The error message"" /></a></p>
<p>Can someone tell me what exactly is the issue.</p>
<p>Everything was running fine.</p>
<p>When I changed the data then the application opened but when I run it, it crashed like this.</p>
<p>Regards</p>
",19,0,-2,5,python;flask;machine-learning;web;server,2022-06-28 11:29:14,2022-06-28 11:29:14,2022-06-28 11:29:14,i am trying to build a machine learning model and implement it in flask  however  i am getting an error  internal server error the server encountered an internal error aand was unable to complete your request   can someone tell me what exactly is the issue  everything was running fine  when i changed the data then the application opened but when i run it  it crashed like this  regards
25,25,10765849,72780436,Deploying the Machine Learning Model in React Native,"<p>I am working on Heart Disease Classification using Scikit Learn. I have generated the model in .h5 extension. Next, I want to deploy the model in react native framework. I am not sure how to go about it.</p>
",18,0,-1,3,react-native;machine-learning;scikit-learn,2022-06-28 08:58:14,2022-06-28 08:58:14,2022-06-28 08:58:14,i am working on heart disease classification using scikit learn  i have generated the model in  h extension  next  i want to deploy the model in react native framework  i am not sure how to go about it 
26,26,19430752,72779695,"flutter camera image convert to ios UIimage, please help me","<p>I want to pass the images from the camera image stream [startImageStream] to the iOS backend (Swift) as FlutterStandardTypedData .</p>
<p>I'd like to take the camera image and be able to create an UIImage out of it, without the necessity of converting it on the flutter side. It takes too much time and is not suitable for machine learning &amp; real-time detection.</p>
<p>anyone can help me?</p>
",17,0,-3,1,ios,2022-06-28 06:34:14,2022-06-28 06:34:14,2022-06-28 06:34:14,i want to pass the images from the camera image stream  startimagestream  to the ios backend  swift  as flutterstandardtypeddata   i d like to take the camera image and be able to create an uiimage out of it  without the necessity of converting it on the flutter side  it takes too much time and is not suitable for machine learning  amp  real time detection  anyone can help me 
27,27,3274103,21966033,Error While trying to connect to DB2 SAMPLE database for the First TIme,"<p>I want to install DB2 UDW in my machine for learning purpose but I am having a hard time configuring the local instance. Any help would be highly appreciated. </p>

<p>I installed DB2 express edition -c . I have selected all the default choices. I am trying to connect using IBM data Studio 4.1,  In the ""DB2 first Steps""  GUI I have chosen to create SAMPLE Database. I am getting the below error</p>

<p><strong>Creating database ""SAMPLE"" on path ""C:""...
  Existing ""SAMPLE"" database found...
    The ""-force"" option was not specified...
  Attempt to create the database ""SAMPLE"" failed
  'db2sampl' processing complete.</strong></p>

<p>I tried connecting from Data Studio using the following options </p>

<p>Database- SAMPLE
Port- 50000
host - localhost</p>

<p>Error I am getting </p>

<p><strong>Explanation</strong>:
An attempt was made to access a database that was not found, has not been started, or does not support transactions.
<strong>User response:</strong>
Ensure that the specified database name exists in the system database directory. If the database name does not exist in the system database directory, either the database does not exist or the database name has not been cataloged. If needed, issue a db2start command and then resubmit the current command.
SQL4499N A fatal error occurred that resulted in a disconnect from the data source.
SQLSTATE: 08004</p>

<p>Problem is I am having zero knowledge in DB2.   If I need to run db2start command from where I should run this?  Please help</p>
",9423,1,0,2,database;db2,2014-02-23 14:10:36,2014-02-23 14:10:36,2022-06-28 05:33:40,i want to install db udw in my machine for learning purpose but i am having a hard time configuring the local instance  any help would be highly appreciated   i installed db express edition  c   i have selected all the default choices  i am trying to connect using ibm data studio     in the db first steps  gui i have chosen to create sample database  i am getting the below error i tried connecting from data studio using the following options  error i am getting  problem is i am having zero knowledge in db    if i need to run dbstart command from where i should run this   please help
28,28,18726236,72778858,"neural network not optimizing weights of first layer, returning all 1&#39;s for z1","<p>So im building a neural network in python right now losely following Andrew Ng's machine learning course. It has 3 layers (all sigmoid) and works on predicting the MNIST dataset. But it fails to actually predict the dataset, and while the cost is decreasing with every iteration accuracy remains at around 0.1, indicating something isnt working properly.
After playing around and letting the program display the different steps i noticed that z1 (calculated from the train_X@theta1 in the activation layer) is pretty much uniformly 1's, which seems to be hindering the network to function. This is due to the train_X@theta1 being high values, making e^(-z) basically 0 and the sigmoid function returning basically 1. But that also leads to the derivative for theta1 (due to being multiplied by (1-z1) to just be 0's, hindering the network from doing anything.
Things i have tried are:</p>
<p>Regularizing the dataset</p>
<p>Playing around with alpha values (which doesnt do anything, as expected)</p>
<p>Making theta1 values start out really small (all calculated through a uniform distribution, which i then multiplied by like 0.000001): this, while fixing the problem for the first iteration just makes the network optimize theta1 to where it leads to uniformly 1's at z1 again, so i suspect backwards/forward propagation to not be working properly? Even though I basically copied that from one of my working solutions to an exercise in the course in octave, which produced resonable results and ended up with a high accuracy.</p>
<p>Here is my algorithm:</p>
<pre><code>def nn_forwardPropagation(train_X, train_Y, theta1, theta2, theta3):
    accuracy = 0
    J = 0
    #forward propagation, always adding 1's for the bias unit
    train_X = np.c_[ np.ones((np.shape(train_X)[0],1)), train_X]
    z1 = sigmoid(train_X@np.transpose(theta1))
    a1 = np.c_[ np.ones((np.shape(z1)[0],1)), z1]
    z2 = sigmoid(a1@np.transpose(theta2))
    a2 = np.c_[ np.ones((np.shape(z2)[0],1)), z2]
    #last step
    predValues = np.zeros((10,len(train_Y)))
    y1 = predValues.copy()
    predValues2 = y1.copy()
    for j in range(len(train_Y)):
        for i in range(np.shape(theta3)[0]):
            predValues[i,j] = sigmoid(a2[j,:]@np.transpose(theta3[i,:]))
        #making y1 our &quot;target matrix&quot; where we would like to see a 1 at the place of the right number and 0's everywhere else 
        y1[train_Y[j],j] = 1
        J += np.sum((np.transpose(-y1[:,j])@np.log(predValues[:,j]))  -  (1-np.transpose(y1[:,j]))@np.log(1 - predValues[:,j]))/len(train_Y)
        #in order to calculate accuracy, we just assume that the highest value in our predicted values dictates which value is &quot;right&quot; and looks if that's right
        predValues2[np.where(predValues[:,j] == max(predValues[:,j])),j] = 1 
        if np.array_equiv(predValues2[:,j], y1[:,j]):
            accuracy += 1/len(train_Y)
    #calculating the derivatives
    delta3 = predValues - y1  
    delta2 = np.transpose(delta3) @ theta3[:,1:] * z2 * (1-z2)
    delta1 = delta2 @ theta2[:,1:] * z1 * (1-z1)
    D3 = delta3 @ a2 
    D2 = np.transpose(delta2) @ a1
    D1 = np.transpose(delta1) @ train_X
    theta3_grad = D3 / len(train_Y)
    theta2_grad = D2 / len(train_Y)
    theta1_grad = D1 / len(train_Y)
    return J, theta3_grad, theta2_grad, theta1_grad, accuracy
    
def nn_runner(train_X, train_Y, iterations, alpha1, alpha2, alpha3):
    theta1 = np.random.rand(4,785)
    theta2 = np.random.rand(7,5)
    theta3 = np.random.rand(10,8)
    for i in range(1,iterations+1):
        J, theta3_grad, theta2_grad, theta1_grad, accuracy = nn_forwardPropagation(train_X, train_Y, theta1, theta2, theta3)
        theta3 -= alpha3*theta3_grad
        theta2 -= alpha2*theta2_grad
        theta1 -= alpha1*theta1_grad
        if i % 1 == 0:
            print(&quot;At iteration&quot;,i,&quot;:&quot;, J)
    print(&quot;This amounts to a total accuracy on training data of&quot;, accuracy)
    return theta3, theta2, theta1
</code></pre>
<p>Does anybody here know what i did wrong/could look into to make this working?
Thanks</p>
",22,0,0,4,python;machine-learning;neural-network;mnist,2022-06-28 03:52:00,2022-06-28 03:52:00,2022-06-28 03:52:00,regularizing the dataset playing around with alpha values  which doesnt do anything  as expected  making theta values start out really small  all calculated through a uniform distribution  which i then multiplied by like     this  while fixing the problem for the first iteration just makes the network optimize theta to where it leads to uniformly  s at z again  so i suspect backwards forward propagation to not be working properly  even though i basically copied that from one of my working solutions to an exercise in the course in octave  which produced resonable results and ended up with a high accuracy  here is my algorithm 
29,29,19428875,72776427,"How to automate Python script run by using GCP apps (secluder, scheduled query in Big Query, or any other ideas)","<p>I am trying to create a batch ETL using a local python script and GCP.</p>
<p>I have built a python script that generates a random json file containing some fake data fields as a &quot;fake data generator&quot;...I am working on a project that will eventually use a more complex algorithm to generate a json, but for designing the framework of my ETL... I am just using a placeholder in the meantime.</p>
<p>The goal is to set up a system in GCP that would allow the scheduled running of this script (on my local repo in visual studio), and then the retrieving of the json file it generates. Then I need to append this file to a data table in BQ.</p>
<p>What would be the best way to do this operation? It is important to note that the request to run the python script on a schedule needs to come from GCP, or else I would just use a cronjobs system on my local machine.</p>
<p>I have tried learning how to create a scheduled query in Google Big Query, although I'm not very familiar with how this could work, and am still learning how to use Big Query and GCP.</p>
<p>All advice is welcome, thanks!!!</p>
",34,0,1,5,python;google-cloud-platform;google-bigquery;etl;data-pipeline,2022-06-27 23:22:43,2022-06-27 23:22:43,2022-06-27 23:22:43,i am trying to create a batch etl using a local python script and gcp  i have built a python script that generates a random json file containing some fake data fields as a  fake data generator    i am working on a project that will eventually use a more complex algorithm to generate a json  but for designing the framework of my etl    i am just using a placeholder in the meantime  the goal is to set up a system in gcp that would allow the scheduled running of this script  on my local repo in visual studio   and then the retrieving of the json file it generates  then i need to append this file to a data table in bq  what would be the best way to do this operation  it is important to note that the request to run the python script on a schedule needs to come from gcp  or else i would just use a cronjobs system on my local machine  i have tried learning how to create a scheduled query in google big query  although i m not very familiar with how this could work  and am still learning how to use big query and gcp  all advice is welcome  thanks   
30,30,18190949,72774090,Applying PCA to 2 dimentional feature arrays,"<p>I am working with a database where for each element (individual sample) of the database I have a 150x160 matrix of features. The columns of the matrix represent spatial dimantion and the rows represent frequency. So for each element, I have data of the intensity at different points of the sample and for different frequencies. I want to apply a PCA to reduce the dimentionality of the dataset and, additionally, maximize the variability of the dataset before applying machine learning algorithms. But I don't know how to apply PCA in this case, as I normally use it for 1 dimentional data arrays. All help is welcome, thanks!</p>
",16,1,0,3,python;r;pca,2022-06-27 20:19:27,2022-06-27 20:19:27,2022-06-27 20:29:23,i am working with a database where for each element  individual sample  of the database i have a x matrix of features  the columns of the matrix represent spatial dimantion and the rows represent frequency  so for each element  i have data of the intensity at different points of the sample and for different frequencies  i want to apply a pca to reduce the dimentionality of the dataset and  additionally  maximize the variability of the dataset before applying machine learning algorithms  but i don t know how to apply pca in this case  as i normally use it for  dimentional data arrays  all help is welcome  thanks 
31,31,19427321,72773807,How to handle dataset which is a csv file that contains image names in one column and image path in other column?,"<p>I am new to python and machine learning. I am just practicing with model training and dataset thingy. I came across this dataset that have test and train folder. In that folder there are several containing different images (It's a music instrument dataset so each music instrument is categorized by names in different folders). And the csv file has this name of the instrument and their path in the folder like this: <a href=""https://i.stack.imgur.com/Wykkf.png"" rel=""nofollow noreferrer"">Instrument.csv</a></p>
<p>Now my question is how do I handle this dataset? Should I iterate through train and test folders or use this csv file?
And if I want to choose the folder option then how can go through each sub-folder and access the images?
Here is the link for the dataset : <a href=""https://www.kaggle.com/datasets/gpiosenka/musical-instruments-image-classification"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/gpiosenka/musical-instruments-image-classification</a></p>
<p>sorry if the question doesn't make any sense or too easy to do it. I agree I am noob</p>
",18,1,0,4,python;csv;machine-learning;dataset,2022-06-27 20:00:58,2022-06-27 20:00:58,2022-06-27 20:08:14,i am new to python and machine learning  i am just practicing with model training and dataset thingy  i came across this dataset that have test and train folder  in that folder there are several containing different images  it s a music instrument dataset so each music instrument is categorized by names in different folders   and the csv file has this name of the instrument and their path in the folder like this   sorry if the question doesn t make any sense or too easy to do it  i agree i am noob
32,32,6333517,72773806,Where to store scripts in a Python project?,"<p>I'm working on a machine learning python project. As a part of this project I have two components: (1) the library itself, with the model definitions, data processing functions, etc., and (2) a set of scripts that execute and train models, generate data, etc. These scripts are using the functionalities implemented in the library. The library component is where functionalities are defined and the scripts component is where functionalities are executed.</p>
<p>My question is how to store this kind of script in my repository. I have two options:</p>
<ol>
<li><strong>Use python entry points</strong>: The first option is to have all these scripts as part of the library and install them using python entry points.</li>
</ol>
<pre><code>project
│
└───library
│   │   __init__.py
│   │
│   └───subfolder1
│   │   │   __init__.py
│   │   │   file1.py
│   │   │   file2.py
│   │   │   ...
│   └───scripts
│       │   __init__.py
│       │   script1.py
│       │   script2.py
│       │   ...
│
│ setup.py
│ setup.cfg
</code></pre>
<p>where <code>setup.cfg</code> installs the scripts like</p>
<pre><code>[options.entry_points]
console_scripts =
    execute-script1 = library.scripts.script1:main
    execute-script2 = library.scripts.script2:main
</code></pre>
<p>And the scripts, after installing the package, are executed from the command line like <code>execute-script1 --arg1 ...</code>.</p>
<ol start=""2"">
<li><strong>Script folder</strong>: the second option is to have a folder of scripts outside the library with all the scripts defined there. This is, something like</li>
</ol>
<pre><code>project
│
└───library
│   │   __init__.py
│   │
│   └───subfolder1
│       │   file1.py
│       │   file2.py
│       │   ...
│   
│ setup.py
│
└───scripts
    │   script1.py
    │   script2.
</code></pre>
<p>where <code>setup.py</code> installs only the modules in <code>library</code>. And the scripts look like</p>
<pre class=""lang-py prettyprint-override""><code>def parse_args():
    ...

def main():
    args = parse_args()
    ...

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>And the scripts are executed from the command line like <code>python scripts/scrip1.py --arg1 ...</code></p>
<p>Which is the recommended way to store these scripts? I've been looking at several python projects but I couldn't find a standard way.</p>
",14,0,0,4,python;package;repository;structure,2022-06-27 20:00:58,2022-06-27 20:00:58,2022-06-27 20:00:58,i m working on a machine learning python project  as a part of this project i have two components     the library itself  with the model definitions  data processing functions  etc   and    a set of scripts that execute and train models  generate data  etc  these scripts are using the functionalities implemented in the library  the library component is where functionalities are defined and the scripts component is where functionalities are executed  my question is how to store this kind of script in my repository  i have two options  where setup cfg installs the scripts like and the scripts  after installing the package  are executed from the command line like execute script   arg      where setup py installs only the modules in library  and the scripts look like and the scripts are executed from the command line like python scripts scrip py   arg     which is the recommended way to store these scripts  i ve been looking at several python projects but i couldn t find a standard way 
33,33,17668281,72711822,Detectron2 - Same Code / Different platforms / highly divergent results,"<p>I use different hardware to benchmark multiple possibilites. The Code runs in a jupyter Notebook.</p>
<p>When i evaluate the different losses i get highly divergent results.</p>
<p>I also checked the full .cfg with <code>cfg.dump()</code> - it is completely consistent.</p>
<p>I appreciate any help. Thanks in advance.</p>
<p><strong>1.Environment</strong></p>
<pre><code>Microsoft Azure - Machine Learning

STANDARD_NC6

Torch: 1.9.0+cu111
</code></pre>
<p>Results:</p>
<p><a href=""https://i.stack.imgur.com/oKXID.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oKXID.png"" alt=""Results Azure"" /></a></p>
<p><strong>2.Environment</strong></p>
<pre><code>GoogleColab free

Torch: 1.9.0+cu111 
</code></pre>
<p>Results:</p>
<p><a href=""https://i.stack.imgur.com/uYQ6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uYQ6y.png"" alt=""Results GoogleColab"" /></a></p>
<p><strong>Detectron2 Parameters:</strong></p>
<pre><code>cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-Detection/retinanet_R_101_FPN_3x.yaml&quot;))
cfg.DATASETS.TRAIN = (&quot;dataset_train&quot;,)
cfg.DATASETS.TEST = (&quot;dataset_test&quot;,)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-Detection/retinanet_R_101_FPN_3x.yaml&quot;)  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025  # 0.00125 pick a good LR
cfg.SOLVER.MAX_ITER = 1200    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)
#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 25  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
cfg.MODEL.RETINANET.NUM_CLASSES = 3
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.
cfg.OUTPUT_DIR = &quot;/content/drive/MyDrive/Colab_Notebooks/testrun/output&quot;
cfg.TEST.EVAL_PERIOD = 25
</code></pre>
",34,0,0,5,azure;computer-vision;conv-neural-network;object-detection;detectron,2022-06-22 13:39:54,2022-06-22 13:39:54,2022-06-27 19:38:31,i use different hardware to benchmark multiple possibilites  the code runs in a jupyter notebook  when i evaluate the different losses i get highly divergent results  i also checked the full  cfg with cfg dump     it is completely consistent  i appreciate any help  thanks in advance   environment results    environment results   detectron parameters 
34,34,14686347,72773295,Encountered error while trying to install package. kaggle,"<p>I am new to machine learning and am currently trying to build a python function that can download a specified datasets from Kaggle given the URL. However, I keep on encountering the below problem when ever I run the <code>pip install kaggle</code> or <code>pip install opendatasets</code> commands in my windows command line.</p>
<blockquote>
<p>× Encountered error while trying to install package.
╰─&gt; kaggle</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/rXNvt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rXNvt.png"" alt=""Error encountered"" /></a></p>
<p>I am using Jupyter Notebook for machine learning practice running on windows 10, python version 3.10.1.</p>
<p>Any help will be truly appreciated</p>
",15,0,0,3,python;jupyter-notebook;kaggle,2022-06-27 19:24:48,2022-06-27 19:24:48,2022-06-27 19:24:48,i am new to machine learning and am currently trying to build a python function that can download a specified datasets from kaggle given the url  however  i keep on encountering the below problem when ever i run the pip install kaggle or pip install opendatasets commands in my windows command line   i am using jupyter notebook for machine learning practice running on windows   python version     any help will be truly appreciated
35,35,5961731,42484746,Two-Class-Logistic VS Binary Logistic Regression,"<p>During a test project on Azure Machine Learning Studio I have some questions based on my understandings.
In my project (in R) I have used Binary Logistic Regression, but in AML I found two Logistic regression Two-Class and MultiClass. So in that case I have used two-class Logistic regression. Am I Right in this case?</p>
<p>In another case during running glm() in R tool it perform Logistic regression and after summary(loreg Eqn) it provides the each variable's co-efficient &amp; estimates.</p>
<p>From R I have the following output:</p>
<p><a href=""https://i.stack.imgur.com/vjcU2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjcU2.png"" alt=""enter image description here"" /></a></p>
<p>From AML after right-clicking Train Model and visualize:</p>
<p><a href=""https://i.stack.imgur.com/YRgEA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRgEA.png"" alt=""enter image description here"" /></a></p>
<p>The weight in the above Picture is the estimates, am I right (Dataset is diff)?</p>
",643,1,0,3,azure;machine-learning;azure-machine-learning-studio,2017-02-27 17:03:23,2017-02-27 17:03:23,2022-06-27 18:58:22,in another case during running glm   in r tool it perform logistic regression and after summary loreg eqn  it provides the each variable s co efficient  amp  estimates  from r i have the following output   from aml after right clicking train model and visualize   the weight in the above picture is the estimates  am i right  dataset is diff  
36,36,17528852,72772758,Want to know about API for multifunctional purpose,"<p>If many online users suppose 10000 users are accessing an API at the same time will the API will become slow ? Because I have a machine learning API and if I used it for android game then the condition will create is that per user will send millions of request as live object detection will be going on by the help of that API! So if millions of users uses that android app the number of request to that machine learning API will be tremendous! So will my API will crash at that time ?
Or it will work smoothly without being slow!</p>
",17,0,-3,1,api,2022-06-27 18:45:40,2022-06-27 18:45:40,2022-06-27 18:45:40,
37,37,18490724,72493838,TensorFlow Error: dictionary update sequence element #0 has length 6; 2 is required,"<p>I am new to Python and machine learning.  I am getting an error whenever I run the following code:</p>
<pre><code>def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():  
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  
    if shuffle:
      ds = ds.shuffle(1000)  
    ds = ds.batch(batch_size).repeat(num_epochs) 
    return ds  
  return input_function  


train_input_fn = make_input_fn(X_train, y_train)  
eval_input_fn = make_input_fn(X_test, y_test, num_epochs=1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

linear_est.train(train_input_fn) 
result = linear_est.evaluate(eval_input_fn) 

clear_output() 
print(result['accuracy']) 
</code></pre>
<p>Error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-47fb35491976&gt; in &lt;module&gt;()
----&gt; 1 linear_est.train(train_input_fn)
      2 result = linear_est.evaluate(eval_input_fn)
      3 
      4 clear_output()
      5 print(result['accuracy'])

5 frames
&lt;ipython-input-36-16cfc32eb7b2&gt; in input_function()
      1 def make_input_fn(X_train, y_train, num_epochs=10, shuffle=True, batch_size=32):
      2   def input_function():
----&gt; 3     ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))
      4     if shuffle:
      5       ds = ds.shuffle(1000)

ValueError: dictionary update sequence element #0 has length 6; 2 is required
</code></pre>
<p>I am not sure if the issue would be related to my data or data types. My data has no blanks.</p>
",35,1,0,3,python;tensorflow;machine-learning,2022-06-04 00:05:33,2022-06-04 00:05:33,2022-06-27 18:25:48,i am new to python and machine learning   i am getting an error whenever i run the following code  error  i am not sure if the issue would be related to my data or data types  my data has no blanks 
38,38,19426239,72771746,How to calculate the accumulated weights of my variables in machine learning,"<p>I am working on a dataset that uses decision tree model, random forest and Gradient boosting. So I am trying to get the importance of their variables through their accumulated weight. How can I deal with this?</p>
",10,0,-2,2,machine-learning;data-science,2022-06-27 17:31:19,2022-06-27 17:31:19,2022-06-27 17:31:19,i am working on a dataset that uses decision tree model  random forest and gradient boosting  so i am trying to get the importance of their variables through their accumulated weight  how can i deal with this 
39,39,15493107,72767023,Fullscreen mode not working in D3D12 raytracing samples,"<p>Presently I'm learning the basics of real-time raytracing with the DXR API in DirectX 12 Ultimate. I'm studying the D3D12 raytracing samples on the official GitHub and am using an i9/Intel Iris Xe/RTX3070 laptop and building the programs in VS2022.</p>
<p>Since the samples were written for Windows 10 and I'm using a hybrid graphics PC, a Debug build will run in Windows 11 after adding <code>D3D12_MESSAGE_ID_RESOURCE_BARRIER_MISMATCHING_COMMAND_LIST_TYPE</code> to <strong>D3D12_INFO_QUEUE_FILTER</strong> during device creation (see <a href=""https://stackoverflow.com/questions/69805245/directx-12-application-is-crashing-in-windows-11"">DirectX 12 application is crashing in Windows 11</a>). The only trouble is that none of the sample programs change to fullscreen (i.e. borderless windowed) mode when pressing the Alt+Enter key combination. The programs always stay in windowed mode.</p>
<p>This hasn't worried me so far, because I've been copying the raytracing code over to a template (based on DirectX Tool Kit for Windows desktop) where fullscreen toggling works properly. In this way, I was able to run the <em>HelloWorld</em> and <em>SimpleLighting</em> samples successfully in both windowed mode and fullscreen (2560x1440 monitor resolution).</p>
<p>However, this hasn't been so successful for the <em>ProceduralGeometry</em> sample, which introduces intersection shaders. Once again, the original sample program renders the scene properly, but only in a bordered window. But when the code is reproduced in the template where I can toggle to fullscreen, the raytraced scene does not render properly.</p>
<p>In the scene, the triangle geometry used for the ground plane of the scene renders ok, but a translucent bounding box around the fractal pyramid is visible, and all other procedural geometry also appears translucent. Every couple of seconds, the bounding box for the metaballs also appears briefly, then vanishes.</p>
<p>I was able to determine that by freezing the scene, the reason for the translucency was that the following frames were being presented in sequence:</p>
<ul>
<li>triangle ground plane quad only</li>
<li>floor geometry plus opaque fractal pyramid bounding box</li>
<li>all of the above plus opaque metaball bounding box</li>
<li>completed scene with opaque geometry and no bounding boxes</li>
</ul>
<p>At the native framerate (165Hz on my machine), this results in both the procedural geometry and bounding boxes always being visible, but 'see-through' due to all the partially complete frames being presented to the display. This happens in both windowed and fullscreen modes, but it's worse in fullscreen, because the scene gets affected by random image corruption not seen in windowed mode.</p>
<p>I've been grappling with this issue for a few days and can't work out the problem. The only changes I've made to the sample program are the Windows 11 fix, and using a template for proper fullscreen rendering, which the original sample ignores or doesn't implement properly.</p>
<p>Hopefully someone can shed light on this perplexing issue!</p>
",37,0,1,5,visual-studio-2022;raytracing;windows-11;directx-12;direct3d12,2022-06-27 10:03:11,2022-06-27 10:03:11,2022-06-27 16:16:19,presently i m learning the basics of real time raytracing with the dxr api in directx  ultimate  i m studying the dd raytracing samples on the official github and am using an i intel iris xe rtx laptop and building the programs in vs  since the samples were written for windows  and i m using a hybrid graphics pc  a debug build will run in windows  after adding dd_message_id_resource_barrier_mismatching_command_list_type to dd_info_queue_filter during device creation  see    the only trouble is that none of the sample programs change to fullscreen  i e  borderless windowed  mode when pressing the alt enter key combination  the programs always stay in windowed mode  this hasn t worried me so far  because i ve been copying the raytracing code over to a template  based on directx tool kit for windows desktop  where fullscreen toggling works properly  in this way  i was able to run the helloworld and simplelighting samples successfully in both windowed mode and fullscreen  x monitor resolution   however  this hasn t been so successful for the proceduralgeometry sample  which introduces intersection shaders  once again  the original sample program renders the scene properly  but only in a bordered window  but when the code is reproduced in the template where i can toggle to fullscreen  the raytraced scene does not render properly  in the scene  the triangle geometry used for the ground plane of the scene renders ok  but a translucent bounding box around the fractal pyramid is visible  and all other procedural geometry also appears translucent  every couple of seconds  the bounding box for the metaballs also appears briefly  then vanishes  i was able to determine that by freezing the scene  the reason for the translucency was that the following frames were being presented in sequence  at the native framerate  hz on my machine   this results in both the procedural geometry and bounding boxes always being visible  but  see through  due to all the partially complete frames being presented to the display  this happens in both windowed and fullscreen modes  but it s worse in fullscreen  because the scene gets affected by random image corruption not seen in windowed mode  i ve been grappling with this issue for a few days and can t work out the problem  the only changes i ve made to the sample program are the windows  fix  and using a template for proper fullscreen rendering  which the original sample ignores or doesn t implement properly  hopefully someone can shed light on this perplexing issue 
40,40,1486279,46571431,Using NLP or machine learning to extract keywords off a sentence,"<p>I'm new to the ML/NLP field so my question is what technology would be most appropriate to achieve the following goal:</p>

<p>We have a short sentence - ""Where to go for dinner?"" or ""What's your favorite bar?"" or ""What's your favorite cheap bar?""</p>

<p>Is there a technology that would enable me to train it providing the following data sets:</p>

<ul>
<li>""Where to go for dinner?"" -> Dinner</li>
<li>""What's your favorite bar?"" -> Bar</li>
<li>""What's your favorite cheap restaurant?"" -> Cheap, Restaurant</li>
</ul>

<p>so that next time we have a similar question about an unknown activity, say, ""What is your favorite expensive [whatever]"" it would be able to extract ""expensive"" and [whatever]?</p>

<p>The goal is if we can train it with hundreds of variations(or thousands) of the question asked and relevant output data expected, so that it can work with everyday language.</p>

<p>I know how to make it even without NLP/ML if we have a dictionary of expected terms like Bar, Restaurant, Pool, etc., but we also want it to work with unknown terms.</p>

<p>I've seen examples with Rake and Scikit-learn for classification of ""things"", but I'm not sure how would I feed text into those and all those examples had predefined outputs for training.</p>

<p>I've also tried Google's NLP API, Amazon Lex and Wit to see how good they are at extracting entities, but the results are disappointing to say the least.</p>

<p>Reading about summarization techniques, I'm left with the impression it won't work with small, single-sentence texts, so I haven't delved into it.</p>
",791,2,0,5,machine-learning;nlp;deep-learning;artificial-intelligence;summarization,2017-10-04 23:37:44,2017-10-04 23:37:44,2022-06-27 16:09:35,i m new to the ml nlp field so my question is what technology would be most appropriate to achieve the following goal  we have a short sentence   where to go for dinner  or what s your favorite bar  or what s your favorite cheap bar  is there a technology that would enable me to train it providing the following data sets  so that next time we have a similar question about an unknown activity  say  what is your favorite expensive  whatever  it would be able to extract expensive and  whatever   the goal is if we can train it with hundreds of variations or thousands  of the question asked and relevant output data expected  so that it can work with everyday language  i know how to make it even without nlp ml if we have a dictionary of expected terms like bar  restaurant  pool  etc   but we also want it to work with unknown terms  i ve seen examples with rake and scikit learn for classification of things  but i m not sure how would i feed text into those and all those examples had predefined outputs for training  i ve also tried google s nlp api  amazon lex and wit to see how good they are at extracting entities  but the results are disappointing to say the least  reading about summarization techniques  i m left with the impression it won t work with small  single sentence texts  so i haven t delved into it 
41,41,17012121,72770116,Using Vertex AI model in Android Studio,"<p>I'm trying to use Vertex AI model in Android Studio. I already deployed the model in Vertex AI but I don't know how to use it in android studio. My model is based on Machine Learning (scikit-learn (.pkl)) so I don't use firebase which usually came with tensorflow. Does anyone has any reference?</p>
<p>I already searched the Internet but haven't got any clue.</p>
",18,0,-1,5,android;android-studio;machine-learning;google-cloud-platform;google-cloud-vertex-ai,2022-06-27 15:23:22,2022-06-27 15:23:22,2022-06-27 15:32:37,i m trying to use vertex ai model in android studio  i already deployed the model in vertex ai but i don t know how to use it in android studio  my model is based on machine learning  scikit learn   pkl   so i don t use firebase which usually came with tensorflow  does anyone has any reference  i already searched the internet but haven t got any clue 
42,42,14759019,72770172,What strategy to use for industry prediction in a ML project where we have a website URL?,"<p>I have a machine learning project where they ask me to predict the industry of new customers where their website is given.
For example, the model should predict the industry as &quot; social media &quot; if the given website is &quot; <a href=""https://www.instagram.com"" rel=""nofollow noreferrer"">https://www.instagram.com</a> &quot;.
I have 50000 rows of labeled data which contains the websites URLs and their corresponding industries.
What kind of strategy should I follow in order to create a good model?</p>
",12,0,-1,3,machine-learning;statistics;data-science,2022-06-27 15:27:26,2022-06-27 15:27:26,2022-06-27 15:27:26,
43,43,19420872,72764054,Classification ML Model Training with Unbalanced Dataset,"<p>I am trying to do classification with machine learning.
I have &quot;good&quot; and &quot;bad&quot; classes in my dataset.</p>
<pre><code>Dataset shape: (248857, 12)
</code></pre>
<p>Due to some conditions, I am not able to collect more &quot;good&quot; class results, there are around 40k good, and 210k bad results. Is that an issue more with the models?</p>
<p>I trained the model in this way:
(as an example I used here Naive Bayes but I use KNN, SVM, MLP, Random Forest, and Decision Tree as well)</p>
<pre><code>X = df.drop(['Label'], axis=1)
y = df['Label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)
classifier = GaussianNB()  
classifier.fit(X_train, y_train)  
y_predNaive = classifier.predict(X_test)  
print(f'Test score {accuracy_score(y_predNaive,y_test)}')
plot_confusionmatrix(y_predNaive,y_test,dom='Test')
print('Classification Report for Naive Bayes\n\n', classification_report(y_test, y_predNaive))
</code></pre>
",33,1,-1,5,python;machine-learning;scikit-learn;classification;imbalanced-data,2022-06-26 23:36:50,2022-06-26 23:36:50,2022-06-27 13:26:39,due to some conditions  i am not able to collect more  good  class results  there are around k good  and k bad results  is that an issue more with the models 
44,44,11233731,72764222,How to treat different size images in ML pipeline,"<p>I'm here asking a general question about image processing applied to a machine learning pipeline. In this post, I will refer to ML as every algorithm that is not deep learning (therefore it doesn't use a neural network).</p>
<p>I'm developing a classifier to catalog different clothes .png images. I have labels (for each image I know the category) so it's a supervised learning problem.
My objective is to use PCA to reduce the problem's dimensionality and then use bag of visual words to perform the classification. I'm using python for this project.</p>
<p>The problem is that each photo has a different size and a different ratio between width and height (therefore I can't only resize them because I wouldn't have a unique height value for each image).</p>
<p>My, inelegant, solution is to fix the width at 200 px and then pad a bunch of zeros rows to each image (each image is a NumPy array of maximum_h rows and each row is width long).
Here the script:</p>
<pre><code>#help function to convert images in array
def get_image(image_path: str, resize=True, w=300):
    &quot;&quot;&quot;
    :param image_path: string, path of the image
    :param resize: boolean, if True the image is resized. Default: True
    :param w: integer, specify the width of the resized image
    :return: numpy array of the greyscale version of the image
    &quot;&quot;&quot;
    try:
        image = Image.open(image_path).convert(&quot;L&quot;)
        if resize:
            wpercent = (w/float(image.size[0]))
            hsize = int((float(image.size[1])*float(wpercent)))
            image = image.resize((w,hsize), Image.ANTIALIAS)
        #pixel_values = np.array(image.getdata())

        return image

    except:
        #AI19/04442.png corrupted
        #AI18/02971.png corrupted
        #print(image_path)
        return None



def extract_images(paths:list, categories: list, w: int, maximum_h: int):
    A = np.zeros([len(paths), w * maximum_h])
    y = []
    counter = 0

    for image_path, label in tqdm(zip(paths, categories)):
        im = get_image(image_path, w=w)
        if im:
            #adapt images to fit
            h,w = np.array(im).shape
            delta_h = maximum_h-h
            zeros_ = np.zeros((delta_h, w), dtype=int)
            im = np.concatenate((im, zeros_), axis=0)

            A[counter, :] = im.reshape(1, -1)
            y.append(label)

            counter += 1
        else:
            continue

    return (A,y)
</code></pre>
<p>The problem here is the classifier performs badly (20%) because I add a significant amount of zeros to each image that increases the dimensionality but doesn't add information.</p>
<p>Looking at the biggest eigenvectors of the PCA algorithm I see that a lot of information is concentrated in these &quot;padding&quot; area (and this confirm my impression).</p>
<p>Is there a better way to handle different size images in python?</p>
",19,0,0,4,python;machine-learning;image-processing;pca,2022-06-27 00:03:52,2022-06-27 00:03:52,2022-06-27 13:26:00,i m here asking a general question about image processing applied to a machine learning pipeline  in this post  i will refer to ml as every algorithm that is not deep learning  therefore it doesn t use a neural network   the problem is that each photo has a different size and a different ratio between width and height  therefore i can t only resize them because i wouldn t have a unique height value for each image   the problem here is the classifier performs badly     because i add a significant amount of zeros to each image that increases the dimensionality but doesn t add information  looking at the biggest eigenvectors of the pca algorithm i see that a lot of information is concentrated in these  padding  area  and this confirm my impression   is there a better way to handle different size images in python 
45,45,18399367,72731429,Machine Learning: Using a model that overfits but performs well in CV,"<p>I am currently training a KNeighborsClassifier model on sports betting data where the outcomes are normally 50/50.</p>
<p>My current training dataset overfits when trained, but has a mean precision score of 0.54 when training using cross validation.</p>
<p>What next steps can I take to ensure the model does not overfit? To note, I am using the whole training dataset and only 3 features.</p>
<pre><code>params = {
             'leaf_size': [10,15,20],
             'n_jobs':[1,2,3],
             'n_neighbors':[10,20,30],
}

gs_knn = RandomizedSearchCV(estimator=KNeighborsClassifier(),
                   param_distributions=params,
                      scoring='precision', cv=10, n_iter=100,
                   random_state=0)
gs_knn.fit(X_train, y_train)
gs_knn.best_estimator_
</code></pre>
",35,1,-1,4,machine-learning;cross-validation;knn;overfitting-underfitting,2022-06-23 19:27:08,2022-06-23 19:27:08,2022-06-27 13:12:46,i am currently training a kneighborsclassifier model on sports betting data where the outcomes are normally    my current training dataset overfits when trained  but has a mean precision score of   when training using cross validation  what next steps can i take to ensure the model does not overfit  to note  i am using the whole training dataset and only  features 
46,46,3286984,72766857,Memory isn&#39;t freed when scope is left in C++,"<p>I'm fairly new to C++ so please forgive me for my ignorance. I'm under the impression that anything between <code>{</code> and <code>}</code> is called a scope, and that you can create a separate scope inside a function, or anything else, just by adding more brackets.  For example:</p>
<pre class=""lang-cpp prettyprint-override""><code>int foo(){
    std::cout &lt;&lt; &quot;I'm inside the scope of foo&quot; &lt;&lt; std::endl;
    {
        std::cout &lt;&lt; &quot;I'm inside a scope that's inside the scope of foo&quot; &lt;&lt; std::endl;
    }
}
</code></pre>
<p>I was learning about this in relation to pointers and memory leaks.  My understanding is when you leave a scope all variables should be freed from memory unless the memory was manually allocated with <code>new</code> or <code>malloc</code>. In my testing, however, this does not seem to be the case.  I've written the following script to test this:</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &lt;iostream&gt;

void test(){

    {
        int regdata = 240;
        int* pointerInt = new int(1);
        *pointerInt = 15;
        std::cout &lt;&lt; &quot;RegData Addr: &quot; &lt;&lt; &amp;regdata &lt;&lt; std::endl;
        std::cout &lt;&lt; &quot;Value:        &quot; &lt;&lt; regdata &lt;&lt; std::endl;
        std::cout &lt;&lt; &quot;Pointer Addr: &quot; &lt;&lt; &amp;pointerInt &lt;&lt; std::endl;
        std::cout &lt;&lt; &quot;Pointer:      &quot; &lt;&lt; pointerInt &lt;&lt; std::endl;
        std::cout &lt;&lt; &quot;Value:        &quot; &lt;&lt; *pointerInt &lt;&lt; std::endl;
        std::cout &lt;&lt; std::endl;
        std::cout &lt;&lt; &quot;Press any key then enter to leave the scope.&quot;;
        char temp;
        std::cin &gt;&gt; temp;
        //delete pointerInt;
    }

    std::cout &lt;&lt; &quot;The scope has been left.&quot; &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;Press any key then enter to leave the function.&quot;;
    char temp;
    std::cin &gt;&gt; temp;
}

int main(){
    test();
    std::cout &lt;&lt; &quot;The function has been left.&quot; &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;Press any key then enter to leave the program.&quot;;
    char temp;
    std::cin &gt;&gt; temp;
}
</code></pre>
<p>I start this program on my Windows 10 computer and have been monitoring the memory usage using the program Cheat Engine.  Now, depending on whether or not I have <code>delete</code> commented out it will delete the bytes that hold <code>15</code> and replace them with random bytes when I leave the scope as it should.  However, the memory holding the <code>240</code> is not freed until after I leave the scope of <code>test</code> (at which point the <code>240</code> is replaced with <code>1</code>). And regardless of if the <code>delete</code> is commented out, the actual pointer itself is never deleted out of memory.</p>
<p>Is my compiler or my machine not compiling/running my code correctly? Or am I misunderstanding memory management between scopes?  If it's the latter, please correct me so I can properly understand what is supposed to happen.  Also let me know if something doesn't make sense!</p>
",89,2,-1,3,c++;memory;memory-leaks,2022-06-27 09:27:15,2022-06-27 09:27:15,2022-06-27 10:54:25,i m fairly new to c   so please forgive me for my ignorance  i m under the impression that anything between   and   is called a scope  and that you can create a separate scope inside a function  or anything else  just by adding more brackets   for example  i was learning about this in relation to pointers and memory leaks   my understanding is when you leave a scope all variables should be freed from memory unless the memory was manually allocated with new or malloc  in my testing  however  this does not seem to be the case   i ve written the following script to test this  i start this program on my windows  computer and have been monitoring the memory usage using the program cheat engine   now  depending on whether or not i have delete commented out it will delete the bytes that hold  and replace them with random bytes when i leave the scope as it should   however  the memory holding the  is not freed until after i leave the scope of test  at which point the  is replaced with    and regardless of if the delete is commented out  the actual pointer itself is never deleted out of memory  is my compiler or my machine not compiling running my code correctly  or am i misunderstanding memory management between scopes   if it s the latter  please correct me so i can properly understand what is supposed to happen   also let me know if something doesn t make sense 
47,47,18186848,72767263,1 I&#39;m trying to implement a machine learning for insider threat detection using CERT dataset,"<p>The dataset has many csv files, Can anyone help me out in doing merging and feature extraction to suit my Machine learning model?</p>
",20,0,-1,1,python,2022-06-27 10:47:12,2022-06-27 10:47:12,2022-06-27 10:47:12,the dataset has many csv files  can anyone help me out in doing merging and feature extraction to suit my machine learning model 
48,48,19423179,72766936,Tensorflow - Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered,"<p>I am trying to setup a deep learning machine with dual rtx 3070 gpus in Ubuntu 20.04. I have installed Nvidia drivers 460,CUDA 11.2 and Cudnn 8.1. When i try to test the gpu with a sample tensorflow code i am getting CUDA_ERROR_ILLEGAL_ADDRESS on both GPUs. Can someone let me know what the issue is?</p>
<p>Hitting this issue in Python3.8 and 3.9 and also in tensorflow 2.5.0,2.9.0</p>
<pre><code>Mon Jun 27 14:10:22 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3070    Off  | 00000000:09:00.0 Off |                  N/A |
| 57%   46C    P8    28W / 270W |     15MiB /  7979MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 3070    Off  | 00000000:0A:00.0 Off |                  N/A |
|  0%   48C    P8    23W / 270W |      5MiB /  7982MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1264      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      1463      G   /usr/bin/gnome-shell                3MiB |
|    1   N/A  N/A      1264      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
</code></pre>
<pre><code>2022-06-27 13:59:11.843491: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-27 13:59:12.901104: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-06-27 13:59:12.943243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.943685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-06-27 13:59:12.943725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.944127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties:
pciBusID: 0000:0a:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-06-27 13:59:12.944141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-27 13:59:12.945421: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-27 13:59:12.945447: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-06-27 13:59:12.945900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-06-27 13:59:12.946021: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-06-27 13:59:12.946360: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-06-27 13:59:12.946647: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-06-27 13:59:12.946717: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-06-27 13:59:12.946758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.947192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.947610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.948028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:12.948421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-27 13:59:12.948662: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-27 13:59:13.250592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.250986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-06-27 13:59:13.251025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.251384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties:
pciBusID: 0000:0a:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2022-06-27 13:59:13.251420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.251794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.252260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.252633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.252984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-27 13:59:13.253013: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-27 13:59:13.721007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-27 13:59:13.721033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1
2022-06-27 13:59:13.721041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N
2022-06-27 13:59:13.721045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N
2022-06-27 13:59:13.721180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.721614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.722002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.722382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.722757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.723125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6114 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:09:00.0, compute capability: 8.6)
2022-06-27 13:59:13.723332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-27 13:59:13.723699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6126 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 3070, pci bus id: 0000:0a:00.0, compute capability: 8.6)
2022-06-27 13:59:14.182153: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-06-27 13:59:14.201622: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3700310000 Hz
Epoch 1/10
2022-06-27 13:59:14.370438: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-27 13:59:14.763996: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-06-27 13:59:14.764035: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1563/1563 [==============================] - 3s 1ms/step - loss: 1.8131 - accuracy: 0.3549
Epoch 2/10
 500/1563 [========&gt;.....................] - ETA: 1s - loss: 1.6540 - accuracy: 0.4167Traceback (most recent call last):
  File &quot;/home/vicky/testtf/testf.py&quot;, line 24, in &lt;module&gt;
    model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 10)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1188, in fit
    callbacks.on_train_batch_end(end_step, logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 457, in on_train_batch_end
    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 317, in _call_batch_hook
    self._call_batch_end_hook(mode, batch, logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 337, in _call_batch_end_hook
    self._call_batch_hook_helper(hook_name, batch, logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 375, in _call_batch_hook_helper
    hook(batch, logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 1029, in on_train_batch_end
    self._batch_update_progbar(batch, logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py&quot;, line 1101, in _batch_update_progbar
    logs = tf_utils.sync_to_numpy_or_python_type(logs)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py&quot;, line 519, in sync_to_numpy_or_python_type
    return nest.map_structure(_to_single_numpy_or_python_type, tensors)
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/util/nest.py&quot;, line 867, in map_structure
    structure[0], [func(*x) for x in entries],
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/util/nest.py&quot;, line 867, in &lt;listcomp&gt;
    structure[0], [func(*x) for x in entries],
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py&quot;, line 515, in _to_single_numpy_or_python_type
    x = t.numpy()
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1094, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File &quot;/home/vicky/testtf/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1062, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
</code></pre>
<p>This is the sample code i am running</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
import numpy as np
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()
# scaling image values between 0-1
X_train_scaled = X_train/255
X_test_scaled = X_test/255
# one hot encoding labels
y_train_encoded = keras.utils.to_categorical(y_train, num_classes = 10, dtype = 'float32')
y_test_encoded = keras.utils.to_categorical(y_test, num_classes = 10, dtype = 'float32')
def get_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(32,32,3)),
        keras.layers.Dense(3000, activation='relu'),
        keras.layers.Dense(1000, activation='relu'),
        keras.layers.Dense(10, activation='sigmoid')
    ])
    model.compile(optimizer='SGD',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model
with tf.device('/GPU:0'):
    model_gpu = get_model()
    model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 10)
</code></pre>
",28,0,0,3,python;tensorflow;keras,2022-06-27 09:44:11,2022-06-27 09:44:11,2022-06-27 09:59:14,i am trying to setup a deep learning machine with dual rtx  gpus in ubuntu    i have installed nvidia drivers  cuda   and cudnn    when i try to test the gpu with a sample tensorflow code i am getting cuda_error_illegal_address on both gpus  can someone let me know what the issue is  hitting this issue in python  and   and also in tensorflow       this is the sample code i am running
49,49,17013454,69342342,Google Cloud Platform SSH error code 1006,"<p>I made an instance and connected SSH in my project. My project is for machine learning, so I opened jupyter notebook and ran it. But the terminal always sends me an error like this.
&quot;Connection via Cloud Identity-Aware Proxy Failed
Code: 1006
Please ensure you can make a proper https connection to the IAP for TCP hostname:<a href=""https://tunnel.cloudproxy.app"" rel=""nofollow noreferrer"">https://tunnel.cloudproxy.app</a>.
You may be able to connect without using the Cloud Identity-Aware Proxy.&quot;</p>
<p>I have tried to create firewall rule but this doesn't work.
Someone says that this is because of session timeout. If it is, what can I do to solve it?</p>
",876,0,2,3,google-cloud-platform;ssh;google-compute-engine,2021-09-27 12:23:08,2021-09-27 12:23:08,2022-06-27 02:20:13,
50,50,19074930,72760632,How can i use a movie subtitles api to create a dataset of movie subtitles,"<p>To be more specific i want to get the subtitles of every movie in the MovieLens Dataset because i want to create a new dataset that contains the title of a movie and the script.I need to use it for a machine learning project.I created a dataset with only 1209 movies by web scraping IMSDB but i need more scripts for my project</p>
",21,0,-3,5,python;dataframe;api;data-science;moviepy,2022-06-26 15:41:05,2022-06-26 15:41:05,2022-06-26 19:08:28,to be more specific i want to get the subtitles of every movie in the movielens dataset because i want to create a new dataset that contains the title of a movie and the script i need to use it for a machine learning project i created a dataset with only  movies by web scraping imsdb but i need more scripts for my project
51,51,17224304,72733362,create error bars for random forest regression,"<p>I'm new to the world of machine learning and more generally to AI.
I am analyzing a dataset containing characteristics of different houses and their prices using Python and JupyterLab.</p>
<p>Here is the dataset in use:
<a href=""https://www.kaggle.com/datasets/harlfoxem/housesalesprediction"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/harlfoxem/housesalesprediction</a></p>
<p>I applied random forest (scikit-learn) on this dataset and now I would like to plot the error bars of the model.
Specifically, I'm using the ForestCI package and applying exactly this code to my case:
<a href=""http://contrib.scikit-learn.org/forest-confidence-interval/auto_examples/plot_mpg.html"" rel=""nofollow noreferrer"">http://contrib.scikit-learn.org/forest-confidence-interval/auto_examples/plot_mpg.html</a></p>
<p>This is my code:</p>
<pre class=""lang-py prettyprint-override""><code># Regression Forest Example
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn import metrics
from sklearn.metrics import r2_score
import numpy as np
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestRegressor
import sklearn.model_selection as xval
import forestci as fci

#import dataset
mpg_data = pd.read_csv(path_to_dataset)

#drop some useless features
mpg_data=mpg_data.drop('date', axis=1)
mpg_data=mpg_data.drop('yr_built', axis=1)
mpg_data = mpg_data.drop([&quot;id&quot;],axis=1)

#separate mpg data into predictors and outcome variable
mpg_X = mpg_data.drop(labels='price', axis=1)
mpg_y = mpg_data['price']

# remove rows where the data is nan
not_null_sel = np.where(mpg_X.isna().sum(axis=1).values == 0)
mpg_X = mpg_X.values[not_null_sel]
mpg_y = mpg_y.values[not_null_sel]

# split mpg data into training and test set
mpg_X_train, mpg_X_test, mpg_y_train, mpg_y_test = xval.train_test_split(
    mpg_X,
    mpg_y,
    test_size=0.25,
    random_state=42)

# Create RandomForestRegressor
mpg_forest = RandomForestRegressor(random_state=42)
mpg_forest.fit(mpg_X_train, mpg_y_train)
mpg_y_hat = mpg_forest.predict(mpg_X_test)

# Plot predicted MPG without error bars
plt.scatter(mpg_y_test, mpg_y_hat)
plt.xlabel('Reported MPG')
plt.ylabel('Predicted MPG')
plt.show()

print(r2_score(mpg_y_test, mpg_y_hat))

# Calculate the variance
mpg_V_IJ_unbiased = fci.random_forest_error(mpg_forest, mpg_X_train,
                                            mpg_X_test)

# Plot error bars for predicted MPG using unbiased variance
plt.errorbar(mpg_y_test, mpg_y_hat, yerr=np.sqrt(mpg_V_IJ_unbiased), fmt='o')
plt.xlabel('Reported MPG')
plt.ylabel('Predicted MPG')
plt.show()
</code></pre>
<p>It seems to work but when the graphs are plotted, neither the error bar nor the prediction line appears:</p>
<p><a href=""https://i.stack.imgur.com/pS4tJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pS4tJ.png"" alt=""the error bar and the prediction line not appears"" /></a></p>
<p>Instead, as visible in the documentation, it should look like the picture here: <a href=""http://contrib.scikit-learn.org/forest-confidence-interval/auto_examples/plot_mpg.html"" rel=""nofollow noreferrer"">http://contrib.scikit-learn.org/forest-confidence-interval/auto_examples/plot_mpg.html</a></p>
",41,1,0,5,machine-learning;scikit-learn;regression;random-forest;confidence-interval,2022-06-23 21:39:07,2022-06-23 21:39:07,2022-06-26 14:25:40,this is my code  it seems to work but when the graphs are plotted  neither the error bar nor the prediction line appears   instead  as visible in the documentation  it should look like the picture here  
52,52,16744902,72714050,Matrix operation in an Attention Mechanism,"<p>I am reading an article dealing with Transformer Machine Learning models applied to finance. I am trying to understand the math behind the architecture, but I failed to understand this part :</p>
<p><a href=""https://i.stack.imgur.com/QKjx3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QKjx3.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/7s3Im.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7s3Im.png"" alt=""enter image description here"" /></a></p>
<p>Especially, I don't get why the dimensions are not matching between the operations.
According to my comprehension:</p>
<ol>
<li><strong>step (8)</strong> : <em>u</em> should be <em>M(d_model, 1)</em></li>
<li><strong>step (9)</strong> : this should not be possible as the matrix    multiplication dimensions does not match to perform the operation:
<em>M(d_model, K) . M(1,d_model)</em></li>
</ol>
<p>Here is the full part of the study :</p>
<p><a href=""https://i.stack.imgur.com/aDUun.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aDUun.png"" alt=""enter image description here"" /></a></p>
<p>I guess, I am missunderstanding something with this notation <a href=""https://i.stack.imgur.com/u761f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u761f.png"" alt=""enter image description here"" /></a>
or with the <em>&quot;non-linearly project the matrix M to u&quot;</em> sentence.</p>
<p>Can someone enlighten me about this, please ?</p>
<blockquote>
<p><strong>Transformer-based attention network for stock movement prediction</strong>, 2022,
<em>Qiuyue Zhang, Chao Qin, Yunfeng Zhang , Fangxun Bao, CaimingZhang, Peide Liu</em></p>
</blockquote>
",55,1,-1,4,machine-learning;math;matrix;transformer,2022-06-22 16:12:09,2022-06-22 16:12:09,2022-06-26 13:38:24,i am reading an article dealing with transformer machine learning models applied to finance  i am trying to understand the math behind the architecture  but i failed to understand this part   here is the full part of the study    can someone enlighten me about this  please  
53,53,7963676,72759371,CNN feature extraction having multiple column classes,"<p>I have a dataset which consists of power signals and the targets are multiple house appliances that might be on or off. I only need to do feature extraction on the signal using cnn and then save the dataset as a csv file to use it with another Machine learning method.</p>
<p>I used CNN before for classification on signals consisting of 6 classes. However, i am a bit confused and i need your help. I have two questions (might be stupid and am sorry)</p>
<ol>
<li>Do i need the target variable in order to do feature extraction?</li>
<li>The shape of my dataset is for example 40000x100. I need my extracted dataset (the features learned using CNN) to have the same amount of rows (i.e. 40000). How can i do that?</li>
</ol>
<p>I know that the answer might be simpler than i think but at the moment i feel quite lost.</p>
<p>I would appreciate any help.</p>
<p>Thanks</p>
",15,0,0,3,python;conv-neural-network;feature-extraction,2022-06-26 11:59:25,2022-06-26 11:59:25,2022-06-26 11:59:25,i have a dataset which consists of power signals and the targets are multiple house appliances that might be on or off  i only need to do feature extraction on the signal using cnn and then save the dataset as a csv file to use it with another machine learning method  i used cnn before for classification on signals consisting of  classes  however  i am a bit confused and i need your help  i have two questions  might be stupid and am sorry  i know that the answer might be simpler than i think but at the moment i feel quite lost  i would appreciate any help  thanks
54,54,13740835,66830006,YOLO graphs explanation,"<p>I don't know if I can ask this here but I have a question regarding machine learning. I'm fairly new to machine learning and followed a tutorial for YOLO object detection system. I managed to get it work and it's highly accurate. However, I was looking at the results and found these graph results, showing the Precision, Recall, etc. Now, even though I know how to calculate the precision and recall, I'm not sure how the other graphs are calculated.</p>
<p>Would be nice if someone explained or maybe shared an article regarding YOLO output graphs as I haven't found any. Thanks</p>
<p><a href=""https://imgur.com/a/55fnMvb"" rel=""nofollow noreferrer"">https://imgur.com/a/55fnMvb</a></p>
",154,1,0,3,graph;artificial-intelligence;yolo,2021-03-27 15:24:29,2021-03-27 15:24:29,2022-06-26 11:55:38,i don t know if i can ask this here but i have a question regarding machine learning  i m fairly new to machine learning and followed a tutorial for yolo object detection system  i managed to get it work and it s highly accurate  however  i was looking at the results and found these graph results  showing the precision  recall  etc  now  even though i know how to calculate the precision and recall  i m not sure how the other graphs are calculated  would be nice if someone explained or maybe shared an article regarding yolo output graphs as i haven t found any  thanks 
55,55,14372590,72752564,Any specific python library to accurately distinguish minority class/ outliers,"<p>I am wondering is there any specific python library to correctly predict y(dependent variable) for the minority class or outliers than using any resampling techniques.</p>
<p>My data set is detecting fraud credit card transactions.</p>
<p>I tried using resampling techniques and yet the prediction accuracy of the minority class with machine learning algorithms such as SVM, Naive Bayes is around 80%, this is not sufficient as before resampling the minority class consists of around just only 5% of the total data set.</p>
<p>That is why I ask is there any specific python library to detect the minority class</p>
",22,0,-2,4,python-3.x;machine-learning;outliers;pattern-recognition,2022-06-25 14:31:40,2022-06-25 14:31:40,2022-06-26 09:12:43,i am wondering is there any specific python library to correctly predict y dependent variable  for the minority class or outliers than using any resampling techniques  my data set is detecting fraud credit card transactions  i tried using resampling techniques and yet the prediction accuracy of the minority class with machine learning algorithms such as svm  naive bayes is around    this is not sufficient as before resampling the minority class consists of around just only   of the total data set  that is why i ask is there any specific python library to detect the minority class
56,56,17480851,72758043,How to get keyboard/key selection to work in JavaScript,"<p>I am trying to build a simple sound machine with various sounds only using JavaScript, specifically, Zelda Ocarina of Time sounds. Yes, I know it's basic stuff, but I am still learning and can't for the life of me figure this out.</p>
<p>Now, I can get the left and right arrow keys to cycle through the choices, but when I use the keys: A,S,D,F,G, or H I can't get them to make a selection. No matter how many avenues I have tried.</p>
<p>Any help, tips, pointers, or ANYTHING constructive would be appreciated.</p>
<p><a href=""https://i.stack.imgur.com/rc5aK.jpg"" rel=""nofollow noreferrer"">Sound Machine in browser</a></p>
<p>CODE:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const regions = Array.from(document.querySelectorAll('section[role=""region""]'));
const keys = Array.from(document.querySelectorAll('#keys button'))
const keyTypes = keys.map(op =&gt; op.innerHTML);



const moveFocus = (region, item) =&gt; {
    region[item].focus();
}

const handleArrowEvent = (event, items, currentRegion) =&gt; {
    let currentItem =0;
    if(
        event.code === 'ArrowLeft' ||
        event.code === 'ArrowRight' ||
        event.code === 'KeyA' ||
        event.code === 'KeyS' ||
        event.code === 'KeyD' ||
        event.code === 'KeyF' ||
        event.code === 'KeyG' ||
        event.code === 'KeyH'
    ) {
        event.preventDefault();
        event.stopPropagation();
        console.log(event.target);
        const regionItems = Array.from(currentRegion.children);
        regionItems.forEach(child =&gt; {
            items.push(child)
        })
        currentItem = items.indexOf(event.target);
        const lastItem = items.length - 1;
        const isFirst = currentItem === 0;
        const isLast = currentItem === lastItem;

        if(event.code === 'ArrowRight') {
            currentItem = isLast ? 0 : currentItem + 1;
        } else if (event.code === 'ArrowLeft') {
            currentItem = isFirst ? lastItem : currentItem - 1;
        }

            moveFocus(regionItems, currentItem)
    }
}

const handleClick = event =&gt; {
    registerInput(event.target.innerHTML)
}



const handleKeyEvent = event =&gt; {
    const items = [];
    const currentRegion = event.target.closest('section[role=""region""]')
    if (
        event.code === 'ArrowLeft' ||
        event.code === 'ArrowRight' ||
        event.code === 'KeyA' ||
        event.code === 'KeyS' ||
        event.code === 'KeyD' ||
        event.code === 'KeyF' ||
        event.code === 'KeyG' ||
        event.code === 'KeyH'
    ) { handleArrowEvent(event, items, currentRegion) }
}

const registerInput = input =&gt; {
    console.log(input)
}</code></pre>
<pre class=""snippet-code-css lang-css prettyprint-override""><code>@import url('https://fonts.googleapis.com/css2?family=Bangers&amp;family=Heebo:wght@300&amp;family=Press+Start+2P&amp;display=swap');

html {
    font-size: 10px;
    background-image: url(../assets/img/lofz_ocTime.jpg);
    background-size: cover;
}

body , html {
    margin: 0;
    padding: 0;
    font-family: 'Bangers', cursive;
    font-size: 12px;
}

#keys {
    display: flex;
    flex: 1;
    min-height: 80vh;
    align-items: center;
    justify-content: center;
}


button {
    border: .4rem solid rgb(255, 5, 5);
    border-radius: .5rem;
    margin: 1rem;
    font-size: 1.5rem;
    padding: 1rem .5rem;
    transition: all 0.07s ease;
    width: 10rem;
    text-align: center;
    color: rgb(31, 240, 3);
    background: rgba(8, 7, 94, 0.753);
    font-weight: bold;
    text-shadow: 0 0 .5rem black;
}</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;

&lt;head&gt;
  &lt;meta charset=""UTF-8""&gt;
  &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1.0""&gt;
  &lt;link rel=""stylesheet"" href=""./css/stylesheet.css""&gt;
  &lt;link rel=""stylesheet"" href=""./css/normalize.css""&gt;
  &lt;title&gt;Zelda: Ocarina of Time Sounds&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;


  &lt;section id=""keys"" aria-label=""Zelda Sounds"" role=""region""&gt;
    &lt;button  data-key=""65"" class=""sound"" class=""key"" aria-label=""Key A""&gt;A&lt;aside&gt;Hey! Over Here!&lt;/aside&gt;&lt;/button&gt;
    &lt;button  data-key=""83"" class=""sound"" class=""key"" aria-label=""Key S""&gt;S&lt;aside&gt;Link, Listen!&lt;/aside&gt;&lt;/button&gt;
    &lt;button  data-key=""68"" class=""sound"" class=""key"" aria-label=""Key D""&gt;D&lt;aside&gt;Gold Skulltula&lt;/aside&gt;&lt;/button&gt;
    &lt;button  data-key=""70"" class=""sound"" class=""key"" aria-label=""Key F""&gt;F&lt;aside&gt;Item Found!&lt;/aside&gt;&lt;/button&gt;
    &lt;button  data-key=""71"" class=""sound"" class=""key"" aria-label=""Key G""&gt;G&lt;aside&gt;Silver Rupee&lt;/aside&gt;&lt;/button&gt;
    &lt;button  data-key=""72"" class=""sound"" class=""key"" aria-label=""Key H""&gt;H&lt;aside&gt;Found a Heart!&lt;/aside&gt;&lt;/button&gt;
  &lt;/section&gt;

  &lt;script src=""js/main.js"" type=""module"" defer&gt;&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;</code></pre>
</div>
</div>
</p>
",26,1,0,4,javascript;html;event-handling;key,2022-06-26 05:24:08,2022-06-26 05:24:08,2022-06-26 06:25:34,i am trying to build a simple sound machine with various sounds only using javascript  specifically  zelda ocarina of time sounds  yes  i know it s basic stuff  but i am still learning and can t for the life of me figure this out  now  i can get the left and right arrow keys to cycle through the choices  but when i use the keys  a s d f g  or h i can t get them to make a selection  no matter how many avenues i have tried  any help  tips  pointers  or anything constructive would be appreciated   code 
57,57,12309795,72755996,ValueError: The estimator function should be a regressor,"<p>I'm trying to use the stacking method from ensemble learning for my regression problem, I write 2 machine learning models (FNN, Random Forest, and CNN).
My codes for fnn, RF, and CNN are:</p>
<pre><code># Load Library
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import train_test_split
from scipy.io import loadmat
from scipy.io import savemat
import numpy as np
from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
from keras.models import load_model
import matplotlib.pyplot as plt
from sklearn import metrics
# from keras.utils.vis_utils import plot_model

from sklearn.ensemble import StackingRegressor

import numpy
from models import  CNN_train , CNN_model, CNN_predict 
#from scipy.misc import imresize
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
# load training and testing datasets

X = loadmat(&quot;OptimizationData_V2_4_2&quot;+ &quot;.mat&quot;)[&quot;R_mat&quot;]
y = loadmat(&quot;OptimizationData_V2_4_2&quot;+ &quot;.mat&quot;)[&quot;index_mat&quot;]
X=X.T
y=y.T
a= np.max(y)
y_scaled = y / np.max(y)
in_dim=len(X.T)
out_dim=len(y.T)
test_size= 0.2      
Validation_size= 0.2
N_epoch=1                # Number of epoch
seed = 42

# Splitting Data into test &amp; train
X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size = test_size, random_state = seed)


########################################## FNN MODEL #######################################

fnn_model = Sequential()
fnn_model._estimator_type='regressor'
fnn_model.add(Dense(256, input_dim=in_dim, activation='relu'))
fnn_model.add(Dense(128, activation='relu'))
#Output layer
fnn_model.add(Dense(out_dim, activation='linear'))
#Compile
fnn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','accuracy'])
fnn_model.summary()


# Save model
checkpoint = ModelCheckpoint(&quot;FNN_check.h5&quot;, monitor='val_loss', verbose=1, save_best_only=True,
                              save_weights_only=False, mode='min')  #Use Mode = max for accuracy and min for loss.

early_stop = EarlyStopping(monitor='val_loss', patience=7, verbose=1)
#This callback will stop the training when there is no improvement in
# the validation loss for three consecutive epochs.

#CSVLogger logs epoch, acc, loss, val_acc, val_loss
log_csv = CSVLogger('my_fnn_logs.csv', separator=',', append=False)

callbacks_list = [checkpoint, early_stop, log_csv]




history = fnn_model.fit(X_train, y_train, validation_split=0.2,batch_size=512,
                        callbacks=callbacks_list,epochs =1)
# Score = model.score(X_train,y_train)

from matplotlib import pyplot as plt
#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.figure(3)

plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(4)
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('accuracy')
plt.legend()
plt.show()


############################################
#Predict on test data
predictions = fnn_model.predict(X)
Y_train_pred_FNN = fnn_model.predict(X_train)
Y_test_pred_FNN = fnn_model.predict(X_test)


savemat(&quot;FNN_Optimization_Data_V2_2_4.mat&quot;, {&quot;X&quot;: X , &quot;Y&quot;: y, &quot;Y_predict&quot;: predictions,  &quot;X_test&quot;: X_test ,
                &quot;Y_test&quot;: y_test,&quot;Y_predict_test&quot;: Y_test_pred_FNN,&quot;Y_predict_train&quot;: Y_train_pred_FNN,
                 &quot;train_loss&quot;: loss, &quot;test_loss&quot;: val_loss,&quot;Epoch&quot;: epochs}) 

fnn_model.save('FNN_Model.h5')  # always save your weights after training or during training     
                                       
#All Good
print(&quot;For FNN resuls_Saved \nAll Good&quot;)


################################################################

#################################### RF MODEL ####################################

### Decision tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

tree = DecisionTreeRegressor()
tree._estimator_type='regressor'

tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
mse_dt = mean_squared_error(y_test, y_pred_tree)
mae_dt = mean_absolute_error(y_test, y_pred_tree)
print('Mean squared error using decision tree: ', mse_dt)
print('Mean absolute error using decision tree: ', mae_dt)

from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators = 5, random_state=42)
rf_model._estimator_type='regressor'





rf_model.fit(X_train, y_train)
Score = rf_model.score(X_train,y_train)
print('Score = ',Score)
y_pred_RF = rf_model.predict(X_test)
Y_train_pred_RF = rf_model.predict(X_train)
Y_test_pred_RF = rf_model.predict(X_test)
predictions = rf_model.predict(X)
mse_RF = mean_squared_error(y_test, y_pred_RF)
mae_RF = mean_absolute_error(y_test, y_pred_RF)
print('Mean squared error using Random Forest: ', mse_RF)
print('Mean absolute error Using Random Forest: ', mae_RF)



############################################
#saving
# savemat(&quot;matlab_matrix_tree.mat&quot;, {&quot;X_test&quot;: X_test ,&quot;Y_test&quot;: rf_model.predict(X_test)})
# #model.save(&quot;model.Random_forest&quot;)
# print(&quot;Saved model to disk&quot;)



savemat(&quot;RF_Optimization_Data_V2_2_4.mat&quot;, {&quot;X&quot;: X , &quot;Y&quot;: y, &quot;Y_predict&quot;: predictions,  &quot;X_test&quot;: X_test ,
                &quot;Y_test&quot;: y_test,&quot;Y_predict_test&quot;: Y_test_pred_RF,&quot;Y_predict_train&quot;: Y_train_pred_RF,
                 &quot;Score&quot;: Score}) 

# rf_model.save('RF_Model.h5')  # always save your weights after training or during training     
                                       
#All Good
print(&quot;FOR RF resuls_Saved \nAll Good&quot;)


# #Feature ranking...
# import pandas as pd
# feature_list = list(X.columns)
# feature_imp = pd.Series(model.feature_importances_, index=feature_list).sort_values(ascending=False)
# print(feature_imp)
#####################################################################################


################################################# CNN MODEL ##########################################



# load datasets

perfect = loadmat(&quot;OptimizationData_V2_4_2&quot;+ &quot;.mat&quot;)[&quot;index_mat&quot;]

noisy_input = loadmat(&quot;OptimizationData_v2_4_2&quot;+ &quot;.mat&quot;)[&quot;R_mat&quot;]

# noisy_input = loadmat(&quot;Noisy_&quot; + channel_model + &quot;_&quot; + &quot;SNR_&quot; + str(SNR) + &quot;.mat&quot;) [channel_model+&quot;_noisy_&quot;+ str(SNR)]
noisy_input=noisy_input.T
perfect=perfect.T
X_r=np.reshape(noisy_input,( 2, 4, 1000000))
noisy_input=X_r.T
a= np.max(perfect)
perfect = perfect / np.max(perfect)
# noisy_input=noisy_input.shape[1]
# y_r=np.reshape(perfect,( 2, 1, 10000))
# perfect=y_r.T
# %
#interp_noisy = numpy.load('drive/codes/my_srcnn/SUI5_12_48_rbf.npy')
#perfect = loadmat('drive/codes/my_srcnn/SUI5_perfect.mat')['SUI5_perfect_H']

#%###### ------ training SRCNN ------ #######
# idx_random = numpy.random.rand(len(perfect_image)) &lt; (1/9)  # uses 32000 from 36000 as training and the rest as validation
# train_data, train_label = interp_noisy[idx_random,:,:,:] , perfect_image[idx_random,:,:,:]
# val_data, val_label = interp_noisy[~idx_random,:,:,:] , perfect_image[~idx_random,:,:,:]
X_train, test_data, y_train, test_label = train_test_split(noisy_input,perfect,
                                                     test_size=0.1,random_state=42)

train_data, val_data, train_label, val_label = train_test_split(X_train,y_train,
                                                     test_size=1/9.,random_state=42)

# %
CNN_train(train_data ,train_label, val_data , val_label )


#%###### ------ prediction using SRCNN ------ #######
cnn_pred_train = CNN_predict(train_data)
cnn_pred_validation = CNN_predict(val_data)
cnn_pred_test = CNN_predict(test_data)
cnn_predictions= CNN_predict(noisy_input)
##save

savemat(&quot;CNN_from_python_Optimization_DataV2.mat&quot;, {&quot;train_label&quot;: train_label , &quot;cnn_pred_train&quot;: cnn_pred_train,
                &quot;val_label&quot;: val_label,&quot;cnn_pred_validation&quot;: cnn_pred_validation,&quot;Y_predict&quot;: cnn_predictions,&quot;test_label&quot;: test_label,
                 &quot;cnn_pred_test&quot;: cnn_pred_test})


# s = numpy.reshape(srcnn_pred_train,(800000, 4,2))
mse_train = (np.square(train_label - cnn_pred_train)).mean()
mse_val = (np.square(val_label - cnn_pred_validation)).mean()
mse_test = (np.square(test_label - cnn_pred_test)).mean()
# CNN_model.save('CNN_Model.h5')
print(f'Mean Squared Error for Train Data(CNN)      : {round(mse_train,4)*100} %')
print(f'Mean Squared Error for Validation Data(CNN) : {round(mse_val,4)*100} %')
print(f'Mean Squared Error for Test Data(CNN)       : {round(mse_test,4)*100} %')
</code></pre>
<p>and my models for CNN is:</p>
<pre><code>from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
import tensorflow as tf
def CNN_model():
  
    model = tf.keras.Sequential()
    model._estimator_type='regressor'


    model.add(
    tf.keras.layers.Conv2D(256, kernel_size=(2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape=(4, 2, 1), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2D(256, kernel_size=(2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())

    # model.add(tf.keras.layers.MaxPooling2D((2, 2)))

    model.add(tf.keras.layers.Conv2D(128, (2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2D(64, (2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())
    
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Flatten())    
    model.add(tf.keras.layers.Dense(2, activation='softmax', name=&quot;output&quot;))
    # adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
    model.compile(loss='mean_squared_error',
              optimizer=&quot;adam&quot;,
              metrics=['mse'])
   
    return model

def CNN_train(train_data ,train_label, val_data , val_label  ):
    cnn_model = CNN_model()
    print(cnn_model.summary())
    early_stop = EarlyStopping(monitor='val_loss', patience=7, verbose=1)
    #This callback will stop the training when there is no improvement in
    # the validation loss for three consecutive epochs.

    #CSVLogger logs epoch, acc, loss, val_acc, val_loss
    log_csv = CSVLogger('my_fnn_logs.csv', separator=',', append=False)

    checkpoint = ModelCheckpoint(&quot;CNN_check.h5&quot;, monitor='val_loss', verbose=1, save_best_only=True,
                                 save_weights_only=False, mode='min')
    callbacks_list = [checkpoint, early_stop, log_csv]

    cnn_model.fit(train_data, train_label, batch_size=1024, validation_data=(val_data, val_label),
                    callbacks=callbacks_list, shuffle=True, epochs= 1 , verbose=1)

def CNN_predict(input_data ):
    cnn_model = CNN_model()
    predicted  = cnn_model.predict(input_data)
    return predicted
</code></pre>
<p>my stacking code is:</p>
<pre><code>    from sklearn.linear_model import LinearRegression
from scikeras.wrappers import KerasRegressor

estimator_list = [
                    ('rf',rf_model),
                    ('fnn',fnn_model),
                    ('cnn',CNN_model)]
# # Building Stack Model
stack_model = StackingRegressor( estimators = estimator_list, final_estimator =LinearRegression())
   
# Training Stack Model
# flat_array_y_train= y_train.flatten()
stack_model.fit(X_train, y_train)
y_train_pred = stack_model.predict(X_train)
y_test_pred = stack_model.predict(X_test)
</code></pre>
<p>they work perfectly but after I run the Stacking code I get this error:</p>
<p>&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;</p>
<pre><code>from sklearn.linear_model import LinearRegression
from scikeras.wrappers import KerasRegressor

estimator_list = [
                    ('rf',rf_model),
                    ('fnn',fnn_model),
                    ('cnn',CNN_model)]


# # Building Stack Model
stack_model = StackingRegressor( estimators = estimator_list, final_estimator =LinearRegression())



# Training Stack Model
# flat_array_y_train= y_train.flatten()
stack_model.fit(X_train, y_train)
y_train_pred = stack_model.predict(X_train)
y_test_pred = stack_model.predict(X_test)
Traceback (most recent call last):

  File &quot;C:\Users\Vali\AppData\Local\Temp/ipykernel_19716/2796820606.py&quot;, line 17, in &lt;module&gt;
    stack_model.fit(X_train, y_train)

  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\_stacking.py&quot;, line 758, in fit
    y = column_or_1d(y, warn=True)

  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py&quot;, line 1038, in column_or_1d
    raise ValueError(

ValueError: y should be a 1d array, got an array of shape (900000, 2) instead.
</code></pre>
<p>I solve it by using &quot;<strong>y_train= y_train.flatten()</strong>&quot;
after than I get this error:</p>
<pre><code>from sklearn.linear_model import LinearRegression
from scikeras.wrappers import KerasRegressor

estimator_list = [
                    ('rf',rf_model),
                    ('fnn',fnn_model),
                    ('cnn',CNN_model)]
# # Building Stack Model
stack_model = StackingRegressor( estimators = estimator_list, final_estimator =LinearRegression())
stack_model._estimator_type='regressor'

# Training Stack Model
y_train= y_train.flatten()
stack_model.fit(X_train, y_train)
y_train_pred = stack_model.predict(X_train)
y_test_pred = stack_model.predict(X_test)
Traceback (most recent call last):

  File &quot;C:\Users\Vali\AppData\Local\Temp/ipykernel_19716/1358099195.py&quot;, line 14, in &lt;module&gt;
    stack_model.fit(X_train, y_train)

  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\_stacking.py&quot;, line 759, in fit
    return super().fit(X, y, sample_weight)

  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\_stacking.py&quot;, line 150, in fit
    names, all_estimators = self._validate_estimators()

  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\ensemble\_base.py&quot;, line 262, in _validate_estimators
    raise ValueError(

ValueError: The estimator function should be a regressor.
</code></pre>
<p>I found some posts where other users had this issue, but they were able to fix it using the <strong>stack_model._estimator_type</strong> = &quot;<strong>regressor</strong>&quot; line. Unfortunately, that isn't solving the issue for me. I'm really new to all of this, so any advice is appreciated. :)</p>
",23,0,-1,4,python;machine-learning;regression;ensemble-learning,2022-06-25 23:03:49,2022-06-25 23:03:49,2022-06-25 23:05:40,and my models for cnn is  my stacking code is  they work perfectly but after i run the stacking code i get this error                                                        i found some posts where other users had this issue  but they were able to fix it using the stack_model _estimator_type    regressor  line  unfortunately  that isn t solving the issue for me  i m really new to all of this  so any advice is appreciated    
58,58,19414683,72754314,No Qt5 found when using CMake to build mantaflow,"<p>I'm a Masters student studying machine learning, I'm trying to build a mantaflow binary from source to make datasets for training fluid simulation models. I have effectively 0 experience with building from source or using C++ and I'm struggling to get this to work.</p>
<p>The error I'm getting from cmake when trying to build is this
<a href=""https://i.stack.imgur.com/mtUQH.png"" rel=""nofollow noreferrer"">error output</a></p>
<p>I Have edited CMakeLists to include &quot;set (CMAKE_PREFIX_PATH &quot;C:/Qt/5.15.2/msvc2019_64&quot;)&quot;
I have also tried adding this to the Qt5Core_DIR cache in the CMake GUI and from running through terminal.</p>
<p>I've browsed stackoverflow for other solutions but cant seem to get anything to work. Any help would be greatly appreciated.</p>
",31,0,0,3,cmake;qt5;fluid-dynamics,2022-06-25 19:07:28,2022-06-25 19:07:28,2022-06-25 19:24:11,i m a masters student studying machine learning  i m trying to build a mantaflow binary from source to make datasets for training fluid simulation models  i have effectively  experience with building from source or using c   and i m struggling to get this to work  i ve browsed stackoverflow for other solutions but cant seem to get anything to work  any help would be greatly appreciated 
59,59,2485799,72753242,Is there a way to speed up this pandas function that extracts from a list by its index position?,"<p>I'm using some machine learning from the <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">SBERT</a> python module to calculate the top K most common strings given an input coprus and a target corpus (in this case 100K vs 100K in size).</p>
<p>The module is pretty robust and gets the comparison done pretty fast,returning me a list of dictionaries containing the top-K most similar comparisons for each input string in the format:</p>
<p><code>{Corpus ID : Similarity_Score}</code></p>
<p>I can then wrap it up in a dataframe with the query string list used as an index. Getting me a dataframe in the format:</p>
<p><code>Query_String | Corpus_ID | Similarity_Score</code></p>
<p>The main time-sink with my approach however is matching up the Corpus ID with the string in the Corpus so I know what string the input is matched against. My current solution is using a combination of pandas <code>apply</code> with the pandarallel module:</p>
<pre><code>def retrieve_match_text(row, corpus_list):

    dict_obj = row['dictionary']

    corpus_id = dict_obj['corpus_id'] #corpus ID is an integer representing the index of a list
    score = dict_obj['score']

    matched_corpus_keyword = corpus_list[corpus_id] #list index lookup (speed this up)

    return [matched_corpus_keyword, score]

.....
.....

# expand the dictionary into two columns and match the corpus KW to its ID
output_df[['Matched Corpus KW', 'Score']] = output_df.parallel_apply(
                            lambda x: pd.Series(retrieve_match_text(x, sentence_list_2)), axis=1)

</code></pre>
<p>This takes around 2 minutes to do for an input corpus of 100K against another corpus of 100K in size. However I'm dealing with a corpus in the size of several million so any further increase in speed here is welcomed.</p>
",39,1,1,5,python;pandas;list;semantics;pandarallel,2022-06-25 16:20:30,2022-06-25 16:20:30,2022-06-25 18:19:14,i m using some machine learning from the  python module to calculate the top k most common strings given an input coprus and a target corpus  in this case k vs k in size   the module is pretty robust and gets the comparison done pretty fast returning me a list of dictionaries containing the top k most similar comparisons for each input string in the format   corpus id   similarity_score  i can then wrap it up in a dataframe with the query string list used as an index  getting me a dataframe in the format  query_string   corpus_id   similarity_score the main time sink with my approach however is matching up the corpus id with the string in the corpus so i know what string the input is matched against  my current solution is using a combination of pandas apply with the pandarallel module  this takes around  minutes to do for an input corpus of k against another corpus of k in size  however i m dealing with a corpus in the size of several million so any further increase in speed here is welcomed 
60,60,18316181,72753718,splitting data for machine learning,"<p>I have a dataframe that includes texts and i want to split the data according to the &quot;writer&quot; column for the machine learning process. For example, I want to train with texts from Aeschylus and Sophocles and test with the texts from Euripides. How can I do that? I am using sklearn.</p>
",35,2,-1,4,python;dataframe;machine-learning;scikit-learn,2022-06-25 17:38:04,2022-06-25 17:38:04,2022-06-25 18:13:06,i have a dataframe that includes texts and i want to split the data according to the  writer  column for the machine learning process  for example  i want to train with texts from aeschylus and sophocles and test with the texts from euripides  how can i do that  i am using sklearn 
61,61,18987667,72732583,Machine Learning implimentation on image classification,"<p>I would like to develop a program in order to measure the length of the coating . by importing image of samples. like one to apply a alogorithm to handel the length measurement of the sample coating.
first a imagew need to be imported then we can use a algo to find the height</p>
<p>can I atleast get some basic ideas about it how to proceed?</p>
",19,0,-1,3,python;machine-learning;jupyter-notebook,2022-06-23 20:44:34,2022-06-23 20:44:34,2022-06-25 17:54:24,can i atleast get some basic ideas about it how to proceed 
62,62,16370432,72751585,How do I obtain the data to train my ML model?,"<p>I am building a machine learning model that would suggest attractions in a specific location.</p>
<p>I have most of the details worked out. However, I still need to collect the data of the attractions to train my model.</p>
<p>Is there somewhere I could find a dataset for this (I already checked Kaggle)? If not which websites should I scrape?</p>
",33,1,-1,3,machine-learning;web-scraping;data-collection,2022-06-25 11:26:45,2022-06-25 11:26:45,2022-06-25 16:37:29,i am building a machine learning model that would suggest attractions in a specific location  i have most of the details worked out  however  i still need to collect the data of the attractions to train my model  is there somewhere i could find a dataset for this  i already checked kaggle   if not which websites should i scrape 
63,63,19409126,72746323,Temperature Probability Distribution using Softmax,"<p>I am fairly new to Machine Learning and am now trying my hand at few algorithms.</p>
<p>My problem is formulated in the following manner:</p>
<ol>
<li>Dataset (15 min spaced readings of temperature for a year) : two columns - datetime , temperature</li>
<li>I have converted the datetime to cyclic features - day , month, hour</li>
<li>Temperature - around 47 unique readings (eg: -5, 10, 21)</li>
</ol>
<p>Problem Statement: I want to build a model to find the probability distribution (using softmax) of temperature 'x' at a particular time</p>
<p>My procedure:</p>
<ol>
<li>I have selected the day, month, hour as X and temperature as Y.</li>
<li>Applied scaling to X</li>
<li>One hot encoded Y (to make it a multiclass problem)</li>
<li>Built the model :</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>model = models.Sequential()
model.add(layers.Dense(32, activation=tf.nn.relu, input_shape=(X_train.shape[1],)))
model.add(layers.Dense(16,activation=tf.nn.relu,))
model.add(layers.Dense(8,activation=tf.nn.relu,))

model.add(layers.Dense(47, activation='softmax'))


model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
</code></pre>
<p>I am getting a accuracy of 0.20 for this model. I am sure that I am going wrong somewhere in the interpretation of my problem.</p>
<p>Any hints on how to approach this correctly would be greatly appreciated!</p>
<p>Thankyou</p>
",23,0,-1,4,python;keras;multiclass-classification;softmax,2022-06-24 21:01:04,2022-06-24 21:01:04,2022-06-25 15:03:42,i am fairly new to machine learning and am now trying my hand at few algorithms  my problem is formulated in the following manner  problem statement  i want to build a model to find the probability distribution  using softmax  of temperature  x  at a particular time my procedure  i am getting a accuracy of   for this model  i am sure that i am going wrong somewhere in the interpretation of my problem  any hints on how to approach this correctly would be greatly appreciated  thankyou
64,64,0,72752283,Jupyter Notebook Code Not Working in Text Editor (MacOS),"<p>Recently, I wrote some code for machine learning (NLP) on Jupyter Notebook. Now, I am trying to incorporate this code into VSCode, so I can run it in a browser using PyScript. However, the terminal says</p>
<pre><code>File &quot;/Users/eduardoculloch/Desktop/HTML/test6.py&quot;, line 1, in &lt;module&gt;
import pandas as pd
ModuleNotFoundError: No module named 'pandas'
</code></pre>
<p>My code is:</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
import random
from sklearn.metrics import accuracy_score

df=pd.read_csv('Book8.csv', names=['Phrase', 'Anger'])

df_x = df['Phrase']

df_y = df['Anger']

x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=4)

tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')
x_traincv = tfidfvectorizer.fit_transform(x_train.astype('U'))

a = x_traincv.toarray()

x_testcv=tfidfvectorizer.transform(x_test)

x_testcv = x_testcv.toarray()

mnb = MultinomialNB()

b=np.array(y_test)
error_score = 0
b=np.array(y_test)
mnb.fit(x_traincv,y_train)
for i in range(len(x_test)):
    testmessage=x_test.iloc[i]
    predictions = mnb.predict(x_testcv[i].reshape(1,-1))
    error_score = error_score + (predictions-int(b[i]))**2
    print(testmessage + ': ')
    print(predictions)
print(error_score/len(x_test))
</code></pre>
<p>Why is this happening? How can I fix this?</p>
",13,0,0,2,python;jupyter-notebook,2022-06-25 13:43:35,2022-06-25 13:43:35,2022-06-25 13:43:35,recently  i wrote some code for machine learning  nlp  on jupyter notebook  now  i am trying to incorporate this code into vscode  so i can run it in a browser using pyscript  however  the terminal says my code is  why is this happening  how can i fix this 
65,65,13611138,72751664,Client sent an HTTP request to an HTTPS server Docker Django Nginx,"<p>I have recently started learning docker and I created a django app which I went ahead and dockerized. I am however experiencing an error I can't get past. The error message is
<code> Client sent an HTTP request to an HTTPS server.</code></p>
<p>Here is my nginx config file</p>
<pre><code>server {

    listen 80;
    listen [::]:80;

    server_name 192.168.99.106;
    charset utf-8;

    location /static {
        alias /usr/src/app/static;
    }

    location / {
        proxy_pass http://web:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

}
</code></pre>
<p>And here is my docker-compose.yml</p>
<pre><code>version: '3'

services:
  web:
    restart: always
    build: ./web
    expose:
      - &quot;8000&quot;
    links:
      - postgres:postgres
      - redis:redis
    volumes:
      - web-django:/usr/src/app
      - web-static:/usr/src/app/static
    env_file: .env
    environment:
      DEBUG: 'true'
    command: sh -c &quot;python manage.py makemigrations &amp;&amp;
                    python manage.py migrate &amp;&amp;
                    usr/local/bin/gunicorn inventory.wsgi:application -w 2 -b :8000&quot; 

  nginx:
    restart: always
    build: ./nginx/
    ports:
      - &quot;80:80&quot;
    volumes:
      - web-static:/www/static
      - ./certbot/www:/var/www/certbot/:ro
      - ./certbot/conf/:/etc/nginx/ssl/:ro
    links:
      - web:web

  certbot:
    image: certbot/certbot:latest
    volumes:
      - ./certbot/www/:/var/www/certbot/:rw
      - ./certbot/conf/:/etc/letsencrypt/:rw
    command: certonly --webroot -w /var/www/certbot --force-renewal --email example@gmail.com -d 192.168.99.106 --agree-tos

  postgres:
    restart: always
    image: postgres:latest
    ports:
      - &quot;5432:5432&quot;
    volumes:
      - pgdata:/var/lib/postgresql/data/
    environment:
      POSTGRES_DB: &quot;db&quot;
      POSTGRES_HOST_AUTH_METHOD: &quot;trust&quot;
      POSTGRES_PASSWORD: ${DB_PASS}

  redis:
    restart: always
    image: redis:latest
    ports:
      - &quot;6379:6379&quot;
    volumes:
      - redisdata:/data
volumes:
  web-django:
  web-static:
  pgdata:
  redisdata:
</code></pre>
<p>The docker machine I am using is running on port <code>tcp://192.168.99.106:2376</code></p>
<p>The first thing i tried was to use https:// but that just brings an issue of not accepting self-signed certificates. I have tried using certbot to install the LetsEncrypt certificates but this is not possible on bare IP address.</p>
",25,0,0,3,django;docker;nginx,2022-06-25 11:45:34,2022-06-25 11:45:34,2022-06-25 11:45:34,here is my nginx config file and here is my docker compose yml the docker machine i am using is running on port tcp        the first thing i tried was to use https    but that just brings an issue of not accepting self signed certificates  i have tried using certbot to install the letsencrypt certificates but this is not possible on bare ip address 
66,66,17023118,72750518,How to retrain the model or create new dataset based on the recognized content?,"<p>I am new to machine learning and would like to know one thing. I labeled and trained my small dataset on <code>YOLOv5</code> and it recognizes quite well, 0.4-0.7 confidence. There are only 9 classes in my dataset, so even 0.15 confidence shows correct recognition.</p>
<p>I would like to retrain the model on these new data. So that when recognition is working, a new dataset is created in parallel, and then I add this data to additional training. It is desirable that I can check it and clean it where necessary. I tried to search in Google, but I did not find any information, the terms are too complicated.</p>
<p>I tried to google information about recognition that learns in real time, but as logically I understand it, this is not yet possible. Therefore, I decided that creating a real-time dataset and then retraining separately it is a great idea. But I dont know how. I'm using PyTorch CUDA but i'm free in this case.</p>
<p>Maybe there are other options to do what I want?</p>
",19,0,-1,5,python;machine-learning;pytorch;dataset;yolo,2022-06-25 06:03:58,2022-06-25 06:03:58,2022-06-25 06:03:58,i am new to machine learning and would like to know one thing  i labeled and trained my small dataset on yolov and it recognizes quite well      confidence  there are only  classes in my dataset  so even   confidence shows correct recognition  i would like to retrain the model on these new data  so that when recognition is working  a new dataset is created in parallel  and then i add this data to additional training  it is desirable that i can check it and clean it where necessary  i tried to search in google  but i did not find any information  the terms are too complicated  i tried to google information about recognition that learns in real time  but as logically i understand it  this is not yet possible  therefore  i decided that creating a real time dataset and then retraining separately it is a great idea  but i dont know how  i m using pytorch cuda but i m free in this case  maybe there are other options to do what i want 
67,67,13957125,72750335,"classification of ft4, ft8 and psk31 digital modes","<p>I want to classify ft4, ft8 and psk31 ham modes using machine learning. I have audio files (.wav) of these signals. I will convert them to spectrogram first. To classify them using ML, I have to extract important features from them. <strong>Now my question is what are the most important features of ft4, ft8 and psk31 and how to extract them in Matlab or python?</strong></p>
",5,0,-1,4,python;matlab;machine-learning;feature-extraction,2022-06-25 05:18:06,2022-06-25 05:18:06,2022-06-25 05:18:06,i want to classify ft  ft and psk ham modes using machine learning  i have audio files   wav  of these signals  i will convert them to spectrogram first  to classify them using ml  i have to extract important features from them  now my question is what are the most important features of ft  ft and psk and how to extract them in matlab or python 
68,68,4621141,72728073,Can Podman change host environments per container or does it behave exactly like Docker?,"<p>I'm learning Podman so I apologize for silly mistakes.</p>
<p>Docker Redis has a <a href=""https://github.com/docker-library/redis/issues/165"" rel=""nofollow noreferrer"">notorious problem</a> where the database might fail to write if in the container <code>/proc/sys/vm/overcommit_memory</code> is not set to 1 (the default value is 0).</p>
<p>I know that Podman doesn't use a Daemon and I thought that this could allow it to set specific host environmental values per container (Docker doesn't allow this -- one must change the host variables and then all containers being created will copy them; thus, if you set a different value for the host environmental variables it will be applied to <em>all</em> subsequent containers, there's no way to apply it only to a specific one).</p>
<p>The <a href=""https://docs.podman.io/en/latest/markdown/podman-run.1.html"" rel=""nofollow noreferrer"">documentation</a> says that, for <code>--env</code>, &quot;--env: Any environment variables specified will override previous settings&quot;. But alas, I think it behaves identically to Docker and doesn't allow one to change host env per container. I tried <code>podman run .... --env 'overcommit_memory=1'</code> and it made no difference at all. I guess this approach would only make sense for general environmental variables and not for specific <code>vm</code> ones.</p>
<p>But I was curious: <strong>is it possible at all to change host env per container in Podman?</strong> And in specific, is there any way to change <code>/proc/sys/vm/overcommit_memory</code> per container?</p>
<hr />
<p>EDIT: can <code>podman play kube</code> be of any help?</p>
<p>EDIT2: one might wonder why I don't encapsulate the <code>podman run</code> command with <code>echo 1 &gt; overcommit_memory</code> and afterwards revert to <code>echo 0 &gt; overcommit_memory</code>, but I need to use a Windows machine to develop this and I think this wouldn't be possible</p>
<p>EDIT 3: Eureka! <del>Found a solution</del> [<strong>not really, see criztovyl's answer</strong>] to my original problem, I just need create a dir (say, <code>mkdir vm</code>), add a <code>overcommit_memory</code> file to it with content equal to <code>1</code>, and add to the <code>podman run</code> instruction <code>-v vm:/proc/sys/vm:rw</code>. This way a volume is bound to the container in <code>rw</code> mode and rewrites the value of <code>overcommit_memory</code>. But I'm still curious as to whether there's a more straightforward way to change that env</p>
<p>EDIT 4: Actually, <code>COPY init.sh</code> is the best option so far <a href=""https://r-future.github.io/post/how-to-fix-redis-warnings-with-docker/"" rel=""nofollow noreferrer"">https://r-future.github.io/post/how-to-fix-redis-warnings-with-docker/</a> [<strong>again, not really, see criztovyl's answer below</strong>]</p>
",46,1,0,2,environment-variables;podman,2022-06-23 15:17:47,2022-06-23 15:17:47,2022-06-25 04:11:04,i m learning podman so i apologize for silly mistakes  docker redis has a  where the database might fail to write if in the container  proc sys vm overcommit_memory is not set to   the default value is    i know that podman doesn t use a daemon and i thought that this could allow it to set specific host environmental values per container  docker doesn t allow this    one must change the host variables and then all containers being created will copy them  thus  if you set a different value for the host environmental variables it will be applied to all subsequent containers  there s no way to apply it only to a specific one   the  says that  for   env     env  any environment variables specified will override previous settings   but alas  i think it behaves identically to docker and doesn t allow one to change host env per container  i tried podman run        env  overcommit_memory   and it made no difference at all  i guess this approach would only make sense for general environmental variables and not for specific vm ones  but i was curious  is it possible at all to change host env per container in podman  and in specific  is there any way to change  proc sys vm overcommit_memory per container  edit  can podman play kube be of any help  edit  one might wonder why i don t encapsulate the podman run command with echo   gt  overcommit_memory and afterwards revert to echo   gt  overcommit_memory  but i need to use a windows machine to develop this and i think this wouldn t be possible edit   eureka  found a solution  not really  see criztovyl s answer  to my original problem  i just need create a dir  say  mkdir vm   add a overcommit_memory file to it with content equal to   and add to the podman run instruction  v vm  proc sys vm rw  this way a volume is bound to the container in rw mode and rewrites the value of overcommit_memory  but i m still curious as to whether there s a more straightforward way to change that env edit   actually  copy init sh is the best option so far   again  not really  see criztovyl s answer below 
69,69,14586818,72749317,Flask API error deploy on Heroku: The browser (or proxy) sent a request that this server could not understand,"<p>Im trying to deploy a Machine Learning model on Heroku to access from an external JS web page. The web page will sent features data and receive back predictions from the model. Despite of the code being running ok in localhost, Im not finding the error when deploying it to heroku.</p>
<blockquote>
<p>My app.py code:</p>
</blockquote>
<pre><code>from flask import Flask, render_template, request
from flask_restful import reqparse, abort, Api, Resource
import joblib
import numpy as np
from sklearn.linear_model import LogisticRegression

# instantiating Flask RESTFul API
app = Flask(__name__)
api = Api(app)


# loading model
model = joblib.load('model.pkl')

# argument parsing
try:
    # features to serve the model
    parser = reqparse.RequestParser()
    parser.add_argument('idade', type=float)
    parser.add_argument('esv', type = float, action='append')
    parser.add_argument('essv', type = float, action='append')
    parser.add_argument('imve', type = float, action='append')
    parser.add_argument('vae', type = float, action='append')
    parser.add_argument('u', type = float, action='append')
    parser.add_argument('creat', type = float, action='append')
    parser.add_argument('k', type = float, action='append')
    parser.add_argument('ct', type = float, action='append')
except:
    print('no request have been made yet')

class PredictProbability(Resource):
    def get(self):
        # use parser and find the user's query
        args = parser.parse_args()
        try:
            idade = float(request.args.get('idade'))
            esv = float(request.args.get('esv'))
            essv = float(request.args.get('essv'))
            imve = float(request.args.get('imve'))
            vae = float(request.args.get('vae'))
            u = float(request.args.get('u'))
            creat = float(request.args.get('creat'))
            k = float(request.args.get('k'))
            ct = float(request.args.get('ct'))
        except:
            pass
        try:
            pred_proba = model.predict_proba(np.array([idade, esv,
                essv, imve, vae, u,
                creat, k, ct]).reshape(-1, 9))[:,1]


            # round the predict proba value and set to new variable
            var = int(pred_proba*100)

            # create JSON object
            output = {'prediction': str(var)+'%'}
            
            return output
        except:
            return 'Request already not made'

# Setup the Api resource routing here
# Route the URL to the resource

api.add_resource(PredictProbability, '/')


if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>Heroku deploy goes without any error, but when I access the model url, get the following error: <code>{&quot;message&quot;: &quot;The browser (or proxy) sent a request that this server could not understand.&quot;}</code>
Anyone could help me to find where is the problem? Thanks!</p>
",18,0,-1,3,python;flask;heroku,2022-06-25 02:26:48,2022-06-25 02:26:48,2022-06-25 03:10:40,im trying to deploy a machine learning model on heroku to access from an external js web page  the web page will sent features data and receive back predictions from the model  despite of the code being running ok in localhost  im not finding the error when deploying it to heroku  my app py code 
70,70,14146866,72747853,"Image segmentatino: Convert GeoJSON to COCO JSON, pycocotools return empty mask","<p>I am labeling my maps (in TIF format) in <a href=""https://groundwork.azavea.com/"" rel=""nofollow noreferrer"">Azavea GroundWork</a>. The files I downloaded are organized in STAC catalog. The labeled segmentation is in .geojson. Labeled segmentations are polygons. The coordinates of the polygons are stored in the .geojson file. I hope to convert this .geojson file into COCOJSON format, so I can use pycocotools from <a href=""https://github.com/cocodataset/cocoapi"" rel=""nofollow noreferrer"">cocoapi</a> to create binary masks. I hope this can make downstream machine learning image segmentation easier in tensorflow or pytorch. So far, I have parsed the .geojson file, extracted the coordinates of the polygons, used Shapely python library to get the exterior coordinates of the polygon, and formatted the image information, categories information, and annotations according to COCOJSON format which is then saved as .json file Then, I used <code>pycocotools.COCO</code> class to read in this file. And to get the annotation as a binary mask, I used <code>.annToMask()</code> method. However, this returns an empty mask. It seems that the coordinates under <code>segmentation</code> in my COCOJSON file are not acceptable. I am not supuersied because mine coordiantes are mostly [-7.1xxxx, 51.xxxxx ...], and the scale is quite small, while I see that in sample COCO json data, the coordiantes under segmentation are positive number and the range is larger relatively. I am really new to GIS, working with tile, GEOJSON, also image segmentation.</p>
<ul>
<li>Could someone explain to me why the segmentation coordinates I am getting negative numbers that span across a much smaller range compared to those in the sample COCO dataset?</li>
<li>And why the <code>.annToMask()</code> return an empty array?</li>
<li>any suggestions on converting the GEOJSON label file into NumPy array for image segmentation machine learning?</li>
</ul>
<p>I have attached the original geojson file, the COCO formatted json file I converted, and finally the code I used to create a binary Numpy array mask using <code>pycocotools</code>. I am only working with one image and one segmentation mask for now. Appreciation in advance, and please comment if my descriptions are not clear.</p>
<p>pastbinlink to my original .geojson file: <a href=""https://pastebin.com/hgHDTaRY"" rel=""nofollow noreferrer"">https://pastebin.com/hgHDTaRY</a></p>
<p>pastbinlink to COCO formatted json I extracted from my original .geojson file: <a href=""https://pastebin.com/4Kc0MYBf"" rel=""nofollow noreferrer"">https://pastebin.com/4Kc0MYBf</a></p>
<p>The code I used to show polygon segmentation and attempt to create a binary mark with <code>pycocotools</code>:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
from pycocotools.coco import COCO
from shapely.geometry import Polygon

# define coco.json file path
root_path = os.path.abspath(os.path.join(os.getcwd() ,&quot;../../&quot;))
data_rootpath = root_path + &quot;/Data&quot;
coco_annotation_file_path = data_rootpath +&quot;/convert/kilbixyCOCO.json&quot;


coco_annotation = COCO(annotation_file=coco_annotation_file_path)

# Category IDs.
cat_ids = coco_annotation.getCatIds()
print(f&quot;Number of Unique Categories: {len(cat_ids)}&quot;)
print(&quot;Category IDs:&quot;)
print(cat_ids)  # The IDs are not necessarily consecutive.

# All categories.
cats = coco_annotation.loadCats(cat_ids)
cat_names = [cat[&quot;name&quot;] for cat in cats]
print(&quot;Categories Names:&quot;)
print(cat_names)

# get segmentations of one category
query_id = cat_ids[0]
query_name = cat_names[0]

# Get the ID of all the images containing the object of the category.
img_ids = coco_annotation.getImgIds(catIds=[query_id])
print(f&quot;Number of Images Containing {query_name}: {len(img_ids)}&quot;)

# Pick one image.
img_id = img_ids[0]
img_info = coco_annotation.loadImgs([img_id])[0]
img_file_name = img_info[&quot;file_name&quot;]
print(f&quot;Image ID: {img_id}, File Name: {img_file_name}&quot;)

# Get all the annotations for the specified image.
ann_ids = coco_annotation.getAnnIds(imgIds=[img_id], iscrowd=0)
anns = coco_annotation.loadAnns(ann_ids)

# let's try visualize one
ann = anns[0]
coords = ann['segmentation'][0]

# use Shaplely to confirm the polygon
points = [[coords[i],coords[i+1]] for i in range(0,len(coords),2)]
polygon_mask = Polygon(points)
plt.plot(*polygon_mask.exterior.xy)
plt.show()

# use pycocotools annToMaks to create a binary mask of this image
print(f&quot;Annotations for Image ID {img_id}:&quot;)
mask = coco_annotation.annToMask(ann)
print(mask.shape)
plt.imshow(mask)
</code></pre>
",25,0,0,5,gis;geojson;image-segmentation;shapely;pycocotools,2022-06-24 23:36:10,2022-06-24 23:36:10,2022-06-24 23:38:33,i am labeling my maps  in tif format  in   the files i downloaded are organized in stac catalog  the labeled segmentation is in  geojson  labeled segmentations are polygons  the coordinates of the polygons are stored in the  geojson file  i hope to convert this  geojson file into cocojson format  so i can use pycocotools from  to create binary masks  i hope this can make downstream machine learning image segmentation easier in tensorflow or pytorch  so far  i have parsed the  geojson file  extracted the coordinates of the polygons  used shapely python library to get the exterior coordinates of the polygon  and formatted the image information  categories information  and annotations according to cocojson format which is then saved as  json file then  i used pycocotools coco class to read in this file  and to get the annotation as a binary mask  i used  anntomask   method  however  this returns an empty mask  it seems that the coordinates under segmentation in my cocojson file are not acceptable  i am not supuersied because mine coordiantes are mostly    xxxx   xxxxx       and the scale is quite small  while i see that in sample coco json data  the coordiantes under segmentation are positive number and the range is larger relatively  i am really new to gis  working with tile  geojson  also image segmentation  i have attached the original geojson file  the coco formatted json file i converted  and finally the code i used to create a binary numpy array mask using pycocotools  i am only working with one image and one segmentation mask for now  appreciation in advance  and please comment if my descriptions are not clear  pastbinlink to my original  geojson file   pastbinlink to coco formatted json i extracted from my original  geojson file   the code i used to show polygon segmentation and attempt to create a binary mark with pycocotools 
71,71,3337766,25745215,"Error in sample.int(m, k) : cannot take a sample larger than the population","<p>First, let me say that I'm fairly new to Machine Learning, kmeans, and r, and this project is a means to learn more about this and also to present this data to our CIO so I can use it in the development of a new help desk system.</p>
<p>I have a 60K line text file. The file contains the titles of help desk tickets entered in by teachers over a 3 year period.</p>
<p>I would like create a r program that takes these titles and creates a set of categories. For instance, terms related to printing issues, or a group of terms related to projector bulbs. I have used r to open the text document, clean up the data, remove stop words and other words that I felt were not necessary. I've gotten a list of all the terms with a frequency &gt;= 400 and saved those to a text file.</p>
<p>But now I want to apply (if it can be done or is appropriate) kmeans clustering to that same data set and see if I can come up with categories.</p>
<p>The code below includes code that will write out the list of terms used &gt;= 400. It is at the end, and is commented out.</p>
<pre><code>library(tm) #load text mining library
library(SnowballC)
options(max.print=5.5E5) 
setwd('c:/temp/') #sets R's working directory to near where my files are
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #check what went in
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;)  
myStopwords &lt;- c(stopwords('english'), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))


dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
  m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
  cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}


#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
#      ncolumns = 1,
#      append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)
</code></pre>
<p>When I run this using RStudio, I get:</p>
<pre><code>&gt; results &lt;- kmeans(m_norm,25)
</code></pre>
<blockquote>
<p>Error in sample.int(m, k) : cannot take a sample larger than the population when 'replace = FALSE'</p>
</blockquote>
<p>I'm not really sure what this means, and I haven't found a lot of info about this online. Any thoughts?</p>
<p>TIA</p>
",9028,2,2,5,r;machine-learning;data-mining;k-means;text-mining,2014-09-09 18:25:28,2014-09-09 18:25:28,2022-06-24 23:34:30,first  let me say that i m fairly new to machine learning  kmeans  and r  and this project is a means to learn more about this and also to present this data to our cio so i can use it in the development of a new help desk system  i have a k line text file  the file contains the titles of help desk tickets entered in by teachers over a  year period  i would like create a r program that takes these titles and creates a set of categories  for instance  terms related to printing issues  or a group of terms related to projector bulbs  i have used r to open the text document  clean up the data  remove stop words and other words that i felt were not necessary  i ve gotten a list of all the terms with a frequency  gt    and saved those to a text file  but now i want to apply  if it can be done or is appropriate  kmeans clustering to that same data set and see if i can come up with categories  the code below includes code that will write out the list of terms used  gt     it is at the end  and is commented out  when i run this using rstudio  i get  error in sample int m  k    cannot take a sample larger than the population when  replace   false  i m not really sure what this means  and i haven t found a lot of info about this online  any thoughts  tia
72,72,19400385,72732309,Passing Arguments through PythonScriptStep() in AMLS,"<p>we want to built a pipeline in Azure Machine Learning Studio. Within the pipeline script we want to pass arguments via PythonScriptStep() to the scoring.py file. This is working so far as you can see in the code below.</p>
<pre><code> scoring_step1 = PythonScriptStep(
    name=&quot;Scoring_Step1&quot;,
    source_directory='./source_directory',
    script_name=&quot;scoring1.py&quot;,
    arguments=[
    '--input-data-label', input_data_label,
    '--input-data-folder', input_datasetfiles.as_named_input(input_data_label).as_mount(),
    '--output-data-folder-name', cache_folder_name, 
    '--output-data-folder', cache_data_folder,
    '--message', &quot;This is a test message&quot;
    ],
    outputs=[cache_data_folder],
    compute_target=aml_compute,
    runconfig=pipeline_run_config,
    allow_reuse=False
)
</code></pre>
<p>To call the parameters within the scoring.py file we use the ArgumentParser as you can see below.</p>
<pre><code># Get parameters
parser = argparse.ArgumentParser()

# argument names need to have the same name when you call them somewhere else
parser.add_argument(&quot;--input-data-label&quot;, type=str, dest='input_data_label', default='input_data_label', help='given input data label')
parser.add_argument(&quot;--input-data-folder&quot;, type=str, dest='not_important_when_as_mount', help='arbitrary text')
parser.add_argument('--output-data-folder-name', type=str, dest='output_data_folder_name', default='output_data_folder_name', 
                    help='given output data folder name')
parser.add_argument('--output-data-folder', type=str, dest='output_data_folder', default='output_data_folder', help='Folder for results')
parser.add_argument('--message', type=str, dest='message', default='Hallo Alex', help='Folder for results')

args = parser.parse_args()
input_data_label = args.input_data_label
output_data_folder_name = args.output_data_folder_name
output_data_folder_path = args.output_data_folder # get the path to the datafolder when using &quot;PipelineData&quot;
message = args.message
</code></pre>
<p>This is working alright so far. When we additional passing the &quot;ds_consumption&quot; argument, which we need for further processing, the passing is NOT working anymore. You can see the code below.</p>
<pre><code>
customer_dataset = Dataset.get_by_name(ws, name='Dreg_Test')
pipeline_param = PipelineParameter(name=&quot;pipeline_param&quot;, default_value=customer_dataset)                                                  
ds_consumption = DatasetConsumptionConfig(&quot;dataset&quot;, pipeline_param)

scoring_step2 = PythonScriptStep(
    name=&quot;Scoring_Step2&quot;,
    source_directory='./source_directory',
    script_name=&quot;scoring2.py&quot;,
    arguments=[
    '--ds_consumption', ds_consumption,
    '--input-data-label', input_data_label,
    '--input-data-folder', input_datasetfiles.as_named_input(input_data_label).as_mount(),
    '--output-data-folder-name', cache_folder_name, 
    '--output-data-folder', cache_data_folder,
    '--message', &quot;This is a test message&quot;
    ],
    inputs=[ds_consumption],
    outputs=[cache_data_folder],
    compute_target=aml_compute,
    runconfig=pipeline_run_config,
    allow_reuse=False
)

# Get the experiment run context
run = Run.get_context()
dataset = run.input_datasets[&quot;dataset&quot;]
data = dataset.to_pandas_dataframe()

# End the run
run.complete()
</code></pre>
<p>We getting the following error message when running scoring2.py at the line<code>args = parser.parse_args</code></p>
<p><a href=""https://i.stack.imgur.com/plkzq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/plkzq.png"" alt=""enter image description here"" /></a></p>
<p>Can anybody help us with this specific problem. We don't see why the argument passing is not working anymore when passing the &quot;ds_consumption&quot; object.</p>
<p>Thank you very much for any help!</p>
",48,1,2,3,python;pipeline;azure-machine-learning-studio,2022-06-23 20:26:34,2022-06-23 20:26:34,2022-06-24 18:46:18,we want to built a pipeline in azure machine learning studio  within the pipeline script we want to pass arguments via pythonscriptstep   to the scoring py file  this is working so far as you can see in the code below  to call the parameters within the scoring py file we use the argumentparser as you can see below  this is working alright so far  when we additional passing the  ds_consumption  argument  which we need for further processing  the passing is not working anymore  you can see the code below  we getting the following error message when running scoring py at the lineargs   parser parse_args  can anybody help us with this specific problem  we don t see why the argument passing is not working anymore when passing the  ds_consumption  object  thank you very much for any help 
73,73,19320507,72744124,Efficient way to create a new column with a count of a value in a set of other columns,"<p>The dataset I am using has number of columns which hold criminal offence codes (eg, 90, 120, 10) for prisoners. The columns are  sparsely populated because of the complex survey routing logic used to capture the data. The data needs to be one hot encoded to feed into a machine learning model. Having (number of columns where offenses are held) x (number of offense codes) does one-hot encode the data, but it creates a dataset that is far too sparse.</p>
<p>I therefore want to create one column for each offense code and, for each row in the dataset, populate it with the count of that code across all columns that hold offenses.</p>
<p>I can imagine a way to do this by converting the dataframe to a dictionary, but this seems very slow and bad practice for pandas.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>#dataset is a dataframe
#offense_columns is a list of strings corresponding to column names in the dataset

#create a list of all the codes that appear across all offense columns
all_possible_offense_codes=[]
for colname in all_possible_offense_codes.values():
    for value in dataset[colname].values():
        if value not in all_possible_offense_codes:
            all_possible_offense_codes.append(value)


#create a copy subset of the dataframe with just the offense columns
offense_cols_subset=dataset[offense_columns]

#convert to dictionary- quicker to loop through than df
offense_cols_dict=offense_cols_subset.to_dict(orient='index')

#create an empty dictionary to hold the counts and append back onto the main dataframe
all_offense_counts={}

#look at each row in the dataframe (converted into a dict) one by one
for row,variables in offense_cols_dict.items():
    
    #create a dict with all offense code as key and value as 0 (starting count)
    #considered using get(code,0) rather than prepopulating keys and vals...
    #but think different vals across dicts would create alignment issues...
    #when appending back onto dataset df
    this_row_offense_counts={code:0 for code in all_possible_offense_codes}
    
    #then go through each offense column
    for column in offense_columns:
        #find the code stored in this column for this row
        code=offense_cols_dict[row,column]
        #increment count by 1
        this_row_offense_counts[code]=this_row_offense_offense_counts[code]+1
    
    #once all columns have been counted, store counts in dictionary
    all_offense_counts[row]=this_row_offense_counts

#once all rows have been counted, turn into a dataframe 
offense_counts_cols=pf.DataFrame.from_dict(all_offense_counts,orient=index)
#join to the original dataframe
dataset.join(offense_counts)
#drop the sparsely populated offense_columns
dataset.drop(offense_columns,axis=1)</code></pre>
</div>
</div>
</p>
",29,1,1,3,python;pandas;dataframe,2022-06-24 18:00:29,2022-06-24 18:00:29,2022-06-24 18:22:33,the dataset i am using has number of columns which hold criminal offence codes  eg        for prisoners  the columns are  sparsely populated because of the complex survey routing logic used to capture the data  the data needs to be one hot encoded to feed into a machine learning model  having  number of columns where offenses are held  x  number of offense codes  does one hot encode the data  but it creates a dataset that is far too sparse  i therefore want to create one column for each offense code and  for each row in the dataset  populate it with the count of that code across all columns that hold offenses  i can imagine a way to do this by converting the dataframe to a dictionary  but this seems very slow and bad practice for pandas 
74,74,19407273,72743181,Bart Large MNLI - Get predicted label in a single column,"<p>I'm trying to classify the sentences of a specific column into three labels with <strong>Bart Large MNLI</strong>. The problem is that the output of the model is &quot;sentence + the three labels + the scores for each label. Output example:</p>
<blockquote>
<p>{'sequence': 'Growing special event set production/fabrication company
is seeking a full-time accountant with experience in entertainment
accounting. This position is located in a fast-paced production office
located near downtown Los Angeles.Responsibilities:â€¢ Payroll
management for 12+ employees, including processing new employee
paperwork.', 'labels': ['senior', 'middle', 'junior'], 'scores':
[0.5461998581886292, 0.327671617269516, 0.12612852454185486]}</p>
</blockquote>
<p>What I need is to get a single column with only the label with the highest score, in this case <strong>&quot;senior&quot;</strong>.</p>
<p>Any feedback which can help me to do it? Right now my code looks like:</p>
<pre><code>df_test = df.sample(frac = 0.0025)

classifier = pipeline(&quot;zero-shot-classification&quot;,
model=&quot;facebook/bart-large-mnli&quot;)
sequence_to_classify = df_test[&quot;full_description&quot;]
candidate_labels = ['senior', 'middle', 'junior']

df_test[&quot;seniority_label&quot;] = df_test.apply(lambda x: classifier(x.full_description, candidate_labels, multi_label=True,), axis=1)

df_test.to_csv(&quot;Seniority_Classified_SampleTest.csv&quot;)
</code></pre>
<p>(Using a Sample of the df for testing code).</p>
<p>And the code I've followed comes from this web, where they do receive a column with labels as an output idk how: <a href=""https://practicaldatascience.co.uk/machine-learning/how-to-classify-customer-service-emails-with-bart-mnli"" rel=""nofollow noreferrer"">https://practicaldatascience.co.uk/machine-learning/how-to-classify-customer-service-emails-with-bart-mnli</a></p>
",18,0,0,1,bart,2022-06-24 16:38:45,2022-06-24 16:38:45,2022-06-24 16:38:45,i m trying to classify the sentences of a specific column into three labels with bart large mnli  the problem is that the output of the model is  sentence   the three labels   the scores for each label  output example  what i need is to get a single column with only the label with the highest score  in this case  senior   any feedback which can help me to do it  right now my code looks like   using a sample of the df for testing code   and the code i ve followed comes from this web  where they do receive a column with labels as an output idk how  
75,75,19399426,72730104,Make data breakdowns using machine learning,"<p>i have data with region breakdowns for many years , and for only one year i have a breakdown on both regions and sectors.
Is there a machine learning technique that i can use to make a simulation between the year with (regions / sectors) and make a prediction for the others years.</p>
<p>input for year 2017:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">regions</th>
<th style=""text-align: center;"">sectors</th>
<th style=""text-align: right;"">x</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">One</td>
<td style=""text-align: center;"">sec1</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">two</td>
<td style=""text-align: center;"">sec1</td>
<td style=""text-align: right;"">100</td>
</tr>
<tr>
<td style=""text-align: left;"">three</td>
<td style=""text-align: center;"">sec1</td>
<td style=""text-align: right;"">21</td>
</tr>
<tr>
<td style=""text-align: left;"">One</td>
<td style=""text-align: center;"">sec2</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">two</td>
<td style=""text-align: center;"">sec2</td>
<td style=""text-align: right;"">10</td>
</tr>
<tr>
<td style=""text-align: left;"">three</td>
<td style=""text-align: center;"">sec2</td>
<td style=""text-align: right;"">70</td>
</tr>
</tbody>
</table>
</div>
<p>input for year 2018-2019-2020-2021:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">regions</th>
<th style=""text-align: right;"">x-2018</th>
<th style=""text-align: right;"">x-2019</th>
<th style=""text-align: right;"">x-2020</th>
<th style=""text-align: right;"">x-2021</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">One</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">two</td>
<td style=""text-align: right;"">100</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">three</td>
<td style=""text-align: right;"">21</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
</tbody>
</table>
</div>
<p>the output should be :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">regions</th>
<th style=""text-align: right;"">sectors</th>
<th style=""text-align: right;"">x-2018</th>
<th style=""text-align: right;"">x-2019</th>
<th style=""text-align: right;"">x-2020</th>
<th style=""text-align: right;"">x-2021</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">One</td>
<td style=""text-align: right;"">sec1</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">two</td>
<td style=""text-align: right;"">sec1</td>
<td style=""text-align: right;"">100</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">three</td>
<td style=""text-align: right;"">sec1</td>
<td style=""text-align: right;"">21</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">One</td>
<td style=""text-align: right;"">sec2</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">two</td>
<td style=""text-align: right;"">sec2</td>
<td style=""text-align: right;"">100</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
<tr>
<td style=""text-align: left;"">three</td>
<td style=""text-align: right;"">sec2</td>
<td style=""text-align: right;"">21</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
<td style=""text-align: right;"">52</td>
</tr>
</tbody>
</table>
</div>",28,0,-1,3,machine-learning;data-science;forecasting,2022-06-23 17:53:50,2022-06-23 17:53:50,2022-06-24 16:06:27,input for year   input for year      the output should be  
76,76,0,72730095,Machine Learning Model Only Predicting Mode in Data Set,"<p>I am trying to do sentiment analysis for text. I have 909 phrases commonly used in emails, and I scored them out of ten for how angry they are, when isolated. </p> Now, I upload this .csv file to a Jupyter Notebook, where I import the following modules:</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
</code></pre>
<p>Now, I define both columns as 'phrases' and 'anger':</p>
<pre><code>df=pd.read_csv('Book14.csv', names=['Phrase', 'Anger'])
df_x = df['Phrase']
df_y = df['Anger']
</code></pre>
<p>Subsequently, I split this data such that 20% is used for testing and 80% is used for training:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=4)
</code></pre>
<p>Now, I convert the words in <code>x_train</code> to numerical data using TfidfVectorizer:</p>
<pre><code>tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='en')
x_traincv = tfidfvectorizer.fit_transform(x_train.astype('U'))
</code></pre>
<p>Now, I convert <code>x_traincv</code> to an array:</p>
<pre><code>a = x_traincv.toarray()
</code></pre>
<p>I also convert <code>x_testcv</code> to a numerical array:</p>
<pre><code>x_testcv=tfidfvectorizer.fit_transform(x_test)
x_testcv = x_testcv.toarray()
</code></pre>
<p>Now, I have</p>
<pre><code>mnb = MultinomialNB()
b=np.array(y_test)
error_score = 0
b=np.array(y_test)
for i in range(len(x_test)):
    mnb.fit(x_testcv,y_test)
    testmessage=x_test.iloc[i]
    predictions = mnb.predict(x_testcv[i].reshape(1,-1))
    error_score = error_score + (predictions-int(b[i]))**2
    print(testmessage)
    print(predictions)
print(error_score/len(x_test))
</code></pre>
<p>However, an example of the results I get are:</p>
<blockquote>
<p>Bring it back
[0]
It is greatly appreciatd when
[0]
Apologies in advance
[0]
Can you please
[0]
See you then
[0]
I hope this email finds you well.
[0]
Thanks in advance
[0]
I am sorry to inform
[0]
You’re absolutely right
[0]
I am deeply regretful
[0]
Shoot me through
[0]
I’m looking forward to
[0]
As I already stated
[0]
Hello
[0]
We expect all students
[0]
If it’s not too late
[0]</p>
</blockquote>
<p>and this repeats on a large scale, even for phrases that are obviously very angry. When I removed all data containing a '0' from the .csv file, the now modal value (a 10) is the only prediction for my sentences. </p> Why is this happening? Is it some weird way to minimise error? Are there any inherent flaws in my code? Should I take a different approach?</p>
",41,1,0,5,python;machine-learning;scikit-learn;jupyter-notebook;sentiment-analysis,2022-06-23 17:53:13,2022-06-23 17:53:13,2022-06-24 16:05:54,i am trying to do sentiment analysis for text  i have  phrases commonly used in emails  and i scored them out of ten for how angry they are  when isolated   now  i define both columns as  phrases  and  anger   subsequently  i split this data such that   is used for testing and   is used for training  now  i convert the words in x_train to numerical data using tfidfvectorizer  now  i convert x_traincv to an array  i also convert x_testcv to a numerical array  now  i have however  an example of the results i get are  and this repeats on a large scale  even for phrases that are obviously very angry  when i removed all data containing a    from the  csv file  the now modal value  a   is the only prediction for my sentences  
77,77,19405703,72741184,is a p2p network reinforcement learning possible?,"<p>I want to run reinforcement learning on a p2p network so that that every client does one node of machine learning and each client writes code that belongs to one part of this machine.
Is this possible?</p>
",14,0,-1,2,python;python-3.x,2022-06-24 14:02:08,2022-06-24 14:02:08,2022-06-24 14:05:28,
78,78,19397268,72732185,.reshape function sometimes works and sometimes fails on vectorised data,"<p>So I'm new to the field of machine learning and i'm trying to build a KNN classifier, i noticed this while trying to reshape the array</p>
<p>When i use the <code>.reshape(100,-1)</code> i get the following error:</p>
<p><code>ValueError: Expected a 1D array, got an array with shape (100, 512)</code></p>
<p>Then when i update it to <code>.reshape(100,1,-1)</code> i get no error</p>
<p>And when i update it <em>again</em> to  <code>.reshape(100,-1)</code>, i get no error.</p>
<p>This does not happen everytime i update it or the cell of the python book, but does happen for the majority of the time.</p>
<p>Why does this happen?</p>
<p>Edit: I apologise for not sharing the code snippet, i've added it below</p>
<pre><code>vectorized = tfh(df['url'])

df['vectorized_urls'] = np.array(vectorized).reshape((100,-1))  
</code></pre>
",21,0,0,3,python;numpy;numpy-ndarray,2022-06-23 20:17:25,2022-06-23 20:17:25,2022-06-24 13:10:49,so i m new to the field of machine learning and i m trying to build a knn classifier  i noticed this while trying to reshape the array when i use the  reshape     i get the following error  valueerror  expected a d array  got an array with shape      then when i update it to  reshape      i get no error and when i update it again to   reshape      i get no error  this does not happen everytime i update it or the cell of the python book  but does happen for the majority of the time  why does this happen  edit  i apologise for not sharing the code snippet  i ve added it below
79,79,8240859,72730160,AWS Lambda Docker Custom Python Library (Runtime.ImportModuleError),"<p>I'm trying to deploy a custom machine learning library on Lambda using a <a href=""https://docs.aws.amazon.com/lambda/latest/dg/images-create.html"" rel=""nofollow noreferrer"">Lambda docker image</a>.</p>
<p>The image is looking approximately as follows:</p>
<pre><code>FROM public.ecr.aws/lambda/python:3.9

CMD mkdir -p /workspace
WORKDIR /workspace

# Copy necessary files (e.g. setup.py and library/)
COPY ...

# Install library and requirements
RUN pip3 install . --target &quot;${LAMBDA_TASK_ROOT}&quot;

# Copy lambda scripts
COPY ... &quot;${LAMBDA_TASK_ROOT}/&quot;

# CMD [ my_script.my_handler ]
</code></pre>
<p>Thus, it installs a local python package including dependencies to LAMBDA_TASK_ROOT (/var/task). The CMD (handler) is overriden in AWS, e.g. <code>preprocessing.lambda_handler</code>.</p>
<p>The container works fine for handlers that DO NOT use the custom python library (on AWS and locally). However, trying to use the custom library on AWS, it fails with <code>Runtime.ImportModuleError</code> claiming that <code>&quot;errorMessage&quot;: &quot;Unable to import module 'MY_HANDLER': No module named 'MY_LIBRARY'&quot;</code>.</p>
<p>Everything works when running the container locally with the <a href=""https://docs.aws.amazon.com/lambda/latest/dg/images-test.html"" rel=""nofollow noreferrer"">runtime interface emulator</a>. The file-level permissions should be ok as well.</p>
<p>Is there anything wrong with this approach?</p>
",30,1,0,4,python;amazon-web-services;docker;aws-lambda,2022-06-23 17:57:48,2022-06-23 17:57:48,2022-06-24 13:09:48,i m trying to deploy a custom machine learning library on lambda using a   the image is looking approximately as follows  thus  it installs a local python package including dependencies to lambda_task_root   var task   the cmd  handler  is overriden in aws  e g  preprocessing lambda_handler  the container works fine for handlers that do not use the custom python library  on aws and locally   however  trying to use the custom library on aws  it fails with runtime importmoduleerror claiming that  errormessage    unable to import module  my_handler   no module named  my_library    everything works when running the container locally with the   the file level permissions should be ok as well  is there anything wrong with this approach 
80,80,19345593,72736795,What Machine Learning Model would be best for this data?,"<p>What is the best machine learning model for this kind of dataset?</p>
<p><a href=""https://i.stack.imgur.com/XYN83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XYN83.png"" alt=""Data Set"" /></a></p>
<p>I was thinking a simple linear regression, but it gets to thicc in the middle. Also, I'm new at this, jw if a linear regression model always picks a point on the best fit line? Can you introduce variance easily? like if X=1 and Y=1 on the slopey boi, is the output always (1,1)? Can it choose a random variance between maybe (1,.75) and (1,1.25). I figure maybe the average square distance for variance. Is that part of the whole thing or should it be introduced as a side piece. Thanks!</p>
",34,0,-3,2,python;machine-learning,2022-06-24 03:01:15,2022-06-24 03:01:15,2022-06-24 12:49:46,what is the best machine learning model for this kind of dataset   i was thinking a simple linear regression  but it gets to thicc in the middle  also  i m new at this  jw if a linear regression model always picks a point on the best fit line  can you introduce variance easily  like if x  and y  on the slopey boi  is the output always      can it choose a random variance between maybe      and       i figure maybe the average square distance for variance  is that part of the whole thing or should it be introduced as a side piece  thanks 
81,81,11555860,72739466,Encoder-Decoder Model Not Returning Integers,"<p>I'm currently trying to create a model that translates from English to ASL Gloss. I'm using <a href=""https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"" rel=""nofollow noreferrer"">this blog post</a> as a template on how to write my encoder-decoder model. However, when it comes time to test my model, the model outputs are not integers, but rather, floats that cannot be decoded into words.</p>
<p>I'm new to using encoder-decoder models in machine learning.</p>
<p>Here is my Colab Notebook: <a href=""https://colab.research.google.com/drive/1IgFQi7iNto6HS4Qz2yggOiDvoGy4PhJt?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1IgFQi7iNto6HS4Qz2yggOiDvoGy4PhJt?usp=sharing</a></p>
<p>Here are my training and testing datasets:</p>
<pre><code>trainY = encode_sequences(gloss_tokenizer, gloss_length, train[:, 0])
trainX = encode_sequences(en_tokenizer, en_length, train[:, 1])

# prepare validation data
testY = encode_sequences(gloss_tokenizer, gloss_length, test[:, 0])
testX = encode_sequences(en_tokenizer, en_length, test[:, 1])
</code></pre>
<p>My model:</p>
<pre><code># build NMT model
def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):
      model = Sequential()
      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
      model.add(LSTM(units))
      model.add(RepeatVector(out_timesteps))
      model.add(LSTM(units, return_sequences=True))
      model.add(Dense(out_vocab, activation='softmax'))
      return model

model = define_model(gloss_vocab_size, en_vocab_size, gloss_length, en_length, 512)

rms = optimizers.RMSprop(learning_rate=0.001)
model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')

history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),
                    epochs=5, batch_size=32, validation_split = 0.2,
                    verbose=1)
</code></pre>
<p>Testing the model:</p>
<pre><code>preds = model.predict(testX)
</code></pre>
<p>Output of testing:</p>
<pre><code>array([[[3.49550214e-06, 2.11346801e-03, 6.40318403e-03, ...,
         1.51255328e-08, 1.74671229e-08, 2.40322451e-09],
        [2.09845457e-05, 2.31786608e-03, 5.53839654e-03, ...,
         3.84917413e-08, 1.62990492e-08, 3.94067001e-09],
        [6.46019835e-05, 3.92230712e-02, 8.71841330e-03, ...,
         3.98166229e-08, 3.91140276e-08, 1.53634758e-08]]]]
</code></pre>
<p>I'm not able to convert these encoded values into words again, as <code>gloss_tokenizer.word_index.items()</code> needs integers to output the corresponding words.</p>
<p>Here is my Colab Notebook: <a href=""https://colab.research.google.com/drive/1IgFQi7iNto6HS4Qz2yggOiDvoGy4PhJt?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1IgFQi7iNto6HS4Qz2yggOiDvoGy4PhJt?usp=sharing</a></p>
<p>Where am I going wrong? Is there a way for the model to output the prediction as readable text?</p>
",19,0,0,5,python;tensorflow;keras;neural-network;lstm,2022-06-24 11:11:56,2022-06-24 11:11:56,2022-06-24 11:11:56,i m currently trying to create a model that translates from english to asl gloss  i m using  as a template on how to write my encoder decoder model  however  when it comes time to test my model  the model outputs are not integers  but rather  floats that cannot be decoded into words  i m new to using encoder decoder models in machine learning  here is my colab notebook   here are my training and testing datasets  my model  testing the model  output of testing  i m not able to convert these encoded values into words again  as gloss_tokenizer word_index items   needs integers to output the corresponding words  here is my colab notebook   where am i going wrong  is there a way for the model to output the prediction as readable text 
82,82,13068174,72730725,RAPL on AWS cloud VM? No intel_rapl and related files in directory /sys/class/powercapp,"<p>I want to use the Intel RAPL interface to record the energy consumption on CPUs. I am using two servers: an AWS EC2 instance and a physical machine.
However, I found that there is no <code>powercap</code> directory under the directory <code>/sys/class</code> on the AWS EC2 instance. After I changed the Linux kernel to version 5.13.19 and have the Intel RAPL configured, there is a <code>power cap</code> directory under <code>/sys/class</code>. But there is no <code>intel-rapl</code> under <code>/sys/class/powercap</code>, so I can not use the RAPL interface as well.</p>
<p>This is what I have under <code>/sys/class/powercap</code> on the AWS EC2 instance:</p>
<pre><code>$ ls /sys/class/powercap
dtpm
</code></pre>
<p>This is what I have under <code>/sys/class/powercap</code> on the physical machine, and is also what I expect on the AWS EC2 instance:</p>
<pre><code>$ ls /sys/class/powercap
dtpm  intel-rapl  intel-rapl:0  intel-rapl:0:0  intel-rapl:1  intel-rapl:1:0
</code></pre>
<p>These are the RAPL-related modules on the AWS EC2 instance, which are the same on my physical machine:</p>
<pre><code>$ find /lib/modules/$(uname -r) -name *rapl*
/lib/modules/5.13.19/kernel/drivers/powercap/intel_rapl_common.ko
/lib/modules/5.13.19/kernel/drivers/powercap/intel_rapl_msr.ko
/lib/modules/5.13.19/kernel/drivers/thermal/intel/int340x_thermal/processor_thermal_rapl.ko
/lib/modules/5.13.19/kernel/arch/x86/events/rapl.ko
</code></pre>
<p>I can tell from <code>sudo dmidecode -t4 | grep Socket.Designation: | wc -l</code> that there are 4 CPU sockets on the EC2 instance. The cpu for the EC2 instance is Intel Xeon. RAPL is supported for these cpus. So there should be something more under <code>/sys/class/powercap</code>!</p>
<p>I am using the Deep Learning AMI (Ubuntu 18.04) Version 61.2 for the AWS EC2 instance.</p>
<p>Does anyone know why I cannot use powercap and RAPL on the AWS EC2 instance? Is it because the EC2 instance is a virtual machine and the RAPL interface is not supported by the virtual machine? Or have I missed out any steps? I would really appreciate any help. Thank you!</p>
",26,0,2,5,linux;amazon-ec2;linux-kernel;intel;energy,2022-06-23 18:38:15,2022-06-23 18:38:15,2022-06-24 10:11:08,this is what i have under  sys class powercap on the aws ec instance  this is what i have under  sys class powercap on the physical machine  and is also what i expect on the aws ec instance  these are the rapl related modules on the aws ec instance  which are the same on my physical machine  i can tell from sudo dmidecode  t   grep socket designation    wc  l that there are  cpu sockets on the ec instance  the cpu for the ec instance is intel xeon  rapl is supported for these cpus  so there should be something more under  sys class powercap  i am using the deep learning ami  ubuntu    version   for the aws ec instance  does anyone know why i cannot use powercap and rapl on the aws ec instance  is it because the ec instance is a virtual machine and the rapl interface is not supported by the virtual machine  or have i missed out any steps  i would really appreciate any help  thank you 
83,83,14670755,72695702,Can we detect user&#39;s details from database? Detect in web - camera in real time YOLO machine learning model?,"<p>Can we detect user's details from database? Detect in  web - camera in real time YOLO machine learning model?</p>
<p>I have trained the model using YOLO model by taking a custom dataset,</p>
<p><em><strong>About Model: Model detects which person is wearing a helmet and which one is not</strong></em></p>
<p>And the model detection is working well, but also who is that user, I want his details, which I had saved in the database at the time of registering the user.</p>
<p>but as soon as the person comes in the video stream, I want his all details not only name ,which is saved in the database and who is that person?,its like Face detection</p>
",22,0,-3,4,python;machine-learning;computer-vision;yolov5,2022-06-21 10:52:46,2022-06-21 10:52:46,2022-06-24 09:53:30,can we detect user s details from database  detect in  web   camera in real time yolo machine learning model  i have trained the model using yolo model by taking a custom dataset  about model  model detects which person is wearing a helmet and which one is not and the model detection is working well  but also who is that user  i want his details  which i had saved in the database at the time of registering the user  but as soon as the person comes in the video stream  i want his all details not only name  which is saved in the database and who is that person  its like face detection
84,84,19403938,72738139,Loading CSV file in Jupyter Notebook,"<pre><code>import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
diabetes_dataset = pd.read_csv('//Users/rac07317/Documents/Projects')
</code></pre>
<p>I am currently coding along a machine learning project to understand the work flow. However my CSV file is not showing in it's path as in .csv. I have searched all over but I have not seen a similar issue.</p>
<p>IsADirectoryError: error code I am receiving</p>
<p>Any guidance would be much appreciated ☺️</p>
",39,0,0,4,python;pandas;csv;jupyter-notebook,2022-06-24 06:58:55,2022-06-24 06:58:55,2022-06-24 07:25:43,i am currently coding along a machine learning project to understand the work flow  however my csv file is not showing in it s path as in  csv  i have searched all over but i have not seen a similar issue  isadirectoryerror  error code i am receiving any guidance would be much appreciated   
85,85,5150942,72585198,Can not read csv into Polars dataframe in Rust with LazyCsvReader,"<p>I was trying rust version of polars for the first time. So I have set up a project and added polars into the cargo.toml file the cargo file looks as follows:</p>
<pre class=""lang-ini prettyprint-override""><code>[package]
name = &quot;polar_test&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
polars = &quot;0.22.6&quot;
</code></pre>
<p>Then I have written the following code in my main.rs file. The code is taken directly <a href=""https://www.pola.rs/%5D"" rel=""nofollow noreferrer"">Polars website</a>. But the compiler is complaining about the LazyCsvReader and other types like col, sort etc. It looks like the <code>::prelude::*</code> has no effect. Here is the code for the main.rs file:</p>
<pre class=""lang-rust prettyprint-override""><code>use polars::prelude::*;

fn example() -&gt; Result&lt;DataFrame&gt; {
    LazyCsvReader::new(&quot;wine.data&quot;.into()).collect()
}

fn main() {
    println!(&quot;Hello, world!&quot;);
}

</code></pre>
<p>Here is the error Logs:</p>
<pre><code>datapsycho@dataops:~/.../PolarTest$ cargo build
   Compiling polar_test v0.1.0 (/home/datapsycho/IdeaProjects/PolarTest)
error[E0433]: failed to resolve: use of undeclared type `LazyCsvReader`
 --&gt; src/main.rs:4:5
  |
4 |     LazyCsvReader::new(&quot;foo.csv&quot;.into()).collect()
  |     ^^^^^^^^^^^^^ use of undeclared type `LazyCsvReader`

For more information about this error, try `rustc --explain E0433`.
error: could not compile `polar_test` due to previous error
</code></pre>
<p>My understanding is using <code>prelude::*</code> does not bring the types like col, groupby, LazyCsvReader,  into the scopes. Can someone give me an example how can I read a CSV file  with polars and do some operation. Here is the corresponding python version of the code with pandas looks as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from pathlib import Path
import pandas as pd


def read_data(path: Path) -&gt; pd.DataFrame:
    columns = [
        &quot;Class label&quot;, &quot;Alcohol&quot;,
        &quot;Malic acid&quot;, &quot;Ash&quot;,
        &quot;Alcalinity of ash&quot;, &quot;Magnesium&quot;,
        &quot;Total phenols&quot;, &quot;Flavanoids&quot;,
        &quot;Nonflavanoid phenols&quot;,
        &quot;Proanthocyanins&quot;,
        &quot;Color intensity&quot;, &quot;Hue&quot;,
        &quot;OD280/OD315 of diluted wines&quot;,
        &quot;Proline&quot;
    ]
    _df = pd.read_csv(path, names=columns)
    return _df


def count_classes(df: pd.DataFrame) -&gt; pd.DataFrame:
    _df = df.groupby(&quot;Class label&quot;).agg(total=(&quot;Class label&quot;, &quot;count&quot;)).reset_index()
    _df.to_csv(Path(&quot;datastore&quot;).joinpath(&quot;data_count.csv&quot;), index=False)
    return _df


def main():
    file_path = Path(&quot;datastore&quot;).joinpath(&quot;wine.data&quot;)
    main_df = read_data(file_path)
    class_stat_df = count_classes(main_df)
    print(class_stat_df)


if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
<p>The data can be downloaded from the following command:</p>
<pre class=""lang-bash prettyprint-override""><code>wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data
</code></pre>
<p>Can someone help me, how I can write the same transformation pipeline in Rust with polars. This is the example given in Polars's <a href=""https://www.pola.rs/"" rel=""nofollow noreferrer"">front page</a> which might need some modification:</p>
<pre class=""lang-rust prettyprint-override""><code>use polars::prelude::*;

fn example() -&gt; Result&lt;DataFrame&gt; {
    LazyCsvReader::new(&quot;foo.csv&quot;.into())
        .finish()
        .filter(col(&quot;bar&quot;).gt(lit(100)))
        .groupby(vec![col(&quot;ham&quot;)])
        .agg(vec![col(&quot;spam&quot;).sum(), col(&quot;ham&quot;).sort(false).first()])
        .collect()
}
</code></pre>
",64,2,1,2,rust;rust-polars,2022-06-11 20:01:00,2022-06-11 20:01:00,2022-06-24 05:06:00,i was trying rust version of polars for the first time  so i have set up a project and added polars into the cargo toml file the cargo file looks as follows  then i have written the following code in my main rs file  the code is taken directly   but the compiler is complaining about the lazycsvreader and other types like col  sort etc  it looks like the   prelude    has no effect  here is the code for the main rs file  here is the error logs  my understanding is using prelude    does not bring the types like col  groupby  lazycsvreader   into the scopes  can someone give me an example how can i read a csv file  with polars and do some operation  here is the corresponding python version of the code with pandas looks as follows  the data can be downloaded from the following command  can someone help me  how i can write the same transformation pipeline in rust with polars  this is the example given in polars s  which might need some modification 
86,86,5150942,72586023,How to add Column names in a Polars DataFrame while using CsvReader,"<p>I can read a csv file which does not have column headers in the file. With the following code using polars in rust:</p>
<pre class=""lang-rust prettyprint-override""><code>use polars::prelude::*;

fn read_wine_data() -&gt; Result&lt;DataFrame&gt; {
    let file = &quot;datastore/wine.data&quot;;
    CsvReader::from_path(file)?
        .has_header(false)
        .finish()
}


fn main() {
    let df = read_wine_data();
    match df {
        Ok(content) =&gt; println!(&quot;{:?}&quot;, content.head(Some(10))),
        Err(error) =&gt; panic!(&quot;Problem reading file: {:?}&quot;, error)
    }
}

</code></pre>
<p>But now I want to add column names into the dataframe while reading or after reading, how can I add the columns names. Here is a column name vector:</p>
<pre class=""lang-rust prettyprint-override""><code>let COLUMN_NAMES = vec![
    &quot;Class label&quot;, &quot;Alcohol&quot;,
    &quot;Malic acid&quot;, &quot;Ash&quot;,
    &quot;Alcalinity of ash&quot;, &quot;Magnesium&quot;,
    &quot;Total phenols&quot;, &quot;Flavanoids&quot;,
    &quot;Nonflavanoid phenols&quot;,
    &quot;Proanthocyanins&quot;,
    &quot;Color intensity&quot;, &quot;Hue&quot;,
    &quot;OD280/OD315 of diluted wines&quot;,
    &quot;Proline&quot;
];
</code></pre>
<p>How can I add these names to the dataframe. The data can be downloaded with the following code:</p>
<pre class=""lang-bash prettyprint-override""><code>wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data
</code></pre>
",51,2,1,2,rust;rust-polars,2022-06-11 21:52:50,2022-06-11 21:52:50,2022-06-24 04:17:28,i can read a csv file which does not have column headers in the file  with the following code using polars in rust  but now i want to add column names into the dataframe while reading or after reading  how can i add the columns names  here is a column name vector  how can i add these names to the dataframe  the data can be downloaded with the following code 
87,87,13515511,72703575,Creating a ML Pipeline with AutoML component using Azure ML Python SDK v2,"<p>How would I create a machine learning pipeline with AutoML component using Azure Machine Learning Python SDK v2? I see that there is a way to pass in a custom user script as a component in this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk"" rel=""nofollow noreferrer"">official guide</a>, but I want to pass in Microsoft AutoML as a component instead.</p>
<p>I tried doing something like below:</p>
<pre><code>...

train_component = automl.regression(
    compute=compute_target,
    experiment_name=args.experiment_name,
    training_data=Input(type=&quot;uri_folder&quot;),
    validation_data=Input(type=&quot;uri_folder&quot;),
    target_column_name=args.target,
    primary_metric=&quot;accuracy&quot;,
)

...

@dsl.pipeline(
    compute=args.compute_name,
    description=&quot;AutoML pipeline&quot;,
)
def automl_pipeline(
    pipeline_job_data_input,
    pipeline_job_test_size,
):
    data_prep_job = data_prep_component(
        data=pipeline_job_data_input,
        test_size=pipeline_job_test_size,
    )
    train_job = train_component(
        training_data=data_prep_job.outputs.train_data,
        validation_data=data_prep_job.outputs.test_data,
    )
    return {
        &quot;pipeline_job_train_data&quot;: data_prep_job.outputs.train_data,
        &quot;pipeline_job_test_data&quot;: data_prep_job.outputs.test_data,
        &quot;pipeline_job_model&quot;: train_job.outputs.model,
    }

pipeline = automl_pipeline(
    pipeline_job_data_input=data.name,
    pipeline_job_test_size=0.2,
)
pipeline_job = ml_client.jobs.create_or_update(
    pipeline,
    experiment_name=&quot;test_pipeline&quot;,
)
</code></pre>
<p>But I am getting <code>TypeError: 'RegressionJob' object is not callable</code> error. Is this not implemented yet?</p>
",36,1,0,2,python;azure-machine-learning-service,2022-06-21 21:06:16,2022-06-21 21:06:16,2022-06-24 03:58:01,how would i create a machine learning pipeline with automl component using azure machine learning python sdk v  i see that there is a way to pass in a custom user script as a component in this   but i want to pass in microsoft automl as a component instead  i tried doing something like below  but i am getting typeerror   regressionjob  object is not callable error  is this not implemented yet 
88,88,19403148,72737049,Bad Request The browser (or proxy) sent a request that this server could not understand in flask,"<p>I am trying to develop a data application with flask. After loading the dataset, I want to selectively train the x and y columns of the data myself. But I'm getting this error.
&quot;Bad Request
The browser (or proxy) sent a request that this server could not understand.&quot;
I am using the dataset
<a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/00477/"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/machine-learning-databases/00477/</a> in this link.
I will be grateful if you could help me.</p>
<p>index.hmtl and app.py</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;!-- jQuery library --&gt;
    &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js&quot;&gt;&lt;/script&gt;

    &lt;!-- JS &amp; CSS library of MultiSelect plugin --&gt;
    &lt;script src=&quot;multiselect/jquery.multiselect.js&quot;&gt;&lt;/script&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;multiselect/jquery.multiselect.css&quot;&gt;
    {{data | safe}}
    &lt;style&gt;
        .container{
            padding:20px;
            position: relative;
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;form class=&quot;&quot; action=&quot;data&quot; method=&quot;post&quot;&gt;
    
        &lt;input type=&quot;file&quot; name=&quot;upload_file&quot; value=&quot;&quot;&gt;
        &lt;input type=&quot;submit&quot; name=&quot;upload_file&quot; value=&quot;submit&quot;&gt;
    
    &lt;/form&gt;  
    
    &lt;div class=&quot;container&quot;&gt;
    &lt;select name=&quot;x_columns[]&quot; multiple id=&quot;x&quot;&gt;
    &lt;option value=&quot;X1&quot;&gt;X1&lt;/option&gt;
    &lt;option value=&quot;X2&quot;&gt;X2&lt;/option&gt;
    &lt;option value=&quot;X3&quot;&gt;X3&lt;/option&gt;
    &lt;option value=&quot;X4&quot;&gt;X4&lt;/option&gt;
    &lt;option value=&quot;X5&quot;&gt;X5&lt;/option&gt;
    &lt;option value=&quot;X6&quot;&gt;X6&lt;/option&gt;
    &lt;option value=&quot;Y&quot;&gt;Y&lt;/option&gt;
    &lt;/select&gt;
    &lt;script&gt;
    $('#x_columns[]').multiselect({
        columns: 1,
        placeholder: 'Select X Columns'
    });
&lt;/script&gt;&lt;/div&gt; 

      
&lt;div class=&quot;container&quot;&gt;
    &lt;select name=&quot;y_columns[]&quot; multiple id=&quot;y&quot;&gt;
    &lt;option value=&quot;X1&quot;&gt;X1&lt;/option&gt;
    &lt;option value=&quot;X2&quot;&gt;X2&lt;/option&gt;
    &lt;option value=&quot;X3&quot;&gt;X3&lt;/option&gt;
    &lt;option value=&quot;X4&quot;&gt;X4&lt;/option&gt;
    &lt;option value=&quot;X5&quot;&gt;X5&lt;/option&gt;
    &lt;option value=&quot;X6&quot;&gt;X6&lt;/option&gt;
    &lt;option value=&quot;Y&quot;&gt;Y&lt;/option&gt;
&lt;/select&gt;
&lt;script&gt;
    $('#y_columns[]').multiselect({
        columns: 1,
        placeholder: 'Select Y Columns'
    });
&lt;/script&gt;&lt;/div&gt; 
&lt;p name=&quot;y_columns[]&quot;&gt; dogru&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;



import pandas as pd
from flask import Flask,render_template,request
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from pyGRNN import GRNN
from sklearn.metrics import mean_squared_error as MSE
app = Flask(__name__)

@app.route('/',methods=['GET','POST'])
def index():
    return render_template('index.html')

@app.route('/data',methods=['GET','POST'])
def show_data():
 
    if request.method=='POST':
        file=request.form['upload_file']
        data=pd.read_excel(file)
        #return render_template('index.html',data=data.to_html())

        x_names=[]
        
        y_names=[]
        x_names=request.form['x_columns[]']
        print(x_names)
        y_names=request.form['y_columns[]']
        print(y_names)
        
        X=data[x_names]
        Y=data[y_names]
        x_train, x_test, y_train, y_test = train_test_split(preprocessing.minmax_scale(X),  Y,    test_size=0.2,)
        IGRNN = GRNN(bnds=(0, None), calibration='None', kernel='RBF', method='L-BFGS-B',
                    n_restarts_optimizer=0, n_splits=5, seed=42, sigma=input)  
        IGRNN.fit(x_train, y_train.ravel())
        y_pred = IGRNN.predict(x_test)
        mse_IGRNN = MSE(y_test, y_pred)
        
        return render_template('index.html',mse_IGRNN,data=data.to_html())

if __name__=='__main__':
    app.run(debug=True)
</code></pre>
",29,0,0,5,python;html;flask;machine-learning;scikit-learn,2022-06-24 03:34:30,2022-06-24 03:34:30,2022-06-24 03:37:12,index hmtl and app py
89,89,6565662,50123067,"tensorflow.js model.predict() Prints Tensor [[NaN],]","<p>I am completely new to Machine learning and also to tensorflow.js, I am trying to predict the values of the next set but it is giving me ""NaN"" in result. What am  I doing wrong ? </p>

<p>Following <a href=""https://github.com/tensorflow/tfjs-examples/blob/master/getting_started/index.js"" rel=""nofollow noreferrer"">this Github example</a></p>

<pre><code> async function myFirstTfjs(arr) {
    // Create a simple model.
    const model = tf.sequential();
    model.add(tf.layers.dense({units: 1, inputShape: [2]}));

    // Prepare the model for training: Specify the loss and the optimizer.
    model.compile({
      loss: 'meanSquaredError',
      optimizer: 'sgd'
    });
    const xs = tf.tensor([[1,6],
        [2,0],
        [3,1],
        [4,2],
        [5,3],
        [6,4],
        [7,5],
        [8,6],
        [9,0],
        [10,1],
        [11,2],
        [12,3],
        [13,4],
        [14,5],
        [15,6],
        [16,0],
        [17,1],
        [18,2],
        [19,3],
        [20,4],
        [21,5],
        [22,6],
        [23,0],
        [24,1],
        [25,2],
        [26,3]]);
    const ys = tf.tensor([104780,30280,21605,42415,32710,30385,35230,97795,31985,34570,35180,30095,36175,57300,104140,30735,28715,36035,34515,42355,38355,110080,26745,35315,40365,30655], [26, 1]);
    // Train the model using the data.
    await model.fit(xs, ys, {epochs: 500});
    // Use the model to do inference on a data point the model hasn't seen.
  model.predict(tf.tensor(arr, [1, 2])).print();
  }
  myFirstTfjs([28,5]);
</code></pre>
",4592,2,1,5,javascript;tensorflow;machine-learning;deep-learning;tensorflow.js,2018-05-02 01:34:25,2018-05-02 01:34:25,2022-06-24 02:24:47,i am completely new to machine learning and also to tensorflow js  i am trying to predict the values of the next set but it is giving me nan in result  what am  i doing wrong    following 
90,90,3131685,72715741,Accidently removed files from local folder after running &quot;git checkout origin/main&quot;,"<p>I am new to git.</p>
<p>I accidentally deleted all project files from my local machines after running <code>git checkout origin/main</code> command.</p>
<p>I have checked and could not find the files in the <strong>Recycle bin</strong> as well. How can I recover those lost files?</p>
<p>I was following a script that I have written when I was learning Git and this was the first time that I tried it on an actual project.  Following is the message I receive. Should I run <code>git switch</code>. If yes, how.</p>
<p><a href=""https://i.stack.imgur.com/T6l4N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T6l4N.png"" alt=""Message received after running the checkout command"" /></a></p>
<p>I do not want to do something stupid again and loose whatever I am left with. How can I recover those lost files?</p>
",38,3,2,4,git;delete-file;git-checkout;git-reset,2022-06-22 18:12:35,2022-06-22 18:12:35,2022-06-24 01:39:39,i am new to git  i accidentally deleted all project files from my local machines after running git checkout origin main command  i have checked and could not find the files in the recycle bin as well  how can i recover those lost files  i was following a script that i have written when i was learning git and this was the first time that i tried it on an actual project   following is the message i receive  should i run git switch  if yes  how   i do not want to do something stupid again and loose whatever i am left with  how can i recover those lost files 
91,91,12647630,72733539,Normalize dataset column by column and for severals data groups,"<p>before a regression machine learning, I have to normalize numeric columns by using zscore method (z = (x – u) / s)</p>
<p>My pandas dataframes for machine learning are:</p>
<p>y target with T0 column.</p>
<p>X_train with colums :</p>
<pre><code>T_AROME : numerical
T_ARPEGE : numerical
MonthNumber_1 : categorical (0 or 1)
MonthNumber_2 : categorical (0 or 1)
MonthNumber_3 : categorical (0 or 1)
</code></pre>
<p>I don't want to use the global column mean and the global standard deviation but normalize on severals data groups (grouped by Month).</p>
<p>First normalisation with subgroup <code>X.query('MonthNumber_1 == 1')</code>
Second normalisation with subgroup <code>X.query('MonthNumber_2 == 1')</code>
Third normalisation with subgroup <code>X.query('MonthNumber_3 == 1')</code></p>
<p>I don't know how to process with Sklearn.</p>
<p>For example for the first normalization:</p>
<pre><code>from sklearn.preprocessing import StandardScaler

X_train = X_train.query('MonthNumber_1 == 1').copy()

scaler = StandardScaler()
for column in ['T_AROME', 'T_ARPEGE']:
    scaler.fit(X_train) # Compute the mean and std to be used for later scaling.
    print(scaler.mean_)
    scaler.transform(X_train[column])
</code></pre>
<p>returns an error of shape:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead:
array=[ 6.8449264  3.647341   6.5092516 ...  9.953349  12.180361  12.192245 ].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>Thanks you.</p>
",18,0,0,3,machine-learning;scikit-learn;data-preprocessing,2022-06-23 21:53:44,2022-06-23 21:53:44,2022-06-23 21:53:44,before a regression machine learning  i have to normalize numeric columns by using zscore method  z    x   u    s  my pandas dataframes for machine learning are  y target with t column  x_train with colums   i don t want to use the global column mean and the global standard deviation but normalize on severals data groups  grouped by month   i don t know how to process with sklearn  for example for the first normalization  returns an error of shape  thanks you 
92,92,0,46132581,Incorporating prior knowledge to machine learning models,"<p>Say I have a data set of students with features such as income level, gender, parents' education levels, school, etc. And the target variable is say, passing or failing a national exam. We can train a machine learning model to predict, given these values whether a student is likely to pass or fail (say in sklearn, using predict_prob we can say the probability of passing)</p>

<p>Now say I have a different set of information which has nothing to do with the previous data set, which includes the schools and percentage of students from that particular school who has passed that national exam last year and years before. say, schoolA: 10%, schoolB: 15%, etc.</p>

<p>How can I use this additional knowledge to improve my model. For sure this data is valuable. (Students from certain schools have a higher chance of passing the exam due to their educational facilities, qualified staff, etc.).</p>

<p>Do i some how add this information as a new feature to the data set? If so what is the recommend way. Or do I use this information after the model prediction and somehow combine these to get a final probability ? Obviously an average or a weighted average doesn't work due to the second data set having probabilities in the range below 20% which then drags the total probability very low. How do data scientist usually incorporate this kind of prior knowledge? Thank you</p>
",220,2,0,2,machine-learning;scikit-learn,2017-09-09 21:40:35,2017-09-09 21:40:35,2022-06-23 21:28:01,say i have a data set of students with features such as income level  gender  parents  education levels  school  etc  and the target variable is say  passing or failing a national exam  we can train a machine learning model to predict  given these values whether a student is likely to pass or fail  say in sklearn  using predict_prob we can say the probability of passing  now say i have a different set of information which has nothing to do with the previous data set  which includes the schools and percentage of students from that particular school who has passed that national exam last year and years before  say  schoola     schoolb     etc  how can i use this additional knowledge to improve my model  for sure this data is valuable   students from certain schools have a higher chance of passing the exam due to their educational facilities  qualified staff  etc    do i some how add this information as a new feature to the data set  if so what is the recommend way  or do i use this information after the model prediction and somehow combine these to get a final probability   obviously an average or a weighted average doesn t work due to the second data set having probabilities in the range below   which then drags the total probability very low  how do data scientist usually incorporate this kind of prior knowledge  thank you
93,93,6007993,72732469,Training a model with several large CSV files,"<p>I have a dataset composed of several large csv files. Their total size is larger than the RAM of the machine on which the training is executed.</p>
<p>I need to train an ML model from Scikit-Learn or TF or pyTorch (Think SVR, not deep learning). I need to use the whole dataset which is impossible to load at once. Any recommendation on how to overcome this, please?</p>
",40,1,-2,5,python;tensorflow;machine-learning;scikit-learn;bigdata,2022-06-23 20:37:55,2022-06-23 20:37:55,2022-06-23 21:18:58,i have a dataset composed of several large csv files  their total size is larger than the ram of the machine on which the training is executed  i need to train an ml model from scikit learn or tf or pytorch  think svr  not deep learning   i need to use the whole dataset which is impossible to load at once  any recommendation on how to overcome this  please 
94,94,11153845,72733043,Take text from textbox in realtime from any website as input like grammarly,"<p>I am working on a chrome driver which will take text as input from textbox of any website in real time, once the text is taken as input it will be given that to my machine learning model which will predict the sentiment of that user.</p>
<p>My ML model is ready, now I just want to ask, just like grammarly how to take text as input in real time to give certain output.</p>
<p><a href=""https://i.stack.imgur.com/kDUcz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kDUcz.png"" alt=""enter image description here"" /></a></p>
<p>Few thing I thought:</p>
<p>1.) Scrape text (using selenium python) of any textbox in realtime to take as input.</p>
<p>2.) Using onkeydown (using JS) to take input from it save it some where for some amt of time.. predict the output and clean that temp.</p>
<p>3.) Using onchange (using JS) i guess.</p>
<p>Basically my flow should be:</p>
<ul>
<li>User add my extension on there browser.</li>
<li>Any text they write in any textbox of any website will be taken as input in realtime.</li>
<li>That realtime input text should be given to my extension.</li>
<li>Than a pop-up should show up that it's positive or negative sentiment.</li>
</ul>
",22,0,0,5,javascript;python-3.x;angular;selenium;web-deployment,2022-06-23 21:15:33,2022-06-23 21:15:33,2022-06-23 21:15:33,i am working on a chrome driver which will take text as input from textbox of any website in real time  once the text is taken as input it will be given that to my machine learning model which will predict the sentiment of that user  my ml model is ready  now i just want to ask  just like grammarly how to take text as input in real time to give certain output   few thing i thought     scrape text  using selenium python  of any textbox in realtime to take as input     using onkeydown  using js  to take input from it save it some where for some amt of time   predict the output and clean that temp     using onchange  using js  i guess  basically my flow should be 
95,95,12882606,72619764,Compress Large Data in R into csv without NULLS or LIST,"<p>FIRST TIME POSTING:</p>
<p>I'm preparing data for <code>arules() read.transactions</code> and need to compress unique Invoice data (500k+ cases) so that each unique Invoice and its associated info fits on a single line like this:</p>
<blockquote>
<p>Invoice001,CustomerID,Country,StockCodeXYZ,StockCode123</p>
<p>Invoice002...etc</p>
</blockquote>
<p>However, the data reads in repeating the Invoice for each <code>StockCode</code> like this:</p>
<blockquote>
<p>Invoice001,CustomerID,Country,StockCodeXYZ</p>
<p>Invoice001,CustomerID,Country,StockCode123</p>
<p>Invoice002....etc</p>
</blockquote>
<p>I've been trying <code>pivot_wider()</code> and then <code>unite()</code>, but it generates 285M+ MOSTLY NULL cells into a LIST which I'm having a hard time resolving and unable to write to csv or read into <code>arules</code>.  I've also tried <code>keep(~!is.null(.)), discard(is.null), compact()</code> without success and am open to any method to achieve the desired outcome above.</p>
<p>However, I feel like I should be able to solve it using the built-in <code>arules() read.transactions() fx</code>, but am getting various errors as I try different things there too.</p>
<p>The data is opensource from University of California, Irvin and found here: <a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx</a></p>
<p>Any help would be greatly appreciated.</p>
<pre><code>library(readxl)
url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx&quot;
destfile &lt;- &quot;Online_20Retail.xlsx&quot;
curl::curl_download(url, destfile)
Online_20Retail &lt;- read_excel(destfile)

trans &lt;- read.transactions(????????????)
</code></pre>
",44,0,0,5,dataframe;dplyr;tidyr;arules;readxl,2022-06-14 21:06:58,2022-06-14 21:06:58,2022-06-23 20:49:36,first time posting  i m preparing data for arules   read transactions and need to compress unique invoice data  k  cases  so that each unique invoice and its associated info fits on a single line like this  invoice customerid country stockcodexyz stockcode invoice   etc however  the data reads in repeating the invoice for each stockcode like this  invoice customerid country stockcodexyz invoice customerid country stockcode invoice    etc i ve been trying pivot_wider   and then unite    but it generates m  mostly null cells into a list which i m having a hard time resolving and unable to write to csv or read into arules   i ve also tried keep   is null      discard is null   compact   without success and am open to any method to achieve the desired outcome above  however  i feel like i should be able to solve it using the built in arules   read transactions   fx  but am getting various errors as i try different things there too  the data is opensource from university of california  irvin and found here   any help would be greatly appreciated 
96,96,13459797,72680734,Huggingface Trainer only doing 3 epochs no matter the TrainingArguments,"<p>Im new at machine learning and I'm facing an issue where I want to increase the epochs for training but .train() will only do 3 epochs. What am I doing wrong?</p>
<p><strong>This is my dataset:</strong></p>
<blockquote>
<pre><code>&gt; DatasetDict({ train: Dataset({ features: [‘text’, ‘label’], num_rows:
&gt; 85021 }) test: Dataset({ features: [‘text’, ‘label’], num_rows: 15004
&gt; }) })
</code></pre>
</blockquote>
<p><strong>and its features:</strong></p>
<pre><code>&gt; {‘label’: ClassLabel(num_classes=20, names=[‘01. AGRI’, ‘02. ALIM’,
&gt; ‘03. CHEMFER’, ‘04. ATEX’, ‘05. MACH’, ‘06. MARNAV’, ‘07. CONST’, ‘08.
&gt; MINES’, “09. DOM”, ‘10. TRAN’, ‘11. ARARTILL’, ‘12. PREELEC’, ‘13.
&gt; CER’, ‘14. ACHIMI’, ‘15. ECLA’, ‘16. HABI’, ‘17. ANDUS’, ‘18. ARBU’,
&gt; ‘19. CHIRUR’, ‘20. ARPA’], id=None), ‘text’: Value(dtype=‘string’,
&gt; id=None)}
</code></pre>
<p><strong>My Trainer:</strong></p>
<pre><code>trainer = Trainer(
model=model,
args=training_args,
train_dataset=tokenized_datasets[“train”],
eval_dataset=tokenized_datasets[“test”],
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_metrics,
)
</code></pre>
<p><strong>what my .train() is showing:</strong></p>
<blockquote>
<p>***** Running training ***** Num examples = 85021 Num Epochs = 3 Instantaneous batch size per device = 8 Total train batch size (w.
parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps
= 1 Total optimization steps = 31884</p>
<p>|Epoch|Training Loss|Validation Loss|Accuracy|
|1|0.994300|0.972638|0.711610|
|2|0.825400|0.879027|0.736337|
|3|0.660800|0.893457|0.744401|</p>
</blockquote>
<p>I would like to continue training beyond the 3 epochs to increase my accuracy and continue to decrease training and validation loss. I tried changing the <code>num_train_epochs=10</code> as you can see but nothing changes.</p>
<p>This is largely my code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
  logging_steps=10,
)

### Metrics
from datasets import load_metric
metric = load_metric(&quot;accuracy&quot;)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

### Trainer
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
</code></pre>
",29,1,0,2,machine-learning;huggingface-transformers,2022-06-20 04:01:10,2022-06-20 04:01:10,2022-06-23 20:08:00,im new at machine learning and i m facing an issue where i want to increase the epochs for training but  train   will only do  epochs  what am i doing wrong  this is my dataset  and its features  my trainer  what my  train   is showing  i would like to continue training beyond the  epochs to increase my accuracy and continue to decrease training and validation loss  i tried changing the num_train_epochs  as you can see but nothing changes  this is largely my code 
97,97,17385780,72731358,Generating a dependent binary outcome variable that is affected by the independent variable values in R,"<p>I derived data as follows and classified it by the extreme learning machine method. I evaluated the classification performance with AUC and Accuracy. I want the AUC and Accuracy values I get here to be highly affected by the first 5 variables of the X independent variables matrix. In other words, first I will derive the Y dependent binary outcome variable and get the accuracy and AUC by classifying. Then the Y variable will remain the same (will be the same as originally derived), and when I change the values of the first 5 variables of the X independent variables matrix and reclassify, there will be significant changes in the AUC and accuracy values. I want to provide a derivation in this way. This is how I want to derive the Y dependent outcome variable. The Y values I get here are not sufficiently affected by the changes in the first 5 variables in the X independent variables matrix. How can I do that?</p>
<pre><code>install.packages(&quot;MASS&quot;)
install.packages(&quot;elmNNRcpp&quot;)
install.packages(&quot;pROC&quot;)

library(MASS)
library(elmNNRcpp)
library(pROC)

# Data gen
p=30
n=50
pr &lt;- seq(0.7, 0.4, length.out = p)
pr[1] &lt;- 1
covmat &lt;- toeplitz(pr)
mu= rep(0,p)
X_ &lt;- data.frame(mvrnorm(n, mu = mu, Sigma = covmat))
X &lt;- unname(as.matrix(sample(X_)))
vCoef = rnorm(ncol(X))
vProb =exp(X%*%vCoef)/(1+exp(X%*%vCoef))
Y &lt;- rbinom(nrow(X), 1, vProb)
mydata= data.frame(cbind(X,Y))


# Classification 
trainIndex &lt;- sample(1:nrow(mydata), size=0.7*nrow(mydata))
trainSet &lt;- mydata[trainIndex,]
testSet &lt;-mydata[-trainIndex,]
  
xtrain &lt;- as.matrix(trainSet[, 1:(length(trainSet)-1)])
ytrain &lt;- as.matrix(trainSet[, length(trainSet)])
xtest &lt;- as.matrix(testSet[, 1:(length(testSet)-1)])
ytest &lt;- as.matrix(testSet[, length(testSet)])
  
model=elm_train(xtrain, ytrain, nhid=50 , actfun='relu')
pred.class=elm_predict(model,xtest, normalize=TRUE)
  
roc.model=roc(as.factor(ytest),as.numeric(pred.class), direction=c(&quot;auto&quot;))

#best cut off 
cut.opt=InformationValue::optimalCutoff(actuals=ytest, predictedScores= pred.class) 
class.model=ifelse(pred.class&lt;cut.opt,c(&quot;0&quot;),c(&quot;1&quot;)) #short:1, long:0 kodlu sınıflarım
  
crosstab=table(factor(class.model),factor(ytest))
conf.matrix=caret::confusionMatrix(crosstab, positive=&quot;1&quot;)

#Predictive performance
AUC = roc.model$auc
Accuracy = conf.matrix$overall[&quot;Accuracy&quot;]

AUC
Accuracy  
</code></pre>
",24,0,0,5,r;binary;classification;logistic-regression;auc,2022-06-23 19:21:26,2022-06-23 19:21:26,2022-06-23 19:52:28,i derived data as follows and classified it by the extreme learning machine method  i evaluated the classification performance with auc and accuracy  i want the auc and accuracy values i get here to be highly affected by the first  variables of the x independent variables matrix  in other words  first i will derive the y dependent binary outcome variable and get the accuracy and auc by classifying  then the y variable will remain the same  will be the same as originally derived   and when i change the values of the first  variables of the x independent variables matrix and reclassify  there will be significant changes in the auc and accuracy values  i want to provide a derivation in this way  this is how i want to derive the y dependent outcome variable  the y values i get here are not sufficiently affected by the changes in the first  variables in the x independent variables matrix  how can i do that 
98,98,7875624,71659066,Can date data be included when doing K-means Clustering?,"<p>I am planning to use K-means Clustering to perform customer segmentation. This is the first Machine Learning Model I have built. I know K-means Clustering can only use numerical data attributes are numerical. However, some of the attributes are dates. For example, the last date the customer made a purchase.</p>
<ol>
<li>Can date data be included in K-means Clustering, or does it count as categorical data?</li>
<li>If date data can be included does this require any pre-processing?</li>
</ol>
<p>I will be using python to build the machine learning model.</p>
",30,0,0,3,python;machine-learning;k-means,2022-03-29 14:18:25,2022-03-29 14:18:25,2022-06-23 18:34:26,i am planning to use k means clustering to perform customer segmentation  this is the first machine learning model i have built  i know k means clustering can only use numerical data attributes are numerical  however  some of the attributes are dates  for example  the last date the customer made a purchase  i will be using python to build the machine learning model 
99,99,19189644,72728268,implement custom decision tree,"<p>I am trying to implement a custom decision tree.
I have an object with about 20 fields or so, and a list of (about 50) criterions to check with their result. It is supposed to be without any machine learning.</p>
<p>I am open about the technology, and I want it to be as easy to manipulate the tree (manually) as possible.</p>
<p>So does anyone know any package that can help me build that kind of a tree? Or the only way is to do it all on my own?</p>
<p>Thanks in advance.</p>
",17,0,0,3,package;decision-tree;implementation,2022-06-23 15:29:44,2022-06-23 15:29:44,2022-06-23 15:36:33,i am open about the technology  and i want it to be as easy to manipulate the tree  manually  as possible  so does anyone know any package that can help me build that kind of a tree  or the only way is to do it all on my own  thanks in advance 
100,100,6314221,72728216,Parallel simd MUCH slower than serial simd in Julia,"<p><em>Summary: Scroll down for reproducible example which should run-from-scratch in Julia if you have the packages specified in the <code>using</code> lines. (Note: the ODE has a complex, re-usable structure which is specified in a Gist which is downloaded/<code>include</code>d by the script.)</em></p>
<p><strong>Background:</strong> I have to repeatedly solve a large system of ODEs for different initial conditions vectors.  In the example below, it is 127 states/ODEs, but it could easily be 1000-2000.  I will have to run these 100s-1000s of times for inference, so speed is essential.</p>
<p><strong>The Puzzle:</strong> The short version is that, for the serial functions, the @simd version is much faster than the  &quot;plain&quot;, non-<code>@simd</code> version.  But for the parallel versions, the <code>@simd</code> version is <strong>much</strong> slower -- plus, in this case, the answer, <code>sum_of_solutions</code>, is variable and wrong.</p>
<p>I have this set up so that Julia is started with <code>JULIA_NUM_THREADS=auto julia</code>, in my case this creates 8 cores for 8 threads.  Then, I make sure I never have more than 8 jobs spawned at once.</p>
<p>The different calculation times: (runtime, then sum_of_ODE_solutions)</p>
<pre><code># Output is (runtime, sum_of_solutions)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
# (duration, sum_of_solutions)
# (1.1, 8.731365050398926)
# (0.878, 8.731365050398926)
# (0.898, 8.731365050398926)

serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
# (duration, sum_of_solutions)
# (0.046, 8.731365050398928)
# (0.042, 8.731365050398928)
# (0.046, 8.731365050398928)

parallel_with_plain_v5(tspan, p_Ds_v7, solve_results2; number_of_solves=number_of_solves)
# Faster than serial plain version
# (duration, sum_of_solutions)
# (0.351, 8.731365050398926)
# (0.343, 8.731365050398926)
# (0.366, 8.731365050398926)

parallel_with_simd_v7(tspan, p_Ds_v7, solve_results2; number_of_solves=number_of_solves)
# Dramatically slower than serial simd version, plus wrong sum_of_solutions
# (duration, sum_of_solutions)
# (136.966, 9.61313614002137)
# (141.843, 9.616688089683372)

</code></pre>
<p>As you can see, while serial <code>@simd</code> gets the calculation speed down to 0.046 seconds, and while parallel plain is 2.5 times faster than serial plain, when I combine parallelization with the <code>@simd</code> function I get runtimes of 140 seconds, and with variable &amp; wrong answers to boot!  Literally the only difference between the two parallelizng functions is using <code>core_op_plain</code> versus <code>core_op_simd</code> for the core ODE solving operation.</p>
<p>It seems like <code>@simd</code> and <code>@spawn</code> must be conflicting somehow? I have the parallelizing function set up to never employ more than the number of CPU threads available. (8 on my machine.)</p>
<p>I am still learning Julia so there is the chance that some smallish change could isolate the <code>@simd</code> calculations and prevent conflicts across threads (if that is what is happening). Any help is very much appreciated!</p>
<p><strong>PS: Reproducible Example.</strong> The code below should provide a reproducible example on any Julia session running multiple cores. I also have my versioninfo() etc.:</p>
<pre><code>versioninfo()
notes=&quot;&quot;&quot;
My setup is:
Julia Version 1.7.3
Commit 742b9abb4d (2022-05-06 12:58 UTC)
Platform Info:
  OS: macOS (x86_64-apple-darwin21.4.0)
  CPU: Intel(R) Xeon(R) CPU E5-2697 v2 @ 2.70GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-12.0.1 (ORCJIT, ivybridge)
&quot;&quot;&quot;


# Startup notes
notes=&quot;&quot;&quot;
# &quot;If $JULIA_NUM_THREADS is set to auto, then the number of threads will be set to the number of CPU threads.&quot;
JULIA_NUM_THREADS=auto julia --startup-file=no
Threads.nthreads(): 8 # Number of CPU threads
&quot;&quot;&quot;


using LinearAlgebra     # for &quot;I&quot; in: Matrix{Float64}(I, 2, 2)
                                            # https://www.reddit.com/r/Julia/comments/9cfosj/identity_matrix_in_julia_v10/
using Sundials              # for CVODE_BDF
using Statistics            # for mean(), max()
using DataFrames  # for e.g. DataFrame()
using Dates                     # for e.g. DateTime, Dates.now()
using DifferentialEquations # for ODEProblem
using BenchmarkTools    # for @benchmark
using Distributed           # for workers


# Check that you have multiple threads
numthreads = Base.Threads.nthreads()

# Download &amp; include the pre-saved model structure/rates (all precalculated for speed; 1.8 MB)
#include(&quot;/GitHub/BioGeoJulia.jl/test/model_p_object.jl&quot;)
url = &quot;https://gist.githubusercontent.com/nmatzke/ed99ab8f5047794eb25e1fdbd5c43b37/raw/b3e6ddff784bd3521d089642092ba1e3830699c0/model_p_object.jl&quot;
download(url,  &quot;model_p_object.jl&quot;)
include(&quot;model_p_object.jl&quot;)

# Load the ODE functions
url = &quot;https://gist.githubusercontent.com/nmatzke/f116258c78bd43ab7a448f07c4290516/raw/24a210261fd2e090b8ed27bc64a59a1ff9ec62cd/simd_vs_spawn_setup_v2.jl&quot;
download(url,  &quot;simd_vs_spawn_setup_v2.jl&quot;)
include(&quot;simd_vs_spawn_setup_v2.jl&quot;)

#include(&quot;/GitHub/BioGeoJulia.jl/test/simd_vs_spawn_setup_v2.jl&quot;)
#include(&quot;/GitHub/BioGeoJulia.jl/test/simd_vs_spawn_setup_v3.jl&quot;)

# Load the pre-saved model structure/rates (all precalculated for speed; 1.8 MB)
p_Es_v5 = load_ps_127();



# Set up output object
numstates = 127
number_of_solves = 10

solve_results1 = Array{Float64, 2}(undef, number_of_solves, numstates)
solve_results1 .= 0.0
solve_results2 = Array{Float64, 2}(undef, number_of_solves, numstates)
solve_results2 .= 0.0
length(solve_results1)
length(solve_results1[1])
sum(sum.(solve_results1))


# Precalculate the Es for use in the Ds
Es_tspan = (0.0, 60.0)
prob_Es_v7 = DifferentialEquations.ODEProblem(Es_v7_simd_sums, p_Es_v5.uE, Es_tspan, p_Es_v5);
sol_Es_v7 = solve(prob_Es_v7, CVODE_BDF(linear_solver=:GMRES), save_everystep=true, 
abstol=1e-12, reltol=1e-9);

p_Ds_v7 = (n=p_Es_v5.n, params=p_Es_v5.params, p_indices=p_Es_v5.p_indices, p_TFs=p_Es_v5.p_TFs, uE=p_Es_v5.uE, terms=p_Es_v5.terms, sol_Es_v5=sol_Es_v7);


# Set up ODE inputs
u = collect(repeat([0.0], numstates));
u[2] = 1.0
du = similar(u)
du .= 0.0
p = p_Ds_v7;
t = 1.0

# ODE functions to integrate (single-step; ODE solvers will run this many many times)
@time Ds_v5_tmp(du,u,p,t)
@time Ds_v5_tmp(du,u,p,t)
@time Ds_v7_simd_sums(du,u,p,t)
@time Ds_v7_simd_sums(du,u,p,t)

#@btime Ds_v5_tmp(du,u,p,t)
# 7.819 ms (15847 allocations: 1.09 MiB)

#@btime Ds_v7_simd_sums(du,u,p,t)
# 155.858 μs (3075 allocations: 68.66 KiB)



tspan = (0.0, 1.0)
prob_Ds_v7 = DifferentialEquations.ODEProblem(Ds_v7_simd_sums, p_Ds_v7.uE, tspan, p_Ds_v7);

sol_Ds_v7 = solve(prob_Ds_v7, CVODE_BDF(linear_solver=:GMRES), save_everystep=false, abstol=1e-12, reltol=1e-9);

# This is the core operation; plain version (no @simd)
function core_op_plain(u, tspan, p_Ds_v7)
    prob_Ds_v5 = DifferentialEquations.ODEProblem(Ds_v5_tmp, u.+0.0, tspan, p_Ds_v7);

    sol_Ds_v5 = solve(prob_Ds_v5, CVODE_BDF(linear_solver=:GMRES), save_everystep=false, abstol=1e-12, reltol=1e-9);
    return sol_Ds_v5
end


# This is the core operation; @simd version
function core_op_simd(u, tspan, p_Ds_v7)
    prob_Ds_v7 = DifferentialEquations.ODEProblem(Ds_v7_simd_sums, u.+0.0, tspan, p_Ds_v7);

    sol_Ds_v7 = solve(prob_Ds_v7, CVODE_BDF(linear_solver=:GMRES), save_everystep=false, abstol=1e-12, reltol=1e-9);
    return sol_Ds_v7
end

@time core_op_plain(u, tspan, p_Ds_v7);
@time core_op_plain(u, tspan, p_Ds_v7);
@time core_op_simd(u, tspan, p_Ds_v7);
@time core_op_simd(u, tspan, p_Ds_v7);


function serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=10)
    start_time = Dates.now()
    for i in 1:number_of_solves
        # Temporary u
        solve_results1[i,:] .= 0.0
        
        # Change the ith state from 0.0 to 1.0
        solve_results1[i,i] = 1.0
        solve_results1

        sol_Ds_v7 = core_op_plain(solve_results1[i,:], tspan, p_Ds_v7)
        solve_results1[i,:] .=  sol_Ds_v7.u[length(sol_Ds_v7.u)]
    #   print(&quot;\n&quot;)
    #   print(round.(sol_Ds_v7[length(sol_Ds_v7)], digits=3))
    end
    
    end_time = Dates.now()
    duration = (end_time - start_time).value / 1000.0
    sum_of_solutions = sum(sum.(solve_results1))
    return (duration, sum_of_solutions)
end


function serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=10)
    start_time = Dates.now()
    for i in 1:number_of_solves
        # Temporary u
        solve_results1[i,:] .= 0.0
        
        # Change the ith state from 0.0 to 1.0
        solve_results1[i,i] = 1.0
        solve_results1

        sol_Ds_v7 = core_op_simd(solve_results1[i,:], tspan, p_Ds_v7)
        solve_results1[i,:] .=  sol_Ds_v7.u[length(sol_Ds_v7.u)]
    #   print(&quot;\n&quot;)
    #   print(round.(sol_Ds_v7[length(sol_Ds_v7)], digits=3))
    end
    
    end_time = Dates.now()
    duration = (end_time - start_time).value / 1000.0
    sum_of_solutions = sum(sum.(solve_results1))
    return (duration, sum_of_solutions)
end

# Output is (runtime, sum_of_solutions)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_plain_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
# (duration, sum_of_solutions)
# (1.1, 8.731365050398926)
# (0.878, 8.731365050398926)
# (0.898, 8.731365050398926)

serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
serial_with_simd_v7(tspan, p_Ds_v7, solve_results1; number_of_solves=number_of_solves)
# (duration, sum_of_solutions)
# (0.046, 8.731365050398928)
# (0.042, 8.731365050398928)
# (0.046, 8.731365050398928)

using Distributed

function parallel_with_plain_v5(tspan, p_Ds_v7, solve_results2; number_of_solves=10)
    start_time = Dates.now()
    number_of_threads = Base.Threads.nthreads()
    curr_numthreads = Base.Threads.nthreads()
        
    # Individual ODE solutions will occur over different timeperiods,
    # initial values, and parameters.  We'd just like to load up the 
    # cores for the first jobs in the list, then add jobs as earlier
    # jobs finish.
    tasks = Any[]
    tasks_started_TF = Bool[]
    tasks_fetched_TF = Bool[]
    task_numbers = Any[]
    task_inc = 0
    are_we_done = false
    current_running_tasks = Any[]
    
    # List the tasks
    for i in 1:number_of_solves
        # Temporary u
        solve_results2[i,:] .= 0.0
        
        # Change the ith state from 0.0 to 1.0
        solve_results2[i,i] = 1.0

        task_inc = task_inc + 1
        push!(tasks_started_TF, false) # Add a &quot;false&quot; to tasks_started_TF
        push!(tasks_fetched_TF, false) # Add a &quot;false&quot; to tasks_fetched_TF
        push!(task_numbers, task_inc)
    end
    
    # Total number of tasks
    num_tasks = length(tasks_fetched_TF)

    iteration_number = 0
    while(are_we_done == false)
        iteration_number = iteration_number+1
        
        # Launch tasks when thread (core) is available
        for j in 1:num_tasks
            if (tasks_fetched_TF[j] == false)
                if (tasks_started_TF[j] == false) &amp;&amp; (curr_numthreads &gt; 0)
                    # Start a task
                    push!(tasks, Base.Threads.@spawn core_op_plain(solve_results2[j,:], tspan, p_Ds_v7));
                    curr_numthreads = curr_numthreads-1;
                    tasks_started_TF[j] = true;
                    push!(current_running_tasks, task_numbers[j])
                end
            end
        end
        
        # Check for finished tasks
        tasks_to_check_TF = ((tasks_started_TF.==true) .+ (tasks_fetched_TF.==false)).==2
        if sum(tasks_to_check_TF .== true) &gt; 0
            for k in 1:sum(tasks_to_check_TF)
                if (tasks_fetched_TF[current_running_tasks[k]] == false)
                    if (istaskstarted(tasks[k]) == true) &amp;&amp; (istaskdone(tasks[k]) == true)
                        sol_Ds_v7 = fetch(tasks[k]);
                        solve_results2[current_running_tasks[k],:] .= sol_Ds_v7.u[length(sol_Ds_v7.u)].+0.0
                        tasks_fetched_TF[current_running_tasks[k]] = true
                        current_tasknum = current_running_tasks[k]
                        deleteat!(tasks, k)
                        deleteat!(current_running_tasks, k)
                        curr_numthreads = curr_numthreads+1;
                        print(&quot;\nFinished task #&quot;)
                        print(current_tasknum)
                        print(&quot;, current task k=&quot;)
                        print(k)
                        break # break out of this loop, since you have modified current_running_tasks
                    end
                end
            end
        end

        are_we_done = sum(tasks_fetched_TF) == length(tasks_fetched_TF)
        # Test for concluding the while loop
        are_we_done &amp;&amp; break
    end # END while(are_we_done == false)

    end_time = Dates.now()
    duration = (end_time - start_time).value / 1000.0
    sum_of_solutions = sum(sum.(solve_results2))
    print(&quot;\n&quot;)
    return (duration, sum_of_solutions)
end


function parallel_with_simd_v7(tspan, p_Ds_v7, solve_results2; number_of_solves=10)
    start_time = Dates.now()
    number_of_threads = Base.Threads.nthreads()
    curr_numthreads = Base.Threads.nthreads()
        
    # Individual ODE solutions will occur over different timeperiods,
    # initial values, and parameters.  We'd just like to load up the 
    # cores for the first jobs in the list, then add jobs as earlier
    # jobs finish.
    tasks = Any[]
    tasks_started_TF = Bool[]
    tasks_fetched_TF = Bool[]
    task_numbers = Any[]
    task_inc = 0
    are_we_done = false
    current_running_tasks = Any[]
    
    # List the tasks
    for i in 1:number_of_solves
        # Temporary u
        solve_results2[i,:] .= 0.0
        
        # Change the ith state from 0.0 to 1.0
        solve_results2[i,i] = 1.0

        task_inc = task_inc + 1
        push!(tasks_started_TF, false) # Add a &quot;false&quot; to tasks_started_TF
        push!(tasks_fetched_TF, false) # Add a &quot;false&quot; to tasks_fetched_TF
        push!(task_numbers, task_inc)
    end
    
    # Total number of tasks
    num_tasks = length(tasks_fetched_TF)

    iteration_number = 0
    while(are_we_done == false)
        iteration_number = iteration_number+1
        
        # Launch tasks when thread (core) is available
        for j in 1:num_tasks
            if (tasks_fetched_TF[j] == false)
                if (tasks_started_TF[j] == false) &amp;&amp; (curr_numthreads &gt; 0)
                    # Start a task
                    push!(tasks, Base.Threads.@spawn core_op_simd(solve_results2[j,:], tspan, p_Ds_v7))
                    curr_numthreads = curr_numthreads-1;
                    tasks_started_TF[j] = true;
                    push!(current_running_tasks, task_numbers[j])
                end
            end
        end
        
        # Check for finished tasks
        tasks_to_check_TF = ((tasks_started_TF.==true) .+ (tasks_fetched_TF.==false)).==2
        if sum(tasks_to_check_TF .== true) &gt; 0
            for k in 1:sum(tasks_to_check_TF)
                if (tasks_fetched_TF[current_running_tasks[k]] == false)
                    if (istaskstarted(tasks[k]) == true) &amp;&amp; (istaskdone(tasks[k]) == true)
                        sol_Ds_v7 = fetch(tasks[k]);
                        solve_results2[current_running_tasks[k],:] .= sol_Ds_v7.u[length(sol_Ds_v7.u)].+0.0
                        tasks_fetched_TF[current_running_tasks[k]] = true
                        current_tasknum = current_running_tasks[k]
                        deleteat!(tasks, k)
                        deleteat!(current_running_tasks, k)
                        curr_numthreads = curr_numthreads+1;
                        print(&quot;\nFinished task #&quot;)
                        print(current_tasknum)
                        print(&quot;, current task k=&quot;)
                        print(k)
                        break # break out of this loop, since you have modified current_running_tasks
                    end
                end
            end
        end

        are_we_done = sum(tasks_fetched_TF) == length(tasks_fetched_TF)
        # Test for concluding the while loop
        are_we_done &amp;&amp; break
    end # END while(are_we_done == false)

    end_time = Dates.now()
    duration = (end_time - start_time).value / 1000.0
    sum_of_solutions = sum(sum.(solve_results2))
    print(&quot;\n&quot;)
    return (duration, sum_of_solutions)
end

tspan = (0.0, 1.0)
parallel_with_plain_v5(tspan, p_Ds_v7, solve_results2; number_of_solves=number_of_solves)
# Faster than serial plain version
# (duration, sum_of_solutions)
# (0.351, 8.731365050398926)
# (0.343, 8.731365050398926)
# (0.366, 8.731365050398926)

parallel_with_simd_v7(tspan, p_Ds_v7, solve_results2; number_of_solves=number_of_solves)
# Dramatically slower than serial simd version
# (duration, sum_of_solutions)
# (136.966, 9.61313614002137)
# (141.843, 9.616688089683372)



</code></pre>
<p>Thanks again, Nick</p>
",56,0,3,5,parallel-processing;julia;simd;ode;sundials,2022-06-23 15:26:07,2022-06-23 15:26:07,2022-06-23 15:26:07,summary  scroll down for reproducible example which should run from scratch in julia if you have the packages specified in the using lines   note  the ode has a complex  re usable structure which is specified in a gist which is downloaded included by the script   background  i have to repeatedly solve a large system of odes for different initial conditions vectors   in the example below  it is  states odes  but it could easily be     i will have to run these s s of times for inference  so speed is essential  the puzzle  the short version is that  for the serial functions  the  simd version is much faster than the   plain   non  simd version   but for the parallel versions  the  simd version is much slower    plus  in this case  the answer  sum_of_solutions  is variable and wrong  i have this set up so that julia is started with julia_num_threads auto julia  in my case this creates  cores for  threads   then  i make sure i never have more than  jobs spawned at once  the different calculation times   runtime  then sum_of_ode_solutions  as you can see  while serial  simd gets the calculation speed down to   seconds  and while parallel plain is   times faster than serial plain  when i combine parallelization with the  simd function i get runtimes of  seconds  and with variable  amp  wrong answers to boot   literally the only difference between the two parallelizng functions is using core_op_plain versus core_op_simd for the core ode solving operation  it seems like  simd and  spawn must be conflicting somehow  i have the parallelizing function set up to never employ more than the number of cpu threads available    on my machine   i am still learning julia so there is the chance that some smallish change could isolate the  simd calculations and prevent conflicts across threads  if that is what is happening   any help is very much appreciated  ps  reproducible example  the code below should provide a reproducible example on any julia session running multiple cores  i also have my versioninfo   etc   thanks again  nick
101,101,15818287,72727331,"Could you please refer any python notebook, how I perform classification segmentation in a single machine learning model?","<p>I want to build a model which can be performed classification segmentation in a model?</p>
",16,0,-3,4,machine-learning;deep-learning;classification;image-segmentation,2022-06-23 14:25:31,2022-06-23 14:25:31,2022-06-23 14:25:31,i want to build a model which can be performed classification segmentation in a model 
102,102,19397699,72726810,"In machine learning, should I remove original features after a feature combination?","<p>If I made a new feature (c) using two existing features (a,b) like c = a*b or a+b, should I remove the two originals? (to avoid the duplication problem?) Please, help me bro..</p>
",18,1,-1,3,machine-learning;data-analysis;data-preprocessing,2022-06-23 13:47:35,2022-06-23 13:47:35,2022-06-23 13:58:59,if i made a new feature  c  using two existing features  a b  like c   a b or a b  should i remove the two originals   to avoid the duplication problem   please  help me bro  
103,103,816213,16017081,Getting 400 bad request error in Jquery Ajax POST,"<p>I am trying to send an Ajax POST request using Jquery but I am having 400 bad request error.</p>

<p>Here is my code:</p>

<pre><code>$.ajax({
  type: 'POST',
  url: ""http://localhost:8080/project/server/rest/subjects"",
  data: {
    ""subject:title"":""Test Name"",
    ""subject:description"":""Creating test subject to check POST method API"",
    ""sub:tags"": [""facebook:work"", ""facebook:likes""],
    ""sampleSize"" : 10,
    ""values"": [""science"", ""machine-learning""]
  },
  error: function(e) {
    console.log(e);
  }
});
</code></pre>

<p>It Says: Can not build resource from request.
What am I missing ?</p>
",220209,6,62,4,jquery;ajax;post;http-status-code-400,2013-04-15 19:27:30,2013-04-15 19:27:30,2022-06-23 13:54:02,i am trying to send an ajax post request using jquery but i am having  bad request error  here is my code 
104,104,11159998,72724844,Outlier elimination in a imblearn pipeline affecting both X and y,"<p>I aim to integrate outlier elimination into a machine learning pipeline with a continuous dependent variable. The challenge is to keep <code>X</code> and <code>y</code> at the same length, thus I have eliminate outliers in both datasets.</p>
<p>As this task turned out to be difficult or impossible using <code>sklearn</code>, I switched to <code>imblearn</code> and <code>FunctionSampler</code>. Inspired by the documentation, I tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code>from imblearn import FunctionSampler
from imblearn.pipeline import make_pipeline
from sklearn.ensemble import IsolationForest
from sklearn.linear_model import LinearRegression

def outlier_rejection(X, y):
    model = IsolationForest(max_samples=100, contamination=0.4, random_state=rng)
    model.fit(X)
    y_pred = model.predict(X)
    
    return X[y_pred == 1], y[y_pred == 1]

pipe = make_pipeline(
    FunctionSampler(func = outlier_rejection),
    LinearRegression()
)

pipe.fit(X_train, y_train) # y_train is a continuous variable!
</code></pre>
<p>However, when I tried to fit the pipeline I got the error</p>
<pre><code>ValueError: Unknown label type: 'continuous'
</code></pre>
<p>which I think is because my dependent variable is continuous.</p>
<p>I suspect that <code>imblearn</code> is only compatible with nominal data. Is that true? If yes, is there another approach to solve my problem (e.g. with classic <code>sklearn</code> pipeline)? If not, where did I make a mistake in the code above?</p>
",23,0,1,3,scikit-learn;pipeline;imblearn,2022-06-23 10:43:57,2022-06-23 10:43:57,2022-06-23 13:18:55,i aim to integrate outlier elimination into a machine learning pipeline with a continuous dependent variable  the challenge is to keep x and y at the same length  thus i have eliminate outliers in both datasets  as this task turned out to be difficult or impossible using sklearn  i switched to imblearn and functionsampler  inspired by the documentation  i tried the following code  however  when i tried to fit the pipeline i got the error which i think is because my dependent variable is continuous  i suspect that imblearn is only compatible with nominal data  is that true  if yes  is there another approach to solve my problem  e g  with classic sklearn pipeline   if not  where did i make a mistake in the code above 
105,105,19396730,72725711,What&#39;s the difference between Transformers and Sentence Transformers in the context of NLP?,"<p>I'm just getting started with machine learning and NLP, and I'm confused about the difference between Transformers and Sentence Transformers.</p>
<ol>
<li><p>According to <a href=""https://www.sbert.net/docs/training/overview.html"" rel=""nofollow noreferrer"">sbert.net</a>, a sentence transformer has an architecture like in the image below. It uses a word-embedding Transformer model like BERT, combined with a pooling layer to generate what's considered to be a Sentence Transformer. So is it correct to think that Sentence Transformers are simply models created using word-embedding Transformer models?</p>
</li>
<li><p>Sentence Transformers are trained specifically to solve tasks like semantic search, sentence similarity etc. If they're made from Transformers and a pooling layer, where does that training take place?</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/qvEDu.png"" rel=""nofollow noreferrer"">Sentence Transformer Architecture</a></p>
",28,0,-3,3,python;machine-learning;nlp,2022-06-23 12:19:56,2022-06-23 12:19:56,2022-06-23 12:19:56,i m just getting started with machine learning and nlp  and i m confused about the difference between transformers and sentence transformers  according to   a sentence transformer has an architecture like in the image below  it uses a word embedding transformer model like bert  combined with a pooling layer to generate what s considered to be a sentence transformer  so is it correct to think that sentence transformers are simply models created using word embedding transformer models  sentence transformers are trained specifically to solve tasks like semantic search  sentence similarity etc  if they re made from transformers and a pooling layer  where does that training take place  
106,106,19381301,72698404,confusion matrix for multilabel classification using prefetch dataset,"<p>I am new to keras and machine learning in general and I have adapted this <a href=""https://keras.io/examples/nlp/multi_label_classification/"" rel=""nofollow noreferrer"">keras tutorial</a> to my case that involves text classification into 8 categories.</p>
<p>In this tutorial a prefetch dataset is created for training and testing, but it does not show how to create a confusion matrix from prefetch datasets. I followed the instructions on this <a href=""https://www.pythonfixing.com/2021/11/fixed-how-to-plot-confusion-matrix-for.html"" rel=""nofollow noreferrer"">https://www.pythonfixing.com/2021/11/fixed-how-to-plot-confusion-matrix-for.html</a> which also involves multi label classification (iris dataset with 3 classes)
but I am getting an error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-81-02116c95e29c&gt; in &lt;module&gt;()
----&gt; 1 confusion_matrix(predicted_categories, true_categories)

1 frames
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in 
_check_targets(y_true, y_pred)
     93         raise ValueError(
     94             &quot;Classification metrics can't handle a mix of {0} and {1} 
targets&quot;.format(
---&gt; 95                 type_true, type_pred
     96             )
     97         )

ValueError: Classification metrics can't handle a mix of multiclass and multilabel- 
indicator targets
</code></pre>
<p>My code is below:</p>
<pre><code>epochs = 20

shallow_mlp_model = make_model()
shallow_mlp_model.compile(
    loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;categorical_accuracy&quot;]
)

history = shallow_mlp_model.fit(
    train_dataset, validation_data=validation_dataset, epochs=epochs
)


def plot_result(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history[&quot;val_&quot; + item], label=&quot;val_&quot; + item)
    plt.xlabel(&quot;Epochs&quot;)
    plt.ylabel(item)
    plt.title(&quot;Train and Validation {} Over Epochs&quot;.format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()


plot_result(&quot;loss&quot;)
plot_result(&quot;categorical_accuracy&quot;)

_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)
print(f&quot;Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.&quot;)


predictions=shallow_mlp_model.predict(test_dataset)

predicted_categories = tf.argmax(predictions, axis=1)

true_categories = tf.concat([y for x, y in test_dataset], axis=0)

confusion_matrix(predicted_categories, true_categories)
</code></pre>
<p>So, anyone could help me to create a confusion matrix in this case?</p>
",25,1,1,4,python;tensorflow;keras;confusion-matrix,2022-06-21 15:00:43,2022-06-21 15:00:43,2022-06-23 11:16:19,i am new to keras and machine learning in general and i have adapted this  to my case that involves text classification into  categories  my code is below  so  anyone could help me to create a confusion matrix in this case 
107,107,17065784,72706415,Predict a data using Pickle file,"<p>I have pre build machine learning model (saved as pickle file) to predict classification.</p>
<p>My question is when I use new dataset to predict using Pickle file do I need do all preprocessing steps (like transformation and encoding) to the new testing dataset or can I use raw data set.</p>
",18,1,-1,5,python;machine-learning;data-science;pickle;data-science-experience,2022-06-22 01:22:54,2022-06-22 01:22:54,2022-06-23 09:31:08,i have pre build machine learning model  saved as pickle file  to predict classification  my question is when i use new dataset to predict using pickle file do i need do all preprocessing steps  like transformation and encoding  to the new testing dataset or can i use raw data set 
108,108,12069922,67656823,How can I load tf.js CDN in chrome Extension?,"<p>I am making a Machine-learning Chrome Extension so I need to use Tf.js but when I'm loading tf.js CDN it's given me an error?</p>
<pre><code>Refused to load the script 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs' because it violates the following Content Security Policy directive: &quot;script-src 'self' https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest 'unsafe-eval'&quot;. Note that 'script-src-elem' was not explicitly set, so 'script-src' is used as a fallback.
</code></pre>
<p>Can anyone help me with this?</p>
<p>manifest.json</p>
<pre><code>&quot;content_security_policy&quot;: &quot;script-src 'self' 'unsafe-eval'; object-src 'self'&quot;,
&quot;mainfest_version&quot;:2
</code></pre>
<p>background.js</p>
<pre><code>const urls = {
  model:
    &quot;https://storage.googleapis.com/tfjs-models/tfjs/sentiment_cnn_v1/model.json&quot;,
  metadata:
    &quot;https://storage.googleapis.com/tfjs-models/tfjs/sentiment_cnn_v1/metadata.json&quot;,
};

// load model
async function loadModel(url) {
  try {
    const model = await tf.loadLayersModel(url);
    console.log(&quot;model Loaded&quot;);
    return model;
  } catch (err) {
    console.log(err);
  }
}

// load meta data
async function loadMetadata(url) {
  try {
    const metadataJson = await fetch(url);
    const metadata = await metadataJson.json();
    console.log(&quot;Metadata Loaded&quot;);
    return metadata;
  } catch (err) {
    console.log(err);
  }
}
</code></pre>
<p>index.html</p>
<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;!-- load tf.js model --&gt;
    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs&quot;&gt;&lt;/script&gt;


</code></pre>
",111,0,2,5,javascript;tensorflow;machine-learning;google-chrome-extension;nlp,2021-05-23 11:59:54,2021-05-23 11:59:54,2022-06-23 08:39:41,i am making a machine learning chrome extension so i need to use tf js but when i m loading tf js cdn it s given me an error  can anyone help me with this  manifest json background js index html
109,109,12580991,63554522,How do I fix this Wide &amp; Deep RNN?,"<p>Working through the book Hands-On Machine Learning and came across the Wide and Deep model, which seems like a flexible, accurate RNN model. However, I keep getting the following error: &quot; AttributeError: 'tuple' object has no attribute 'ndim' &quot;. I Googled the error and there doesn't appear to be a clear solution or explanation for why I'm getting the error, at least to me as a beginner. Anyone have an idea on how to fix this?</p>
<pre><code>housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, 
random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

input_A = keras.layers.Input(shape=[5], name=&quot;wide_input&quot;)
input_B = keras.layers.Input(shape=[6], name=&quot;deep_input&quot;)
hidden1 = keras.layers.Dense(30, activation=&quot;relu&quot;)(input_B)
hidden2 = keras.layers.Dense(30, activation=&quot;relu&quot;)(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name=&quot;output&quot;)(concat)
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])

model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))

X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]

history = model.fit((X_train_A, X_train_B), y_train, epochs=20,
                validation_data=((X_valid_A, X_valid_B), y_valid))
mse_test = model.evaluate((X_test_A, X_test_B), y_test)
y_pred = model.predict((X_new_A, X_new_B))
</code></pre>
",57,2,1,3,python;neural-network;tensorflow2.0,2020-08-24 09:25:49,2020-08-24 09:25:49,2022-06-23 02:19:22,working through the book hands on machine learning and came across the wide and deep model  which seems like a flexible  accurate rnn model  however  i keep getting the following error    attributeerror   tuple  object has no attribute  ndim     i googled the error and there doesn t appear to be a clear solution or explanation for why i m getting the error  at least to me as a beginner  anyone have an idea on how to fix this 
110,110,19232897,72720850,Problem with Chatterbot TypeError: &#39;str&#39; object is not callable,"<p>I'm trying to write a bot with machine learning but the code give me an error when I try to write my input for the bot, this is the code:</p>
<pre><code>from tkinter import Variable
from urllib import response
from chatterbot import ChatBot
from regex import BESTMATCH
from chatterbot.trainers import ListTrainer
from sqlalchemy import true

bot = ChatBot(
    &quot;Francesco&quot;,
    storage_adapter='chatterbot.storage.SQLStorageAdapter',
    database=&quot;./db.sqlite3&quot;,
    input_adapter=&quot;chatterbot.input.VariableInputTypeAdapter&quot;,
    output_adapter=&quot;chatterbot.output.OutputAdapter&quot;,
    
    logic_adapters=[
        {
            &quot;import_path&quot;: &quot;chatterbot.logic.BestMatch&quot;,
            &quot;statement_comparison_function&quot;: &quot;chatterbot.comparisons.LevenshteinDistance&quot;,
            &quot;response_selection_method&quot;: &quot;chatterbot.response_selection.get_first_response&quot;
        }
    ]
)
text = open(r&quot;C:\Users\tures\OneDrive\Desktop\python\Chatterbot\Conversazione.txt&quot;).readlines()
trainer=ListTrainer(bot)
trainer.train(text)

while true:
    try:
        user_input= input (str(&quot;msg: &quot;))
        risposta = bot.get_response(user_input)
        print (str(&quot;bot:&quot;,risposta))
    except(KeyboardInterrupt, SystemExit):
        print(&quot;Ciao a presto!&quot;)
        break
</code></pre>
<p>And this is the error:</p>
<pre><code> response = self.generate_response(input_statement, additional_response_selection_parameters)
  File &quot;C:\Users\tures\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\chatterbot\chatterbot.py&quot;, line 173, in generate_response
    output = adapter.process(input_statement, additional_response_selection_parameters)
  File &quot;C:\Users\tures\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\chatterbot\logic\best_match.py&quot;, line 87, in process
    response = self.select_response(
TypeError: 'str' object is not callable
</code></pre>
",32,0,-1,2,python;chatterbot,2022-06-23 00:42:11,2022-06-23 00:42:11,2022-06-23 00:44:14,i m trying to write a bot with machine learning but the code give me an error when i try to write my input for the bot  this is the code  and this is the error 
111,111,13850885,65545949,Sklearn datasets default data structure is pandas or numPy?,"<p>I'm working through an exercise in <a href=""https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"" rel=""nofollow noreferrer"">https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/</a> and am finding unexpected behavior on my computer when I fetch a dataset. The following code returns</p>
<pre><code>numpy.ndarray 
</code></pre>
<p>on the author's Google Collab page, but returns</p>
<pre><code>pandas.core.frame.DataFrame
</code></pre>
<p>on my local Jupyter notebook.  As far as I know, my environment is using the exact same versions of libraries as the author.  I can easily convert the data to a numPy array, but since I'm using this book as a guide for novices, I'd like to know what could be causing this discrepancy.</p>
<pre><code>from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1)
mnist.keys()
type(mnist['data'])
</code></pre>
<p>The author's Google Collab is at the following link, scrolling down to the &quot;MNIST&quot; heading.  Thanks!
<a href=""https://colab.research.google.com/github/ageron/handson-ml2/blob/master/03_classification.ipynb#scrollTo=LjZxzwOs2Q2P"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/ageron/handson-ml2/blob/master/03_classification.ipynb#scrollTo=LjZxzwOs2Q2P</a>.</p>
",105,1,1,5,pandas;numpy;scikit-learn;dataset;mnist,2021-01-03 07:23:46,2021-01-03 07:23:46,2022-06-22 23:50:38,i m working through an exercise in  and am finding unexpected behavior on my computer when i fetch a dataset  the following code returns on the author s google collab page  but returns on my local jupyter notebook   as far as i know  my environment is using the exact same versions of libraries as the author   i can easily convert the data to a numpy array  but since i m using this book as a guide for novices  i d like to know what could be causing this discrepancy 
112,112,9490832,57888291,How to properly pickle sklearn pipeline when using custom transformer,"<p>I am trying to pickle a sklearn machine-learning model, and load it in another project. The model is wrapped in pipeline that does feature encoding, scaling etc. The problem starts when i want to use self-written transformers in the pipeline for more advanced tasks. </p>

<p>Let's say I have 2 projects: </p>

<ul>
<li>train_project: it has the custom transformers in src.feature_extraction.transformers.py</li>
<li>use_project: it has other things in src, or has no src catalog at all</li>
</ul>

<p>If in ""train_project"" I save the pipeline with joblib.dump(), and then in ""use_project"" i load it with joblib.load() it will not find something such as ""src.feature_extraction.transformers"" and throw exception:</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'src.feature_extraction'</p>
</blockquote>

<p>I should also add that my intention from the beginning was to simplify usage of the model, so programist can load the model as any other model, pass very simple, human readable features, and all ""magic"" preprocessing of features for actual model (e.g. gradient boosting) is happening inside.</p>

<p>I thought of creating /dependencies/xxx_model/ catalog in root of both projects, and store all needed classes and functions in there (copy code from ""train_project"" to ""use_project""), so structure of projects is equal and transformers can be loaded. I find this solution extremely inelegant, because it would force the structure of any project where the model would be used.</p>

<p>I thought of just recreating the pipeline and all transformers inside ""use_project"" and somehow loading fitted values of transformers from ""train_project"".</p>

<p>The best possible solution would be if dumped file contained all needed info and needed no dependencies, and I am honestly shocked that sklearn.Pipelines seem to not have that possibility - what's the point of fitting a pipeline if i can not load fitted object later? Yes it would work if i used only sklearn classes, and not create custom ones, but non-custom ones do not have all needed functionality.</p>

<p>Example code:</p>

<p>train_project</p>

<p>src.feature_extraction.transformers.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.pipeline import TransformerMixin
class FilterOutBigValuesTransformer(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        self.biggest_value = X.c1.max()
        return self

    def transform(self, X):
        return X.loc[X.c1 &lt;= self.biggest_value]
</code></pre>

<p>train_project</p>

<p>main.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.externals import joblib
from sklearn.preprocessing import MinMaxScaler
from src.feature_extraction.transformers import FilterOutBigValuesTransformer

pipeline = Pipeline([
    ('filter', FilterOutBigValuesTransformer()),
    ('encode', MinMaxScaler()),
])
X=load_some_pandas_dataframe()
pipeline.fit(X)
joblib.dump(pipeline, 'path.x')
</code></pre>

<p>test_project</p>

<p>main.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.externals import joblib

pipeline = joblib.load('path.x')
</code></pre>

<p>The expected result is pipeline loaded correctly with transform method possible to use.</p>

<p>Actual result is exception when loading the file.</p>
",12858,7,24,5,python;scikit-learn;persistence;pipeline;joblib,2019-09-11 17:06:03,2019-09-11 17:06:03,2022-06-22 20:09:37,i am trying to pickle a sklearn machine learning model  and load it in another project  the model is wrapped in pipeline that does feature encoding  scaling etc  the problem starts when i want to use self written transformers in the pipeline for more advanced tasks   let s say i have  projects   if in train_project i save the pipeline with joblib dump    and then in use_project i load it with joblib load   it will not find something such as src feature_extraction transformers and throw exception  modulenotfounderror  no module named  src feature_extraction  i should also add that my intention from the beginning was to simplify usage of the model  so programist can load the model as any other model  pass very simple  human readable features  and all magic preprocessing of features for actual model  e g  gradient boosting  is happening inside  i thought of creating  dependencies xxx_model  catalog in root of both projects  and store all needed classes and functions in there  copy code from train_project to use_project   so structure of projects is equal and transformers can be loaded  i find this solution extremely inelegant  because it would force the structure of any project where the model would be used  i thought of just recreating the pipeline and all transformers inside use_project and somehow loading fitted values of transformers from train_project  the best possible solution would be if dumped file contained all needed info and needed no dependencies  and i am honestly shocked that sklearn pipelines seem to not have that possibility   what s the point of fitting a pipeline if i can not load fitted object later  yes it would work if i used only sklearn classes  and not create custom ones  but non custom ones do not have all needed functionality  example code  train_project src feature_extraction transformers py train_project main py test_project main py the expected result is pipeline loaded correctly with transform method possible to use  actual result is exception when loading the file 
113,113,19389117,72711766,Most suited LSTM / Random Forest prediction algorithm,"<p>I have a dataframe of prices from Yahoo for AAPL from yFinance.</p>
<p>I have a column &quot;openclose&quot; which is 1 if the close &gt; open price for that row, and 0 if not.</p>
<pre><code>df['closeopen'] = df['Close'] &gt; df['Open']
df['closeopen'] = df['closeopen'].astype(int)
</code></pre>
<p>I would like to use machine learning to predict - as best as possible - whether the next future 1 row for 'openclose' will be 0 or a 1 - and see the accuracy of this as well - ie. in testing it was accurate x% of the time.</p>
<p>I have other columns of data that may influence this, such as is close price above 50 week high - this is also represented as a 1 or a 0. These are in df['col2'], ['col3'] to ['col6']</p>
<p>There are approx. 10,000 rows of data in this dataframe.</p>
<p>I'm new to ML, and my questions are:</p>
<ul>
<li><p>Would models such as LSTM be best suited for predicting the next 1 row using all of the above columns? Or random forest? Or anything else?</p>
</li>
<li><p>Are there any code samples to help with this? Appreciate this isn't a free labour forum, I'm getting very lost in the tutorials out there especially on medium/towards data science.</p>
</li>
</ul>
",39,2,-1,4,python;machine-learning;scikit-learn;yfinance,2022-06-22 13:35:24,2022-06-22 13:35:24,2022-06-22 18:46:00,i have a dataframe of prices from yahoo for aapl from yfinance  i have a column  openclose  which is  if the close  gt  open price for that row  and  if not  i would like to use machine learning to predict   as best as possible   whether the next future  row for  openclose  will be  or a    and see the accuracy of this as well   ie  in testing it was accurate x  of the time  i have other columns of data that may influence this  such as is close price above  week high   this is also represented as a  or a   these are in df  col      col   to   col   there are approx    rows of data in this dataframe  i m new to ml  and my questions are  would models such as lstm be best suited for predicting the next  row using all of the above columns  or random forest  or anything else  are there any code samples to help with this  appreciate this isn t a free labour forum  i m getting very lost in the tutorials out there especially on medium towards data science 
114,114,6937270,72711180,Create Web Platform for my image datasets - like Kaggle has. Any input?,"<p>I am looking into creating a web platform for my image datasets for machine learning. I would like to structure it kind of like Kaggle <a href=""https://www.kaggle.com/datasets"" rel=""nofollow noreferrer"">Datasets website</a></p>
<p>I have absolutely no knowledge in web development, but lots of general programming skills.
Any suggestions as to what language should be used, are there any resources to more specifically read up on solving my problem? Any templates to use?</p>
",17,0,-1,3,javascript;html;user-interface,2022-06-22 12:48:06,2022-06-22 12:48:06,2022-06-22 16:48:15,i am looking into creating a web platform for my image datasets for machine learning  i would like to structure it kind of like kaggle 
115,115,16538259,72714460,What is Undecidability and its practical application?,"<p>I have a vague understanding of undecidability. I get it that undecidability is that for a problem there exists no algorithm or the Turing machine will never halt. But I cant just visualize it. Can someone explain in a better way. and I don't get why we are learning about it and its applications. Can someone explain in this topic?</p>
",12,0,0,1,computation-theory,2022-06-22 16:41:04,2022-06-22 16:41:04,2022-06-22 16:41:04,i have a vague understanding of undecidability  i get it that undecidability is that for a problem there exists no algorithm or the turing machine will never halt  but i cant just visualize it  can someone explain in a better way  and i don t get why we are learning about it and its applications  can someone explain in this topic 
116,116,19364867,72711638,Frequency of crontab calling a php function - Looking for alternative options,"<p>I'm programming my own &quot;smart home&quot; as a learning project.
My code is running fine. I'm looking for help to improve the efficiency and of the code and/or the setup of crontab + php code.</p>
<p>I'm monitoring the energy consumption of my washing machine with a WIFI energy meter. Target is to notify me once the washing machine is completed so I don't forget to clear it.</p>
<p>on my Pi I have a <strong>crontab</strong> like so:</p>
<pre><code>*/20 7-22 * * * /usr/bin/php '/home/holger/html/plugs/washer.php' 
</code></pre>
<p>which runs following <strong>php code</strong> (I simplified for better readability):</p>
<p>[...]/I call the function, of course, but this function does the main task</p>
<pre><code>function loop($maschine, $watt_init, $trashhold){
    
      $max = 75;//max loops to avoid endless runs
      $i = 1;//start counter
      $tackt = 3;//tact time to check energy consumption
      //$trashhold = 4;//ab x Watt kein standby
      if ($watt_init &lt; 1 ) {//Machine is switched off if energy consumption &lt; 1 Watt
        die;//quit
      }
      elseif ($watt_init &lt; 2 ) {//Machine is switched off or in standby if energy consumption &lt; 1 Watt
        die;//quit
      }
      else {//Any thing else: Machine is running   
        while ($i &lt; $max) {//loop as long as max loops are not reached
          $watt_current = json_combine(IPplug5);//getting current energy consumption from WIFI energy meter via JSON
          sleep(60*$tackt);//sleep and continue every 60s x tact time 
          $i++;//increase counter +1
          //compare actual consumption with defined trashhold
          if ($watt_current[0] &gt;= $trashhold) {//continue while energy consumption bigger then trashhold
            continue;//repeat loop
          }
          elseif ($watt_current[0] &lt; $trashhold) {//stop if energy consumption lower then trashhold
            break;//stop loop
          }
        }
        echo &quot;Program done. please clear. Runtime: &quot; . $i*$tackt. &quot;Min.&quot;
           //[...] message me to my telegram bot
    }
  }
</code></pre>
<p>The code is running fine and I'm getting the output I need.</p>
<p><strong>My question is: Is there a better way to do that?</strong></p>
<p>Currently I'm afraid to overload my Pi with too many open php sessions, therefore I'm starting the code only every 20min and also let the while loop sleep for 3 Min. But for improved accuracy I like to run the cronjob more often and also let the while loop sleep only for 30s.
My requirements are to stick to my PI and php code and not to use any available software like Home Assisant.io as it contradicts with my learning approach.
Any ideas or insights welcome.</p>
",55,1,1,2,php;cron,2022-06-22 13:25:56,2022-06-22 13:25:56,2022-06-22 15:38:13,i m monitoring the energy consumption of my washing machine with a wifi energy meter  target is to notify me once the washing machine is completed so i don t forget to clear it  on my pi i have a crontab like so  which runs following php code  i simplified for better readability         i call the function  of course  but this function does the main task the code is running fine and i m getting the output i need  my question is  is there a better way to do that 
117,117,8165061,72712848,Options for working with bigger NLP models in shiny,"<p>I might be missing something (surely I do) but amongst the many many tutorials, blogs, posts on machine learning with R and Shiny, I never read anything about effective storing/loading/retraining such models. To me this is not trivial, as these files are pretty big (up to 10Gb for some pretrained models).
My case is also concrete:</p>
<ul>
<li>I have trained a text classification model using fastrtext, resulting in a 1Gb bin file.</li>
<li>Over time, the model is likely to face examples never seen before, and retraining will be needed.</li>
<li>So in my shiny app, I would like to allow the user to be able to retrain on a new set.</li>
</ul>
<p>In this scenario, where would we store/keep the trained model file, as the filesystem on shinyapps.io can not (or should not be used). How do some of you solve this problem?
Is there a way to store this file in an sqlite database. If so, what is the workflow of loading/storing/retrieving given the use of fasttext?</p>
<p>So, any pointers/tip/use case welcome.
Best regards
Bernie</p>
",24,0,0,3,r;shiny;nlp,2022-06-22 14:50:08,2022-06-22 14:50:08,2022-06-22 14:50:08,
118,118,13712472,72711579,Deep Learning equivalent Machine Learning,"<p>The question seems simple, can we find a neural network for every classical Machine Learning model?</p>
<p>For example:</p>
<ul>
<li>Linear regression is a perceptron.</li>
<li>PCA is an auto-encoder with a single intermediate layer</li>
<li>We can approximate Ridge or Lasso by adding some decay at the time of the construction of the network</li>
</ul>
<p>If the answer to the first question is yes then how can I find equivalents to decision trees and SVMs?</p>
",17,0,0,3,machine-learning;deep-learning;theory,2022-06-22 13:21:16,2022-06-22 13:21:16,2022-06-22 13:21:16,the question seems simple  can we find a neural network for every classical machine learning model  for example  if the answer to the first question is yes then how can i find equivalents to decision trees and svms 
119,119,9273287,72705495,What&#39;s the best way to load a ML model with Django,"<p>I'm deploying a Machine Learning model (Named Entity Recognition) with Django. In short, the user chooses a field (Politics or Science for example) and writes a text in a search area. Then the model identifies the named entities in the text.</p>
<p>My problem is that the ML model (encoder) is loaded each time the view is triggered, which slows down the process. Any idea how to optimize this and load it only once ?</p>
<p>My views.py :</p>
<pre><code>def search_view(request):

   if request.POST: 
          field = request.POST['field']
          query = request.POST['query']
          encoder = load_encoder(field)
          results = Ner_model(query,encoder)
          context['result'] = results
   return render(request,'ner/results.html', context)
      
</code></pre>
<p>Load encoder function:</p>
<pre><code>def load_encoder(field):
 path_encoder = os.paths.join(field,'field_encoder')
 encoder = AutoTokenizer.from_pretrained(path_encoder)
 return encoder
</code></pre>
<p>Thanks!</p>
",34,2,1,3,django;django-views;bert-language-model,2022-06-21 23:51:52,2022-06-21 23:51:52,2022-06-22 11:53:07,i m deploying a machine learning model  named entity recognition  with django  in short  the user chooses a field  politics or science for example  and writes a text in a search area  then the model identifies the named entities in the text  my problem is that the ml model  encoder  is loaded each time the view is triggered  which slows down the process  any idea how to optimize this and load it only once   my views py   load encoder function  thanks 
120,120,19384279,72703709,Save and reuse ML model,"<p>First of all, let me introduce myself.  I am a young researcher and I am interested in machine learning.  I created a model, trained, tested and validated.  Now I would like to know if there is a way to save my trained model.</p>
<p>I am also interested in knowing if the model is saved trained.</p>
<p>Finally, is there a way to use the saved (and trained) model with new data without having to train the model again?</p>
<p>I work with python!</p>
",28,2,-1,4,python;machine-learning;save;load,2022-06-21 21:17:16,2022-06-21 21:17:16,2022-06-22 09:12:44,first of all  let me introduce myself   i am a young researcher and i am interested in machine learning   i created a model  trained  tested and validated   now i would like to know if there is a way to save my trained model  i am also interested in knowing if the model is saved trained  finally  is there a way to use the saved  and trained  model with new data without having to train the model again  i work with python 
121,121,17903077,72707777,ml.net image classification categories problem,"<p>So i made a Machine Learning project with images classification auto trained and it actually work good!</p>
<p>The only problem im facing is:</p>
<p>let say i have 2 categories, Cars and Boats. The machine is 100% accurate when it come to cars or boats but if i put a random picture like a dog or an house, it still validate it as a car or a boat...? So i guess its just run an algorythm and chose the higher score of similarity.</p>
<p>So i checked on youtube, stack and many others forums and website for an answer but i found nothing about. So my last hope is to ask the question in here.</p>
<p>Can someone can tell me what should i do to have the machine have a mismatch when the image analyzed is not one of the categories? I tried to implement a code that would be based on score to reject scores under X number but the thing is that some image that are out of the categories get an higher score than an image in the categories... at this point im pretty lost.</p>
<p>thank you</p>
",9,0,2,2,c#;ml.net,2022-06-22 04:01:00,2022-06-22 04:01:00,2022-06-22 04:01:00,so i made a machine learning project with images classification auto trained and it actually work good  the only problem im facing is  let say i have  categories  cars and boats  the machine is   accurate when it come to cars or boats but if i put a random picture like a dog or an house  it still validate it as a car or a boat     so i guess its just run an algorythm and chose the higher score of similarity  so i checked on youtube  stack and many others forums and website for an answer but i found nothing about  so my last hope is to ask the question in here  can someone can tell me what should i do to have the machine have a mismatch when the image analyzed is not one of the categories  i tried to implement a code that would be based on score to reject scores under x number but the thing is that some image that are out of the categories get an higher score than an image in the categories    at this point im pretty lost  thank you
122,122,18088469,72707619,Text classification with BERT and PyTorch Lightning,"<p>I am currently working on multi-label text classification with BERT and PyTorch Lightning. I am new to machine learning and am confused on how to train my model on AWS.
My questions include which Accelerated Computing instance (Amazon EC2) do I use considering I have a large database with 377 labels. Along with that, will I be able to run my code through Jupyter Notebooks with libraries like torch? Thank you for your time.</p>
",17,0,0,1,machine-learning,2022-06-22 03:39:54,2022-06-22 03:39:54,2022-06-22 03:39:54,
123,123,9116959,70545304,does RNN suitable with kaggle Titanic - Machine Learning from Disaster,"<p>I've found a tutorial in Kaggle web site that explains how to use RNN (Recurrent Neural Network) on the titanic data set  in order to predict who survived.</p>
<p>my question is - how come RNN is suitable for this problem?
I thought RNN is not suitable for problems with csv file as a data set.</p>
<p>link to the tutorial (you can find the csv files in there) - <a href=""https://www.kaggle.com/lusob04/titanic-rnn"" rel=""nofollow noreferrer"">https://www.kaggle.com/lusob04/titanic-rnn</a></p>
<p>and here is a sample of the dataset -
<a href=""https://i.stack.imgur.com/W9L3N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W9L3N.png"" alt=""enter image description here"" /></a></p>
<p>and another question - do you think CNN or RL is better suited for this problem?</p>
",77,1,0,4,csv;tensorflow;recurrent-neural-network;kaggle,2022-01-01 01:22:01,2022-01-01 01:22:01,2022-06-22 00:32:09,i ve found a tutorial in kaggle web site that explains how to use rnn  recurrent neural network  on the titanic data set  in order to predict who survived  link to the tutorial  you can find the csv files in there     and another question   do you think cnn or rl is better suited for this problem 
124,124,7427786,72684326,How to change Sklearn flavors version in mlflow on azure machine learning?,"<p>I need to change the flavors &quot;sklearn_version&quot; in mlflow from &quot;0.22.1&quot; to &quot;1.0.0&quot; on azure machine learning when I log my trained model, since this model will be incompatible with the sklearn version that I am using for deployment during inference. I could change the version of sklearn in conda.yml file by setting &quot;conda_env&quot; in</p>
<p><code>mlflow.sklearn.log_model(conda_env= 'my_env')</code></p>
<p><a href=""https://i.stack.imgur.com/URygm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/URygm.png"" alt=""enter image description here"" /></a></p>
<p>here is the screen shot of requirements.txt
<a href=""https://i.stack.imgur.com/8us2o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8us2o.png"" alt=""enter image description here"" /></a></p>
<p>however, sklearn version under flavors in MLmodel file remains unchanged and that is the file that causes problem:</p>
<p><a href=""https://i.stack.imgur.com/XfWvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XfWvJ.png"" alt=""enter image description here"" /></a></p>
<p>and here is script that I use to create this mlflow experiment in azure machine learning notebooks.</p>
<pre><code>import mlflow
from sklearn.tree import DecisionTreeRegressor

from azureml.core import Workspace
from azureml.core.model import Model
from azureml.mlflow import register_model


def run_model(ws, experiment_name, run_name, x_train, y_train):
    
    # set up MLflow to track the metrics
    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
    mlflow.set_experiment(experiment_name)  
    
    with mlflow.start_run(run_name=run_name) as run:
        
        # fit model
        regression_model = DecisionTreeRegressor()
        regression_model.fit(x_train, y_train)
    
        # log training score 
        training_score = regression_model.score(x_train, y_train)
        mlflow.log_metric(&quot;Training score&quot;, training_score)

        my_conda_env = {
                    &quot;name&quot;: &quot;mlflow-env&quot;,
                    &quot;channels&quot;: [&quot;conda-forge&quot;],
                    &quot;dependencies&quot;: [
                        &quot;python=3.8.5&quot;,
                        {
                            &quot;pip&quot;: [
                                &quot;pip&quot;,
                                &quot;scikit-learn~=1.0.0&quot;,
                                &quot;uuid==1.30&quot;,
                                &quot;lz4==4.0.0&quot;,
                                &quot;psutil==5.9.0&quot;,
                                &quot;cloudpickle==1.6.0&quot;,
                                &quot;mlflow&quot;,
                            ],
                        },
                    ],
                }

        
        # register the model
        mlflow.sklearn.log_model(regression_model, &quot;model&quot;, conda_env=my_conda_env)

    model_uri = f&quot;runs:/{run.info.run_id}/model&quot;
    model = mlflow.register_model(model_uri, &quot;sklearn_regression_model&quot;)

if __name__ == '__main__':

    # connect to your workspace
    ws = Workspace.from_config()

    # create experiment and start logging to a new run in the experiment
    experiment_name = &quot;exp_name&quot;

    # mlflow run name
    run_name= '1234'

  
    # get train data
    x_train, y_train  = get_train_data()
    
    run_model(ws, experiment_name, run_name, x_train, y_train)
</code></pre>
<p>Any idea how can change the flavor sklearn version in MLmodel file from <strong>&quot;0.22.1&quot;</strong> to <strong>&quot;1.0.0&quot;</strong> in my script?</p>
<p>With many thanks in advance!</p>
",51,2,0,5,python;azure;scikit-learn;azure-machine-learning-service;mlflow,2022-06-20 14:08:43,2022-06-20 14:08:43,2022-06-21 22:07:40,i need to change the flavors  sklearn_version  in mlflow from      to      on azure machine learning when i log my trained model  since this model will be incompatible with the sklearn version that i am using for deployment during inference  i could change the version of sklearn in conda yml file by setting  conda_env  in mlflow sklearn log_model conda_env   my_env    however  sklearn version under flavors in mlmodel file remains unchanged and that is the file that causes problem   and here is script that i use to create this mlflow experiment in azure machine learning notebooks  any idea how can change the flavor sklearn version in mlmodel file from      to      in my script  with many thanks in advance 
125,125,19383228,72701941,index 1 is out of bounds for axis 0 with size 1 in machine learning,"<p>I don't know why but its showing index 1 is out of bounds for axis 0 with size 1 on acc[i], Its a program for semi supervised learning. Can someone help me what no should be in np.empty()</p>
<pre><code>nc =np.arange(.40, 1, .03)
acc = np.empty(1)
i = 0
for k in np.nditer(nc):
    conf_ind = df['max'] &gt; k
    X_train1 = np.append(x_train_np, unl_np[conf_ind, :], axis=0)
    Y_train1 = np.append(y_train_np, df.loc[conf_ind, ['lab']])
    clf = svm.SVC(kernel='linear', probability= True, C=1).fit(X_train1, Y_train1)
    acc[i] = clf.score(x_test, y_test)
    i = i + 1
    ```

Note: This is just part of the whole code but the problem is most probably within this code.
</code></pre>
",29,0,0,3,pandas;numpy;semisupervised-learning,2022-06-21 19:15:20,2022-06-21 19:15:20,2022-06-21 19:26:09,i don t know why but its showing index  is out of bounds for axis  with size  on acc i   its a program for semi supervised learning  can someone help me what no should be in np empty  
126,126,7373787,72389648,What is the main purpose of Feature Selection?,"<p>I have a small medical dataset (200 samples) that contains only 6 cases of the condition I am trying to predict using machine learning. So far, the dataset is not proving useful for predicting the target variable and is resulting in models with 0% recall and precision, probably due to the scarcity of the minority class.</p>
<p>However, in order to learn from the dataset, I applied Feature Selection techniques to deduct what features are useful in predicting the target variable and see if this supports or contradicts previous literature on the matter.</p>
<p>When I reran my models using the reduced dataset, this still resulted in 0% recall and precision. So the prediction performance has not improved using feature selection. But the features returned by the applying Feature Selection have given me more insight into the data.</p>
<p>So my question is, is the purpose of Feature Selection:</p>
<ul>
<li>to improve prediction performance</li>
<li>or can the purpose be identifying relevant features in the prediction and learning more about the dataset</li>
</ul>
<p>So in other words, is Feature Selection just a tool to achieve improved performance, or can it be an end in itself?</p>
",54,0,0,4,python;machine-learning;feature-selection;dimensionality-reduction,2022-05-26 15:00:18,2022-05-26 15:00:18,2022-06-21 17:13:19,i have a small medical dataset   samples  that contains only  cases of the condition i am trying to predict using machine learning  so far  the dataset is not proving useful for predicting the target variable and is resulting in models with   recall and precision  probably due to the scarcity of the minority class  however  in order to learn from the dataset  i applied feature selection techniques to deduct what features are useful in predicting the target variable and see if this supports or contradicts previous literature on the matter  when i reran my models using the reduced dataset  this still resulted in   recall and precision  so the prediction performance has not improved using feature selection  but the features returned by the applying feature selection have given me more insight into the data  so my question is  is the purpose of feature selection  so in other words  is feature selection just a tool to achieve improved performance  or can it be an end in itself 
127,127,19380434,72697115,Create a dataset for Machine learning using different range values of features,"<p>I'm working on a Soil modelling project. There is no public data available. I have the features and their values range to consider. I want to create a dataset using those ranges of different features and predict how fertile is a soil like whether it is high fertile soil, medium fertile soil or low fertile soil. I was thinking I could create a dataset using permutations or combinations of the features but now, I am unable do it. I request an approach for it.</p>
<p>Below are features and their ranges:</p>
<ol>
<li>Nitrogen(mg/kg):
&lt;90 : Low,
90-120 : Medium,
120-150 : Upper medium,</li>
</ol>
<blockquote>
<p>150 : High fertile;</p>
</blockquote>
<ol start=""2"">
<li>Phosphorous(mg/kg):
&lt;10 : Low,
10-20 : Medium,
20-40 : Upper medium,</li>
</ol>
<blockquote>
<p>40 : High fertile;</p>
</blockquote>
<ol start=""3"">
<li>Potassium(mg/kg):
&lt;100 : Low,
100-150 : Medium,
150-200 : Upper medium,</li>
</ol>
<blockquote>
<p>200 : High fertile;</p>
</blockquote>
<ol start=""4"">
<li>Organic carbon:
&lt;5 : Low,
5-7.5 : Medium,</li>
</ol>
<blockquote>
<p>7.5 : High;</p>
</blockquote>
<ol start=""5"">
<li><p>PH value:
&lt;6.5 (Acidic) : Harmful for plants to grow. Low fertile,
6.5 - 8.2 (Normal) : Fertile and suitable for plants to grow,
8.2 or high (Alkaline) : Harmful  for plants to grow. Low fertile;</p>
</li>
<li><p>E.C (Electricity conductivity:
&lt;1.0 : Normal and Good for plants to grow,
1.0 - 3.0 : Medium,
3.0 or high : Harmful  for plants to grow;</p>
</li>
<li><p>Soil moisture%:
&lt;20% : Low and not good for plants to grow,
20-60% : Normal and Good for plants to grow,</p>
</li>
</ol>
<blockquote>
<p>60% : High and not good for plants to grow;</p>
</blockquote>
<ol start=""8"">
<li>Soil temperature in Celsius:
&lt;20 : Low and not good for plants to grow,
20-30: Normal and Good for plants to grow,</li>
</ol>
<blockquote>
<p>30: High and not good for plants to grow.</p>
</blockquote>
",20,0,-1,3,machine-learning;data-science;data-collection,2022-06-21 13:23:08,2022-06-21 13:23:08,2022-06-21 16:34:09,i m working on a soil modelling project  there is no public data available  i have the features and their values range to consider  i want to create a dataset using those ranges of different features and predict how fertile is a soil like whether it is high fertile soil  medium fertile soil or low fertile soil  i was thinking i could create a dataset using permutations or combinations of the features but now  i am unable do it  i request an approach for it  below are features and their ranges     high fertile     high fertile     high fertile      high      high and not good for plants to grow    high and not good for plants to grow 
128,128,4313064,72698918,windows resource monitor shows all CPUs running at 70%+. So why in wsl2 does mpstat show all 97%+idle?,"<p>I am using a machine learning model to make predictions and wanted to see if it uses all 8 processors if I pass in batches of 8. On windows resource manager it shows graphs with each CPU at 70%+. However when I use mpstat in wsl2 linux it shows they are all 97%+ idle.</p>
<p>What is going on here?</p>
",9,0,0,3,linux;windows;cpu-usage,2022-06-21 15:36:34,2022-06-21 15:36:34,2022-06-21 15:36:34,i am using a machine learning model to make predictions and wanted to see if it uses all  processors if i pass in batches of   on windows resource manager it shows graphs with each cpu at     however when i use mpstat in wsl linux it shows they are all    idle  what is going on here 
129,129,1901071,72698110,Python translate matplotlib to a plotnine chart,"<p>I am currently working through the book <a href=""https://rads.stackoverflow.com/amzn/click/com/1492032646"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Hands On Machine Learning</a> and am trying to replicate a visualization where we plot the lat and lon co-ordinates on a scatter plot of San Diego. I have taken the plot code from the book which uses the code below (matplotlib method). I would like to replicate the same visualization using <a href=""https://plotnine.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">plotnine</a>. Could someone help me with the translation.</p>
<h3>matplotlib method</h3>
<pre><code># DATA INGEST -------------------------------------------------------------    
# Import the file from github
url = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv&quot; # Make sure the url is the raw version of the file on GitHub
download = requests.get(url).content

# Reading the downloaded content and turning it into a pandas dataframe
housing = pd.read_csv(io.StringIO(download.decode('utf-8')))

# Then plot
import matplotlib.pyplot as plt

# The size is now related to population divided by 100
# the colour is related to the median house value
housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, 
              s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7),
              c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True)
plt.legend()
plt.show()

</code></pre>
<h3>plotnine method</h3>
<pre><code>from plotnine import ggplot, geom_point, aes, stat_smooth, scale_color_cmap

# Lets try the same thing in ggplot
(ggplot(housing, aes('longitude', 'latitude', size = &quot;population&quot;, color = &quot;median_house_value&quot;))
 + geom_point(alpha = 0.1)
 + scale_color_cmap(name=&quot;jet&quot;))
 
</code></pre>
",19,0,0,4,python;matplotlib;ggplot2;plotnine,2022-06-21 14:40:43,2022-06-21 14:40:43,2022-06-21 14:40:43,i am currently working through the book  and am trying to replicate a visualization where we plot the lat and lon co ordinates on a scatter plot of san diego  i have taken the plot code from the book which uses the code below  matplotlib method   i would like to replicate the same visualization using   could someone help me with the translation 
130,130,18089331,72683969,How to identify potential fuzzy duplicates with spelling typos,"<p>My basic problem is as follows. I have data like these:</p>
<pre><code>PersonID;forename;name;birthday
1;Mike;Miller;11/29/1986
2;Peter;Stan;08/14/1984
3;Mike;Miler;11/29/1986
4;Liza;Johnson;11/29/1986
5;John;Doe;11/29/1986
6;Peter;Sten;08/14/1984
7;Joe;Doe;09/26/1954
8;Linda;Mueller;06/16/1982
9;Joanna;Guso;04/19/1990
10;John;Doé;11/29/1986
11;Max;Mueller;07/19/1981
12;Martina Elizabeth;McSmith;06/21/1982
13;Martina-Elizabeth,McSmith;06/21/1982
14;MartinaElizabeth;McSmith;06/21/1982
15;Martina Elisabeth;McSmith;06/21/1982
16;Luisann;Davis;02/10/1984
</code></pre>
<p>There are duplicate entries, in one case (Martina Elizabeth) you can see a special case where there are 4 entries, not just 2.</p>
<p>I am aiming at spelling typos in the name. It might be that there were also mistakes being made when the birthday was entered, but to be able to tackle this problem a bit better I made the assumption, to:</p>
<blockquote>
<p>search for cases, where the birthday is exact the same and the
difference between different names is exactly one letter (would be good actually to have it flexible and say at maximum &quot;n&quot; letters, but at least one letter different)</p>
</blockquote>
<p>My problem is that I thought this should be easy. However, it is not. When thinking about it, I came across this <a href=""https://maxhalford.github.io/blog/transitive-duplicates/"" rel=""nofollow noreferrer"">post</a>. So first of all I think that calling it fuzzy duplicates is actually better, as it points out that I am not just searching for exact duplictes, but instead where there is a small difference in the spelling. I said maximum one letter. Or could be also something like in ID 10 and 5 &quot;Doe&quot; and &quot;Doé&quot;. So it is not even a letter, just an accent being different.</p>
<p>In my specific case there is the birthday: This is one &quot;hard&quot; criteria, it must match. This has nothing to do with thinking about calculating a distance metric for estimating the difference between two entris - no - it just hast to match exactly.</p>
<p>With the name it is a bit more difficult. But the rule I made is quite simple: Exactly one letter difference (or flexible: maximum &quot;n&quot; letters, but at least one letter is different).</p>
<p>I thought I could easily somehow group up the names by birthday. This indeed gives me all different names for each birthday, however as my data is large, there will be many people on one birthday, so this does not mean that these entries are fuzzy duplicates. So my problem here is that as the article mentions too, I run into the fact that I think I have to compare each entry to all others.</p>
<p>My other problem is a methodological one: I also thought about machine learning (as the article also mentions) or other statistical approaches, like cluster the names and identify groups of same names or whatever. However, I actually do not have statistical inference here: The first rule with same birthday leaves no insecurness open: It is the case or not. The second one is actually also a quite hard rule. Although I must admit that regarding the difference between two names or the measure of equalness an approach like machine learning would probably lead to more flexible results. However, I dont' have an idea of how to apply machine learning here. How would I apply machine learning here in my case to takle the names problem?</p>
<p>I am aware that a letter is represented as a number, depending on the codes being used, like ANSI or whatever. How can I implement the rule of identifying records that differ in one letter? Especially taking into account cases like with the accent &quot;´&quot; or &quot;`&quot; or spaces or &quot;-&quot; or any others? I think this could be achieved by using the Levenshtein-distance, from the python-Levenshtein or also utilizing the difflib package. So removing an accent would be distance 1 and one letter difference would be distance 2, I think.</p>
<p>Besides these questions my main question of course is: How can I solve this problem, in order to get the desired output. The desired output would look like:</p>
<pre><code>1;Mike;Miller;11/29/1986
3;Mike;Miler;11/29/1986
2;Peter;Stan;08/14/1984
6;Peter;Sten;08/14/1984
5;John;Doe;11/29/1986
10;John;Doé;11/29/1986
12;Martina Elizabeth;McSmith;06/21/1982
13;Martina-Elizabeth,McSmith;06/21/1982
14;MartinaElizabeth;McSmith;06/21/1982
15;Martina Elisabeth;McSmith;06/21/1982
</code></pre>
<p>I am a bit lost here. One idea was to go through each day of birthday and for each day I somehow identify groups of names, where the difference is at maximum n letter (but at least one difference, so not the same) using the Levenshtein distance with result values 1 or 2. Each group on each day I extract and collect it into one table at the end. But I don't know how to make this &quot;grouping&quot; &quot;dynamically&quot;. There can be lots of groups per birthday, I expect.</p>
<p>(I am not interested in &quot;true&quot; duplicates. So I actually do not want to have these cases in my results, where the names or all information except the id is exactly the same.)</p>
",28,0,-1,2,python;duplicates,2022-06-20 13:36:42,2022-06-20 13:36:42,2022-06-21 13:55:16,my basic problem is as follows  i have data like these  there are duplicate entries  in one case  martina elizabeth  you can see a special case where there are  entries  not just   i am aiming at spelling typos in the name  it might be that there were also mistakes being made when the birthday was entered  but to be able to tackle this problem a bit better i made the assumption  to  my problem is that i thought this should be easy  however  it is not  when thinking about it  i came across this   so first of all i think that calling it fuzzy duplicates is actually better  as it points out that i am not just searching for exact duplictes  but instead where there is a small difference in the spelling  i said maximum one letter  or could be also something like in id  and   doe  and  doé   so it is not even a letter  just an accent being different  in my specific case there is the birthday  this is one  hard  criteria  it must match  this has nothing to do with thinking about calculating a distance metric for estimating the difference between two entris   no   it just hast to match exactly  with the name it is a bit more difficult  but the rule i made is quite simple  exactly one letter difference  or flexible  maximum  n  letters  but at least one letter is different   i thought i could easily somehow group up the names by birthday  this indeed gives me all different names for each birthday  however as my data is large  there will be many people on one birthday  so this does not mean that these entries are fuzzy duplicates  so my problem here is that as the article mentions too  i run into the fact that i think i have to compare each entry to all others  my other problem is a methodological one  i also thought about machine learning  as the article also mentions  or other statistical approaches  like cluster the names and identify groups of same names or whatever  however  i actually do not have statistical inference here  the first rule with same birthday leaves no insecurness open  it is the case or not  the second one is actually also a quite hard rule  although i must admit that regarding the difference between two names or the measure of equalness an approach like machine learning would probably lead to more flexible results  however  i dont  have an idea of how to apply machine learning here  how would i apply machine learning here in my case to takle the names problem  i am aware that a letter is represented as a number  depending on the codes being used  like ansi or whatever  how can i implement the rule of identifying records that differ in one letter  especially taking into account cases like with the accent     or     or spaces or     or any others  i think this could be achieved by using the levenshtein distance  from the python levenshtein or also utilizing the difflib package  so removing an accent would be distance  and one letter difference would be distance   i think  besides these questions my main question of course is  how can i solve this problem  in order to get the desired output  the desired output would look like  i am a bit lost here  one idea was to go through each day of birthday and for each day i somehow identify groups of names  where the difference is at maximum n letter  but at least one difference  so not the same  using the levenshtein distance with result values  or   each group on each day i extract and collect it into one table at the end  but i don t know how to make this  grouping   dynamically   there can be lots of groups per birthday  i expect   i am not interested in  true  duplicates  so i actually do not want to have these cases in my results  where the names or all information except the id is exactly the same  
131,131,13531898,72697204,Difference between roc_auc_score and cross_val_score(scoring=roc_auc),"<p>I'm learning machine learning and am not so clear about this.
I saw similar post in Stack Overflow but I may need your help little bit more to understand.</p>
<p>Code 1</p>
<pre><code>kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)
lrCV = LogisticRegression(class_weight='balanced')
scoring = 'roc_auc'
lr_results = model_selection.cross_val_score(lrCV, X_train, y_train, cv=kfold, scoring=scoring)
</code></pre>
<p>Result</p>
<pre><code>array([0.91374269, 0.70209059, 0.89164087, 0.8021978 , 0.85077519,
   0.80888889, 0.79338843, 0.76446281, 0.84803002, 0.74506579])
</code></pre>
<p>Code 2</p>
<pre><code>lrmodel = LogisticRegression(class_weight='balanced')
lrmodel.fit(X_train, y_train)
lr_auc = roc_auc_score(y_test, lrmodel.predict(X_test))
</code></pre>
<p>Result</p>
<pre><code>Logistic Regression AUC = 0.67
</code></pre>
<p>Obviously Code 2 is a lot lower than Code 1.
What is causing this difference? I'm new to ML and self-learning.
Please let me know little bit more about this.</p>
",30,1,0,4,python;machine-learning;scikit-learn;logistic-regression,2022-06-21 13:30:12,2022-06-21 13:30:12,2022-06-21 13:47:59,code  result code  result
132,132,13896017,72696668,only size-1 arrays : log function,"<pre><code># UNQ_C2
# GRADED FUNCTION: compute_cost
def compute_cost(X, y, w, b, lambda_= 1):
    &quot;&quot;&quot;
    Computes the cost over all examples
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (array_like Shape (m,)) target value 
      w : (array_like Shape (n,)) Values of parameters of the model      
      b : scalar Values of bias parameter of the model
      lambda_: unused placeholder
    Returns:
      total_cost: (scalar)         cost 
    &quot;&quot;&quot;

    m, n = X.shape
    
    ### START CODE HERE ###
    X = X.transpose()
    predictions = np.dot(w,X) + b
    g = sigmoid(predictions)
    error = (-y*np.math.log(predictions)-(1-y)*math.log(1-predictions))
    cost = 1/m * error
    
    ### END CODE HERE ### 

    return total_cost
</code></pre>
<pre><code>m, n = X_train.shape

# Compute and display cost with w initialized to zeroes
initial_w = np.zeros(n)
initial_b = 0.
cost = compute_cost(X_train, y_train, initial_w, initial_b)
print('Cost at initial w (zeros): {:.3f}'.format(cost))
</code></pre>
<p>Error</p>
<pre><code>&lt;ipython-input-134-a0c3de514c37&gt; in compute_cost(X, y, w, b, lambda_)
     21     g = sigmoid(predictions)
     22     print(np.dot(w,X))
---&gt; 23     error = (-y*np.math.log(predictions)-(1-y)*math.log(1-predictions))
     24     cost = 1/m * error
     25 

TypeError: only size-1 arrays can be converted to Python scalars
</code></pre>
<p>Currently doing a machine learning course assignment</p>
<p>Is passing array into log function causes the error ? Any idea what tool to use to fix this ? Tried searching the web.</p>
",27,0,0,1,numpy,2022-06-21 12:45:54,2022-06-21 12:45:54,2022-06-21 12:45:54,error currently doing a machine learning course assignment is passing array into log function causes the error   any idea what tool to use to fix this   tried searching the web 
133,133,19033502,72693278,build an email database with pywin32,"<p>I have to iterate through many shared folders within my company. I need to store all the strings of the messages in a database for a machine learning project.
I read in some threads that I should not iterate through all items using pywin32 and</p>
<pre><code>messages = mapi.Folders(str(key)).Folders(str(i)).Items
</code></pre>
<p>We are talking about 10 shared inboxes with each over 10.000 mails. When i tried to run my code for the first inbox it crashed after reaching 95% (according to tqdm) with this error code</p>
<blockquote>
<p>File
~.conda\envs\myenv\lib\site-packages\spyder_kernels\py3compat.py:356
in compat_exec
exec(code, globals, locals)</p>
<p>File c:\users\user\desktop\untitled0.py:206 in 
for m in tqdm(messages):</p>
<p>File ~.conda\envs\myenv\lib\site-packages\tqdm\std.py:1195 in
<strong>iter</strong>
for obj in iterable:</p>
<p>File
~\AppData\Roaming\Python\Python39\site-packages\win32com\client\dynamic.py:324
in <strong>getitem</strong>
return self.<em>get_good_object</em>(self.<em>enum</em>.<strong>getitem</strong>(index))</p>
<p>File
~\AppData\Roaming\Python\Python39\site-packages\win32com\client\util.py:41
in <strong>getitem</strong>
return self.__GetIndex(index)</p>
<p>File
~\AppData\Roaming\Python\Python39\site-packages\win32com\client\util.py:62
in __GetIndex
result = self.<em>oleobj</em>.Next(1)</p>
<p>com_error: (-459013867, 'OLE error 0xe4a40115', None, None)</p>
</blockquote>
<p>I don't understand this error and i can't find anything googling this error. So i thought I should just slice them just in smaller parts and run the code. But I can't slice the messages as I get the error code</p>
<pre><code>TypeError: Objects of type 'slice' can not be converted to a COM VARIANT
</code></pre>
<p>Is there any other way or even a more efficient way to do this task? Should I use instead of pywin32 any other libary? It seems like this is the most proficient libary in this field as it is from microsoft and should have the best compatibility with outlook. But going through 48000 emails of 50000 and then seeing this crash was not planned. How could I save my progress on the way? Can I just sort the messages first and then iterate and therefore commit my retrieved data to an sql database every 5000 emails?</p>
",24,1,0,4,python;outlook;pywin32;office-automation,2022-06-21 03:13:31,2022-06-21 03:13:31,2022-06-21 12:15:12,we are talking about  shared inboxes with each over   mails  when i tried to run my code for the first inbox it crashed after reaching    according to tqdm  with this error code com_error       ole error xea   none  none  i don t understand this error and i can t find anything googling this error  so i thought i should just slice them just in smaller parts and run the code  but i can t slice the messages as i get the error code is there any other way or even a more efficient way to do this task  should i use instead of pywin any other libary  it seems like this is the most proficient libary in this field as it is from microsoft and should have the best compatibility with outlook  but going through  emails of  and then seeing this crash was not planned  how could i save my progress on the way  can i just sort the messages first and then iterate and therefore commit my retrieved data to an sql database every  emails 
134,134,18651963,72695276,What is wrong with this simple read syntax?,"<p>This is from IBM python for data analyst. I am using Kaggle for python.
Its a simple function which is supposed to import data from the url into python and check if the data has been successfully imported by printing the first 5 rows of the data.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data&quot;
df = pd.read_csv(url, header = None)
Print(&quot;The First Five Rows Of Data are&quot;)
df.head(5)
</code></pre>
<p>The following error followed after a few different tries. I have no idea where to fix it.</p>
<pre><code>Traceback (most recent call last)
/opt/conda/lib/python3.7/urllib/request.py in do_open(self, http_class, req, **http_conn_args)
   1349                 h.request(req.get_method(), req.selector, req.data, headers,
-&gt; 1350                           encode_chunked=req.has_header('Transfer-encoding'))
   1351             except OSError as err: # timeout error

/opt/conda/lib/python3.7/http/client.py in request(self, method, url, body, headers, encode_chunked)
   1280         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;
-&gt; 1281         self._send_request(method, url, body, headers, encode_chunked)
   1282 

/opt/conda/lib/python3.7/http/client.py in _send_request(self, method, url, body, headers, encode_chunked)
   1326             body = _encode(body, 'body')
-&gt; 1327         self.endheaders(body, encode_chunked=encode_chunked)
   1328 

/opt/conda/lib/python3.7/http/client.py in endheaders(self, message_body, encode_chunked)
   1275             raise CannotSendHeader()
-&gt; 1276         self._send_output(message_body, encode_chunked=encode_chunked)
   1277 

/opt/conda/lib/python3.7/http/client.py in _send_output(self, message_body, encode_chunked)
   1035         del self._buffer[:]
-&gt; 1036         self.send(msg)
   1037 

/opt/conda/lib/python3.7/http/client.py in send(self, data)
    975             if self.auto_open:
--&gt; 976                 self.connect()
    977             else:

/opt/conda/lib/python3.7/http/client.py in connect(self)
   1442 
-&gt; 1443             super().connect()
   1444 

/opt/conda/lib/python3.7/http/client.py in connect(self)
    947         self.sock = self._create_connection(
--&gt; 948             (self.host,self.port), self.timeout, self.source_address)
    949         self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

/opt/conda/lib/python3.7/socket.py in create_connection(address, timeout, source_address)
    706     err = None
--&gt; 707     for res in getaddrinfo(host, port, 0, SOCK_STREAM):
    708         af, socktype, proto, canonname, sa = res

/opt/conda/lib/python3.7/socket.py in getaddrinfo(host, port, family, type, proto, flags)
    751     addrlist = []
--&gt; 752     for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
    753         af, socktype, proto, canonname, sa = res

gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

URLError                                  Traceback (most recent call last)
/tmp/ipykernel_33/185401550.py in &lt;module&gt;
      3 
      4 url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data&quot;
----&gt; 5 df = pd.read_csv(url, header = None)
      6 Print(&quot;The First Five Rows Of Data are&quot;)
      7 df.head(5)

/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    309                     stacklevel=stacklevel,
    310                 )
--&gt; 311             return func(*args, **kwargs)
    312 
    313         return wrapper

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    584     kwds.update(kwds_defaults)
    585 
--&gt; 586     return _read(filepath_or_buffer, kwds)
    587 
    588 

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)
    480 
    481     # Create the parser.
--&gt; 482     parser = TextFileReader(filepath_or_buffer, **kwds)
    483 
    484     if chunksize or iterator:

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)
    809             self.options[&quot;has_index_names&quot;] = kwds[&quot;has_index_names&quot;]
    810 
--&gt; 811         self._engine = self._make_engine(self.engine)
    812 
    813     def close(self):

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py in _make_engine(self, engine)
   1038             )
   1039         # error: Too many arguments for &quot;ParserBase&quot;
-&gt; 1040         return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
   1041 
   1042     def _failover_to_python(self):

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py in __init__(self, src, **kwds)
     49 
     50         # open handles
---&gt; 51         self._open_handles(src, kwds)
     52         assert self.handles is not None
     53 

/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py in _open_handles(self, src, kwds)
    227             memory_map=kwds.get(&quot;memory_map&quot;, False),
    228             storage_options=kwds.get(&quot;storage_options&quot;, None),
--&gt; 229             errors=kwds.get(&quot;encoding_errors&quot;, &quot;strict&quot;),
    230         )
    231 

/opt/conda/lib/python3.7/site-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
    612         compression=compression,
    613         mode=mode,
--&gt; 614         storage_options=storage_options,
    615     )
    616 

/opt/conda/lib/python3.7/site-packages/pandas/io/common.py in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)
    310         # assuming storage_options is to be interpreted as headers
    311         req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
--&gt; 312         with urlopen(req_info) as req:
    313             content_encoding = req.headers.get(&quot;Content-Encoding&quot;, None)
    314             if content_encoding == &quot;gzip&quot;:

/opt/conda/lib/python3.7/site-packages/pandas/io/common.py in urlopen(*args, **kwargs)
    210     import urllib.request
    211 
--&gt; 212     return urllib.request.urlopen(*args, **kwargs)
    213 
    214 

/opt/conda/lib/python3.7/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    220     else:
    221         opener = _opener
--&gt; 222     return opener.open(url, data, timeout)
    223 
    224 def install_opener(opener):

/opt/conda/lib/python3.7/urllib/request.py in open(self, fullurl, data, timeout)
    523             req = meth(req)
    524 
--&gt; 525         response = self._open(req, data)
    526 
    527         # post-process response

/opt/conda/lib/python3.7/urllib/request.py in _open(self, req, data)
    541         protocol = req.type
    542         result = self._call_chain(self.handle_open, protocol, protocol +
--&gt; 543                                   '_open', req)
    544         if result:
    545             return result

/opt/conda/lib/python3.7/urllib/request.py in _call_chain(self, chain, kind, meth_name, *args)
    501         for handler in handlers:
    502             func = getattr(handler, meth_name)
--&gt; 503             result = func(*args)
    504             if result is not None:
    505                 return result

/opt/conda/lib/python3.7/urllib/request.py in https_open(self, req)
   1391         def https_open(self, req):
   1392             return self.do_open(http.client.HTTPSConnection, req,
-&gt; 1393                 context=self._context, check_hostname=self._check_hostname)
   1394 
   1395         https_request = AbstractHTTPHandler.do_request_

/opt/conda/lib/python3.7/urllib/request.py in do_open(self, http_class, req, **http_conn_args)
   1350                           encode_chunked=req.has_header('Transfer-encoding'))
   1351             except OSError as err: # timeout error
-&gt; 1352                 raise URLError(err)
   1353             r = h.getresponse()
   1354         except:

URLError: &lt;urlopen error [Errno -3] Temporary failure in name resolution&gt;
</code></pre>
",36,0,0,3,python;pandas;import,2022-06-21 09:46:50,2022-06-21 09:46:50,2022-06-21 10:26:56,the following error followed after a few different tries  i have no idea where to fix it 
135,135,19373273,72684156,How to predict present and absent without class label based on another dataset with class label with present and absent,"<p>I have two datasets (point locations): one data set containing point locations of a weed collected from the field and has class label present and absent while the other dataset was randomly generated in the R program. I want to assign class label to the points generated in the R program using information from the field data so that I can combine both dataset to model weed presence and absence. Both the dataset has environmental variables but as said above, the only difference is the  points generated in R do not have class label. I have explored resources from the net but I did not find anything that meets my need. All the machine learning or modelling required labelled classes in both training and testing datasets. So any helping hand would be highly appreciated as am stuck to move forward. Thanks a lot in advance.
<a href=""https://i.stack.imgur.com/Z0yqg.png"" rel=""nofollow noreferrer"">Complete dataset</a>
<a href=""https://i.stack.imgur.com/bk4y3.png"" rel=""nofollow noreferrer"">Incompplete datset</a>
I have included two excel sheets containing data. First sheet contains complete data while the second sheet has PresentAbsent and Code column empty. So I want to fill up using the information from the first sheet</p>
",24,0,0,1,r,2022-06-20 13:52:41,2022-06-20 13:52:41,2022-06-21 05:02:10,
136,136,2512677,72693190,Using sudo inside a bash script: permission denied,"<p>This is my first real bash script, called makevhost, and I'm running into a permissions error when trying to use sudo to execute commands in the script. A site name is supplied as an argument to the script call in the CLI. The file is executable and the bin directory in which it lives is added to $PATH.</p>
<p>The concept is simple: It creates the directories and sample templates for a web site on my apache development server (Debian 11 VM on a Windows host), then creates the virtual host file and restarts the server. I've tried all the suggestions <a href=""https://www.diskinternals.com/linux-reader/sudo-in-bash-script/"" rel=""nofollow noreferrer"">here</a>, most of which I found repeated on numerous sites and forums.</p>
<p>When I run the script it seems to run fine until line 31, where I get a &quot;Permission Denied&quot; error. I get another error on line 43, but the permissions error is what I'm working on now.</p>
<p>Here's my code:</p>
<pre><code>#!/bin/bash

if mkdir -p /var/www/&quot;$1&quot;/public_html; then
    echo &quot;Creating site directories ...&quot;
    cd /var/www/&quot;$1&quot;/public_html
    mkdir style images js
    chown -R $USER:$USER /var/www/&quot;$1&quot;
else
    echo &quot;Could not create site directories&quot;
    exit
fi

echo &quot;Creating index.html and style.css&quot;
cat &lt;&lt;- EOF &gt; index.html
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en-us&quot;&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;style/style.css&quot;&gt;&lt;/link&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;/body&gt;
&lt;/html&gt;
EOF

touch style/style.css
echo &quot;Site files created&quot;

sudo cat &lt;&lt;- EOF &gt; /etc/apache2/sites-available/&quot;$1&quot;.com.conf
&lt;VirtualHost *:8080&gt;
    ServerAdmin eric@sabresong.com
    ServerName sabresong.local
    ServerAlias &quot;$1&quot;
    DocumentRoot /var/www/&quot;$1&quot;/public_html
    ErrorLog ${APACHE_LOG_DIR}/error.log
    CustomLog ${APACHE_LOG_DIR}/access.log combined
&lt;/VirtualHost&gt;
EOF
echo &quot;Virtual Host File created.&quot;

if [ sudo a2ensite &quot;$1&quot;.com.conf ]; then
    echo &quot;$1 enabled&quot;
    sudo a2dissite 000-default.conf
    echo &quot;Restarting Apache&quot;
    sudo systemctl restart apache2
    echo &quot;Finished!&quot;
else
    echo 'WARNING! Could not enable new site &quot;$1&quot;'
fi
</code></pre>
<p>My /etc/sudoers.d/eric has only one line: <code>eric ALL=(root) NOPASSWD: /home/eric/bin/</code></p>
<p>In visudo, I have added this: <code>eric ALL=(ALL:ALL) NOPASSWD: /home/eric/bin/</code>
I also tried that same line with ALL=(ALL)</p>
<p>I know that using NOPASSWD isn't best practice, but I'm the only person with access to ths machine, and I haven't yet worked out how to use the sudo password for specific lines in the script but not for others, so I'm going this route, mostly for the learning experience.</p>
",66,1,0,3,linux;bash;permissions,2022-06-21 03:00:44,2022-06-21 03:00:44,2022-06-21 03:22:13,this is my first real bash script  called makevhost  and i m running into a permissions error when trying to use sudo to execute commands in the script  a site name is supplied as an argument to the script call in the cli  the file is executable and the bin directory in which it lives is added to  path  the concept is simple  it creates the directories and sample templates for a web site on my apache development server  debian  vm on a windows host   then creates the virtual host file and restarts the server  i ve tried all the suggestions   most of which i found repeated on numerous sites and forums  when i run the script it seems to run fine until line   where i get a  permission denied  error  i get another error on line   but the permissions error is what i m working on now  here s my code  my  etc sudoers d eric has only one line  eric all  root  nopasswd   home eric bin  i know that using nopasswd isn t best practice  but i m the only person with access to ths machine  and i haven t yet worked out how to use the sudo password for specific lines in the script but not for others  so i m going this route  mostly for the learning experience 
137,137,887074,72692364,Dynamically define and run an Airflow dag and immediately run locally,"<p>I'm learning airflow and I'm trying to figure out how to run the simplest possible example where a dag is defined and run end-to-end from Python.</p>
<p>I was originally following the doc tutorial</p>
<p><a href=""https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html"" rel=""nofollow noreferrer"">https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html</a></p>
<p>But I'm noticing that all of these tutorial are defining DAGs in a global scope and then using an external bash command to run it. This seems far more complicated than it needs to be (in the simplest case where everything is local).</p>
<p>What I want is something that is self-contained. I want to import airflow, define the dag, and execute it immediately. I'm OK if I have to start some worker daemon / scheduler on my local machine before I run my code, but what seems not OK is needing to write a custom to define that DAG and then have to execute it somewhere outside of that file.</p>
<p>I'm not sure if this is possible, perhaps it runs contrary to the design philosophy of airflow, but it feels like it should be possible and that I'm just making a simple mistake, or perhaps I didn't configure airflow correctly before running this.</p>
<p>The example I would like to get working is as follows:</p>
<pre><code>from airflow import DAG
from datetime import timezone
from datetime import datetime as datetime_cls
from airflow.operators.bash import BashOperator
now = datetime_cls.utcnow().replace(tzinfo=timezone.utc)
dag = DAG(
    'mycustomdag',
    start_date=now,
    catchup=False,
    tags=['example'],
)
t1 = BashOperator(task_id='task1', bash_command='date', dag=dag)
dag.run(verbose=True, local=True)
</code></pre>
<p>I'm running this directly in IPython, but the code might exist in some library function and get executed on the fly. The important feature is that no file with the sole purpose of defining this dag will exist on disk a-priori. I want to dynamically define the dag and run it.</p>
<p>This seems like it should be fine. But this results in an exception:</p>
<pre><code>BackfillUnfinished: Some task instances failed:
DAG ID       Task ID    Run ID                                        Try number
-----------  ---------  ------------------------------------------  ------------
mycustomdag  task1      backfill__2022-06-20T19:51:38.686880+00:00    

     1
</code></pre>
<p>I'm ok if I need to pre-configure the runner. But I don't want to interact with this at all. I don't want to look at any UI. I just want to define and run the dag in Python. Is that possible or is that just now how airflow works?</p>
<h2>EDIT</h2>
<p>I'm thinking this might not be possible (which is absolutely crazy to me from a design perspective). In these docs (<a href=""https://airflow.apache.org/docs/apache-airflow/2.0.0/concepts.html#scope"" rel=""nofollow noreferrer"">https://airflow.apache.org/docs/apache-airflow/2.0.0/concepts.html#scope</a>) it seems to state that dags must be defined in a global scope.</p>
<p>Is there any way around this? Can you locally define a dag and then export it to a file?</p>
",24,0,0,2,airflow;airflow-scheduler,2022-06-21 01:27:26,2022-06-21 01:27:26,2022-06-21 01:27:26,i m learning airflow and i m trying to figure out how to run the simplest possible example where a dag is defined and run end to end from python  i was originally following the doc tutorial  but i m noticing that all of these tutorial are defining dags in a global scope and then using an external bash command to run it  this seems far more complicated than it needs to be  in the simplest case where everything is local   what i want is something that is self contained  i want to import airflow  define the dag  and execute it immediately  i m ok if i have to start some worker daemon   scheduler on my local machine before i run my code  but what seems not ok is needing to write a custom to define that dag and then have to execute it somewhere outside of that file  i m not sure if this is possible  perhaps it runs contrary to the design philosophy of airflow  but it feels like it should be possible and that i m just making a simple mistake  or perhaps i didn t configure airflow correctly before running this  the example i would like to get working is as follows  i m running this directly in ipython  but the code might exist in some library function and get executed on the fly  the important feature is that no file with the sole purpose of defining this dag will exist on disk a priori  i want to dynamically define the dag and run it  this seems like it should be fine  but this results in an exception  i m ok if i need to pre configure the runner  but i don t want to interact with this at all  i don t want to look at any ui  i just want to define and run the dag in python  is that possible or is that just now how airflow works  i m thinking this might not be possible  which is absolutely crazy to me from a design perspective   in these docs    it seems to state that dags must be defined in a global scope  is there any way around this  can you locally define a dag and then export it to a file 
138,138,12164652,72660715,Streamlit with Tensorflow to analyse image and return the probability if is positive or negative,"<p>I'm trying to use Tensorflow to Machine Learning to analyze an image and return the probability if is positive or negative based on a model created (extension .h5). I couldn't found a documentation exactly for that, or repository, so even a link to read will be awesome.</p>
<p>Link for the application: <a href=""https://share.streamlit.io/felipelx/hackathon/IDC_Detector.py"" rel=""nofollow noreferrer"">https://share.streamlit.io/felipelx/hackathon/IDC_Detector.py</a></p>
<p>Libraries that I'm trying to use.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import streamlit as st
import tensorflow as tf
from keras.models import load_model
</code></pre>
<p>The function to load the model.</p>
<pre class=""lang-py prettyprint-override""><code>@st.cache(allow_output_mutation=True)
def loadIDCModel():
  model_idc = load_model('models/IDC_model.h5', compile=False)
  model_idc.summary()
  return model_idc
</code></pre>
<p>The function to work the image, and what I'm trying to see: model.predict - I can see but is not updating the %, independent of the image the value is always the same.</p>
<pre class=""lang-py prettyprint-override""><code>if uploaded_file is not None:
    # transform image to numpy array
    file_bytes = tf.keras.preprocessing.image.load_img(uploaded_file, target_size=(96,96), grayscale = False, interpolation = 'nearest', color_mode = 'rgb', keep_aspect_ratio = False)
    
    c.image(file_bytes, channels=&quot;RGB&quot;)

    Genrate_pred = st.button(&quot;Generate Prediction&quot;)    
    if Genrate_pred:
         model = loadMetModel()
        input_arr = tf.keras.preprocessing.image.img_to_array(file_bytes)
        input_arr = np.array([input_arr])
        probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
        prediction = probability_model.predict(input_arr)
        dict_pred = {0: 'Benigno/Normal', 1: 'Maligno'}
        result = dict_pred[np.argmax(prediction)]
        value = 0
        if result == 'Benigno/Normal':
            value = str(((prediction[0][0])*100).round(2)) + '%'
        else:
            value = str(((prediction[0][1])*100).round(2)) + '%'
        
        c.metric('Predição', result, delta=value, delta_color='normal')
</code></pre>
<p>Thank you in advance to any help.</p>
",35,1,0,2,tensorflow;streamlit,2022-06-17 19:42:40,2022-06-17 19:42:40,2022-06-21 01:10:49,i m trying to use tensorflow to machine learning to analyze an image and return the probability if is positive or negative based on a model created  extension  h   i couldn t found a documentation exactly for that  or repository  so even a link to read will be awesome  link for the application   libraries that i m trying to use  the function to load the model  the function to work the image  and what i m trying to see  model predict   i can see but is not updating the    independent of the image the value is always the same  thank you in advance to any help 
139,139,15482551,72692171,Cross-validation using when using the Gaussian Naive Bayes model,"<p>Well, I am trying to solve this clustering problem that involves the <strong>Gaussian Naive-Bayes</strong> algorithm.</p>
<p><strong>Question:</strong>
Classification
Consider the data in the file - link below. Train the algorithm Gaussian Naive Bayes using the method of cross-validation holdout (Use the first 700 lines for the training set and the rest for the test set.) What is the accuracy of the training set? What is the accuracy of the test set? Do the same training with the method Leave-One-Out. What is the average accuracy for the training set? What is the average accuracy for the test set?</p>
<p>My solution that I am not sure about:</p>
<p>Basic Code (Full code in the Collab link below):</p>
<pre><code>#Using Holdout 
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred_train = classifier.predict(X_train)
cm0 = confusion_matrix(y_train, y_pred_train )
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))
print(accuracy_score(y_train, y_pred_train))
</code></pre>
<p><strong>My Answer for the holdout:</strong></p>
<pre><code>[[ 23  51]
 [ 21 205]]
0.76
0.7871428571428571
</code></pre>
<p>LOO:</p>
<pre><code>   #Using LOO
    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import LeaveOneOut
    #This is where I got the code: https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/
    cv = LeaveOneOut()
    accuracies = cross_val_score(estimator=classifier, X = X_train, y = y_pred_train, scoring='accuracy',cv=cv)
    print(f&quot;Accuracy Train {accuracies.mean()}&quot;)
    print(f&quot;Standard Deviation {accuracies.std()}&quot;)
    accuraciestest = cross_val_score(estimator=classifier, X = X_test, y = y_test, scoring='accuracy', cv=cv)
    print(f&quot;Accuracy Test {accuraciestest.mean()}&quot;)
    print(f&quot;Standard Deviation Test {accuraciestest.std()}&quot;)
</code></pre>
<p><strong>My Answer for the LeaveOneOut:</strong></p>
<pre><code>Accuracy Train 0.9771428571428571
Standard Deviation 0.1494479637785374
Accuracy Test 0.7433333333333333
Standard Deviation Test 0.43679387460092534
</code></pre>
<p><strong>Data</strong>:
<a href=""https://drive.google.com/file/d/1v9V-007yV3vVckPcQN0Q5VuNZYF_JjBW/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1v9V-007yV3vVckPcQN0Q5VuNZYF_JjBW/view?usp=sharing</a>
Colabs Link:  <a href=""https://colab.research.google.com/drive/1X68-Li6FacnAAQ4ASg3mmqdrU15v2ReP?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1X68-Li6FacnAAQ4ASg3mmqdrU15v2ReP?usp=sharing</a></p>
",13,0,0,5,python-3.x;machine-learning;data-science;artificial-intelligence;leave-one-out,2022-06-21 01:07:15,2022-06-21 01:07:15,2022-06-21 01:07:15,well  i am trying to solve this clustering problem that involves the gaussian naive bayes algorithm  my solution that i am not sure about  basic code  full code in the collab link below   my answer for the holdout  loo  my answer for the leaveoneout 
140,140,17018236,72686828,Read PGM files in python,"<p>I want to read pgm files for glasses detection in Machine Learning
Right now I Import pgm_reader</p>
<pre><code>from pgm_reader import Reader
</code></pre>
<p>then I Import os  and defiend a Series for save Images into and I tried to read files like this</p>
<pre><code>import os
reader = Reader()
Faces = pd.Series()
files_name = os.listdir('Faces/')
for file_name in files_name:
    file_name = 'Faces/' + file_name
    Faces.add(reader.read_pgm(file_name))
Faces
</code></pre>
<p>But I had this Error</p>
<pre><code> not enough values to unpack (expected 2, got 1)
</code></pre>
",16,1,0,4,python;machine-learning;face-detection;pgm,2022-06-20 17:46:02,2022-06-20 17:46:02,2022-06-21 00:00:17,then i import os  and defiend a series for save images into and i tried to read files like this but i had this error
141,141,10624978,72690583,Detect position of a word or number in a sentence with machine learning,"<p>I'm trying to come up with an ML model to determine the position of a word or number in a given sentence. I already have a multilabel model which detects sensitive data in a sentence. After I categorized my sentence I would like to learn the position of that word in a sentence in order to redact it for example.
Example:
My name is Bob and this is my telephone number: 0123456789. Have a nice day.</p>
<p>In this example, I would like to redact only the telephone number and keep the remaining content as it is.</p>
<p>I tried doing a multilabel approach where the labels are the positions. But that does not seem like a viable solution.</p>
<p>Preferably I want to use sci-kit-learn/tensorflow with supervised learning if possible.</p>
",35,0,-1,4,python;tensorflow;machine-learning;scikit-learn,2022-06-20 22:31:58,2022-06-20 22:31:58,2022-06-20 23:00:42,in this example  i would like to redact only the telephone number and keep the remaining content as it is  i tried doing a multilabel approach where the labels are the positions  but that does not seem like a viable solution  preferably i want to use sci kit learn tensorflow with supervised learning if possible 
142,142,19369584,72677862,Client Cancelled Exception when running Apache FlinkRunner via HuggingFace Datasets,"<p>I am trying to download wikipedia corpora through HuggingFace's Datasets library. Most of the languages I've needed have downloaded successfully, but I cannot seem to get Cebuano, Spanish, or Russian to finish downloading. Here is an example of how I'm trying to do this (Colab Notebook):</p>
<pre class=""lang-py prettyprint-override""><code>!pip install datasets
!pip install apache_beam
!pip install mwparserfromhell

import os
from datasets import load_dataset
import apache_beam as beam
import mwparserfromhell
from google.colab import drive

drive_dir = os.path.join(os.getcwd(), 'drive')
drive.mount(drive_dir)

lang = 'ru' # or 'ceb' or 'es'

lang_dir = os.path.join(drive_dir, 'path/to/training/dir', lang)
if not os.path.exists(lang_dir):
  x = load_dataset('wikipedia', '20220301.' + lang, beam_runner='Flink',
                   split='train')
  x.save_to_disk(lang_dir)
</code></pre>
<p>I have also tried playing with the versions of <code>dill</code>, <code>requests</code>, and <code>protobuf</code> because the <code>pip install</code> commands will often produce warnings about compatibility issues. However, the main problem I am encountering is that the above code will run with only a few warnings produced (these warnings were produced for other languages which downloaded successfully) for several hours before suddenly producing the following error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
[/tmp/ipykernel_219/3869142325.py](https://localhost:8080/#) in &lt;module&gt;
     18   x = None
     19   x = load_dataset('wikipedia', '20220301.' + lang, beam_runner='Flink',
---&gt; 20                    split='train')
     21   x.save_to_disk(lang_dir)

3 frames
[/usr/local/lib/python3.7/dist-packages/apache_beam/runners/portability/portable_runner.py](https://localhost:8080/#) in wait_until_finish(self, duration)
    604 
    605     if self._runtime_exception:
--&gt; 606       raise self._runtime_exception
    607 
    608     return self._state

RuntimeError: Pipeline BeamApp-root-0618220708-b3b59a0e_d8efcf67-9119-4f76-b013-70de7b29b54d failed in state FAILED: org.apache.beam.vendor.grpc.v1p43p2.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
</code></pre>
<p>I have no idea why this is happening. I thought about submitting a ticket to Apache Beam on GitHub, but they are overrun with open issues and will likely never touch my ticket in time (I'm using these datasets for an internship project).</p>
<p>The documentation for Datasets is somewhat above my pay grade as I'm new to machine learning as well as the tools Datasets relies on. I have tried to find a way to specify more specific pipeline options, but I can't seem to figure out what to do beyond choosing a runner and letting Datasets decide the rest. I want to be clear that the actual operation of the pipeline is totally abstracted from me as far as I can tell, and I'm not sure if I have control over that here.</p>
<p>Any help is appreciated, and I would even be grateful if someone could direct me to a repo where these datasets have already been downloaded/cleaned via the Datasets library. Thank you!</p>
",38,0,0,5,python;apache-flink;apache-beam;dill;huggingface-datasets,2022-06-19 20:21:41,2022-06-19 20:21:41,2022-06-20 22:48:57,i am trying to download wikipedia corpora through huggingface s datasets library  most of the languages i ve needed have downloaded successfully  but i cannot seem to get cebuano  spanish  or russian to finish downloading  here is an example of how i m trying to do this  colab notebook   i have also tried playing with the versions of dill  requests  and protobuf because the pip install commands will often produce warnings about compatibility issues  however  the main problem i am encountering is that the above code will run with only a few warnings produced  these warnings were produced for other languages which downloaded successfully  for several hours before suddenly producing the following error  i have no idea why this is happening  i thought about submitting a ticket to apache beam on github  but they are overrun with open issues and will likely never touch my ticket in time  i m using these datasets for an internship project   the documentation for datasets is somewhat above my pay grade as i m new to machine learning as well as the tools datasets relies on  i have tried to find a way to specify more specific pipeline options  but i can t seem to figure out what to do beyond choosing a runner and letting datasets decide the rest  i want to be clear that the actual operation of the pipeline is totally abstracted from me as far as i can tell  and i m not sure if i have control over that here  any help is appreciated  and i would even be grateful if someone could direct me to a repo where these datasets have already been downloaded cleaned via the datasets library  thank you 
143,143,17265513,72690606,How to use machine learning to generate metadata for text,"<p>Ideally I'd like to have a machine learning model to look at a textbook and determine information like the author's name, the date it was published, the title of the book, and what the subject is (math, biology, physics, etc.)</p>
<p>The text will be .txt files. I don't mind using any existing packages or algorithms. I'm using Python if that helps.</p>
",15,0,-2,2,python;machine-learning,2022-06-20 22:33:50,2022-06-20 22:33:50,2022-06-20 22:33:50,ideally i d like to have a machine learning model to look at a textbook and determine information like the author s name  the date it was published  the title of the book  and what the subject is  math  biology  physics  etc   the text will be  txt files  i don t mind using any existing packages or algorithms  i m using python if that helps 
144,144,15773956,72687494,Object while writing a pickle file shows it&#39;s not defined,"<p>I trying to write a pickle file from a machine learning model . It uses RandomForest classifier .When I try to write the model into a pickle file,it shows the object forest isn't defined</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
#Function for  different models
def models(X_train,Y_train):

    #Random forest classifier
    forest=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)
    forest.fit(X_train,Y_train)

  

    #Printing accuracy
    print(&quot;Random Forest:&quot;,forest.score(X_train,Y_train))
    
    return forest


#Testing Function for all models
print(&quot;Accuracy&quot;)
model=models(X_train,Y_train)

import pickle
model = pickle.dump(forest,open(&quot;model.pkl&quot;,&quot;wb&quot;))
</code></pre>
<p>Did I put the wrong object ?</p>
",23,0,0,3,python;machine-learning;jsonpickle,2022-06-20 18:34:12,2022-06-20 18:34:12,2022-06-20 22:15:37,i trying to write a pickle file from a machine learning model   it uses randomforest classifier  when i try to write the model into a pickle file it shows the object forest isn t defined did i put the wrong object  
145,145,19373791,72684896,How does one deal with too much missing values in a dataset for machine learning?,"<p>I have a column that can prove to be very important for the machine learning model I need. I have approx. 20% rows of a not too large dataset missing that column. I have tried filling it in with other values but there is no way to fill it in with reasonable values.</p>
",19,1,-1,3,machine-learning;classification;missing-data,2022-06-20 15:00:06,2022-06-20 15:00:06,2022-06-20 21:48:06,i have a column that can prove to be very important for the machine learning model i need  i have approx    rows of a not too large dataset missing that column  i have tried filling it in with other values but there is no way to fill it in with reasonable values 
146,146,17846129,72687989,Where is the React State stored?,"<p>I'm learning React and find the state intriguing.  I have some questions regarding React states.</p>
<ol>
<li><p>Where is the redux storage stored ?  Is only it in the browser ?  Would it be harden to disk ?</p>
</li>
<li><p>What is the lifecycle of the storage ?  With each app begin and closing of the browser ?   Can it live beyond a browser restart ?   Or over a machine reboot ?</p>
</li>
<li><p>Can multiple instance of the same browser (eg firefox) access the same state object ?  What about different browser (ff and chrome) ?</p>
</li>
<li><p>I understand that the state object is recreated everytime the state is updated.  Would that cause corruptions ?  eg if I use multiple useEffect and they all trigger at the same time.  Are they serialised ?  Do I have to take into programming consideration if they are serialised ?</p>
</li>
<li><p>Is the state object accessible by react programs only ?  Could other languages also access the store ?</p>
</li>
<li><p>Is redux state a feature solely by react or react is just making use of the browser functionality ?</p>
</li>
<li><p>I read somewhere that react redux is superceded by react context.  Does it mean I should move away from redux if I write new programs ?  (btw, I just read about context, not sure how to use it yet)</p>
</li>
</ol>
<p>Thanks.</p>
",40,1,-2,1,reactjs,2022-06-20 19:08:28,2022-06-20 19:08:28,2022-06-20 20:44:12,i m learning react and find the state intriguing   i have some questions regarding react states  where is the redux storage stored    is only it in the browser    would it be harden to disk   what is the lifecycle of the storage    with each app begin and closing of the browser     can it live beyond a browser restart     or over a machine reboot   can multiple instance of the same browser  eg firefox  access the same state object    what about different browser  ff and chrome    i understand that the state object is recreated everytime the state is updated   would that cause corruptions    eg if i use multiple useeffect and they all trigger at the same time   are they serialised    do i have to take into programming consideration if they are serialised   is the state object accessible by react programs only    could other languages also access the store   is redux state a feature solely by react or react is just making use of the browser functionality   i read somewhere that react redux is superceded by react context   does it mean i should move away from redux if i write new programs     btw  i just read about context  not sure how to use it yet  thanks 
147,147,17861539,72616017,How would I retrieve real-time sensory data and run it on a machine learning algorithm,"<p>I am currently using Spyder for this problem, I have reached to a point where I can harness the data from the sensors and a model could predict the accuracy based on my pre-recorded sessions.
I would like to know how would this trained model be used to predict incoming live data. My training dataset contains 7 columns in which 5(input) are sensory data and the remaining 2 are what the model would predict based on the input columns.
I have scoured the internet for any relatable example that would help my case, but haven't found anything useful yet, or maybe I have been searching the wrong keywords, any useful guidance would be appreciated.</p>
<p>I will attach a portion of my training data set so to better understand my problem.</p>
<p><img src=""https://i.stack.imgur.com/fxtTG.png"" alt=""I will attach a portion of my training data set so to better understand my problem."" /></p>
",35,1,0,3,python;spyder;live,2022-06-14 16:45:27,2022-06-14 16:45:27,2022-06-20 20:37:39,i will attach a portion of my training data set so to better understand my problem  
148,148,16958680,72688105,Asterisk on filename after using rsync,"<p>I'm learning to copy/sync a directory on my server to my local machine. I have a large amount of live files I need to copy and really can't screw it up...</p>
<p>Locally, I run:</p>
<pre><code>rsync -avzhe ssh --progress username@domain.com:/var/www/live/images/logos .
</code></pre>
<p>This works and everything is fine. I run <code>ll</code> to compare and the only different is an asterisk after the filename...</p>
<pre><code>chris@Charlie:/mnt/c/Users/chris/MA_Images/logos$ ll
total 4
drwxrwxrwx 1 chris chris  512 Dec 29 20:36 ./
drwxrwxrwx 1 chris chris  512 Jun 20 09:26 ../
-rwxrwxrwx 1 chris chris 3619 Dec 29 20:36 dummylogo.gif*
</code></pre>
<p>It happens regardless of what the file is (it's not just that dummylogo.gif file). Is this because I used rsync? I really want to be sure these files are synced identically. Could someone please explain what is going on with this filename? Am I using rsync incorrectly?</p>
<p>I'm running WSL2 locally (Windows 11) if that matters</p>
",28,0,-1,3,bash;windows-subsystem-for-linux;rsync,2022-06-20 19:17:09,2022-06-20 19:17:09,2022-06-20 19:24:57,i m learning to copy sync a directory on my server to my local machine  i have a large amount of live files i need to copy and really can t screw it up    locally  i run  this works and everything is fine  i run ll to compare and the only different is an asterisk after the filename    it happens regardless of what the file is  it s not just that dummylogo gif file   is this because i used rsync  i really want to be sure these files are synced identically  could someone please explain what is going on with this filename  am i using rsync incorrectly  i m running wsl locally  windows   if that matters
149,149,17540557,72687426,Sentiment Analysis on Twitter data,"<p>I would like to do sentiment analysis on Tweets. The algorithm should serve as a background check if we want to hire someone, but it can also give us a general feeling for customers. Say you launch a product and want to know how customers feel about it. In the end, I would like to have a score, e.g., Person X has a score of 100, meaning her background is without any scandals.</p>
<p>I thought about three models that might solve this problem:</p>
<ul>
<li>Naive Bayes</li>
<li>Support Vector Machine</li>
<li>Rule-Based Algorithm</li>
</ul>
<p>What are the theoretical pros and cons to each model? Why might one of these be better for this type of problem? I'm new to machine learning, so what I'd like to understand is why one might do better.</p>
",17,0,-2,4,python;text;nlp;sentiment-analysis,2022-06-20 18:29:13,2022-06-20 18:29:13,2022-06-20 18:29:13,i would like to do sentiment analysis on tweets  the algorithm should serve as a background check if we want to hire someone  but it can also give us a general feeling for customers  say you launch a product and want to know how customers feel about it  in the end  i would like to have a score  e g   person x has a score of   meaning her background is without any scandals  i thought about three models that might solve this problem  what are the theoretical pros and cons to each model  why might one of these be better for this type of problem  i m new to machine learning  so what i d like to understand is why one might do better 
150,150,19182674,72686597,How to correctly compute target variable for stock price prediction 1-day 14-days and 30-days ahead?,"<p>I am trying to predict stock price movement using different machine learning algorithms with various technical indicators as features. I intend to predict whether the stock price will go up or down 1-day ahead 14-days ahead and 30-days ahead.</p>
<p>I am a little bit confused about how to compute the target variables to make the predictions correctly.</p>
<p>So far I have computed daily returns for each firm and constructed a class variable to predict 1-day ahead.</p>
<pre><code>data &lt;- data %&gt;% group_by(company) %&gt;% mutate(ret =(`CLOSING PRICE` / lag(`CLOSING PRICE`)-1))
data$class &lt;- ifelse((data$ret) &gt;= 0, &quot;Up, &quot;Down&quot;)
</code></pre>
<p>The problem now is that I am not sure how to properly make predictions 14 and 30 days ahead.</p>
<p>The accuracy of all the models (SVM, RF, and DT) is very similar, around 82-85%, for 1-day ahead predictions. Is this something to be concerned about or is it logical that the accuracy is very similar for all the models?</p>
",27,1,0,4,r;machine-learning;classification;lag,2022-06-20 17:15:05,2022-06-20 17:15:05,2022-06-20 18:25:59,i am trying to predict stock price movement using different machine learning algorithms with various technical indicators as features  i intend to predict whether the stock price will go up or down  day ahead  days ahead and  days ahead  i am a little bit confused about how to compute the target variables to make the predictions correctly  so far i have computed daily returns for each firm and constructed a class variable to predict  day ahead  the problem now is that i am not sure how to properly make predictions  and  days ahead  the accuracy of all the models  svm  rf  and dt  is very similar  around     for  day ahead predictions  is this something to be concerned about or is it logical that the accuracy is very similar for all the models 
151,151,16939886,72682981,"Machine learning model on (X,Y) plot data set of different sizes","<p>I want to train a machine learning model to take an input of (X, Y) points that when graphed make the rough shape of a square. After that, I want to supply two (X, Y) that will be used for training that are the optimal points of interest I want the machine learning model to learn.</p>
<p>My question is each set of (X, Y) points will be of different lengths and to be trained on two (X, Y) points and output two (X, Y) points. Generally speaking, how should I go about this, what type of model should I use, and how do I deal with passing the model multiple lists of (X, Y) points each being different lengths?</p>
<p>Any advice or videos showing similar concepts would be very helpful. Thanks!</p>
",25,0,-2,4,python;tensorflow;machine-learning;plot,2022-06-20 11:54:25,2022-06-20 11:54:25,2022-06-20 13:27:54,i want to train a machine learning model to take an input of  x  y  points that when graphed make the rough shape of a square  after that  i want to supply two  x  y  that will be used for training that are the optimal points of interest i want the machine learning model to learn  my question is each set of  x  y  points will be of different lengths and to be trained on two  x  y  points and output two  x  y  points  generally speaking  how should i go about this  what type of model should i use  and how do i deal with passing the model multiple lists of  x  y  points each being different lengths  any advice or videos showing similar concepts would be very helpful  thanks 
152,152,3497444,72647709,Why GCloud Builds submit failing after creating image?,"<p>I am learning deploying a pubsub service to run under <code>Cloud Run</code>, by following the guidelines given <a href=""https://cloud.google.com/run/docs/tutorials/pubsub"" rel=""nofollow noreferrer"">here</a><br/>
Steps I followed are:<br/></p>
<ul>
<li><p>Created a new project folder &quot;myProject&quot; in my local machine</p>
</li>
<li><p>Added below files:<br/>
<code>app.js</code><br/><code>index.js</code><br/><code>Dockerfile</code></p>
</li>
<li><p>Executed below command to ship the code</p>
<pre><code>gcloud builds submit --tag gcr.io/Project-ID/pubsub
</code></pre>
</li>
</ul>
<p>It's mentioned in the tutorial document that</p>
<blockquote>
<p>Upon success, you should see a SUCCESS message containing the ID, creation time, and image name. The image is stored in Container Registry and can be re-used if desired.</p>
</blockquote>
<p>But in my case it's returning with error: (Ref: screenshot)
<a href=""https://i.stack.imgur.com/X45uA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X45uA.png"" alt=""command failure with error"" /></a>
<br/><br/>I have verified the build logs, <strong>&quot;It's success&quot;</strong>
<a href=""https://i.stack.imgur.com/AVHQU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AVHQU.png"" alt=""Build logs"" /></a></p>
<p>So I thought to ignore this error and proceed with the next step to deploy the app by running the command:</p>
<pre><code>gcloud run deploy sks-pubsub-cloudrun --image gcr.io/Project-ID/pubsub  --no-allow-unauthenticated
</code></pre>
<p>When I run this command it immediately asking to specify the region (26 is my choice) from the list.
Next it fails with error:</p>
<blockquote>
<p>Deploying container to Cloud Run service [sks-pubsub-cloudrun] in project [Project-ID] region [us-central1]
Deploying new service... Cloud Run error: The user-provided container failed to start and listen on the port defined provided by the PORT=8080 environment variable.
Logs for this revision might contain more information.</p>
</blockquote>
<p>As I am new to this GCP &amp; Dockerizing services, not understanding this issue and unable to fix it. I researched many blogs and articles yet no proper solution for this error.</p>
<p>Any help will be appreciated.</p>
<p>Tried to run the container locally and it's failing with error.</p>
<p>I'm using VS Code IDE, and <code>&quot;Cloud Code: Debug on Cloud Run Emulator&quot;</code> to debug the code.</p>
<pre><code>    Starting to debug the app using configuration `Cloud Run: Run/Debug Locally` from .vscode/launch.json
To view more detailed logs, go to Output channel : &quot;Cloud Run: Run/Debug Locally - Detailed&quot;
Dependency check started
Dependency check succeeded
Unpausing minikube
The minikube profile 'cloud-run-dev-internal' has been scheduled to stop automatically after exiting Cloud Code. To disable this on future deployments, set autoStop to false in your launch configuration d:\POC\promo_run_pubsub\.vscode\launch.json
Configuring minikube gcp-auth addon
Using GCP project 'Project-Id' with minikube gcp-auth
Failed to configure minikube gcp-auth addon. Your app might not be able to authenticate Google or GCP APIs it calls. The addon has been disabled. More details can be found in the detailed logs.






Update initiated
Deploy started
Deploy completed

Status check started
Resource pod/promo-run-pubsub-5d4cd64bf9-8pf4q status updated to In Progress
Resource deployment/promo-run-pubsub status updated to In Progress
Resource pod/promo-run-pubsub-5d4cd64bf9-8pf4q status updated to In Progress
Resource deployment/promo-run-pubsub status failed with waiting for rollout to finish: 0 of 1 updated replicas are available...
Status check failed

Update failed with error code STATUSCHECK_CONTAINER_TERMINATED
1/1 deployment(s) failed
Skaffold exited with code 1.
Cleaning up...
Finished clean up.
</code></pre>
",36,0,0,3,google-cloud-platform;gcloud;google-cloud-run,2022-06-16 20:14:22,2022-06-16 20:14:22,2022-06-20 12:10:49,created a new project folder  myproject  in my local machine executed below command to ship the code it s mentioned in the tutorial document that upon success  you should see a success message containing the id  creation time  and image name  the image is stored in container registry and can be re used if desired  so i thought to ignore this error and proceed with the next step to deploy the app by running the command  as i am new to this gcp  amp  dockerizing services  not understanding this issue and unable to fix it  i researched many blogs and articles yet no proper solution for this error  any help will be appreciated  tried to run the container locally and it s failing with error  i m using vs code ide  and  cloud code  debug on cloud run emulator  to debug the code 
153,153,17156297,72598952,Azure Machine Learning REST API request limits,"<p>My application sends requests to Azure Machine Learning REST API in order to invoke a batch endpoint and start scoring jobs as described <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-batch-with-rest#invoke-the-batch-endpoint-to-start-a-batch-scoring-job"" rel=""nofollow noreferrer"">here</a>. It works well for small number of requests, but if the app sends many concurrent requests the REST API sometimes responds with status code 429 &quot;TooManyRequests&quot; and message &quot;Received too many requests in a short amount of time. Retry again after 1 seconds.&quot;. For example, it happened after sending 77 requests at once.</p>
<p>The message is pretty clear and the best solution I can think about is to throttle outgoing requests. That is making sure the app doesn't exceed limits when it sends concurrent requests. But the problem is I don't know what are the request limits for Azure Machine Learning REST API. Looking through the Microsoft documentation I could only find <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints"" rel=""nofollow noreferrer"">this article</a> which provides limits for Managed online endpoints whereas I'm looking for Batch endpoints.</p>
<p>I would really appreciate if someone helped me to find the Azure ML REST API request limits or suggested a better solution. Thanks.</p>
<p><strong>UPDATE 20 Jun 2022</strong>:
I couldn't find out how many concurrent requests are allowed by Azure Machine Learning batch endpoints. So I ended with a limit of 10 outgoing requests which solved the &quot;TooManyRequests&quot; problem. In order to throttle requests I used SemaphoreSlim as described <a href=""https://codeburst.io/throttling-concurrent-outgoing-http-requests-in-net-core-404b5acd987b"" rel=""nofollow noreferrer"">here</a>.</p>
",54,1,0,4,azure;limit;azure-machine-learning-service;azure-rest-api,2022-06-13 12:44:37,2022-06-13 12:44:37,2022-06-20 12:06:46,my application sends requests to azure machine learning rest api in order to invoke a batch endpoint and start scoring jobs as described   it works well for small number of requests  but if the app sends many concurrent requests the rest api sometimes responds with status code   toomanyrequests  and message  received too many requests in a short amount of time  retry again after  seconds    for example  it happened after sending  requests at once  the message is pretty clear and the best solution i can think about is to throttle outgoing requests  that is making sure the app doesn t exceed limits when it sends concurrent requests  but the problem is i don t know what are the request limits for azure machine learning rest api  looking through the microsoft documentation i could only find  which provides limits for managed online endpoints whereas i m looking for batch endpoints  i would really appreciate if someone helped me to find the azure ml rest api request limits or suggested a better solution  thanks 
154,154,314763,72659937,CI / CD and repository integration for Azure ML Workspace,"<p>I am interested in knowing how can I integrate a repository with Azure Machine Learning Workspace.</p>
<h2>What have I tried ?</h2>
<p>I have some experience with Azure Data Factory and usually I have setup workflows where</p>
<ol>
<li><p>I have a <code>dev</code> azure data factory instance that is linked to azure repository.</p>
</li>
<li><p>Changes made to the repository using the code editor.</p>
</li>
<li><p>These changes are published via the <code>adf_publish</code> branch to the live <code>dev</code> instance</p>
</li>
<li><p>I use CI / CD pipeline and the AzureRMTemplate task to deploy the templates in the publish branch to release the changes to <code>production</code> environment</p>
</li>
</ol>
<h2>Question:</h2>
<ul>
<li>How can I achieve the same / similar workflow with Azure Machine Learning Workspace ?</li>
<li>How is CI / CD done with Azure ML Workspace</li>
</ul>
",21,1,0,2,azure-machine-learning-studio;azure-machine-learning-service,2022-06-17 18:41:31,2022-06-17 18:41:31,2022-06-20 05:15:28,i am interested in knowing how can i integrate a repository with azure machine learning workspace  i have some experience with azure data factory and usually i have setup workflows where i have a dev azure data factory instance that is linked to azure repository  changes made to the repository using the code editor  these changes are published via the adf_publish branch to the live dev instance i use ci   cd pipeline and the azurermtemplate task to deploy the templates in the publish branch to release the changes to production environment
155,155,15993563,72678631,"conda creation NGS environment, sra-tools fastqc multiqc samtools bowtie2 hisat2 subread","<p>I cannot easily create an environment with conda containing the NGS tools I want. I reinstalled conda to get a fresh start since I had many python packages in the base env. I also took the occasion to install the full anaconda3 instead of miniconda I had before now I have way enough space.</p>
<p>Although all the packages are available via bioconda the only 2 i can add to my env are fastqc and multiqc. Before that I could install sra-tools and fastqc in the base env with miniconda3.</p>
<p><strong>Config:</strong> MacOS Monterey 12.1 M1 chip. migration from my old macbook air with the lastest time machine bkp. I uninstalled miniconda with the anaconda-clean procedure and after that I also removed a python 3.9.5 I had in the apps folder I had initially installed to start learning 1yr ago before knowing about conda.</p>
<p><strong>Also to be mentioned in case it may help:</strong> anaconda-navigator was not installed by the Anaconda3-2022.05-MacOSX-arm64.pkg (sha256 integrity check was ok)
in following the check installation/navigator troubleshooting on anaconda website I came across an error upon launching spyder:</p>
<pre><code>```(ModuleNotFoundError: No module named 'PyQt5.QtWebEngineWidgets')```
</code></pre>
<p>Does somebody know where this unability to find the packages comes?</p>
<p>Thank you for your help!
best, Daniel</p>
<p><strong>Env creation command and console output:</strong></p>
<pre><code>(base) mymac:~ D________$ conda create --name ngstools fastqc multiqc sra-tools samtools bowtie2 hisat2 subread -c conda-forge -c bioconda -c bioconda/label/cf201901
</code></pre>
<p>Terminal output:</p>
<p>Collecting package metadata (current_repodata.json): done</p>
<p>Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.</p>
<p>Collecting package metadata (repodata.json): done
Solving environment: failed</p>
<p>PackagesNotFoundError: The following packages are not available from current channels:</p>
<ul>
<li>hisat2</li>
<li>subread</li>
<li>bowtie2</li>
<li>samtools</li>
<li>sra-tools</li>
</ul>
<p>Current channels:</p>
<ul>
<li><a href=""https://conda.anaconda.org/conda-forge/osx-arm64"" rel=""nofollow noreferrer"">https://conda.anaconda.org/conda-forge/osx-arm64</a></li>
<li><a href=""https://conda.anaconda.org/conda-forge/noarch"" rel=""nofollow noreferrer"">https://conda.anaconda.org/conda-forge/noarch</a></li>
<li><a href=""https://conda.anaconda.org/bioconda/osx-arm64"" rel=""nofollow noreferrer"">https://conda.anaconda.org/bioconda/osx-arm64</a></li>
<li><a href=""https://conda.anaconda.org/bioconda/noarch"" rel=""nofollow noreferrer"">https://conda.anaconda.org/bioconda/noarch</a></li>
<li><a href=""https://conda.anaconda.org/bioconda/label/cf201901/osx-arm64"" rel=""nofollow noreferrer"">https://conda.anaconda.org/bioconda/label/cf201901/osx-arm64</a></li>
<li><a href=""https://conda.anaconda.org/bioconda/label/cf201901/noarch"" rel=""nofollow noreferrer"">https://conda.anaconda.org/bioconda/label/cf201901/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/osx-arm64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/osx-arm64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/osx-arm64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/osx-arm64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/noarch</a></li>
</ul>
<p>To search for alternate channels that may provide the conda package you're looking for, navigate to
<a href=""https://anaconda.org"" rel=""nofollow noreferrer"">https://anaconda.org</a>
and use the search bar at the top of the page.</p>
",27,1,0,4,installation;anaconda;conda;repo,2022-06-19 22:06:38,2022-06-19 22:06:38,2022-06-19 23:49:46,i cannot easily create an environment with conda containing the ngs tools i want  i reinstalled conda to get a fresh start since i had many python packages in the base env  i also took the occasion to install the full anaconda instead of miniconda i had before now i have way enough space  although all the packages are available via bioconda the only  i can add to my env are fastqc and multiqc  before that i could install sra tools and fastqc in the base env with miniconda  config  macos monterey   m chip  migration from my old macbook air with the lastest time machine bkp  i uninstalled miniconda with the anaconda clean procedure and after that i also removed a python    i had in the apps folder i had initially installed to start learning yr ago before knowing about conda  does somebody know where this unability to find the packages comes  env creation command and console output  terminal output  collecting package metadata  current_repodata json   done solving environment  failed with repodata from current_repodata json  will retry with next repodata source  packagesnotfounderror  the following packages are not available from current channels  current channels 
156,156,13963027,63493530,How to plot and annotate a grouped bar chart,"<p>I came across a tricky issue about the matplotlib in Python. I want to create a grouped bar chart with several codes, but the chart goes wrong. Could you please offer me some advice? The code is as follows.</p>
<pre><code>import numpy as np
import pandas as pd
file=&quot;https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/coursera/Topic_Survey_Assignment.csv&quot;
df=pd.read_csv(file,index_col=0)

df.sort_values(by=['Very interested'], axis=0,ascending=False,inplace=True)

df['Very interested']=df['Very interested']/2233
df['Somewhat interested']=df['Somewhat interested']/2233
df['Not interested']=df['Not interested']/2233
df

df_chart=df.round(2)
df_chart

labels=['Data Analysis/Statistics','Machine Learning','Data Visualization',
       'Big Data (Spark/Hadoop)','Deep Learning','Data Journalism']
very_interested=df_chart['Very interested']
somewhat_interested=df_chart['Somewhat interested']
not_interested=df_chart['Not interested']

x=np.arange(len(labels))
w=0.8

fig,ax=plt.subplots(figsize=(20,8))
rects1=ax.bar(x-w,very_interested,w,label='Very interested',color='#5cb85c')
rects2=ax.bar(x,somewhat_interested,w,label='Somewhat interested',color='#5bc0de')
rects3=ax.bar(x+w,not_interested,w,label='Not interested',color='#d9534f')

ax.set_ylabel('Percentage',fontsize=14)
ax.set_title(&quot;The percentage of the respondents' interest in the different data science Area&quot;,
            fontsize=16)
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend(fontsize=14)

def autolabel(rects):
    &quot;&quot;&quot;Attach a text label above each bar in *rects*, displaying its height.&quot;&quot;&quot;
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 3, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords=&quot;offset points&quot;,
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

fig.tight_layout()

plt.show()
</code></pre>
<p>The output of this code module is really a mess. But what I expect should look like the bar chart in the picture. Could you please tell me which point is not correct in my codes?</p>
<p><a href=""https://i.stack.imgur.com/kmFRm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kmFRm.png"" alt=""enter image description here"" /></a></p>
",7264,1,4,4,python;pandas;matplotlib;bar-chart,2020-08-20 00:41:51,2020-08-20 00:41:51,2022-06-19 21:57:46,i came across a tricky issue about the matplotlib in python  i want to create a grouped bar chart with several codes  but the chart goes wrong  could you please offer me some advice  the code is as follows  the output of this code module is really a mess  but what i expect should look like the bar chart in the picture  could you please tell me which point is not correct in my codes  
157,157,10355993,54600809,SVD calculation error from lapack function when using scikit-learn&#39;s Linear Discriminant Analysis class,"<p>I'm classifying 2-class, 1-D data using scikit-learn's <a href=""https://scikit-learn.org/stable/modules/lda_qda.html"" rel=""nofollow noreferrer"">LDA</a> classifier in a machine learning pipeline I've created. The following exception occurred:</p>

<blockquote>
  <p>ValueError: Internal work array size computation failed: -10</p>
</blockquote>

<p>at the following line:</p>

<blockquote>
  <p>LinearDiscriminantAnalysis.fit(X,y)</p>
</blockquote>

<p>where X =  [-5e15, -5e15, -5e15, 5.7e16] and y = [0, 0, 0, 1], both float64 data-type</p>

<p>Additionally the following error was printed to console:</p>

<blockquote>
  <p>Intel MKL ERROR: Parameter 10 was incorrect on entry to DGESDD</p>
</blockquote>

<p>After a quick Google search, dgesdd is a function in <a href=""https://en.wikipedia.org/wiki/LAPACK"" rel=""nofollow noreferrer"">LAPACK</a> which scikit-learn <a href=""https://scikit-learn.org/stable/modules/computing.html"" rel=""nofollow noreferrer"">relies upon</a>. The <a href=""http://www.netlib.org/lapack/explore-html/d1/d7e/group__double_g_esing_gad8e0f1c83a78d3d4858eaaa88a1c5ab1.html"" rel=""nofollow noreferrer"">dgesdd documentation</a> tells us that the function computes the <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" rel=""nofollow noreferrer"">singular value decomposition</a> (SVD) of a real M-by-N matrix A. </p>

<p>Going back to the original exception, I found it was raised in <a href=""https://github.com/scipy/scipy/blob/master/scipy/linalg/lapack.py"" rel=""nofollow noreferrer"">scipy.linalg.lapack.py</a> at the _compute_lwork function. This function takes as input a function, which in this case I believe is the dgesdd function. CTRL-F ""-10"" on the dgesdd documentation page gives the logic behind this error code, but I don't know Fortran so I'm not exactly sure what it means.</p>

<p>I want to bet that the SVD calculation is failing due to either (1) large values in X array, or (2) the fact that the 3 of the values in the X array are the exact same number. </p>

<p>I will keep reading into SVD and its limitations. Any insight on how to avoid this error would be tremendously appreciated.</p>

<p>Here is a screenshot of the <a href=""https://i.stack.imgur.com/pVRiy.png"" rel=""nofollow noreferrer"">error</a></p>
",1233,2,1,5,python;scipy;lapack;svd;intel-mkl,2019-02-09 03:28:13,2019-02-09 03:28:13,2022-06-19 20:54:11,i m classifying  class   d data using scikit learn s  classifier in a machine learning pipeline i ve created  the following exception occurred  valueerror  internal work array size computation failed    at the following line  lineardiscriminantanalysis fit x y  where x      e   e   e   e  and y             both float data type additionally the following error was printed to console  intel mkl error  parameter  was incorrect on entry to dgesdd after a quick google search  dgesdd is a function in  which scikit learn   the  tells us that the function computes the   svd  of a real m by n matrix a   going back to the original exception  i found it was raised in  at the _compute_lwork function  this function takes as input a function  which in this case i believe is the dgesdd function  ctrl f   on the dgesdd documentation page gives the logic behind this error code  but i don t know fortran so i m not exactly sure what it means  i want to bet that the svd calculation is failing due to either    large values in x array  or    the fact that the  of the values in the x array are the exact same number   i will keep reading into svd and its limitations  any insight on how to avoid this error would be tremendously appreciated  here is a screenshot of the 
158,158,14668244,72677943,Machine learning or Deep learning for this dataset?,"<p>Automobile Insurance Fraud Dataset has 15k samples (text data), out of which around 14k are not fraud samples and 1k fraud. Is deep learning good for this dataset?</p>
<p>Also, what algorithms are suitable for fraud detection?</p>
",17,1,-2,3,machine-learning;deep-learning;dataset,2022-06-19 20:31:43,2022-06-19 20:31:43,2022-06-19 20:35:58,automobile insurance fraud dataset has k samples  text data   out of which around k are not fraud samples and k fraud  is deep learning good for this dataset  also  what algorithms are suitable for fraud detection 
159,159,13852756,72677454,Predict whether an email address exist or don&#39;t exist using machine learning,"<p>Let's say I have a dataset of lots of existing , non existing and catch-all email address labelled accordingly.</p>
<p>Is there any way to use machine or deep learning algorithm to predict which email address is likely to be catch-all or existing or non-existing?</p>
<p>When some companies say they validate email using AI. What they actually do technically step by step?
For example: <a href=""https://sendgrid.com/blog/validating-email-addresses-with-machine-learning/"" rel=""nofollow noreferrer"">https://sendgrid.com/blog/validating-email-addresses-with-machine-learning/</a></p>
",34,1,-2,5,python;machine-learning;deep-learning;artificial-intelligence;email-validation,2022-06-19 19:26:41,2022-06-19 19:26:41,2022-06-19 19:35:37,let s say i have a dataset of lots of existing   non existing and catch all email address labelled accordingly  is there any way to use machine or deep learning algorithm to predict which email address is likely to be catch all or existing or non existing 
160,160,18187069,72677368,MIMIC III Database benchmark overflow code error,"<p>I am using MIMIC III database for my ML project.</p>
<p>While building the benchmark machine learning datasets, I came across the following error which I am unable to resolve.</p>
<pre><code>def add_age_to_icustays(stays):
    stays['AGE'] = stays.INTIME.subtract(stays.DOB).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60/24/365
    stays.ix[stays.AGE &lt; 0, 'AGE'] = 90
    return stays
</code></pre>
<p>Here is the error :</p>
<pre><code>START:
        ICUSTAY_IDs: 61532
        HADM_IDs: 57786
        SUBJECT_IDs: 46476
REMOVE ICU TRANSFERS:
        ICUSTAY_IDs: 55830
        HADM_IDs: 52834
        SUBJECT_IDs: 43277
REMOVE MULTIPLE STAYS PER ADMIT:
        ICUSTAY_IDs: 50186
        HADM_IDs: 50186
        SUBJECT_IDs: 41587
Traceback (most recent call last):
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\runpy.py&quot;, line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\runpy.py&quot;, line 86, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\KIIT\Documents\mimic3-benchmarks-master\mimic3benchmark\scripts\extract_subjects.py&quot;, line 50, in &lt;module&gt;
    stays = add_age_to_icustays(stays)
  File &quot;C:\Users\KIIT\Documents\mimic3-benchmarks-master\mimic3benchmark\mimic3csv.py&quot;, line 79, in add_age_to_icustays
    stays['AGE'] = stays.INTIME.subtract(stays.DOB).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60/24/365
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\ops\__init__.py&quot;, line 185, in flex_wrapper
    return self._binop(other, op, level=level, fill_value=fill_value)
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\series.py&quot;, line 2983, in _binop
    result = func(this_vals, other_vals)
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\ops\common.py&quot;, line 70, in new_method
    return method(self, other)
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\arrays\datetimelike.py&quot;, line 1340, in __sub__
    result = self._sub_datetime_arraylike(other)
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\arrays\datetimes.py&quot;, line 740, in _sub_datetime_arraylike
    new_values = checked_add_with_arr(self_i8, -other_i8, arr_mask=arr_mask)
  File &quot;C:\Users\KIIT\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\algorithms.py&quot;, line 1114, in checked_add_with_arr
    raise OverflowError(&quot;Overflow in int64 addition&quot;)
OverflowError: Overflow in int64 addition
</code></pre>
<p>I tried it to directly subtract :</p>
<pre><code># stays['date_of_admission'] = stays.INTIME

    # stays['date_of_admission'] = pd.to_datetime(stays['date_of_admission'],errors='coerce').dt.date

    # #print(stays['date_of_admission'])

    # stays['DOB'] = pd.to_datetime(stays['DOB'],errors='coerce').dt.date
    

    # stays['DOB'] = pd.to_datetime(stays.DOB, format='%Y-%m-%d')
    # stays['date_of_admission'] = pd.to_datetime(stays.date_of_admission, format='%Y-%m-%d')
    # print(stays['DOB'])

    # #stays['AGE'] = stays['date_of_admission'] - stays['DOB'].dt.year

    # #stays['DOB'] = stays['DOB'].dt.strftime('%Y-%m-%d')
    # #stays['date_of_admission'] = stays['date_of_admission'].dt.strftime('%Y-%m-%d')

</code></pre>
<p>but then it shows the following error:</p>
<blockquote>
<p>Addition/subtraction of integers and integer-arrays with DatetimeArray is no longer supported.  Instead of adding/subtracting <code>n</code>, use <code>n * obj.freq</code></p>
</blockquote>
<p>It will be extremely helpful if you help me resolve this.</p>
",15,0,0,3,python;pandas;data-science,2022-06-19 19:15:36,2022-06-19 19:15:36,2022-06-19 19:18:30,i am using mimic iii database for my ml project  while building the benchmark machine learning datasets  i came across the following error which i am unable to resolve  here is the error   i tried it to directly subtract   but then it shows the following error  addition subtraction of integers and integer arrays with datetimearray is no longer supported   instead of adding subtracting n  use n   obj freq it will be extremely helpful if you help me resolve this 
161,161,5539588,62337528,How to plot machine learning model horizontally?,"<p>I want to draw a machine learning model.
In some paper, they draw models horizontally like Figure 11, 12, 13, 14, 15 in <a href=""https://tches.iacr.org/index.php/TCHES/article/view/7388/6560"" rel=""nofollow noreferrer"">https://tches.iacr.org/index.php/TCHES/article/view/7388/6560</a>.
These seems to have drawn models from keras.
How can I do like that?</p>
",305,2,1,2,plot;keras,2020-06-12 10:00:28,2020-06-12 10:00:28,2022-06-19 16:22:43,
162,162,18225550,72676046,Unknown loss function: sparse_catecorical_crossentropy,"<p>I have been learning about machine learning and deep neural networks for a while and I only started using Keras and TensorFlow recently. I'm trying to build a digit-recognizing neural network, but I'm facing this error:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import  Dense
from tensorflow import keras
(Xtrain,Ytrain) ,(Xtest,Ytest)=mnist.load_data()
model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.softmax))
model.compile(optimizer='adam',loss='sparse_catecorical_crossentropy',metrics=['accuracy'])
model.fit(Xtrain,Ytrain,epochs=3)
</code></pre>
<p>And this is the error I'm having:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\amelbarouni\Desktop\python yassinne\ppyytthhoonn\mlkjj.py&quot;, line 12, in &lt;module&gt;
    model.fit(Xtrain,Ytrain,epochs=3)
  File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1147, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1021, in train_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1000, in run_step  **
        outputs = model.train_step(data)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 860, in train_step
        loss = self.compute_loss(x, y, y_pred, sample_weight)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 918, in compute_loss
        return self.compiled_loss(
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\compile_utils.py&quot;, line 184, in __call__
        self.build(y_pred)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\compile_utils.py&quot;, line 133, in build
        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\compile_utils.py&quot;, line 272, in _get_loss_object
        loss = losses_mod.get(loss)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\losses.py&quot;, line 2369, in get
        return deserialize(identifier)
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\losses.py&quot;, line 2324, in deserialize
        return deserialize_keras_object(
    File &quot;C:\Users\amelbarouni\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\generic_utils.py&quot;, line 709, in deserialize_keras_object
        raise ValueError(

    ValueError: Unknown loss function: sparse_catecorical_crossentropy. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
</code></pre>
<p>Any ideas how to fix it?</p>
",27,1,0,5,python;tensorflow;keras;deep-learning;neural-network,2022-06-19 15:52:47,2022-06-19 15:52:47,2022-06-19 16:02:52,i have been learning about machine learning and deep neural networks for a while and i only started using keras and tensorflow recently  i m trying to build a digit recognizing neural network  but i m facing this error  and this is the error i m having  any ideas how to fix it 
163,163,2948561,72675054,Is it possible to run python with the tensorflow library on an vps based on kvm,"<p>I'm running a KVM VPS (virtual private server) with Debian 10 installed.
Since the server should do some python3 machine learning work, I installed</p>
<pre><code>pip3 install tensorflow
</code></pre>
<p>Which installs fine, but <code>import tensorflow</code> in a file (Python 3.9.2 installed) ends in the error:</p>
<pre><code>The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
</code></pre>
<p>Some information about the CPU:</p>
<pre><code>lscpu
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   40 bits physical, 48 bits virtual
CPU(s):                          1
On-line CPU(s) list:             0
Thread(s) per core:              1
Core(s) per socket:              1
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      15
Model:                           6
Model name:                      Common KVM processor
Stepping:                        1
CPU MHz:                         2599.998
BogoMIPS:                        5199.99
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       32 KiB
L1i cache:                       32 KiB
L2 cache:                        4 MiB
L3 cache:                        16 MiB
NUMA node0 CPU(s):               0
Vulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host stat
                                 e unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat ps
                                 e36 clflush mmx fxsr sse sse2 syscall nx lm constant_tsc nopl xtopol
                                 ogy cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fa
                                 ult pti
</code></pre>
<p>Installing and old version tensorflow library does not work</p>
<blockquote>
<p>pip3 install tensorflow==1.5
ERROR: Could not find a version
that satisfies the requirement tensorflow==1.5 (from versions: 2.5.0,
2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1) ERROR: No matching distribution found for tensorflow==1.5</p>
</blockquote>
<p>Does anyone have an idea how to use tensorflow inside KVM maschine?</p>
",25,0,0,2,python;tensorflow,2022-06-19 13:04:21,2022-06-19 13:04:21,2022-06-19 13:26:30,which installs fine  but import tensorflow in a file  python    installed  ends in the error  some information about the cpu  installing and old version tensorflow library does not work does anyone have an idea how to use tensorflow inside kvm maschine 
164,164,11529057,72516242,Is there any limitations for runs per users in Azure ML experiments?,"<p>I and my team members are working on a machine learning project through the <strong>Azure ML</strong> portal.
We have created a specific <em>experiment</em> in our <em>workspace</em> in Azure ML and are submitting our Python script <em>runs</em> from our local or remote machines in this experiment.</p>
<p>Although I'm collaborating with my colleagues, most of the runs in this specific experiment are submitted by me.</p>
<p>Recently, I have faced a problem with experiment submissions.
The problem is that after some number of experiments created by me, I cannot add any other runs to this experiment, but my colleagues can!!!</p>
<p>Unfortunately, the Azure ML portal does not show any clear error message for this problem. It continues submitting the run till a <em>timeout</em> exception occurs!</p>
<p>As a temporary solution, I've just changed the name of the experiment and I could conquer this problem.</p>
<p>This solution helped me to submit my run on <strong>Azure ML</strong> but it didn’t satisfy me because we want to collect all related runs under a specific experiment. On the other hand creating multiple number of experiments for each run is overwhelming!</p>
<p>What I know is that there are some service limits for the number of runs in a workspace on this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/resource-limits-quotas-capacity"" rel=""nofollow noreferrer"">page</a>. I am sure that the number of runs in our workspace has not reached to the 10 millions, because I can created new runs under new experiments dashboard. But I don’t know anything about the limitations on the number of runs in a specific experiment or even any limitations for the number of runs per users in a specific experiment.
I couldn't find any clear document explaining this fact.</p>
<p>Is there anyone who can help me for this issue?</p>
",44,0,2,5,azure;azureml;azureml-python-sdk;azure-python-sdk;azuremlsdk,2022-06-06 15:50:00,2022-06-06 15:50:00,2022-06-19 11:41:46,although i m collaborating with my colleagues  most of the runs in this specific experiment are submitted by me  unfortunately  the azure ml portal does not show any clear error message for this problem  it continues submitting the run till a timeout exception occurs  as a temporary solution  i ve just changed the name of the experiment and i could conquer this problem  this solution helped me to submit my run on azure ml but it didn t satisfy me because we want to collect all related runs under a specific experiment  on the other hand creating multiple number of experiments for each run is overwhelming  is there anyone who can help me for this issue 
165,165,3098629,72674407,How to predict an individual value using SKlearn?,"<p>I am very new to Machine Learning and I would like to get a percentage returned for an individual array that I pass in the prediction model I have created.</p>
<p>I'm not sure how to go about getting the match percentage. I thought it was <code>metrics.accuracy_score(Ytest, y_pred)</code> but when I try that it gives me the following error:<br />
<code>**ValueError: Found input variables with inconsistent numbers of samples: [4, 1]**</code></p>
<p>I have no idea if this is the correct way to go about this.</p>
<pre><code>import numpy as np                  #linear algebra
import pandas as pd                 # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt     #For Visualisation
import seaborn as sns               #For better Visualisation
from bs4 import BeautifulSoup       #For Text Parsing
import mysql.connector
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import joblib
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
import docx2txt
import re
import csv
from sklearn import metrics

class Machine:

    TrainData       = ''


    def __init__(self):


        self.TrainData          = self.GetTrain()

        Data                    = self.ProcessData()

        x                       = Data[0]
        y                       = Data[1]

        x, x_test, y, y_test    = train_test_split(x,y, stratify = y, test_size = 0.25, random_state = 42)

        self.Predict(x,y, '',x_test , y_test )

    def Predict(self,X,Y,Data, Xtext, Ytest):

        model = GaussianNB()
        model.fit(Xtext, Ytest)

        y_pred = model.predict([[1.0, 2.00613, 2, 5]])

        print(&quot;Accuracy:&quot;,metrics.accuracy_score(Ytest, y_pred))
        


    def ProcessData(self):

            X = []
            Y = []
            i = 0
            for I in self.TrainData:

                Y.append(I[4])
                X.append(I)

                i = i + 1

            i = 0
            for j in X:

                X[i][0] = float(X[i][0])
                X[i][1] = float(X[i][1])
                X[i][2] = int(X[i][2])
                X[i][3] = int(X[i][3])
                del X[i][4]

                i = i + 1

            return X,Y


    def GetTrain(self):
        file        = open('docs/training/TI_Training.csv')
        csvreader   = csv.reader(file)

        header      = []
        header      = next(csvreader)

        rows        = []

        for row in csvreader:
            rows.append(row)

        file.close()

        return rows



Machine()
</code></pre>
",40,1,-2,4,python;python-3.x;machine-learning;scikit-learn,2022-06-19 10:35:15,2022-06-19 10:35:15,2022-06-19 11:26:28,i am very new to machine learning and i would like to get a percentage returned for an individual array that i pass in the prediction model i have created  i have no idea if this is the correct way to go about this 
166,166,17258571,72671000,TypeError: __init__() got an unexpected keyword argument &#39;categorical_features&#39; How I can solve it?,"<p>I'm working on a Machine Learning course and a I need transform my categorical values. Here is the code:</p>
<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder
LabelEncoder_X = LabelEncoder()
X[:,3] = LabelEncoder_X.fit_transform(X[:,3])

#One Hot encoding o variables dummy. Trata de convertir datos que no son 
#numéricos a datos numéricos de 0 y 1.
onehotencoder = OneHotEncoder(categorical_features=[3])
X = OneHotEncoder.fit_transform(X).toarray() 
</code></pre>
<p>The mistake is with the part of categorical_features, because maybe Python get update to a new version. Thanks</p>
",32,1,0,2,python;data-science,2022-06-18 22:09:36,2022-06-18 22:09:36,2022-06-18 22:25:31,i m working on a machine learning course and a i need transform my categorical values  here is the code  the mistake is with the part of categorical_features  because maybe python get update to a new version  thanks
167,167,12889472,60200088,How to make early stopping in image classification pytorch,"<p>I'm new with Pytorch and machine learning I'm follow this tutorial in this tutorial <a href=""https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/"" rel=""nofollow noreferrer"">https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/</a> and use my custom dataset. Then I have same problem in this tutorial but I dont know how to make early stopping in pytorch and if do you have better without create early stopping process please tell me.</p>
",6445,3,3,3,python;pytorch;early-stopping,2020-02-13 09:08:26,2020-02-13 09:08:26,2022-06-18 19:41:22,i m new with pytorch and machine learning i m follow this tutorial in this tutorial  and use my custom dataset  then i have same problem in this tutorial but i dont know how to make early stopping in pytorch and if do you have better without create early stopping process please tell me 
168,168,15773956,72669298,Flask website for cancer prediction gives same output,"<p>I am working on a flask website which predict prostate cancer using KNN.I don't know why but even if I enter data of benign cell as the input ,it shows the cell is malignant .I even changed the machine learning code still it gives wrong output .</p>
<p>app.py</p>
<pre><code>@app.route('/predict', methods=['POST'])
def predict():
    if request.method == 'POST':
        rad = float(request.form['radius'])
        tex = float(request.form['texture'])
        par = float(request.form['perimeter'])
        area = float(request.form['area'])
        smooth = float(request.form['smoothness'])
        compact = float(request.form['compactness'])
        symme= float(request.form['symmetry'])
        frac = float(request.form['fractal_dimension'])

        mypred = np.array([[rad, tex, par, area, smooth, compact,symme, frac]])
        my_prediction = model.predict(mypred)

        return render_template('cancerresult.html', prediction=my_prediction)


</code></pre>
<p>cancerresult.html :</p>
<pre><code>
&lt;body&gt;

    {% block body %}
       {% if prediction==1 %}
      &lt;h1&gt; You have CANCER&lt;/h1&gt;
      {% elif prediction== 0 %}
      &lt;h1&gt;No CANCER&lt;/h1&gt;
      {% endif %}

    {% endblock %}
&lt;/body&gt;
</code></pre>
<p>This is the full code <a href=""https://github.com/devika-harshan/prostatecancer"" rel=""nofollow noreferrer"">https://github.com/devika-harshan/prostatecancer</a></p>
",20,1,-1,1,python,2022-06-18 18:07:24,2022-06-18 18:07:24,2022-06-18 18:38:21,i am working on a flask website which predict prostate cancer using knn i don t know why but even if i enter data of benign cell as the input  it shows the cell is malignant  i even changed the machine learning code still it gives wrong output   app py cancerresult html   this is the full code 
169,169,11414622,72663243,Splitting of Training and Test set when the dataset could be updated,"<p>I am following the code of the &quot;Hands on Machine learning with Sci-kit learn and tensorflow 2nd edition&quot; (<a href=""https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb"" rel=""nofollow noreferrer"">ipynb link</a>). In the section on selection the training and test data sets, the author brings up the importance of writing the splitting function so that the test set will stay consistent over multiple runs, even if the data set is refreshed. The code is written so that an updated data set will still have the right percentage (<code>test ratio</code>) for splitting the test and training sets, but the new test set won't contain any instance that was previously in the training set. It does this by creating a number for the index value(<code>identifier/id_x</code>) and returning true if that number is between 0 and (<code>test ratio</code>) of the range of possible numbers that could be selected.</p>
<pre><code>from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
</code></pre>
<p>This part makes sense, but what I don't understand is how to implement the same thing using the function <code>train_test_split</code> from <code>skilearn</code> is there something specific to do that if the whole data set is updated then the test set never includes a value that was already selected to be in the training set. Is this something that is already included if we include the <code>random_state</code> argument and make sure that the updated data set only adds rows to the existing data set and never deletes rows? Is that a realistic thing to require?</p>
<p>Is this a problem to worry about with cross validation as well?</p>
<p>Thanks for your help.</p>
",10,1,0,2,scikit-learn;training,2022-06-17 23:35:43,2022-06-17 23:35:43,2022-06-18 16:40:47,i am following the code of the  hands on machine learning with sci kit learn and tensorflow nd edition      in the section on selection the training and test data sets  the author brings up the importance of writing the splitting function so that the test set will stay consistent over multiple runs  even if the data set is refreshed  the code is written so that an updated data set will still have the right percentage  test ratio  for splitting the test and training sets  but the new test set won t contain any instance that was previously in the training set  it does this by creating a number for the index value identifier id_x  and returning true if that number is between  and  test ratio  of the range of possible numbers that could be selected  this part makes sense  but what i don t understand is how to implement the same thing using the function train_test_split from skilearn is there something specific to do that if the whole data set is updated then the test set never includes a value that was already selected to be in the training set  is this something that is already included if we include the random_state argument and make sure that the updated data set only adds rows to the existing data set and never deletes rows  is that a realistic thing to require  is this a problem to worry about with cross validation as well  thanks for your help 
170,170,19363541,72668091,Is it a good practice to train the final model by the whole set in deep learning?,"<p>I am new to the deep learning area and would like to know whether it would be better to train the final model by the whole set in deep learning.</p>
<p>In traditional machine learning such as support vector machines (SVM), the following procedure is common. (<a href=""https://www.csie.ntu.edu.tw/%7Ecjlin/papers/guide/guide.pdf"" rel=""nofollow noreferrer"">reference</a>; see 1.2)</p>
<ol>
<li>Apply K-fold cross-validation to find the best hyper-parameters.</li>
<li>Use the best hyper-parameter to train the final model by the whole set.</li>
</ol>
<p>However, in deep learning, most do not use K-fold cross-validation. They split the whole set into <strong>one split</strong> of training and validation data and take the validation split to conduct hyper-parameter search. The final model may be obtained in the following ways.</p>
<ul>
<li>Return the best model obtained from hyper-parameter search as the final model. (only trained on the training split)</li>
<li>Use the best hyper-parameter to train the final model by the whole set. (trained on both the training and validation split)</li>
</ul>
<p>I wonder which way is <strong>more common</strong> in deep learning.</p>
<p>Specifically, in computer vision or natural language processing (NLP) area, how do researchers commonly obtain the final model?</p>
<p>Or, are there <strong>any papers, textbooks, or packages</strong> that explicitly explain more about how they do it and why they do it?</p>
",32,0,-2,5,machine-learning;deep-learning;nlp;computer-vision;svm,2022-06-18 14:42:57,2022-06-18 14:42:57,2022-06-18 16:06:05,i am new to the deep learning area and would like to know whether it would be better to train the final model by the whole set in deep learning  in traditional machine learning such as support vector machines  svm   the following procedure is common     see    however  in deep learning  most do not use k fold cross validation  they split the whole set into one split of training and validation data and take the validation split to conduct hyper parameter search  the final model may be obtained in the following ways  i wonder which way is more common in deep learning  specifically  in computer vision or natural language processing  nlp  area  how do researchers commonly obtain the final model  or  are there any papers  textbooks  or packages that explicitly explain more about how they do it and why they do it 
171,171,5353461,52830307,conda: remove all installed packages from base/root environment,"<h2>TL:DR: How can I remove all installed packages from <code>base</code>?</h2>
<p>I installed a bunch of machine learning packages in my <code>base</code> conda environment.</p>
<p>I've now created a <code>ml</code> environment for machine learning, and wish to reset my <code>base</code> environment by removing all the packages installed there.</p>
<p>I've tried:</p>
<pre><code>% activate base
% conda uninstall -n base --all

CondaEnvironmentError: cannot remove current environment. deactivate and run conda remove again
</code></pre>
<p>Apparently, I can't remove packages from the current environment(?!), so lets switch to my <code>ml</code> environment first:</p>
<pre><code>% source activate ml
% conda uninstall -n base --all

CondaEnvironmentError: cannot remove root environment,
       add -n NAME or -p PREFIX option
</code></pre>
<p>Orright, I'll use <code>-p</code> then...</p>
<pre><code>% conda uninstall -p ~/.local/share/miniconda3 --all

CondaEnvironmentError: cannot remove root environment,
       add -n NAME or -p PREFIX option
</code></pre>
<p>How do I uninstall all installed packages in the <code>base</code> or <code>root</code> environment?</p>
",111437,5,79,3,python;conda;miniconda,2018-10-16 13:12:22,2018-10-16 13:12:22,2022-06-18 15:09:47,i installed a bunch of machine learning packages in my base conda environment  i ve now created a ml environment for machine learning  and wish to reset my base environment by removing all the packages installed there  i ve tried  apparently  i can t remove packages from the current environment      so lets switch to my ml environment first  orright  i ll use  p then    how do i uninstall all installed packages in the base or root environment 
172,172,17306977,72666463,Machine Learning Using MATLAB but get low precision predictions,"<p>I'm using the regression learner to train the model, then I generate the code and modify the code parameters. But the predictions of my output are high accuracy but low precise. My algorithm is quadratic SVM. Does anyone know what is the problem? You can see the prediction for 4ns and 60n are different from the 14ns. My input dataset size is 1776x1024 and my response data is 1776x1. The BFS 14ns is my ground truth used to train the model.</p>
<p><a href=""https://i.stack.imgur.com/0TQmO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0TQmO.png"" alt=""enter image description here"" /></a></p>
<p>The code can be found in the regression learner app when we have trained the model and then generate the code. I'm using the quadratic SVM to train the model. Since the code is too long, I just show the parameters that I have tuned.</p>
<pre><code>% Train a regression model
% This code specifies all the model options and trains the model.
responseScale = iqr(trainingResponse);
if ~isfinite(responseScale) || responseScale == 0.0
    responseScale = 1.0;
end
boxConstraint = responseScale/1.349;
epsilon = responseScale/13.49;
regressionSVM = fitrsvm(...
    trainingPredictors, ...
    trainingResponse, ...
    'KernelFunction', 'polynomial', ...
    'PolynomialOrder', 3, ...
    'KernelScale', 'auto', ...
    'KernelOffset', 0, ...
    'BoxConstraint', 1, ...
    'ClipAlphas', t, ...
    'CacheSize','maximal', ...
    'ShrinkagePeriod',1000, ...
    'Epsilon', 0.1, ...  
    'Standardize', true);
</code></pre>
",22,0,-1,4,matlab;machine-learning;regression;svm,2022-06-18 08:53:30,2022-06-18 08:53:30,2022-06-18 08:53:30,i m using the regression learner to train the model  then i generate the code and modify the code parameters  but the predictions of my output are high accuracy but low precise  my algorithm is quadratic svm  does anyone know what is the problem  you can see the prediction for ns and n are different from the ns  my input dataset size is x and my response data is x  the bfs ns is my ground truth used to train the model   the code can be found in the regression learner app when we have trained the model and then generate the code  i m using the quadratic svm to train the model  since the code is too long  i just show the parameters that i have tuned 
173,173,15371286,72107817,I Cannot Install SQL Server 2019 Express. It gets stuck on Offline Installation of Microsoft Machine Learning Server Components,"<p>I'm trying to install SQL Server 2019 Express on my laptop. I initially click on custom to start and everything seems to go smooth up to the point where it's at the Offline Installation of Microsoft Machine Learning Server Components section.</p>
<p>When I get there I get a screenshot that looks like this:<br>
<p><img src=""https://bobgatto.com/images/sql_install.jpg""></p>
<p>From this point on I cannot figure out what to do next. I tried creating a directory, downloading all of the files listed to that directory, and enter that directory in the Install Path line. But when I do that the Next button still is disabled.</p>
<p>So what is the next step?</p>
<p>Thanks for any help</p> 
",2180,3,2,1,sql-server,2022-05-04 09:37:37,2022-05-04 09:37:37,2022-06-18 07:00:14,i m trying to install sql server  express on my laptop  i initially click on custom to start and everything seems to go smooth up to the point where it s at the offline installation of microsoft machine learning server components section   from this point on i cannot figure out what to do next  i tried creating a directory  downloading all of the files listed to that directory  and enter that directory in the install path line  but when i do that the next button still is disabled  so what is the next step  thanks for any help
174,174,18181916,72663516,Are RDS files &quot;More Efficient&quot; than CSV Files?,"<p>While working in R, I have made the following informal observation:</p>
<ul>
<li><p>I have noticed that I am able to import RDS files much faster compared to similar sized CSV files.</p>
</li>
<li><p>For example, suppose I have a CSV file on my computer. If I import this CSV file into R, use the &quot;saveRDS&quot; command to save this file as an RDS, and then use the &quot;readRDS&quot; command to re-import this same file - it seems to take less time to import an RDS version of this file compared to the CSV version of this same file.</p>
</li>
</ul>
<p>For example:</p>
<p><strong>Step 1</strong> I create a file for my hypothesis</p>
<pre><code># create file (i.e. imagine this file currently exists on the computer in CSV format)

test_file = data.frame(col1 = sample.int(100, 1000000, replace = TRUE), col2 = sample.int(100, 1000000, replace = TRUE), col3 = sample.int(100, 1000000, replace = TRUE), col4 = sample.int(100, 1000000, replace = TRUE), col5 = sample.int(100, 1000000, replace = TRUE), col6 = sample.int(100, 1000000, replace = TRUE), col7 = sample.int(100, 1000000, replace = TRUE), col8 = sample.int(100, 1000000, replace = TRUE), col9 = sample.int(100, 1000000, replace = TRUE), col10 = sample.int(100, 1000000, replace = TRUE), col11 = sample.int(100, 1000000, replace = TRUE), col12 = sample.int(100, 1000000, replace = TRUE), col13 = sample.int(100, 1000000, replace = TRUE), col14 = sample.int(100, 1000000, replace = TRUE), col15 = sample.int(100, 1000000, replace = TRUE), col16 = sample.int(100, 1000000, replace = TRUE), col17 = sample.int(100, 1000000, replace = TRUE), col18 = sample.int(100, 1000000, replace = TRUE), col19 = sample.int(100, 1000000, replace = TRUE), col20 = sample.int(100, 1000000, replace = TRUE))
</code></pre>
<p><strong>Step 2:</strong> Compare export times: (RDS is faster)</p>
<pre><code> start.time &lt;- Sys.time()
 write.csv(test_file, &quot;test_file.csv&quot;)
 end.time &lt;- Sys.time()
 end.time - start.time

#Time difference of 28.84087 secs

 
 start.time &lt;- Sys.time()
 saveRDS(test_file, &quot;test_file.RDS&quot;)
 end.time &lt;- Sys.time()
  end.time - start.time

#Time difference of 11.96845 secs
</code></pre>
<p><strong>Step 3:</strong> Compare the sizes of both files: RDS is smaller (roughly 2 times)</p>
<pre><code>#I think this in bytes?

&gt; file.info(&quot;test_file.csv&quot;)$size
[1] 68287349


&gt; file.info(&quot;test_file.RDS&quot;)$size
[1] 26169028

68287349/26169028
[1] 2.609472
</code></pre>
<p><strong>Step 4:</strong> Compare the import times : (RDS is faster)</p>
<pre><code> start.time &lt;- Sys.time()
 test = read.csv(&quot;test_file.csv&quot;)
 end.time &lt;- Sys.time()
 end.time - start.time
#Time difference of 8.349364 secs


 start.time &lt;- Sys.time()
 test = readRDS(&quot;test_file.RDS&quot;)
 end.time &lt;- Sys.time()
 end.time - start.time
#Time difference of 0.59674 secs
</code></pre>
<p>Based on these measurements, it seems that RDS is the clear winner - when compared to CSV, RDS takes less time to export and import, as well as occupies less space. My naive interpretation of this is that RDS is a &quot;native file type&quot; for R, and therefore RDS files might have been somehow programmed to naturally run faster in R compared to CSV files - but I am not sure about this.</p>
<p>I was wondering if there was any factual basis to this claim (e.g. perhaps if I were to repeat this experiment again or were to attempt this experiment on larger/smaller files with different data types, the results might be different), or have I incorrectly carried out this experiment?</p>
<p>PS: In the future, I am planning on running Machine Learning/Statistical models on large datasets - in this regard, I am wondering if it might be more advantageous to convert the datasets to RDS and thus save time/resources?</p>
",49,0,0,3,r;csv;memory,2022-06-18 00:07:26,2022-06-18 00:07:26,2022-06-18 05:20:41,while working in r  i have made the following informal observation  i have noticed that i am able to import rds files much faster compared to similar sized csv files  for example  suppose i have a csv file on my computer  if i import this csv file into r  use the  saverds  command to save this file as an rds  and then use the  readrds  command to re import this same file   it seems to take less time to import an rds version of this file compared to the csv version of this same file  for example  step  i create a file for my hypothesis step   compare export times   rds is faster  step   compare the sizes of both files  rds is smaller  roughly  times  step   compare the import times    rds is faster  based on these measurements  it seems that rds is the clear winner   when compared to csv  rds takes less time to export and import  as well as occupies less space  my naive interpretation of this is that rds is a  native file type  for r  and therefore rds files might have been somehow programmed to naturally run faster in r compared to csv files   but i am not sure about this  i was wondering if there was any factual basis to this claim  e g  perhaps if i were to repeat this experiment again or were to attempt this experiment on larger smaller files with different data types  the results might be different   or have i incorrectly carried out this experiment  ps  in the future  i am planning on running machine learning statistical models on large datasets   in this regard  i am wondering if it might be more advantageous to convert the datasets to rds and thus save time resources 
175,175,930125,10074385,Linux daemon localhost works but not the actual IP,"<p>I am learning APUE.2e (Advanced Programming in the UNIX® Environment, Second Edition) these days and have got to the chapter 16 Network IPC: sockets. When I run the program <code>ruptime</code> (for the client end) and <code>ruptimed</code> (for the server end), something weird happened. The client program shows a client command that communicates with a server to obtain the output from a
system's <code>uptime</code> command, and the server program <code>ruptimed</code> (note the 'd') receives the command, exec's the <code>uptime</code> command and return the output of <code>uptime</code> to the client. The two programs compiled OK, and I add the <code>ruptime   4000/tcp</code> to the file <code>/etc/service</code> which means the service <code>ruptime</code> binds to the port <code>4000/tcp</code>. I ran the two programs on the same machine (Ubuntu 11.04) and the <code>ruptimed</code> first (of course):</p>
<pre><code>$ ./ruptimed
$ ./ruptime 127.0.0.1
 21:35:48 up 13:06,  3 users,  load average: 0.56, 0.85, 0.94 
</code></pre>
<p>The output is what I have expected. However, when I ran:</p>
<pre><code>$ ./ruptimed
$ ./ruptime 192.168.1.221
  # the terminal blocks here
</code></pre>
<p>The IP address 192.168.1.221 is the machine's actual IP in the intranet. What's wrong here? I searched the Internet, and found out it seems to need a shell script in the <code>/etc/init.d/</code>. Then I add a file named <code>ruptime</code> in the <code>/etc/init.d/</code> directory. Here it is:</p>
<pre><code>#! /bin/sh

### BEGIN INIT INFO
# Provides:     ruptimed
# Required-Start:   $remote_fs $syslog
# Required-Stop:    $remote_fs $syslog
# Default-Start:    2 3 4 5
# Default-Stop:     
# Short-Description:    ruptime
### END INIT INFO
start()
{
    echo &quot;start ruptime&quot;
    /home/tlh1987/apue-practice/tlh-practice/ruptimed
    exit 0;
}
stop()
{
    killall ruptimed
    echo &quot;stop ruptime&quot;
}

case &quot;$1&quot; in
    start)
    start
    ;;
    stop)
    stop
    ;;
    restart)
    stop
    start
    ;;
    *)
    echo &quot;usage: $0 start|stop|restart&quot;
    exit 0;
esac
</code></pre>
<p>However, it didn't work and I don't know why. I also set the ufw enabled and allowed ruptime
like this:</p>
<pre><code>sudo ufw allow ruptime
</code></pre>
<p>It didn't work either. The source code for <code>ruptime</code> and <code>ruptimed</code> are as follows:</p>
<h3>connect_retry()</h3>
<pre><code>// the  functions used in the two program
// the  connect_retry() function 
#include &quot;apue.h&quot;
#include &lt;sys/socket.h&gt;

#define MAXSLEEP 128

int
connect_retry(int sockfd, const struct sockaddr *addr, socklen_t alen)
{
  int nsec;

  for (nsec = 1; nsec &lt;= MAXSLEEP; nsec &lt;&lt;= 1) {
    if (connect(sockfd, addr, alen) == 0) {
      return (0);
    }
    if (nsec &lt;= MAXSLEEP/2)
      sleep(nsec);
  }
  return (-1);
}
</code></pre>
<h3>init_server()</h3>
<pre><code>// the initserver function 
#include &quot;apue.h&quot;
#include &lt;errno.h&gt;
#include &lt;sys/socket.h&gt;

int
initserver(int type, const struct sockaddr *addr, socklen_t alen, 
       int qlen)
{
  int fd;
  int err = 0;
  if ((fd = socket(addr-&gt;sa_family, type, 0)) &lt; 0)
    return (-1);
  if (bind(fd, addr, alen) &lt; 0) {
    err = errno;
    goto errout;
  }
  if (type == SOCK_STREAM || type == SOCK_SEQPACKET) {
    if (listen(fd, qlen) &lt; 0) {
      err = errno;
      goto errout;
    }
  }
  return(fd);

 errout:
  close(fd);
  errno = err;
  return (-1);
}
</code></pre>
<h3>The client end</h3>
<pre><code>// the client end i.e. ruptime.c
#include &quot;apue.h&quot;
#include &lt;netdb.h&gt;
#include &lt;errno.h&gt;
#include &lt;sys/socket.h&gt;

#define MAXADDRLEN 256

#define BUFLEN 128
extern int connect_retry(int, const struct sockaddr *, socklen_t);

void
print_uptime(int sockfd)
{
  int n;
  char buf[BUFLEN];

  while ((n = recv(sockfd, buf, BUFLEN, 0)) &gt; 0)
    write(STDOUT_FILENO, buf, n);
  if (n &lt; 0)
    err_sys(&quot;recv error&quot;);
}

int
main(int argc, char *argv[])
{
  struct addrinfo   *ailist, *aip;
  struct addrinfo    hint;
  int            sockfd, err;

  if (argc !=2 )
    err_quit(&quot;usage: ruptime hostname&quot;);
  hint.ai_flags = 0;
  hint.ai_family = 0;
  hint.ai_socktype = SOCK_STREAM;
  hint.ai_protocol = 0;
  hint.ai_addrlen = 0;
  hint.ai_canonname = NULL;
  hint.ai_addr = NULL;
  hint.ai_next = NULL;
  if ((err = getaddrinfo(argv[1], &quot;ruptime&quot;, &amp;hint, &amp;ailist)) != 0)
    err_quit(&quot;getaddrinfo error: %s&quot;, gai_strerror(err));
  for (aip = ailist; aip != NULL; aip = aip-&gt;ai_next) {
    if ((sockfd = socket(aip-&gt;ai_family, SOCK_STREAM, 0)) &lt; 0)
      err = errno;
    if (connect_retry(sockfd, aip-&gt;ai_addr, aip-&gt;ai_addrlen) &lt; 0) {
      err = errno;
    } else {
      print_uptime(sockfd);
      exit(0);
    }
  }
  fprintf(stderr, &quot;can't connect to %s: %s\n&quot;, argv[1], 
      strerror(err));
  exit(1);
}
</code></pre>
<h3>The server end</h3>
<pre><code>//the server end i.e. ruptimed is as follows:
#include &quot;apue.h&quot;
#include &lt;netdb.h&gt;
#include &lt;errno.h&gt;
#include &lt;syslog.h&gt;
#include &lt;sys/socket.h&gt;

#define BUFLEN 128
#define QLEN 10

#ifndef HOST_NAME_MAX
#define HOST_NAME_MAX 256
#endif

extern int initserver(int, struct sockaddr *, socklen_t, int);

void
serve(int sockfd)
{
  int   clfd;
  FILE *fp;
  char buf[BUFLEN];

  for ( ; ; ) {
    clfd = accept(sockfd, NULL, NULL);
    if (clfd &lt; 0) {
      syslog(LOG_ERR, &quot;ruptimed: accept error: %s&quot;, 
         strerror(errno));
      exit(1);
    }
    if ((fp = popen(&quot;/usr/bin/uptime&quot;, &quot;r&quot;)) == NULL) {
      sprintf(buf, &quot;error: %s\n&quot;, strerror(errno));
      send(clfd, buf, strlen(buf), 0);
    } else {
      while (fgets(buf, BUFLEN, fp) != NULL)
    send(clfd, buf, strlen(buf), 0);
      pclose(fp);
    }
    close(clfd);
  }
}

int 
main(int argc, char *argv[])
{
  struct addrinfo   *ailist, *aip;
  struct addrinfo    hint;
  int            sockfd, err, n;
  char          *host;

  if (argc != 1)
    err_quit(&quot;usage: ruptimed&quot;);
#ifdef _SC_HOST_NAME_MAX
  n = sysconf(_SC_HOST_NAME_MAX);
  if (n &lt; 0)
#endif 
    n = HOST_NAME_MAX;
  host = malloc(n);
  if (host == NULL)
    err_sys(&quot;malloc error&quot;);
  if (gethostname(host, n) &lt; 0)
    err_sys(&quot;gethostname error&quot;);
  daemonize(&quot;ruptimed&quot;);
  hint.ai_flags = AI_CANONNAME;
  hint.ai_family = 0;
  hint.ai_socktype = SOCK_STREAM;
  hint.ai_protocol = 0;
  hint.ai_addrlen = 0;
  hint.ai_canonname = NULL;
  hint.ai_addr = NULL;
  hint.ai_next = NULL;
  if ((err = getaddrinfo(host, &quot;ruptime&quot;, &amp;hint, &amp;ailist)) != 0) {
    syslog(LOG_ERR, &quot;ruptimed: getaddrinfo error: %s&quot;, 
       gai_strerror(err));
    exit(1);
  }
  for (aip = ailist; aip != NULL; aip = aip-&gt;ai_next) {
    if ((sockfd = initserver(SOCK_STREAM, aip-&gt;ai_addr, 
                 aip-&gt;ai_addrlen, QLEN)) &gt;= 0)
      serve(sockfd);
    exit(0);
  }
  exit(1);
}
</code></pre>
<p>Is there anyone can help?</p>
<p>The header lines of the command netstat -nap:</p>
<pre><code>Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:4000          0.0.0.0:*               LISTEN      2252/ruptimed   
tcp        0  13033 192.168.1.221:47380     91.189.89.114:443       LAST_ACK    -               
tcp       74      0 192.168.1.221:43914     91.189.89.76:443        ESTABLISHED 1846/python     
tcp        0      0 127.0.0.1:4000          127.0.0.1:56344         TIME_WAIT   -               
tcp        0      1 192.168.1.221:35442     192.168.1.89:24800      SYN_SENT    1478/synergyc   
tcp        0      0 192.168.1.221:34957     74.125.71.125:5222      ESTABLISHED 1618/chrome     
tcp        0      0 192.168.1.221:57795     203.208.46.163:443      ESTABLISHED 1618/chrome     
tcp6       0      0 :::22                   :::*                    LISTEN      -  
</code></pre>
<p>The 3th record is my service ruptime. Is it something wrong? I don't know much about its meanig.</p>
",2517,3,4,5,c;linux;sockets;service;daemon,2012-04-09 19:33:20,2012-04-09 19:33:20,2022-06-18 01:20:10,the output is what i have expected  however  when i ran  the ip address     is the machine s actual ip in the intranet  what s wrong here  i searched the internet  and found out it seems to need a shell script in the  etc init d   then i add a file named ruptime in the  etc init d  directory  here it is  it didn t work either  the source code for ruptime and ruptimed are as follows  is there anyone can help  the header lines of the command netstat  nap  the th record is my service ruptime  is it something wrong  i don t know much about its meanig 
176,176,13355222,71219215,Convert year value to a periodic value in Pandas DataFrame,"<p>I have a DataFrame as follows:</p>
<pre><code>    close   year    Day Sin     Day Cos   Month Sin  Month Cos   Hour Sin   Hour Cos
0   278.00  2015    -0.790776   -0.612106   -0.5    -0.866025   -0.707107   0.707107
1   278.14  2015    -0.790776   -0.612106   -0.5    -0.866025   -0.500000   0.866025
2   280.00  2015    -0.790776   -0.612106   -0.5    -0.866025   -0.258819   0.965926
3   280.89  2015    -0.897805   -0.440394   -0.5    -0.866025    0.000000   1.000000
4   280.36  2015    -0.897805   -0.440394   -0.5    -0.866025    0.258819   0.965926
</code></pre>
<p>I have converted Days, Months and Hours into corresponding Sin and Cosine values since they are periodic/cyclical values which repeat themselves after a certain interval. I did that as follows:</p>
<pre><code>import numpy as np
#Month
df1['Month Sin'] = np.sin(2*np.pi*df1.month/12)
df1['Month Cos'] = np.cos(2*np.pi*df1.month/12)

#Hour
df1['Hour Sin'] = np.sin(2*np.pi*df1.hour/24)
df1['Hour Cos'] = np.cos(2*np.pi*df1.hour/24)
</code></pre>
<p>But I am getting stuck on how to convert <code>year</code> value into a meaningful value since <code>year</code> is not periodic/cyclical value. It increments as the time progresses. I want to give all the features like year, day sin, day cosine, month sin, month cos etc to my Machine Learning model to predict stock <code>close</code> value. Is there anyway to extract some meaningful insight from <code>year</code> value so that it may be able to predict accurately stock <code>close</code> value, even in the future say 2023, past and present.</p>
",99,2,0,4,python;python-3.x;pandas;dataframe,2022-02-22 15:28:59,2022-02-22 15:28:59,2022-06-18 00:38:08,i have a dataframe as follows  i have converted days  months and hours into corresponding sin and cosine values since they are periodic cyclical values which repeat themselves after a certain interval  i did that as follows  but i am getting stuck on how to convert year value into a meaningful value since year is not periodic cyclical value  it increments as the time progresses  i want to give all the features like year  day sin  day cosine  month sin  month cos etc to my machine learning model to predict stock close value  is there anyway to extract some meaningful insight from year value so that it may be able to predict accurately stock close value  even in the future say   past and present 
177,177,4533188,72657318,How is Flyte tailored to &quot;Data and Machine Learning&quot;?,"<p><a href=""https://flyte.org/"" rel=""nofollow noreferrer"">https://flyte.org/</a> says that it is</p>
<blockquote>
<p>The Workflow Automation Platform for Complex, Mission-Critical Data and Machine Learning Processes at Scale</p>
</blockquote>
<p>I went through quite a bit of documentation and I fail to see why it is &quot;Data and Machine Learning&quot;. It seem to me that it is a workflow manager on top of a container orchastration (here Kubernetes), where workflow manager means, that I can define a Directed Acyclic Graphs (DAG) and then the DAG nodes are deployed as containers and the DAG is run.</p>
<p>Of course this is usefull and important for &quot;Data and Machine Learning&quot;, but I might as well use it for any other microservice DAG with this. Except for features/details, how is this different than <a href=""https://airflow.apache.org"" rel=""nofollow noreferrer"">https://airflow.apache.org</a> or other workflow managers (of which there are many). There are even more specialized workflow managers for &quot;Data and Machine Learning&quot;, e.g., <a href=""https://spark.apache.org"" rel=""nofollow noreferrer"">https://spark.apache.org</a>.</p>
<p>What should I keep in mind as a Software Achitect?</p>
",67,1,0,4,airflow;workflow;directed-acyclic-graphs;flyte,2022-06-17 15:07:01,2022-06-17 15:07:01,2022-06-17 23:58:15, says that it is the workflow automation platform for complex  mission critical data and machine learning processes at scale i went through quite a bit of documentation and i fail to see why it is  data and machine learning   it seem to me that it is a workflow manager on top of a container orchastration  here kubernetes   where workflow manager means  that i can define a directed acyclic graphs  dag  and then the dag nodes are deployed as containers and the dag is run  of course this is usefull and important for  data and machine learning   but i might as well use it for any other microservice dag with this  except for features details  how is this different than  or other workflow managers  of which there are many   there are even more specialized workflow managers for  data and machine learning   e g     what should i keep in mind as a software achitect 
178,178,4161241,72662807,What is the term for using a Neural Network to create new data based on training data?,"<p>I have a large set of Training data which consists of various texts. They should be the input for my neural network. I have no output, or I don't know what to put as output.</p>
<p>Anyway, after the learning phase I want the neural network to create new texts based on the training data.
I read about this like „I made a bot watch 1000 hours of xy and asked it to write a new xy“.</p>
<p>Now my question is, what kind of machine learning is this? I am not looking for instructions on how to write it, but just a hint on how to find some keywords or tutorials. My Google searches so far were useless.</p>
",21,1,-2,2,machine-learning;neural-network,2022-06-17 22:49:49,2022-06-17 22:49:49,2022-06-17 23:22:01,i have a large set of training data which consists of various texts  they should be the input for my neural network  i have no output  or i don t know what to put as output  now my question is  what kind of machine learning is this  i am not looking for instructions on how to write it  but just a hint on how to find some keywords or tutorials  my google searches so far were useless 
179,179,14225979,72662322,How do I get my machine learning app to correctly recognise vegetable images?,"<p>I have generated a custom image classifier model using Create ML in Xcode and images of vegetables I have found on <a href=""http://www.kaggle.com"" rel=""nofollow noreferrer"">www.kaggle.com</a>.</p>
<p>I have built an iOS app in Xcode to use this model to recognise images of vegetables in the Xcode simulator but its not correctly recognising the vegetable images. I select the image of a carrot for example and it says its a tomato.</p>
<p>Here is my view controller file;</p>
<pre><code>//
//  ViewController.swift
//  WhatVegetable
//
//  Created by Stephen Learmonth on 16/06/2022.
//

import UIKit
import CoreML
import Vision
import Alamofire
import SwiftyJSON
import SDWebImage

class ViewController: UIViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate {

    @IBOutlet weak var imageView: UIImageView!
    @IBOutlet weak var extractLabel: UILabel!
    @IBOutlet weak var activityIndicator: UIActivityIndicatorView!
    
    let imagePicker = UIImagePickerController()
    
    let wikipediaURL = &quot;http://en.wikipedia.org/w/api.php&quot;
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        imagePicker.delegate = self
        imagePicker.allowsEditing = false
        imagePicker.sourceType = .photoLibrary
        
        activityIndicator.stopAnimating()

    }
    
    @IBAction func cameraWasTapped(_ sender: UIBarButtonItem) {
        
        present(imagePicker, animated: true)
        
    }
    
    func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
        
        if let userPickedImage = info[UIImagePickerController.InfoKey.originalImage] as? UIImage {
            
            guard let convertedCIImage = CIImage(image: userPickedImage) else {
                fatalError(&quot;Cannot convert to CIImage&quot;)
            }
            
            detect(image: convertedCIImage)
            
        }
        
        dismiss(animated: true, completion: nil)
    }
    
    private func detect(image: CIImage) {
        let config = MLModelConfiguration()
        
        guard let coreMLModel = try? VegetableClassifier(configuration: config),
              let model = try? VNCoreMLModel(for: coreMLModel.model) else {
            fatalError(&quot;Loading CoreML Model failed.&quot;)
        }
        
        let request = VNCoreMLRequest(model: model) { request, error in
            guard let classification = request.results?.first as? VNClassificationObservation else {
                fatalError(&quot;Could not classify image.&quot;)
            }
            
            self.title = classification.identifier.capitalized
            
            self.requestInfo(objectName: classification.identifier)
            
        }
        
        let handler = VNImageRequestHandler(ciImage: image)
        
        do {
            try handler.perform([request])
        } catch {
            print(error)
        }
    }

    private func requestInfo(objectName: String) {
        
        activityIndicator.startAnimating()
        
        let outerParameters  : [String : String] = [
            &quot;format&quot; : &quot;json&quot;,
            &quot;action&quot; : &quot;query&quot;,
            &quot;prop&quot; : &quot;extracts|images&quot;,
            &quot;imlimit&quot; : &quot;max&quot;,
            &quot;exintro&quot; : &quot;&quot;,
            &quot;explaintext&quot; : &quot;&quot;,
            &quot;titles&quot;: objectName,
            &quot;indexpageids&quot; : &quot;&quot;,
            &quot;redirects&quot; : &quot;1&quot;]
        
        Alamofire.request(wikipediaURL, method: .get, parameters: outerParameters).responseJSON { response in
            if response.result.isSuccess {
                
                let objectJSON : JSON = JSON(response.result.value as Any)
                let pageId = objectJSON[&quot;query&quot;][&quot;pageids&quot;][0].stringValue
                let objectDescription = objectJSON[&quot;query&quot;][&quot;pages&quot;][pageId][&quot;extract&quot;].stringValue
                
                var imageTitle = &quot;&quot;
                guard let objectImageDicts = objectJSON[&quot;query&quot;][&quot;pages&quot;][pageId][&quot;images&quot;].arrayObject as? [[String:Any]] else { return }
                let subStrings = objectName.split(separator: &quot; &quot;)
                
            outerloop: for dict in objectImageDicts {
                if let title = dict[&quot;title&quot;] as? String {
                    for subString in subStrings {
                        if let _ = title.range(of: String(subString), options: .caseInsensitive) {
                            imageTitle = title
                            break outerloop
                        }
                    }
                }
            }
                
                if imageTitle != &quot;&quot; {
                    
                    let innerParameters : [String : String] =
                    [&quot;format&quot; : &quot;json&quot;,
                     &quot;action&quot; : &quot;query&quot;,
                     &quot;prop&quot; : &quot;imageinfo&quot;,
                     &quot;titles&quot; : imageTitle,
                     &quot;iiprop&quot; : &quot;url&quot;]
                    
                    Alamofire.request(self.wikipediaURL, method: .get, parameters: innerParameters).responseJSON { response in
                        
                        if response.result.isSuccess {
                            
                            let objectJSON : JSON = JSON(response.result.value as Any)
                            let imageURL = objectJSON[&quot;query&quot;][&quot;pages&quot;][&quot;-1&quot;][&quot;imageinfo&quot;][0][&quot;url&quot;].stringValue
                            
                            self.activityIndicator.stopAnimating()
                            
                            self.imageView.sd_setImage(with: URL(string: imageURL))
                        }
                    }
                } else {
                    
                    self.activityIndicator.stopAnimating()
                    
                    self.imageView.image = UIImage(systemName: &quot;questionmark.app&quot;)
                }
                
                self.extractLabel.text = objectDescription
                
            }
        }
    }
}
</code></pre>
<p>The image detection code is in the function <code>detect(image:)</code>. The other function <code>requestInfo(objectName:)</code> just fetches information from Wikipedia to display about the recognised vegetable.</p>
<p>Create ML in Xcode shows the model works very well with test images reporting Precision and Recall percentages of almost 100% nearly all the time.</p>
<p>I would appreciate it if someone could confirm my swift code is correct for using the custom model <code>VegetableClassifier</code>.</p>
",40,0,-2,5,swift;xcode;machine-learning;coreml;vision,2022-06-17 22:00:40,2022-06-17 22:00:40,2022-06-17 22:09:23,i have generated a custom image classifier model using create ml in xcode and images of vegetables i have found on   i have built an ios app in xcode to use this model to recognise images of vegetables in the xcode simulator but its not correctly recognising the vegetable images  i select the image of a carrot for example and it says its a tomato  here is my view controller file  the image detection code is in the function detect image    the other function requestinfo objectname   just fetches information from wikipedia to display about the recognised vegetable  create ml in xcode shows the model works very well with test images reporting precision and recall percentages of almost   nearly all the time  i would appreciate it if someone could confirm my swift code is correct for using the custom model vegetableclassifier 
180,180,17810039,72661223,Using a coding language other than dart in Flutter,"<p>Can I use a coding language other than dart in Flutter?<br />
For example, I want to install a server, I want to use javascript for this.or I want to do machine learning can I write the code with python in it?</p>
<p>can I call a function I wrote in python in dart?<br />
for example:<br />
a function written in python = pytFunc()</p>
<p>dart code [</p>
<p>var x = pytFunc();</p>
<p>]</p>
",34,0,-1,2,flutter;dart,2022-06-17 20:22:05,2022-06-17 20:22:05,2022-06-17 20:22:05,dart code   var x   pytfunc     
181,181,18322549,72658548,How to create a abstractive summary using supervised Machine learning?,"<p>I want to summarise my document to about 7-8% of total words in document.The summary should be abstractive not extractive. I've referred some of the previous abstractive summarisation strategies however they're using deep learning models like seq2seq, lstm, etc. And I would like to do this task using some basic supervised Machine learning algorithms like svm, logistic regression, etc. The accuracy is not a concern for me. I did a lot of research but was not able to get something relevant.</p>
",16,0,0,3,machine-learning;nlp;summarization,2022-06-17 16:44:36,2022-06-17 16:44:36,2022-06-17 17:42:05,i want to summarise my document to about    of total words in document the summary should be abstractive not extractive  i ve referred some of the previous abstractive summarisation strategies however they re using deep learning models like seqseq  lstm  etc  and i would like to do this task using some basic supervised machine learning algorithms like svm  logistic regression  etc  the accuracy is not a concern for me  i did a lot of research but was not able to get something relevant 
182,182,14027466,72632081,Adjusting dataset/model from example to predict value four timesteps ahead,"<p>I'm just beginning to start with machine learning and want to predict values/sales in a timeseries. I found this two blog posts, which basically match what I'm looking for.</p>
<ol>
<li><a href=""https://iq.opengenus.org/time-series-prediction/"" rel=""nofollow noreferrer"">Basics of Time Series Prediction</a> - Setup of the Timeseries and Datasets found in here</li>
<li><a href=""https://iq.opengenus.org/time-series-prediction-techniques/"" rel=""nofollow noreferrer"">Techniques for Time Series Prediction</a> - NN Setup in here</li>
</ol>
<p>Instead of predicting the value for the next timestep I would like to predict the value 4 timesteps ahead. Originally I have weekly data, so I want to predict the value 4 weeks / 1 month ahead.</p>
<p>As I understand this, I therefor need to change the &quot;label&quot; the model is trained with, which will be done within the function <code>windowed_dataset()</code> (Source 2).</p>
<pre><code>def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  dataset = tf.data.Dataset.from_tensor_slices(series)
  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) # &lt;-- change will be in here
  dataset = dataset.batch(batch_size).prefetch(1)
  return dataset
</code></pre>
<p>If I change <code>dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))</code> to <code>dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-4], window[-1]))</code> the labels in my opinion are correctly adjusted to my goal.</p>
<p>But running the next step</p>
<pre><code>dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
print(dataset)
l0 = tf.keras.layers.Dense(1, input_shape=[window_size])
model = tf.keras.models.Sequential([l0])


model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))
model.fit(dataset,epochs=100,verbose=0)
</code></pre>
<p>throws an error:</p>
<pre><code>runcell('Build model', 'C:/Users/USER/Desktop/Local/Prediction/untitled0.py')
&lt;PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))&gt;
Traceback (most recent call last):

  File &quot;C:\Users\USER\Desktop\Local\Prediction\untitled0.py&quot;, line 102, in &lt;module&gt;
    model.fit(dataset,epochs=100,verbose=0)

  File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;C:\Users\USER\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

InvalidArgumentError: Graph execution error:

Detected at node 'sequential_9/dense_10/BiasAdd' defined at (most recent call last):
    File &quot;C:\Users\USER\Anaconda3\lib\runpy.py&quot;, line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File &quot;C:\Users\USER\Anaconda3\lib\runpy.py&quot;, line 87, in _run_code
      exec(code, run_globals)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\spyder_kernels\console\__main__.py&quot;, line 23, in &lt;module&gt;
      start.main()
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\spyder_kernels\console\start.py&quot;, line 328, in main
      kernel.start()
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\kernelapp.py&quot;, line 677, in start
      self.io_loop.start()
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\tornado\platform\asyncio.py&quot;, line 199, in start
      self.asyncio_loop.run_forever()
    File &quot;C:\Users\USER\Anaconda3\lib\asyncio\base_events.py&quot;, line 596, in run_forever
      self._run_once()
    File &quot;C:\Users\USER\Anaconda3\lib\asyncio\base_events.py&quot;, line 1890, in _run_once
      handle._run()
    File &quot;C:\Users\USER\Anaconda3\lib\asyncio\events.py&quot;, line 80, in _run
      self._context.run(self._callback, *self._args)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 457, in dispatch_queue
      await self.process_one()
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 446, in process_one
      await dispatch(*args)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 353, in dispatch_shell
      await result
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\kernelbase.py&quot;, line 648, in execute_request
      reply_content = await reply_content
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\ipkernel.py&quot;, line 353, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\ipykernel\zmqshell.py&quot;, line 533, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 2901, in run_cell
      result = self._run_cell(
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 2947, in _run_cell
      return runner(coro)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\async_helpers.py&quot;, line 68, in _pseudo_sync_runner
      coro.send(None)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3172, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3364, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py&quot;, line 3444, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;C:\Users\USER\AppData\Local\Temp/ipykernel_15784/4252985979.py&quot;, line 1, in &lt;module&gt;
      runcell('Build model', 'C:/Users/USER/Desktop/Local/Prediction/untitled0.py')
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py&quot;, line 673, in runcell
      exec_code(cell_code, filename, ns_globals, ns_locals,
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py&quot;, line 465, in exec_code
      exec(compiled, ns_globals, ns_locals)
    File &quot;C:\Users\USER\Desktop\Local\Prediction\untitled0.py&quot;, line 102, in &lt;module&gt;
      model.fit(dataset,epochs=100,verbose=0)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1051, in train_function
      return step_function(self, iterator)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1030, in run_step
      outputs = model.train_step(data)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 889, in train_step
      y_pred = self(x, training=True)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 490, in __call__
      return super().__call__(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\base_layer.py&quot;, line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\sequential.py&quot;, line 374, in call
      return super(Sequential, self).call(inputs, training=training, mask=mask)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\functional.py&quot;, line 458, in call
      return self._run_internal_graph(
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\functional.py&quot;, line 596, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\engine\base_layer.py&quot;, line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;C:\Users\USER\Anaconda3\lib\site-packages\keras\layers\core\dense.py&quot;, line 232, in call
      outputs = tf.nn.bias_add(outputs, self.bias)
Node: 'sequential_9/dense_10/BiasAdd'
Matrix size-incompatible: In[0]: [24,9], In[1]: [12,1]
     [[{{node sequential_9/dense_10/BiasAdd}}]] [Op:__inference_train_function_231055]
</code></pre>
<p>What am I missing here? Is there another, better approach to model the timeseries?</p>
<p><em>Note: Somewhere in the future I also would like to add more parameters/indicators to the model to test if this increases the accuracy.</em></p>
<p><strong>Edit:</strong>
Creation of Data and Series:</p>
<pre><code>#%% Setup 

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras

#%% Creating Timeseries

def plot_series(time, series, format=&quot;-&quot;, start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel(&quot;Time&quot;)
    plt.ylabel(&quot;Value&quot;)
    plt.grid(True)

def trend(time, slope=0):
    return slope * time

def seasonal_pattern(season_time):
    &quot;&quot;&quot;Just an arbitrary pattern, you can change it if you wish&quot;&quot;&quot;
    return np.where(season_time &lt; 0.4,
                    np.cos(season_time * 2 * np.pi),
                    1 / np.exp(3 * season_time))

def seasonality(time, period, amplitude=1, phase=0):
    &quot;&quot;&quot;Repeats the same pattern at each period&quot;&quot;&quot;
    season_time = ((time + phase) % period) / period
    return amplitude * seasonal_pattern(season_time)

def noise(time, noise_level=1, seed=None):
    rnd = np.random.RandomState(seed)
    return rnd.randn(len(time)) * noise_level

time = np.arange(4 * 365 + 1, dtype=&quot;float32&quot;)
baseline = 10
series = trend(time, 0.1)  
baseline = 10
amplitude = 40
slope = 0.05
noise_level = 5

# Create the series
series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)
# Update with noise
series += noise(time, noise_level, seed=42)

plt.figure(figsize=(10, 6))
plot_series(time, series)
plt.show()

#%% Create Data Sets
split_time = 1000
time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]
plt.figure(figsize=(10, 6))
plot_series(time_train, x_train)
plt.show()

plt.figure(figsize=(10, 6))
plot_series(time_valid, x_valid)
plt.show()
</code></pre>
<p>Parameters:</p>
<pre><code>#%% Set Parameters
window_size = 4
batch_size = 4
shuffle_buffer_size = 10
</code></pre>
",20,1,0,3,python;tensorflow;machine-learning,2022-06-15 18:42:25,2022-06-15 18:42:25,2022-06-17 13:47:04,i m just beginning to start with machine learning and want to predict values sales in a timeseries  i found this two blog posts  which basically match what i m looking for  instead of predicting the value for the next timestep i would like to predict the value  timesteps ahead  originally i have weekly data  so i want to predict the value  weeks    month ahead  as i understand this  i therefor need to change the  label  the model is trained with  which will be done within the function windowed_dataset    source    if i change dataset   dataset shuffle shuffle_buffer  map lambda window   window      window      to dataset   dataset shuffle shuffle_buffer  map lambda window   window      window      the labels in my opinion are correctly adjusted to my goal  but running the next step throws an error  what am i missing here  is there another  better approach to model the timeseries  note  somewhere in the future i also would like to add more parameters indicators to the model to test if this increases the accuracy  parameters 
183,183,14526913,72655204,Axios Get reques to Express backend implementation,"<p>I'm totally fresh at the back-end, right now learning Fron-End but decided to create my own server with Node.js. I installed Express, Cors, and Axios. It seems that it is working since I can see response from API in my terminal but I cannot make a GET request to fire up my machine.</p>
<p>How can I call the GET method? Now data from API is printed in terminal only, on Back-end I get this error:</p>
<p>**GET http://localhost:3002/ 404 (Not Found)</p>
<p>My back-end code:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const express = require('express');
const bodyParser = require('body-parser');
const router = require('express').Router();
var cors = require('cors');
const axios = require('axios').default;

const app = express();
app.use(cors())

axios.get('https://rickandmortyapi.com/api/character').then(res =&gt; {
    console.log(res.data);
});

app.use('/', router);
app.use(bodyParser.json());
app.listen(3002, () =&gt; {
    console.log(`Port is dummy. At least it started.`);
});</code></pre>
</div>
</div>
</p>
",31,2,0,5,javascript;node.js;express;axios;backend,2022-06-17 12:04:15,2022-06-17 12:04:15,2022-06-17 12:50:45,i m totally fresh at the back end  right now learning fron end but decided to create my own server with node js  i installed express  cors  and axios  it seems that it is working since i can see response from api in my terminal but i cannot make a get request to fire up my machine  how can i call the get method  now data from api is printed in terminal only  on back end i get this error    get http   localhost     not found  my back end code 
184,184,19356852,72655408,How to properly write a machine learning algorithm?,"<p>I am new to machine learning, and I wanted to make a model that can predict a time series graph, and I keep getting errors. Is there something I am missing? I find out that I learn from getting reference code, then modifiying it, and learning over time what each component does.</p>
<pre><code>&quot;&quot;&quot; Original Repository (Reference)
https://github.com/nicknochnack/Tensorflow-in-10-Minutes/blob/main/Tensorflow%20in%2010.ipynb
&quot;&quot;&quot;
import pandas as pd
import numpy
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

df = pd.read_csv('Marble &amp; Slope Internal Data.csv')
x = pd.get_dummies(df['Distance Travelled(CM)'])
y = df['Height Off Ground(CM)']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)
x_train.head()
y_train.head()

model = Sequential()
model.add(Dense(units=32, activation='relu',input_dim=25))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))


model.compile(loss='binary_crossentropy', optimizer='sgd', metrics='accuracy')
model.fit(x_train, y_train, epochs=20, batch_size=32)

myline = numpy.linspace(0, 100, 100)

plt.plot(myline, model.predict(myline),color='#ff8003',linewidth=3)
plt.show()
</code></pre>
<p>Output from SHELL</p>
<pre><code>Epoch 1/20

1/1 [==============================] - ETA: 0s - loss: 0.9882 - accuracy: 0.0000e+00
...

...
1/1 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0000e+00
1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.0000e+00
Epoch 20/20
WARNING:tensorflow:Model was constructed with shape (None, 25) for input KerasTensor(type_spec=TensorSpec(shape=(None, 25), dtype=tf.float32, name='dense_input'), name='dense_input', description=&quot;created by layer 'dense_input'&quot;), but it was called on an input with incompatible shape (None,).
Traceback (most recent call last):
  File &quot;C:\Users\___\OneDrive\Documents\Python\ALGA.py&quot;, line 30, in &lt;module&gt;
    plt.plot(myline, model.predict(myline),color='#ff8003',linewidth=3)
  File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\___\AppData\Local\Temp\__autograph_generated_file5t873kv5.py&quot;, line 15, in tf__predict_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
ValueError: in user code:

    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1845, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1834, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1823, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py&quot;, line 1791, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;C:\Users\___\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\input_spec.py&quot;, line 228, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;sequential&quot; (type Sequential).
    
    Input 0 of layer &quot;dense&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)
    
    Call arguments received by layer &quot;sequential&quot; (type Sequential):
      • inputs=tf.Tensor(shape=(None,), dtype=float32)
      • training=False
      • mask=None
</code></pre>
",54,0,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-06-17 12:24:29,2022-06-17 12:24:29,2022-06-17 12:45:06,i am new to machine learning  and i wanted to make a model that can predict a time series graph  and i keep getting errors  is there something i am missing  i find out that i learn from getting reference code  then modifiying it  and learning over time what each component does  output from shell
185,185,14376959,72653823,Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2,"<p>I have the following model in a jupyter notebook:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers



physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

SIZE = (549, 549)
SHUFFLE = False 
BATCH = 32
EPOCHS = 20

train_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)
test_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)


inp = layers.Input(shape=(*SIZE, 1))

x = layers.Conv2D(filters=549, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)

model = Model(inp, x)

model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())

model.summary()
</code></pre>
<p>Sagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.</p>
<p>nvidia-smi output on SageMaker:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   37C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>nvidia-smi output on EC2:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    51W / 300W |   2460MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     11802      C   /bin/python3.8                    537MiB |
|    0   N/A  N/A     26391      C   python3.8                        1921MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>The model.summary() output on SageMaker is (this is what I want it to be):</p>
<pre class=""lang-py prettyprint-override""><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d (Conv2D)             (None, 549, 549, 1)       7535574   
                                                                 
 batch_normalization (BatchN  (None, 549, 549, 1)      4         
 ormalization)                                                   
                                                                 
 conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   
                                                                 
 batch_normalization_1 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    
                                                                 
 batch_normalization_2 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   
                                                                 
=================================================================
Total params: 13,263,852
Trainable params: 13,263,846
Non-trainable params: 6

</code></pre>
<p>The model.summary() output on EC2 is (notice the shape change):</p>
<pre class=""lang-py prettyprint-override""><code>
Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d (Conv2D)             (None, 549, 549, 549)     14274     
                                                                 
 batch_normalization (BatchN  (None, 549, 549, 549)    2196      
 ormalization)                                                   
                                                                 
 conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   
                                                                 
 batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      
 hNormalization)                                                 
                                                                 
 conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    
                                                                 
 batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   
                                                                 
=================================================================
Total params: 5,749,128
Trainable params: 5,745,834
Non-trainable params: 3,294
_________________________________________________________________
</code></pre>
<p>One other thing that is interesting, if I change my model on the EC2 instance to:</p>
<pre class=""lang-py prettyprint-override""><code>inp = layers.Input(shape=(*SIZE, 1))

x = layers.Conv2D(filters=1, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)

model = Model(inp, x)

model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())
</code></pre>
<p>My model.summary() output becomes:</p>
<pre class=""lang-py prettyprint-override""><code>Model: &quot;model_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        
                                                                 
 batch_normalization_6 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        
                                                                 
 batch_normalization_7 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         
                                                                 
 batch_normalization_8 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        
                                                                 
=================================================================
Total params: 60
Trainable params: 54
Non-trainable params: 6
_________________________________________________________________
</code></pre>
<p>In the last model the shape is correct but the trainable parameters is very low.</p>
<p>Any ideas as to why the output shape is different and why this is happening with the filters?</p>
",28,0,0,5,python;tensorflow;keras;amazon-ec2;amazon-sagemaker,2022-06-17 08:15:17,2022-06-17 08:15:17,2022-06-17 09:49:15,i have the following model in a jupyter notebook  sagemaker and ec are running tensorflow     the ec instance is p xlarge with deep learning ami gpu tensorflow     amazon linux     the sagemaker notebook is using ml p xlarge and i am using the conda_tensorflow_p kernel  the notebook is in an fsx lustre file system that is mounted to both sagemaker and ec so it is definitely the same code running on both machines  nvidia smi output on sagemaker  nvidia smi output on ec  the model summary   output on sagemaker is  this is what i want it to be   the model summary   output on ec is  notice the shape change   one other thing that is interesting  if i change my model on the ec instance to  my model summary   output becomes  in the last model the shape is correct but the trainable parameters is very low  any ideas as to why the output shape is different and why this is happening with the filters 
186,186,12051503,72641789,Azure Machine Learning workspace&#39;s storage account permission issue,"<p>Was working on az ml cli v2 to deploy real-time endpoint with command <code>az ml online-deployment</code> through Azure pipeline. had double confirmed that the service connection used in this pipeline task had added the permissions below in Azure Portal but still showing the same error.</p>
<pre><code>ERROR: Error with code: You don't have permission to alter this storage account. Ensure that you have been assigned both Storage Blob Data Reader and Storage Blob Data Contributor roles.
</code></pre>
<p>Using the same service connection, we are able to perform the creation of online endpoint with <code>az ml online-endpoint create</code> in the same and other workspaces.</p>
",40,1,0,3,azure-devops;azure-storage;azure-machine-learning-service,2022-06-16 12:40:03,2022-06-16 12:40:03,2022-06-17 07:07:08,was working on az ml cli v to deploy real time endpoint with command az ml online deployment through azure pipeline  had double confirmed that the service connection used in this pipeline task had added the permissions below in azure portal but still showing the same error  using the same service connection  we are able to perform the creation of online endpoint with az ml online endpoint create in the same and other workspaces 
187,187,12468754,72652866,Using machine learning to detect fish spawning in audio files,"<p>My friend is doing his thesis related to fish spawning in rivers, for this, he collects hours of data that he then analysis manually in Audacity and looks for specific patterns in the spectrograms that might indicate the sound of fish spawning.</p>
<p>Since he has days' worth of data I proposed a challenge to myself: to create an algorithm that might help him in detecting these patterns.</p>
<p>I am fairly new to Machine Learning, but a junior in programming and this sounds like a fun learning experience.</p>
<p>I identify the main problems as:</p>
<ul>
<li><strong>samples are 1 hour in length.</strong></li>
<li><strong>noise in the background</strong> (such as cars and the rivers)</li>
</ul>
<p>Is this achievable with machine learning or should I look into other options? If yes which ones?</p>
<p>Thank you for taking the time to read!</p>
",16,1,-1,4,python;machine-learning;audio;classification,2022-06-17 04:56:20,2022-06-17 04:56:20,2022-06-17 05:08:45,my friend is doing his thesis related to fish spawning in rivers  for this  he collects hours of data that he then analysis manually in audacity and looks for specific patterns in the spectrograms that might indicate the sound of fish spawning  since he has days  worth of data i proposed a challenge to myself  to create an algorithm that might help him in detecting these patterns  i am fairly new to machine learning  but a junior in programming and this sounds like a fun learning experience  i identify the main problems as  is this achievable with machine learning or should i look into other options  if yes which ones  thank you for taking the time to read 
188,188,17578467,72652042,How to extract .tar file of Imagenet dataset for window in JupyterLab?,"<p>I'm new to machine learning. I'm trying to use the Alexnet model and the imagenet dataset for my learning. However, I don't know how to extract the .tar file of the imagenet dataset on JupyterLab. All the instructions I found are for Linux, but I'm using a Window. Has anyone used the imagenet data on Window before? Please send help!</p>
",17,0,0,5,windows;conv-neural-network;artificial-intelligence;jupyter-lab;imagenet,2022-06-17 02:54:30,2022-06-17 02:54:30,2022-06-17 03:09:03,i m new to machine learning  i m trying to use the alexnet model and the imagenet dataset for my learning  however  i don t know how to extract the  tar file of the imagenet dataset on jupyterlab  all the instructions i found are for linux  but i m using a window  has anyone used the imagenet data on window before  please send help 
189,189,11313341,72651512,How to use tensorflow to solve explicit equation,"<p>I am using tensorflow to process images and semantically segment clouds and classify them. To compare my machine learning results, I want to compare them to some simple measures such as the ratio of red to blue in a given pixel. One heuristic says if this is above 0.7, the pixel is likely a cloud.</p>
<p>I'd like to optimize that single parameter using tensorflow so that the input is an RGB image, and the output is a 2d matrix of a binary truth map: cloud/no cloud for each image. Loss could then be IOU, MAE, or SSE for the classification.</p>
<p>How can I use tensorflow to optimize that single threshold parameter on the train data set and then view performance over the validation and test sets? It would be nice to have all compared models (ML and heuristics) done in the same way for runtime and loss comparison.</p>
<p>An example of an input image:
<a href=""https://i.stack.imgur.com/ZWpZz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZWpZz.png"" alt=""Example Image"" /></a><br />
And the resulting binary truth:
<a href=""https://i.stack.imgur.com/FVEqq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FVEqq.png"" alt=""Example Truth"" /></a></p>
",31,0,0,3,python;tensorflow;computer-vision,2022-06-17 01:54:06,2022-06-17 01:54:06,2022-06-17 02:06:14,i am using tensorflow to process images and semantically segment clouds and classify them  to compare my machine learning results  i want to compare them to some simple measures such as the ratio of red to blue in a given pixel  one heuristic says if this is above    the pixel is likely a cloud  i d like to optimize that single parameter using tensorflow so that the input is an rgb image  and the output is a d matrix of a binary truth map  cloud no cloud for each image  loss could then be iou  mae  or sse for the classification  how can i use tensorflow to optimize that single threshold parameter on the train data set and then view performance over the validation and test sets  it would be nice to have all compared models  ml and heuristics  done in the same way for runtime and loss comparison 
190,190,11420112,72651224,get and send data from android application to python machine learning model,"<p>we made a recommender system (written in python), we want our android application to send a specific item (object) to our model and then the model should return an array of objects (most similar items) to our app.</p>
<p>Idk if we should call it a ML model, it basically converts textual data for items to vectors and then calculates the cosine similarity between a specific vector and all other vectors (items) in the data base. So there’s no training done, i think..</p>
<p>Ik that’s done by building a web service, but we couldn’t figure out how to exactly do that so if you can elaborate more ir comment below links for resources that might be useful</p>
",16,0,-2,5,python;android;flask;android-volley;recommender-systems,2022-06-17 01:22:37,2022-06-17 01:22:37,2022-06-17 01:22:37,we made a recommender system  written in python   we want our android application to send a specific item  object  to our model and then the model should return an array of objects  most similar items  to our app  idk if we should call it a ml model  it basically converts textual data for items to vectors and then calculates the cosine similarity between a specific vector and all other vectors  items  in the data base  so there s no training done  i think   ik that s done by building a web service  but we couldn t figure out how to exactly do that so if you can elaborate more ir comment below links for resources that might be useful
191,191,19354106,72650196,TypeErrorr: fit() missing 1 required positional argument: &#39;y&#39;,"<p>please while i were doing me learning machine i saw that the code gives me these error, can you help me? thz</p>
<pre><code>from asyncore import read
from pyexpat import model
from re import X
from pandas import read_csv
from sklearn.tree import DecisionTreeClassifier
giocatori = read_csv(r&quot;C:\Users\alessandrini\Desktop\giocatori.csv&quot;)
X = giocatori.drop(columns=['videogame'])
y = giocatori['videogame']

modello = DecisionTreeClassifier
modello.fit(y.values, X.values)
previsione = modello.predict([[0,31]])
print(previsione)
</code></pre>
",15,0,0,1,python,2022-06-16 23:42:33,2022-06-16 23:42:33,2022-06-16 23:42:33,please while i were doing me learning machine i saw that the code gives me these error  can you help me  thz
192,192,12763966,72648721,"Error writing TFRecords, Networks reads double the values (Input to reshape is a tensor with n*2 values, but the requested shape has n values)","<p>I'm writing this question as a reminder for myself cause i already know that i will recreate this error again and i don't want to spend again half an hour fixing it.</p>
<p>I'm currently working on a machine learning project, and i encountered an error during the execution of the network:
when i execute the neural network after writing the Tfrecords like this</p>
<pre><code>def write_to_tfrec_spatial(training_directories, path, filename):
  record_file = filename
  n_samples = len(training_directories)
  print()
  print(n_samples)
  with tf.io.TFRecordWriter(record_file) as writer:

    print(&quot;writing&quot;, end=&quot;: &quot;)
    for i in range(n_samples):
      if(i % 50) == 0:
        print()
      print(i, end=&quot;,&quot;)

      dir = path + training_directories[i]

      loaded = np.load(dir)
      ground = loaded[&quot;rad&quot;]

      if normalization:
        ground = ground / max_norm_value
        print(np.amax(ground), end=&quot;,&quot;)

      padded_ground = np.pad(ground, [(3, 2), (0, 0)], mode='constant')
      inputs = data_augmentation(padded_ground)

      for input in inputs:
        tf_example = image_example_spatial(input=input, ground=padded_ground)
        writer.write(tf_example.SerializeToString())
  return record_file
</code></pre>
<p>I then executed the network like this:</p>
<pre><code>model.fit(training_dataset, steps_per_epoch=steps, epochs=60, validation_data=validation_dataset, callbacks=my_callbacks)
</code></pre>
<p>But i get the following error:</p>
<pre><code>2 root error(s) found.
  (0) INVALID_ARGUMENT:  Input to reshape is a tensor with 376832 values, but the requested shape has 188416
     [[{{node Reshape}}]]
     [[IteratorGetNext]]
     [[IteratorGetNext/_428]]
  (1) INVALID_ARGUMENT:  Input to reshape is a tensor with 376832 values, but the requested shape has 188416
     [[{{node Reshape}}]]
     [[IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_165085]
</code></pre>
<p>I can't understand why i get exactly double the values, i checked multiple times the shapes and they were always correct, but the TFRecord keeps returning the wrong number of values</p>
",9,1,0,5,python;tensorflow;keras;deep-learning;neural-network,2022-06-16 21:29:19,2022-06-16 21:29:19,2022-06-16 21:29:19,i m writing this question as a reminder for myself cause i already know that i will recreate this error again and i don t want to spend again half an hour fixing it  i then executed the network like this  but i get the following error  i can t understand why i get exactly double the values  i checked multiple times the shapes and they were always correct  but the tfrecord keeps returning the wrong number of values
193,193,19352900,72647712,Big Query Table as Artifact in Kubeflow Pipelines,"<p>I am running a custom component in kubeflow to do some data manipulation and then save the result as a big query table.  How do I register the table as an artifact so that I can pass it down to the different stages of the pipeline?</p>
<p>Eventually I am planning on making a parallelfor up to create multiple bigquery tables from which i will create multiple machine learning models.  I would like to be able to pass these tables to the next stage so that I can create models from them.</p>
<p>Currently what i am doing is just saving the uri into a pandas dataframe.</p>
<pre><code>def get_the_data(
    project_id: str,
    url: str,
    dataset_uri: Output[Dataset],
    lag: int = 0, 
):
  ## table name
  table_id = url + &quot;_lag_&quot;  + str(lag)
  ## code to query and create new table
  ##
  ##
  
  ## store URI in a dataframe which can be passed to next stage
  df=pd.DataFrame(data=[table_id], columns = ['path'])
  df.to_csv(dataset_uri.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')

</code></pre>
<p>Eventually i am going to be using a parallelfor op to run this component multiple times in parallel and create multiple tables.  I don't know how to manage and collect the table ids so i can run subsequent ops on them.</p>
",22,0,1,4,python;google-bigquery;kubeflow;kubeflow-pipelines,2022-06-16 20:14:26,2022-06-16 20:14:26,2022-06-16 20:15:10,i am running a custom component in kubeflow to do some data manipulation and then save the result as a big query table   how do i register the table as an artifact so that i can pass it down to the different stages of the pipeline  eventually i am planning on making a parallelfor up to create multiple bigquery tables from which i will create multiple machine learning models   i would like to be able to pass these tables to the next stage so that i can create models from them  currently what i am doing is just saving the uri into a pandas dataframe  eventually i am going to be using a parallelfor op to run this component multiple times in parallel and create multiple tables   i don t know how to manage and collect the table ids so i can run subsequent ops on them 
194,194,4097457,70517614,ElasticSearch vs. OpenSearch,"<p>we were using Elasticsearch version 7.10.2 until now. By changing Elasticsearch license in version 7.11 and further, it is no more open-source and we are forcing to change the platform.
OpenSearch which is supported by AWS is making a fork with Elasticsearch 7.10.2. I think it has not support whole X-Pack features from Elasticsearch completely special Machine Learning.
What is your comment? Shall we continue in OpenSearch or not?</p>
",6151,2,5,4,amazon-web-services;elasticsearch;kibana;opensearch,2021-12-29 15:10:04,2021-12-29 15:10:04,2022-06-16 19:23:14,
195,195,3318300,72641644,Android Emulator stopped on Mac,"<p>I'm using old MacBook Pro 2010 model with 8GB RAM running MacOS High Sierra 10.13.6.</p>
<p>Recently started learning Flutter and setup the environment, initially found difficult on setting up and running the emulators for both iPhone and Android. But, somehow managed to run iPhone emulator after downgrading the Android Studio and setting up the emulator.</p>
<p>However, still facing the issues on setting up Android Emulator after multiple attempts trying to do all the work arounds found on google.</p>
<p>Finally, found a thread asking to change the ENTITLEMENTS setting and tried that with emulator ending 30.10.2 version.</p>
<p>While trying to start the emulator, the emulator started successfully and stopped immediately with the following message on terminal:</p>
<h1>MacBookPros-MacBook-Pro:~ iPhone$ ~/Library/Android/sdk/emulator/emulator -avd &quot;Nexus_5X_API_30&quot; -gpu auto</h1>
<p>INFO    | Android emulator version 31.2.10.0 (build_id 8420304) (CL:N/A)
WARNING | unexpected system image feature string, emulator might not function correctly, please try updating the emulator.
Warning: Quick Boot / Snapshots not supported on this machine. A CPU with EPT + UG features is currently needed. We will address this in a future release.
INFO    | configAndStartRenderer: setting vsync to 60 hz
INFO    | added library /Users/iPhone/Library/Android/sdk/emulator/qemu/darwin-x86_64/lib64/vulkan/libvulkan.dylib
HAX is working and emulator runs in fast virt mode.
INFO    | Started GRPC server at 127.0.0.1:8554, security: Local
INFO    | Advertising in: /Users/iPhone/Library/Caches/TemporaryItems/avd/running/pid_2825.ini
VCPU shutdown request
VCPU shutdown request
INFO    | Shutting down gRPC endpoint
INFO    | Shutting down gRPC endpoint</p>
<p>Following message was found on the error log of the android studio:</p>
<p>Process:               qemu-system-x86_64 [1259]
Path:                  /Users/USER/Library/Android/*/qemu-system-x86_64
Identifier:            qemu-system-x86_64
Version:               0
Code Type:             X86-64 (Native)
Parent Process:        studio [1010]
Responsible:           qemu-system-x86_64 [1259]
User ID:               502</p>
<p>Date/Time:             2022-06-15 19:54:58.636 +0530
OS Version:            Mac OS X 10.13.6 (17G14042)
Report Version:        12
Anonymous UUID:        83A0426B-5C37-491B-B5E7-6EFC277F94DF</p>
<p>Time Awake Since Boot: 960 seconds</p>
<p>System Integrity Protection: enabled</p>
<p>Crashed Thread:        20</p>
<p>Exception Type:        EXC_CRASH (SIGABRT)
Exception Codes:       0x0000000000000000, 0x0000000000000000
Exception Note:        EXC_CORPSE_NOTIFY</p>
<p>....</p>
<p>External Modification Summary:
Calls made by other processes targeting this process:
task_for_pid: 0
thread_create: 0
thread_set_state: 0
Calls made by this process:
task_for_pid: 0
thread_create: 0
thread_set_state: 0
Calls made by all processes on this machine:
task_for_pid: 840
thread_create: 0
thread_set_state: 0</p>
<p>VM Region Summary:
ReadOnly portion of Libraries: Total=709.9M resident=0K(0%) swapped_out_or_unallocated=709.9M(100%)
Writable regions: Total=2.8G written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=2.8G(100%)</p>
<pre><code>                            VIRTUAL   REGION 
</code></pre>
<p>REGION TYPE                        SIZE    COUNT (non-coalesced)
===========                     =======  =======
Accelerate framework               128K        2
Activity Tracing                   256K        2
CG backing stores                 1356K        5
CG image                            80K        3
CoreAnimation                       28K        5
CoreGraphics                         8K        2
CoreImage                            8K        3
CoreServices                        68K        2
CoreUI image data                  772K        6
CoreUI image file                  180K        3
Dispatch continuations            4096K        2
Foundation                           4K        2
Kernel Alloc Once                    8K        2
MALLOC                           228.7M       67
MALLOC guard page                   48K       12
MALLOC_LARGE (reserved)           11.8M        6         reserved VM address space (unallocated)
Memory Tag 242                      12K        2
OpenGL GLSL                        512K        7
PROTECTED_MEMORY                     4K        2
STACK GUARD                       56.1M       33
Stack                             23.8M       38
VM_ALLOCATE                      277.9M       73
VM_ALLOCATE (reserved)             1.8G        4         reserved VM address space (unallocated)
__DATA                            62.1M      325
__FONT_DATA                          4K        2
__GLSLBUILTINS                    2588K        2
__LINKEDIT                       336.1M       45
__TEXT                           373.9M      322
__UNICODE                          560K        2
mapped file                       49.3M       18
shared memory                    512.7M       16
===========                     =======  =======
TOTAL                              3.6G      984
TOTAL, minus reserved VM space     1.9G      984</p>
<p>Model: MacBookPro7,1, BootROM 68.0.0.0.0, 2 processors, Intel Core 2 Duo, 2.4 GHz, 16 GB, SMC 1.62f7
Graphics: NVIDIA GeForce 320M, NVIDIA GeForce 320M
Memory Module: BANK 0/DIMM0, 8 GB, DDR3, 1067 MHz, 0x02B5, -
Memory Module: BANK 1/DIMM0, 8 GB, DDR3, 1067 MHz, 0x02B5, -
AirPort: spairport_wireless_card_type_airport_extreme (0x14E4, 0x8D), Broadcom BCM43xx 1.0 (5.106.98.102.30)
Bluetooth: Version 6.0.7f22, 3 services, 27 devices, 1 incoming serial ports
Network Service: Wi-Fi, AirPort, en1
Serial ATA Device: CT500BX100SSD1, 500.11 GB
Serial ATA Device: HL-DT-ST DVDRW  GS23N
USB Device: USB Bus
USB Device: USB Bus
USB Device: BRCM2046 Hub
USB Device: Bluetooth USB Host Controller
USB Device: IR Receiver
USB Device: Apple Internal Keyboard / Trackpad
USB Device: USB 2.0 Bus
USB Device: Built-in iSight
USB Device: USB 2.0 Bus
USB Device: Internal Memory Card Reader
Thunderbolt Bus:</p>
<p>Any help to fix the issue will be greatly appreciated</p>
",24,0,-1,4,android;flutter;macos;emulation,2022-06-16 12:26:49,2022-06-16 12:26:49,2022-06-16 12:26:49,i m using old macbook pro  model with gb ram running macos high sierra     recently started learning flutter and setup the environment  initially found difficult on setting up and running the emulators for both iphone and android  but  somehow managed to run iphone emulator after downgrading the android studio and setting up the emulator  however  still facing the issues on setting up android emulator after multiple attempts trying to do all the work arounds found on google  finally  found a thread asking to change the entitlements setting and tried that with emulator ending    version  while trying to start the emulator  the emulator started successfully and stopped immediately with the following message on terminal  following message was found on the error log of the android studio  time awake since boot   seconds system integrity protection  enabled crashed thread               any help to fix the issue will be greatly appreciated
196,196,2686261,72637756,Azure Machine Learning Model deployment as AKS web service from multiple workspaces,"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!</p>
",29,1,0,2,azure-aks;azure-machine-learning-service,2022-06-16 02:29:19,2022-06-16 02:29:19,2022-06-16 07:45:08,i am trying to determine how the az ml model deployment works with aks  if you have a single aks cluster but attach from two separate workspaces  will models from both workspaces get deployed into different azureml fe s with different ip addresses or a single azureml fe with a single ip address  reason i ask is because i want to purchase a certificate but am unsure if all the models  regardless of workspace  will get exposed under the same ip address or multiple ip addresses  if its the former  i can do it with one certificate   otherwise i have to do it with multiple certificates or san based certificates  so if anyone has experience with this  please let me know 
197,197,13214782,72467117,How to find which features are responsible for predicted label?,"<p>I am working on a machine learning project and wants to know by using the sklearn how we can find the best feature responsible for predicted label in python.</p>
<p>Let suppose we fit the model and the wants to predict
<code>model.predict([1,2,3])-&gt; let suppose it says you passed the test.</code> but what the weightage of the features for predicting only for this prediction
<code>model.predict([1,2,3]) </code></p>
<p>Suppose a dataset with 5 columns. Let's call them: id, X_1, X_2, X_3, result. X_1,X_2,X_3 have the numerical values 1-5.</p>
<p>I need to show that this result was caused by X_1,X_2 with weightage of 0.8900% and 0.3900% or any graph which through i can use fully understand. How can I show that X_1 and X_2 has more influence on result than X_3? only for this prediction <code>model.predict([1,2,3])</code></p>
<p>I need a simple answer or any code which can helps me with this problem.</p>
",60,1,-3,5,python;pandas;dataframe;machine-learning;data-science,2022-06-02 00:54:55,2022-06-02 00:54:55,2022-06-16 06:25:26,i am working on a machine learning project and wants to know by using the sklearn how we can find the best feature responsible for predicted label in python  suppose a dataset with  columns  let s call them  id  x_  x_  x_  result  x_ x_ x_ have the numerical values    i need to show that this result was caused by x_ x_ with weightage of    and    or any graph which through i can use fully understand  how can i show that x_ and x_ has more influence on result than x_  only for this prediction model predict       i need a simple answer or any code which can helps me with this problem 
198,198,14481226,72637361,Google Earth Engine how to turn image collection bands into histograms,"<p>I am trying to replicate this paper here <a href=""https://www.nature.com/articles/s41598-021-89779-z"" rel=""nofollow noreferrer"">https://www.nature.com/articles/s41598-021-89779-z</a> where they use satellite data to predict crop yield using machine learning. One of the steps they do is shown in this image here: <a href=""https://i.stack.imgur.com/v08vt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v08vt.png"" alt=""enter image description here"" /></a></p>
<p>The authors turn a image collection into a frequency histogram by observing all of the pixel values in a select region. How would you code this using google earth engine in google colab?</p>
",21,0,0,3,image-processing;histogram;google-earth-engine,2022-06-16 01:46:44,2022-06-16 01:46:44,2022-06-16 01:46:44,i am trying to replicate this paper here  where they use satellite data to predict crop yield using machine learning  one of the steps they do is shown in this image here   the authors turn a image collection into a frequency histogram by observing all of the pixel values in a select region  how would you code this using google earth engine in google colab 
199,199,19298023,72612680,What is the MOST cost-effective solution to implement an image batch processing cluster on AWS?,"<p>I've some questions related to AWS Cloud practitioner, DevOps, and machine learning as I'm preparing for the AWS certifications.</p>
<p>What is the MOST cost-effective solution to implement an image batch processing cluster on AWS? Configuring the cluster software from a generic EC2 Linux image takes 30 minutes. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS and can tolerate interruptions.</p>
<p>Should I Use GlusterFS on EC2 instances for checkpoint data? To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually?</p>
<p>And also if there is a resource with solutions to the problems related to AWS Cloud practitioner, DevOps, and machine learning would be great.</p>
",36,0,3,2,amazon-web-services;machine-learning,2022-06-14 12:26:58,2022-06-14 12:26:58,2022-06-16 00:14:41,i ve some questions related to aws cloud practitioner  devops  and machine learning as i m preparing for the aws certifications  what is the most cost effective solution to implement an image batch processing cluster on aws  configuring the cluster software from a generic ec linux image takes  minutes  the application cannot run in docker containers and must run on amazon ec  the batch job stores checkpoint data on an nfs and can tolerate interruptions  should i use glusterfs on ec instances for checkpoint data  to run the batch job  configure ec instances manually  when the job completes  shut down the instances manually  and also if there is a resource with solutions to the problems related to aws cloud practitioner  devops  and machine learning would be great 
200,200,18904889,72632351,Tokenize sentence based on existing punctuation (TF-IDF vectorizer),"<p>In a dataframe, I have rows which include sentences like &quot;<em>machine learning, data, ia, segmentation, analysis</em>&quot; or &quot;<em>big data, data lake, data visualisation, marketing, seo</em>&quot;.</p>
<p>I want to use TF-IDF and kmeans in order to create clusters based on each word.</p>
<p>My problem is that when I use TF-IDFvectorizer, it tokenizes sentences wrongly. I get terms like &quot;<em>analyse analyse</em>&quot; or &quot;<em>english excel</em>&quot; which are not supposed to be put together.</p>
<p>Instead, I would like sentences to be tokenized based on the commas in the sentence. So terms would be &quot;<em>analyse</em>&quot;, &quot;<em>big data</em>&quot;, &quot;<em>data lake</em>&quot;, &quot;<em>english</em>&quot;, etc.</p>
<p>I guess I should change something in the TF-IDFvectorize params but I don't understand how.</p>
<p>Do you please have any idea how to realize this ?</p>
",15,1,0,3,python;tokenize;tfidfvectorizer,2022-06-15 18:58:31,2022-06-15 18:58:31,2022-06-15 21:50:24,in a dataframe  i have rows which include sentences like  machine learning  data  ia  segmentation  analysis  or  big data  data lake  data visualisation  marketing  seo   i want to use tf idf and kmeans in order to create clusters based on each word  my problem is that when i use tf idfvectorizer  it tokenizes sentences wrongly  i get terms like  analyse analyse  or  english excel  which are not supposed to be put together  instead  i would like sentences to be tokenized based on the commas in the sentence  so terms would be  analyse    big data    data lake    english   etc  i guess i should change something in the tf idfvectorize params but i don t understand how  do you please have any idea how to realize this  
201,201,17184561,69942158,I can&#39;t obtain the cosine similarity of my tf-idf matrix because Google Colaboratory gives me a memory RAM error,"<p>Could anyone help me please? I've been trying for days to solve a problem that is concerned about the workspace Google Colaboratory, an error in the available RAM on it and the calculus of the similarity punctuations of a TF-IDF matrix by means of the cosine similarity and other different metrics. I can't fix it and this should be really important for me because it is a piece of code that I must program for my TFG, which is about a content-based recommendation system that gives you the 10 most similar films of one by using the description of the films (here is the reason why I have to use the TD-IDF matrix, to convert that text or &quot;document&quot; for any film in the BBDD in a numeric format that my machine learning algorithm could understand).</p>
<p>Firstly, I load the .csv file in which we can find the films with the descriptions of all of them in the column 'overview' of the pandas DataFrame (I let you the code for simplicity):</p>
<pre><code>import pandas as pd
metadata = pd.read_csv(&quot;movies_metadata.csv&quot;, low_memory = False)
metadata.loc[:, [&quot;original_title&quot;,&quot;overview&quot;]].head()
</code></pre>
<p>Then, I want to obtain the TF-IDF matrix of that film's descriptions in 'overview'. To do so, I implement this piece of code:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

tfidf = TfidfVectorizer(stop_words = &quot;English&quot;)
metadata[&quot;overview&quot;] = metadata[&quot;overview&quot;].fillna(&quot;&quot;)
tfidf_matrix = tfidf.fit_transform(metadata[&quot;overview&quot;])
tfidf_matrix.shape
</code></pre>
<p>As you can see by yourself, the matrix (a sparse matrix) is obtained and it is a huge one! Its dimensions are 45466 rows and 75827 columns. And I think the reason of my problem is justly this.</p>
<p>When I want to compute the pair cosine similarities punctuations between films, I should obtain a (45466,45466) matrix; but instead, Google Colaboratory gives me an error which says something like &quot;your session has failed because you have used all the available RAM memory&quot; when I run the following code:</p>
<pre><code>from sklearn.metrics.pairwise import linear_kernel
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
</code></pre>
<p>I don't know how to solve it... But what I know is that I really need to obtain that cosine similarities between film for my study.</p>
<p>In the datacamp tutorial that I've been using for learning about this machine learning topic <a href=""https://www.datacamp.com/community/tutorials/recommender-systems-python"" rel=""nofollow noreferrer"">enter link description here</a></p>
<p>you could see that they obtain the cosine_sim matrix without problem... Why cannot be the same for me?</p>
<p>I've been trying with other lines of code like:</p>
<pre><code>from sklearn.metrics.pairwise import pairwise_distances as pd
cosine_sim = pd(tfidf_matrix, tfidf_matrix, metric = &quot;cosine&quot;)
</code></pre>
<p>and even, I tried to enlarge the RAM of Google Colab with a code that I found on Internet:</p>
<pre><code>a = []
while(1):
a.append('1')
</code></pre>
<p>but it doesn't go right. I hope that anyone could give me a solution...</p>
<p>Thank you very much for your attention and sorry for the inconvenience!</p>
",104,1,1,5,python;out-of-memory;tf-idf;cosine-similarity;recommender-systems,2021-11-12 16:52:47,2021-11-12 16:52:47,2022-06-15 21:45:31,could anyone help me please  i ve been trying for days to solve a problem that is concerned about the workspace google colaboratory  an error in the available ram on it and the calculus of the similarity punctuations of a tf idf matrix by means of the cosine similarity and other different metrics  i can t fix it and this should be really important for me because it is a piece of code that i must program for my tfg  which is about a content based recommendation system that gives you the  most similar films of one by using the description of the films  here is the reason why i have to use the td idf matrix  to convert that text or  document  for any film in the bbdd in a numeric format that my machine learning algorithm could understand   firstly  i load the  csv file in which we can find the films with the descriptions of all of them in the column  overview  of the pandas dataframe  i let you the code for simplicity   then  i want to obtain the tf idf matrix of that film s descriptions in  overview   to do so  i implement this piece of code  as you can see by yourself  the matrix  a sparse matrix  is obtained and it is a huge one  its dimensions are  rows and  columns  and i think the reason of my problem is justly this  when i want to compute the pair cosine similarities punctuations between films  i should obtain a     matrix  but instead  google colaboratory gives me an error which says something like  your session has failed because you have used all the available ram memory  when i run the following code  i don t know how to solve it    but what i know is that i really need to obtain that cosine similarities between film for my study  in the datacamp tutorial that i ve been using for learning about this machine learning topic  you could see that they obtain the cosine_sim matrix without problem    why cannot be the same for me  i ve been trying with other lines of code like  and even  i tried to enlarge the ram of google colab with a code that i found on internet  but it doesn t go right  i hope that anyone could give me a solution    thank you very much for your attention and sorry for the inconvenience 
202,202,16445922,72633238,Google PlayStore - Requested permissions do not match core functionality of the app,"<p>Having trouble with permissions declarations when I publish the app on PlayStore</p>
<p>Built a loan app for unbanked microbusinesses to access credit. The phone data is used for machine learning to predict credit scores using this data
I have specified data collection in the manifest but I keep being rejected in the permissions declarations for the functionalities in my app requiring SMS and Call Log permissions.</p>
<p>Error message:</p>
<p><em>&quot;Requested permissions do not match core functionality of the app
You declared SMS-based financial transactions (e.g., 5 digit messages), and related activity including OTP account verification for financial transactions and fraud detection as the core functionality of your app. However, after review, we found that your app does not match the declared use case(s).&quot;</em></p>
<p>These are the options:</p>
<ul>
<li><p>Default SMS handler</p>
</li>
<li><p>Default Phone handler</p>
</li>
<li><p>Default Assistant handler</p>
</li>
<li><p>Transactional backup and restore for users and archive for enterprise (time-limited/non-continuous)</p>
</li>
<li><p>Enterprise archive, CRM, and device management</p>
</li>
<li><p>Caller ID, spam detection, and blocking</p>
</li>
<li><p>Connected device companion apps (for example, smartwatch or automotive)</p>
</li>
<li><p>Cross-device synchronization or transfer of SMS or calls</p>
</li>
<li><p>SMS-based financial transactions and related activity where access is restricted to financial SMS transactions (for example, 5-digit messages)</p>
</li>
<li><p>SMS based money management</p>
</li>
<li><p>Proxy calls</p>
</li>
<li><p>Services - Carrier</p>
</li>
<li><p>Services - OEM</p>
</li>
<li><p>Device Automation</p>
</li>
<li><p>Physical safety / emergency alert apps (e.g., senior safety)</p>
</li>
<li><p>Call-based OTP account verification</p>
</li>
<li><p>Using SMS_CB_RECEIVED for customer communications (e.g., Smart Zone Cast service)</p>
</li>
<li><p>Write and Show Call History in Dialer</p>
</li>
<li><p>In-vehicle hands-free use or projected display</p>
</li>
<li><p>Anti-SMS Phishing</p>
</li>
</ul>
<p>I read the Play Console Help guide. I tried</p>
<ul>
<li>Default SMS handler,</li>
<li>Default Phone handler,</li>
<li>SMS-based financial transactions and related activity where access is restricted to financial SMS transactions</li>
</ul>
<p>They are all being rejected and yes, I read through the Play Console Help for exceptions and permitted uses, but other alternative credit scoring apps on the market are able to be listed successfully such as:</p>
<p><a href=""https://play.google.com/store/apps/details?id=mx.com.tala&amp;hl=en_ZA&amp;gl=MX"" rel=""nofollow noreferrer"">https://play.google.com/store/apps/details?id=mx.com.tala&amp;hl=en_ZA&amp;gl=MX</a></p>
<p><a href=""https://play.google.com/store/apps/details?id=io.quickcheck.loans&amp;hl=en_ZA&amp;gl=NG"" rel=""nofollow noreferrer"">https://play.google.com/store/apps/details?id=io.quickcheck.loans&amp;hl=en_ZA&amp;gl=NG</a></p>
<p><a href=""https://play.google.com/store/apps/details?id=com.branch_international.branch.branch_demo_android&amp;hl=en_ZA&amp;gl=KE"" rel=""nofollow noreferrer"">https://play.google.com/store/apps/details?id=com.branch_international.branch.branch_demo_android&amp;hl=en_ZA&amp;gl=KE</a></p>
<p>Please help, thank you.</p>
",38,1,0,5,android;flutter;google-play;google-play-console;mobile-application,2022-06-15 20:01:50,2022-06-15 20:01:50,2022-06-15 21:44:37,having trouble with permissions declarations when i publish the app on playstore error message  these are the options  default sms handler default phone handler default assistant handler transactional backup and restore for users and archive for enterprise  time limited non continuous  enterprise archive  crm  and device management caller id  spam detection  and blocking connected device companion apps  for example  smartwatch or automotive  cross device synchronization or transfer of sms or calls sms based financial transactions and related activity where access is restricted to financial sms transactions  for example   digit messages  sms based money management proxy calls services   carrier services   oem device automation physical safety   emergency alert apps  e g   senior safety  call based otp account verification using sms_cb_received for customer communications  e g   smart zone cast service  write and show call history in dialer in vehicle hands free use or projected display anti sms phishing i read the play console help guide  i tried they are all being rejected and yes  i read through the play console help for exceptions and permitted uses  but other alternative credit scoring apps on the market are able to be listed successfully such as     please help  thank you 
203,203,10309712,72624523,Customising training process of a support vector classifier process,"<p>OK, with the intention to produce a balanced class distribution for an imbalanced binary problem based on <code>SVC</code>, I  created this custom function.</p>
<p>Basically, it takes binary imbalance data, fits an <code>SVC</code> model to determine support vectors for the two classes. Then uses the support vector of the majority class, iterates over all points of majority class to compute each point's distance from the support vector point. Using this point's distances, selects those points near the class's support vector until an equal number of examples in minority class. The function thus returns a balance data containing all minority examples, and the examples selected from majority class only.</p>
<pre><code>from collections import Counter
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVC
import seaborn as sns
from math import dist   # python 3.8 and above

def support_vector_nn(X, y):
  # class distribution
  counter = Counter(y)
  maj_class = counter.most_common()[0][0]
  min_class = counter.most_common()[-1][0]
  # number of minority examples
  num_minority = len(X[ y == min_class])
  svc = SVC(kernel='linear', random_state=32)
  svc.fit(X,y)
  # majority class support vectors
  maj_sup_vector = svc.support_vectors_[maj_class]
  # compute distances to support vector point
  distances = []
  for i, x in enumerate(X[y == maj_class]):
    d = math.dist(maj_sup_vector, x)
    distances.append((i, d))
  # sort distances (ascending)
  distances.sort(key=lambda tup: tup[1])
  index = [i for i, d in distances][:num_minority]
  X_ds = np.concatenate((X[y == maj_class][index], X[y == min_class]))
  y_ds = np.concatenate((y[y == maj_class][index], y[y == min_class]))

  return X_ds, y_ds
</code></pre>
<p>I illustrate how this function works with the following dummy imbalanced data:</p>
<pre><code># sample data
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
    n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=1)

</code></pre>
<p>The following show original data before resample:</p>
<pre><code>svc_model = SVC(kernel='linear', random_state=32)
svc_model.fit(X, y)

# Constructing a hyperplane using a formula.
w = svc_model.coef_[0]           
b = svc_model.intercept_[0]  
y_points = np.linspace(X[:, 1].min(), X[:, 1].max())
x_points = -(w[1] * y_points + b) / w[0]

plt.figure(figsize=(10, 8))

# Plotting data in features-space
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=50)
# Plotting a red hyperplane
plt.plot(x_points, y_points, c='r')
</code></pre>
<p><a href=""https://i.stack.imgur.com/N8IlG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N8IlG.png"" alt=""enter image description here"" /></a></p>
<p>With undersampling using the custom function, we see that:</p>
<pre><code>X_res, y_res = support_vector_nn(X,y)
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=50)
plt.scatter(X_res[y_res == 0][:,0], X_res[y_res == 0][:,1], color='black', alpha=0.4)
plt.plot(x_points, y_points, c='r')
plt.savefig('final', dpi=300)
</code></pre>
<p><a href=""https://i.stack.imgur.com/ZPgAq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZPgAq.png"" alt=""enter image description here"" /></a></p>
<p>The points in black are the ones near majority class's support vector, and their the ones selected (thus equal to the number of minority examples).</p>
<p>The balanced data <code>X_res, y_res</code> can then be used to train any machine learning classifier, including SVC.</p>
<p><strong>Question</strong></p>
<p>I want to extend this approach to multiclass, specifically, using <code>SVC</code>. I know <code>sklearn</code>'s <code>svm.SVC</code>  provides <code>decision_function_shape</code> that can be used with <code>ovo</code> which is the decision function I want to use also. But I'm not sure where to fit my custom function in the learning process of <code>SVC</code> such that every time it does perform each of the one-class against another, it calls my custom function to balance those classes it is currently handling before fitting the classifier.</p>
<p>Can someone suggest how to fit the resampling (in custom function) to the training routines of <code>SVC</code> with <code>decision_function_shape='ovo'</code> decision shape?</p>
",35,0,0,5,python;machine-learning;scikit-learn;classification;svm,2022-06-15 05:19:59,2022-06-15 05:19:59,2022-06-15 20:10:41,ok  with the intention to produce a balanced class distribution for an imbalanced binary problem based on svc  i  created this custom function  basically  it takes binary imbalance data  fits an svc model to determine support vectors for the two classes  then uses the support vector of the majority class  iterates over all points of majority class to compute each point s distance from the support vector point  using this point s distances  selects those points near the class s support vector until an equal number of examples in minority class  the function thus returns a balance data containing all minority examples  and the examples selected from majority class only  i illustrate how this function works with the following dummy imbalanced data  the following show original data before resample   with undersampling using the custom function  we see that   the points in black are the ones near majority class s support vector  and their the ones selected  thus equal to the number of minority examples   the balanced data x_res  y_res can then be used to train any machine learning classifier  including svc  question i want to extend this approach to multiclass  specifically  using svc  i know sklearn s svm svc  provides decision_function_shape that can be used with ovo which is the decision function i want to use also  but i m not sure where to fit my custom function in the learning process of svc such that every time it does perform each of the one class against another  it calls my custom function to balance those classes it is currently handling before fitting the classifier  can someone suggest how to fit the resampling  in custom function  to the training routines of svc with decision_function_shape  ovo  decision shape 
204,204,12125901,58204710,"How can I train a sequential model in keras, giving a list as outputs and inputs?","<p>I am very new to Keras and to machine learning in general, but here is what I want. I have a list of inputs (1 value for every input node) and a list of targets (1 value for every output node).</p>

<pre><code>    input_list = [1, 0, 1, 0, 1, 0] # maybe longer

    wanted_output_list = [1, 0, 0, 0] # also maybe longer
</code></pre>

<p>And now I want to give these as input to train my neural network:</p>

<pre><code>    # create model
    model = Sequential()

    # get number of columns in training data
    n_cols = 6

    # add model layers
    model.add(Dense(6, activation='relu', input_shape= (n_cols,)))
    model.add(Dense(2, activation='relu'))
    model.add(Dense(2, activation='relu'))
    model.add(Dense(3))

    # compile the model
    model.compile(optimizer='adam', loss='mean_squared_error')

    # train model
    model.fit(input_list, wanted_output_list, validation_split=0.2, epochs=30)
</code></pre>

<p>However I get this error:</p>

<pre><code>    ValueError: Error when checking input: dense_1_input to have shape (6,) but got with shape (1,)
</code></pre>

<p>Can anyone please tell me why and how I can fix this?</p>
",3667,3,2,3,python;tensorflow;keras,2019-10-02 21:00:11,2019-10-02 21:00:11,2022-06-15 19:38:48,i am very new to keras and to machine learning in general  but here is what i want  i have a list of inputs   value for every input node  and a list of targets   value for every output node   and now i want to give these as input to train my neural network  however i get this error  can anyone please tell me why and how i can fix this 
205,205,8507917,72626134,Is there a way to check how human like does a voice sound in an audio file?,"<p>I am working on a project where I want to check if speech sounds monotonic, with flat intonation, etc. i.e. very robotic / slow and less human-like.</p>
<p>Is there any existing research work on how we could quantify this either through statistical properties of the voice signal or by leveraging machine learning? The dataset I work with consists of audio with human speech.</p>
",13,0,-1,4,machine-learning;audio;signal-processing;speech,2022-06-15 10:35:21,2022-06-15 10:35:21,2022-06-15 15:47:44,i am working on a project where i want to check if speech sounds monotonic  with flat intonation  etc  i e  very robotic   slow and less human like  is there any existing research work on how we could quantify this either through statistical properties of the voice signal or by leveraging machine learning  the dataset i work with consists of audio with human speech 
206,206,19343679,72629353,Using Python the find Heart rate variability from heart rate,"<p>I'm currently working on a machine learning project and I found out that I need more features to get better accuracy because I only have 2 features , so I decided to use the heartrate to find heart rate variability.<br />
Is there's a way I can do it using Python?</p>
",20,0,-1,2,python;machine-learning,2022-06-15 15:28:25,2022-06-15 15:28:25,2022-06-15 15:28:25,
207,207,10008777,72625815,looping through folders to create a dataframe constisting of filename and class it belong to,"<p>I am looping through folders to create a dataframe constisting of filename and class it belong to. This will be used for flow_from_dataframe() keras machine learning package. However, the output only produces results for 2 classes, the rest is not done at all.  Hoodie and t_shirt classes are found in output dataframe with correctly having filename and class info.  I cannot understand why only these 2 classes have data while rest is left out..</p>
<p>Folders and image count
<a href=""https://i.stack.imgur.com/V2jY8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V2jY8.png"" alt=""folders and image count in folders"" /></a></p>
<p>'''</p>
<pre><code>#make the dataframe for flow from DF, by looping through image folders. 
import os
import pandas as pd

dfTest = pd.DataFrame(columns = ['filename', 'class'])
 
for c in os.listdir('/home/ubuntu/imageTrain_dobby/SKJEWELLERY/BC4U/google_version/v1.1/lingyau_lee/output/test'):
    for d,e in enumerate(os.listdir('/home/ubuntu/imageTrain_dobby/SKJEWELLERY/BC4U/google_version/v1.1/lingyau_lee/output/test'+'/'+c)): 
        dfTest.loc[d, 'filename'] = e
        dfTest.loc[d,'class'] = c
        #dfTest.iloc[k]['filename']= j #cannot use iloc as i not exist yet, 
dfTest

output below: 
    filename    class
0   uptheir-victory-HOODY-khaki-5XL.jepg    hoodie
1   upt-champions-HOODY-black-8XL.jepg  hoodie
2   uptheir-victory-HOODY-charcoal-5XL.jepg hoodie
3   113880-terrain-hoody-NAVY-6XL.jepg  hoodie
4   uptheir-lightning-HOODY-navy-black-5XL.jepg hoodie
... ... ...
2451    12174602-jorflexer-tee-WHITE-4XL.jepg   t_shirt
2452    bc-upt-sid-TEE-black-5XL.jepg   t_shirt
2453    53321-Blue-XLT.jepg t_shirt
2454    bc-upt-Dancer-TEE-black-7XL.jepg    t_shirt
2455    12174602-jorflexer-tee-NAVY-BLAZER-6XL.jepg t_shirt
2456 rows × 2 columns
</code></pre>
<p>'''</p>
",7,0,0,2,dataframe;loops,2022-06-15 09:36:57,2022-06-15 09:36:57,2022-06-15 09:36:57,i am looping through folders to create a dataframe constisting of filename and class it belong to  this will be used for flow_from_dataframe   keras machine learning package  however  the output only produces results for  classes  the rest is not done at all   hoodie and t_shirt classes are found in output dataframe with correctly having filename and class info   i cannot understand why only these  classes have data while rest is left out          
208,208,13474390,64238127,Getting Index Out of Range while iterating through list,"<p>I wrote on machine learning algorithm that works perfectly now I have to iterate all the items of list against one another to generate a similarity token between 0.01 to 1.00. Here's code</p>
<pre><code>    temp[]
    start_node = 0
    end_node = 0
    length = len(temp)
    for start_node in range(length):
        doc1 = nlp(temp[start_node])
        for end_node in range(++start_node, length):
            doc2 = nlp(temp[end_node])
            similar = doc1.similarity(doc2)
            exp_value = float(0.85)
            if similar == 1.0:
                print(&quot;Exact match&quot;, similar, temp[end_node], &quot;---------||---------&quot;,  temp[start_node])
            elif 0.96 &lt; similar &lt; 0.99:
                print(&quot;possible match&quot;, similar, temp[end_node], &quot;---------||---------&quot;, temp[start_node])
                temp.remove(temp[end_node])
</code></pre>
<p>Here, I am trying to check one item with all others in the list if any items are similar then delete that item from the list as there is no benefit to check the similarity of sentences back again with other elements, that will be a waste of computing power. But when I am trying to pop out elements I am getting Out of index error.</p>
<pre><code>&lt;ipython-input-12-c1959947bdd1&gt; in &lt;module&gt;
      4 length = len(temp)
      5 for start_node in range(length):
----&gt; 6     doc1 = nlp(temp[start_node])
      7     for end_node in range(++start_node, length):
      8         doc2 = nlp(temp[end_node])
</code></pre>
<p>I am just trying to keep original sentences, delete all the sentences which are similar in list so it doesn't check back with those items.</p>
<p>Temp list have 351 items, here i am just referencing as a list.</p>
<p>here;s a test of it</p>
<pre><code>print(temp[:1])

['malicious: caliche development partners &quot;financial statement&quot;has been shared with you']
</code></pre>
<p>I tried creating another duplicated list and delete similar items from that list</p>
<pre><code>final_items = temp
start_node = 0
end_node = 0
length = len(temp)
for start_node in range(length):
    doc1 = nlp(temp[start_node])
    for end_node in range(++start_node, length):
        doc2 = nlp(temp[end_node])
        similar = doc1.similarity(doc2)
        exp_value = float(0.85)
        if similar == 1.0:
            print(&quot;Exact match&quot;, similar, temp[end_node], &quot;---------||---------&quot;,  temp[start_node])
        elif 0.96 &lt; similar &lt; 0.99:
            print(&quot;possible match&quot;, similar, temp[end_node], &quot;---------||---------&quot;, temp[start_node])
            final_items.remove(temp[end_node])
</code></pre>
<p>But still got the same list index out of range while I am deleting elements from another list which I am not iterating even.</p>
",104,2,2,3,python;machine-learning;nlp,2020-10-07 11:36:07,2020-10-07 11:36:07,2022-06-15 07:02:19,i wrote on machine learning algorithm that works perfectly now i have to iterate all the items of list against one another to generate a similarity token between   to    here s code here  i am trying to check one item with all others in the list if any items are similar then delete that item from the list as there is no benefit to check the similarity of sentences back again with other elements  that will be a waste of computing power  but when i am trying to pop out elements i am getting out of index error  i am just trying to keep original sentences  delete all the sentences which are similar in list so it doesn t check back with those items  temp list have  items  here i am just referencing as a list  here s a test of it i tried creating another duplicated list and delete similar items from that list but still got the same list index out of range while i am deleting elements from another list which i am not iterating even 
209,209,19313314,72615010,15 x 15 numpy array as input of tensorflow,"<p>I'm new to machine learning</p>
<p>usually, input of CNN is image file(.png, ...)</p>
<p>i have bunchs of the 15*15 matrix in one .csv file.
and matrices are arranged.</p>
<p>like this:</p>
<p><img src=""https://i.stack.imgur.com/db6rT.png"" alt=""like this."" /></p>
<p>and i'd like to feed them sequentially to CNN module.</p>
<p>i can convert them to numpy array and dataframe too.</p>
<p>How can i code them correctly instead of image file?</p>
<p>my goal is to train AI model to be able to predict next step matrix.</p>
",29,0,-2,2,python;tensorflow,2022-06-14 15:28:18,2022-06-14 15:28:18,2022-06-15 06:52:14,i m new to machine learning usually  input of cnn is image file  png       like this   and i d like to feed them sequentially to cnn module  i can convert them to numpy array and dataframe too  how can i code them correctly instead of image file  my goal is to train ai model to be able to predict next step matrix 
210,210,19008231,72610973,How to considered whether implementation of ml is completed properly?,"<p>I constructed LR, RF and XGBoost models to predict gestational diabetes. The outcome is a binary problem. However, these three models did not achieve good results (AUC=0.65). Therefore, I used stacking to ensemble the three models together and another LR as the meta-learner, tries to build an ensemble learning model with better effects.</p>
<p>The following comments were made:</p>
<ol>
<li>It is not considered whether implementation is completed properly.</li>
<li>Improvements must be made to the intended scenario.</li>
</ol>
<p>However, I am not sure what these two questions mean.Are my machine learning models being used incorrectly? Should I switch to another algorithm? Does &quot;intended scenario&quot; mean supervised learning? Or should I optimize the supervised machine learning model I use?</p>
",29,0,-1,1,machine-learning,2022-06-14 08:13:49,2022-06-14 08:13:49,2022-06-15 06:22:15,i constructed lr  rf and xgboost models to predict gestational diabetes  the outcome is a binary problem  however  these three models did not achieve good results  auc     therefore  i used stacking to ensemble the three models together and another lr as the meta learner  tries to build an ensemble learning model with better effects  the following comments were made  however  i am not sure what these two questions mean are my machine learning models being used incorrectly  should i switch to another algorithm  does  intended scenario  mean supervised learning  or should i optimize the supervised machine learning model i use 
211,211,12240018,72623676,Pickle multiple machine learning model,"<p>I have multilayer model consist of two models, kmeans and random forest classifier; that work inside single function.
My question is how to pickle both and keep them work in sequence???
Thanks in advance</p>
",18,0,0,3,artificial-intelligence;pickle;random-forest,2022-06-15 03:10:41,2022-06-15 03:10:41,2022-06-15 03:10:41,
212,212,19340094,72623619,ROC Curves look like step functions for imbalanced data?,"<p>I'm making ROC curves for an imbalanced dataset, but they do not look like normal ROC curves at all. They look more like step functions (see image provided). From all the sources I can find, this should indicate that my machine learning algorithm is effective, but I'm still getting a lot of false positives. Why does my ROC curve so nicely shaped if my algorithm is not effective? Is there a better way to measure how 'good' this algorithm is? Thanks!</p>
<p><a href=""https://i.stack.imgur.com/uCIOx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uCIOx.png"" alt=""Step-Function ROC Curve"" /></a></p>
",12,1,2,1,python,2022-06-15 03:04:35,2022-06-15 03:04:35,2022-06-15 03:09:18,i m making roc curves for an imbalanced dataset  but they do not look like normal roc curves at all  they look more like step functions  see image provided   from all the sources i can find  this should indicate that my machine learning algorithm is effective  but i m still getting a lot of false positives  why does my roc curve so nicely shaped if my algorithm is not effective  is there a better way to measure how  good  this algorithm is  thanks  
213,213,13862759,72620370,Loss function that does not depend on ground truth,"<p>From what I know, the loss function in machine learning context is usually a function of two variables: the prediction and the ground truth. Is there such thing as a loss function that does not depend on the ground truth? For a simple example, if I'm predicting a real-valued variable, I can use a mean squared error as the loss function. But if I know, on physical grounds, that the output describes some positive quantity, I can define an improved custom loss function that is mainly the MSE, but maybe has an additional term that penalizes every time a negative prediction is made. This additional term need not take into account the value of the ground truth. Is this kind of idea prevalent? Does it have some kind of name?</p>
",13,1,-1,2,neural-network;loss-function,2022-06-14 21:54:25,2022-06-14 21:54:25,2022-06-15 02:52:42,from what i know  the loss function in machine learning context is usually a function of two variables  the prediction and the ground truth  is there such thing as a loss function that does not depend on the ground truth  for a simple example  if i m predicting a real valued variable  i can use a mean squared error as the loss function  but if i know  on physical grounds  that the output describes some positive quantity  i can define an improved custom loss function that is mainly the mse  but maybe has an additional term that penalizes every time a negative prediction is made  this additional term need not take into account the value of the ground truth  is this kind of idea prevalent  does it have some kind of name 
214,214,19339604,72621919,Fit_transform method for scaling data throws value error. Please help me resolve it,"<p>I am learning Machine Learning from scratch from a book. I am sorry if this is a naive question or something that was discussed already here. I reviewed various other similar posts here and learned that I need to use Label Encoder to resolve this but I am not sure how to code Label Encoder and hoping someone here will help me. I really appreciate your time and your help with this.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>housing_feature_engineered = pd.read_csv(&quot;todaytest.csv&quot;)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
housing_scaled = scaler.fit_transform(housing_feature_engineered)
housing_scaled
</code></pre>
<p>Output:</p>
<blockquote>
<p>ValueError: could not convert string to float: 'INLAND'</p>
</blockquote>
",21,1,0,2,python;machine-learning,2022-06-15 00:10:49,2022-06-15 00:10:49,2022-06-15 01:12:54,i am learning machine learning from scratch from a book  i am sorry if this is a naive question or something that was discussed already here  i reviewed various other similar posts here and learned that i need to use label encoder to resolve this but i am not sure how to code label encoder and hoping someone here will help me  i really appreciate your time and your help with this  code  output  valueerror  could not convert string to float   inland 
215,215,19294559,72607792,Why does SMOTE not work with more than 15 features / What method does work with more than 15 features?,"<p>I'm currently implementing machine learning using SMOTE from imblearn.over_sampling, and as I'm synthesizing data for it, I see a very noticeable cutoff for when the SMOTE method breaks. When I synthesize data using the following code and run it through SMOTE (courtesy of Jason Brownlee):</p>
<pre><code> from imblearn.over_sampling import SMOTE
 from sklearn.datasets import make_classification
 X, y = make_classification(n_samples=10000, n_features=15, n_redundant=0,
 n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)
 oversample = SMOTE()
 X, y = oversample.fit_resample(X, y)
</code></pre>
<p>It works fine. However, when the number of features is 16...</p>
<pre><code> from imblearn.over_sampling import SMOTE
 from sklearn.datasets import make_classification
 X, y = make_classification(n_samples=10000, n_features=16, n_redundant=0,
 n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)
 oversample = SMOTE()
 X, y = oversample.fit_resample(X, y)
</code></pre>
<p>SMOTE breaks. Why is this? Does anyone know of a SMOTE method that works for more than 15 parameters? By SMOTE breaking, I mean I get the error below:</p>
<pre><code>Traceback (most recent call last):



 File &quot;\\arete\shared\Los Angeles\Users\Active\bbonifacio\New ADVANCE\untitled1.py&quot;, line 13, in &lt;module&gt;
    X, y = oversample.fit_resample(X, y)

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\imblearn\base.py&quot;, line 83, in fit_resample
    output = self._fit_resample(X, y)

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\imblearn\over_sampling\_smote\base.py&quot;, line 324, in _fit_resample
    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\sklearn\neighbors\_base.py&quot;, line 763, in kneighbors
    results = PairwiseDistancesArgKmin.compute(

  File &quot;sklearn\metrics\_pairwise_distances_reduction.pyx&quot;, line 691, in sklearn.metrics._pairwise_distances_reduction.PairwiseDistancesArgKmin.compute

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\sklearn\utils\fixes.py&quot;, line 151, in threadpool_limits
    return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 171, in __init__
    self._original_info = self._set_threadpool_limits()

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 268, in _set_threadpool_limits
    modules = _ThreadpoolInfo(prefixes=self._prefixes,

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 340, in __init__
    self._load_modules()

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 373, in _load_modules
    self._find_modules_with_enum_process_module_ex()

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 485, in _find_modules_with_enum_process_module_ex
    self._make_module_from_path(filepath)

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 515, in _make_module_from_path
    module = module_class(filepath, prefix, user_api, internal_api)

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 606, in __init__
    self.version = self.get_version()

  File &quot;C:\Users\bbonifacio\Anaconda3\lib\site-packages\threadpoolctl.py&quot;, line 646, in get_version
    config = get_config().split()

AttributeError: 'NoneType' object has no attribute 'split'
</code></pre>
<p>And here are the versions of packages:</p>
<p>Sklearn: 1.1.1
Imblearn: 0.9.1
Threadpoolctl: 2.1.0</p>
",50,1,2,5,python;machine-learning;scikit-learn;smote;imblearn,2022-06-14 00:26:20,2022-06-14 00:26:20,2022-06-14 23:34:30,i m currently implementing machine learning using smote from imblearn over_sampling  and as i m synthesizing data for it  i see a very noticeable cutoff for when the smote method breaks  when i synthesize data using the following code and run it through smote  courtesy of jason brownlee   it works fine  however  when the number of features is     smote breaks  why is this  does anyone know of a smote method that works for more than  parameters  by smote breaking  i mean i get the error below  and here are the versions of packages 
216,216,1899010,45515031,How to remove columns with too many missing values in Python,"<p>I'm working on a machine learning problem in which there are many missing values in the features. There are 100's of features and I would like to remove those features that have too many missing values (it can be features with more than 80% missing values). How can I do that in Python?</p>
<p>My data is a Pandas dataframe.</p>
",36301,9,9,5,python;pandas;dataframe;scikit-learn;missing-data,2017-08-05 02:00:35,2017-08-05 02:00:35,2022-06-14 22:25:06,i m working on a machine learning problem in which there are many missing values in the features  there are  s of features and i would like to remove those features that have too many missing values  it can be features with more than   missing values   how can i do that in python  my data is a pandas dataframe 
217,217,1601580,41048002,How to use a python library that is constantly changing in a docker image or new container?,"<p>I organize my code in a python package (usually in a virtual environment like <code>virtualenv</code> and/or <code>conda</code>) and then usually call:</p>
<pre><code>python &lt;path_to/my_project/setup.py&gt; develop
</code></pre>
<p>or</p>
<pre><code>pip install -e &lt;path_to/my_project/setup.py&gt;
</code></pre>
<p>so that I can use the most recent version of my code. Since I develop mostly statistical or machine learning algorithms, I prototype a lot and change my code daily. However, recently the recommended way to run our experiments on the clusters I have access is through docker. I learned about docker and I think I have a rough idea of how to make it work but wanted wasn't quite sure if my solutions was good or if there might be better solutions out there.</p>
<p>The first solution that I thought is having a solution that copied the data in my docker image with:</p>
<pre><code>COPY /path_to/my_project
pip install /path_to/my_project
</code></pre>
<p>and then pip installing it. The issue with this solution is that I have to actually build a new image each time which seems silly and was hoping I could have something better. To do this I was thinking of having a bash file like:</p>
<pre><code>#BASH FILE TO BUILD AND REBUILD MY STUFF
# build the image with the newest version of 
# my project code and it pip installs it and its depedencies
docker build -t image_name .
docker run --rm image_name python run_ML_experiment_file.py 
docker kill current_container #not sure how to do get id of container
docker rmi image_name
</code></pre>
<p>as I said, my intuition tells me this is silly so I was hoping there was a single command way to do this with Docker or with a single Dockerfile. Also, note the command should use <code>-v ~/data/:/data</code> to be able to get the data and some other volume/mount to write to (in the host) when it finishes training.</p>
<p>Another solution that I thought was to have all the python dependencies or other dependencies that my library needs in the Dockerfile (and hence in the image) and then somehow executing in the running container the installation of my library. Maybe with <code>docker exec [OPTIONS] CONTAINER COMMAND</code> as:</p>
<pre><code>docker exec CONTAINER pip install /path_to/my_project
</code></pre>
<p>in the running container. After that then I could run the real experiment I want to run with the same exec command:</p>
<pre><code>docker exec CONTAINER python run_ML_experiment_file.py
</code></pre>
<p>though, I still don't know how to systematically get the container id though (because I probably don't want to look up the container id every time I do this).</p>
<p>Ideally in my head the best conceptual solution would be to simply have the Dockerfile know from the beginning to which file it should mount to (i.e. <code>/path_to/my_project</code>) and then somehow do <code>python [/path_to/my_project] develop</code> inside the image so that it would always be linked to the potentially changing python package/project. That way I can run my experiments with a <strong>single docker command</strong> as in:</p>
<pre><code>docker run --rm -v ~/data/:/data python run_ML_experiment_file.py
</code></pre>
<p>and not have to explicitly update the image myself every time (that includes not having to re install parts of the image that should be static) since its always in sync with the real library. Also, having some other script build a new image from scratch each time is not what I am looking for. Also, It would be nice to be able to avoid writing any bash too if possible.</p>
<hr />
<p>I think I am very close to a good solution. What I will do instead of building a new image each time I will simply run the <code>CMD</code> command to do python develop as follow:</p>
<pre><code># install my library (only when the a container is spun)
CMD python ~/my_tf_proj/setup.py develop
</code></pre>
<p>the advantage is that it will only pip install my library whenever I run a new container. This solves the development issue because re creating a new image takes to long. Though I just realized that if I use the <code>CMD</code> command then I can't run other commands given to my docker run, so I actually mean to run <code>ENTRYPOINT </code>.</p>
<p>Right now the only issue to complete this is that I am having issues using volume because I can't successfully link to <strong>my host project library within the Dockerfile</strong> (which seems to require an absolute path for some reason). I am currently doing doing (which doesn't seem to work):</p>
<pre><code>VOLUME /absolute_path_to/my_tf_proj /my_tf_proj
</code></pre>
<p>why can't I link using the VOLUME command in my Dockerfile? My main intention with using VOLUME is making my library (and other files that are always needed by this image) accessible when the CMD command tries to install my library. Is it possible to just have my library available all the time when a container is initiated?</p>
<p>Ideally I wanted to just have the library be installed automatically when a container is run and if possible, since the most recent version of the library is <strong>always</strong> required, have it install when a container is initialized.</p>
<p>As a reference right now my non-working Dockerfile looks as follow:</p>
<pre><code># This means you derive your docker image from the tensorflow docker image
# FROM gcr.io/tensorflow/tensorflow:latest-devel-gpu
FROM gcr.io/tensorflow/tensorflow
#FROM python
FROM ubuntu

RUN mkdir ~/my_tf_proj/
# mounts my tensorflow lib/proj from host to the container
VOLUME /absolute_path_to/my_tf_proj

#
RUN apt-get update

#
apt-get install vim

#
RUN apt-get install -qy python3
RUN apt-get install -qy python3-pip
RUN pip3 install --upgrade pip

#RUN apt-get install -y python python-dev python-distribute python-pip

# have the dependecies for my tensorflow library
RUN pip3 install numpy
RUN pip3 install keras
RUN pip3 install namespaces
RUN pip3 install pdb

# install my library (only when the a container is spun)
#CMD python ~/my_tf_proj/setup.py develop
ENTRYPOINT python ~/my_tf_proj/setup.py develop
</code></pre>
<hr />
<p>As a side remark:</p>
<p>Also, for some reason it requires me to do <code>RUN apt-get update</code> to be able to even install pip or vim in my container. Do people know why? I wanted to do this because just in case I wanted to attach to the container with a <code>bash</code> terminal, it would be really helpful.</p>
<p>Seems that Docker just forces you to apt install to always have the most recent version of software in the container?</p>
<hr />
<p>Bounty:</p>
<p>what a solution with <code>COPY</code>? and perhaps <code>docker build -f path/Docker .</code>. See: <a href=""https://stackoverflow.com/questions/68840188/how-does-one-build-a-docker-image-from-the-home-user-directory?noredirect=1#comment121660916_68840188"">How does one build a docker image from the home user directory?</a></p>
",2682,5,6,3,python;docker;dockerfile,2016-12-09 01:40:55,2016-12-09 01:40:55,2022-06-14 21:49:59,i organize my code in a python package  usually in a virtual environment like virtualenv and or conda  and then usually call  or so that i can use the most recent version of my code  since i develop mostly statistical or machine learning algorithms  i prototype a lot and change my code daily  however  recently the recommended way to run our experiments on the clusters i have access is through docker  i learned about docker and i think i have a rough idea of how to make it work but wanted wasn t quite sure if my solutions was good or if there might be better solutions out there  the first solution that i thought is having a solution that copied the data in my docker image with  and then pip installing it  the issue with this solution is that i have to actually build a new image each time which seems silly and was hoping i could have something better  to do this i was thinking of having a bash file like  as i said  my intuition tells me this is silly so i was hoping there was a single command way to do this with docker or with a single dockerfile  also  note the command should use  v   data   data to be able to get the data and some other volume mount to write to  in the host  when it finishes training  another solution that i thought was to have all the python dependencies or other dependencies that my library needs in the dockerfile  and hence in the image  and then somehow executing in the running container the installation of my library  maybe with docker exec  options  container command as  in the running container  after that then i could run the real experiment i want to run with the same exec command  though  i still don t know how to systematically get the container id though  because i probably don t want to look up the container id every time i do this   ideally in my head the best conceptual solution would be to simply have the dockerfile know from the beginning to which file it should mount to  i e   path_to my_project  and then somehow do python   path_to my_project  develop inside the image so that it would always be linked to the potentially changing python package project  that way i can run my experiments with a single docker command as in  and not have to explicitly update the image myself every time  that includes not having to re install parts of the image that should be static  since its always in sync with the real library  also  having some other script build a new image from scratch each time is not what i am looking for  also  it would be nice to be able to avoid writing any bash too if possible  i think i am very close to a good solution  what i will do instead of building a new image each time i will simply run the cmd command to do python develop as follow  the advantage is that it will only pip install my library whenever i run a new container  this solves the development issue because re creating a new image takes to long  though i just realized that if i use the cmd command then i can t run other commands given to my docker run  so i actually mean to run entrypoint   right now the only issue to complete this is that i am having issues using volume because i can t successfully link to my host project library within the dockerfile  which seems to require an absolute path for some reason   i am currently doing doing  which doesn t seem to work   why can t i link using the volume command in my dockerfile  my main intention with using volume is making my library  and other files that are always needed by this image  accessible when the cmd command tries to install my library  is it possible to just have my library available all the time when a container is initiated  ideally i wanted to just have the library be installed automatically when a container is run and if possible  since the most recent version of the library is always required  have it install when a container is initialized  as a reference right now my non working dockerfile looks as follow  as a side remark  also  for some reason it requires me to do run apt get update to be able to even install pip or vim in my container  do people know why  i wanted to do this because just in case i wanted to attach to the container with a bash terminal  it would be really helpful  seems that docker just forces you to apt install to always have the most recent version of software in the container  bounty  what a solution with copy  and perhaps docker build  f path docker    see  
218,218,7501303,46428870,How to handle date variable in machine learning data pre-processing,"<p>I have a data-set that contains among other variables the time-stamp of the transaction in the format 26-09-2017 15:29:32. I need to find possible correlations and predictions of the sales (lets say in logistic regression). My questions are:</p>

<ol>
<li>How to handle the date format? Shall I convert it to one number (like excel does automatically)? Shall I split it in more variables like day, month, year, hour, mins, seconds? any other possible suggestions?</li>
<li>What if I would like to add distinct week number per year? shall I add variable like 342017(week 34 of year 2017)?</li>
<li>Shall I make the same for question 2 for quarter of year?</li>
</ol>

<blockquote>
<pre><code>#         Datetime               Gender        Purchase
1    23/09/2015 00:00:00           0             1
2    23/09/2015 01:00:00           1             0
3    25/09/2015 02:00:00           1             0
4    27/09/2015 03:00:00           1             1
5    28/09/2015 04:00:00           0             0
</code></pre>
</blockquote>
",30331,2,20,5,python;r;machine-learning;logistic-regression;feature-selection,2017-09-26 19:45:18,2017-09-26 19:45:18,2022-06-14 21:32:30,i have a data set that contains among other variables the time stamp of the transaction in the format        i need to find possible correlations and predictions of the sales  lets say in logistic regression   my questions are 
219,219,19316951,72620046,Google OCR and Vertex AI combo,"<p>I am new to creating applications with vision. I’m trying to find resources outlining a way to both extract text via google OCR, and utilize machine learning to classify the images.</p>
<p>What’s the best way to do this within google?</p>
",22,0,-1,5,machine-learning;google-cloud-platform;ocr;google-vision;google-cloud-vertex-ai,2022-06-14 21:27:14,2022-06-14 21:27:14,2022-06-14 21:27:14,i am new to creating applications with vision  i m trying to find resources outlining a way to both extract text via google ocr  and utilize machine learning to classify the images  what s the best way to do this within google 
220,220,11061320,72619782,How to Deploy a classification Machine Learning Model on application web?,"<p>I am working on a project which consists in detecting epileptic seizures for patients from the EEG signal for this I developed a web application using spring boot, mysql and react js, and I built my classification model machine leraning with Jupyter using the scikit-learn library.
My problem is how can I integrate this model in my application so that a notification will be sent to the user when the patient suffers a epileptic seizure .</p>
",11,0,0,1,machine-learning,2022-06-14 21:08:22,2022-06-14 21:08:22,2022-06-14 21:10:38,
221,221,17136258,72528978,How to use a material number as a feature for Machine Learning?,"<p>I have a problem. I would like to use a classification algorithm. For this I have a column <code>materialNumber</code>, like the name the column represents the material number.</p>
<p><em>How could I use that as a feature for my Machine Learning algorithm?</em></p>
<p>I can not use them e.g. as a One Hot Enconding matrix, because there is too much different material numbers (~4500 unique material numbers).</p>
<p>How can I use this column in a classification algorithm? Do I need to standardize/normalize it? I would like to use a RandomForest classifier.</p>
<pre class=""lang-py prettyprint-override""><code>   customerId  materialNumber  
0           1          1234.0    
1           1          4562.0     
2           2          1234.0    
3           2          4562.0     
4           3          1547.0     
5           3          1547.0  
</code></pre>
",56,1,0,3,machine-learning;classification;feature-selection,2022-06-07 15:07:30,2022-06-07 15:07:30,2022-06-14 20:53:13,i have a problem  i would like to use a classification algorithm  for this i have a column materialnumber  like the name the column represents the material number  how could i use that as a feature for my machine learning algorithm  i can not use them e g  as a one hot enconding matrix  because there is too much different material numbers    unique material numbers   how can i use this column in a classification algorithm  do i need to standardize normalize it  i would like to use a randomforest classifier 
222,222,19077411,72618391,Python: ModuleNotFoundError: No module named &#39;sklearn.linear_model&#39;,"<p>I have been following a YouTube tutorial on machine learning and how to set up a web page for a simple project.</p>
<p>YouTube link: <a href=""https://www.youtube.com/watch?v=Q5JyawS8f5Q&amp;t=683s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Q5JyawS8f5Q&amp;t=683s</a></p>
<p>GitHub link: <a href=""https://github.com/codebasics/py/tree/master/DataScience/BangloreHomePrices/server"" rel=""nofollow noreferrer"">https://github.com/codebasics/py/tree/master/DataScience/BangloreHomePrices/server</a></p>
<p>In the script, I was using the pickle.load() method to load the machine learning model stored in a pickle file. However, I have been getting an error saying that there is no module named sklearn.linear_model. I have tried to import it manually on the same python file, but that also gives me an error as it seems as though PyCharm does not recognize the module.</p>
<p>Error Message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Code\BHP\server\util.py&quot;, line 30, in &lt;module&gt;
    load_saved_artifacts()
  File &quot;C:\Code\BHP\server\util.py&quot;, line 24, in load_saved_artifacts
    __model = pickle.load(f)
ModuleNotFoundError: No module named 'sklearn.linear_model'

Process finished with exit code 1
</code></pre>
<p>Code:</p>
<pre><code>import json
import pickle


__locations = None
__data_columns = None
__model = None


def get_location_names():
    return __locations


def load_saved_artifacts():
    print(&quot;loading saved artifacts...start&quot;)
    global __data_columns
    global __locations

    with open(&quot;./artifacts/columns.json&quot;, 'r') as f:
        __data_columns = json.load(f)[&quot;data_columns&quot;]
        __locations = __data_columns[3:]

    with open(&quot;./artifacts/banglore_home_prices_model.pickle&quot;, 'rb') as f:
        __model = pickle.load(f)

    print(&quot;Loading saved artifacts... Done.&quot;)


if __name__ == '__main__':
    load_saved_artifacts()
    print(get_location_names())
</code></pre>
<p>Text file:</p>
<pre><code>{&quot;data_columns&quot;: [&quot;total_sqft&quot;, &quot;bath&quot;, &quot;bhk&quot;, &quot;1st block jayanagar&quot;, &quot;1st phase jp nagar&quot;, &quot;2nd phase judicial layout&quot;, &quot;2nd stage nagarbhavi&quot;, &quot;5th block hbr layout&quot;, &quot;5th phase jp nagar&quot;, &quot;6th phase jp nagar&quot;, &quot;7th phase jp nagar&quot;, &quot;8th phase jp nagar&quot;, &quot;9th phase jp nagar&quot;, &quot;aecs layout&quot;, &quot;abbigere&quot;, &quot;akshaya nagar&quot;, &quot;ambalipura&quot;, &quot;ambedkar nagar&quot;, &quot;amruthahalli&quot;, &quot;anandapura&quot;, &quot;ananth nagar&quot;, &quot;anekal&quot;, &quot;anjanapura&quot;, &quot;ardendale&quot;, &quot;arekere&quot;, &quot;attibele&quot;, &quot;beml layout&quot;, &quot;btm 2nd stage&quot;, &quot;btm layout&quot;, &quot;babusapalaya&quot;, &quot;badavala nagar&quot;, &quot;balagere&quot;, &quot;banashankari&quot;, &quot;banashankari stage ii&quot;, &quot;banashankari stage iii&quot;, &quot;banashankari stage v&quot;, &quot;banashankari stage vi&quot;, &quot;banaswadi&quot;, &quot;banjara layout&quot;, &quot;bannerghatta&quot;, &quot;bannerghatta road&quot;, &quot;basavangudi&quot;, &quot;basaveshwara nagar&quot;, &quot;battarahalli&quot;, &quot;begur&quot;, &quot;begur road&quot;, &quot;bellandur&quot;, &quot;benson town&quot;, &quot;bharathi nagar&quot;, &quot;bhoganhalli&quot;, &quot;billekahalli&quot;, &quot;binny pete&quot;, &quot;bisuvanahalli&quot;, &quot;bommanahalli&quot;, &quot;bommasandra&quot;, &quot;bommasandra industrial area&quot;, &quot;bommenahalli&quot;, &quot;brookefield&quot;, &quot;budigere&quot;, &quot;cv raman nagar&quot;, &quot;chamrajpet&quot;, &quot;chandapura&quot;, &quot;channasandra&quot;, &quot;chikka tirupathi&quot;, &quot;chikkabanavar&quot;, &quot;chikkalasandra&quot;, &quot;choodasandra&quot;, &quot;cooke town&quot;, &quot;cox town&quot;, &quot;cunningham road&quot;, &quot;dasanapura&quot;, &quot;dasarahalli&quot;, &quot;devanahalli&quot;, &quot;devarachikkanahalli&quot;, &quot;dodda nekkundi&quot;, &quot;doddaballapur&quot;, &quot;doddakallasandra&quot;, &quot;doddathoguru&quot;, &quot;domlur&quot;, &quot;dommasandra&quot;, &quot;epip zone&quot;, &quot;electronic city&quot;, &quot;electronic city phase ii&quot;, &quot;electronics city phase 1&quot;, &quot;frazer town&quot;, &quot;gm palaya&quot;, &quot;garudachar palya&quot;, &quot;giri nagar&quot;, &quot;gollarapalya hosahalli&quot;, &quot;gottigere&quot;, &quot;green glen layout&quot;, &quot;gubbalala&quot;, &quot;gunjur&quot;, &quot;hal 2nd stage&quot;, &quot;hbr layout&quot;, &quot;hrbr layout&quot;, &quot;hsr layout&quot;, &quot;haralur road&quot;, &quot;harlur&quot;, &quot;hebbal&quot;, &quot;hebbal kempapura&quot;, &quot;hegde nagar&quot;, &quot;hennur&quot;, &quot;hennur road&quot;, &quot;hoodi&quot;, &quot;horamavu agara&quot;, &quot;horamavu banaswadi&quot;, &quot;hormavu&quot;, &quot;hosa road&quot;, &quot;hosakerehalli&quot;, &quot;hoskote&quot;, &quot;hosur road&quot;, &quot;hulimavu&quot;, &quot;isro layout&quot;, &quot;itpl&quot;, &quot;iblur village&quot;, &quot;indira nagar&quot;, &quot;jp nagar&quot;, &quot;jakkur&quot;, &quot;jalahalli&quot;, &quot;jalahalli east&quot;, &quot;jigani&quot;, &quot;judicial layout&quot;, &quot;kr puram&quot;, &quot;kadubeesanahalli&quot;, &quot;kadugodi&quot;, &quot;kaggadasapura&quot;, &quot;kaggalipura&quot;, &quot;kaikondrahalli&quot;, &quot;kalena agrahara&quot;, &quot;kalyan nagar&quot;, &quot;kambipura&quot;, &quot;kammanahalli&quot;, &quot;kammasandra&quot;, &quot;kanakapura&quot;, &quot;kanakpura road&quot;, &quot;kannamangala&quot;, &quot;karuna nagar&quot;, &quot;kasavanhalli&quot;, &quot;kasturi nagar&quot;, &quot;kathriguppe&quot;, &quot;kaval byrasandra&quot;, &quot;kenchenahalli&quot;, &quot;kengeri&quot;, &quot;kengeri satellite town&quot;, &quot;kereguddadahalli&quot;, &quot;kodichikkanahalli&quot;, &quot;kodigehaali&quot;, &quot;kodigehalli&quot;, &quot;kodihalli&quot;, &quot;kogilu&quot;, &quot;konanakunte&quot;, &quot;koramangala&quot;, &quot;kothannur&quot;, &quot;kothanur&quot;, &quot;kudlu&quot;, &quot;kudlu gate&quot;, &quot;kumaraswami layout&quot;, &quot;kundalahalli&quot;, &quot;lb shastri nagar&quot;, &quot;laggere&quot;, &quot;lakshminarayana pura&quot;, &quot;lingadheeranahalli&quot;, &quot;magadi road&quot;, &quot;mahadevpura&quot;, &quot;mahalakshmi layout&quot;, &quot;mallasandra&quot;, &quot;malleshpalya&quot;, &quot;malleshwaram&quot;, &quot;marathahalli&quot;, &quot;margondanahalli&quot;, &quot;marsur&quot;, &quot;mico layout&quot;, &quot;munnekollal&quot;, &quot;murugeshpalya&quot;, &quot;mysore road&quot;, &quot;ngr layout&quot;, &quot;nri layout&quot;, &quot;nagarbhavi&quot;, &quot;nagasandra&quot;, &quot;nagavara&quot;, &quot;nagavarapalya&quot;, &quot;narayanapura&quot;, &quot;neeladri nagar&quot;, &quot;nehru nagar&quot;, &quot;ombr layout&quot;, &quot;old airport road&quot;, &quot;old madras road&quot;, &quot;padmanabhanagar&quot;, &quot;pai layout&quot;, &quot;panathur&quot;, &quot;parappana agrahara&quot;, &quot;pattandur agrahara&quot;, &quot;poorna pragna layout&quot;, &quot;prithvi layout&quot;, &quot;r.t. nagar&quot;, &quot;rachenahalli&quot;, &quot;raja rajeshwari nagar&quot;, &quot;rajaji nagar&quot;, &quot;rajiv nagar&quot;, &quot;ramagondanahalli&quot;, &quot;ramamurthy nagar&quot;, &quot;rayasandra&quot;, &quot;sahakara nagar&quot;, &quot;sanjay nagar&quot;, &quot;sarakki nagar&quot;, &quot;sarjapur&quot;, &quot;sarjapur  road&quot;, &quot;sarjapura - attibele road&quot;, &quot;sector 2 hsr layout&quot;, &quot;sector 7 hsr layout&quot;, &quot;seegehalli&quot;, &quot;shampura&quot;, &quot;shivaji nagar&quot;, &quot;singasandra&quot;, &quot;somasundara palya&quot;, &quot;sompura&quot;, &quot;sonnenahalli&quot;, &quot;subramanyapura&quot;, &quot;sultan palaya&quot;, &quot;tc palaya&quot;, &quot;talaghattapura&quot;, &quot;thanisandra&quot;, &quot;thigalarapalya&quot;, &quot;thubarahalli&quot;, &quot;tindlu&quot;, &quot;tumkur road&quot;, &quot;ulsoor&quot;, &quot;uttarahalli&quot;, &quot;varthur&quot;, &quot;varthur road&quot;, &quot;vasanthapura&quot;, &quot;vidyaranyapura&quot;, &quot;vijayanagar&quot;, &quot;vishveshwarya layout&quot;, &quot;vishwapriya layout&quot;, &quot;vittasandra&quot;, &quot;whitefield&quot;, &quot;yelachenahalli&quot;, &quot;yelahanka&quot;, &quot;yelahanka new town&quot;, &quot;yelenahalli&quot;, &quot;yeshwanthpur&quot;]}
</code></pre>
<p>Please help me with how to resolve this issue. Any advice is appreciated.</p>
",36,2,0,2,python;scikit-learn,2022-06-14 19:33:46,2022-06-14 19:33:46,2022-06-14 19:41:56,i have been following a youtube tutorial on machine learning and how to set up a web page for a simple project  youtube link   github link   in the script  i was using the pickle load   method to load the machine learning model stored in a pickle file  however  i have been getting an error saying that there is no module named sklearn linear_model  i have tried to import it manually on the same python file  but that also gives me an error as it seems as though pycharm does not recognize the module  error message  code  text file  please help me with how to resolve this issue  any advice is appreciated 
223,223,4554243,72617036,Weight transmission protocol in Federated Machine Learning,"<p>I am wondering, in federated machine learning, when we train our local models, and intend to update the cloud model, what protocol we use to transmit those weight? Also, when we use the tensorflow federated machine learning, how we transmit the weight (using which library and protocol)?</p>
<p>Kind regards,</p>
",15,1,0,3,machine-learning;protocols;tensorflow-federated,2022-06-14 18:01:12,2022-06-14 18:01:12,2022-06-14 18:33:28,i am wondering  in federated machine learning  when we train our local models  and intend to update the cloud model  what protocol we use to transmit those weight  also  when we use the tensorflow federated machine learning  how we transmit the weight  using which library and protocol   kind regards 
224,224,11508786,72613150,How to reduce heroku slug size,"<p>I am trying to deploy machine learning app on heroku
Its giving me the following error</p>
<pre><code>remote:  !     Compiled slug size: 534M is too large (max is 500M).
</code></pre>
<p>Files:</p>
<pre><code>app.py
chatbot.h5
classes.pkl
words.pkl
data.json
preprocessor.py
Procfile
requirements.txt
runtime.txt
wsgi.py
</code></pre>
<p>requirement.txt</p>
<pre><code>Flask==2.0.3
Jinja2==3.0.3
keras==2.6.0
nltk==3.6.7
numpy==1.19.5
tensorflow==2.6.2
</code></pre>
",17,1,-1,2,python;heroku,2022-06-14 13:09:27,2022-06-14 13:09:27,2022-06-14 18:10:27,files  requirement txt
225,225,19055037,72600575,One-Hot Encoding to a list feature. Pyspark,"<p>I would like to prepare my dataset to be used by machine learning algorithms. I have a feature composed by a list of the tags associated to every TV series (my records).
It is possible to apply the one-hot encoding directly or it would be preferable to first extract all the possible elements of the aforementioned lists?
My idea is to use this tags for the next analysis.</p>
<p>Here is an example of my dataset and the code applied to it.</p>
<p><img src=""https://i.stack.imgur.com/T6LwU.png"" alt=""my dataframe"" /></p>
<pre><code>from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder

indexer = StringIndexer(inputCol=&quot;tags&quot;, outputCol=&quot;tagsIndex&quot;)

df = indexer.fit(df).transform(df)

ohe = OneHotEncoder(inputCol=&quot;tagsIndex&quot;, outputCol=&quot;tagsOHEVector&quot;)

df = ohe.fit(df).transform(df)
</code></pre>
",50,1,0,5,python;pyspark;apache-spark-sql;apache-spark-mllib;one-hot-encoding,2022-06-13 14:58:55,2022-06-13 14:58:55,2022-06-14 18:09:09,here is an example of my dataset and the code applied to it  
226,226,19334669,72611870,How to deal with features with more than 80% missingness,"<p>I'm working with a really bad clinical dataset, it has 300 samples, 400 features, which will be used for machine learning. My advisor told me about some biologically meaningful features in this dataset and asked me to keep them, but many of them are missing more than 50%, or even more than 80%. What should I do? Does padding with mode affect their performance.</p>
",32,1,-1,2,machine-learning;missing-data,2022-06-14 10:53:12,2022-06-14 10:53:12,2022-06-14 14:17:31,i m working with a really bad clinical dataset  it has  samples   features  which will be used for machine learning  my advisor told me about some biologically meaningful features in this dataset and asked me to keep them  but many of them are missing more than    or even more than    what should i do  does padding with mode affect their performance 
227,227,11267281,72612782,Python SDK v2 for Azure Machine Learning SDK (preview) - how to retrieve workspace from WorkspaceOperations,"<p>I’m following <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk"" rel=""nofollow noreferrer"">this article</a> to create ML pipelines with the new SDK.</p>
<p>So I started by loading the first class</p>
<pre><code>from azure.ai.ml import MLClient
</code></pre>
<p>and then I used it to authenticated on my workspace</p>
<pre><code>ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group_name,
    workspace_name=&quot; mmAmlsWksp01&quot;,
)
</code></pre>
<p>However, I can’t understand how I can retrieve the objects it refers to. For example, it contains a “workspaces” member, but if I run</p>
<pre><code>ml_client.workspaces[&quot;mmAmlsWksp01&quot;] 
</code></pre>
<p>, I get the error “<em><strong>'WorkspaceOperations' object is not subscriptable</strong></em>”.</p>
<p>So I tried to run</p>
<pre><code>for w in ml_client.workspaces.list():
    print(w)
</code></pre>
<p>and it returns the workspace details (name, displayName, id…) for a SINGLE workspace, but not the workspace object.</p>
<p>In fact, the ml_client.workspaces object is a</p>
<p>&lt;azure.ai.ml._operations.workspace_operations.WorkspaceOperations at 0x7f3526e45d60&gt;</p>
<p>, but I don’t want a WorkspaceOperation, I want the Workspace itself. How can I retrieve it?</p>
",34,0,0,3,python-3.x;azure;azure-machine-learning-service,2022-06-14 12:35:44,2022-06-14 12:35:44,2022-06-14 12:35:44,i m following  to create ml pipelines with the new sdk  so i started by loading the first class and then i used it to authenticated on my workspace however  i can t understand how i can retrieve the objects it refers to  for example  it contains a  workspaces  member  but if i run   i get the error   workspaceoperations  object is not subscriptable   so i tried to run and it returns the workspace details  name  displayname  id   for a single workspace  but not the workspace object  in fact  the ml_client workspaces object is a  lt azure ai ml _operations workspace_operations workspaceoperations at xfed gt    but i don t want a workspaceoperation  i want the workspace itself  how can i retrieve it 
228,228,11451547,72265652,"Failure to use ml5.js image classification model-Error: Based on the provided shape, [1,64,64,4], the tensor should have 16384 values but has 20155392","<p>I trained an image classification neural network model written in <code>ml5.js</code>. When I try to use the model files in a <code>p5.js</code> web editor, I get an error 'Based on the provided shape, [1,64,64,4], the tensor should have 16384 values but has 20155392'.<br/><br/>
The code is in this p5 sketch - <a href=""https://editor.p5js.org/konstantina1/sketches/85Ny1SC2J"" rel=""nofollow noreferrer"">https://editor.p5js.org/konstantina1/sketches/85Ny1SC2J</a> (clicking on the arrow in the top right corner will show the files).<br/>
When I run a local server on a web page with the same structure, I see 'model ready!' (a confirmation that the model has loaded) and that's it.<br/><br/>
I read a lot of comments that the <code>bin</code> file may be corrupt - I saved the model myself producing the <code>bin</code> file so it should be ok.
As suggested here by the author of very similar code, <a href=""https://www.youtube.com/watch?v=3MqJzMvHE3E"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=3MqJzMvHE3E</a>, adding <code>pixelDensity(1)</code> in <code>setup()</code> doesn't help.<br/><br/>
I am new to machine learning, could someone please help? Thank you in advance.</p>
",47,2,0,5,conv-neural-network;p5.js;tensorflow.js;image-classification;ml5.js,2022-05-17 02:38:53,2022-05-17 02:38:53,2022-06-14 12:04:24,
229,229,16373136,72607471,My CNN model accuracy is not improving with ResNet50,"<p>I'm working on a machine learning project with TensorFlow for sign language recognition.
When I first tried to build my CNN model it has a validation accuracy of 94% :</p>
<pre><code>def create_model():  
 model = Sequential(name='SIGNS_LANGUAGE')
 model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE, IMG_SIZE , 3) ) )
 model.add(layers.MaxPooling2D(pool_size = (2, 2)))
 model.add(layers.Conv2D(64, (3, 3), activation='relu' ,input_shape=(IMG_SIZE, IMG_SIZE , 3) ) )
 model.add(layers.MaxPooling2D(pool_size = (2, 2)))
 #classification layers 
 model.add(layers.Flatten())
 model.add(layers.Dense(512, activation='relu'))
 model.add(layers.Dropout(0.2))
 model.add(layers.Dense(512, activation='relu'))
 model.add(layers.Dropout(0.2))
 model.add(layers.Dense(512, activation='relu'))
 model.add(layers.Dropout(0.2))
 model.add(layers.Dense(targetCount, activation='softmax'))
 print(model.summary())
 return model`
</code></pre>
<p>BUT IT HASN'T BEEN GOOD AT <strong>generalizing</strong> with a different data.</p>
<p>So I tried to work with a pre-trained model like ResNet50 but I couldn't figure out why the accuracy is not improving with time, it hit 40% and stopped:</p>
<pre><code>def create_model():
  model = Sequential(name='SIGNS')
  input_shape = (IMG_SIZE , IMG_SIZE , 3)
  res_layer = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False , weights='imagenet' )
  res_layer.trainable = False
  model.add( res_layer )
  model.add(layers.GlobalAveragePooling2D())
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(1024, activation='relu'))
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(targetCount, activation='softmax'))
  print(model.summary())
  return model
</code></pre>
<p>Any suggestions on how to improve the model??</p>
<p>N.B: I input 54049 grayscale images in the 3 channels and I use Adam optimizer.</p>
",19,1,0,4,python;tensorflow;keras;conv-neural-network,2022-06-13 23:56:11,2022-06-13 23:56:11,2022-06-14 11:12:00,but it hasn t been good at generalizing with a different data  so i tried to work with a pre trained model like resnet but i couldn t figure out why the accuracy is not improving with time  it hit   and stopped  any suggestions on how to improve the model   n b  i input  grayscale images in the  channels and i use adam optimizer 
230,230,1477388,72589399,What is Threshold in the Evaluate Model Module?,"<p>Notice in the image below, if I increase the value of &quot;Threshold,&quot; the accuracy of the model seems to increase (with diminishing returns after about .62).</p>
<p>What does this mean and can I somehow update this value such that my model will retain this setting?</p>
<p>For example, I am using a boosted decision tree, but I don't see any such value for &quot;threshold.&quot;</p>
<p>Ref. <a href=""https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/evaluate-model?redirectedfrom=MSDN"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/evaluate-model?redirectedfrom=MSDN</a></p>
<p><a href=""https://i.stack.imgur.com/E0o5I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E0o5I.png"" alt=""enter image description here"" /></a></p>
",38,1,0,1,azure-machine-learning-studio,2022-06-12 09:31:44,2022-06-12 09:31:44,2022-06-14 10:34:54,notice in the image below  if i increase the value of  threshold   the accuracy of the model seems to increase  with diminishing returns after about     what does this mean and can i somehow update this value such that my model will retain this setting  for example  i am using a boosted decision tree  but i don t see any such value for  threshold   ref   
231,231,6442390,72610242,Improve efficiency of distance matrix calculation for time series with dtw in big dataset with python,"<p>I'm trying to improve the distance matrix calculation of all possible paris in a dataset of 109489 elements.</p>
<p>All the elements on the column are time series (stored as lists) and the metric I'm using to do the calculations is DTW.</p>
<p>I've created the following code that's working kind of good but I want to improve it because it will take almost a year to finish the calculations:</p>
<pre><code>import numpy as np
import joblib
import _ucrdtw

def compute_distance(cycle1, cycle2):
    return _ucrdtw.ucrdtw(cycle1, cycle2, 0.05, False)

cycles = joblib.load('new_cycles.pkl')

dim_matrix = np.empty((len(cycles),len(cycles)), dtype=object)
with joblib.Parallel(n_jobs=15, verbose=10) as parallel:
    for i in range(len(cycles)):
        print(i)
        dim_matrix[i] = parallel(joblib.delayed(compute_distance)(
            cycles.iloc[i]['cycle_info_scaled'], 
            cycles.iloc[j]['cycle_info_scaled']) for j in range(len(cycles)))
</code></pre>
<p>This is an example of what's stored in the column <code>cycle_info_scaled</code> of the data frame:</p>
<pre><code>cycle_info_scaled
[0.9948399033268663, 0.9948399033268663, 0.995...   
[1.0005639107922013, 1.00041937494038, 1.00020...   
[0.9975316557982873, 0.9975316557982873, 0.997...   
[1.0004695161595962, 1.000252692084482, 1.0002...   
[0.9919345197891065, 0.9919345197891065, 0.991...   
...
[0.9888568683957731,...
[1.0588396740044068,...
[0.9830848045072209,...
[0.9918426003000614, 0.9918426003000614, 0.991...
</code></pre>
<p>The purpose if to use this code to do a clustering of the time series
I'm kind of amateur in coding and Machine learning...
How can I improve the efficiency of this code?</p>
",30,0,0,5,python;machine-learning;parallel-processing;hierarchical-clustering;dtw,2022-06-14 05:37:57,2022-06-14 05:37:57,2022-06-14 05:40:59,i m trying to improve the distance matrix calculation of all possible paris in a dataset of  elements  all the elements on the column are time series  stored as lists  and the metric i m using to do the calculations is dtw  i ve created the following code that s working kind of good but i want to improve it because it will take almost a year to finish the calculations  this is an example of what s stored in the column cycle_info_scaled of the data frame 
232,232,11747069,72551630,How to setup a DVC shared cache without git repository between different services in minikube?,"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.</p>
<p>More specifically, I have a docker image called <code>inference-service</code> that should only <code>dvc pull</code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models</code> and <code>data</code> folders (see dockerfile) in minikube. I have another image called <code>test-service</code> that
runs the ML pipeline using <code>dvc repro</code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service</code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service</code> can pull it and use it instead of running dvc repro by itself. <code>test-service</code> should only re-train and write the updated models and data into the shared cache while <code>inference-service</code> should only read and recreate the updated/latest models and data from the shared cache.</p>
<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f</code>) the data and models after the test service is done with <code>dvc repro</code> and results the following warnings and failures:</strong></em></p>
<p><em>relevant kubernetes pod log of inference-service</em></p>
<pre><code>WARNING: Output 'data/processed/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit preprocess` to associate existing 'data/processed/train_preprocessed.pkl' with stage: 'preprocess'.
WARNING: Output 'data/processed/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit preprocess` to associate existing 'data/processed/validation_preprocessed.pkl' with stage: 'preprocess'.
WARNING: Output 'data/processed/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit preprocess` to associate existing 'data/processed/test_preprocessed.pkl' with stage: 'preprocess'.
WARNING: Output 'data/interim/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit featurize` to associate existing 'data/interim/train_featurized.pkl' with stage: 'featurize'.
WARNING: Output 'data/interim/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit featurize` to associate existing 'data/interim/validation_featurized.pkl' with stage: 'featurize'.
WARNING: Output 'data/interim/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
You can also use `dvc commit featurize` to associate existing 'data/interim/test_featurized.pkl' with stage: 'featurize'.
WARNING: Output 'models/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
WARNING: Output 'models/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
WARNING: Output 'models/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
WARNING: Output 'reports/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.
WARNING: No file hash info found for '/root/models/model.pkl'. It won't be created.
WARNING: No file hash info found for '/root/reports/scores.json'. It won't be created.
WARNING: No file hash info found for '/root/data/processed/train_preprocessed.pkl'. It won't be created.
WARNING: No file hash info found for '/root/data/processed/validation_preprocessed.pkl'. It won't be created.
WARNING: No file hash info found for '/root/data/processed/test_preprocessed.pkl'. It won't be created.
WARNING: No file hash info found for '/root/data/interim/train_featurized.pkl'. It won't be created.
WARNING: No file hash info found for '/root/data/interim/validation_featurized.pkl'. It won't be created.
WARNING: No file hash info found for '/root/data/interim/test_featurized.pkl'. It won't be created.
WARNING: No file hash info found for '/root/models/mlb.pkl'. It won't be created.
WARNING: No file hash info found for '/root/models/tfidf_vectorizer.pkl'. It won't be created.
10 files failed
ERROR: failed to pull data from the cloud - Checkout failed for following targets:
/root/models/model.pkl
/root/reports/scores.json
/root/data/processed/train_preprocessed.pkl
/root/data/processed/validation_preprocessed.pkl
/root/data/processed/test_preprocessed.pkl
/root/data/interim/train_featurized.pkl
/root/data/interim/validation_featurized.pkl
/root/data/interim/test_featurized.pkl
/root/models/mlb.pkl
/root/models/tfidf_vectorizer.pkl
Is your cache up to date?
</code></pre>
<p><em>relevant kubernetes pod log of test-service</em></p>
<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs
Generating lock file 'dvc.lock'
Updating lock file 'dvc.lock'
Stage 'featurize' is cached - skipping run, checking out outputs
Updating lock file 'dvc.lock'
Stage 'train' is cached - skipping run, checking out outputs
Updating lock file 'dvc.lock'
Stage 'evaluate' is cached - skipping run, checking out outputs
Updating lock file 'dvc.lock'
Use `dvc push` to send your updates to remote storage.
</code></pre>
<p><strong>Project Tree</strong></p>
<pre><code>├─ .dvc
│  ├─ .gitignore
│  ├─ config
│  └─ tmp
├─ deployment
│  ├─ docker-compose
│  │  ├─ docker-compose.yml
│  ├─ minikube-dep
│  │  ├─ inference-test-services_dep.yaml
│  ├─ startup_minikube_with_mount.sh.sh
├─ Dockerfile # for inference service
├─ dvc-cache # services should push and pull from this cache folder and see this as the DVC repo
├- dvc.yaml
├- params.yaml
├─ src
│  ├─ build_features.py
|  ├─ preprocess_data.py
|  ├─ serve_model.py
|  ├─ startup.sh  
|  ├─ requirements.txt
├─ test_dep
│  ├─ .dvc # same as .dvc in the root folder
|  |  ├─...
│  ├─ Dockerfile # for test service
│  ├─ dvc.yaml
|  ├─ params.yaml
│  └─ src
│     ├─ build_features.py # same as root src folder
|     ├─ preprocess_data.py # same as root src folder
|     ├─ serve_model.py # same as root src folder
|     ├─ startup_test.sh  
|     ├─ requirements.txt  # same as root src folder
</code></pre>
<p><strong>dvc.yaml</strong></p>
<pre><code>stages:
  preprocess:
    cmd: python ${preprocess.script}
    params:
      - preprocess
    deps:
      - ${preprocess.script}
      - ${preprocess.input_train}
      - ${preprocess.input_val}
      - ${preprocess.input_test}
    outs:
      - ${preprocess.output_train}
      - ${preprocess.output_val}
      - ${preprocess.output_test}
  featurize:
    cmd: python ${featurize.script}
    params:
      - preprocess
      - featurize
    deps:
      - ${featurize.script}
      - ${preprocess.output_train}
      - ${preprocess.output_val}
      - ${preprocess.output_test}
    outs:
      - ${featurize.output_train}
      - ${featurize.output_val}
      - ${featurize.output_test}
      - ${featurize.mlb_out}
      - ${featurize.tfidf_vectorizer_out}
  train:
    cmd: python ${train.script}
    params:
      - featurize
      - train
    deps:
      - ${train.script}
      - ${featurize.output_train}
    outs:
      - ${train.model_out}
  evaluate:
    cmd: python ${evaluate.script}
    params:
      - featurize
      - train
      - evaluate
    deps:
      - ${evaluate.script}
      - ${train.model_out}
      - ${featurize.output_val}
    metrics:
      - ${evaluate.scores_path}
</code></pre>
<p><strong>params.yaml</strong></p>
<pre><code>preprocess:
  script: src/preprocess/preprocess_data.py
  input_train: data/raw/train.tsv
  input_val: data/raw/validation.tsv
  input_test: data/raw/test.tsv
  output_train: data/processed/train_preprocessed.pkl
  output_val: data/processed/validation_preprocessed.pkl
  output_test: data/processed/test_preprocessed.pkl

featurize:
  script: src/features/build_features.py
  output_train: data/interim/train_featurized.pkl
  output_val: data/interim/validation_featurized.pkl
  output_test: data/interim/test_featurized.pkl
  mlb_out: models/mlb.pkl
  tfidf_vectorizer_out: models/tfidf_vectorizer.pkl

train:
  script: src/models/train_model.py
  model_out: models/model.pkl

evaluate:
  script: src/models/evaluate_model.py
  scores_path: reports/scores.json
  roc_json: reports/roc_plot.json
  prc_json: reports/prc_plot.json
</code></pre>
",73,1,0,4,docker;kubernetes;minikube;dvc,2022-06-09 01:37:48,2022-06-09 01:37:48,2022-06-14 03:20:56,i need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update dvc models and data needed for training machine learning models  the structure of the project is to use  pod to periodically update the cache with new models and outputs  then  multiple pods can read the cache to recreate the updated models and data  so i need to be able to update the local cache directory and pull from it using dvc commands  so that all the services have consistent view on the latest models and data created by a service  problem  the cache does get mounted on the minikube vm  but the inference service does not pull  using dvc pull  f  the data and models after the test service is done with dvc repro and results the following warnings and failures  relevant kubernetes pod log of inference service relevant kubernetes pod log of test service project tree dvc yaml params yaml
233,233,16086649,72603517,Which unsupervised algorithm we can use to detect anomaly in transaction data?,"<p>I was trying to look for an algorithm which can be used to find anomalies in transaction data which also contains Timestamp as one of the columns. I tried using Isolation forest but I think it's not possible to use it with the DateTime column Or is it possible to use isolation forest?? Well, I'm new to machine learning, so seeking some help here.</p>
",17,1,-2,5,python-3.x;machine-learning;jupyter-notebook;unsupervised-learning;anomaly-detection,2022-06-13 18:45:09,2022-06-13 18:45:09,2022-06-14 01:01:07,i was trying to look for an algorithm which can be used to find anomalies in transaction data which also contains timestamp as one of the columns  i tried using isolation forest but i think it s not possible to use it with the datetime column or is it possible to use isolation forest   well  i m new to machine learning  so seeking some help here 
234,234,19332186,72607297,Blockchain - Machine Learning in Cartesi Rollup,"<p>Is it possible somehow to train a machine learnig model inside the Cartesi Machine? I believe, if models are trained outside Cartesi, its not possible to audit the results integrity, if it is biased or not. If everything is within Cartesi, I think this would be possible.</p>
<p>projects that I saw:</p>
<p><a href=""https://github.com/souzavinny/rollups-examples/tree/main/biometrics"" rel=""nofollow noreferrer"">https://github.com/souzavinny/rollups-examples/tree/main/biometrics</a></p>
<p><a href=""https://medium.com/cartesi/ecosystem-update-mainstream-developers-on-the-blockchain-os-e7210b381ca4"" rel=""nofollow noreferrer"">https://medium.com/cartesi/ecosystem-update-mainstream-developers-on-the-blockchain-os-e7210b381ca4</a></p>
",49,1,0,4,machine-learning;blockchain;decentralized-applications;cartesi,2022-06-13 23:37:25,2022-06-13 23:37:25,2022-06-14 00:46:34,is it possible somehow to train a machine learnig model inside the cartesi machine  i believe  if models are trained outside cartesi  its not possible to audit the results integrity  if it is biased or not  if everything is within cartesi  i think this would be possible  projects that i saw   
235,235,12204620,72589339,Can you work with Kotlin Multiplatform Mobile (KMM) using Windows for Android part?,"<p>I wanted to enable one of my Android project to the new KMM but will deploy only for Android platform for the time being. According to this <a href=""https://kotlinlang.org/docs/multiplatform-mobile-faq.html#what-is-the-kotlin-multiplatform-mobile-plugin"" rel=""nofollow noreferrer"">FAQ page</a> it seems that the plugin works only for macOS. But was confused on the next paragraph stating</p>
<blockquote>
<p>The good news is that you can work with cross-platform projects on
Android even without the Kotlin Multiplatform Mobile plugin. If you
are going to work with shared code or Android-specific code, you can
work on any operating system supported by Android Studio.</p>
</blockquote>
<p>I am not planning yet to run the iOS part as I am still learning it, all I want for now is to prepare my project so when I am confident with my iOS skill starting it would be easy. I do not have a good Mac device as of now that is why I prefer to use my Windows machine for now which is more reliable for heavy stuff like Android development.</p>
",41,1,0,5,android;ios;kotlin-multiplatform;kotlin-multiplatform-mobile;kmm,2022-06-12 09:14:17,2022-06-12 09:14:17,2022-06-13 22:32:33,i wanted to enable one of my android project to the new kmm but will deploy only for android platform for the time being  according to this  it seems that the plugin works only for macos  but was confused on the next paragraph stating i am not planning yet to run the ios part as i am still learning it  all i want for now is to prepare my project so when i am confident with my ios skill starting it would be easy  i do not have a good mac device as of now that is why i prefer to use my windows machine for now which is more reliable for heavy stuff like android development 
236,236,11237476,62583856,how to log hydra&#39;s multi-run in mlflow,"<p>I am trying to manage the results of machine learning with mlflow and hydra.
So I tried to run it using the multi-run feature of hydra.
I used the following code as a test.</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    with mlflow.start_run() :
        mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()
</code></pre>
<p>This code will not work.
I got the following error</p>
<pre><code>Exception: Run with UUID [RUNID] is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
</code></pre>
<p>So I modified the code as follows</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    with mlflow.start_run(nested=True) :
        mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()

</code></pre>
<p>This code works, but the artifact is not saved.
The following corrections were made to save the artifacts.</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    
    mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()
</code></pre>
<p>As a result, the artifacts are now saved.
However, when I run the following command</p>
<pre><code>python test.py model=A,B hidden=12,212,31 -m
</code></pre>
<p>Only the artifact of the last execution condition was saved.</p>
<p>How can I modify mlflow to manage the parameters of the experiment by taking advantage of the multirun feature of hydra?</p>
",936,3,1,5,python;machine-learning;data-science;mlflow;fb-hydra,2020-06-26 02:00:55,2020-06-26 02:00:55,2022-06-13 20:49:58,so i modified the code as follows only the artifact of the last execution condition was saved  how can i modify mlflow to manage the parameters of the experiment by taking advantage of the multirun feature of hydra 
237,237,5496040,64093979,Weights and Biases: Login and network errors,"<p>I recently installed Weights and Biases (wandb) for recording the metrics of my machine learning projects. Everything worked fine when connected to wandb cloud instance or when I used a local docker image. Now, when I tried to access my local wandb instance from over the network, I started to get API error messages. However, I also noticed that wandb was trying to access my server using port 80, instead of 8080. I installed wandb client on a new cloud server and tried to access my server from there. Still, same error message shown below.</p>
<p>This error happens when I use the command: <code>wandb login host=https://api.wandb.ai</code>
I have tried to delete the .netrc file where the api settings are stored and re-installed wandb. Still same error. Using wandb version 0.10.2 on Ubuntu 18.04; Also, tried downgrading to version 0.8.36, no change.
If I try the command: <code>wandb login --relogin</code>, I get the same error.</p>
<p>Is there some way to reset wandb so it forgets all these settings, or to resolve this issue directly?</p>
<p>Many thanks</p>
<p>Best Regards,</p>
<p>Adeel</p>
<pre><code>Retry attempt failed:
Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/connection.py&quot;, line 84, in create_connection
    raise err
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
    chunked=chunked,
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1277, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1323, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1272, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1032, in _send_output
    self.send(msg)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 972, in send
    self.connect()
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 187, in connect
    conn = self._new_conn()
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 172, in _new_conn
    self, &quot;Failed to establish a new connection: %s&quot; % e
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 727, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/retry.py&quot;, line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: /graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/old/retry.py&quot;, line 96, in __call__
    result = self._call_fn(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/internal/internal_api.py&quot;, line 128, in execute
    return self.client.execute(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/client.py&quot;, line 52, in execute
    result = self._get_result(document, *args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/client.py&quot;, line 60, in _get_result
    return self.transport.execute(document, *args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/transport/requests.py&quot;, line 38, in execute
    request = requests.post(self.url, **post_args)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/api.py&quot;, line 119, in post
    return request('post', url, data=data, json=json, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/sessions.py&quot;, line 530, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/sessions.py&quot;, line 643, in send
    r = adapter.send(request, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: /graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
wandb: Network error (ConnectionError), entering retry loop. See wandb/debug-internal.log for full traceback.
Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/bin/wandb&quot;, line 8, in &lt;module&gt;
    sys.exit(cli())
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 829, in __call__
    return self.main(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 782, in main
    rv = self.invoke(ctx)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 610, in invoke
    return callback(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/cli/cli.py&quot;, line 72, in wrapper
    return func(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/cli/cli.py&quot;, line 212, in login
    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 29, in login
    anonymous=anonymous, key=key, relogin=relogin, host=host, force=force
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 128, in _login
    apikey.write_key(settings, key)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/lib/apikey.py&quot;, line 223, in write_key
    raise ValueError(&quot;API key must be 40 characters long, yours was %s&quot; % len(key))
ValueError: API key must be 40 characters long, yours was 26
</code></pre>
",4666,1,2,1,python,2020-09-28 04:11:51,2020-09-28 04:11:51,2022-06-13 18:16:02,i recently installed weights and biases  wandb  for recording the metrics of my machine learning projects  everything worked fine when connected to wandb cloud instance or when i used a local docker image  now  when i tried to access my local wandb instance from over the network  i started to get api error messages  however  i also noticed that wandb was trying to access my server using port   instead of   i installed wandb client on a new cloud server and tried to access my server from there  still  same error message shown below  is there some way to reset wandb so it forgets all these settings  or to resolve this issue directly  many thanks best regards  adeel
238,238,614157,31610971,Spark - repartition() vs coalesce(),"<p>According to Learning Spark</p>

<blockquote>
  <p>Keep in mind that repartitioning your data is a fairly expensive operation.
  Spark also has an optimized version of <code>repartition()</code> called <code>coalesce()</code> that allows avoiding data movement, but only if you are decreasing the number of RDD partitions.</p>
</blockquote>

<p>One difference I get is that with <code>repartition()</code> the number of partitions can be increased/decreased, but with <code>coalesce()</code> the number of partitions can only be decreased.</p>

<p>If the partitions are spread across multiple machines and <code>coalesce()</code> is run, how can it avoid data movement?</p>
",322023,19,373,3,apache-spark;distributed-computing;rdd,2015-07-24 18:19:19,2015-07-24 18:19:19,2022-06-13 18:06:03,according to learning spark one difference i get is that with repartition   the number of partitions can be increased decreased  but with coalesce   the number of partitions can only be decreased  if the partitions are spread across multiple machines and coalesce   is run  how can it avoid data movement 
239,239,12052478,72599948,Tensorflow path in conda enviroment,"<p>Each time I try to create a python environment with conda it uses my .local/lib/python3.6/site-packages path as the file path for the python packages, I noticed this because when trying to use tensorflow.</p>
<p><strong>this is the command that i use to create the conda env</strong></p>
<pre><code>conda create -n tensorflow_od pip python=3.9
</code></pre>
<p>** then activate the environment with</p>
<pre><code>conda activate tensorflow_od
</code></pre>
<p><strong>this is what I get when I check the tensorflow that is used within the environment using &quot;pip show tensorflow&quot;</strong></p>
<p><em>Name: tensorflow
Version: 2.3.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/</a>
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/nwoke/.local/lib/python3.6/site-packages
Requires: absl-py, astunparse, gast, google-pasta, grpcio, h5py, keras-preprocessing, numpy, opt-einsum, protobuf, scipy, six, tensorboard, tensorflow-estimator, termcolor, wheel, wrapt
Required-by: tensorflow-io, tensorflow-text, tf-models-official</em></p>
<p>How can I resolve this</p>
",20,1,1,3,python;tensorflow;conda,2022-06-13 14:09:42,2022-06-13 14:09:42,2022-06-13 17:37:35,each time i try to create a python environment with conda it uses my  local lib python  site packages path as the file path for the python packages  i noticed this because when trying to use tensorflow  this is the command that i use to create the conda env    then activate the environment with this is what i get when i check the tensorflow that is used within the environment using  pip show tensorflow  how can i resolve this
240,240,17838535,72601328,Group dictionaries in list of dictionaries based on consecutive integers as key values,"<p>I have a list with dictionaries that I would like to split or group into new dictionary lists based on  a key with consecutive integers. This is my input:</p>
<pre><code>lst=[{'name':'sam','class':1},
     {'name':'adam','class':2},
     {'name':'smith','class':3},
     {'name':'john','class':10},
     {'name':'r.','class':11},
     {'name':'doe','class':12}
     ]
</code></pre>
<p>Expected output:</p>
<pre><code>[[{'name':'sam','class':1},
 {'name':'adam','class':2},
 {'name':'smith','class':3}],
 [{'name':'john','class':10},
 {'name':'r.','class':11},
 {'name':'doe','class':12}]
 ]
</code></pre>
<p>I'm aware I can sort a list like this, but I don't know how to do the same with a list of dictionaries. In the end, I would like to group the names together. As you can tell from the example, I have a list of dictionaries with person's names as strings and am grouping them so that first, middle and last names are in unique list. I get these person dictionaries based on a NER machine learning algorithm that I run over a string containing names, but also other information.</p>
<p>Thank you!</p>
",35,1,-1,2,python;dictionary,2022-06-13 15:59:12,2022-06-13 15:59:12,2022-06-13 16:20:17,i have a list with dictionaries that i would like to split or group into new dictionary lists based on  a key with consecutive integers  this is my input  expected output  i m aware i can sort a list like this  but i don t know how to do the same with a list of dictionaries  in the end  i would like to group the names together  as you can tell from the example  i have a list of dictionaries with person s names as strings and am grouping them so that first  middle and last names are in unique list  i get these person dictionaries based on a ner machine learning algorithm that i run over a string containing names  but also other information  thank you 
241,241,700268,5607563,How to scrape logos from websites?,"<p>First off, this is not a question about how to scrape websites. I am fully aware of the tools available to me to scrape (css_parser, nokogiri, etc. I'm using Ruby to do the scraping).</p>

<p>This is more of an overarching question on the best possible solution to scrape the <strong>logo</strong> of a website starting with nothing but a website address.</p>

<p>The two solutions I've begun to create are these:</p>

<ol>
<li>Use Google AJAX APIs to do an image search that is scoped to the site in question, with the query ""logo"", and grab the first result. This gets the logo, I'd say, about 30% of the time.</li>
<li>The problem with the above is that Google doesn't really seem to care about CSS image replaced logos (ie. H1 text that is image replaced with the logo). The solution I've tentatively come up with is to pull down all CSS files, scan for url() declarations, and then look for the words header or logo in the file names.</li>
</ol>

<p>Solution two is problematic because of the many idiosyncrasies of all the people who write CSS for websites. They use Header instead of logo in the file name. Sometimes the file name is random, saying nothing about a logo. Other times, it's just the wrong image.</p>

<p>I realize I <em>might</em> be able to do something with some sort of machine learning, but I'm on a bit of a deadline for a client and need something fairly capable soon. </p>

<p>So with all that said, if anyone has any ""out of the box"" thinking on this one, I'd love to hear it. If I can create a solution that works well enough, I plan on open-sourcing the library for any other interested parties :)</p>

<p>Thanks! </p>
",10032,5,8,2,screen-scraping;html-parsing,2011-04-10 01:38:55,2011-04-10 01:38:55,2022-06-13 16:00:24,first off  this is not a question about how to scrape websites  i am fully aware of the tools available to me to scrape  css_parser  nokogiri  etc  i m using ruby to do the scraping   this is more of an overarching question on the best possible solution to scrape the logo of a website starting with nothing but a website address  the two solutions i ve begun to create are these  solution two is problematic because of the many idiosyncrasies of all the people who write css for websites  they use header instead of logo in the file name  sometimes the file name is random  saying nothing about a logo  other times  it s just the wrong image  i realize i might be able to do something with some sort of machine learning  but i m on a bit of a deadline for a client and need something fairly capable soon   so with all that said  if anyone has any out of the box thinking on this one  i d love to hear it  if i can create a solution that works well enough  i plan on open sourcing the library for any other interested parties    thanks  
242,242,16345037,72597099,Has anyone gotten a 3060 TI to run Microsoft.ML in .NET Core C#?,"<p>I'm using JetBrains Rider to build an image recognition system and predict the image. I was using the CPU but it's terribly slow. I recently got a 3060 TI and I've followed the MS Docs (<a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-gpu-model-builder"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-gpu-model-builder</a>) to set it up. Using CUDA 10.1 and cudnn 7.6.4 with Microsoft.ML Version 2.3.1 got everything started, but the system hung for so long I thought it was broken. 10, maybe 15 minutes went by so I stopped the program. Further research lead me to finding that the Ampere architecture requires CUDA 11.2 and cudnn 8.1.0 and ML Version 2.4.0. This spits out &quot;Unable to find an entry point named 'TF_StringEncodedSize' in DLL 'tensorflow'.&quot;</p>
<p>I've tried installing every version of CUDA and cudnn, using every combination of Ml versions, and can't, for the life of me, find anything else useful.</p>
<p>How can I make this work?</p>
<p>Edit: using visual studio's model builder extension, I manage to get it to work, however it took 40 minutes to train on 25 images. Seems a bit ludicrous. How can I improve the speed with which .NET sets up the card?</p>
",24,0,-1,4,tensorflow;.net-core;gpu;jetbrains-rider,2022-06-13 07:27:00,2022-06-13 07:27:00,2022-06-13 15:36:46,i m using jetbrains rider to build an image recognition system and predict the image  i was using the cpu but it s terribly slow  i recently got a  ti and i ve followed the ms docs    to set it up  using cuda   and cudnn    with microsoft ml version    got everything started  but the system hung for so long i thought it was broken    maybe  minutes went by so i stopped the program  further research lead me to finding that the ampere architecture requires cuda   and cudnn    and ml version     this spits out  unable to find an entry point named  tf_stringencodedsize  in dll  tensorflow    i ve tried installing every version of cuda and cudnn  using every combination of ml versions  and can t  for the life of me  find anything else useful  how can i make this work  edit  using visual studio s model builder extension  i manage to get it to work  however it took  minutes to train on  images  seems a bit ludicrous  how can i improve the speed with which  net sets up the card 
243,243,3083243,43253461,Neural-net Regression predicts same value for all test samples,"<p>My <strong>neural-network regression model predicts one value for all the test samples</strong>. Playing with hyperparameters like epochs, batch_size, number of layers, hidden units, learning rate, etc. only changes the prediction values to a new constant.</p>

<p>For testing, if I test on the training data itself, I get almost accurate results and a RMSE ~ 1.</p>

<p>Note: The task is to predict remaining life of a machine from its run-till-failure time series data. I have used tsfresh library to generate 1045 features from the original time series data with only 24 features.</p>

<p><strong><em>What should be causing this behavior? How should I visualize a neural network model development to make sure things are going in the right direction?</em></strong></p>

<pre><code>print ""Shape of training_features is"", train_X.shape
print ""Shape of train_labels is"", train_Y.shape
print ""Shape of test_features is"", test_X.shape
print ""shape of test_labels is"", test_Y.shape

input_dim = train_X.shape[1]
# Function to create model, required for KerasRegressor
def create_model(h1=50, h2=50, act1='sigmoid', act2='sigmoid', init='he_normal', learn_rate=0.001, momentum=0.1, loss='mean_squared_error'):
    # create model
    model = Sequential()
    model.add(Dense(h1, input_dim=input_dim, init=init, activation=act1))
    model.add(Dense(h2, init=init, activation=act2))
    model.add(Dense(1, init=init))
    # Compile model
    optimizer = SGD(lr=learn_rate, momentum=momentum)
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])
    return model

''' THE REAL THING '''
# create model
model = KerasRegressor(build_fn=create_model, verbose=0)

# SCORING FUNCTION
grid_scorer = make_scorer(mean_squared_error, greater_is_better=False)
# Grid Search
batch_size = [8]
epochs = [500]
init_mode = ['glorot_uniform']
learn_rate = [0.0001]
momentum = [0.1]

hidden_layer_1 = [75]
activation_1 = ['sigmoid']
hidden_layer_2 = [15]
activation_2 = ['sigmoid']

param_grid = dict(batch_size=batch_size, nb_epoch=epochs, init=init_mode, h1=hidden_layer_1, h2=hidden_layer_2, act1 = activation_1, act2=activation_2, learn_rate=learn_rate, momentum=momentum)

print ""\n...BEGIN SEARCH...""
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=grid_scorer, verbose=1)

print ""\nLet's fit the training data...""
grid_result = grid.fit(train_X, train_Y)

# summarize results
print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(""%f (%f) with: %r"" % (mean, stdev, param))

predicted = grid.predict(test_X)  
print ""\nPrediction array is\n"", predicted
rmse = numpy.sqrt(((predicted - test_Y) ** 2).mean(axis=0))
print ""Test RMSE is"", rmse
</code></pre>

<p>Output:</p>

<pre><code>Shape of training_features is (249, 1045)
Shape of train_labels is (249,)
Shape of test_features is (248, 1045)
shape of test_labels is (248,)

...BEGIN SEARCH...

Let's fit the training data...
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Best: -891.761863 using {'learn_rate': 0.0001, 'h2': 15, 'act1': 'sigmoid', 'act2': 'sigmoid', 'h1': 75, 'batch_size': 8, 'init': 'glorot_uniform', 'nb_epoch': 500, 'momentum': 0.1}
-891.761863 (347.253351) with: {'learn_rate': 0.0001, 'h2': 15, 'act1': 'sigmoid', 'act2': 'sigmoid', 'h1': 75, 'batch_size': 8, 'init': 'glorot_uniform', 'nb_epoch': 500, 'momentum': 0.1}

Prediction array is
[ 295.72067261  295.72067261  295.72067261  295.72067261  295.72067261
  295.72067261  295.72067261  ...
                              295.72067261  295.72067261  295.72067261
  295.72067261  295.72067261  295.72067261]
Test RMSE is 95.0019297411
</code></pre>
",2798,2,6,5,python;machine-learning;neural-network;regression;keras,2017-04-06 16:41:36,2017-04-06 16:41:36,2022-06-13 13:24:04,my neural network regression model predicts one value for all the test samples  playing with hyperparameters like epochs  batch_size  number of layers  hidden units  learning rate  etc  only changes the prediction values to a new constant  for testing  if i test on the training data itself  i get almost accurate results and a rmse     note  the task is to predict remaining life of a machine from its run till failure time series data  i have used tsfresh library to generate  features from the original time series data with only  features  what should be causing this behavior  how should i visualize a neural network model development to make sure things are going in the right direction  output 
244,244,1332870,72583717,pySpark --&gt; NoSuchTableException: Table or view &#39;my_test_table_thread_1&#39; not found in database &#39;default&#39;,"<p>I run a spark job and it works without any error. In my pyspark code , I run 3 machine learning job sequently. But when I try them work in a thread concurently i got an error. It gives error on this part:</p>
<pre><code>def run(.....):
(
......



   sc = SparkContext.getOrCreate(conf=conf)
   sc.setCheckpointDir(&quot;/tmp/ersing/&quot;)
   spark = SparkSession(sc)


    temp_name = &quot;my_test_table_thread_&quot;+str(thread_id)
    my_table.createOrReplaceTempView(temp_name)

    print(temp_name +&quot; count(*) --&gt; &quot; + str(my_table.count()))
    print(&quot;&quot;&quot;spark.catalog.tableExists(&quot;&quot;&quot;+temp_name+&quot;&quot;&quot;) = &quot;&quot;&quot; + str(spark._jsparkSession.catalog().tableExists(temp_name))) 

    model_sql = &quot;&quot;&quot;select id from {sample_table_name} where 
                   id= {id} &quot;&quot;&quot;.format(id=id, sample_table_name=temp_name)
    my_df= spark.sql(model_sql).select(&quot;id&quot;,)  #this part gives error --&gt; no such table
    my_df= broadcast(my_df)
......
)
</code></pre>
<p>my main code is :</p>
<pre><code>....

from multiprocessing.pool import ThreadPool
import threading
    
def run_worker(job): 
     returned_sample_table= run('sampling',...)  # i call run method twice. First run get df and I call second run for modeling
     run('modeling',...,returned_sample_table)

def mp_handler():
    p = ThreadPool(8)
    p.map(run_worker, jobs)
    p.join()
    p.close()

mp_handler()
</code></pre>
<p>I run 3 jobs concurently and every time just one job <strong>createOrReplaceTempView</strong> works fine because i logged this : <code>print(&quot;&quot;&quot;spark.catalog.tableExists(&quot;&quot;&quot;+temp_name+&quot;&quot;&quot;) = &quot;&quot;&quot; + str(spark._jsparkSession.catalog().tableExists(temp_name)))</code> and I saw one of jobs is exists and others not.</p>
<p>So what i am missing?</p>
<p>Thanks in advance.</p>
",28,1,0,3,multithreading;apache-spark;pyspark,2022-06-11 16:24:28,2022-06-11 16:24:28,2022-06-13 12:52:19,i run a spark job and it works without any error  in my pyspark code   i run  machine learning job sequently  but when i try them work in a thread concurently i got an error  it gives error on this part  my main code is   i run  jobs concurently and every time just one job createorreplacetempview works fine because i logged this   print    spark catalog tableexists     temp_name              str spark _jsparksession catalog   tableexists temp_name    and i saw one of jobs is exists and others not  so what i am missing  thanks in advance 
245,245,19116900,72510914,Jupyter notebook not loading ipynb abd showing blank page,"<p>I am new to machine learning and i have installed miniconda, i have created the env, downloaded packages and installed jupyter but the problem i have is that when i run the <strong>$jupyter notebook</strong> it opens the dashboard and everything looks good until i pick one notebook( i have created new ones but the outcome was the same) it opens a new tab and it is blank after that <a href=""https://i.stack.imgur.com/Rbd63.jpg"" rel=""nofollow noreferrer"">when i open the notebook</a>
If someone can solve this i will be thankfull</p>
",16,0,-1,2,python;jupyter-notebook,2022-06-06 01:59:34,2022-06-06 01:59:34,2022-06-13 12:36:23,
246,246,8127672,72584999,ImportError: cannot import name &#39;_argmax&#39; from &#39;sklearn.utils.fixes&#39;,"<p>I am setting up a Machine Learning model using Linear Regression however trying to run</p>
<pre><code>from sklearn.model_selection import train_test_split 
</code></pre>
<p>throws below exception:</p>
<pre><code>ImportError: cannot import name '_argmax' from 'sklearn.utils.fixes'
</code></pre>
<p>I am working with PyCharm (Jupyter notebook) and also tried restarting kernel.</p>
<p>However, when I try to run code from Pycharm in a Python file it works.</p>
",33,1,0,2,python;scikit-learn,2022-06-11 19:33:25,2022-06-11 19:33:25,2022-06-13 05:14:00,i am setting up a machine learning model using linear regression however trying to run throws below exception  i am working with pycharm  jupyter notebook  and also tried restarting kernel  however  when i try to run code from pycharm in a python file it works 
247,247,19325927,72595734,Github Pages Jekyll - Can&#39;t override css from theme,"<p>Just to preface I'm terrible with front-end and I'm learning Ruby/Jekyll.</p>
<p>I'm working on a GitHub pages site, and I'm attempting to overwrite some css class that is inherited from the Jekyll theme that I'm using.</p>
<p>I've created a table with a custom css class, and all I'm trying to do is get rid of the border on this table.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>.portfolio-table td {
  font-size: 14px;
  overflow: hidden;
  padding: 10px 5px;
  word-break: normal;
}
.portfolio-table th {
  font-size: 14px;
  font-weight: normal;
  overflow: hidden;
  padding: 10px 5px;
  word-break: normal;
}
.portfolio-table .portfolio-table-text-header {
  font-weight: bold;
  text-align: left;
  vertical-align: top;
}
.portfolio-table .portfolio-table-text-body {
  color: #9b9b9b;
  text-align: left;
  vertical-align: top;
}
.portfolio-table .portfolio-table-header,
.portfolio-table .portfolio-table-image,
.portfolio-table .portfolio-table-button {
  text-align: left;
  vertical-align: top;
}</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;table class=""portfolio-table"" style=""border: 0px transparent #ffffff;""&gt;
            &lt;tbody&gt;
                &lt;tr&gt;
                    &lt;td class=""portfolio-table-image""&gt;&lt;img
                            src=""data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 512 512'%3e%3cpath fill='%23000000' d='M157.52 272h36.96L176 218.78 157.52 272zM352 256c-13.23 0-24 10.77-24 24s10.77 24 24 24 24-10.77 24-24-10.77-24-24-24zM464 64H48C21.5 64 0 85.5 0 112v288c0 26.5 21.5 48 48 48h416c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zM250.58 352h-16.94c-6.81 0-12.88-4.32-15.12-10.75L211.15 320h-70.29l-7.38 21.25A16 16 0 0 1 118.36 352h-16.94c-11.01 0-18.73-10.85-15.12-21.25L140 176.12A23.995 23.995 0 0 1 162.67 160h26.66A23.99 23.99 0 0 1 212 176.13l53.69 154.62c3.61 10.4-4.11 21.25-15.11 21.25zM424 336c0 8.84-7.16 16-16 16h-16c-4.85 0-9.04-2.27-11.98-5.68-8.62 3.66-18.09 5.68-28.02 5.68-39.7 0-72-32.3-72-72s32.3-72 72-72c8.46 0 16.46 1.73 24 4.42V176c0-8.84 7.16-16 16-16h16c8.84 0 16 7.16 16 16v160z'/%3e%3c/svg%3e""
                            alt=""Image"" width=""100"" height=""100""&gt;&lt;/td&gt;
                    &lt;td class=""portfolio-table-image""&gt;&lt;img
                            src=""data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 512 512'%3e%3cpath fill='%23000000' d='M157.52 272h36.96L176 218.78 157.52 272zM352 256c-13.23 0-24 10.77-24 24s10.77 24 24 24 24-10.77 24-24-10.77-24-24-24zM464 64H48C21.5 64 0 85.5 0 112v288c0 26.5 21.5 48 48 48h416c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zM250.58 352h-16.94c-6.81 0-12.88-4.32-15.12-10.75L211.15 320h-70.29l-7.38 21.25A16 16 0 0 1 118.36 352h-16.94c-11.01 0-18.73-10.85-15.12-21.25L140 176.12A23.995 23.995 0 0 1 162.67 160h26.66A23.99 23.99 0 0 1 212 176.13l53.69 154.62c3.61 10.4-4.11 21.25-15.11 21.25zM424 336c0 8.84-7.16 16-16 16h-16c-4.85 0-9.04-2.27-11.98-5.68-8.62 3.66-18.09 5.68-28.02 5.68-39.7 0-72-32.3-72-72s32.3-72 72-72c8.46 0 16.46 1.73 24 4.42V176c0-8.84 7.16-16 16-16h16c8.84 0 16 7.16 16 16v160z'/%3e%3c/svg%3e""
                            alt=""Image"" width=""100"" height=""100""&gt;&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                    &lt;td class=""portfolio-table-text-header""&gt;LEAN STARTUP PRODUCT DEVELOPMENT&lt;/td&gt;
                    &lt;td class=""portfolio-table-text-header""&gt;IMPLEMENTING AGILE - KANBAN &amp;amp; SCRUM&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                    &lt;td class=""portfolio-table-text-body""&gt;Lunna is an app that provides assistance on women's
                        health. The app learns about user choices, preferences, and symptoms using machine learning;
                        allowing the user to improve life quality. Developed an MVP (minimum viable product) and
                        ready to scale version of the app in React Native and Elixir. Ran 2+ workshops during design
                        and also development phases&lt;/td&gt;
                    &lt;td class=""portfolio-table-text-body""&gt;SoFi is an online personal finance company. I managed the
                        projects with a team of 30+ people. Including designers, copywriters, product marketing
                        managers and sales teams ofdifferent areas. Delivered 94 projects within a month including
                        billboards, social media campaigns,TV and YouTube commercials, eCRM campaigns, and events
                        for different marketing campaigns.&lt;/td&gt;
                &lt;/tr&gt;
                &lt;tr&gt;
                    &lt;td class=""portfolio-table-button""&gt;BUTTON&lt;/td&gt;
                    &lt;td class=""portfolio-table-button""&gt;BUTTON&lt;/td&gt;
                &lt;/tr&gt;
            &lt;/tbody&gt;
        &lt;/table&gt;</code></pre>
</div>
</div>
</p>
<p>This table generates nicely through this, where the border does not show up. But it is contained in a <code>&lt;main id=&quot;content&quot; class=&quot;main-content&quot; role=&quot;main&quot;&gt;</code> tag that is bringing css rules into it from Jekyll's theme.</p>
<p>I have tried !important, inline css, style tags in the HTMl. Nothing I have tried can get rid of this border.</p>
",24,0,1,4,css;ruby;jekyll;github-pages,2022-06-13 02:10:42,2022-06-13 02:10:42,2022-06-13 02:10:42,just to preface i m terrible with front end and i m learning ruby jekyll  i m working on a github pages site  and i m attempting to overwrite some css class that is inherited from the jekyll theme that i m using  i ve created a table with a custom css class  and all i m trying to do is get rid of the border on this table  this table generates nicely through this  where the border does not show up  but it is contained in a  lt main id  content  class  main content  role  main  gt  tag that is bringing css rules into it from jekyll s theme  i have tried  important  inline css  style tags in the html  nothing i have tried can get rid of this border 
248,248,17965106,72586582,ML Algorithm to make Teams with almost equal average performance based on Player&#39;s performance,"<p>I have a dataset containing Player Names and Their Performance (in numbers). Let's say I have 20 Players and I want to make 4 teams (A B C D) with 5 players each.</p>
<p><a href=""https://i.stack.imgur.com/uS2mb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uS2mb.png"" alt=""Data Set Containing Players"" /></a></p>
<h3><strong>Question:</strong> What machine learning algorithm will help me to make four teams with equal average performance and minimum performance difference between each team?</h3>
<p>I went through this <a href=""https://stackoverflow.com/questions/8951996/multi-player-team-game-auto-team-balancing-algorithm-based-on-player-rank"">Auto team-balancing question</a> but it is asked for simple maths
and logic.</p>
<p><strong>What I thought can be done:</strong></p>
<ol>
<li>Sum up total Performance, divide it by number of teams needed then use different combinations until you have 4 teams with min avg difference.</li>
<li>Use KMEANS (but the KMEANS will cluster different groups in 5 performance classes.</li>
<li>Divide data into 5 (total/teams) clusters using KMEANS then taking one player from each cluster for every team.</li>
</ol>
<p>Please guide me / answer me what can be done for the above problem.</p>
<p>Also, It would be helpful if I can get answers for the following too:</p>
<ol>
<li>What if i increase the complexity in the question, let's say each team has a specific budget and each player has some salary they need.</li>
<li>Let's say each team can have 2 batsmen, 2 baller , 1 Allrounder.</li>
</ol>
<p>How will the above questions change our Algorithm, and how can we adapt these changes.</p>
<p>Also, Can it be considered as a <strong>NP Hard</strong> problem? or are there any optimized algorithms available for this? <strong>Is ML even needed for this Algorithm?</strong></p>
<p><strong>REFERENCE IMAGE FOR Budget and Salary Question</strong></p>
<p>Let's say budget for each team is <strong>4.5</strong></p>
<p><a href=""https://i.stack.imgur.com/ZnGlh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZnGlh.png"" alt=""Salary of players"" /></a></p>
",32,0,-1,3,algorithm;machine-learning;combinations,2022-06-11 23:08:42,2022-06-11 23:08:42,2022-06-13 01:29:01,i have a dataset containing player names and their performance  in numbers   let s say i have  players and i want to make  teams  a b c d  with  players each   what i thought can be done  please guide me   answer me what can be done for the above problem  also  it would be helpful if i can get answers for the following too  how will the above questions change our algorithm  and how can we adapt these changes  also  can it be considered as a np hard problem  or are there any optimized algorithms available for this  is ml even needed for this algorithm  reference image for budget and salary question let s say budget for each team is   
249,249,19235906,72496667,Finding a specific feature in time series data,"<p>I have a large collection of time series data, and I am essentially looking to isolate specific instances of a kind of feature in the data. I have a collection of these features that I've found manually, and I would like to find more of them in the data in an automated way. For reference the features I'm looking for are essentially a train of Gaussians with decreasing amplitude, which are pretty easy to identify by eye but can vary in spacing, amplitude and width, so I've had trouble coming up with an algorithm to look for them.</p>
<p>My naive suspicion was that this sounded like a job for machine learning techniques, but I know very little about ML and I am having trouble formulating the problem. Are there ML algorithms that are suited for my problem?</p>
<p>EDIT:</p>
<p>Maybe this will be clearer:</p>
<p>I have a very large 1D time series dataset, x(t). Within this dataset are visually similar features, but the dataset is too large to look for them by hand. I've collected a number of examples of this feature, say [q_i], where each feature q &quot;looks&quot; similar and is of the same length.
I would like to find more features in x(t) that &quot;look&quot; similar to the examples in [q_i].</p>
",28,0,-1,2,machine-learning;time-series,2022-06-04 08:04:54,2022-06-04 08:04:54,2022-06-13 00:57:55,i have a large collection of time series data  and i am essentially looking to isolate specific instances of a kind of feature in the data  i have a collection of these features that i ve found manually  and i would like to find more of them in the data in an automated way  for reference the features i m looking for are essentially a train of gaussians with decreasing amplitude  which are pretty easy to identify by eye but can vary in spacing  amplitude and width  so i ve had trouble coming up with an algorithm to look for them  my naive suspicion was that this sounded like a job for machine learning techniques  but i know very little about ml and i am having trouble formulating the problem  are there ml algorithms that are suited for my problem  edit  maybe this will be clearer 
250,250,17102070,72569742,ML-insert new data to machine learning model,"<p>i'm new to Ml , i want to enter new data to <strong>trained</strong> model to make prediction ,new data is in the <strong>SAMPLE.csv</strong>,but i get this error</p>
<pre><code> modelfile = 'voting.pkl'
 model = p.load(open(modelfile, 'rb'))
 iris = pd.read_csv(&quot;Sample3.csv&quot;)
    x = np.array(iris)
    response = np.array2string(model.predict(x))
    return jsonify(response)
</code></pre>
<p>error :
TypeError: predict() missing 1 required positional argument: 'X'</p>
",24,0,1,1,model,2022-06-10 11:35:50,2022-06-10 11:35:50,2022-06-13 00:51:19,i m new to ml   i want to enter new data to trained model to make prediction  new data is in the sample csv but i get this error
251,251,17354801,72594779,Is there any trick to read images from path and put them in other path in Kaggle?,"<p>I am working on Kaggle to implement a machine learning model. My data set are COCO dataset  set which contain about 118k images. I inserted it from Kaggle dataset .so its just inside one path. Can i read just the first 50k images and put them in other path in Kaggle?</p>
<p>Thanks</p>
",13,0,-1,4,image;machine-learning;path;kaggle,2022-06-12 23:46:41,2022-06-12 23:46:41,2022-06-12 23:50:53,i am working on kaggle to implement a machine learning model  my data set are coco dataset  set which contain about k images  i inserted it from kaggle dataset  so its just inside one path  can i read just the first k images and put them in other path in kaggle  thanks
252,252,19225716,72591567,Classification based on a list of words,"<p>I have a dataset like this one below:</p>
<pre><code>Id | ArticleName | Pages | Topics             | ...
1  | abcd...     | 9999  | Animals            | ...
2  | aabbcc..    | 8888  | AI, Computer, HiFi | ...
3  | aaabbb      | 7777  | Hot Dog, Animals   | ...
4  | cccbb       | 6666  | Dataset, R         | ...
5  | dddss       | 64    | Hamburger, AI      | ...
</code></pre>
<p>Each row of the ds represents an article which has, in the column <strong>Topics</strong>, a list of words of the topics of the article itself.</p>
<p>Topics have a main Area which refer to.  For example, let's say:</p>
<ul>
<li>Nature: (Animals, Plants)</li>
<li>Technology: (Computer, HiFi, AI, Intelligent Systems, IOT, Machine Learning)</li>
<li>Food: (Pizza, Fast Food, Hamburger, Hot Dog, Salad, Fries)</li>
</ul>
<p>I've to come up with a result where if the list of topics covered by the article contains a word of the list Nature, for example, I'd have a mark (let's say 1) if the article covers &gt;=1 arguments of the list Nature, and so on with others Areas. If no matches are found we'd have a mark in NC (Not Classified)
In other words I need a classification based on the presence of words.</p>
<p>Here's the example taking the ds shown up as input.</p>
<pre><code>Id | ArticleName | Pages | Topics             | Nature | Technology | Food | NC
1  | abcd...     | 9999  | Animals            | 1      | 0          | 0    | 0
2  | aabbcc..    | 8888  | AI, Computer, HiFi | 0      | 1          | 0    | 0
3  | aaabbb      | 7777  | Hot Dog, Animals   | 1      | 0          | 1    | 0
4  | cccbb       | 6666  | Dataset, R         | 0      | 0          | 0    | 1
5  | dddss       | 64    | Hamburger, AI      | 0      | 1          | 1    | 0
</code></pre>
",52,2,0,2,r;classification,2022-06-12 16:29:01,2022-06-12 16:29:01,2022-06-12 23:43:32,i have a dataset like this one below  each row of the ds represents an article which has  in the column topics  a list of words of the topics of the article itself  topics have a main area which refer to   for example  let s say  here s the example taking the ds shown up as input 
253,253,1497720,58469775,Are pipelines capable of cacheing intermediate results?,"<p>I use pandas to do feature extraction for machine learning.
I hope to achieve the following: Consider I have five data processing steps done sequentially, and I execute thme once. Eesults will be saved automatically. Next time, if I change the fourth step, the library will automatically start from the third step.</p>
<p>Would this cache function be supported in Pandas or <code>sklearn.pipeline.Pipeline</code> or other data processing libraries naturally without our need to save them explicitly?</p>
",333,2,1,5,python;python-3.x;pandas;scikit-learn;feature-extraction,2019-10-20 08:50:17,2019-10-20 08:50:17,2022-06-12 21:45:21,would this cache function be supported in pandas or sklearn pipeline pipeline or other data processing libraries naturally without our need to save them explicitly 
254,254,17244035,72593445,Mongo: Same port used in Local Mongo Instance and when exposed through docker,"<p>I recently started learning docker. I already had a local instance of Mongo Server running on my local machine, I used two ports 27017 and 27018 (replica set). When I was running mongo container on my local machine, I used the following command</p>
<pre><code>docker run -d --rm --name mongodb -p 27017:27017 mongo
</code></pre>
<p>I assumed I would get an error that 27017 port is already in use in local machine, but I did not get any error and the container started. I am unable to understand why. When i used Mongo Compass to see which mongo is running on port 27107, it showed the one that was already installed on my local machine from earlier (recognized it by looking at DB names). It was only when i stopped local mongo instances, and reconnected compass to localhost:27017 I saw a new mongo instance (probably the one that was created using docker).</p>
<p>How did docker bind 27017 internally when it was already in use?</p>
",12,0,0,4,mongodb;docker;containers;port,2022-06-12 20:41:58,2022-06-12 20:41:58,2022-06-12 20:41:58,i recently started learning docker  i already had a local instance of mongo server running on my local machine  i used two ports  and   replica set   when i was running mongo container on my local machine  i used the following command i assumed i would get an error that  port is already in use in local machine  but i did not get any error and the container started  i am unable to understand why  when i used mongo compass to see which mongo is running on port   it showed the one that was already installed on my local machine from earlier  recognized it by looking at db names   it was only when i stopped local mongo instances  and reconnected compass to localhost  i saw a new mongo instance  probably the one that was created using docker   how did docker bind  internally when it was already in use 
255,255,11547698,72583957,Reducing the dimensions of a 4D feature tensor from ResNet to fit into a 2D LSTM model,"<p>I am designing a machine learning model that takes a feature tensor from ResNet and uses an LSTM to identify the sequences of letters in the image. The feature tensor that's from ResNet is 4-D , however, LSTM_cell wants inputs that are 2-D. I know about other methods such as .view() and .squeeze() that are able to reduce dimensions. However, it seems as if I do this, it changes the size of the dimensions of the feature vectors. At first the vector is [128, 2, 5, 512] but it needs to be [128, 512]. However, calling .view(-1,512) multiplies the dimensions to get [1280, 512]. How would you change dimensions without multiplying?</p>
",34,2,0,5,deep-learning;pytorch;conv-neural-network;lstm;recurrent-neural-network,2022-06-11 17:00:41,2022-06-11 17:00:41,2022-06-12 20:02:49,i am designing a machine learning model that takes a feature tensor from resnet and uses an lstm to identify the sequences of letters in the image  the feature tensor that s from resnet is  d   however  lstm_cell wants inputs that are  d  i know about other methods such as  view   and  squeeze   that are able to reduce dimensions  however  it seems as if i do this  it changes the size of the dimensions of the feature vectors  at first the vector is          but it needs to be       however  calling  view     multiplies the dimensions to get       how would you change dimensions without multiplying 
256,256,19323625,72591402,How do I initialize DMatrix from list,"<p>I have trained an XGBoost regression model in Python and serialized it using pickle library. However, when I try to feed the model to a machine learning engine and carry out online predictions, I get an error:</p>
<blockquote>
<p>TypeError: can not initialize DMatrix from list.</p>
</blockquote>
<p>How can I resolve this problem as I have tried almost everything?</p>
",17,0,-1,5,python;machine-learning;deployment;xgboost;streamlit,2022-06-12 16:05:19,2022-06-12 16:05:19,2022-06-12 18:25:51,i have trained an xgboost regression model in python and serialized it using pickle library  however  when i try to feed the model to a machine learning engine and carry out online predictions  i get an error  typeerror  can not initialize dmatrix from list  how can i resolve this problem as i have tried almost everything 
257,257,6934798,41271947,How do I go about predicting Closing Price of a Financial Symbol (EURUSD) using Machine Learning?,"<p>I did a simple experiment using EURUSD OHLC 1-Day data.<br>My features were Open Price, Low Price, High Price, and I was trying to predict the future Closing price.</p>

<p>The code worked, as expected, but the results were very misleading.</p>

<p>I got a 99% Accuracy score, which as we all know is impossible.</p>

<p>1) So what I am I doing wrong?<br>
2) How can I correct my mistakes?</p>

<p>The official system I am building would have BoP, PPI, Interest Rate, GDP, and a lot of Momentum indicators, etc. as Features, over some 60 features.</p>

<pre><code>import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
#import pickle

# 1. Read the EURUSD csv data.
# 2. Process the DataFrame, using only the Open, High, Low, Close columns.
df = pd.read_csv( 'EURUSD1440.csv', index_col= 'Date' )
df = df[['Open','High','Low','Close']]
array = df.values

# Features consist of Open, High, Low column, and stored in x.
# Label is the Close column stored in y.
x = array[:,0:3]
y = array[:,3]


# Split Data into Test and Train.
# 60% Train and 40% Test.
from sklearn.cross_validation import train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.4 )


# 1. Train the Model using .fit method.
# 2. Predict the future Closing prices using the .predict method.
# 3. Know how Accurate the Model is using the .score method.
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score

model = LinearRegression()
model.fit( x_train, y_train )
forecast = model.predict( x_test )
accuracy = model.score( x_test, y_test )

print( forecast, accuracy )
</code></pre>
",667,2,0,4,python-3.x;machine-learning;time-series;quantitative-finance,2016-12-22 02:39:49,2016-12-22 02:39:49,2022-06-12 14:44:11,i did a simple experiment using eurusd ohlc  day data my features were open price  low price  high price  and i was trying to predict the future closing price  the code worked  as expected  but the results were very misleading  i got a   accuracy score  which as we all know is impossible  the official system i am building would have bop  ppi  interest rate  gdp  and a lot of momentum indicators  etc  as features  over some  features 
258,258,10970202,72589784,InvalidArgument error when trying to build RNN using functional api,"<p>I'm following <a href=""https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb"" rel=""nofollow noreferrer"">https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb</a> tutorial, &quot;Building an RNN model for the sentiment analysis task&quot;</p>
<p>In the book it implements RNN using sequential API however I am trying to implement using functional API.</p>
<p>code in sequential API looks like this:</p>
<pre><code>embedding_dim = 20
vocab_size = len(token_counts) + 2

bi_lstm_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        name='embed-layer'),
    
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(64, name='lstm-layer'),
        name='bidir-lstm'), 

    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

bi_lstm_model.summary()

bi_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])

history = bi_lstm_model.fit(train_data, validation_data=valid_data, epochs=10)
</code></pre>
<p>This is my try on converting it to functional API:</p>
<pre><code>inputs = tf.keras.Input(shape=(vocab_size, ))
emb_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=emb_dim)(inputs)
lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(emb_layer)
dense = tf.keras.layers.Dense(64, activation='relu')(lstm)
outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense)

lstm_model = tf.keras.Model(inputs, outputs, name=&quot;LSTM&quot;)
</code></pre>
<p>However it keeps on giving me following error:
<code>InvalidArgumentError:  Input to reshape is a tensor with 19840 values, but the requested shape has 2780480</code></p>
<p>what is the problem?</p>
<p>also I thought in order to run .summary() with sequential API we needed to add input layer or fit the model before running .summary(). Why not here?</p>
",15,0,0,4,python;tensorflow;keras;deep-learning,2022-06-12 11:17:54,2022-06-12 11:17:54,2022-06-12 11:17:54,i m following  tutorial   building an rnn model for the sentiment analysis task  in the book it implements rnn using sequential api however i am trying to implement using functional api  code in sequential api looks like this  this is my try on converting it to functional api  what is the problem  also i thought in order to run  summary   with sequential api we needed to add input layer or fit the model before running  summary    why not here 
259,259,13844181,72588945,NotFittedError - Titanic Project Kaggle,"<p>I am trying different machine learning projects from Kaggle to make myself better. Here is the model that I am using:</p>
<pre><code>from sklearn.ensemble import RandomForestClassifier

y = train_data[&quot;Survived&quot;]

features = [&quot;Pclass&quot;, &quot;Sex&quot;, &quot;SibSp&quot;, &quot;Parch&quot;]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 1)
model.fit = (X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index = False)
print('Your submission was successfully saved!')
</code></pre>
<p>Here is the error I get:</p>
<pre><code>---------------------------------------------------------------------------
NotFittedError                            Traceback (most recent call last)
/tmp/ipykernel_33/1528591149.py in &lt;module&gt;
      9 forest_clf = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 1)
     10 forest_clf.fit = (X, y)
---&gt; 11 predictions = forest_clf.predict(X_test)
     12 
     13 output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})

/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py in predict(self, X)
    806             The predicted classes.
    807         &quot;&quot;&quot;
--&gt; 808         proba = self.predict_proba(X)
    809 
    810         if self.n_outputs_ == 1:

/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py in predict_proba(self, X)
    846             classes corresponds to that in the attribute :term:`classes_`.
    847         &quot;&quot;&quot;
--&gt; 848         check_is_fitted(self)
    849         # Check data
    850         X = self._validate_X_predict(X)

/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
   1220 
   1221     if not fitted:
-&gt; 1222         raise NotFittedError(msg % {&quot;name&quot;: type(estimator).__name__})
   1223 
   1224 

NotFittedError: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
</code></pre>
<p>I think this is an example of the estimator cloning itself, but I am not sure which line is the issue here. This is the Titanic project that is seen on Kaggle, whose tutorial code I have copied amidst trying to learn. Any help is appreciated.</p>
",28,1,2,4,python;jupyter-notebook;jupyter;kaggle,2022-06-12 07:04:10,2022-06-12 07:04:10,2022-06-12 10:43:16,i am trying different machine learning projects from kaggle to make myself better  here is the model that i am using  here is the error i get  i think this is an example of the estimator cloning itself  but i am not sure which line is the issue here  this is the titanic project that is seen on kaggle  whose tutorial code i have copied amidst trying to learn  any help is appreciated 
260,260,13468738,68443284,How to implement Bi-Directional Conv LSTM in Pytorch,"<pre><code>import torch
from torch import nn


def initialize_weights(self, layer):
        &quot;&quot;&quot;Initialize a layer's weights and biases.

        Args:
            layer: A PyTorch Module's layer.&quot;&quot;&quot;
        if isinstance(layer, (nn.BatchNorm2d, nn.BatchNorm1d)):
            pass
        else:
            try:
                nn.init.xavier_normal_(layer.weight)
            except AttributeError:
                pass
            try:
                nn.init.uniform_(layer.bias)
            except (ValueError, AttributeError):
                pass
class HadamardProduct(nn.Module):
    &quot;&quot;&quot;A Hadamard product layer.
    
    Args:
        shape: The shape of the layer.&quot;&quot;&quot;
       
    def __init__(self, shape):
        super().__init__()
        self.weights = nn.Parameter(torch.empty(*shape))
        self.bias = nn.Parameter(torch.empty(*shape))
           
    def forward(self, x):
        return x * self.weights

    
class ConvLSTMCell(nn.Module):
    &quot;&quot;&quot;A convolutional LSTM cell.

    Implementation details follow closely the following paper:

    Shi et al. -'Convolutional LSTM Network: A Machine Learning 
    Approach for Precipitation Nowcasting' (2015).
    Accessible at https://arxiv.org/abs/1506.04214

    The parameter names are drawn from the paper's Eq. 3.

    Args:
        input_bands: The number of bands in the input data.
        input_dim: The length of of side of input data. Data is
            presumed to have identical width and heigth.&quot;&quot;&quot;

    def __init__(self, input_bands, input_dim,  kernels, dropout, batch_norm):
        super().__init__()

        self.input_bands = input_bands
        self.input_dim = input_dim
        self.kernels = kernels
        self.dropout = dropout
        self.batch_norm = batch_norm

        self.kernel_size = 3
        self.padding = 1  # Preserve dimensions

        self.input_conv_params = {
            'in_channels': self.input_bands,
            'out_channels': self.kernels,
            'kernel_size': self.kernel_size,
            'padding': self.padding,
            'bias': True
        }

        self.hidden_conv_params = {
            'in_channels': self.kernels,
            'out_channels': self.kernels,
            'kernel_size': self.kernel_size,
            'padding': self.padding,
            'bias': True
        }

        self.state_shape = (
            1,
            self.kernels,
            self.input_dim,
            self.input_dim
        )

        self.batch_norm_layer = None
        if self.batch_norm:
            self.batch_norm_layer = nn.BatchNorm2d(num_features=self.input_bands)

        # Input Gates
        self.W_xi = nn.Conv2d(**self.input_conv_params)
        self.W_hi = nn.Conv2d(**self.hidden_conv_params)
        self.W_ci = HadamardProduct(self.state_shape)

        # Forget Gates
        self.W_xf = nn.Conv2d(**self.input_conv_params)
        self.W_hf = nn.Conv2d(**self.hidden_conv_params)
        self.W_cf = HadamardProduct(self.state_shape)

        # Memory Gates
        self.W_xc = nn.Conv2d(**self.input_conv_params)
        self.W_hc = nn.Conv2d(**self.hidden_conv_params)

        # Output Gates
        self.W_xo = nn.Conv2d(**self.input_conv_params)
        self.W_ho = nn.Conv2d(**self.hidden_conv_params)
        self.W_co = HadamardProduct(self.state_shape)

        # Dropouts
        self.H_drop = nn.Dropout2d(p=self.dropout)
        self.C_drop = nn.Dropout2d(p=self.dropout)

        self.apply(initialize_weights)
class ConvLSTM(nn.Module):

    def __init__(self, input_bands, input_dim, kernels, num_layers, bidirectional, dropout):
        super().__init__()
        self.input_bands = input_bands
        self.input_dim = input_dim
        self.kernels = kernels
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.dropout = dropout
        
        self.layers_fwd = self.initialize_layers()
        self.layers_bwd = None
        if self.bidirectional:
            self.layers_bwd = self.initialize_layers()
        self.fc_output = nn.Sequential(
            nn.Flatten(),
            nn.Linear(
                in_features=self.kernels*self.input_dim**2*(1 if not self.bidirectional else 2), 
                out_features=1024
            ),
            nn.Linear(
                in_features=1024, 
                out_features=1
            )
        )
            
        self.apply(initialize_weights)
        
    def initialize_layers(self):
        &quot;&quot;&quot;Initialize a single direction of the model's layers.
        
        This function takes care of stacking layers, allocating
        dropout and assigning correct input feature number for
        each layer in the stack.&quot;&quot;&quot;
        
        layers = nn.ModuleList()
        
        for i in range(self.num_layers):
            layers.append(
                ConvLSTMCell(
                    input_bands=self.input_bands if i == 0 else self.kernels, 
                    input_dim=self.input_dim,
                    dropout=self.dropout if i+1 &lt; self.num_layers else 0,
                    kernels=self.kernels,
                    batch_norm=False
                )
            )
            
        return layers
    
        
    def forward(self, x):
        &quot;&quot;&quot;Perform forward pass with the model.
        
        For each item in the sequence, the data is propagated 
        through each layer and both directions, if possible.
        In case of a bidirectional model, the outputs are 
        concatenated from both directions. The output of the 
        last item of the sequence is further given to the FC
        layers to produce the final batch of predictions. 
        
        Args:
            x:  A batch of spatial data sequences. The data
                should be in the following format:
                [Batch, Seq, Band, Dim, Dim]
                    
        Returns:
            A batch of predictions.&quot;&quot;&quot;
        
        seq_len = x.shape[1]
        
        for seq_idx in range(seq_len):
            
            layer_in_out = x[:,seq_idx,::]
            states = None
            for layer in self.layers_fwd:
                layer_in_out, states = layer(layer_in_out, states)
                
            if not self.bidirectional:
                continue
                
            layer_in_out_bwd = x[:,-seq_idx,::]
            states = None
            for layer in self.layers_bwd:
                layer_in_out_bwd, states = layer(layer_in_out_bwd, states)
            
            layer_in_out = torch.cat((layer_in_out,layer_in_out_bwd),dim=1)
            

      return self.fc_output(layer_in_out)
</code></pre>
<p>I found this in the Pytorch forum <a href=""https://discuss.pytorch.org/t/passing-hidden-layers-to-convlstm/52814"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/passing-hidden-layers-to-convlstm/52814</a>
This is the implementation I found for Bi-Directional Conv LSTM in Pytorch. After going through the code I was stuck trying to test it but it seems some of the functions are either wrongly defined or incomplete. I found other implementations also for Conv LSTM here <a href=""https://github.com/ndrplz/ConvLSTM_pytorch"" rel=""nofollow noreferrer"">https://github.com/ndrplz/ConvLSTM_pytorch</a> but this doesn't support Bi directional. I need some help regrading the above code.</p>
<pre><code>ConvLSTM2D = ConvLSTM(128,128,3,1,True,0.0)
x = torch.randn([5,1,128,224,224])
t1 = ConvLSTM2D(x)

print(t1)
</code></pre>
<p><strong><code>TypeError: initialize_weights() missing 1 required positional argument: 'layer'</code></strong></p>
",1014,0,0,5,pytorch;lstm;recurrent-neural-network;bilstm;sequence-modeling,2021-07-19 21:06:43,2021-07-19 21:06:43,2022-06-11 20:07:54,typeerror  initialize_weights   missing  required positional argument   layer 
261,261,19303916,72555192,How to generate passwords using PassGAN,"<p>Sorry if this question is very basic and simple, I'm a beginner at programming and especially machine learning.</p>
<p>I'm trying to evaluate the performance of the PassGAN AI by having it generate passwords, and then I compare them to a testing list that contains around a million passwords and see how many matches I get.</p>
<p>I have managed to train the algorithm, but I'm not sure how to get it to generate a password file with generated passwords.</p>
<p>Link to the GitHub source of PassGAN: <a href=""https://github.com/d4ichi/PassGAN"" rel=""nofollow noreferrer"">https://github.com/d4ichi/PassGAN</a>
<br/>
Training &amp; Testing file: <a href=""https://github.com/d4ichi/PassGAN/releases/download/data/rockyou-test.txt"" rel=""nofollow noreferrer"">https://github.com/d4ichi/PassGAN/releases/download/data/rockyou-test.txt</a></p>
<p>*Note: I did have to modify some of the code, and downgrade the TensorFlow version to get it to work.
<br/>
I simply replaced all:
<br/>
<code>import tensorflow as tf</code>
<br/>
with
<br/>
<code>import tensorflow.compat.v1 as tf</code>
<br/>
<code>tf.disable_v2_behavior()</code>
<br/>
in every .py file.</p>
<p>I'd really appreciate any help on this.</p>
",29,0,0,5,python;tensorflow;machine-learning;artificial-intelligence;generative-adversarial-network,2022-06-09 11:09:21,2022-06-09 11:09:21,2022-06-11 14:25:20,sorry if this question is very basic and simple  i m a beginner at programming and especially machine learning  i m trying to evaluate the performance of the passgan ai by having it generate passwords  and then i compare them to a testing list that contains around a million passwords and see how many matches i get  i have managed to train the algorithm  but i m not sure how to get it to generate a password file with generated passwords  i d really appreciate any help on this 
262,262,19122949,72582267,ImportError: cannot import name &#39;saveable_objects_from_trackable&#39; from &#39;tensorflow.python.training.saving.saveable_object_util&#39;,"<p>I can't import tensrflow:</p>
<pre><code>import tensorflow as tf
</code></pre>
<p>is responded by:</p>
<pre><code>ImportError: cannot import name 'saveable_objects_from_trackable' from 'tensorflow.python.training.saving.saveable_object_util' (C:\Users\Lior\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\saving\saveable_object_util.py)
</code></pre>
<p>I had a working tf so how did I accidently came here? I tried to fix the problem of</p>
<p><a href=""https://stackoverflow.com/questions/58956619/tensorflow-2-0-list-physical-devices-doesnt-detect-my-gpu"">Tensorflow 2.0 list_physical_devices doesn&#39;t detect my GPU</a></p>
<p>one of the things I did was</p>
<pre><code>rm -rf anaconda_folder
</code></pre>
<p>Becose I thought that how you need to remove your tf before install it again with GPU detection.</p>
<p>Sagnificant remark about my tf version:</p>
<pre><code>Name: tensorflow
Version: 2.8.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\lior\appdata\roaming\python\python39\site-packages
Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, tf-estimator-nightly, typing-extensions, wrapt
Required-by:
</code></pre>
",14,0,0,2,python;tensorflow2,2022-06-11 12:13:00,2022-06-11 12:13:00,2022-06-11 12:13:00,i can t import tensrflow  is responded by  i had a working tf so how did i accidently came here  i tried to fix the problem of  one of the things i did was becose i thought that how you need to remove your tf before install it again with gpu detection  sagnificant remark about my tf version 
263,263,19122949,72579705,tensorflow 2.8 list physical devices doesn&#39;t detect my gpu,"<p>I have exactly the problem that described in:</p>
<p><a href=""https://stackoverflow.com/questions/58956619/tensorflow-2-0-list-physical-devices-doesnt-detect-my-gpu"">Tensorflow 2.0 list_physical_devices doesn&#39;t detect my GPU</a></p>
<p>I have tried the solution</p>
<pre><code>pip3 install --upgrade tensorflow-gpu
</code></pre>
<p>But It didn't solved it, the test func:</p>
<pre><code>physical_devices = tf.config.experimental.list_physical_devices('GPU')
print(physical_devices)
if physical_devices:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
</code></pre>
<p>still gave empty list.</p>
<p>When I runed this pip3 command it said to me:</p>
<pre><code>WARNING: Ignoring invalid distribution -rotobuf (c:\users\lior\appdata\roaming\python\python39\site-packages)
</code></pre>
<p>About 6 times. And also:</p>
<pre><code>WARNING: There was an error checking the latest version of pip.
</code></pre>
<p>I am afraid to remove tensorflow and reinstall is with the GPU, as he recommended secondly. I think I will not be able to install it again. Do you recommend me to try it?</p>
<p>I asked google how to remove my temsorflow. It said me to erase the anconda folder by</p>
<pre><code>rm -rf anaconda_folder
</code></pre>
<p>and now I can't import tensorflow becose it say to me:</p>
<pre><code>ImportError: cannot import name 'saveable_objects_from_trackable' from 'tensorflow.python.training.saving.saveable_object_util' (C:\Users\Lior\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\saving\saveable_object_util.py)
</code></pre>
<p>Another sagnificant remark is I was mistake my tf version, I have 2.8.0 and not 2.0:</p>
<pre><code>Name: tensorflow
Version: 2.8.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\lior\appdata\roaming\python\python39\site-packages
Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, tf-estimator-nightly, typing-extensions, wrapt
Required-by:
</code></pre>
",27,0,0,2,python;tensorflow,2022-06-11 02:21:10,2022-06-11 02:21:10,2022-06-11 11:57:43,i have exactly the problem that described in   i have tried the solution but it didn t solved it  the test func  still gave empty list  when i runed this pip command it said to me  about  times  and also  i am afraid to remove tensorflow and reinstall is with the gpu  as he recommended secondly  i think i will not be able to install it again  do you recommend me to try it  i asked google how to remove my temsorflow  it said me to erase the anconda folder by and now i can t import tensorflow becose it say to me  another sagnificant remark is i was mistake my tf version  i have    and not   
264,264,19316763,72580044,Not all rows are imported to MySQL workbench from CSV file?,"<p>I’m a new SQL learner and I’m having problems while trying to import CSV file into MySQL Workbench.</p>
<p>The CSV file has 541909 Rows but when it is imported into MySQL, only around 35000 rows are imported and I knew the number difference by using the COUNT command.</p>
<p>No error is shown during the import process and I even changed to 1000000 limit rows in the preference setting. I’m using MacBook.</p>
<p>Below is my table query and link to my CSV file</p>
<p><a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/00352/"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/machine-learning-databases/00352/</a></p>
<p>(Original file is xlsx but I changed it to a CSV file)</p>
<p>Link for the cdv is
<a href=""https://docs.google.com/spreadsheets/d/1EyZ9JrlsXcAxQz5BdAXTDVYjSLy25ZWOE1fpv0jDjxM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1EyZ9JrlsXcAxQz5BdAXTDVYjSLy25ZWOE1fpv0jDjxM/edit?usp=sharing</a></p>
<p>yet it only imports 76318 rows out of over 500,000 rows. That's why I am stuck. Here is csv link docs.google.com/spreadsheets/d/</p>
<p>Any idea of resolving this issue is highly appreciated. Thank you in advance.</p>
<p>SQL query:</p>
<pre><code>CREATE TABLE Online_Retail (
    InvoiceNo   VARCHAR(100),
    Stock_Code  VARCHAR(50),
    Description TEXT,
    Quantity DECIMAL,
    Invoice_Date DATE,
    Unit_Price  DECIMAL,
    Customer_ID VARCHAR(50),
    Country VARCHAR(100)
);
</code></pre>
",37,0,0,5,mysql;sql;csv;mysql-workbench;import-csv,2022-06-11 03:05:47,2022-06-11 03:05:47,2022-06-11 08:28:45,i m a new sql learner and i m having problems while trying to import csv file into mysql workbench  the csv file has  rows but when it is imported into mysql  only around  rows are imported and i knew the number difference by using the count command  no error is shown during the import process and i even changed to  limit rows in the preference setting  i m using macbook  below is my table query and link to my csv file   original file is xlsx but i changed it to a csv file  yet it only imports  rows out of over   rows  that s why i am stuck  here is csv link docs google com spreadsheets d  any idea of resolving this issue is highly appreciated  thank you in advance  sql query 
265,265,11547698,72569340,How do you design an LSTM to recognize images after extracting features with a CNN?,"<p>I am creating a captcha image recognition system. It first extracts the features of the images with ResNet and then uses LSTM to recognize the words and letter in the image. An fc layer is supposed to connect the two. I have not designed a LSTM model before and am very new to machine learning, so I am pretty confused and overwhelmed by this.</p>
<p>I am confused enough that I am not even totally sure what questions I should ask. But here are a couple things that stand out to me:</p>
<ul>
<li>What is the purpose of embedding the captions if the captcha images are all randomized?</li>
<li>Is the linear fc layer in the first part of the for loop the correct way to connect the CNN feature vectors to the LSTM?</li>
<li>Is this a correct use of the LSTM cell in the LSTM?</li>
</ul>
<p>And, in general, if there are any suggestions of general directions to look into, that would be really appreciated.</p>
<p>So far, I have:</p>
<pre><code>class LSTM(nn.Module):
    def __init__(self, cnn_dim, hidden_size, vocab_size, num_layers=1):
        super(LSTM, self).__init__()
    
        self.cnn_dim = cnn_dim      #i think this is the input size
        self.hidden_size = hidden_size   
        self.vocab_size = vocab_size      #i think this should be the output size
        
        # Building your LSTM cell
        self.lstm_cell = nn.LSTMCell(input_size=self.vocab_size, hidden_size=hidden_size)
    

        '''Connect CNN model to LSTM model'''
        # output fully connected layer
        # CNN does not necessarily need the FCC layers, in this example it is just extracting the features, that gets set to the LSTM which does the actual processing of the features
        self.fc_in = nn.Linear(cnn_dim, vocab_size)   #this takes the input from the CNN takes the features from the cnn              #cnn_dim = 512, hidden_size = 128
        self.fc_out = nn.Linear(hidden_size, vocab_size)     # this is the looper in the LSTM           #I think this is correct?
    
        # embedding layer
        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.vocab_size)
    
        # activations
        self.softmax = nn.Softmax(dim=1)


    def forward(self, features, captions): 
        
        #features: extracted features from ResNet
        #captions: label of images

        batch_size = features.size(0)
        cnn_dim = features.size(1)
        
        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()  # Initialize hidden state with zeros
        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()   # Initialize cell state with zeros
        
        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()   
        captions_embed = self.embed(captions)  
        
        '''Design LSTM model for captcha image recognition'''
        # Pass the caption word by word for each time step
        # It receives an input(x), makes an output(y), and receives this output as an input again recurrently
        '''Defined hidden state, cell state, outputs, embedded captions'''

        # can be designed to be word by word or character by character

        for t in range(captions).size(1):
            # for the first time step the input is the feature vector
            if t == 0:
                # probably have to get the output from the ResNet layer
                # use the LSTM cells in here i presume

                x = self.fc_in(features)
                hidden_state, cell_state = self.lstm_cell(x[t], (hidden_state, cell_state))
                x = self.fc_out(hidden_state)
                outputs.append(hidden_state)

            # for the 2nd+ time steps
            else:
                hidden_state, cell_state = self.lstm_cell(x[t], (hidden_state, cell_state))
                x = self.fc_out(hidden_state)
                outputs.append(hidden_state)
                

            # build the output tensor
            outputs = torch.stack(outputs,dim=0)
    
        return outputs
</code></pre>
",101,1,0,5,python;pytorch;conv-neural-network;lstm;captcha,2022-06-10 10:40:19,2022-06-10 10:40:19,2022-06-11 07:39:23,i am creating a captcha image recognition system  it first extracts the features of the images with resnet and then uses lstm to recognize the words and letter in the image  an fc layer is supposed to connect the two  i have not designed a lstm model before and am very new to machine learning  so i am pretty confused and overwhelmed by this  i am confused enough that i am not even totally sure what questions i should ask  but here are a couple things that stand out to me  and  in general  if there are any suggestions of general directions to look into  that would be really appreciated  so far  i have 
266,266,16837511,72579927,dataset for ddos in ICN,"<p>I'm currently working on a project related to ddos attacks in ICN networks using machine learning, however, ICN doesn't has a publicly available dataset like in SDN so I'm trying to create my own dataset using the network traffic.
my question is, what is the ratio between the positive records and the negative ones. I can see that some datasets has an equal number of them and others have the positive more than the negative. so which one is correct?</p>
<p>if I apply different attack rates in the network, do I have to create a new dataset?</p>
",11,0,-2,2,machine-learning;dataset,2022-06-11 02:47:37,2022-06-11 02:47:37,2022-06-11 02:47:37,if i apply different attack rates in the network  do i have to create a new dataset 
267,267,6317874,72495715,How do I convert a winrt::Microsoft::AI::MachineLearning::TensorFloat type back to ID3D12Resource,"<p>I am loading an image to the GPU by leveraging <code>ID3D12Resource</code> type. And I found some documentation on how to convert the <code>ID3D12Resource</code> to a <code>Microsoft::AI::MachineLearning::TensorFloat</code> in the <a href=""https://github.com/microsoft/Windows-Machine-Learning/blob/addb76a2ed4757bcf6f1114a007b8df168e42914/Samples/CustomTensorization/CustomTensorization/TensorConvertor.cpp#L109"" rel=""nofollow noreferrer"">Microsoft documentation</a>, but I can't seem to find how to revert it back to <code>ID3D12Resource</code>.</p>
<p>I am following the Machine Learning samples from Microsoft documentation, and the output from the model evaluation is a <code>Microsoft::AI::MachineLearning::TensorFloat</code> type:</p>
<p><a href=""https://github.com/microsoft/Windows-Machine-Learning/blob/e901cc2791f4f3d22e2382e779996bad49173718/Samples/AdapterSelection/AdapterSelection/cpp/main.cpp#L110"" rel=""nofollow noreferrer"">https://github.com/microsoft/Windows-Machine-Learning/blob/e901cc2791f4f3d22e2382e779996bad49173718/Samples/AdapterSelection/AdapterSelection/cpp/main.cpp#L110</a></p>
<p>How do I take the model output that is a <code>TensorFloat</code> type and convert it back to <code>ID3D12Resource</code>. Ideally, I would like to override the content from the original <code>ID3D12Resource</code> input with the return of the model.</p>
<p>Thank you for your help :)</p>
",34,1,1,4,windows;machine-learning;directx-12;windows-machine-learning,2022-06-04 03:59:04,2022-06-04 03:59:04,2022-06-11 02:17:38,i am loading an image to the gpu by leveraging iddresource type  and i found some documentation on how to convert the iddresource to a microsoft  ai  machinelearning  tensorfloat in the   but i can t seem to find how to revert it back to iddresource  i am following the machine learning samples from microsoft documentation  and the output from the model evaluation is a microsoft  ai  machinelearning  tensorfloat type   how do i take the model output that is a tensorfloat type and convert it back to iddresource  ideally  i would like to override the content from the original iddresource input with the return of the model  thank you for your help   
268,268,1833945,35383922,Why this Hadoop example that use Combiner class can&#39;t work properly? (don&#39;t perform the &quot;local reduction&quot; provided by the Combiner),"<p>I am absolutely new in Hadoop and I am doing some experiment trying to use the <strong>Combiner</strong> class to perform the reduce operation locally on the same node of the mapper. I am using Hadoop 1.2.1.</p>
<p>So I have these 3 classes:</p>
<p><strong>WordCountWithCombiner.java</strong>:</p>
<pre><code>// Learning MapReduce by Nitesh Jain
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;

/* 
 * Extend Configured class: g
 * Implement Tool interface:
 * 
 */
public class WordCountWithCombiner extends Configured implements Tool{

  @Override
  public int run(String[] args) throws Exception {
    Configuration conf = getConf(); 
    
    Job job = new Job(conf, &quot;MyJob&quot;);   // Job is a &quot;dashboard&quot; with levers to control the execution of the job
    
    job.setJarByClass(WordCountWithCombiner.class);             // Name of the driver class into the jar
    job.setJobName(&quot;Word Count With Combiners&quot;);    // Set the name of the job

    FileInputFormat.addInputPath(job, new Path(args[0]));           // The input file is the first paramether of the main() method
    FileOutputFormat.setOutputPath(job, new Path(args[1]));         // The output file is the second paramether of the main() method
    
    job.setMapperClass(WordCountMapper.class);          // Set the mapper class
    
    /* Set the combiner: the combiner is a reducer performed locally on the same mapper node (we are resusing the previous WordCountReduces
     * class because it perform the same task, but locally to the mapper):
     */
    job.setCombinerClass(WordCountReducer.class);
    job.setReducerClass(WordCountReducer.class);        // Set the reducer class

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    
    return job.waitForCompletion(true) ? 0 : 1;
   
   }
  
  public static void main(String[] args) throws Exception {
    /* The ToolRunner object is used to trigger the run() function which contains all the batch execution logic. 
     * What it does is gie the ability to set properties at the own time so we need not to write a single line of code to handle it
     */
    int exitCode = ToolRunner.run(new Configuration(), new WordCountWithCombiner(), args);
    System.exit(exitCode);
}

}
</code></pre>
<p><strong>WordCountMapper.java</strong>:</p>
<pre><code>// Learning MapReduce by Nitesh J.
// Word Count Mapper. 
import java.io.IOException;
import java.util.StringTokenizer;

// Import KEY AND VALUES DATATYPE:
import org.apache.hadoop.io.IntWritable;    // Similiar to Int
import org.apache.hadoop.io.LongWritable;   // Similar to Long
import org.apache.hadoop.io.Text;           // Similar to String

import org.apache.hadoop.mapreduce.Mapper;

/* Every mapper class extend the Hadoop Mapper class.
 * @param input key (the progressive number)
 * @param input type (it is a word so something like a String)
 * @param output key
 * @param output value
 * 
 */
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();

  /* Override the map() function defined by the Mapper extended class:
   * The input parameter have to match with these defined into the extended Mapper class
   * @param context: is used to cast the output key and value paired.
   * 
   * Tokenize the string into words and write these words into the context with words as key, and one (1) as value for each word
   */
  @Override
  public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    
      
      String line = value.toString();
      StringTokenizer itr = new StringTokenizer(line);
    
      while (itr.hasMoreTokens()) {
          //just added the below line to convert everything to lower case 
          word.set(itr.nextToken().toLowerCase());
          // the following check is that the word starts with an alphabet. 
          if(Character.isAlphabetic((word.toString().charAt(0)))){
              context.write(word, one);
          }
    }
  }

}
</code></pre>
<p><strong>WordCountReducer.java</strong>:</p>
<pre><code>// Learning MapReduce by Nitesh Jain
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

/* Every reduceer calss have to extender the Hadoop Reducer class
 * @param the mapper output key  (text, the word)
 * @param the mapper output value (the number of occurrence of the related word: 1)
 * @param the redurcer output key (the word)
 * @param the reducer output value (the number of occurrence of the related word)
 * Have to map the Mapper() param
 */
public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    
    /*
     * I have to override the reduce() function defined by the extended Reducer class
     * @param key: the current word
     * @param Iterable&lt;IntWritable&gt; values: because the input of the recudce() function is a key and a list of values associated to this key
     * @param context: collects the output &lt;key, values&gt; pairs
     */
    @Override
    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
        throws IOException, InterruptedException {
        
        int sum = 0;
        for (IntWritable value : values) {
          sum += value.get();
        }
        context.write(key, new IntWritable(sum));
      }

}
</code></pre>
<p>So as you can see into the <strong>WordCountWithCombiner</strong> driver class I have set the <strong>WordCountReducer</strong> class as combiner to perform a reduction directly on the mapper node, by this line:</p>
<pre><code>job.setCombinerClass(WordCountReducer.class);
</code></pre>
<p>Then I have this input file on the Hadoop File System:</p>
<pre><code>andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ hadoop fs -cat  in
to be or not to be
</code></pre>
<p>And I want to operate on it.</p>
<p>If I perform the previous batch in the classical way passing through the 2 phased of <strong>map</strong> and <strong>reduce</strong> it works fine, in fact performing this statement into the Linux shell:</p>
<pre><code>andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ hadoop jar WordCount.jar WordCountWithCombiner in out6
</code></pre>
<p>Hadoop do it works and then I obtain the expected result:</p>
<pre><code>andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ hadoop fs -cat  out6/p*
be  2
not 1
or  1
to  2
andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ 
</code></pre>
<p>Ok, it works fine.</p>
<p>The problem is that now I want don't perform the <strong>reduce</strong> phase and I expect the same result because I have setted the combiner that do the same thing on the same node of the reducer.</p>
<p>So, into the Linux shell I perform this statement that exclude the reducer phase:</p>
<pre><code>hadoop jar WordCountWithCombiner.jar WordCountWithCombiner -D mapred.reduce.tasks=0 in out7
</code></pre>
<p>But it don't works fine because this is what I obtain (I post the entire output to add more information about what is happening):</p>
<pre><code>andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ hadoop jar WordCountWithCombiner.jar WordCountWithCombiner -D mapred.reduce.tasks=0 in out7
16/02/13 19:43:44 INFO input.FileInputFormat: Total input paths to process : 1
16/02/13 19:43:44 INFO util.NativeCodeLoader: Loaded the native-hadoop library
16/02/13 19:43:44 WARN snappy.LoadSnappy: Snappy native library not loaded
16/02/13 19:43:45 INFO mapred.JobClient: Running job: job_201601242121_0008
16/02/13 19:43:46 INFO mapred.JobClient:  map 0% reduce 0%
16/02/13 19:44:00 INFO mapred.JobClient:  map 100% reduce 0%
16/02/13 19:44:05 INFO mapred.JobClient: Job complete: job_201601242121_0008
16/02/13 19:44:05 INFO mapred.JobClient: Counters: 19
16/02/13 19:44:05 INFO mapred.JobClient:   Job Counters 
16/02/13 19:44:05 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=18645
16/02/13 19:44:05 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
16/02/13 19:44:05 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
16/02/13 19:44:05 INFO mapred.JobClient:     Launched map tasks=1
16/02/13 19:44:05 INFO mapred.JobClient:     Data-local map tasks=1
16/02/13 19:44:05 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0
16/02/13 19:44:05 INFO mapred.JobClient:   File Output Format Counters 
16/02/13 19:44:05 INFO mapred.JobClient:     Bytes Written=31
16/02/13 19:44:05 INFO mapred.JobClient:   FileSystemCounters
16/02/13 19:44:05 INFO mapred.JobClient:     HDFS_BYTES_READ=120
16/02/13 19:44:05 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=55503
16/02/13 19:44:05 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=31
16/02/13 19:44:05 INFO mapred.JobClient:   File Input Format Counters 
16/02/13 19:44:05 INFO mapred.JobClient:     Bytes Read=19
16/02/13 19:44:05 INFO mapred.JobClient:   Map-Reduce Framework
16/02/13 19:44:05 INFO mapred.JobClient:     Map input records=1
16/02/13 19:44:05 INFO mapred.JobClient:     Physical memory (bytes) snapshot=93282304
16/02/13 19:44:05 INFO mapred.JobClient:     Spilled Records=0
16/02/13 19:44:05 INFO mapred.JobClient:     CPU time spent (ms)=2870
16/02/13 19:44:05 INFO mapred.JobClient:     Total committed heap usage (bytes)=58195968
16/02/13 19:44:05 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=682741760
16/02/13 19:44:05 INFO mapred.JobClient:     Map output records=6
16/02/13 19:44:05 INFO mapred.JobClient:     SPLIT_RAW_BYTES=101
andrea@andrea-virtual-machine:~/workspace/HadoopExperiment/bin$ hadoop fs -cat  out7/p*to   1
be  1
or  1
not 1
to  1
be  1
</code></pre>
<p>So as you can see it seems that the local reduction provided by the <strong>Combiner</strong> doesn't work.</p>
<p>Why? What am I missing? How can I try to solve this issue?</p>
",1405,1,0,3,java;hadoop;bigdata,2016-02-14 00:18:54,2016-02-14 00:18:54,2022-06-11 01:42:35,i am absolutely new in hadoop and i am doing some experiment trying to use the combiner class to perform the reduce operation locally on the same node of the mapper  i am using hadoop     so i have these  classes  wordcountwithcombiner java  wordcountmapper java  wordcountreducer java  so as you can see into the wordcountwithcombiner driver class i have set the wordcountreducer class as combiner to perform a reduction directly on the mapper node  by this line  then i have this input file on the hadoop file system  and i want to operate on it  if i perform the previous batch in the classical way passing through the  phased of map and reduce it works fine  in fact performing this statement into the linux shell  hadoop do it works and then i obtain the expected result  ok  it works fine  the problem is that now i want don t perform the reduce phase and i expect the same result because i have setted the combiner that do the same thing on the same node of the reducer  so  into the linux shell i perform this statement that exclude the reducer phase  but it don t works fine because this is what i obtain  i post the entire output to add more information about what is happening   so as you can see it seems that the local reduction provided by the combiner doesn t work  why  what am i missing  how can i try to solve this issue 
269,269,18184561,72579399,Is there a way in R to build an LSTM model for regression using numeric and factor predictors?,"<p>I am trying to build an LSTM model to make predictions on Y in my dataset below using X1..i. There are so many examples online to build LSTM models but I am not an expert in machine learning. I just use it for my work and especially because there are comments in X3 that have very important information. Is it possible to help me build an LSTM model using R? I would really appreciate that help. My sample dataset is below:</p>
<pre><code>data&lt;- structure(list(X1 = c(1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L), X2 = c(0, 
4.221288681, 0, 0, 0, 0, 0, 0), X3 = c(&quot;There wear three events at this day&quot;, 
&quot;10 balls were used in this game&quot;, &quot;one ball was used in this game&quot;, 
&quot;the referee was injured during this game&quot;, &quot;all the audience left right after the game &quot;, 
&quot;this game was held on Madrid stadium &quot;, &quot;The stadium was full&quot;, 
&quot;65000 people attended the game&quot;), Y = c(14L, 120L, 59L, 24L, 
27L, 41L, 68L, 9L)), class = &quot;data.frame&quot;, row.names = c(NA, 
-8L))
</code></pre>
<p>I also have less than 1000 observations, do you think LSTM is going to be an efficient algorithm in this scenario?</p>
",29,0,-1,4,r;machine-learning;neural-network;lstm,2022-06-11 01:40:28,2022-06-11 01:40:28,2022-06-11 01:40:28,i am trying to build an lstm model to make predictions on y in my dataset below using x  i  there are so many examples online to build lstm models but i am not an expert in machine learning  i just use it for my work and especially because there are comments in x that have very important information  is it possible to help me build an lstm model using r  i would really appreciate that help  my sample dataset is below  i also have less than  observations  do you think lstm is going to be an efficient algorithm in this scenario 
270,270,596985,72578668,How to define and validate a folder schema?,"<p>I need to validate the structure of a large number of 3rd party datasets. The datasets are a collection of files used in Machine Learning, and each have their own structure.</p>
<p>For example, one image dataset keeps the annotation next to the image file, like so:</p>
<pre><code>/item1.jpeg
/item1.json
...
</code></pre>
<p>and another one uses a different structure:</p>
<pre><code>/item1/image.jpeg
/item1/annotation.json
...
</code></pre>
<p>Instead of writing individual validators for each folder schema, I am trying to find a generic way to define each folder structure, and then validate them against such definitions.</p>
<p>I believe building such a tool is rather straightforward: I can define the folder schema using something like JSON Schema or Avro, convert the folder structure to JSON, and validate it agains the given schema. But before doing that, I want to know if such a tool already exists, or if there are better approaches, tools, libraries, or languages for defining and validating generic folder structures.</p>
",28,0,-1,5,python;machine-learning;filesystems;schema;jsonschema,2022-06-11 00:15:57,2022-06-11 00:15:57,2022-06-11 01:23:24,i need to validate the structure of a large number of rd party datasets  the datasets are a collection of files used in machine learning  and each have their own structure  for example  one image dataset keeps the annotation next to the image file  like so  and another one uses a different structure  instead of writing individual validators for each folder schema  i am trying to find a generic way to define each folder structure  and then validate them against such definitions  i believe building such a tool is rather straightforward  i can define the folder schema using something like json schema or avro  convert the folder structure to json  and validate it agains the given schema  but before doing that  i want to know if such a tool already exists  or if there are better approaches  tools  libraries  or languages for defining and validating generic folder structures 
271,271,14090897,72578091,Making a Graph Neural Network to Identify Patterns in Dataset,"<p>The project is focused on creating an internal “Graph technology” Machine Learning algorithm that goes above and beyond simple recommendations like this value should be populated or blank. It involves taking a data set, assessing patterns and values in a similar method to data profiling, and providing rule recommendations.</p>
<p>The algorithm must be able to create new rule recommendations for a data steward. To do this it must be able to identify if a rule already exists or not.</p>
<p>The rules are currently made by data stewards when they talk with business partners and create SKUs and other data information. They also try to create rules by scanning excel and trying to find their own connection with eyes only.</p>
<p>I understand that graph technology is a good way to identify and recognize patterns. As of right now, there are 20,000 datasets in the database, each with a primary key, allowing for them to be joined to each other. So for me to develop an ML algorithm, I’d eventually have to join all of the tables, so it can find patterns between multiple tables and their values.</p>
<p>I was thinking I could create a script that takes in a dataset and finds relationships by:</p>
<p>1)Clustering each table as its own “neighborhood”</p>
<p>2)Then finding and clustering distinct values in each table as its own node.</p>
<p>3)Then iterating through each row in the master table, and drawing a line or “edge” to the next node in the sequence.</p>
<p>4)The algorithm will find which relationships are strong by the number of times a line or edge was created between two nodes. Those with strong relationships will then be possible rule recommendations.</p>
<p>5)The algorithm will then look at the current rule bank, and determine if those rules currently exist. If they do, they will be nullified as possible rule recommendations.</p>
<p>6)Those that are strong AND new recommendations will then be recommended to the data steward.</p>
<p>I have provided a diagram example of how the tables could look, and how a recommendation could be made:</p>
<p><a href=""https://i.stack.imgur.com/aTpif.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aTpif.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/30El9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/30El9.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/hFlVp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hFlVp.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/dCfYr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCfYr.png"" alt=""enter image description here"" /></a></p>
<p>Current Rule Bank:</p>
<p>IF MATERIAL.PRODUCT = M1234 and MATERIAL.TYPE = RED, then FARM.STATE = KY
IF FARM.STATE = TX and FARM.FACTORY = P167, then STORE.TAX_JURI = J500</p>
<p>New Found Rules:</p>
<p>IF STORE.TAX_JURI = J500 and FARM.FACTORY = P167, then FARM.STATE = TX
IF MATERIAL.PRODUCT = M1234 and FARM.STATE = KY, then MATERIAL.TYPE = RED
IF MATERIAL.PRODUCT = M5678 and MATERIAL.TYPE = YELLOW, then STORE.TAX_JURI = J500</p>
<p>Algorithm would compare current rules and new found rules. They would eliminate the first two since they’re the same as the current rules, and recommend the third.</p>
<p>IF MATERIAL.PRODUCT = M5678 and MATERIAL.TYPE = YELLOW, then STORE.TAX_JURI = J500</p>
<p>I know how to code and use python and SQL, and have worked on data science projects before, but never Machine Learning. So this will be new for me.</p>
<p>Any tips on how I can go about doing this. Or if you could give me a roadmap of things to solve to create a finished product.</p>
",12,0,-1,5,python;sql;algorithm;machine-learning;graph,2022-06-10 23:15:09,2022-06-10 23:15:09,2022-06-10 23:15:09,the project is focused on creating an internal  graph technology  machine learning algorithm that goes above and beyond simple recommendations like this value should be populated or blank  it involves taking a data set  assessing patterns and values in a similar method to data profiling  and providing rule recommendations  the algorithm must be able to create new rule recommendations for a data steward  to do this it must be able to identify if a rule already exists or not  the rules are currently made by data stewards when they talk with business partners and create skus and other data information  they also try to create rules by scanning excel and trying to find their own connection with eyes only  i understand that graph technology is a good way to identify and recognize patterns  as of right now  there are   datasets in the database  each with a primary key  allowing for them to be joined to each other  so for me to develop an ml algorithm  i d eventually have to join all of the tables  so it can find patterns between multiple tables and their values  i was thinking i could create a script that takes in a dataset and finds relationships by   clustering each table as its own  neighborhood   then finding and clustering distinct values in each table as its own node   then iterating through each row in the master table  and drawing a line or  edge  to the next node in the sequence   the algorithm will find which relationships are strong by the number of times a line or edge was created between two nodes  those with strong relationships will then be possible rule recommendations   the algorithm will then look at the current rule bank  and determine if those rules currently exist  if they do  they will be nullified as possible rule recommendations   those that are strong and new recommendations will then be recommended to the data steward  i have provided a diagram example of how the tables could look  and how a recommendation could be made      current rule bank  new found rules  algorithm would compare current rules and new found rules  they would eliminate the first two since they re the same as the current rules  and recommend the third  if material product   m and material type   yellow  then store tax_juri   j i know how to code and use python and sql  and have worked on data science projects before  but never machine learning  so this will be new for me  any tips on how i can go about doing this  or if you could give me a roadmap of things to solve to create a finished product 
272,272,18904889,72575379,Extract word and compound word from text in python nlp,"<p>I have difficulties to identify compound words in strings.</p>
<p>I am trying to extract hardskills from a list of job descriptions. For this, I have a list of hardskills like 'python','ux,'ui','r','analytics','collaboration', etc. I then extract each word from job descriptions that is also comprised in my hardskills list.</p>
<p>To process the matchmaking, I need to split words in my job descriptions. The problem is, it also splits compound words that I cannot identify anymore. Like 'Google Data Studio', or 'machine learning'.</p>
<p>My job descriptions are contained in df['Descriptions'] and my skills in a list called 'skills'.</p>
<p>Do you please have a solution for this ?</p>
<p>Here is the code proceeding the matchmarking:</p>
<pre><code>
def count_occurrences(word, sentence):
    return sentence.split().count(word)



keylist = []

for i in df['Descriptions']:
    key = &quot;&quot;
    for word in skills:        
        if word in i:
            n = count_occurrences(word, i)
            if n &gt; 0:
                key += f' {word}'*n
    
    keylist.append(key)

</code></pre>
<p>--&gt; For info, a job description looks like this (in french):</p>
<p>&quot;l activite concernee open data gazette est une solution en mode saas concue pour apporter une reponse concrete aux problematiques de pilotage de la data pour les petites et grandes collectivites et pour etre un facilitateur dans leur demarche de maitrise et d ouverture des donnees
la gazette est une marque du groupe infopro digital qui offre toute une gamme de rubriques pour repondre a leurs besoins et attentes debats sur les enjeux des collectivites retours d experience afin de gerer sa carriere dossiers d actualite mise en avant de projets innovants reglementation juridique le site lagazettedescommunes fr enregistre plus de visites uniques par mois et le magazine rassemble chaque semaine pres de lecteurs description de vos missions au sein de l equipe open data gazette combinant l agilite de la start-up et l assurance d une eti leader dans son secteur vous serez en lien avec les developpeurs de la solution la directrice de projet et le data-journaliste afin de contribuer l enrichissement des indicateurs integres a la solution et a l evolution du process et des fonctionnalites de l outil
l offre de services inclut
une base de donnees composee de indicateurs agreges a l echelle de chaque territoire et permettant la comparaison le partage et la comprehension des donnees de reference du territoire
une plateforme editorialisee de publication des donnees en open data inedite dans son approche
dans le cadre de votre mission vous serez rattache e a la directrice de projet vous serez en charge de
suivre la mise a jour des donnees en open data qui constituent la source de calcul des indicateurs
sourcer les nouvelles donnees disponibles en open data et evaluer leur qualite fichiers ou api
ecrire les briefs a destination des equipes techniques specifications fonctionnelles
contribuer a la construction de nouveaux indicateurs calcul methodologie qui doivent enrichir le service
contribuer a structurer les donnees
votre profil
issu e d une formation en etudes statistiques vous avez l habitude de manier des donnees
dote d un excellent relationnel vous etes autonome facilement adaptable et vous avez un bon sens de l ecoute
force de proposition polyvalent vous aimez relever des challenges et vous savez travailler en multi projets
vous avez une reelle appetence pour les technologies autour de la data <strong>big data</strong> <strong>data lake</strong> <strong>data visualisation</strong> etc et pour l analyse de donnees en masse
si vous vous retrouvez dans ce profil rejoignez-nous des que possible
type d emploi temps plein stage
duree du contrat mois
avantages
participation au transport
horaires
periodes de travail de heures&quot;</p>
<p>--&gt; And the hardskills extracted:
&quot;analyse api data data data data data data data data data data lake statistiques visualisation&quot;</p>
<p>In this example, &quot;data lake&quot; and &quot;data visualisation&quot; are not identified as compound words.
&quot;</p>
",22,0,-1,3,python;nlp;matchmaking,2022-06-10 19:17:01,2022-06-10 19:17:01,2022-06-10 19:17:01,i have difficulties to identify compound words in strings  i am trying to extract hardskills from a list of job descriptions  for this  i have a list of hardskills like  python   ux  ui   r   analytics   collaboration   etc  i then extract each word from job descriptions that is also comprised in my hardskills list  to process the matchmaking  i need to split words in my job descriptions  the problem is  it also splits compound words that i cannot identify anymore  like  google data studio   or  machine learning   my job descriptions are contained in df  descriptions   and my skills in a list called  skills   do you please have a solution for this   here is the code proceeding the matchmarking     gt  for info  a job description looks like this  in french  
273,273,11815954,72574758,Why is the actual value of this variable not being used as the index number of the object I&#39;m trying to return,"<p>This has been driving me bonkers since Monday June 6 2022, I really hope someone can tell me what I am doing wrong.</p>
<p>I have a music machine written (as a hobby/learning experience) in JavaScript using the Web Audio API and I'm trying to do something very simple:- update a variable value via a user interface, then use the new value of the variable to reference an array index.</p>
<p>In English! The user should be able to change the octave via a slider on the UI.</p>
<p>What I have is:</p>
<ul>
<li>a slider to set a value</li>
<li>a variable to hold the slider value</li>
<li>an array of object literals with key/value pairs that hold note frequencies (each object holds an octave)</li>
<li>a programmed keyboard key 'Y'</li>
</ul>
<p>What should happen:</p>
<ul>
<li>press keyboard key 'Y' to play a note</li>
<li>a note should play in a specific octave</li>
<li>moving the slider should change the value of the variable</li>
<li>the value of the variable should be used to reference the index of an array</li>
<li>pressing key 'Y' should play a note in a different octave (or return undefined)</li>
</ul>
<p>What is actually happening:</p>
<ul>
<li>pressing keyboard key 'Y' plays a note</li>
<li>moving the slider changes the value of the variable</li>
<li>pressing key 'Y' again plays the same note in the same octave</li>
<li>the current value of the variable set by the user is not being used to reference the specific array index</li>
</ul>
<p>The code for my music machine is pretty chunky, so I've managed to reproduce the problem I'm having in the code below, I was rather hoping that by doing this I would be able to see what is going wrong, but, alas no! :o(</p>
<p>For the sake of brevity, the array holding the scales has just four indexes, 0 &amp; 1 should return undefined, index 2 should return <code>440</code> and index 2 should return <code>880</code></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>// get the range slider and label from the document
const slider = document.getElementById(""slider"");
const labelVal = document.getElementById(""label"");

// var to hold the value of the slider
// We'll use this as the index value later
let oct = 2;

// add an event listener to the slider
window.addEventListener(""change"", handelChange, false);

// the change event handler
function handelChange(event) {
  // set the value of var oct to the slider value
  oct = parseFloat(slider.value);
  console.log(""oct Value:"", oct);

  // set the label content to equal value of `oct`
  labelVal.innerHTML = oct;

  return oct;
}

// Create the audio context
const actx = new AudioContext();

// function to call `function createOsc(freq)`
function playNote(freq) {
  ceateOsc(freq);
}

// function to create an audio graph that
// starts and stops the oscillator
function ceateOsc(freq) {
  // create the audio nodes
  const osc = actx.createOscillator();
  const vol = actx.createGain();

  // set the nodes' values
  osc.frequency.value = freq;
  vol.gain.value = 0.1;

  // connect the nodes to the audio context destintion
  osc.connect(vol).connect(actx.destination);

  // start &amp; stop the oscillator
  osc.start();
  osc.stop(actx.currentTime + 1);
}

// array of objects holding musical note frequencies
let scale = [{}, // return undefined
  {}, // return undefined
  {
    A: 440, // return 440
  },
  {
    A: 880, // return 880
  },
];

// map keyboard to notes using var `oct`
// for the index number of array `scale`
const notes = {
  // scale[0].A should undefined
  // scale[1].A should undefined
  // scale[2].A should return 440
  // scale[3].A should return 880
  y: scale[oct].A,
};

// ************* Listen For Keyboard Input START ************ \\
window.addEventListener(""keydown"", keydownHandler, false);

function keydownHandler(event) {
  const key = event.key;
  const freq = notes[key];
  console.log(""keydown event: oct val ="", oct, ""frequency ="", freq);
  // if our notes object has this keyboard key defined
  // play the note:
  if (freq) {
    playNote(freq);
  } else {
    console.log(""Only key 'Y' can play a note"");
  }
}
// ************* Listen For Keyboard Input END ************ \\</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;h3&gt;Test To Set Index Via User Interface&lt;/h3&gt;

&lt;p&gt;Press key 'Y' to play note 'A'&lt;/p&gt;
&lt;p&gt;Set slider to value 2 or 3 to change the octave&lt;/p&gt;
&lt;p&gt;
  &lt;!-- Range input: value to be used as the index number --&gt;
  &lt;input type=""range"" id=""slider"" min=""0"" max=""3"" value=""2"" /&gt;&lt;label id=""label"" for=""setOctave""&gt;2&lt;/label
      &gt;
    &lt;/p&gt;</code></pre>
</div>
</div>
</p>
<p>What I've tried:</p>
<ul>
<li>If I manually set the index like this: <code>scale[3].A</code> key 'Y' plays note A @ 880hz</li>
<li>If I manually set the index like this: <code> scale[2].A</code> key 'Y' plays note A @ 440hz</li>
<li>If I manually set the index like this: <code>scale[0].A</code> key 'Y' returns <code>undefined</code></li>
</ul>
<p>Also, if I manually set the initial value of the var <code>oct</code> key 'Y' returns the correct note/octave.</p>
<p>I've included <code>console.log(oct)</code> at various points in the code including the point of <code>keydown</code> and at each point we can see that the value of <code>oct</code> is equal to the value of the slider. In fact, the value of <code>oct</code> is actually being used to update the UI 'slider text value' shown to the user, however, the current value of <code>oct</code> is not being used at this point in  the code <code>scale[oct].A</code></p>
<p>Am I missing something completely obvious here or is there something going on that I am just not aware of?</p>
<p>I much would much appreciate any feedback on this at all.</p>
<p>Thanks.</p>
",27,1,0,4,javascript;arrays;object;web-audio-api,2022-06-10 18:34:38,2022-06-10 18:34:38,2022-06-10 18:41:11,this has been driving me bonkers since monday june    i really hope someone can tell me what i am doing wrong  i have a music machine written  as a hobby learning experience  in javascript using the web audio api and i m trying to do something very simple   update a variable value via a user interface  then use the new value of the variable to reference an array index  in english  the user should be able to change the octave via a slider on the ui  what i have is  what should happen  what is actually happening  the code for my music machine is pretty chunky  so i ve managed to reproduce the problem i m having in the code below  i was rather hoping that by doing this i would be able to see what is going wrong  but  alas no   o  for the sake of brevity  the array holding the scales has just four indexes    amp   should return undefined  index  should return  and index  should return  what i ve tried  also  if i manually set the initial value of the var oct key  y  returns the correct note octave  i ve included console log oct  at various points in the code including the point of keydown and at each point we can see that the value of oct is equal to the value of the slider  in fact  the value of oct is actually being used to update the ui  slider text value  shown to the user  however  the current value of oct is not being used at this point in  the code scale oct  a am i missing something completely obvious here or is there something going on that i am just not aware of  i much would much appreciate any feedback on this at all  thanks 
274,274,12363378,72573963,Negative R-squared value for random forest regression - stock price prediction,"<p>im currently writing my thesis on machine learning in finance. My thesis is based on this paper: <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3159577"" rel=""nofollow noreferrer"">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3159577</a>. The data is from the WRDS CRSP database currently stored in a df. The idea is, that the inital training set contains the first 18 years of data with the next 12 years as test set. After every guess the train year is added to the train set and the model is refitted to guess the next years Returns. Data is one permno per month. Unfortunately I get a close to zero or negative r-squared. I tried hyperparameter tuning with only a few variables as the whole set contains 96 variables, but the results don´t improve the model. Does any one have suggestion what I can do to improve or if I have made a mistake somewhere? Code below:</p>
<pre><code>df2 = df2.reset_index()
df2 = df2.sort_values(['year','month', 'permno'])
df2 = df2.set_index(['permno' , 'year' , 'month'])
df2 = df2.fillna(0)

df2 = df2.drop(['index'], axis = 1)


start , end = 1957 , 1974
x = 1975


r_squared = []
returns = []


while x &lt; 1987:  
    if x == 1987:
        print('done')
        break
    else:        
        df2 = df2.reset_index()
        df2 = df2.sort_values(['year','month', 'permno'])
        
        xtr = df2.loc[(df2['year'] &gt;= 1970) &amp; (df2['year'] &lt;= 1974)]
        xts = df2.loc[df2['year'] == 1975]
        ytr = xtr['RET']
        yts = xts['RET']
        ytr = ytr.fillna(0)
        yts = yts.fillna(0)
        end += 1
        x +=1
       
        xtr, xts = xtr.drop(['RET' ] , axis=1) , xts.drop(['RET' ] , axis = 1)
        xtr, xts = xtr.set_index(['permno', 'year' , 'month']), xts.set_index(['permno' , 'year' , 'month'])
    
        
        mdl = RandomForestRegressor(n_estimators=500, min_samples_split = 2, min_samples_leaf= 1, max_features=0.2, max_depth= 50, bootstrap = False, n_jobs=-1 , warm_start=(True))
        mdl.fit(xtr, ytr)
        
        p = mdl.predict(xts)
        
        error = rsq(yts, p)
        print((error))
        r_squared.append(error)
        print(end)
        print('RSQ = %.5f' % np.mean(r_squared))
        temp = (yts.index.to_frame())
        temp['pred'] = p
        returns.append (temp)

</code></pre>
<p>edit: clarification</p>
",23,0,0,5,python;scikit-learn;regression;random-forest;prediction,2022-06-10 17:29:43,2022-06-10 17:29:43,2022-06-10 17:56:45,im currently writing my thesis on machine learning in finance  my thesis is based on this paper    the data is from the wrds crsp database currently stored in a df  the idea is  that the inital training set contains the first  years of data with the next  years as test set  after every guess the train year is added to the train set and the model is refitted to guess the next years returns  data is one permno per month  unfortunately i get a close to zero or negative r squared  i tried hyperparameter tuning with only a few variables as the whole set contains  variables  but the results don t improve the model  does any one have suggestion what i can do to improve or if i have made a mistake somewhere  code below  edit  clarification
275,275,19135414,72572032,Removing duplicates for ML training set?,"<p>I am wondering what is the common practice (if there is any) for handling duplicate observations for machine learning training sets.</p>
<p>Dropping duplicate observations would surely speed up the computations so that's a benefit.</p>
<p>But would it not throw the model off by simplifying it? Do models take the number of duplicates into account? I have a feeling it depends on the model, but am not able to find a clear answer.</p>
",21,1,-1,3,machine-learning;duplicates;data-fitting,2022-06-10 14:59:07,2022-06-10 14:59:07,2022-06-10 16:15:42,i am wondering what is the common practice  if there is any  for handling duplicate observations for machine learning training sets  dropping duplicate observations would surely speed up the computations so that s a benefit  but would it not throw the model off by simplifying it  do models take the number of duplicates into account  i have a feeling it depends on the model  but am not able to find a clear answer 
276,276,10779494,72562037,Adding input and output parameter to a DatabricksStep - Azure Machine Learning Studio,"<p>We are using AMLS for creating and registering a pipeline which runs on a pre-defined Databricks cluster.
In the AMLS workspace, there is our Databricks notebook which should be executed in the DatabricksStep.</p>
<p>We want to save a file into a Blob-storage container. Therefore, we have added the parameters &quot;outputs&quot; and &quot;notebook_params&quot; to the DatabricksStep:
It is important that we use the DatabricksStep, but irrelevant whether the Notebook (Python script basic_DatabricksStep_script.py) is in AMLS or the Databricks workspace.</p>
<p>How we try to save a file into the Blob storage container:</p>
<p>Here is the Python script, which should be executed in the DatabricksStep</p>
<pre><code>%%writefile $source_directory/basic_DatabricksStep_script.py

dbutils.widgets.get(&quot;input&quot;)
i = getArgument(&quot;input&quot;)
print (&quot;Param -\'input':&quot;)
print (i)

dbutils.widgets.get(&quot;output&quot;)
dbutils.widgets.get(&quot;output&quot;)
o = getArgument(&quot;output&quot;)
print (&quot;Param -\'output':&quot;)
print (o)
data = [('value1', 'value2')]
df2 = spark.createDataFrame(data)

z = o + &quot;/output.txt&quot;
df2.write.csv(z)
</code></pre>
<p>This is how we define the DatabricksStep</p>
<pre><code>def_blob_store = Datastore(ws, &quot;input_datastore&quot;)
step_1_input = DataReference(datastore=def_blob_store, path_on_datastore=&quot;dbtest&quot;,
data_reference_name=&quot;input&quot;)

output_data_folder_name = &quot;output&quot;
output_data_folder = PipelineData(output_data_folder_name, Datastore.get(ws, &quot;output_datastore&quot;))

dbNbWithExistingClusterStep = DatabricksStep(
name=&quot;DBFSReferenceWithExisting&quot;,
run_name='DBFS_Reference_With_Existing',
source_directory = source_directory,
python_script_name = &quot;basic_DatabricksStep_script.py&quot;,
inputs=[step_1_input],
outputs=[output_data_folder],
compute_target=databricks_compute,
existing_cluster_id=&quot;XXXXXX&quot;,
allow_reuse=True,
permit_cluster_restart=True
)
</code></pre>
<p>Here is a picture for making it clearer what we want to achieve:
<a href=""https://i.stack.imgur.com/pzbYY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pzbYY.png"" alt=""enter image description here"" /></a></p>
<p>Currently, our pipeline is not getting built by AMLS even though we followed the examples of the official GitHub notebook for learning about the DatabricksStep class.
Can you make our pipeline work, please?</p>
<p>Thank you in advance for your support!</p>
",49,1,2,3,python;azure-databricks;azure-machine-learning-studio,2022-06-09 19:53:38,2022-06-09 19:53:38,2022-06-10 14:56:21,how we try to save a file into the blob storage container  here is the python script  which should be executed in the databricksstep this is how we define the databricksstep thank you in advance for your support 
277,277,5825268,58910023,KeyError when training a model with pyspark.ml on AWS EMR with data from s3 bucket,"<p>I am training a machine learning model with pyspark.ml on .json data from an s3 bucket on AWS EMR in a JupyterLab notebook. The bucket is not mine, but I think access works fine because data preprocessing, feature engineering etc. works fine. But when I call the <code>cv.fit(training_data)</code> function, the training process runs until it almost finishes (indicated by the status bar), but then throws an error:</p>

<pre><code>Exception in thread cell_monitor-64:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""/opt/conda/lib/python3.7/threading.py"", line 865, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py"", line 178, in cell_monitor
    job_binned_stages[job_id][stage_id] = all_stages[stage_id]
KeyError: 6571
</code></pre>

<p>I could not find any information on this error yet. What is going on? </p>

<p>This is my pipeline:</p>

<pre><code>train, test = clean_df.randomSplit([0.8, 0.2], seed=42)

va1 = VectorAssembler(inputCols=""vars"", outputCol=""vars"")

scaler = StandardScaler(inputCol=""to_scale"", outputCol=""scaled_features"")

va2 = VectorAssembler(inputCols=[""more_vars"",""scaled_features""], outputCol=""features"")

gbt = GBTClassifier()   

pipeline = Pipeline(stages=[va1, scaler,va2,gbt])

paramGrid = ParamGridBuilder()\
    .addGrid(gbt.maxDepth, [2, 5])\
    .addGrid(gbt.maxIter, [10, 100])\
    .build() 

crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),
                          numFolds=3)

cvModel = crossval.fit(train)
</code></pre>

<p>Second, I have a hunch that I might by resolved in Python 3.8; can I install Python 3.8 on EMR? </p>
",1051,2,3,3,apache-spark;pyspark;amazon-emr,2019-11-18 13:18:41,2019-11-18 13:18:41,2022-06-10 14:13:39,i am training a machine learning model with pyspark ml on  json data from an s bucket on aws emr in a jupyterlab notebook  the bucket is not mine  but i think access works fine because data preprocessing  feature engineering etc  works fine  but when i call the cv fit training_data  function  the training process runs until it almost finishes  indicated by the status bar   but then throws an error  i could not find any information on this error yet  what is going on   this is my pipeline  second  i have a hunch that i might by resolved in python    can i install python   on emr  
278,278,12858727,71953397,Signal (chromatograms) processing using Machine Learning,"<p>Lets say I have four raw chromatogram signals and I also have their respected processed signals.</p>
<p>Is it possible to train the Machine learning model to learn signal processing steps? The idea is to have ML model that takes raw signal and outputs processed signals. (Without applying any manual processing steps)</p>
<p><strong>Raw signals and Desired signals</strong></p>
<p><a href=""https://i.stack.imgur.com/5rGyK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5rGyK.png"" alt=""enter image description here"" /></a></p>
",36,0,0,5,python;machine-learning;deep-learning;signal-processing;pattern-recognition,2022-04-21 16:36:23,2022-04-21 16:36:23,2022-06-10 13:39:47,lets say i have four raw chromatogram signals and i also have their respected processed signals  is it possible to train the machine learning model to learn signal processing steps  the idea is to have ml model that takes raw signal and outputs processed signals   without applying any manual processing steps  raw signals and desired signals 
279,279,8102639,72571024,Detect and extract the navigation menu of webpage using Machine Learning,"<p>I'm totally new to machine learning and I am wondering if it is possible to use machine learning to detect and extract the HTML code portion for the navigation menu. The dataset will be the raw html of webpages.</p>
<p>As there are quite a number of ways to develop a navigation menu, I can't just use the nav tag to find the menu.</p>
<p>Is this possible, if it is, do you have any idea on how to go about it?</p>
<p>I saw some online libs that can detect the boilerplate of the webpage but i just want to detect and extract the html code of the navigation menu.</p>
<p>Please help</p>
<p>Thank you:)</p>
",25,0,-2,5,python;html;machine-learning;navbar;navigationbar,2022-06-10 13:39:43,2022-06-10 13:39:43,2022-06-10 13:39:43,i m totally new to machine learning and i am wondering if it is possible to use machine learning to detect and extract the html code portion for the navigation menu  the dataset will be the raw html of webpages  as there are quite a number of ways to develop a navigation menu  i can t just use the nav tag to find the menu  is this possible  if it is  do you have any idea on how to go about it  i saw some online libs that can detect the boilerplate of the webpage but i just want to detect and extract the html code of the navigation menu  please help thank you  
280,280,19012454,72570410,Combining features in Machine Learning,"<p>so I was looking at the titanic ML solution on kaggle and noticed that one solution combined 2 features (age, class) to make a new artificial feature (Age*Class) for their classifier. Class here is ~(1,2,3) meaning upper class, lower class on the ship.</p>
<ul>
<li>How common is this (multiplying 2 or more variables together)?</li>
<li>Why do this?</li>
<li>Are there methods to choose which variables to multiply together?</li>
<li>Are there other &quot;types&quot; of combining features that I can look into?</li>
</ul>
",14,0,-1,3,machine-learning;feature-selection;feature-engineering,2022-06-10 12:44:11,2022-06-10 12:44:11,2022-06-10 12:44:55,so i was looking at the titanic ml solution on kaggle and noticed that one solution combined  features  age  class  to make a new artificial feature  age class  for their classifier  class here is       meaning upper class  lower class on the ship 
281,281,19301271,72550803,Visual Studio for Mac 2022 - Where is GIT and Version Control?,"<p>I am bit new to this Visual Studio for Mac 2022 as it came out just recently for Mac OS.
I am learning by following a video course, which is done on Windows. And there is supposed to be an option to &quot;Add to Source Control&quot; or from the official docs for Visual Studio for Mac =&gt; <a href=""https://docs.microsoft.com/en-us/visualstudio/mac/working-with-git?view=vsmac-2019"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/visualstudio/mac/working-with-git?view=vsmac-2019</a> there is supposed to be a tab &quot;Version Control&quot; to push my project to my Github.</p>
<p>But there is not. If i try to go like this:</p>
<ol>
<li>Select Solution</li>
<li>Commit or Stash</li>
<li>Push
i get error &quot;Create an initial commit first&quot;</li>
</ol>
<p>But i can not commit as this i the only way to commit. It is bit frustrating.
Also there is no way to connect my Github account to the VSforMac 2022 even if I use the &quot;Help&quot; and put in Git or Github this it the only option that it finds.</p>
<p>Did they just forgot to add Git integration to the new Visual Studio code for 2022 ? Or I am missing something in settings or possible a package?</p>
<p>See screenshot <a href=""https://ibb.co/N2nPMbg"" rel=""nofollow noreferrer"">https://ibb.co/N2nPMbg</a>
PS: I have Git installed on my machine</p>
<p>Thanks!</p>
",115,1,0,2,c#;visual-studio-mac,2022-06-09 00:21:26,2022-06-09 00:21:26,2022-06-10 12:43:17,but there is not  if i try to go like this  did they just forgot to add git integration to the new visual studio code for    or i am missing something in settings or possible a package  thanks 
282,282,19311951,72570126,add your rest api to moodle,"<p>Hello guys i created a machine learning model and deploy it using Django REST API
now i want to add my API to moodle does anyone know how to do that ?</p>
",117,0,-2,5,python;api;rest;django-rest-framework;moodle,2022-06-10 12:17:07,2022-06-10 12:17:07,2022-06-10 12:17:07,
283,283,7222983,72568900,A question about splitting training dataset in Machine Learning,"<p>Suppose I have training dataset with the size of 50 million, I would like to train it with a batch_size and epoch.
I wonder to know if I separate the dataset into 5 subset with 10 million each one. and I train my model like this:
first I train my model with subset_1 under the same batch_size and epoch as I mentioned before, and get Model_1.
then I train Model_1 with subset_2 under the same batch_size and epoch, repeat this loop until I get the final Model.
Will this influence the precision of my final Model compared to training on the whole dataset at each epoch with the same batch_size.</p>
",12,0,-2,4,machine-learning;deep-learning;neural-network;training-data,2022-06-10 09:14:53,2022-06-10 09:14:53,2022-06-10 09:14:53,
284,284,17712392,72557917,How do i use machine learning to predict the location of a boundary box in an image after training,"<p>I have several images of cars and I propose to label each with a boundary box to identify the vehicle and a boundary box to the label the number plate. I propose to train a machine learning model to predict the number plate location.</p>
<p>All images are from a single camera with a fixed perspective, the images are frames of a video, so the training and testing dataset are consistent.</p>
<p>I know the four corners of the boundary box for the car (x1,y1,x2,y2) and the 4 corners of the boundary box for the plate (a1,b1,a2,b2). I want to train a model using these variables to then predict a number plate location.</p>
<p>Can anyone point me in the right direction? I imagine it is some sort of linear or multi-linear regression...</p>
",13,0,0,4,object;machine-learning;regression;detection,2022-06-09 15:01:43,2022-06-09 15:01:43,2022-06-10 04:18:01,i have several images of cars and i propose to label each with a boundary box to identify the vehicle and a boundary box to the label the number plate  i propose to train a machine learning model to predict the number plate location  all images are from a single camera with a fixed perspective  the images are frames of a video  so the training and testing dataset are consistent  i know the four corners of the boundary box for the car  x y x y  and the  corners of the boundary box for the plate  a b a b   i want to train a model using these variables to then predict a number plate location  can anyone point me in the right direction  i imagine it is some sort of linear or multi linear regression   
285,285,19250420,72566105,I am getting attribute error in my machine learning project title is Old car price prediction,"<p>so I am implementing machine learning algorithm for old car price prediction using Linear Regression and all work done in jupyter and then using Flask framework I created a web page
in jupyter program runs sucessfully and give the accurate output but when i render in web page it gives error</p>
<p>Error message
AttributeError: 'OneHotEncoder' object has no attribute '_infrequent_enabled'
[at line number 40 it gives error<a href=""https://i.stack.imgur.com/L45u9.jpg"" rel=""nofollow noreferrer"">][1]</a></p>
<p><a href=""https://i.stack.imgur.com/wyWu3.jpg"" rel=""nofollow noreferrer"">when i click on predict price it gives error</a></p>
<pre><code>#python file for importing Flask Framework
from flask import Flask, render_template, request
from flask_cors import CORS, cross_origin
import pandas as pd
import pickle
import numpy as np
import warnings
warnings.filterwarnings(&quot;ignore&quot;)
app = Flask(__name__)
cors = CORS(app)

model = pickle.load(open(&quot;LinearRegressionModel.pkl&quot;, 'rb'))
car = pd.read_csv(&quot;Cleaned car.csv&quot;)


@app.route('/', methods=[&quot;GET&quot;, &quot;POST&quot;])
def index():
    companies = sorted(car['company'].unique())
    car_models = sorted(car['name'].unique())
    year = sorted(car['year'].unique(), reverse=True)
    fuel_type = sorted(car['fuel_type'].unique())

    companies.insert(0, &quot;Select Company&quot;)
    year.insert(0, &quot;Select Year&quot;)
    fuel_type.insert(0, &quot;Select Fuel Type&quot;)

    return render_template('index.html', companies=companies, car_models=car_models, years=year, fuel_type=fuel_type)


@app.route('/predict', methods=['POST'])
@cross_origin(supports_credentials=True)
def predict():
    car_model = request.form.get('car_models')
    company = request.form.get('company')
    year = int(request.form.get('year'))
    fuel_type = request.form.get('fuel_type')
    kms_driven = int(request.form.get('kilo_driven'))
    print(company, car_model, year, fuel_type, kms_driven)



    prediction = model.predict(pd.DataFrame(columns=['name', 'company', 'year', 'kms_driven', 'fuel_type']),
                               data=np.array([car_model, company, year, kms_driven, fuel_type]).reshape(1,5)),
    print(prediction)
   

    return str(np.round(prediction[0], 2))


if __name__ == &quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
<pre><code>#jupyter Notebook Code
import pandas as pd
import numpy as np
car=pd.read_csv('quikr_car.csv')
car.head()
name    company year    Price   kms_driven  fuel_type
0   Hyundai Santro Xing XO eRLX Euro III    Hyundai 2007    80,000  45,000 kms  Petrol
1   Mahindra Jeep CL550 MDI Mahindra    2006    4,25,000    40 kms  Diesel
2   Maruti Suzuki Alto 800 Vxi  Maruti  2018    Ask For Price   22,000 kms  Petrol
3   Hyundai Grand i10 Magna 1.2 Kappa VTVT  Hyundai 2014    3,25,000    28,000 kms  Petrol
4   Ford EcoSport Titanium 1.5L TDCi    Ford    2014    5,75,000    36,000 kms  Diesel
car.shape
(892, 6)
car.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 892 entries, 0 to 891
Data columns (total 6 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   name        892 non-null    object
 1   company     892 non-null    object
 2   year        892 non-null    object
 3   Price       892 non-null    object
 4   kms_driven  840 non-null    object
 5   fuel_type   837 non-null    object
dtypes: object(6)
memory usage: 41.9+ KB
Quality
year has many non-year values
year object to int
price has ask for price
price object to int
kms_driven has kms with integers
kms_driven object to int
kms_driven has nan values
fuel_type has nan values
keep first 3 words of name 
Cleaning
backup=car.copy()
car=car[car['year'].str.isnumeric()]
car['year']=car['year'].astype(int)
car.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 842 entries, 0 to 891
Data columns (total 6 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   name        842 non-null    object
 1   company     842 non-null    object
 2   year        842 non-null    int32 
 3   Price       842 non-null    object
 4   kms_driven  840 non-null    object
 5   fuel_type   837 non-null    object
dtypes: int32(1), object(5)
memory usage: 42.8+ KB
car['Price']
0             80,000
1           4,25,000
2      Ask For Price
3           3,25,000
4           5,75,000
           ...      
886         3,00,000
888         2,60,000
889         3,90,000
890         1,80,000
891         1,60,000
Name: Price, Length: 842, dtype: object
car=car[car['Price']!=&quot;Ask For Price&quot;]
car.head()
name    company year    Price   kms_driven  fuel_type
0   Hyundai Santro Xing XO eRLX Euro III    Hyundai 2007    80,000  45,000 kms  Petrol
1   Mahindra Jeep CL550 MDI Mahindra    2006    4,25,000    40 kms  Diesel
3   Hyundai Grand i10 Magna 1.2 Kappa VTVT  Hyundai 2014    3,25,000    28,000 kms  Petrol
4   Ford EcoSport Titanium 1.5L TDCi    Ford    2014    5,75,000    36,000 kms  Diesel
6   Ford Figo   Ford    2012    1,75,000    41,000 kms  Diesel
car['Price']=car['Price'].str.replace(',','').astype(int)
car.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 819 entries, 0 to 891
Data columns (total 6 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   name        819 non-null    object
 1   company     819 non-null    object
 2   year        819 non-null    int32 
 3   Price       819 non-null    int32 
 4   kms_driven  819 non-null    object
 5   fuel_type   816 non-null    object
dtypes: int32(2), object(4)
memory usage: 38.4+ KB
car['kms_driven']=car['kms_driven'].str.split(' ').str.get(0).str.replace(',','')
car=car[car['kms_driven'].str.isnumeric()]
car['kms_driven']=car['kms_driven'].astype(int)
car.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 817 entries, 0 to 889
Data columns (total 6 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   name        817 non-null    object
 1   company     817 non-null    object
 2   year        817 non-null    int32 
 3   Price       817 non-null    int32 
 4   kms_driven  817 non-null    int32 
 5   fuel_type   816 non-null    object
dtypes: int32(3), object(3)
memory usage: 35.1+ KB
car=car[~car['fuel_type'].isna()]
car['name']=car['name'].str.split(' ').str.slice(0,3).str.join(' ')
car
name    company year    Price   kms_driven  fuel_type
0   Hyundai Santro Xing Hyundai 2007    80000   45000   Petrol
1   Mahindra Jeep CL550 Mahindra    2006    425000  40  Diesel
3   Hyundai Grand i10   Hyundai 2014    325000  28000   Petrol
4   Ford EcoSport Titanium  Ford    2014    575000  36000   Diesel
6   Ford Figo   Ford    2012    175000  41000   Diesel
... ... ... ... ... ... ...
883 Maruti Suzuki Ritz  Maruti  2011    270000  50000   Petrol
885 Tata Indica V2  Tata    2009    110000  30000   Diesel
886 Toyota Corolla Altis    Toyota  2009    300000  132000  Petrol
888 Tata Zest XM    Tata    2018    260000  27000   Diesel
889 Mahindra Quanto C8  Mahindra    2013    390000  40000   Diesel
816 rows × 6 columns

car=car.reset_index(drop=True)
car=car[car['Price']&lt;6e6].reset_index(drop=True)
car
name    company year    Price   kms_driven  fuel_type
0   Hyundai Santro Xing Hyundai 2007    80000   45000   Petrol
1   Mahindra Jeep CL550 Mahindra    2006    425000  40  Diesel
2   Hyundai Grand i10   Hyundai 2014    325000  28000   Petrol
3   Ford EcoSport Titanium  Ford    2014    575000  36000   Diesel
4   Ford Figo   Ford    2012    175000  41000   Diesel
... ... ... ... ... ... ...
810 Maruti Suzuki Ritz  Maruti  2011    270000  50000   Petrol
811 Tata Indica V2  Tata    2009    110000  30000   Diesel
812 Toyota Corolla Altis    Toyota  2009    300000  132000  Petrol
813 Tata Zest XM    Tata    2018    260000  27000   Diesel
814 Mahindra Quanto C8  Mahindra    2013    390000  40000   Diesel
815 rows × 6 columns

car.to_csv('Cleaned car.csv')
Models
X=car.drop(columns='Price')
y=car['Price']
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline 
ohe = OneHotEncoder()
ohe.fit(X[['name','company','fuel_type']])
OneHotEncoder()
column_trans = make_column_transformer((OneHotEncoder(categories=ohe.categories_),['name','company','fuel_type']),remainder='passthrough')
lr=LinearRegression()
pipe=make_pipeline(column_trans,lr)
pipe.fit(X_train,y_train)
Pipeline(steps=[('columntransformer',
                 ColumnTransformer(remainder='passthrough',
                                   transformers=[('onehotencoder',
                                                  OneHotEncoder(categories=[array(['Audi A3 Cabriolet', 'Audi A4 1.8', 'Audi A4 2.0', 'Audi A6 2.0',
       'Audi A8', 'Audi Q3 2.0', 'Audi Q5 2.0', 'Audi Q7', 'BMW 3 Series',
       'BMW 5 Series', 'BMW 7 Series', 'BMW X1', 'BMW X1 sDrive20d',
       'BMW X1 xDrive20d', 'Chevrolet Beat', 'Chevrolet Beat...
                                                                            array(['Audi', 'BMW', 'Chevrolet', 'Datsun', 'Fiat', 'Force', 'Ford',
       'Hindustan', 'Honda', 'Hyundai', 'Jaguar', 'Jeep', 'Land',
       'Mahindra', 'Maruti', 'Mercedes', 'Mini', 'Mitsubishi', 'Nissan',
       'Renault', 'Skoda', 'Tata', 'Toyota', 'Volkswagen', 'Volvo'],
      dtype=object),
                                                                            array(['Diesel', 'LPG', 'Petrol'], dtype=object)]),
                                                  ['name', 'company',
                                                   'fuel_type'])])),
                ('linearregression', LinearRegression())])
y_pred= pipe.predict(X_test)
r2_score(y_test,y_pred)
0.7137908798101541
scores=[]
for i in range(1000):
    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=i)
    lr=LinearRegression()
    pipe=make_pipeline(column_trans,lr)
    pipe.fit(X_train,y_train)
    y_pred=pipe.predict(X_test)
    print(r2_score(y_test,y_pred),i)
    scores.append(r2_score(y_test,y_pred))
np.argmax(scores)
661
scores[np.argmax(scores)]
0.8900224346636179
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=np.argmax(scores))
lr=LinearRegression()
pipe=make_pipeline(column_trans,lr)
pipe.fit(X_train,y_train)
y_pred=pipe.predict(X_test)
r2_score(y_test,y_pred)
0.8900224346636179
import pickle
pickle.dump(pipe,open('LinearRegressionModel.pkl','wb'))

</code></pre>
",20,0,-1,2,python;jupyter,2022-06-10 01:40:09,2022-06-10 01:40:09,2022-06-10 02:43:50,
286,286,16388439,68265012,ImportError: cannot import name &#39;__version__&#39; from partially initialized module &#39;keras&#39; (most likely due to a circular import),"<p><strong>I have imported the following libraries for my machine learning project but have problem when I try to run my model in the command prompt...</strong></p>
<pre><code>from tensorflow.python.keras import Model

from tensorflow.python.keras.layers import Layer, Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Lambda

from tensorflow.python.keras.initializers import TruncatedNormal

from keras.optimizers import Adam

from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, Callback

from tensorflow.python.keras.models import load_model

from tensorflow.python.keras.utils import Sequence
</code></pre>
<p>This is the error message which I get when trying to run the model in the command prompt.</p>
<pre><code>ImportError: cannot import name '__version__' from partially initialized module 'keras' (most likely due to a circular import) (C:\Users\gurun\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py)
</code></pre>
",3859,2,1,3,python;tensorflow;keras,2021-07-06 10:58:17,2021-07-06 10:58:17,2022-06-09 21:12:12,i have imported the following libraries for my machine learning project but have problem when i try to run my model in the command prompt    this is the error message which i get when trying to run the model in the command prompt 
287,287,14999984,72562029,I can&#39;t deploy using sshagent,"<p>im learning jenkins for the moment and i had a problem with sshagent . i can't connect to my remote machine .
first i'm trying to deploy into kubernetes using jenkins .and i'm using ssh agent to get into kubernetes-master so i can run command from it .
so the probleme is i genareted a ssh key in my kubernetes-master and i added   the private key to jenkins but it not working .
any help here plz
here you find  the stage .of deployment in the pipeline<br />
and the output console when i executed the job
<a href=""https://i.stack.imgur.com/X2RWJ.jpg"" rel=""nofollow noreferrer"">jenkins output  console</a></p>
<pre><code> stage('deploy to K8s cluster '){
        steps{
             sshagent(['Jenkins-Access-Kube']) {
                         sh&quot;scp -r -o StrictHostKeyChecking=no  /home/automate-deployment-on-k8s/complete-demo.yaml younes@192.168.8.199:/home/younes/k8s&quot;
                script{
                  try{
                        sh 'ssh younes@192.168.8.199  kubectl apply -f . '
                  }catch(error){
                        sh 'ssh younes@192.168.8.199 kubectl create -f . '
                  }
                }

                }
</code></pre>
",38,1,-1,4,kubernetes;jenkins-pipeline;continuous-deployment;ssh-agent,2022-06-09 19:53:16,2022-06-09 19:53:16,2022-06-09 21:03:42,
288,288,19306215,72559543,I want to get more than two targets in output of model machine learning,"<p>I want to get two classes in the prediction of the classification model with text.</p>
<pre><code>model_RandomForest = RandomForestClassifier(n_estimators=100,criterion='gini', random_state=0) 
</code></pre>
<p>if I can add a parameter of number output</p>
",15,1,1,4,python;machine-learning;scikit-learn;deep-learning,2022-06-09 17:02:36,2022-06-09 17:02:36,2022-06-09 20:57:04,i want to get two classes in the prediction of the classification model with text  if i can add a parameter of number output
289,289,19078458,72561533,"Machine learning, using a model inside of another model (KERAS)","<p>I have two machine learning models (all in keras):</p>
<p>Model 1:
4 inputs and 1 output for my model.</p>
<p>Model 2:
trained on 3 of the inputs of model 1, gives a new output.</p>
<p>I wish to train model 1 to use model 2 on the 3 congruent inputs, then using the output of model 2 alongside the last input of model 1.</p>
<p>Basically, I need model 1 to be able to utilise model 2 to give me the correct output.</p>
<p>Is this a possibility in Keras?
If it is could I have some direction as to how to complete this task?</p>
<p>Thank you</p>
",19,0,0,4,python;keras;deep-learning;unsupervised-learning,2022-06-09 19:21:39,2022-06-09 19:21:39,2022-06-09 19:21:39,i have two machine learning models  all in keras   i wish to train model  to use model  on the  congruent inputs  then using the output of model  alongside the last input of model   basically  i need model  to be able to utilise model  to give me the correct output  thank you
290,290,5672970,60490742,CPU usage/speed in Jupyter Notebooks for machine learning tasks,"<p>I've just built a brand new powerful desktop PC in order to speed up my scikit learn computations (<a href=""https://pcpartpicker.com/list/VVvsK4"" rel=""nofollow noreferrer"">specs here</a>).</p>

<p>I run my code in a Jupyter Notebook and I noticed that if I run the same computation on my old dying laptop and my super-PC the time difference is often small, although on some very demanding cells in can vary from simple to double between the two computers… But my new PC is suppose to be at least 5 times more powerful than my old laptop!</p>

<p>Demanding code example:</p>

<pre><code>y_train_large = (y_train &gt;= 7)
y_train_odd = (y_train % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)    
y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)
f1_score(y_multilabel, y_train_knn_pred, average=""macro"")
</code></pre>

<p>Also, when I check the CPU usage during a classifier training for instance, it's very low on both computers (around 5% on the new one and 15-20% on the old one).</p>

<p>I realise that it may be a big noob question but why is that?
I read <a href=""https://stackoverflow.com/questions/47259358/jupyter-notebook-low-cpu-usage"">here</a> that Jupyter Notebooks run on the host machine not mine.
How to use my own hardware instead? I probably search the wrong way but I cannot find a lot of informations on that subject. What to search for?</p>

<p>Thanks !</p>

<p>Time report for the code above with the small change of setting n_jobs=4 for cross_val_predict():</p>

<blockquote>
  <p>Computing time for AMD Ryzen 9 3900x 12 cores, RAM 32Go  :  <strong>12'45''</strong>
  approx. average CPU usage 15%</p>
  
  <p>Computing time for Intel i7 4750HQ @ 2.00 GHz, RAM 16Go  :  <strong>19'50''</strong>
  approx. average CPU usage 62%</p>
</blockquote>
",7343,1,0,5,python;scikit-learn;jupyter-notebook;cpu-usage;cpu-speed,2020-03-02 19:47:23,2020-03-02 19:47:23,2022-06-09 17:32:20,i ve just built a brand new powerful desktop pc in order to speed up my scikit learn computations     i run my code in a jupyter notebook and i noticed that if i run the same computation on my old dying laptop and my super pc the time difference is often small  although on some very demanding cells in can vary from simple to double between the two computers  but my new pc is suppose to be at least  times more powerful than my old laptop  demanding code example  also  when i check the cpu usage during a classifier training for instance  it s very low on both computers  around   on the new one and    on the old one   thanks   time report for the code above with the small change of setting n_jobs  for cross_val_predict   
291,291,3976008,72559416,Swift - Remove image background with CoreML,"<p>I am using CoreML with the DeepLabV3 model to remove the background from an image: <a href=""https://developer.apple.com/machine-learning/models/"" rel=""nofollow noreferrer"">https://developer.apple.com/machine-learning/models/</a></p>
<p>This is working well for removing the background from photos where the subject it a person/dog/car, but for other cases, such as skylines and some objects on a table (please see example images), it is unable to detect the object from the background.</p>
<p>Should I be using a different method for this?</p>
<p>Thank you</p>
<pre><code>var imageSegmentationModel = DeepLabV3()
var request :  VNCoreMLRequest?

func setUpModel() {
        if let visionModel = try? VNCoreMLModel(for: imageSegmentationModel.model) {
                request = VNCoreMLRequest(model: visionModel, completionHandler: visionRequestDidComplete)
                request?.imageCropAndScaleOption = .scaleFill
        }
        else {
                fatalError()
        }
}

func predict() {
        DispatchQueue.global(qos: .userInitiated).async {
            guard let request = self.request else { fatalError() }
            let handler = VNImageRequestHandler(cgImage: (self.originalImage?.cgImage)!, options: [:])
            do {
                try handler.perform([request])
            }catch {
                print(error)
            }
        }
   }

func visionRequestDidComplete(request: VNRequest, error: Error?) {
        DispatchQueue.main.async {
            if let observations = request.results as? [VNCoreMLFeatureValueObservation],
                let segmentationmap = observations.first?.featureValue.multiArrayValue {
                
                self.maskImage = segmentationmap.image(min: 0, max: 255)
                print(self.maskImage!.size)
                
                self.maskImage = self.maskImage?.resizedImage(for: self.originalImage!.size)
                if let image:UIImage = self.maskOriginalImage(){
                    print(&quot;Success&quot;)
                    self.outputImageView.image = image
                }
            }
        }
            
    }

func maskOriginalImage() -&gt; UIImage? {
        if(self.maskImage != nil &amp;&amp; self.originalImage != nil){
            let maskReference = self.maskImage?.cgImage!
            let imageMask = CGImage(maskWidth: maskReference!.width,
                                    height: maskReference!.height,
                                    bitsPerComponent: maskReference!.bitsPerComponent,
                                    bitsPerPixel: maskReference!.bitsPerPixel,
                                    bytesPerRow: maskReference!.bytesPerRow,
                                    provider: maskReference!.dataProvider!, decode: nil, shouldInterpolate: true)
            
            let maskedReference = self.originalImage?.cgImage!.masking(imageMask!)
            return UIImage(cgImage: maskedReference!)
            
        }
        return nil
    }
</code></pre>
<p><a href=""https://i.stack.imgur.com/lSOLm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSOLm.jpg"" alt=""Good Image Example"" /></a></p>
<p><a href=""https://i.stack.imgur.com/e8VQ0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e8VQ0.jpg"" alt=""Bad Image Example"" /></a></p>
",53,0,1,5,ios;swift;coreml;apple-vision;deeplab,2022-06-09 16:51:15,2022-06-09 16:51:15,2022-06-09 16:51:15,i am using coreml with the deeplabv model to remove the background from an image   this is working well for removing the background from photos where the subject it a person dog car  but for other cases  such as skylines and some objects on a table  please see example images   it is unable to detect the object from the background  should i be using a different method for this  thank you  
292,292,15255615,72559040,How we use salib sensitivity analysis in model of machine learning (example Random forest),"<p>I created a model of prevision machine learning-based (regression problem), and I want to do a sensitivity analysis using the SAlib library.
how to get the sensitivity analysis parameters of my model</p>
<p><code>!pip install SAlib</code></p>
<pre><code>from SALib.sample import latin
from SALib.sample import sobol_sequence
from SALib.sample import morris as morris_samp
from SALib.sample import saltelli
from SALib.analyze import sobol
from SALib.analyze import morris
from SALib.test_functions import Ishigami
from SALib.test_functions import Sobol_G
</code></pre>
<pre><code>problem ={'num_vars': 8,
          'names': ['x1',   'x2',   'x3',   'x4',   'x5',   'x6',   'x7','x8'],
          'bounds': [[1.3,35.25],
                     [28, 111.50],
                     [12, 38.50],
                     [8, 225],
                     [0,6],
                     [1, 7],
                     [0.38, 2.33],
                     [2.5, 89.55]]}
</code></pre>
",13,0,0,3,python;model;salib,2022-06-09 16:23:11,2022-06-09 16:23:11,2022-06-09 16:23:11, pip install salib
293,293,19305321,72558513,Machine Learning Problem about Representing Data (What does representing data mean?),"<p>Firstly, I just want to make sure that I do not want from anyone to solve this problem for me. I didnt post this for that. :)
I'm just a beginner. I saw this question. I gave some answers. For example, I gave this answer for the first question, but I am not sure whether I am solving it in a wrong way or the question is asking this. I just want to ask you that whether my approach is correct. Especially, I didnt understand What representing data mean.</p>
<p>My Answer:
For unknown areas, they can use Unsupervised Learning, and they can try to cluster similar areas in terms similar features. For known areas, they can use Supervised Learning. For the water quality, they can categorize using two different labels such as &quot;clean&quot; and &quot;not clean&quot;.In addition, they can use binary data by giving value &quot;1&quot; as clean and value &quot;0&quot; as not clean. For the storms, they can use numerical data. They can store how many times a storm occured for every areas as numerical data For the food, they can use binary data for every food. If the specified food is located on the island in that row, value of 1 will be given. Otherwise, value of 0 will be given.</p>
<p>Problem:
In the TV Series Lost, Oceanic Flight 815 crashed on an island with its passengers.
There are 70 survivors and a dog after the crash. Survivors keep living in one area of the island. However, the living conditions in that area are not good enough: there is little clean water, there are storms, and there isn’t enough food.</p>
<p>Considering these situations, they have to explore better areas to live in. Jack, Sawyer and Sun will collect data from various areas of the island. While collecting data, they have two types of areas, “known area”, and “unknown area”. For “known” areas, they know if the area is safe, so they categorize (label) the area as either safe or not safe. They also collect features of the area, for the “known” areas. For “unknown” areas, they do not know if the area is safe or not. So, for “unknown” areas, they can only collect the features of each area. Here are the features they collect for the (known / unknown) areas:
For the water quality, they take pictures of the water ponds from the different areas of the island. In these pictures, it is easy to see if the water is clean or not. So, survivors categorize each of the pictures as clean, or not clean. For the storms, they ask the locals: “How many storm hazards occurred in the last year?”. They do this for different areas of the island, and they write down how many hazards occurred for each area in the island. For the food, they write down in text the various types of food they found on different parts of the island.They do this for every area in the island. So, for each area of the island, they write down a row of text. Within each row, names of different foods are written, separated by commas.
Please remember that for “known” areas, they note down if the area is safe, or not safe.
For unknown areas, they want to know which areas are safe, and which areas are not safe. Desmond, who knows how to use computers on the island, proposes using Machine Learning algorithms to solve this problem.</p>
<p>How could they represent each type of data they have (water, storm, food, safe) on a computer, to feed the data into a machine learning model?</p>
",45,0,-1,4,python;algorithm;machine-learning;model,2022-06-09 15:41:07,2022-06-09 15:41:07,2022-06-09 16:14:56,how could they represent each type of data they have  water  storm  food  safe  on a computer  to feed the data into a machine learning model 
294,294,13414218,67805919,Problem with setting up Tensorflow GPU support,"<p>I am trying to install support for Tensorflow GPU using the following guide:</p>
<p><a href=""https://www.tensorflow.org/install/gpu"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/gpu</a></p>
<p>I am on Ubuntu (20.04 LTS)</p>
<p>I've followed the instruction for the latest Ubuntu below (Cuda 11):</p>
<pre class=""lang-sh prettyprint-override""><code># Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo add-apt-repository &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /&quot;
sudo apt-get update

wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb

sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt install ./libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt-get update

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-11-0 \
    libcudnn8=8.0.4.30-1+cuda11.0  \
    libcudnn8-dev=8.0.4.30-1+cuda11.0

# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install TensorRT. Requires that libcudnn8 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer7=7.1.3-1+cuda11.0 \
    libnvinfer-dev=7.1.3-1+cuda11.0 \
    libnvinfer-plugin7=7.1.3-1+cuda11.0
</code></pre>
<p>After running this and rebooting, I have Cuda 11 and CuDNN 8.</p>
<p>After this I installed tensorflow with a simple <code>pip install tensorflow</code>, as I understood online there's no need to install <code>tensorflow-gpu</code> explicitly in the newer versions of tensorflow.</p>
<p>This is what I'm getting after trying to import tensorflow and check physical devices:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
</code></pre>
<p>Result:</p>
<pre><code>2021-06-02 16:04:03.347039: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
</code></pre>
<pre class=""lang-py prettyprint-override""><code>tf.config.list_physical_devies('GPU')
</code></pre>
<p>Result:</p>
<pre><code>2021-06-02 16:11:19.035743: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-06-02 16:11:19.067500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-02 16:11:19.067753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.759GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s
2021-06-02 16:11:19.067771: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-06-02 16:11:19.069485: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-06-02 16:11:19.069529: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-06-02 16:11:19.069625: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-06-02 16:11:19.069689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-06-02 16:11:19.069736: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2021-06-02 16:11:19.069796: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-06-02 16:11:19.069930: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-06-02 16:11:19.069938: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
</code></pre>
<p>It seems like tensorflow is complaining about 4 files (.so libraries):</p>
<ul>
<li>libcufft.so.10</li>
<li>libcurand.so.10</li>
<li>libcusolver.so.11</li>
<li>libcusparse.so.11</li>
</ul>
<p>I've tried to look for these in my system using the <code>locate</code> command on Ubuntu, they do not exist anywhere.</p>
<p>I haven't added anything to my .bashrc since I was not sure what the LD_LIBRARY_PATH must be.</p>
",904,2,0,4,python;linux;tensorflow;ubuntu,2021-06-02 18:47:58,2021-06-02 18:47:58,2022-06-09 16:00:02,i am trying to install support for tensorflow gpu using the following guide   i am on ubuntu    lts  i ve followed the instruction for the latest ubuntu below  cuda    after running this and rebooting  i have cuda  and cudnn   after this i installed tensorflow with a simple pip install tensorflow  as i understood online there s no need to install tensorflow gpu explicitly in the newer versions of tensorflow  this is what i m getting after trying to import tensorflow and check physical devices  result  result  it seems like tensorflow is complaining about  files   so libraries   i ve tried to look for these in my system using the locate command on ubuntu  they do not exist anywhere  i haven t added anything to my  bashrc since i was not sure what the ld_library_path must be 
295,295,19305065,72557443,Keras: solving new problems I do not know the answer,"<p>I try to summarize my problem as good as I can, but it is really don't easy for me because I do not really know which key words to ask about...</p>
<p>I am quite new in machine learning and used Keras to solve problems I know the answer.</p>
<p>Example:
As an input have tons of photos of different object like cars, birds... (x-value) and a label what they are (y-value). The program can then recognize objects on new photos. The usual stuff.</p>
<p>How can I write a program that discovers new ways to solve a problem?</p>
<p>Example:
Players play a game, and I record their moves as well as the result/score. How can I write program that finds the optimal strategy? I tried to just sort out games with bad scores but I guess there is a better way, something like</p>
<p>if score is really good, then learn this strongly
if score is slightly over average, then learn this
if score is slightly under average, then  do not learn this
if score is really bad,then avoid this strongly</p>
<p>Can you give me a hint? Would be really nice.</p>
",33,1,-1,4,python;machine-learning;keras;deep-learning,2022-06-09 14:26:32,2022-06-09 14:26:32,2022-06-09 14:47:19,i try to summarize my problem as good as i can  but it is really don t easy for me because i do not really know which key words to ask about    i am quite new in machine learning and used keras to solve problems i know the answer  how can i write a program that discovers new ways to solve a problem  can you give me a hint  would be really nice 
296,296,19274104,72516652,Error while using pymc3 and Theano-PyMC package,"<p>I am trying to use AutomatedRecommendationTool - A machine learning Automated Recommendation Tool for guiding synthetic biology. It uses a package named pymc3. But there are some issues regarding the compiler.
Following is the Error:</p>
<pre><code>Exception: ('Compilation failed (return status=1): C:\\Users\\vaibh\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.19044-SP0-AMD64_Family_23_Model_96_Stepping_1_AuthenticAMD-3.9.12-64\\tmpqepy79wz\\mod.cpp:1:0: sorry, unimplemented: 64-bit mode not compiled in\r.  #include &lt;Python.h&gt;\r.  \r. ', 'FunctionGraph(Elemwise{mul,no_inplace}(TensorConstant{3.141592653589793}, TensorConstant{0.01}))')
</code></pre>
<p>Following is the code cell that I am trying to run:</p>
<pre><code>%%time
if run_art:
    art = RecommendationEngine(df, **art_params)
else:
    with open(os.path.join(art_params['output_directory'], 'art.pkl'), 'rb') as output:
        art = pickle.load(output)
</code></pre>
<p>I am using a jupyter notebook for executing the code.</p>
<p>Following are the system specifications -</p>
<p>Processor: AMD Ryzen 5 4600H with Radeon Graphics 3.00 GHz</p>
<p>System type: 64-bit Operating System, x64-based processor</p>
<p>Operating System: Windows 10</p>
<p>I have also installed a c++ compiler (MingW):</p>
<pre><code>C:\Users\vaibh&gt;g++ --version
g++ (MinGW.org GCC-6.3.0-1) 6.3.0
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>
<p>It seems that this error is related to theano, I tried to view the GitHub issues of theano but it doesn't seem to be helpful.</p>
<p>Following are the packages that I have installed -</p>
<ol>
<li>pipenv: 2022.5.2</li>
<li>depinfo: 1.7.0</li>
<li>python-libsbml(or just libsbml): 5.19.5</li>
<li>rfc3986: 2.0.0</li>
<li>h11: 0.13.0</li>
<li>rich: 12.4.4</li>
<li>pydantic: 1.9.1</li>
<li>diskcache: 5.4.0</li>
<li>importlib_resources: 5.7.1</li>
<li>Semver: 2.13.0</li>
<li>Pathvalidate: 2.5.0</li>
<li>pydoe: 0.3.8</li>
<li>tpot: 0.11.7</li>
<li>edd-utils: 0.0.12</li>
<li>pytorch: 1.11.0</li>
<li>mpi4py: 3.1.3</li>
<li>pymc3: 3.11.4</li>
<li>blas: 1.0</li>
</ol>
<p>Update: Theano-PyMC:1.1.2 was also installed as a dependency</p>
",69,0,0,4,python;c++;theano;pymc3,2022-06-06 16:28:37,2022-06-06 16:28:37,2022-06-09 13:59:38,following is the code cell that i am trying to run  i am using a jupyter notebook for executing the code  following are the system specifications   processor  amd ryzen  h with radeon graphics   ghz system type   bit operating system  x based processor operating system  windows  i have also installed a c   compiler  mingw   it seems that this error is related to theano  i tried to view the github issues of theano but it doesn t seem to be helpful  following are the packages that i have installed   update  theano pymc    was also installed as a dependency
297,297,1783739,70892367,Transfer files saved in filestore to either the workspace or to a repo,"<p>I built a machine learning model:</p>
<pre><code>lr = LinearRegression()
lr.fit(X_train, y_train)
</code></pre>
<p>which I can save to the filestore by:</p>
<pre><code>filename = &quot;/dbfs/FileStore/lr_model.pkl&quot;
with open(filename, 'wb') as f:
    pickle.dump(lr, f)
</code></pre>
<p>Ideally, I wanted to save the model directly to a workspace or a repo so I tried:</p>
<pre><code>filename = &quot;/Users/user/lr_model.pkl&quot;
os.makedirs(os.path.dirname(filename), exist_ok=True)
with open(filename, 'wb') as f:
    pickle.dump(lr, f)
</code></pre>
<p>but it is not working because the file is not showing up in the workspace.</p>
<p>The only alternative I have now is to transfer the model from the filestore to the workspace or a repo, how do I go about that?</p>
",106,1,1,5,python-3.x;machine-learning;databricks;azure-databricks;databricks-repos,2022-01-28 16:11:26,2022-01-28 16:11:26,2022-06-09 13:31:05,i built a machine learning model  which i can save to the filestore by  ideally  i wanted to save the model directly to a workspace or a repo so i tried  but it is not working because the file is not showing up in the workspace  the only alternative i have now is to transfer the model from the filestore to the workspace or a repo  how do i go about that 
298,298,738999,72551097,Multiclass classification problem with loose (not quite accurate) labeling. How to approach?,"<p>I'm looking for alt. ways to to build multiclass classification problem described below. The problem with using regular classification algorithms like Decision Tree or Logistic Regression is that they consider one value only to be True Positive, while in my data some classification are loosely done, and by that I mean instead of one single right label, 2 or more could be kindof &quot;right&quot;, some of them just a bit more accurate than the others, but only one is always chosen. Here is example to demonstrate what I meant:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Feature</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. &quot;My credentials do not work&quot;</td>
<td>&quot;Can't login&quot;</td>
</tr>
<tr>
<td>2. &quot;I do not get email with confirmation code&quot;</td>
<td>&quot;Two Factor Verification Issue&quot;</td>
</tr>
<tr>
<td>3. &quot;Do not get a receipt&quot;</td>
<td>&quot;Billing issue&quot;</td>
</tr>
</tbody>
</table>
</div>
<p>Feature #2 is classified as &quot;Two Factor Verification Issue&quot; in the training set, but could be classified as &quot;Can't login&quot;, which is Ok too. The classification was done by human in the past, and it was up to human how to label the feature, and someone did really classified &quot;2-factor verification&quot; issues as just &quot;can't login&quot;. The dataset is huge and re-labeling is not possible. Due to such labeling <strong>ambiguity</strong> training score comes out pretty lame.</p>
<p>I think about ways to better measure quality of a training: What if algorithm offers 2-3 options of classification outcome, and if at least one of them matches defined label, we count it as a successful guess. However a gap in my knowledge of machine learning algorithms do not let me figure out how to do that. Any suggestions? Thanks</p>
",26,1,0,1,machine-learning,2022-06-09 00:47:43,2022-06-09 00:47:43,2022-06-09 13:29:34,i m looking for alt  ways to to build multiclass classification problem described below  the problem with using regular classification algorithms like decision tree or logistic regression is that they consider one value only to be true positive  while in my data some classification are loosely done  and by that i mean instead of one single right label   or more could be kindof  right   some of them just a bit more accurate than the others  but only one is always chosen  here is example to demonstrate what i meant  feature   is classified as  two factor verification issue  in the training set  but could be classified as  can t login   which is ok too  the classification was done by human in the past  and it was up to human how to label the feature  and someone did really classified   factor verification  issues as just  can t login   the dataset is huge and re labeling is not possible  due to such labeling ambiguity training score comes out pretty lame  i think about ways to better measure quality of a training  what if algorithm offers   options of classification outcome  and if at least one of them matches defined label  we count it as a successful guess  however a gap in my knowledge of machine learning algorithms do not let me figure out how to do that  any suggestions  thanks
299,299,14364672,66360264,Machine Learning Services 2017 - Unable to launch the ( python ) runtime. ErrorCode 0x80070057: 87(The parameter is incorrect.),"<p>We changed over our SQL server 2017 with Machine learning Services ( MLS ) from running R 3.3.3 and Python 3.5.2 - to R 3.5.2 and Python 3.7.1.</p>
<p>SQL 2017 CU22 installs the higher value of R &amp; Python, so you then have to run an exe to tell SQL to use the higher R &amp; Python versions that were installed on disk via CU22.</p>
<p>After this was done successfully, when we went to restart Python we got the error :</p>
<pre><code>Msg 39021, Level 16, State 1, Line 0
Unable to launch runtime for 'Python' script. Please check the configuration of the 'Python' runtime.
Msg 39019, Level 16, State 2, Line 0
An external script error occurred: 
Unable to launch the runtime. ErrorCode 0x80070057: 87(The parameter is incorrect.).
</code></pre>
<p>This is a problem description and placeholder for the solution below.</p>
<p>See below for a solution that worked successfully .</p>
",672,2,-1,4,python;r;machine-learning;sql-server-2017,2021-02-25 04:53:23,2021-02-25 04:53:23,2022-06-09 13:21:57,we changed over our sql server  with machine learning services   mls   from running r    and python      to r    and python     sql  cu installs the higher value of r  amp  python  so you then have to run an exe to tell sql to use the higher r  amp  python versions that were installed on disk via cu  after this was done successfully  when we went to restart python we got the error   this is a problem description and placeholder for the solution below  see below for a solution that worked successfully  
300,300,1783739,71804344,Workspace url for machine learning experiment notebook,"<p>I am running a machine learning experiment in Databricks and I want to obtain the workspace URL for certain uses.</p>
<p>I know how to manually obtain the workspace URL of notebook from this link <a href=""https://docs.microsoft.com/en-us/azure/databricks/workspace/per-workspace-urls"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/workspace/per-workspace-urls</a></p>
<p>Similar to how you can obtain the path of your notebook by</p>
<pre><code>dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
</code></pre>
<p>How do I programmatically obtain the notebook's URL?</p>
",44,1,2,3,python-3.x;databricks;azure-databricks,2022-04-09 05:57:32,2022-04-09 05:57:32,2022-06-09 13:14:59,i am running a machine learning experiment in databricks and i want to obtain the workspace url for certain uses  i know how to manually obtain the workspace url of notebook from this link  similar to how you can obtain the path of your notebook by how do i programmatically obtain the notebook s url 
301,301,1565754,72556417,Definition of a truncated image file?,"<p>I'm doing machine learning in Python with TensorFlow on image files and while reading a set of .jpg-images at some point I get the error:</p>
<blockquote>
<p><code>OSError: image file is truncated (49 bytes not processed)</code></p>
</blockquote>
<p>I've been googling for a definition of a <code>truncated image file</code> without finding a clear definition (many sources talk about truncated image file without first explaining what it is), so my simple question is: <strong>What is a truncated image file? Definition? What does it mean?</strong></p>
<p>P.S. this post does NOT answer my question (it deals with handling truncated image error, not what it is as I understood):</p>
<p><a href=""https://stackoverflow.com/questions/60584155/oserror-image-file-is-truncated"">OSError: image file is truncated</a></p>
",21,0,0,3,python;image;truncate,2022-06-09 13:09:20,2022-06-09 13:09:20,2022-06-09 13:09:20,i m doing machine learning in python with tensorflow on image files and while reading a set of  jpg images at some point i get the error  oserror  image file is truncated   bytes not processed  i ve been googling for a definition of a truncated image file without finding a clear definition  many sources talk about truncated image file without first explaining what it is   so my simple question is  what is a truncated image file  definition  what does it mean  p s  this post does not answer my question  it deals with handling truncated image error  not what it is as i understood   
302,302,19185238,72543225,How to use TF-IDF+SVM， Word2Vec and BERT for text classification?,"<p>At present, I am using <strong>TF-IDF+SVM/logistic regression</strong> for text classification. I want to replace TF-IDF with <strong>Word2Vec</strong>, use Word2Vec to train word vectors, and then use SVM for classification. In addition, how to use <strong>CNN, LSTM</strong>, etc. as classifiers for classification, I want to compare the performance of traditional machine learning methods(SVM) and deep learning(CNN,LSTM). In addition, can I use <strong>pre-trained models</strong> such as <strong>BERT</strong> for word vector training, and then use svm/cnn/lstm for classification? What should I do?
Here is my current code, how can I improve it?</p>
<pre><code>df = pd.read_csv('/home/admin/data/2.csv',delimiter=',')
categories = ['Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column12', 'Column13', 'Column14', 'Column15', 'Column16', 'Column17', 'Column18', 'Column19', 'Column20', 'Column21','Column22']
train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)
X_train = train.Column1
X_test = test.Column1

NB_pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(stop_words=stop_words)),
                ('clf', OneVsRestClassifier(MultinomialNB(
                    fit_prior=True, class_prior=None))),])
for category in categories:
    print('... Processing {}'.format(category))
    # train the model using X_dtm &amp; y
    NB_pipeline.fit(X_train, train[category].astype('int'))
    # compute the testing accuracy
    prediction = NB_pipeline.predict(X_test)
    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))
    print('Test f1 is {}'.format(f1_score(test[category], prediction, average='weighted')))
    print('Test precision is {}'.format(precision_score(test[category], prediction, average='weighted')))
    print('Test recall is {}'.format(recall_score(test[category], prediction, average='weighted')))
</code></pre>
<p>Can someone help me, or give a reference link, thanks!</p>
",44,0,0,5,python;machine-learning;word2vec;text-classification;bert-language-model,2022-06-08 14:54:45,2022-06-08 14:54:45,2022-06-09 07:20:35,can someone help me  or give a reference link  thanks 
303,303,19282462,72518537,Xamarin: &#39;CustomBottomNavAppearance&#39; does not implement interface member &#39;IShellBottomNavViewAppearanceTracker.SetAppearance[...]&#39; error,"<p>I've recently tried to customize the tabbar using a custom ShellRenderer in my app using Xamarin and Visual Studio 2019. I've found <a href=""https://stackoverflow.com/questions/58635349/xamarin-forms-shell-custom-icon-for-selected-tab"">this</a> question where someone does exactly what I need and the question has an answer which explains it in great detail.</p>
<p>Now, my problem is that when trying to build the app, I get the following error:</p>
<p>'CustomBottomNavAppearance' does not implement interface member 'IShellBottomNavViewAppearanceTracker.SetAppearance(BottomNavigationView,IShellAppearanceElement)' (Errorcode CS0535, line 32)</p>
<p>Now, the sample project that the user in the other question provides works perfectly on my machine, yet implementing it in my project doesn't work, although they are practiacally identical.</p>
<p>I expect this to be very obvious for an advanced user, so sorry about that, I'm still learning and would like to just implement this feature right now, even if my skills might not allow me to do that.</p>
<p>Thanks in advance for any replies!</p>
<p>This is my 'MyShellRenderer.cs' class:</p>
<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using Android.App;
using Android.Content;
using Android.OS;
using Android.Runtime;
using Android.Views;
using Android.Widget;
using Google.Android.Material.BottomNavigation;
using MyProject;
using MyProject.Droid;
using Xamarin.Forms;
using Xamarin.Forms.Platform.Android;

[assembly: ExportRenderer(typeof(AppShell), typeof(MyShellRenderer))]
namespace MyProject.Droid
{
    public class MyShellRenderer : ShellRenderer
    {
        public MyShellRenderer(Context context) : base(context)
        {
        }

        protected override IShellBottomNavViewAppearanceTracker CreateBottomNavViewAppearanceTracker(ShellItem shellItem)
        {
            return new CustomBottomNavAppearance();
        }
    }

    public class CustomBottomNavAppearance : IShellBottomNavViewAppearanceTracker
    {
        public void Dispose()
        {

        }

        public void ResetAppearance(BottomNavigationView bottomView)
        {

        }

        public void SetAppearance(BottomNavigationView bottomView, ShellAppearance appearance)
        {
            IMenu myMenu = bottomView.Menu;

            IMenuItem myItemOne = myMenu.GetItem(0);

            if (myItemOne.IsChecked)
            {
                myItemOne.SetIcon(Resource.Drawable.icon_about);
            }
            else
            {
                myItemOne.SetIcon(Resource.Drawable.icon_feed);
            }
        }
    }
}
</code></pre>
<p>Edit: I've just found out that the issue actually only exists with the Android version (well, the iOS version at least doesn't throw the error).</p>
<p>Update: I have been able to fix the issue by using &quot;using Google.Android.Material.BottomNavigation;&quot; &quot;using Android.Support.Design.Widget;&quot;. My guess it that the old one is now deprecated and only works with the 4.x.x releases, not my 5.x.x release. Thanks everyone!</p>
",65,1,0,4,c#;xamarin;xamarin.forms;xamarin.forms.shell,2022-06-06 19:06:38,2022-06-06 19:06:38,2022-06-09 06:29:49,i ve recently tried to customize the tabbar using a custom shellrenderer in my app using xamarin and visual studio   i ve found  question where someone does exactly what i need and the question has an answer which explains it in great detail  now  my problem is that when trying to build the app  i get the following error   custombottomnavappearance  does not implement interface member  ishellbottomnavviewappearancetracker setappearance bottomnavigationview ishellappearanceelement    errorcode cs  line   now  the sample project that the user in the other question provides works perfectly on my machine  yet implementing it in my project doesn t work  although they are practiacally identical  i expect this to be very obvious for an advanced user  so sorry about that  i m still learning and would like to just implement this feature right now  even if my skills might not allow me to do that  thanks in advance for any replies  this is my  myshellrenderer cs  class  edit  i ve just found out that the issue actually only exists with the android version  well  the ios version at least doesn t throw the error   update  i have been able to fix the issue by using  using google android material bottomnavigation    using android support design widget    my guess it that the old one is now deprecated and only works with the  x x releases  not my  x x release  thanks everyone 
304,304,16996638,72553381,Task queues with Flask - &quot;module &#39;os&#39; has no attribute &#39;fork&#39;&quot;,"<p><strong>Introduction</strong></p>
<p>So, I'm trying to learn more about Flask, and I've been following this guide from Pythonise.com series. The website is no longer up, but I can still access it through Wayback machine. Here is the link to guide which I'm currently working with:
<a href=""https://web.archive.org/web/20210410211818/https://pythonise.com/series/learning-flask/flask-rq-task-queue"" rel=""nofollow noreferrer"">https://web.archive.org/web/20210410211818/https://pythonise.com/series/learning-flask/flask-rq-task-queue</a></p>
<p>I'm using Windows, so I had to install Redis on a Linux subsystem. I've used Ubuntu for this. My code editor is VScode. I get a <code>module 'os' has no attribute 'fork'</code> error when I try to add a new task to the queue.</p>
<p><strong>Error details</strong></p>
<p>Anyway, when I run the <code>rq worker</code> command in my terminal, I get this (which looks correct):</p>
<pre><code>01:21:13 Worker rq:worker:e78a93050f0f400f9f2e75ce25fb2f63: started, version 1.10.1
01:21:13 Subscribing to channel rq:pubsub:e78a93050f0f400f9f2e75ce25fb2f63
01:21:13 *** Listening on ←[32mdefault←[39;49;00m...
</code></pre>
<p>I then run the <code>flask run</code> command in a second terminal, and then visits <code>http://127.0.0.1:5000/task?n=100</code> in my browser. The browser returns <code>Task (fad0b6b8-63a7-4213-a02a-edd40e41b284) added to queue at 2022-06-08 23:27:16.294717</code> (which looks correct). However, I get this error in my first terminal:</p>
<pre><code>$ rq worker
01:21:13 Worker rq:worker:e78a93050f0f400f9f2e75ce25fb2f63: started, version 1.10.1
01:21:13 Subscribing to channel rq:pubsub:e78a93050f0f400f9f2e75ce25fb2f63
01:21:13 *** Listening on ←[32mdefault←[39;49;00m...
01:27:16 ←[32mdefault←[39;49;00m: ←[34mapp.test.background_task('100')←[39;49;00m (fad0b6b8-63a7-4213-a02a-edd40e41b284)
01:27:16 ←[31mWorker rq:worker:e78a93050f0f400f9f2e75ce25fb2f63: found an unhandled exception, quitting...←[39;49;00m
Traceback (most recent call last):
  File &quot;C:\Users\Username\Desktop\test_app\env\lib\site-packages\rq\worker.py&quot;, line 606, in work
    self.execute_job(job, queue)
  File &quot;C:\Users\Username\Desktop\test_app\env\lib\site-packages\rq\worker.py&quot;, line 866, in execute_job
    self.fork_work_horse(job, queue)
  File &quot;C:\Users\Username\Desktop\test_app\env\lib\site-packages\rq\worker.py&quot;, line 770, in fork_work_horse
    child_pid = os.fork()
AttributeError: module 'os' has no attribute 'fork'
01:27:16 Unsubscribing from channel rq:pubsub:e78a93050f0f400f9f2e75ce25fb2f63
</code></pre>
<p><strong>Folder structure and files</strong></p>
<pre><code>test_app/
          app/
                __init__.py
                test.py
          run.py
          env/
</code></pre>
<p><strong>run.py</strong></p>
<pre><code>from app import app

if __name__ == &quot;__main__&quot;:
    app.run()
</code></pre>
<p><strong><strong>init</strong>.py</strong></p>
<pre><code>from flask import Flask
app = Flask(__name__)

from app import test
</code></pre>
<p><strong>test.py</strong></p>
<pre><code>from flask import Flask, request
from app import app
import redis
from rq import Queue
import time

r = redis.Redis()
q = Queue(connection=r)

def background_task(n):

    &quot;&quot;&quot; Function that returns len(n) and simulates a delay &quot;&quot;&quot;

    delay = 2

    print(&quot;Task running&quot;)
    print(f&quot;Simulating a {delay} second delay&quot;)

    time.sleep(delay)

    print(len(n))
    print(&quot;Task complete&quot;)

    return len(n)

@app.route(&quot;/task&quot;)
def index():

    if request.args.get(&quot;n&quot;):

        job = q.enqueue(background_task, request.args.get(&quot;n&quot;))

        return f&quot;Task ({job.id}) added to queue at {job.enqueued_at}&quot;

    return &quot;No value for count provided&quot;
</code></pre>
",29,0,0,2,python;flask,2022-06-09 05:26:54,2022-06-09 05:26:54,2022-06-09 05:26:54,introduction i m using windows  so i had to install redis on a linux subsystem  i ve used ubuntu for this  my code editor is vscode  i get a module  os  has no attribute  fork  error when i try to add a new task to the queue  error details anyway  when i run the rq worker command in my terminal  i get this  which looks correct   i then run the flask run command in a second terminal  and then visits http        task n  in my browser  the browser returns task  fadbb a  aa eddeb  added to queue at         which looks correct   however  i get this error in my first terminal  folder structure and files run py init py test py
305,305,15589661,72547638,Python MemoryError when fitting scikit-learn model,"<p>For a research project, I am analyzing correlations using various machine learning algorithms. As such, I run the following code (simplified for demonstration):</p>
<pre><code># Make a custom scorer for pearson's r (from scipy)    
scorer = lambda regressor, X, y: pearsonr(regressor.predict(X), y)[0]

# Create a progress bar
progress_bar = tqdm(14400)

# Initialize a dataframe to store scores
df = pd.DataFrame(columns=[&quot;data&quot;, &quot;pipeline&quot;, &quot;r&quot;])

# Loop over datasets
for data in datasets: #288 datasets
    X_train = data.X_train
    X_test = data.X_test
    y_train = data.y_train
    y_test = data.y_test
    
    # Loop over pipelines
    for pipeline in pipelines: #50 pipelines
        scores = cross_val_score(pipeline, X_train, y_train, cv=int(len(X_train)/3), scoring=scorer)
        r = scores.mean()
        # Create a new row to save data
        df.loc[(df.last_valid_index() or 0) + 1] = {&quot;data&quot;: data.name, &quot;pipeline&quot;: pipeline, &quot;r&quot;: r}
        progress_bar.update(1)

progress_bar.close()
    
</code></pre>
<p>X_train is a pandas dataframe with shape (20, 34)</p>
<p>X_test is a pandas dataframe with shape (9, 34)</p>
<p>y_train is pandas series with length 20</p>
<p>y_test is a pandas series with length 9</p>
<p>An example of pipeline is:</p>
<pre><code>Pipeline(steps=[('scaler', StandardScaler()),
                ('poly', PolynomialFeatures(degree=9)),
                ('regressor', LinearRegression())])
</code></pre>
<p>However, after approximately 8700 iterations (total), I get the following MemoryError:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-9ff48105b8ff&gt; in &lt;module&gt;
     40                 y = targets[label]
     41                 #Finally, we can test the correlation
---&gt; 42                 scores = cross_val_score(regressor, X_train, y.loc[train_indices], cv=int(len(X_train)/3), scoring=lambda regressor, X, y: pearsonr(regressor.predict(X), y)[0]) #Three samples per test set, as that seems like the logical minimum for Pearson
     43                 r = scores.mean()
     44 #                     print(f&quot;{regressor} was able to predict {label} based on the {band} band of the {network} network with a Pearson's r of {r} of the data that could be explained.\n&quot;)

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    513     scorer = check_scoring(estimator, scoring=scoring)
    514 
--&gt; 515     cv_results = cross_validate(
    516         estimator=estimator,
    517         X=X,

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    283     )
    284 
--&gt; 285     _warn_or_raise_about_fit_failures(results, error_score)
    286 
    287     # For callabe scoring, the return type is only know after calling. If the

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in _warn_or_raise_about_fit_failures(results, error_score)
    365                 f&quot;Below are more details about the failures:\n{fit_errors_summary}&quot;
    366             )
--&gt; 367             raise ValueError(all_fits_failed_message)
    368 
    369         else:

ValueError: 
All the 6 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
2 fits failed with the following error:
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\pipeline.py&quot;, line 382, in fit
    self._final_estimator.fit(Xt, y, **fit_params_last_step)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py&quot;, line 692, in fit
    X, y, X_offset, y_offset, X_scale = _preprocess_data(
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py&quot;, line 262, in _preprocess_data
    X = check_array(X, copy=copy, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;], dtype=FLOAT_DTYPES)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py&quot;, line 925, in check_array
    array = np.array(array, dtype=dtype, order=order)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 41.8 GiB for an array with shape (16, 350343565) and data type float64

--------------------------------------------------------------------------------
4 fits failed with the following error:
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\pipeline.py&quot;, line 382, in fit
    self._final_estimator.fit(Xt, y, **fit_params_last_step)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py&quot;, line 692, in fit
    X, y, X_offset, y_offset, X_scale = _preprocess_data(
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py&quot;, line 262, in _preprocess_data
    X = check_array(X, copy=copy, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;], dtype=FLOAT_DTYPES)
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py&quot;, line 925, in check_array
    array = np.array(array, dtype=dtype, order=order)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 44.4 GiB for an array with shape (17, 350343565) and data type float64
</code></pre>
<p>What can I do to prevent this error, and how did it originate in the first place? I tried using sklearn's clone function on the pipeline that was still in my memory, and then calling fit, but I got the same error. However, when I created a new pipeline (still in the same session), and called fit on it, it did work.</p>
",65,2,0,3,python;numpy;scikit-learn,2022-06-08 20:07:10,2022-06-08 20:07:10,2022-06-09 05:24:46,for a research project  i am analyzing correlations using various machine learning algorithms  as such  i run the following code  simplified for demonstration   x_train is a pandas dataframe with shape      x_test is a pandas dataframe with shape      y_train is pandas series with length  y_test is a pandas series with length  an example of pipeline is  however  after approximately  iterations  total   i get the following memoryerror  what can i do to prevent this error  and how did it originate in the first place  i tried using sklearn s clone function on the pipeline that was still in my memory  and then calling fit  but i got the same error  however  when i created a new pipeline  still in the same session   and called fit on it  it did work 
306,306,19290090,72543891,Lowered accuracy when deploying a model to Docker,"<p>I am deploying a Machine Learning Model on Docker and I’ve run into some problems.
When I run the model locally and make predictions, the model gives predictions
with an accuracy of 0.8.</p>
<p><em>Local example execution</em><br />
<a href=""https://i.stack.imgur.com/wGTGQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wGTGQ.png"" alt=""Local example execution."" /></a></p>
<p>But when I take the model to Docker, it has happened to me that the accuracy of
the predictions has gone from a 0.8 to a 0.3 approximately, despite being the
same model with the same weights.</p>
<p><em>Docker execution example loading the weights with the Keras functions</em><br />
<a href=""https://i.stack.imgur.com/nmipf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nmipf.png"" alt=""Docker execution example loading the weights with the Keras functions."" /></a></p>
<p>I tried to compress the weights and decompress them inside the container,
thinking it was a problem when copying the weights. But the result obtained was
the same, 0.3 accuracy.</p>
<p>I have also tried to train the model itself inside the Docker container, but the
results I have obtained have been the same. The accuracy in predictions was 0.3</p>
<p>In addition, I have tried to save the weights with Pickle, copy them in the
container and load them in the model. For some reason, this procedure has worked
for me and has given accuracy results of approximately 0.78.</p>
<p><em>Docker execution example loading the weights with pickle</em><br />
<a href=""https://i.stack.imgur.com/nu4Jy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nu4Jy.png"" alt=""Docker execution example loading the weights with pickle."" /></a></p>
<p>I am using Docker with the following versions: v20.10.13 and v20.10.14.</p>
<p>The Dockerfile file I’m using is as follows</p>
<pre><code>FROM continuumio/anaconda3

ENV APP_HOME /modelo_docker
WORKDIR $APP_HOME
COPY . $APP_HOME

RUN pip install --upgrade pip

RUN apt-get update
RUN apt-get install ffmpeg libsm6 libxext6  -y

RUN pip install -r requirements.txt

CMD [&quot;python&quot;, &quot;embeddedtnet.py&quot;]
</code></pre>
<p>And the versions of the libraries I’m using are as follows</p>
<pre><code>uvicorn==0.17.6
fastapi==0.78.0
tensorflow==2.8.1
tensorflow-text==2.8.1
seaborn==0.11.2
scikit-learn==1.1.1
art==5.6
matplotlib==3.5.2
opencv-python==4.5.5.64
pandas==1.4.2
waitress==2.1.1
tensorflow-addons==0.17.0
pytesseract
nltk==3.7
requests==2.27.1
numpy==1.22.4
contractions==0.1.72
unidecode==1.3.4
protobuf~=3.19.0
</code></pre>
<p>These are the layers of the model:</p>
<pre><code>class LSTMBLock(tf.keras.Model):
def __init__(self, vocabulary):
    super(LSTMBLock, self).__init__()

    self.vectorization_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(
        max_tokens=2000, standardize=None, output_mode='int', output_sequence_length=200, name='vectorization_layer')

    self.vectorization_layer.set_vocabulary(vocabulary)

    self.from_ragged_to_dense = tf_text.keras.layers.ToDense(
        pad_value=0, mask=True)

    self.embedding = tf.keras.layers.Embedding(
        input_dim=len(vocabulary), output_dim=200, mask_zero=True)

    self.maxpooling = tf.keras.layers.MaxPooling1D(pool_size=3)

    self.conv1 = tf.keras.layers.Conv1D(
        filters=32, kernel_size=3, padding='same', activation='relu')

    self.spadrop = tf.keras.layers.SpatialDropout1D(0.25)

    self.bidirectionalLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation='tanh', recurrent_activation='sigmoid',recurrent_dropout=0, dropout=0.25, kernel_initializer='glorot_uniform', return_sequences=True),
                                                           merge_mode='concat', name=&quot;bidirectional&quot;)

    self.conv2 = tf.keras.layers.Conv1D(
        filters=32, kernel_size=3, padding='same', activation='relu')

    self.flatten = tf.keras.layers.Flatten()

def call(self, inputs: List[str]) -&gt; tf.Tensor:

    x = self.vectorization_layer(inputs)
    x = self.from_ragged_to_dense(x)
    x = self.embedding(x)
    x = self.maxpooling(x)
    x = self.conv1(x)
    x = self.spadrop(x)
    x = self.bidirectionalLSTM(x)
    x = self.conv2(x)
    x = self.flatten(x)

    return x
</code></pre>
<p>So far, I haven’t found the reason why this is happening and neither why it has worked with Pickle.</p>
",48,0,1,5,python;docker;tensorflow;machine-learning;keras,2022-06-08 15:41:33,2022-06-08 15:41:33,2022-06-09 04:53:54,i am using docker with the following versions  v   and v    the dockerfile file i m using is as follows and the versions of the libraries i m using are as follows these are the layers of the model  so far  i haven t found the reason why this is happening and neither why it has worked with pickle 
307,307,19018475,72548388,Is there a machine learning or NLP model to separate questions and answers in raw text?,"<p>I have some raw text that has questions and answers in it.  I would like to identify which parts of the text are questions and which parts are the answers.  This seems like it would be easy, but the questions aren't necessarily terminated with question marks. The only thing I know for sure is that after a question is over the answer begins, and after the answer is over another question begins, but there is no consistent format on how many \n are included in the answers.  A question is definitely its own paragraph though.</p>
<p>I'm hoping for some sort of pre-trained model for this?</p>
<p>One possibility would be to take some existing data, manually tag each paragraph as q vs a and then use google's universal sentence encoder for each paragraph to get the 512 dimension output and then use that as the input to train a neural net or some other classification model on the labeled data.  I'm hoping to avoid this path because I don't want to manually tag a few thousand paragraphs, and after all that work, who knows if the model will have a decent classification error.</p>
<p>Another possibility is to use something like gpt3: feed it the entire text and just ask it what are the questions/requests.  The problem with this is that the gpt3 api is still a bit sandboxed.  I tried a sample on the gpt3 playground and it only identified 80% of the questions.</p>
<p>Any other suggestions?</p>
<p>To give you an idea, the text may look like this:</p>
<p>What is the name of the company?</p>
<p>We are Acme Inc.</p>
<p>How many employees are there.</p>
<p>There are 50 employees.</p>
<p>Describe a day in the life of an employee.</p>
<p>An employee arrives at 9am.</p>
<p>Then they go to the factory and make widgets for 4 hours.  After making widgets they eat lunch and then go to the QA engineer to make sure their widgets are good enough.</p>
<p>After QA, they write a report about how many widgets they made.</p>
<p>Most employees leave around 5pm.</p>
<p>List the pay range of your employees.</p>
<p>The starting salary is $22/hours.</p>
<p>After 1 year pay increases to $25 an hour and then increases 3% per year.</p>
<p>Contact information:</p>
<p>Acme Inc</p>
<p>123 Main Street</p>
<p>Anyplace, USA</p>
",32,1,0,2,machine-learning;nlp,2022-06-08 20:57:58,2022-06-08 20:57:58,2022-06-09 03:38:55,i have some raw text that has questions and answers in it   i would like to identify which parts of the text are questions and which parts are the answers   this seems like it would be easy  but the questions aren t necessarily terminated with question marks  the only thing i know for sure is that after a question is over the answer begins  and after the answer is over another question begins  but there is no consistent format on how many  n are included in the answers   a question is definitely its own paragraph though  i m hoping for some sort of pre trained model for this  one possibility would be to take some existing data  manually tag each paragraph as q vs a and then use google s universal sentence encoder for each paragraph to get the  dimension output and then use that as the input to train a neural net or some other classification model on the labeled data   i m hoping to avoid this path because i don t want to manually tag a few thousand paragraphs  and after all that work  who knows if the model will have a decent classification error  another possibility is to use something like gpt  feed it the entire text and just ask it what are the questions requests   the problem with this is that the gpt api is still a bit sandboxed   i tried a sample on the gpt playground and it only identified   of the questions  any other suggestions  to give you an idea  the text may look like this  what is the name of the company  we are acme inc  how many employees are there  there are  employees  describe a day in the life of an employee  an employee arrives at am  then they go to the factory and make widgets for  hours   after making widgets they eat lunch and then go to the qa engineer to make sure their widgets are good enough  after qa  they write a report about how many widgets they made  most employees leave around pm  list the pay range of your employees  the starting salary is   hours  after  year pay increases to   an hour and then increases   per year  contact information  acme inc  main street anyplace  usa
308,308,16091823,72550983,how to weight imbalance class for ordered logistic classification machine learning,"<p>I have a question about predicting wine quality (classic wine dataset from UCI). I use ordered logistic classification in PLSR after joining white and red in the same dataset. There are imbalanced numbers between white and red. I have tried SMOTE but it is not ideal, there is a lot of weight methods, which one should I choose? Thanks!!!! Any references will be fantastic!</p>
",13,0,0,4,r;machine-learning;multiclass-classification;pls,2022-06-09 00:37:51,2022-06-09 00:37:51,2022-06-09 00:37:51,i have a question about predicting wine quality  classic wine dataset from uci   i use ordered logistic classification in plsr after joining white and red in the same dataset  there are imbalanced numbers between white and red  i have tried smote but it is not ideal  there is a lot of weight methods  which one should i choose  thanks     any references will be fantastic 
309,309,19283252,72548927,Postgres 14.3 created user authentication failed,"<p>I am new to this SQL stuff and I recently installed Postgres 14.3 on my windows machine as part of an online learning requirement. I created a database and a user to connect to the database in the following lines from the shell:</p>
<pre class=""lang-none prettyprint-override""><code>postgres=# create database staff;
postgres=# create user Naruto with encrypted password 'secret';
postgres=# grant all privileges on database staff to Naruto;
postgres=# \c staff Naruto;

password for user Naruto:
</code></pre>
<p>After inputting the password I get an error message like this</p>
<blockquote>
<p>connection to server at &quot;local host&quot; (127.0.0.1), port 5432 failed: FATAL: password authentication failed for user &quot;Naruto&quot;
Previous connection kept</p>
</blockquote>
<p>Whereas the video description from which I am taking tutorials didn't ask for a password prompt but it connected to the database straight up with the designated user.</p>
<p>I have tried numerous suggestions on stack overflow but still, no breakthrough in any way. I'd appreciate any hint because I haven't recorded any progress with my learning recently. Thanks!</p>
",32,2,0,1,postgresql,2022-06-08 21:37:23,2022-06-08 21:37:23,2022-06-08 23:36:23,i am new to this sql stuff and i recently installed postgres   on my windows machine as part of an online learning requirement  i created a database and a user to connect to the database in the following lines from the shell  after inputting the password i get an error message like this whereas the video description from which i am taking tutorials didn t ask for a password prompt but it connected to the database straight up with the designated user  i have tried numerous suggestions on stack overflow but still  no breakthrough in any way  i d appreciate any hint because i haven t recorded any progress with my learning recently  thanks 
310,310,10357734,52310993,Error is returned while using getURL() function in R language,"<p>i have started learning data science and new to R language, 
i am trying to read data from below <em>HTTPS URL using getURL funtion and Rcurl pacakge.</em></p>

<p>while executing below code, receiving <code>SSL</code> protocal issue.</p>

<h1>R Code</h1>

<h1>load the library Rcurl</h1>

<p>library(RCurl)</p>

<h1>specify the URL for the Iris data CSV</h1>

<pre><code>urlfile = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
</code></pre>

<h1>download the file</h1>

<pre><code>downloaded = getURL(urlfile, ssl.verifypeer=FALSE)
</code></pre>

<p>Error </p>

<blockquote>
  <p>Error in function (type, msg, asError = TRUE)  :    Unknown SSL
  protocol error in connection to archive.ics.uci.edu:443</p>
</blockquote>

<p>can anyone help me with this answer?</p>
",2824,1,3,1,r,2018-09-13 15:14:14,2018-09-13 15:14:14,2022-06-08 22:06:56,while executing below code  receiving ssl protocal issue  library rcurl  error  can anyone help me with this answer 
311,311,19299397,72547300,A machine learning model to distinguish numeric columns,"<p>I’m trying to build a model to recognize fields, based on their values.</p>
<p>As you can see in the picture, I build a model to recognize which field is Rubrique and that works just fine. I took a list of the words that it contains which are specific to this columns, and based on that I built the model.</p>
<p>Now as you can see I have other fields which are numeric and somehow similar. I don’t know what approach should be used in order to classify them.
<a href=""https://i.stack.imgur.com/GFJFO.jpg"" rel=""nofollow noreferrer"">Show Image</a></p>
",14,0,-1,1,machine-learning,2022-06-08 19:45:03,2022-06-08 19:45:03,2022-06-08 20:15:44,i m trying to build a model to recognize fields  based on their values  as you can see in the picture  i build a model to recognize which field is rubrique and that works just fine  i took a list of the words that it contains which are specific to this columns  and based on that i built the model 
312,312,18193186,72546508,Machine Learning Ensemble,"<p>I am a newbie to machine learning, and I wonder if averaging the result of two models(different algorithm) trained by the same data a good idea.</p>
<p>For example(regression problem):</p>
<pre><code>Result of XGBoost: 1, 2, 3
Result of Random Forest: 3, 2, 1
</code></pre>
<p>use a*(1+3)+b*(2+2)+c*(3+1) as my final result, a+b+c=1</p>
<p>If this approach is effective, how do I decide the weight of every model? Thanks in advance!</p>
",19,0,-1,1,machine-learning,2022-06-08 18:56:21,2022-06-08 18:56:21,2022-06-08 18:56:21,i am a newbie to machine learning  and i wonder if averaging the result of two models different algorithm  trained by the same data a good idea  for example regression problem   use a     b     c     as my final result  a b c  if this approach is effective  how do i decide the weight of every model  thanks in advance 
313,313,12135090,72545450,model.fit gives me Graph execution error. How do I solve?,"<p>I am new to image processing and machine learning in python. I have been trying to execute a model in google colab using inceptionv3 but i am stuck at fitting the model.</p>
<pre><code>r = model.fit(
    training_set,
    validation_data=test_set,
    epochs=10,
    steps_per_epoch=len(training_set),
    validation_steps=len(test_set)
    )
</code></pre>
<p>it is showing me the below errors</p>
<pre><code>Epoch 1/10
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
&lt;ipython-input-24-c27d8fba63ce&gt; in &lt;module&gt;()
      6   epochs=10,
      7   steps_per_epoch=len(training_set),
----&gt; 8   validation_steps=len(test_set)
      9 )

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

UnimplementedError: Graph execution error:

Detected at node 'model/conv2d/Conv2D' defined at (most recent call last):
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
      &quot;__main__&quot;, mod_spec)
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
      exec(code, run_globals)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py&quot;, line 16, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py&quot;, line 846, in launch_instance
      app.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py&quot;, line 499, in start
      self.io_loop.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 132, in start
      self.asyncio_loop.run_forever()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 541, in run_forever
      self._run_once()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 1786, in _run_once
      handle._run()
    File &quot;/usr/lib/python3.7/asyncio/events.py&quot;, line 88, in _run
      self._context.run(self._callback, *self._args)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 122, in _handle_events
      handler_func(fileobj, events)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 577, in _handle_events
      self._handle_recv()
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 606, in _handle_recv
      self._run_callback(callback, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 556, in _run_callback
      callback(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 283, in dispatcher
      return self.dispatch_shell(stream, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 233, in dispatch_shell
      handler(stream, idents, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 399, in execute_request
      user_expressions, allow_stdin)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py&quot;, line 208, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py&quot;, line 537, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2718, in run_cell
      interactivity=interactivity, compiler=compiler, result=result)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2822, in run_ast_nodes
      if self.run_code(code, result):
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2882, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;&lt;ipython-input-20-8cba7706098f&gt;&quot;, line 12, in &lt;module&gt;
      validation_steps=validation_data
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1051, in train_function
      return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1030, in run_step
      outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 889, in train_step
      y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 490, in __call__
      return super().__call__(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 459, in call
      inputs, training=training, mask=mask)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 596, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional/base_conv.py&quot;, line 250, in call
      outputs = self.convolution_op(inputs, self.kernel)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional/base_conv.py&quot;, line 232, in convolution_op
      name=self.__class__.__name__)
Node: 'model/conv2d/Conv2D'
DNN library is not found.
     [[{{node model/conv2d/Conv2D}}]] [Op:__inference_train_function_12299]
</code></pre>
<p>the whole code is in my git repository:  <a href=""https://github.com/Aditya757/MyRepository.git"" rel=""nofollow noreferrer"">https://github.com/Aditya757/MyRepository.git</a></p>
<p>the image of the dataset is here: <a href=""https://i.stack.imgur.com/jWaJ8.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/jWaJ8.png</a></p>
",55,0,0,3,python;tensorflow;machine-learning,2022-06-08 17:38:52,2022-06-08 17:38:52,2022-06-08 17:38:52,i am new to image processing and machine learning in python  i have been trying to execute a model in google colab using inceptionv but i am stuck at fitting the model  it is showing me the below errors the whole code is in my git repository    the image of the dataset is here  
314,314,14794604,72491832,TBPTT with a multivariate time series,"<p>I am trying to use TBPTT on a multivariate time series, and I am facing a problem, my loss doesn’t decrease, and I don’t know what I am doing wrong.</p>
<p>Inputs shape (Batch_size,1270,6)
Output shape (Batch_size,1270)</p>
<p>There is a particularity with the Inputs:</p>
<ol>
<li><p>6 Features correspond to A-B A-C A-D where A is the time step,</p>
</li>
<li><p>Between two inputs (Inputs[0] and Inputs[1]) features don’t have the same length, I padded all the Inputs using
torch.nn.utils.rnn.pad_sequence(Mise_en_donnees,padding_value=-1,batch_first=True)
I tried padding_value=0. But it doesn’t change anything)</p>
</li>
<li><p>All Inputs are normalized using get_mean_std</p>
</li>
</ol>
<pre><code>def get_mean_std(loader,ignore_idx=-1.):
    channels_sum,channels_squared_sum,num_batches=0,0,0
    for data in loader:
        a=torch.sum((data[:,0]!=ignore_idx)).item()-1
        channels_sum+=torch.mean(data[:a],dim=[0])
        channels_squared_sum+=torch.mean(data[:a]**2,dim=[0])
        num_batches+=1
    mean=channels_sum/num_batches
    std=(channels_squared_sum/num_batches -mean**2)**0.5
    return mean,std
</code></pre>
<p>There is my Model</p>
<pre><code>    #A classic Conv_Block
class conv_block (nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(conv_block, self).__init__()
        self.relu = nn.LeakyReLU()
        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)
        self.batchnorm = nn.BatchNorm1d(out_channels)
        

    def forward(self, x):
        x=self.conv(x)
        x= self.batchnorm(x)
        return self.relu(x)


class Test (nn.Module):
    def __init__(self,in_channels,num_layers,hidden_size, p,out_size):
        super(Test ,self).__init__()
        
        self.CNN=nn.Sequential(
           #I am trying to apply filters on every two columns (A-B  A-C  A-D) using groups
            conv_block(in_channels,3,kernel_size=2,stride=1,padding=1,groups=3),#,padding_mode=&quot;reflect&quot;), 
            conv_block(3,32,kernel_size=2,stride=1,padding=0),
            #SqueezeExcitation(32,16), #i tried but same results
            conv_block(32,16,kernel_size=3,stride=1,padding=1),
            conv_block(16,8,kernel_size=3,stride=1,padding=1),
           
        )

        self.rnn = nn.LSTM(8, hidden_size, num_layers)
        self.rnn1 = nn.LSTM(hidden_size, hidden_size, num_layers)
        #self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size) # in case of using bidirectional 
        #self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(p)
        self.num_layers=num_layers
        self.fc_f=nn.Linear(out_size*hidden_size,out_size)
        
    def forward(self,x,hidden, cell):
        x=x.permute(0,2,1)
        x=self.CNN(x)
        x=x.permute(2,0,1)
        x, (hidden, cell) = self.rnn(x) #i tried bidirectional but same results
        #hidden = self.dropout(self.fc_hidden(torch.cat((hidden[0:self.num_layers], hidden[self.num_layers:2*self.num_layers]), dim=2)))
        #cell = self.dropout(self.fc_cell(torch.cat((cell[0:self.num_layers], cell[self.num_layers:2*self.num_layers]), dim=2)))
        x, (hidden, cell) = self.rnn1(x, (hidden, cell))
        #hidden=hidden.repeat(2,1,1)
        #cell=cell.repeat(2,1,1)
        x=x.permute(1,0,2)
        x=x.reshape(x.shape[0],-1)
        x=self.fc_f(x) #final result

        return x, hidden, cell

#hyperparameters

in_channels=6
num_layers=64 
hidden_size=90  
p=0.2
out_size=tbptt_steps=20 #truncated bptt steps
split_dim=1
nb_epoch=100
learning_rate=3e-4

Model=Test(in_channels,num_layers,hidden_size, p,out_size).to(device)
optimizer = optim.Adam(Model.parameters(), lr=learning_rate)

# I tired to test my model on the same inputs

X=Inputs[:5,:500,:-1].to(device)
Y=Inputs[:5,:500,-1].to(device)

#training loop

hidden=None
cell=None

for ep in range (nb_epoch):

    Losses=0
    for i, (x_, y_) in enumerate(zip(X.split(tbptt_steps, dim=split_dim), Y.split(tbptt_steps, dim=split_dim))):
        optimizer.zero_grad()
        #Model.train()

        # Detach last hidden state, so the backprop-graph will be cut
        if hidden is not None:
            hidden.detach_()
        if cell is not None:
            cell.detach_()

        # Forward path
        y_pred, hidden, cell = Model(x_, hidden, cell)
        #print(&quot;predict&quot;,y_pred.shape,y_.shape)
        # Compute loss
        loss = nn.functional.mse_loss(y_, y_pred)
        # Backward path
        loss.backward()
        Losses+=loss.item()
        # Update weights
        optimizer.step()
        if i==0:
            print(&quot;Epoch &quot;,ep,&quot; Loss &quot;,loss.item())
        
    print(&quot;#################################################&quot;)
    print(Losses)
    print(&quot;#################################################&quot;)
      
</code></pre>
<p>There is two problems with this Model:</p>
<ul>
<li>It doesn’t catch the padding_value
-The loss is high and didn’t decrease</li>
</ul>
<p>I really hope that the Model is understandable, and we will correct it.
As you can see I am not a professional in Machine learning, I am really eager to understand more about my errors .</p>
<p>Thank you very much for your help</p>
",15,0,0,2,pytorch;lstm,2022-06-03 20:55:30,2022-06-03 20:55:30,2022-06-08 17:19:01,i am trying to use tbptt on a multivariate time series  and i am facing a problem  my loss doesn t decrease  and i don t know what i am doing wrong  there is a particularity with the inputs   features correspond to a b a c a d where a is the time step  all inputs are normalized using get_mean_std there is my model there is two problems with this model  thank you very much for your help
315,315,9628339,65016881,Using scikit-learn on Databricks,"<p>Scikit-Learn algorithms are single node implementations. Does this mean, that they are not an appropriate choice for building machine learning models on Databricks cluster for the reason that they cannot take advantage of the cluster computing resources ?</p>
",202,3,1,4,machine-learning;scikit-learn;databricks;azure-databricks,2020-11-26 11:38:26,2020-11-26 11:38:26,2022-06-08 15:57:05,scikit learn algorithms are single node implementations  does this mean  that they are not an appropriate choice for building machine learning models on databricks cluster for the reason that they cannot take advantage of the cluster computing resources  
316,316,19282134,72521399,cross-domain analysis fake news detection,"<p>I am new to machine learning and I am doing a project for my dissertation on fake news detection. I am doing a cross-domain analysis. Training classifier on one dataset and testing it on another dataset. But for that first, I have to check the similarity of two different datasets and I cannot find a method on how to do that.</p>
<p>I would appreciate it if someone could answer my question simply, without the use of any complex terms and high-end detail.</p>
",29,0,-1,3,machine-learning;nlp;cross-domain,2022-06-06 23:00:27,2022-06-06 23:00:27,2022-06-08 15:11:29,i am new to machine learning and i am doing a project for my dissertation on fake news detection  i am doing a cross domain analysis  training classifier on one dataset and testing it on another dataset  but for that first  i have to check the similarity of two different datasets and i cannot find a method on how to do that  i would appreciate it if someone could answer my question simply  without the use of any complex terms and high end detail 
317,317,18193889,72542723,How does a colon calculate in a function,"<p>I am reading Hands-on Machine Learning and I had stumbled upon this code, you can find the code <a href=""https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>from sklearn.base import BaseEstimator, TransformerMixin

# column index
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
</code></pre>
<pre><code># How is this being calculated internally
X[:, rooms_ix] / X[:, households_ix]
</code></pre>
<p>I want to know what does the above do, especially what does the <code>X</code> and <code>:</code> do behind the scenes.</p>
",48,0,0,3,python;math;scikit-learn,2022-06-08 14:18:27,2022-06-08 14:18:27,2022-06-08 14:21:45,i am reading hands on machine learning and i had stumbled upon this code  you can find the code  i want to know what does the above do  especially what does the x and   do behind the scenes 
318,318,17915836,72542615,Rllib &amp; Remote Desktop Connection: [Errno 10054] An existing connection was forcibly closed by the remote host,"<p><strong>The general setting:</strong></p>
<p>I am currently running several rllib processes on the machine at my workplace. The goal is to run multi-agent reinforcement learning simulations with a varying number of agents, differing start states and three different configurations. The computer only has the capacity to run one process at a time, which is why I loop through all configurations to allow for overnight and weekend simulations.</p>
<p><strong>The Problem:</strong></p>
<p>To oversee the process (e.g. if there are any problems with disk space) off work I have configured a remote desktop connection. Now after approximately 6-8 hours of running following error message is produced:</p>
<pre><code>2022-06-08 08:01:04,008 ERROR worker.py:1259 -- listen_error_messages_raylet: [WinError 10054] Eine vorhandene Verbindung wurde vom Remotehost geschlossen
2022-06-08 08:01:04,008 ERROR worker.py:488 -- print_logs: [WinError 10054] Eine vorhandene Verbindung wurde vom Remotehost geschlossen
[2022-06-08 08:02:04,321 C 4548 1908] gcs_client.cc:343: Couldn't reconnect to GCS server. The last attempted GCS server address was 127.0.0.1:61369
*** StackTrace Information ***
Windows fatal exception: access violation
</code></pre>
<p><em>Translation: Eine vorhandene Verbindung wurde vom Remotehost geschlossen =&gt; An existing connection was forcibly closed by the remote host</em></p>
<p><strong>The code:</strong></p>
<p>Here is my code, even though I believe that it must have something to do with the Windows Remote Desktop connection or with the GCS server (no idea what this is or what I need it for actually):</p>
<pre><code>def main(debug, framework=&quot;tf&quot;):
    n_agents = [8, 16, 32, 64]
    init_state = [0.1, 0.25, 0.5, 1., 2.]
    tax = [&quot;none&quot;, &quot;vote&quot;, &quot;central&quot;]

    for n in n_agents:
        for s in init_state:
            for t in tax:
                shutdown()

                register_env(args.env_name, lambda cnfg: ParallelPettingZooEnv(env_creator(cnfg)))

                train_n_replicates = 1 if debug else 10
                seeds = list(range(train_n_replicates))

                ray.init(num_cpus=os.cpu_count(), num_gpus=0, local_mode=debug)

                rllib_config, stop_config = get_rllib_config(seeds=seeds,
                                                             n_agents=n,
                                                             init_state=s,
                                                             tax=t,
                                                             debug=debug,
                                                             framework=framework)

                # Define logger to use (e.g. which output formats)
                custom_logger = LifecycleLoggerCallback(
                    logger_classes=[CSVLogger, TBXLogger],
                )

                log_dir = os.path.join(os.getcwd(), &quot;run_configurations/checkpoints&quot;)

                tune_analysis = tune.run(
                    args.run,
                    config=rllib_config,
                    stop=stop_config,
                    checkpoint_freq=0,
                    checkpoint_at_end=True,
                    name=args.experiment_name,
                    local_dir=log_dir,
                    callbacks=[custom_logger],
                    trial_name_creator=trial_str_creator,
                    raise_on_failed_trial=False,
                )
                ray.shutdown()

                for i in range(len(tune_analysis.trials)):
                    results_path = os.path.join(log_dir, args.experiment_name, str(tune_analysis.trials[i].logdir))
                    results_to_csv(results_path)


if __name__ == &quot;__main__&quot;:
    debug_mode = False
    args = parser.parse_args()
    main(debug_mode, args.framework)
</code></pre>
<p>P.S.: I run the PPO RLlib-registered algorithm.</p>
<p>Grateful for any tips. Cheers :)</p>
",12,0,0,5,windows;remote-desktop;gcs;rllib;ray-tune,2022-06-08 14:10:25,2022-06-08 14:10:25,2022-06-08 14:10:25,the general setting  i am currently running several rllib processes on the machine at my workplace  the goal is to run multi agent reinforcement learning simulations with a varying number of agents  differing start states and three different configurations  the computer only has the capacity to run one process at a time  which is why i loop through all configurations to allow for overnight and weekend simulations  the problem  to oversee the process  e g  if there are any problems with disk space  off work i have configured a remote desktop connection  now after approximately   hours of running following error message is produced  translation  eine vorhandene verbindung wurde vom remotehost geschlossen   gt  an existing connection was forcibly closed by the remote host the code  here is my code  even though i believe that it must have something to do with the windows remote desktop connection or with the gcs server  no idea what this is or what i need it for actually   p s   i run the ppo rllib registered algorithm  grateful for any tips  cheers   
319,319,18429257,72542382,Generating pipeline using sklearn in python,"<p>I am trying out code from Aurelien Geron's book 'Hands-on machine learning'. The part on preparing data for ML algos has the following code on transformation pipelines:</p>
<pre><code>from sklearn.pipeline import FeatureUnion 
num_attribs = list(housing_num)
cat_attribs = [&quot;ocean_proximity&quot;]

num_pipeline = pipeline([
('selector', DataFrameSelector(num_attribs)), ('imputer', Imputer(strategy=&quot;median&quot;)), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()),
])

cat_pipeline = pipeline([
('selector', DataFrameSelector(cat_attribs)), ('label_binarizer', LabelBinarizer()),
])

full_pipeline = FeatureUnion(transformer_list=[ (&quot;num_pipeline&quot;, num_pipeline), (&quot;cat_pipeline&quot;, cat_pipeline),
])
</code></pre>
<p>As I run this, I get an error 'name 'pipeline' is not defined'. How do I overcome this?</p>
",31,0,0,3,python;scikit-learn;pipeline,2022-06-08 13:51:58,2022-06-08 13:51:58,2022-06-08 13:51:58,i am trying out code from aurelien geron s book  hands on machine learning   the part on preparing data for ml algos has the following code on transformation pipelines  as i run this  i get an error  name  pipeline  is not defined   how do i overcome this 
320,320,14431425,72530367,Azure machine learning studio Pipeline run: ModuleNotFoundError: No module named &#39;pyodbc&#39;,"<p>Scenario:</p>
<p>1.I am using Machine learning studio for creating machine learning pipeline and when I am trying to call the .py file which has below code:</p>
<pre><code>import os
os.system(f&quot;pip install pandas&quot;)
os.system(f&quot;pip install scikit-learn&quot;)
os.system(f&quot;pip install pyodbc&quot;)
os.system(f&quot;pip install SQLAlchemy&quot;)

import glob
import json
import pandas as pd
from sklearn import preprocessing
import logging
import os
import sys
import pyodbc
import urllib
from sqlalchemy.pool import NullPool
import sqlalchemy
</code></pre>
<p>and when I am trying to create and run pipeline from note book getting error:</p>
<pre><code>    Collecting pyodbc
    Downloading pyodbc-4.0.32.tar.gz (280 kB)
    Building wheels for collected packages: pyodbc
    Building wheel for pyodbc (setup.py): started
    Building wheel for pyodbc (setup.py): finished with status 'error
.....
....
....
...
ModuleNotFoundError: No module named 'pyodbc'
</code></pre>
",47,1,0,3,azure;machine-learning;azure-machine-learning-studio,2022-06-07 16:54:57,2022-06-07 16:54:57,2022-06-08 13:07:34,scenario   i am using machine learning studio for creating machine learning pipeline and when i am trying to call the  py file which has below code  and when i am trying to create and run pipeline from note book getting error 
321,321,896377,72541403,Spyder python logs unexpected,"<p>I m new to python and tensorflow...<br>
installed anaconda, tensorflow, and cuda<br>
when running a machine learning program (download from web and modified a bit) on Spyder, the following logs explodes the console:</p>
<pre><code>Processing agent:  t-dqn
1/1 [==============================] - 0s 36ms/step/s]
1/1 [==============================] - 0s 35ms/step
...
1/1 [==============================] - 0s 11ms/step
Episode 1/10:   1%|          | 32/3202 [00:02&lt;03:18, 15.95it/s]
...
</code></pre>
<p>&quot;1/1 [==============================] xxx &quot; <BR>
I don't know where is this message born from?
Anyone could help? Thanks.</p>
<p>[update: code added]</p>
<p>[Agent]</p>
<pre><code>import random

from collections import deque

import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K

from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model, clone_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam


def huber_loss(y_true, y_pred, clip_delta=1.0):
    ...


class Agent:
    def __init__(self, state_size, strategy=&quot;t-dqn&quot;, reset_every=1000, pretrained=False, model_name=None):
        self.strategy = strategy

        # agent config
        self.state_size = state_size        # normalized previous days
        self.action_size = 3                
        self.model_name = model_name
        self.inventory = []
        self.memory = deque(maxlen=10000)
        self.first_iter = True

        # model config
        self.model_name = model_name
        self.gamma = 0.95 # affinity for long term reward
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.loss = huber_loss
        self.custom_objects = {&quot;huber_loss&quot;: huber_loss}  # important for loading the model from memory
        self.optimizer = Adam(lr=self.learning_rate)

        if pretrained and self.model_name is not None:
            self.model = self.load()
        else:
            self.model = self._model()

        # strategy config
        if self.strategy in [&quot;t-dqn&quot;, &quot;double-dqn&quot;]:
            self.n_iter = 1
            self.reset_every = reset_every

            # target network
            self.target_model = clone_model(self.model)
            self.target_model.set_weights(self.model.get_weights())

    def _model(self):
        &quot;&quot;&quot;Creates the model
        &quot;&quot;&quot;
        model = Sequential()
        model.add(Dense(units=128, activation=&quot;relu&quot;, input_dim=self.state_size))
        model.add(Dense(units=256, activation=&quot;relu&quot;))
        model.add(Dense(units=256, activation=&quot;relu&quot;))
        model.add(Dense(units=128, activation=&quot;relu&quot;))
        model.add(Dense(units=self.action_size))

        model.compile(loss=self.loss, optimizer=self.optimizer)
        return model

    def remember(self, state, action, reward, next_state, done):
        &quot;&quot;&quot;Adds relevant data to memory
        &quot;&quot;&quot;
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, is_eval=False):
        &quot;&quot;&quot;Take action from given possible set of actions
        &quot;&quot;&quot;
        # take random action in order to diversify experience at the beginning
        if not is_eval and random.random() &lt;= self.epsilon:
            return random.randrange(self.action_size)

        if self.first_iter:
            self.first_iter = False
            return 1 # make a definite buy on the first iter

        action_probs = self.model.predict(state)
        return np.argmax(action_probs[0])

    def train_experience_replay(self, batch_size):
        &quot;&quot;&quot;Train on previous experiences in memory
        &quot;&quot;&quot;
        mini_batch = random.sample(self.memory, batch_size)
        X_train, y_train = [], []

        # DQN
        if self.strategy == &quot;dqn&quot;:
            for state, action, reward, next_state, done in mini_batch:
                if done:
                    target = reward
                else:
                    # approximate deep q-learning equation
                    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

                # estimate q-values based on current state
                q_values = self.model.predict(state)
                # update the target for current action based on discounted reward
                q_values[0][action] = target

                X_train.append(state[0])
                y_train.append(q_values[0])

        # DQN with fixed targets
        elif self.strategy == &quot;t-dqn&quot;:
            if self.n_iter % self.reset_every == 0:
                # reset target model weights
                self.target_model.set_weights(self.model.get_weights())

            for state, action, reward, next_state, done in mini_batch:
                if done:
                    target = reward
                else:
                    # approximate deep q-learning equation with fixed targets
                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])

                # estimate q-values based on current state
                q_values = self.model.predict(state)
                # update the target for current action based on discounted reward
                q_values[0][action] = target

                X_train.append(state[0])
                y_train.append(q_values[0])

        # Double DQN
        elif self.strategy == &quot;double-dqn&quot;:
            if self.n_iter % self.reset_every == 0:
                # reset target model weights
                self.target_model.set_weights(self.model.get_weights())

            for state, action, reward, next_state, done in mini_batch:
                if done:
                    target = reward
                else:
                    # approximate double deep q-learning equation
                    target = reward + self.gamma * self.target_model.predict(next_state)[0][np.argmax(self.model.predict(next_state)[0])]

                # estimate q-values based on current state
                q_values = self.model.predict(state)
                # update the target for current action based on discounted reward
                q_values[0][action] = target

                X_train.append(state[0])
                y_train.append(q_values[0])

        else:
            raise NotImplementedError()

        # update q-function parameters based on huber loss gradient
        loss = self.model.fit(
            np.array(X_train), np.array(y_train),
            epochs=1, verbose=0
        ).history[&quot;loss&quot;][0]

        # as the training goes on we want the agent to
        # make less random and more optimal decisions
        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        return loss

    def save(self, episode):
        self.model.save(&quot;models/{}_{}&quot;.format(self.model_name, episode))
    

    def load(self):
        return load_model(&quot;models/&quot; + self.model_name, custom_objects=self.custom_objects)
</code></pre>
<p>[caller]</p>
<pre><code>import logging
import tensorflow as tf
from tensorflow.python.client import device_lib
import os
from tqdm import tqdm
from agent import Agent
import math
import numpy as np

from utils import (
    get_data,
    format_currency,
    format_position,
    show_train_result,
    switch_k_backend_device
)

def sigmoid(x):
    &quot;&quot;&quot;Performs sigmoid operation
    &quot;&quot;&quot;
    try:
        if x &lt; 0:
            return 1 - 1 / (1 + math.exp(x))
        return 1 / (1 + math.exp(-x))
    except Exception as err:
        print(&quot;Error in sigmoid: &quot; + err)

def get_state(data, t, n_days):
    &quot;&quot;&quot;Returns an n-day state representation ending at time t
    &quot;&quot;&quot;
    d = t - n_days + 1
    block = data[d: t + 1] if d &gt;= 0 else -d * [data[0]] + data[0: t + 1]  # pad with t0
    res = []
    for i in range(n_days - 1):
        res.append(sigmoid(block[i + 1] - block[i]))
    return np.array([res])


def train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10):
    total_profit = 0
    data_length = len(data) - 1

    agent.inventory = []
    avg_loss = []

    state = get_state(data, 0, window_size + 1)

    for t in tqdm(range(data_length), total=data_length, leave=True, desc='Episode {}/{}'.format(episode, ep_count)):
        reward = 0
        next_state = get_state(data, t + 1, window_size + 1)

        # select an action
        action = agent.act(state)

        if action == 1:
            agent.inventory.append(data[t])

        elif action == 2 and len(agent.inventory) &gt; 0:
            bought_price = agent.inventory.pop(0)
            delta = data[t] - bought_price
            reward = delta #max(delta, 0)
            total_profit += delta

        else:
            pass

        done = (t == data_length - 1)
        agent.remember(state, action, reward, next_state, done)

        if len(agent.memory) &gt; batch_size:
            loss = agent.train_experience_replay(batch_size)
            avg_loss.append(loss)

        state = next_state

    if episode % 10 == 0:
        agent.save(episode)

    return (episode, ep_count, total_profit, np.mean(np.array(avg_loss)))





window_size=16
train_data=&quot;&quot;
val_data=&quot;&quot;
ep_count=10
batch_size=32
debug=False
agent = Agent(window_size, strategy=&quot;t-dqn&quot;, pretrained=False, model_name=&quot;model_TDQN&quot;)

train_data = get_data(&quot;set1&quot;)
val_data = get_data(&quot;set2&quot;)

initial_offset = val_data[1] - val_data[0]

print(&quot;Processing agent: &quot;, agent.strategy)
for episode in range(1, ep_count + 1):
    train_result = train_model(agent, episode, train_data, ep_count=ep_count,batch_size=batch_size, window_size=window_size)
</code></pre>
",27,0,-1,3,python;console;spyder,2022-06-08 12:35:11,2022-06-08 12:35:11,2022-06-08 12:57:41, update  code added   agent   caller 
322,322,16670157,72540383,Machine learning model for multi label and multi class classification,"<p>Can anyone suggests models that can be used for multi label + multi class classification</p>
",11,0,-1,1,machine-learning,2022-06-08 10:46:08,2022-06-08 10:46:08,2022-06-08 10:46:08,can anyone suggests models that can be used for multi label   multi class classification
323,323,1608273,72539703,Perceptron model: Effect of the learning rate when all weights are initialized to zeros,"<p>In Python Machine Learning 2rd book, related to the weight initialization step in the Perceptron, the author wrote:</p>
<p><a href=""https://i.stack.imgur.com/sLJ7I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sLJ7I.png"" alt=""Python Machine Learning"" /></a></p>
<p>I'm not expert at Trigonometry and Algebra so I don't really understand how the angle between two vectors (is zero) relates to the effect of learning rate to the weights (when it's initialized to all zeros). Please help to connect the two points?</p>
",17,0,0,4,neural-network;linear-algebra;trigonometry;perceptron,2022-06-08 08:55:04,2022-06-08 08:55:04,2022-06-08 08:55:04,in python machine learning rd book  related to the weight initialization step in the perceptron  the author wrote   i m not expert at trigonometry and algebra so i don t really understand how the angle between two vectors  is zero  relates to the effect of learning rate to the weights  when it s initialized to all zeros   please help to connect the two points 
324,324,6001009,72038418,Cannot Connect to $(minikube ip):$NODE_PORT on mac M1,"<p>I am learning kubernetes on minikube. I studied the kubernetes official documentation and followed their <a href=""https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-interactive/"" rel=""nofollow noreferrer"">interactive tutorial</a> in a sandboxed environment. Everything worked fine in the sandbox but I tried the same thing on my system it failed.</p>
<h3>My Setup :</h3>
<ul>
<li>I am using macOS Big Sur version 11.6.2(20G314) on Apple M1.</li>
<li>I have used docker instead of virtual machine environment for minikube.</li>
</ul>
<h3>Steps to reproduce :</h3>
<p>First I created a deployment, then I created a <code>NodePort</code> type service to expose it to external traffic.</p>
<p>The pod is running fine and no issues are seen in the service description.</p>
<p>To test if the app is exposed outside of the cluster I used <code>curl</code> to send a request to the node :</p>
<pre class=""lang-sh prettyprint-override""><code>curl $(minikube ip):$NODE_PORT
</code></pre>
<p>But I get no response from the server :</p>
<blockquote>
<p>curl: (7) Failed to connect to 192.168.XX.X port 32048: Operation timed out.</p>
</blockquote>
<p>I have copied everything that was done in the tutorial. Same deployment name, same image, same service-name, literally EVERYTHING.</p>
<p>I tried <code>LoadBalancer</code> type, but found out that minikube doesn't support it. To access the <code>LoadBalancer</code> deployment, I used the command <code>minikube tunnel</code> but this did not help.</p>
<p>What could be the possible reasons? Is it my system?</p>
",177,1,2,3,docker;kubernetes;minikube,2022-04-28 10:57:33,2022-04-28 10:57:33,2022-06-08 05:29:39,i am learning kubernetes on minikube  i studied the kubernetes official documentation and followed their  in a sandboxed environment  everything worked fine in the sandbox but i tried the same thing on my system it failed  first i created a deployment  then i created a nodeport type service to expose it to external traffic  the pod is running fine and no issues are seen in the service description  to test if the app is exposed outside of the cluster i used curl to send a request to the node   but i get no response from the server   curl     failed to connect to   xx x port   operation timed out  i have copied everything that was done in the tutorial  same deployment name  same image  same service name  literally everything  i tried loadbalancer type  but found out that minikube doesn t support it  to access the loadbalancer deployment  i used the command minikube tunnel but this did not help  what could be the possible reasons  is it my system 
325,325,15209073,67641883,Run a ML program locally using the Colab GPU,"<p>I have a Lenovo as computer, but there is no GPU installed. So when I run a machine learning program written in python, it runs it on my local CPU. I know that Colab provides us a GPU for free. To use it, I need to take the content of all the python files from my ML program and put it in this Colab notebook. It is not very convenient at this point. Is it possible to run in any ways my ML program from my computer using directly the Colab GPU without using the Colab Notebook directly.</p>
<p><strong>EDIT</strong></p>
<p>Be aware that I don't want to work from Jupiter Notebook. I would like to work in Visual Studio Code and run the code on the Colab GPU directly instead of my CPU</p>
",698,4,0,2,python;google-colaboratory,2021-05-21 23:31:47,2021-05-21 23:31:47,2022-06-08 02:17:50,i have a lenovo as computer  but there is no gpu installed  so when i run a machine learning program written in python  it runs it on my local cpu  i know that colab provides us a gpu for free  to use it  i need to take the content of all the python files from my ml program and put it in this colab notebook  it is not very convenient at this point  is it possible to run in any ways my ml program from my computer using directly the colab gpu without using the colab notebook directly  edit be aware that i don t want to work from jupiter notebook  i would like to work in visual studio code and run the code on the colab gpu directly instead of my cpu
326,326,6562793,72536408,How to drop columns from a pandas DataFrame that have elements containing a string?,"<p>This is not about dropping columns whose name contains a string.</p>
<p>I have a dataframe with 1600 columns. Several hundred are garbage. Most of the garbage columns contain a phrase such as <code>invalid value encountered in double_scalars (XYZ)</code> where `XYZ' is a filler name for the column name.</p>
<p>I would like to delete all columns that contain, in any of their elements, the string <code>invalid</code></p>
<p>Purging columns with strings in general would work too. What I want is to clean it up so I can fit a machine learning model to it, so removing any/all columns that are not boolean or real would work.</p>
<p>This must be a duplicate question, but I can only find answers to how to remove a column with a specific column name.</p>
",30,2,0,3,python-3.x;pandas;dataframe,2022-06-08 00:38:33,2022-06-08 00:38:33,2022-06-08 00:51:40,this is not about dropping columns whose name contains a string  i have a dataframe with  columns  several hundred are garbage  most of the garbage columns contain a phrase such as invalid value encountered in double_scalars  xyz  where  xyz  is a filler name for the column name  i would like to delete all columns that contain  in any of their elements  the string invalid purging columns with strings in general would work too  what i want is to clean it up so i can fit a machine learning model to it  so removing any all columns that are not boolean or real would work  this must be a duplicate question  but i can only find answers to how to remove a column with a specific column name 
327,327,17171942,69601364,How do I draw bounding boxes from &quot;results.xyxy[0]&quot; with cv2 rectangle (YOLOv5)?,"<p>New to both python and machine learning. I'm trying to draw bounding boxes on my mss screen capture. This is the part of the code where I believe I should be receiving the coordinates to draw the rectangle.</p>
<pre><code>    while True:

            img = screnshot.grab(bounding_box)

            frame = np.array(img)

            cv2.imshow('Testing', frame)

            results = model(frame)

            results.print()  
            pandasbox=results.pandas().xyxy[0]
            print(pandasbox)
</code></pre>
<p>The coordinates and classes are printed just fine using pandas, for example:</p>
<pre><code>image 1/1: 400x350 1 person, 1 truck
Speed: 0.0ms pre-process, 14.4ms inference, 0.0ms NMS per image at shape (1, 3, 640, 576)
         xmin        ymin        xmax      ymax  confidence  class    name
0   61.171875  134.765625  133.046875  334.0625    0.810547      0  person
1  181.562500    0.273438  347.500000   86.2500    0.768555      7   truck
</code></pre>
<p>But... I have no idea how to form a rectangle using these coordinates. Adding &quot;(xmin, xmax),(ymin, ymax)&quot; to the cv2.rectangle function was unsuccessful as well as many other attempts, including this attempt, which never drew bounding boxes in the mss screen despite printing detections:</p>
<pre><code>for box in results.xyxy[0]: 
                if box[5]==0:
        
                    xB = int(box[2])
                    xA = int(box[0])
                    yB = int(box[3])
                    yA = int(box[1])
                    
                    cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)
</code></pre>
<p>If anyone could show me an example of using the coordinates from &quot;results.pandas().xyxy[0]&quot; to draw a bounding box with cv2.rectangle that would be great! As well as any other pointers or insight that someone new to this would be unaware of...</p>
",1425,3,2,4,python;machine-learning;object-detection;yolov5,2021-10-17 10:10:45,2021-10-17 10:10:45,2022-06-08 00:33:58,new to both python and machine learning  i m trying to draw bounding boxes on my mss screen capture  this is the part of the code where i believe i should be receiving the coordinates to draw the rectangle  the coordinates and classes are printed just fine using pandas  for example  but    i have no idea how to form a rectangle using these coordinates  adding   xmin  xmax   ymin  ymax   to the cv rectangle function was unsuccessful as well as many other attempts  including this attempt  which never drew bounding boxes in the mss screen despite printing detections  if anyone could show me an example of using the coordinates from  results pandas   xyxy    to draw a bounding box with cv rectangle that would be great  as well as any other pointers or insight that someone new to this would be unaware of   
328,328,17867544,72535417,Machine Learning Vison - Sequence Length setting for CNN +LSTM approach,"<p>I have few events which are captured by cameras at certain timestep. These events are having sequences of images representing that entire event. I need to feed these sequence of images to ConvLSTM for clsassification of events, Thus I am currently having the problem in setting uniform sequence length for all events.I do not want use skip/ignore images as my sequence is just having an average of 55 frame and i loose out information if i do that. Can I pad np.zeros((64,64,3)) images to each events so that all events can be of same sequence length. Or do you suggest any other methodologies like pad_sequences?</p>
<p>32 events having number of images in sequences representing those each event is given below.</p>
<p>[50,
67,
54,
64,
63,
53,
53,
31,
136,
36,
30,
31,
29,
31,
38,
46,
49,
65,
47,
44,
75,
55,
35,
29,
31,
41,
35,
33,
33,
131,
197,
62]</p>
<p>I am expecting the dimension as ( 32, sequence length, 64,64,3) to ConvLSTM model.
where 32 are the events, sequence length is the length of event with number of images that are sequence,64*64 image size, 3 is for RGB channel.
so how shall fix my sequence lentgh now.</p>
",26,0,0,5,opencv;machine-learning;deep-learning;conv-neural-network;lstm,2022-06-07 23:07:26,2022-06-07 23:07:26,2022-06-07 23:07:26,i have few events which are captured by cameras at certain timestep  these events are having sequences of images representing that entire event  i need to feed these sequence of images to convlstm for clsassification of events  thus i am currently having the problem in setting uniform sequence length for all events i do not want use skip ignore images as my sequence is just having an average of  frame and i loose out information if i do that  can i pad np zeros       images to each events so that all events can be of same sequence length  or do you suggest any other methodologies like pad_sequences   events having number of images in sequences representing those each event is given below 
329,329,16910081,72535002,Youtube Python API repeating comments,"<p>I am trying to use Google's Youtube API to scrape comments from videos. Since videos can have many comments, I have decided to break this job into many smaller jobs</p>
<p>Ideally, the workflow would be:</p>
<ol>
<li>scrape x number comments from a youtube video</li>
<li>keep track of the timestamp for the latest comment processed and save to a database (dynamodb)</li>
<li>when scraping is started again for the same video, only process comments new than the latest timestamp</li>
</ol>
<p>However. It is not clear if there is a way to search a comment newer than a timestamp in order not to scrape comments already processed?</p>
<p>For reference, I'm currently trying to implement a solution similar to:</p>
<p><a href=""https://github.com/aws-solutions/discovering-hot-topics-using-machine-learning/blob/main/source/lambda/ingestion-youtube/util/video.py"" rel=""nofollow noreferrer"">https://github.com/aws-solutions/discovering-hot-topics-using-machine-learning/blob/main/source/lambda/ingestion-youtube/util/video.py</a></p>
<p>But I am not sure how that solution goes about avoiding comments that have already been processed.</p>
",33,0,-2,4,python;amazon-web-services;web-scraping;youtube-api,2022-06-07 22:27:36,2022-06-07 22:27:36,2022-06-07 22:52:12,i am trying to use google s youtube api to scrape comments from videos  since videos can have many comments  i have decided to break this job into many smaller jobs ideally  the workflow would be  however  it is not clear if there is a way to search a comment newer than a timestamp in order not to scrape comments already processed  for reference  i m currently trying to implement a solution similar to   but i am not sure how that solution goes about avoiding comments that have already been processed 
330,330,15454165,72535088,How to create segment to predict income or age using Machine Learning models,"<p>I am working on a project where I need to <strong>create segments using geography data</strong> like Geo key, Latitude, and Longitude. The segments should predict the average income per geo key.
The <strong>data I have has geo keys, lat, long</strong>, and average income, and another scenario is where I need to get the geo keys falling into the range of age (people age 18-40).
I tried to use lat and long as features to predict or classify data but the results are not promising. I also tried multiple models like Logistic regression, Random forest, and SGD classifier for multi-class classification.
I want to know what's the best way to create these segments.</p>
<p>Data looks like this (consists of lat and long as a geopoint):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Geopoint</th>
<th>Avg. income</th>
</tr>
</thead>
<tbody>
<tr>
<td>POINT(-122.425123 37.723603)</td>
<td>38199.5</td>
</tr>
<tr>
<td>POINT(-81.27178 28.814807)</td>
<td>681428.3571</td>
</tr>
</tbody>
</table>
</div>",36,0,1,5,python;google-cloud-platform;data-science;geography;gcp-ai-platform-notebook,2022-06-07 22:35:15,2022-06-07 22:35:15,2022-06-07 22:35:15,data looks like this  consists of lat and long as a geopoint  
331,331,18250015,72534946,Drawing up histogram of MAE on Test Dataset after finishing training?,"<p>I am creating a machine learning algorithm to estimate bone age from left hand X-ray image.</p>
<p>Figure 1 shows the MAE histogram for Validation data evaluated after training on the train dataset (of 10000 images). Training was done with 70/30 split just like this:</p>
<pre><code>Total Data:     10000
Train:          7000
Test:           1500
Validation:     1500
</code></pre>
<p><a href=""https://i.stack.imgur.com/y13gv.png"" rel=""nofollow noreferrer"">Figure 1</a></p>
<p>the variable &quot;history&quot; is appointed to the fitting the model for training:</p>
<pre><code>history = model.fit(train_gen, validation_data = (X_valid, y_valid), epochs = config.epoch, callbacks = callbacks_list)
</code></pre>
<p>This produces an output like:</p>
<pre><code>Epoch 1/30
438/438 [==============================] - ETA: 0s - loss: 0.4371 - mae_months: 20.4250
Epoch 1: val_loss improved from inf to 0.56135, saving model to bone_age_weights.best.hdf5
438/438 [==============================] - 198s 437ms/step - loss: 0.4371 - mae_months: 20.4250 - val_loss: 0.5614 - val_mae_months: 21.5402 - lr: 1.0000e-04 - _timestamp: 1654043901.0000 - _runtime: 266.0000
Epoch 2/30
438/438 [==============================] - ETA: 0s - loss: 0.1902 - mae_months: 14.0811
Epoch 2: val_loss improved from 0.56135 to 0.22898, saving model to bone_age_weights.best.hdf5
438/438 [==============================] - 179s 408ms/step - loss: 0.1902 - mae_months: 14.0811 - val_loss: 0.2290 - val_mae_months: 14.9082 - lr: 1.0000e-04 - _timestamp: 1654044080.0000 - _runtime: 445.0000
</code></pre>
<p>I produced a MAE histogram for validation data (Figure 1) to see what kind of images of certain epoch gives me high MAE metrics. However, I am stuck with producing MAE histogram on test data. As you can see, as you fit the model, it has variables like &quot;val_mae_months&quot; that I could match to the x-axis and display the histogram. I did this to produce the histogram of MAE for Validation Data:</p>
<pre><code>def MAE_histogram(history):
# Function for MAE histogram

   fig, ax = plt.subplots(figsize=(20,10))
   plt.hist(x = history.history['val_mae_months'], bins = 10, color = 'orange')
   plt.title('MAE Histogram')
   plt.xlabel('Mean Absolute Error')
   plt.legend(['Val'], loc = 'upper right')
   ax.grid(color = 'black')
   plt.show()

MAE_histogram(history)
</code></pre>
<p>How do I do this for test data? I am not training on the test data hence I won't be getting variables like &quot;val_mae_months&quot; like I did for validation data.</p>
<p>This is what I have for evaluating on test data:</p>
<pre><code>model.load_weights(weight_path)

from sklearn.metrics import mean_absolute_error as sk_mae

pred_Y = boneage_mean + (boneage_div * model.predict(X_test, batch_size = 16, verbose = True))
test_Y_months = boneage_mean + (boneage_div * y_test)

mae_value = str(sk_mae(test_Y_months, pred_Y))

print(&quot;Mean absolute error on test data: &quot; + mae_value)
</code></pre>
<p>This output:</p>
<pre><code>94/94 [==============================] - 11s 45ms/step
Mean absolute error on test data: 8.865146
</code></pre>
",21,0,0,4,python;tensorflow;machine-learning;image-processing,2022-06-07 22:24:01,2022-06-07 22:24:01,2022-06-07 22:24:01,i am creating a machine learning algorithm to estimate bone age from left hand x ray image  figure  shows the mae histogram for validation data evaluated after training on the train dataset  of  images   training was done with   split just like this   the variable  history  is appointed to the fitting the model for training  this produces an output like  i produced a mae histogram for validation data  figure   to see what kind of images of certain epoch gives me high mae metrics  however  i am stuck with producing mae histogram on test data  as you can see  as you fit the model  it has variables like  val_mae_months  that i could match to the x axis and display the histogram  i did this to produce the histogram of mae for validation data  how do i do this for test data  i am not training on the test data hence i won t be getting variables like  val_mae_months  like i did for validation data  this is what i have for evaluating on test data  this output 
332,332,11903199,72534092,How to map one set of cordinates to another using machine learning,"<p>I'm doing a project which involves getting x-y coordinates of a position of a ball from the touch screen and mapping them to step motor x-y coordinates. For every position of ball (i.e. x-y coordinate) the motor moves x-y direction to balance the ball.</p>
<p>For every set of ball position coordinates the motor has fixed pattern of movement.</p>
<p>Can I use machine learning approach to train an algorithm for these coordinates mapping to another ? If yes which domain of machine learning I should be looking into like LSTM etc ?</p>
",6,0,0,4,machine-learning;deep-learning;neural-network;lstm,2022-06-07 21:11:38,2022-06-07 21:11:38,2022-06-07 21:11:38,i m doing a project which involves getting x y coordinates of a position of a ball from the touch screen and mapping them to step motor x y coordinates  for every position of ball  i e  x y coordinate  the motor moves x y direction to balance the ball  for every set of ball position coordinates the motor has fixed pattern of movement  can i use machine learning approach to train an algorithm for these coordinates mapping to another   if yes which domain of machine learning i should be looking into like lstm etc  
333,333,19291602,72533096,Suggestions for nonparametric machine learning models,"<p>I am new to machine learning, but I have decent experience in python. I am faced with a problem: I need to find a machine learning model that would work well to predict the speed of a boat given current environmental and physical conditions. I have looked into Scikit-Learn, Pytorch, and Tensorflow, but I am having trouble finding information on what type of model I should use. I am almost certain that linear regression models would be useless for this task. I have been told that non-parametric regression models would be ideal for this, but I am unable to find many in the Scikit Library. Should I be trying to use regression models at all, or should I be looking more into Neural Networks? I'm open to any suggestions, thanks in advance.</p>
",28,1,0,4,python;machine-learning;data-science;artificial-intelligence,2022-06-07 20:05:00,2022-06-07 20:05:00,2022-06-07 20:30:13,i am new to machine learning  but i have decent experience in python  i am faced with a problem  i need to find a machine learning model that would work well to predict the speed of a boat given current environmental and physical conditions  i have looked into scikit learn  pytorch  and tensorflow  but i am having trouble finding information on what type of model i should use  i am almost certain that linear regression models would be useless for this task  i have been told that non parametric regression models would be ideal for this  but i am unable to find many in the scikit library  should i be trying to use regression models at all  or should i be looking more into neural networks  i m open to any suggestions  thanks in advance 
334,334,12135090,72530496,ValueError: `y` argument is not supported when using `keras.utils.Sequence` as input,"<p>I am new to image processing and machine learning in python. I have been trying to execute a model in google colab using inceptionv3 but i am stuck at fitting the model.</p>
<pre><code>    # fit the model
    # Run the cell. It will take some time to execute
    validation_data=test_set
    epochs=10
    steps_per_epoch=len(training_set)
    validation_steps=len(test_set)
    r = model.fit(
    training_set,
    validation_data,
    epochs,
    steps_per_epoch,
    validation_steps
    )
</code></pre>
<p>the whole code is in my git repository.
<a href=""https://github.com/Aditya757/MyRepository.git"" rel=""nofollow noreferrer"">https://github.com/Aditya757/MyRepository.git</a></p>
<p>this is my dataset image link below</p>
<pre><code>      https://i.stack.imgur.com/jWaJ8.png
</code></pre>
",26,2,0,2,python;tensorflow,2022-06-07 17:04:18,2022-06-07 17:04:18,2022-06-07 20:16:44,i am new to image processing and machine learning in python  i have been trying to execute a model in google colab using inceptionv but i am stuck at fitting the model  this is my dataset image link below
335,335,19290994,72532056,sam build botocore.exceptions.NoCredentialsError: Unable to locate credentials,"<p>I am trying to deploy my machine learning model with sam for couple of days and I am getting this error:
botocore.exceptions.NoCredentialsError: Unable to locate credentials
I am also make sure that my aws config is fine
the &quot;aws s3 ls&quot; command works fine with me any help will be useful thanks in advance</p>
",31,0,0,4,amazon-web-services;machine-learning;deep-learning;sam,2022-06-07 18:58:06,2022-06-07 18:58:06,2022-06-07 18:59:18,
336,336,12051503,72471691,How to deploy multiple models to an endpoint using Azure Machine Learning CLI v2?,"<p>At the GA of <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/azure-machine-learning-release-notes-cli-v2#2022-05-24"" rel=""nofollow noreferrer"">az ml cli v2</a>, we've been working on some POC using <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-deployment-managed-online"" rel=""nofollow noreferrer"">yml online deployment</a> on top of managed endpoint and it all went well for single model, until when there's certain scenario where there is requirement to deploy multiple trained and registered models to one managed endpoint, it seems there is no documentations on how to achieve that.</p>
<p>Previously using Python SDK, it was able to deploy list of models to AKS cluster.</p>
<p>Checking if there's any limitation or could be some docs I might have missed?</p>
",48,1,0,2,azure-cli;azure-machine-learning-service,2022-06-02 12:04:23,2022-06-02 12:04:23,2022-06-07 17:44:26,at the ga of   we ve been working on some poc using  on top of managed endpoint and it all went well for single model  until when there s certain scenario where there is requirement to deploy multiple trained and registered models to one managed endpoint  it seems there is no documentations on how to achieve that  previously using python sdk  it was able to deploy list of models to aks cluster  checking if there s any limitation or could be some docs i might have missed 
337,337,18572589,72525239,Productize a model with Target Encoding,"<p>I am new to data science and I am experimenting with target encoding for my dataset that has several columns with multiple categories (I have discovered that one hot encoding has failed me from encountering a real-world dataset). While I was building my model using the insights I gained from <a href=""https://github.com/groverpr/Machine-Learning/tree/9963e59823fe0ff18cc8e1b2657b71c01f133193"" rel=""nofollow noreferrer"">https://github.com/groverpr/Machine-Learning/tree/9963e59823fe0ff18cc8e1b2657b71c01f133193</a> and <a href=""https://github.com/scikit-learn-contrib/category_encoders"" rel=""nofollow noreferrer"">https://github.com/scikit-learn-contrib/category_encoders</a> I couldn't help but wonder how these models are able to be used for real-world situations post training. I also wonder how target encoding can even be used for feature selection. I would be very grateful for any clarification on this topic.</p>
",7,0,0,5,encoding;model;target;categorical-data;production,2022-06-07 07:42:00,2022-06-07 07:42:00,2022-06-07 07:42:00,i am new to data science and i am experimenting with target encoding for my dataset that has several columns with multiple categories  i have discovered that one hot encoding has failed me from encountering a real world dataset   while i was building my model using the insights i gained from  and  i couldn t help but wonder how these models are able to be used for real world situations post training  i also wonder how target encoding can even be used for feature selection  i would be very grateful for any clarification on this topic 
338,338,18067785,72192511,ModuleNotFoundError: No module named &#39;pyarrow.lib&#39;,"<p>This is the full error message.</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\machine learning project.py&quot;, line 3, in &lt;module&gt;
    import streamlit as st
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\__init__.py&quot;, line 70, in &lt;module&gt;
    from streamlit.delta_generator import DeltaGenerator as _DeltaGenerator
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\delta_generator.py&quot;, line 19, in &lt;module&gt;
    from streamlit import cursor, caching
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\cursor.py&quot;, line 18, in &lt;module&gt;
    from streamlit.scriptrunner import get_script_run_ctx
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\scriptrunner\__init__.py&quot;, line 16, in &lt;module&gt;
    from .script_runner import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\scriptrunner\script_runner.py&quot;, line 35, in &lt;module&gt;
    from streamlit.state import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\state\__init__.py&quot;, line 27, in &lt;module&gt;
    from .session_state_proxy import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\state\session_state_proxy.py&quot;, line 24, in &lt;module&gt;
    from streamlit.type_util import Key
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\type_util.py&quot;, line 22, in &lt;module&gt;
    import pyarrow as pa
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\pyarrow\__init__.py&quot;, line 65, in &lt;module&gt;
    import pyarrow.lib as _lib
</code></pre>
<p>I am working with streamlit for a project but can't work out this problem.
I have tried uninstalling and reinstalling streamlit but that did'nt help.</p>
<p>i using python 3.8</p>
",148,2,1,4,python;pyarrow;streamlit;facebook-prophet,2022-05-11 01:50:03,2022-05-11 01:50:03,2022-06-07 03:30:57,this is the full error message  i using python  
339,339,16089095,72522012,How to get in python historical data prices btc usdt,"<p>Hello guys im trying make a machine learning thing that needs btc usdt historical  prices for every minute. I gotted an similar thing on yfinance  wit this code (to usd):</p>
<pre><code>import yfinance as yf
import pandas as pd
obpandas = pd.DataFrame(yf.download(tickers=&quot;BTC-USD&quot;, period=&quot;7d&quot;, interval=&quot;1m&quot;))
</code></pre>
<p>Notes i want to get the biggest volume possible with an free api.
How can I get it to usdt .</p>
",36,0,-1,1,python,2022-06-06 23:57:37,2022-06-06 23:57:37,2022-06-07 02:14:48,hello guys im trying make a machine learning thing that needs btc usdt historical  prices for every minute  i gotted an similar thing on yfinance  wit this code  to usd  
340,340,6151356,72508823,ML.NET OLSTrainer on Macos with an arm M1 processor error with libomp,"<p>I am trying to use a ml.net's OLSTrainer on a Mac with an M1 processor but I get the error below and was looking for some assistance.</p>
<p>Unhandled exception. System.NotSupportedException: The MKL library (libMklImports) or one of its dependencies is missing.
at Microsoft.ML.Trainers.OlsTrainer.TrainCore(IChannel ch, Factory cursorFactory, Int32 featureCount)</p>
<p>Microsoft documentation points me to this link/version to install, but makes no difference.</p>
<p><a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-extra-dependencies"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-extra-dependencies</a></p>
<pre><code>wget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb &amp;&amp; brew install ./libomp.rb &amp;&amp; brew link libomp --force
</code></pre>
",30,1,0,4,macos;.net-core;apple-m1;ml.net,2022-06-05 21:19:36,2022-06-05 21:19:36,2022-06-07 01:20:53,i am trying to use a ml net s olstrainer on a mac with an m processor but i get the error below and was looking for some assistance  microsoft documentation points me to this link version to install  but makes no difference  
341,341,11943415,72522483,postgres EXPLAIN: approximate cost of a node without its children,"<p>I want to use postgres EXPLAIN data for some machine learning. Here's an example output:</p>
<pre><code>                                                                           QUERY PLAN                                                                           
----------------------------------------------------------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=41022.32..41022.33 rows=1 width=32)
   -&gt;  Gather  (cost=1001.14..41022.31 rows=2 width=17)
         Workers Planned: 2
         -&gt;  Nested Loop  (cost=1.14..40022.11 rows=1 width=17)
               -&gt;  Nested Loop  (cost=0.99..40021.95 rows=1 width=21)
                     -&gt;  Nested Loop  (cost=0.86..40020.90 rows=3 width=25)
                           Join Filter: (t.id = mc.movie_id)
                           -&gt;  Nested Loop  (cost=0.43..40005.45 rows=20 width=16)
                                 -&gt;  Parallel Seq Scan on movie_companies mc  (cost=0.00..37814.90 rows=25 width=8)
                                       Filter: (((note)::text ~~ '%(VHS)%'::text) AND ((note)::text ~~ '%(USA)%'::text) AND ((note)::text ~~ '%(1994)%'::text))
                                 -&gt;  Index Scan using movie_id_movie_info on movie_info mi  (cost=0.43..87.61 rows=1 width=8)
                                       Index Cond: (movie_id = mc.movie_id)
                                       Filter: ((info)::text = ANY ('{USA,America}'::text[]))
                           -&gt;  Index Scan using title_pkey on title t  (cost=0.43..0.76 rows=1 width=21)
                                 Index Cond: (id = mi.movie_id)
                                 Filter: (production_year &gt; 2010)
                     -&gt;  Index Scan using company_type_pkey on company_type ct  (cost=0.13..0.35 rows=1 width=4)
                           Index Cond: (id = mc.company_type_id)
                           Filter: ((kind)::text = 'production companies'::text)
               -&gt;  Index Only Scan using info_type_pkey on info_type it  (cost=0.14..0.16 rows=1 width=4)
                     Index Cond: (id = mi.info_type_id)
(21 rows)
</code></pre>
<p>In code, it's better to use the JSON format of course. My problem is how to get the cost of a join for example, but without the cost of its children. I tried to subtract the cost of its direct children, but of course there are parallel-aware plans, so in that case, I subtract the max cost of the children. Nevertheless, I still get negative values. Is there something that I'm missing?</p>
",17,0,0,3,postgresql;sql-execution-plan;explain,2022-06-07 00:45:42,2022-06-07 00:45:42,2022-06-07 00:45:42,i want to use postgres explain data for some machine learning  here s an example output  in code  it s better to use the json format of course  my problem is how to get the cost of a join for example  but without the cost of its children  i tried to subtract the cost of its direct children  but of course there are parallel aware plans  so in that case  i subtract the max cost of the children  nevertheless  i still get negative values  is there something that i m missing 
342,342,18483700,72426814,Export 16Bit Grayscale Depth Image (1440x1920) - Swift iOS15.3,"<p>I'd like to export a 16 bit  grayscale PNG of my captured sceneDepth data on iOS (for machine learning purposes).
I am using the new iPadPro with lidar sensor to capture the datra in an ARSession.
I already get the 192x256 depth map, which i scaled up by 7.5 to match the 1440x1920 resolution of my rgb images. This is the code i have so far:</p>
<pre><code>func convertDepthToImg(frame: ARFrame) {
        let depthBuffer = frame.sceneDepth?.depthMap
        var ciImageDepth:CIImage    = CIImage(cvPixelBuffer: depthBuffer!) 

        // Transform image on pixel level to get the same size as rgb, apply nearest neighbour sampling or linear sampling (depends on performance in network)   
        let transformation          = CGAffineTransform(scaleX: 7.5, y: 7.5) 
            ciImageDepth            = ciImageDepth.samplingLinear()
                                        .transformed(by: combined_transf_matrix)
        let contextDepth:CIContext  = CIContext(options: nil)  
        let cgImageDepth:CGImage    = contextDepth.createCGImage((ciImageDepth), from: ciImageDepth.extent)!

        // convert to required 16 bits gray png img
        convertTo16BitGrayPng(image: cgImageDepth)
    }

    // Function to create vImageBuffer for more functionality on Images
    func createVImg(image: CGImage) -&gt; vImage_Buffer? {
        guard let vImageBuffer = try? vImage_Buffer(cgImage: image)
        else {
            return nil
        }
        return vImageBuffer
    }

    func convertTo16BitGrayPng(image: CGImage){
        let width = 1440
        let height = 1920

        //create vImageBuffer vor UIImage
        var srcBuf = createVImg(image: image)
        print(&quot;Height: &quot;, String(srcBuf!.height))
        print(&quot;Width: &quot;, String(srcBuf!.width))

        // allocate memory for final size:
        let bv = malloc(width * height * 4)!
        var db = vImage_Buffer(data: bv,
                               height: vImagePixelCount(height),
                               width: vImagePixelCount(width),
                               rowBytes: width*2)

        // create pointer to Buffer that contains the image data
        vImageConvert_PlanarFtoPlanar16F(&amp;(srcBuf)!, &amp;db, vImage_Flags(kvImageNoFlags))
        let bp = bv.assumingMemoryBound(to: UInt16.self)
        let prov = CGDataProvider(data: CFDataCreateWithBytesNoCopy(kCFAllocatorDefault,
                                                                    bp,
                                                                    height * width * 4,
                                                                kCFAllocatorDefault))!
        let cgImage = CGImage(width: width,
                              height: height,
                              bitsPerComponent: 5,  
                              bitsPerPixel: 16,
                              bytesPerRow: 2 * width, 
                              space: CGColorSpace(name: CGColorSpace.linearSRGB)!, 
                              bitmapInfo: CGBitmapInfo(rawValue: CGImageAlphaInfo.noneSkipFirst.rawValue), 
                              provider: prov,
                              decode: nil,
                              shouldInterpolate: false,
                              intent: .defaultIntent)

        // save processed image to documents dir
        saveDptToDocs(cgImage: cgImage!, type: &quot;dpt&quot;)
    }

... save Image to documentpath (works fine)
</code></pre>
<p>I used <a href=""https://stackoverflow.com/questions/64227917/exporting-16-bit-image-with-swift-in-ios"">this questions</a> answer to convert my images to 16 bit and save them to my documents directory, but i only get 24 bit images. I really can't get the 16 bit export to work.
I already exported images in 32, 64, 8 and even 24 bit. However, 16 bit is somewhat tricky?
Please help.</p>
",39,1,0,5,ios;swift;image-processing;depth;lidar,2022-05-30 01:19:41,2022-05-30 01:19:41,2022-06-06 20:21:57,
343,343,19280283,72515097,Python State Machine attribute error - Mat Buckland tutorial,"<p>I´m currently learning Python and going through Mat Buckland´s book Programming Game AI by Example, chapter 2 on State Driven Agent Design. The book is written in C++ I believe, which I know nothing about. I´m using the codes example and basically trying to &quot;convert&quot; it to Python + a Github repo that did part of the examples of the chapter.</p>
<p>I did manage the first Westworld example up until the creation of a state machine. One can find notes on the website of the author: <a href=""http://www.ai-junkie.com/architecture/state_driven/tut_state3.html"" rel=""nofollow noreferrer"">http://www.ai-junkie.com/architecture/state_driven/tut_state3.html</a></p>
<p>From Creating a State Machine Class &amp; modifying the Miner class, I can´t get it to work. I get the error: <strong>AttributeError: 'Miner' object has no attribute 'change_state'</strong>
I tried a bunch of combinations but can´t get it to work. I guess there is some hierarchy problem I missed: BaseEntity &gt; Miner &gt; StateMachine. The StateMachine &quot;owns&quot; the states and the Miner &quot;owns&quot; the conditions for transition to a state (in separate classes). I assume I´m not sending the right entity. The original code I think is the problem:</p>
<pre><code>//if a global state exists, call its execute method
if (m_pGlobalState) m_pGlobalState-&gt;Execute(m_pOwner);
</code></pre>
<p>Here´s the relevant Python code, the StateMachine class is new and only the Miner class should be changed to create the statemachine + call the update every loop:</p>
<pre><code>class StateMachine(object):
    def __init__(self, owner, entity_type=-1):
        super().__init__()
        self.owner = owner
        self.entity_type = entity_type
        self.current_state = None
        self.previous_state = None
        self.global_state = None

    def set_current_state(self, current_state):
        self.current_state = current_state

    def set_previous_state(self, previous_state):
        self.previous_state = previous_state

    def set_global_state(self, global_state):
        self.global_state = global_state

    def update(self):
        print(f&quot; start update fsm: {self.current_state} &quot;)

        if self.global_state:
            self.global_state.execute(self.owner())

        if self.current_state:
            self.current_state.execute(self.owner())

        print(f&quot; end update fsm: {self.current_state} &quot;)

    def change_state(self, new_state):
        print(f&quot; start change state: {self.current_state}&quot;)

        if not self.current_state and not new_state:
            return

        self.set_previous_state(self.current_state)
        self.previous_state.exit(self.owner())

        self.set_current_state(new_state)
        self.current_state.enter(self.owner())

        print(f&quot; end change state: {self.current_state}&quot;)

    def revert_to_previous_state(self):
        self.change_state(self.previous_state)


class Miner(BaseEntity):

    def __init__(self, current_state=None, location=None):
        super().__init__()
        self.current_state = current_state
        self.location = location
        self.wealth = 0
        #other omitted

        self.COMFORT_LEVEL = 5
        #other omitted

        self.fsm = StateMachine(owner=Miner)
        self.fsm.set_current_state(GoHomeAndSleepTilRested())

    def update(self):
        self.thirst += 1
        self.fsm.update()


class EnterMineAndDigForNugget(State):
    def enter(self, entity):
        if entity.location is not Locations.GOLDMINE:
            print(&quot;Walking to the goldmine&quot;)
            entity.location = Locations.GOLDMINE

    def execute(self, entity):
        # Increase the gold
        entity.increase_gold_carried(1)

        # Digging is hard work
        entity.increase_fatigue()

        print(f&quot;Picking up  a nugget, currently have: {entity.gold_carried} &quot;)

        if entity.pockets_full():
            entity.fsm.change_state(VisitBankAndDepositGold())

        if entity.is_thirsty():
            entity.fsm.change_state(QuenchThirst())

    def exit(self, entity):
        print('I am leaving the gold mine with mah pockets full o sweet gold')


class GoHomeAndSleepTilRested(State):
    def enter(self, entity):
        if entity.location is not Locations.SHACK:
            print('Walking home')
            entity.change_location(Locations.SHACK)

    def execute(self, entity):
        if entity is not entity.is_fatigue():
            print('What a darn fantastic nap! Time to find more gold')
            entity.fsm.change_state(EnterMineAndDigForNugget())
        else:
            entity.descrease_fatigue()
            print('ZZZZ....')

    def exit(self, entity):
        print('Leaving the house')
</code></pre>
<p>Thanks for the help !</p>
<p>Edit: traceback</p>
<pre><code>Traceback (most recent call last):
  File &quot;...\west_world\main.py&quot;, line 13, in &lt;module&gt; main()
  File &quot;...\west_world\main.py&quot;, line 9, in main miner.update()
  File &quot;...\west_world\miner.py&quot;, line 28, in update self.fsm.update()
  File &quot;...\west_world\state_machine.py&quot;, line 25, in update self.current_state.execute(self.owner())
  File &quot;...\west_world\state.py&quot;, line 49, in execute
    entity.change_state(EnterMineAndDigForNugget())
AttributeError: 'Miner' object has no attribute 'change_state'
</code></pre>
<p>Edit 2 : I got it to somehow work but the state machine is looping on the same state. It reverts to the initial state every loop, here´s a trace (see the memory reverts to the first line):</p>
<pre><code> start update fsm: &lt;state.GoHomeAndSleepTilRested object at 0x00000116B89D4A60&gt; 
What a darn fantastic nap! Time to find more gold
 start change state: &lt;state.GoHomeAndSleepTilRested object at 0x00000116B8D30130&gt;
Leaving the house
Walking to the goldmine
 end change state: &lt;state.EnterMineAndDigForNugget object at 0x00000116B8D30B50&gt;
 end update fsm: &lt;state.GoHomeAndSleepTilRested object at 0x00000116B89D4A60&gt; 
current state : &lt;state.GoHomeAndSleepTilRested object at 0x00000116B89D4A60&gt;

</code></pre>
",30,0,0,5,python;oop;memory;artificial-intelligence;state-machine,2022-06-06 14:10:58,2022-06-06 14:10:58,2022-06-06 18:08:07,i m currently learning python and going through mat buckland s book programming game ai by example  chapter  on state driven agent design  the book is written in c   i believe  which i know nothing about  i m using the codes example and basically trying to  convert  it to python   a github repo that did part of the examples of the chapter  i did manage the first westworld example up until the creation of a state machine  one can find notes on the website of the author   here s the relevant python code  the statemachine class is new and only the miner class should be changed to create the statemachine   call the update every loop  thanks for the help   edit  traceback edit    i got it to somehow work but the state machine is looping on the same state  it reverts to the initial state every loop  here s a trace  see the memory reverts to the first line  
344,344,8633026,72047014,Including unlabelled data in sklearn pipeline,"<p>I'm setting up a machine learning pipeline to classify some data. I have lots of unlabelled data (i.e. target variable is unknown) that I would like to make use of. One of the ways I would like to do this is to use the unlabelled data to fit the transformers in my pipeline. For example, for the variables I am scaling when <code>StandardScaler</code> is called I want it to fit on the given training data plus the unlabelled data and then transform the training data.</p>
<p>For clarity, outside of a pipeline I can implement it like this:</p>
<pre><code>    all_data =  pd.concat([labelled_data, unlabelled_data])

    s_scaler = StandardScaler()
    s_scaler.fit(all_data)
    scaled_labelled_df = s_scaler.transform(labelled_data)
</code></pre>
<p>Is there a way of implementing this in the sklearn pipeline? I've had a look at the <code>FunctionTransformer</code> method but don't understand how I could use it in this case.</p>
",38,1,0,3,machine-learning;scikit-learn;pipeline,2022-04-28 21:42:14,2022-04-28 21:42:14,2022-06-06 16:02:02,i m setting up a machine learning pipeline to classify some data  i have lots of unlabelled data  i e  target variable is unknown  that i would like to make use of  one of the ways i would like to do this is to use the unlabelled data to fit the transformers in my pipeline  for example  for the variables i am scaling when standardscaler is called i want it to fit on the given training data plus the unlabelled data and then transform the training data  for clarity  outside of a pipeline i can implement it like this  is there a way of implementing this in the sklearn pipeline  i ve had a look at the functiontransformer method but don t understand how i could use it in this case 
345,345,10772654,72510483,Google Cloud Run: interpreting CPU utilization metrics for concurrency,"<p>In GCR docs about concurrency, it's recommended to allow concurrent connections unless you anticipate that each request will max out the CPU/RAM (<a href=""https://cloud.google.com/run/docs/about-concurrency#concurrency-1"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/about-concurrency#concurrency-1</a>).</p>
<p>I'm having trouble interpreting the &quot;metrics&quot; graph (picture below).</p>
<p>Questions:</p>
<ol>
<li><p>Does this mean that my requests are using about 20% of the CPU?  The graph says so, but the legend lists the red line as 95%:17%, which means nothing to me.</p>
</li>
<li><p>If yes (i.e. 20% CPU), does this mean I can safely increase concurrency to 4-5 (20% x 5 = 100%)?</p>
</li>
<li><p>If I increase the number of CPUs, will I see better performance for a single request, or the ability to handle more requests?  (I'm running a machine-learning task that is CPU intensive.)  Maybe that can't be answered without more specifics, but asking in case there's a general answer.</p>
</li>
<li><p>In the &quot;revisions&quot; tab, you can opt into a preview of &quot;slower cold starts but more effective CPU usage&quot;, but it's not clear how much of a tradeoff each is... does anyone have concrete knowledge/experience with &quot;how much slower to start vs. faster to process?&quot;</p>
</li>
</ol>
<p>Thanks!</p>
<p><a href=""https://i.stack.imgur.com/cFrgg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cFrgg.jpg"" alt=""enter image description here"" /></a></p>
",87,1,0,3,google-cloud-platform;concurrency;google-cloud-run,2022-06-06 00:54:16,2022-06-06 00:54:16,2022-06-06 13:33:06,in gcr docs about concurrency  it s recommended to allow concurrent connections unless you anticipate that each request will max out the cpu ram     i m having trouble interpreting the  metrics  graph  picture below   questions  does this mean that my requests are using about   of the cpu   the graph says so  but the legend lists the red line as      which means nothing to me  if yes  i e    cpu   does this mean i can safely increase concurrency to      x        if i increase the number of cpus  will i see better performance for a single request  or the ability to handle more requests    i m running a machine learning task that is cpu intensive    maybe that can t be answered without more specifics  but asking in case there s a general answer  in the  revisions  tab  you can opt into a preview of  slower cold starts but more effective cpu usage   but it s not clear how much of a tradeoff each is    does anyone have concrete knowledge experience with  how much slower to start vs  faster to process   thanks  
346,346,14017369,72513808,"&#39;numpy.ndarray&#39; object is not callable, code is as follows","<p>When I was working on a machine learning project on housing price prediction, I encountered the following problem，it shows:</p>
<pre><code>'numpy.ndarray' object is not callable.
</code></pre>
<p>The error code is as follows:</p>
<pre><code>X = df.values(['bedrooms', 'bathrooms', 'sqft_living',\
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\
       'lat', 'long', 'sqft_living15', 'sqft_lot15'])
y = df['price']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)
</code></pre>
",39,1,0,3,python;pandas;numpy,2022-06-06 11:55:00,2022-06-06 11:55:00,2022-06-06 12:48:51,when i was working on a machine learning project on housing price prediction  i encountered the following problem it shows  the error code is as follows 
347,347,19265664,72492779,Module not found using Input function from tensorflow.keras.layers,"<p>Im quite new learning machine learning and my first project is the creation of a Neural Network in order to detect key facial points on google colab. Everything has been working ok but today when I wanted to train my neural network I came accross with an error that has never appeared before when I trained my neural network.</p>
<p>The error is:</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-189-47fd3efd0229&gt; in &lt;module&gt;()
      5
      6 
----&gt; 7 X_input = Input(input_shape)
      8 
      9 # Zero-padding

4 frames
/usr/lib/python3.7/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

ModuleNotFoundError: No module named 'keras.engine.base_layer_v1'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.
</code></pre>
<p>I don't understand the line <code>ModuleNotFoundError: No module named 'keras.engine.base_layer_v1'</code> because the line that is not working is when I'm using <strong>Input</strong> from <em>tensorflow.keras.layers</em>.</p>
<p>I really don't know what is going on because I never got this error before. I've seen that it could be the version of TensorFlow or maybe my libraries.</p>
<p>I am using 2.3.0 versions in TensorFlow and Keras and these are the libraries I am importing:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
from tensorflow.python.keras import *
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import *
from tensorflow.keras import backend as K
from keras import optimizers
</code></pre>
<p>I would really appreciate any help :)</p>
",41,1,0,4,tensorflow;keras;deep-learning;neural-network,2022-06-03 22:16:44,2022-06-03 22:16:44,2022-06-06 01:38:38,im quite new learning machine learning and my first project is the creation of a neural network in order to detect key facial points on google colab  everything has been working ok but today when i wanted to train my neural network i came accross with an error that has never appeared before when i trained my neural network  the error is  i don t understand the line modulenotfounderror  no module named  keras engine base_layer_v  because the line that is not working is when i m using input from tensorflow keras layers  i really don t know what is going on because i never got this error before  i ve seen that it could be the version of tensorflow or maybe my libraries  i am using    versions in tensorflow and keras and these are the libraries i am importing  i would really appreciate any help   
348,348,17046659,72510756,Importing a List of JSON to Pandas,"<p>I am trying to import a list of json to a pandas MultiIndex. My json currently looks like this:</p>
<pre class=""lang-json prettyprint-override""><code>[
    {
        &quot;concept-tagsConf&quot;: [
            &quot;diagnostic medicine::175.778&quot;,
            &quot;machine learning::175.778&quot;,
            &quot;consumer health::175.778&quot;,
            &quot;natural language::175.778&quot;,
            &quot;artificial intelligence::175.778&quot;,
            &quot;medical record::175.778&quot;,
            &quot;health &amp; medicine::175.778&quot;,
            &quot;survey article::175.778&quot;,
            &quot;cardiology::175.778&quot;,
            &quot;computing system::47.134&quot;,
            &quot;china::62.7681&quot;,
            &quot;ehr system::43.7491&quot;,
            &quot;diagnosis::57.1278&quot;,
            &quot;rural clinic::39.7585&quot;,
            &quot;brilliant ai doctor::38.2777&quot;,
            &quot;human factor::37.2736&quot;,
            &quot;ai-cdss system::97.6451&quot;
        ],
        &quot;cdid&quot;: &quot;arxivorg:CBBFE8A2&quot;,
        &quot;taxnodesConf&quot;: [
            &quot;Genre|Personal|Interview::0.477154&quot;,
            &quot;Genre|Personal::0.477154&quot;,
            &quot;Genre|Research Report::1.0&quot;,
            &quot;Genre|Research Report|New Finding::0.338758&quot;,
            &quot;Industry|Health &amp; Medicine|Diagnostic Medicine::1.0&quot;,
            &quot;Country|North America|United States::1.0&quot;
        ],
        &quot;modified&quot;: &quot;2021-01-12T00:00:00Z&quot;,
        &quot;authorsRaw&quot;: [
            &quot;Wang, Dakuo&quot;,
            &quot;Wang, Liuping&quot;,
            &quot;Zhang, Zhan&quot;,
            &quot;Wang, Ding&quot;,
            &quot;Zhu, Haiyi&quot;,
            &quot;Gao, Yvonne&quot;,
            &quot;Fan, Xiangmin&quot;,
            &quot;Tian, Feng&quot;
        ],
        &quot;title&quot;: &quot;\&quot;Brilliant AI Doctor\&quot; in Rural China: Tensions and Challenges in AI-Powered CDSS Deployment&quot;
    },
    {
        &quot;concept-tagsConf&quot;: [
            &quot;machine learning::262.338&quot;,
            &quot;reinforcement learning::262.338&quot;,
            &quot;participant::50.3558&quot;,
            &quot;representation::81.8108&quot;,
            &quot;connotative state::45.3314&quot;,
            &quot;emotion::79.7823&quot;,
            &quot;null::63.7695&quot;,
            &quot;interaction::55.1516&quot;,
            &quot;agent::262.338&quot;
        ],
        &quot;cdid&quot;: &quot;arxivorg:3E2F104A&quot;,
        &quot;taxnodesConf&quot;: [
            &quot;Industry|Health &amp; Medicine|Therapeutic Area::1.0&quot;,
            &quot;Industry|Education::0.923835&quot;,
            &quot;Industry|Health &amp; Medicine::1.0&quot;,
            &quot;Industry|Health &amp; Medicine|Therapeutic Area|Neurology|Alzheimer\&quot;s Disease: : 0.456842&quot;,
            &quot;Industry|Health &amp; Medicine|Therapeutic Area|Neurology::1.0&quot;,
            &quot;Genre|Overview::0.671961&quot;,
            &quot;Technology|Information Technology|Artificial Intelligence|Machine Learning|Learning Graphical Models::1.0&quot;,
            &quot;Technology|Information Technology|Artificial Intelligence|Cognitive Science::1.0&quot;,
            &quot;Country|North America|United States::1.0&quot;
        ],
        &quot;modified&quot;: &quot;2019-08-15T00:00:00Z&quot;,
        &quot;authorsRaw&quot;: [
            &quot;Hoey, Jesse&quot;,
            &quot;MacKinnon, Neil J.&quot;
        ],
        &quot;title&quot;: &quot;\&quot;Conservatives Overfit, Liberals Underfit\&quot;: The Social-Psychological Control of Affect and Uncertainty&quot;
    },
    {
        &quot;concept-tagsConf&quot;: [
            &quot;machine learning::593.251&quot;,
            &quot;natural language::593.251&quot;,
            &quot;artificial intelligence::593.251&quot;,
            &quot;health &amp; medicine::593.251&quot;,
            &quot;acm::40.6487&quot;,
            &quot;ieee international conference::37.0403&quot;,
            &quot;trb::41.7473&quot;,
            &quot;manuscript::82.8931&quot;,
            &quot;implicit assurance::46.2459&quot;,
            &quot;designer::49.9066&quot;,
            &quot;literature::39.1439&quot;,
            &quot;interaction::43.1621&quot;,
            &quot;explicit assurance::63.5882&quot;
        ],
        &quot;cdid&quot;: &quot;arxivorg:70E22CB3&quot;,
        &quot;taxnodesConf&quot;: [
            &quot;Country|North America::1.0&quot;,
            &quot;Country|North America|United States|Pennsylvania::0.274787&quot;,
            &quot;Industry|Leisure &amp; Entertainment::0.922986&quot;,
            &quot;Industry|Government|Regional Government|North America Government::0.672895&quot;,
            &quot;Industry|Education::1.0&quot;,
            &quot;Industry|Transportation|Ground::0.93171&quot;,
            &quot;Industry|Government|Regional Government::0.672895&quot;,
            &quot;Industry|Government::1.0&quot;,
            &quot;Genre|Research Report|Experimental Study::0.45421&quot;,
            &quot;Genre|Research Report|New Finding::0.454625&quot;,
            &quot;Genre|Instructional Material::1.0&quot;,
            &quot;Genre|Research Report::0.924232&quot;,
            &quot;Genre|Overview::1.0&quot;
        ],
        &quot;modified&quot;: &quot;2017-11-14T00:00:00Z&quot;,
        &quot;authorsRaw&quot;: [
            &quot;Israelsen, Brett W&quot;,
            &quot;Ahmed, Nisar R&quot;
        ],
        &quot;title&quot;: &quot;\&quot;Dave...I can assure you...that it\&quot;s going to be all right...\&quot; -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships&quot;
    }
]
</code></pre>
<p>I can get it to give me a pandas DataFrame that looks like the one below, I just cannot get the MultiIndexing correct.<br />
<a href=""https://i.stack.imgur.com/6CpCa.png"" rel=""nofollow noreferrer"">My Current Pandas Array</a></p>
<p>I want it to look something like this:</p>
<p><a href=""https://i.stack.imgur.com/2FK4d.png"" rel=""nofollow noreferrer"">Goal</a></p>
<p>Do I have to put it into a regular pandas array then add the multiplexing later or do I have to do it when I import the json file?</p>
",23,0,0,5,python;arrays;json;pandas;multi-index,2022-06-06 01:36:25,2022-06-06 01:36:25,2022-06-06 01:37:07,i am trying to import a list of json to a pandas multiindex  my json currently looks like this  i want it to look something like this   do i have to put it into a regular pandas array then add the multiplexing later or do i have to do it when i import the json file 
349,349,2339034,72510660,Springboot RabbitMq no consumer connected,"<p>I'm using springboot and rabbitmq to receive a message.</p>
<p>The first consumer i created works, declared as below:</p>
<pre><code>@Component
public class UserConsumer {

    @Autowired
    private RabbitTemplate template;

    @RabbitListener(queues = MessagingConfig.CONSUME_QUEUE)
    public void consumeMessageFromQueue(MassTransitRequest userRequest) {

     ...
    }
}
</code></pre>
<p>I then needed a second consumer so i duplicated the above and called it another name:</p>
<p>@Component
public class PackConsumer {</p>
<pre><code>    @Autowired
    private RabbitTemplate template;

    @RabbitListener(queues = MessagingConfig.CONSUME_QUEUE_CREATE_PACK)
    public void consumeMessageFromQueue(MassTransitRequest fileRequest) {

     ...
    }
}
</code></pre>
<p>Everything works locally on my machine, however when i deploy it the new queue does not process messages because there is no consumer connected to it. The UserConsumer continues to work.</p>
<p>Is there something else i should be doing in order to connect to the new queue at the same time as the original?</p>
<p>During my learning i did add a &quot;MessagingConfig&quot; class as below, however i believe it relates to sending messages and not receiving them or an alternative configuration:</p>
<pre><code>@Configuration
public class MessagingConfig {

    public static final String CONSUME_QUEUE = &quot;merge-document-request&quot;;
    public static final String CONSUME_EXCHANGE = &quot;merge-document-request&quot;;
    public static final String CONSUME_ROUTING_KEY = &quot;&quot;;

    public static final String PUBLISH_QUEUE = &quot;merge-document-response&quot;;
    public static final String PUBLISH_EXCHANGE = &quot;merge-document-response&quot;;
    public static final String PUBLISH_ROUTING_KEY = &quot;&quot;;

    public static final String CONSUME_QUEUE_CREATE_PACK = &quot;create-pack-request&quot;;
    public static final String CONSUME_EXCHANGE_CREATE_PACK = &quot;create-pack-request&quot;;
    public static final String CONSUME_ROUTING_KEY_CREATE_PACK = &quot;&quot;;

    public static final String PUBLISH_QUEUE_CREATE_PACK = &quot;create-pack-response&quot;;
    public static final String PUBLISH_EXCHANGE_CREATE_PACK = &quot;create-pack-response&quot;;
    public static final String PUBLISH_ROUTING_KEY_CREATE_PACK = &quot;&quot;;

    @Bean
    public Queue queue() {
        return new Queue(CONSUME_QUEUE);
    }

    @Bean
    public TopicExchange exchange() {
        return new TopicExchange(CONSUME_EXCHANGE);
    }

    @Bean
    public Binding binding(Queue queue, TopicExchange exchange) {
        return BindingBuilder.bind(queue).to(exchange).with(CONSUME_ROUTING_KEY);
    }

    @Bean
    public MessageConverter converter() {
        return new Jackson2JsonMessageConverter();
    }

    @Bean
    public AmqpTemplate template(ConnectionFactory connectionFactory) {
        final RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
        rabbitTemplate.setMessageConverter(converter());
        
        return rabbitTemplate;
    }
}
</code></pre>
<p>Thanks in advance</p>
",12,0,0,1,spring-amqp,2022-06-06 01:19:15,2022-06-06 01:19:15,2022-06-06 01:19:15,i m using springboot and rabbitmq to receive a message  the first consumer i created works  declared as below  i then needed a second consumer so i duplicated the above and called it another name  everything works locally on my machine  however when i deploy it the new queue does not process messages because there is no consumer connected to it  the userconsumer continues to work  is there something else i should be doing in order to connect to the new queue at the same time as the original  during my learning i did add a  messagingconfig  class as below  however i believe it relates to sending messages and not receiving them or an alternative configuration  thanks in advance
350,350,15696495,67167708,How can I get Octave GUI to load/read .m files correctly in editor and console?,"<p>I am taking a class on using Octave for machine learning algorithms, and as part of the assignments, they provide a series of .m files to build upon with our own code as well as to run for submission credit through the auto-grader. My problem is that the .m files load perfectly fine in a regular text editor program like Atom or Notepad, but in Octave, the files are best described as nonsense, and thus will not run in the console. If I open the files in a regular editor and copy/paste over the crazy into Octave, it seems to save it and reopen fine. But, I have close to 20 files for the first project alone, and this solution is untenable in the long run. I have a screenshot of how it's loading. Is there some setting I need to change? Uninstall/reinstall Octave? I'm new to Octave and the Octave GUI, and I'm striking out with Google for a solution. I am using version 6.2.0. Thank you for any help/advice!</p>
<p><a href=""https://i.stack.imgur.com/6azjV.png"" rel=""nofollow noreferrer"">screenshot of how octave is loading my .m files</a></p>
<p>Update: I responded to this in a comment below, but I tried loading it another way into Octave GUI and received the following error: &quot;&gt;&gt; error: load: unable to determine file format of 'C:/Users/sophi/documents/octave/assignment_1/computeCost.m&quot; This tracks for me because it makes sense why it would open the files in such a weird way. It's simply not sure what they are. However, I created my own simple functions from scratch to test, saved them as .m files, and was able to run them perfectly fine. I'm including one of the files below. Maybe there's a key in the formatting of the files offered by the class which is impacting Octave's ability to process it correctly?</p>
<pre><code>function plotData(x, y)
%PLOTDATA Plots the data points x and y into a new figure
%   PLOTDATA(x,y) plots the data points and gives the figure axes labels of
%   population and profit.

figure; % open a new figure window
% ====================== YOUR CODE HERE ======================
% Instructions:  .... goes on about assignment
% ============================================================
end
</code></pre>
<p><strong>MOST RECENT UPDATE:</strong> The plot thickens. Yesterday, I was able to open the files I created and run them in the Octave environment, and I (wrongly) assumed they would still work today. They are doing the same stupid thing as the files included by the course. <strong>I checked inside preferences for the editor, and it says it is loading and saving them as IBM273 if that helps.</strong> Thank you for everyone has pitched in ideas. I really appreciate it!</p>
",308,2,1,2,octave;octave-gui,2021-04-20 00:16:47,2021-04-20 00:16:47,2022-06-06 00:33:19,i am taking a class on using octave for machine learning algorithms  and as part of the assignments  they provide a series of  m files to build upon with our own code as well as to run for submission credit through the auto grader  my problem is that the  m files load perfectly fine in a regular text editor program like atom or notepad  but in octave  the files are best described as nonsense  and thus will not run in the console  if i open the files in a regular editor and copy paste over the crazy into octave  it seems to save it and reopen fine  but  i have close to  files for the first project alone  and this solution is untenable in the long run  i have a screenshot of how it s loading  is there some setting i need to change  uninstall reinstall octave  i m new to octave and the octave gui  and i m striking out with google for a solution  i am using version     thank you for any help advice   update  i responded to this in a comment below  but i tried loading it another way into octave gui and received the following error    gt  gt  error  load  unable to determine file format of  c  users sophi documents octave assignment_ computecost m  this tracks for me because it makes sense why it would open the files in such a weird way  it s simply not sure what they are  however  i created my own simple functions from scratch to test  saved them as  m files  and was able to run them perfectly fine  i m including one of the files below  maybe there s a key in the formatting of the files offered by the class which is impacting octave s ability to process it correctly  most recent update  the plot thickens  yesterday  i was able to open the files i created and run them in the octave environment  and i  wrongly  assumed they would still work today  they are doing the same stupid thing as the files included by the course  i checked inside preferences for the editor  and it says it is loading and saving them as ibm if that helps  thank you for everyone has pitched in ideas  i really appreciate it 
351,351,12585467,72428936,Why does model.learn() return a numpy error?,"<p>I am trying to train my model, which is a breakout game in gym. I am trying to train the environment with 100000 timesteps. However, it keeps returning this error message. Can someone explain why and help me solve this?
I am a beginner in machine learning.
Here is the code and the error message below:</p>
<pre><code>import gym
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_atari_env
import os
import numpy as np
env = make_atari_env(&quot;ALE/Breakout-v5&quot;, n_envs=4, seed=0)
env = VecFrameStack(env, n_stack=4)
log_path = os.path.join(&quot;Traning&quot;, &quot;Logs&quot;)
model = A2C(&quot;CnnPolicy&quot;, env, verbose=1, tensorboard_log=log_path)
model.learn(total_timesteps=100000)

ERROR MESSAGE:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_7676/2886439321.py in &lt;module&gt;
----&gt; 1 model.learn(total_timesteps=100000)

D:\Anaconda\lib\site-packages\stable_baselines3\a2c\a2c.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    189     ) -&gt; &quot;A2C&quot;:
    190 
--&gt; 191         return super(A2C, self).learn(
    192             total_timesteps=total_timesteps,
    193             callback=callback,

D:\Anaconda\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    240         iteration = 0
    241 
--&gt; 242         total_timesteps, callback = self._setup_learn(
    243             total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, tb_log_name
    244         )

D:\Anaconda\lib\site-packages\stable_baselines3\common\base_class.py in _setup_learn(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)
    427         # Avoid resetting the environment when calling ``.learn()`` consecutive times
    428         if reset_num_timesteps or self._last_obs is None:
--&gt; 429             self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch
    430             self._last_episode_starts = np.ones((self.env.num_envs,), dtype=bool)
    431             # Retrieve unnormalized observation for saving into the buffer

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py in reset(self)
    108         Reset all environments
    109         &quot;&quot;&quot;
--&gt; 110         return self.transpose_observations(self.venv.reset())
    111 
    112     def close(self) -&gt; None:

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\vec_frame_stack.py in reset(self)
     56         Reset all environments
     57         &quot;&quot;&quot;
---&gt; 58         observation = self.venv.reset()  # pytype:disable=annotation-type-mismatch
     59 
     60         observation = self.stackedobs.reset(observation)

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py in reset(self)
     59     def reset(self) -&gt; VecEnvObs:
     60         for env_idx in range(self.num_envs):
---&gt; 61             obs = self.envs[env_idx].reset()
     62             self._save_obs(env_idx, obs)
     63         return self._obs_from_buf()

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    290 
    291     def reset(self, **kwargs):
--&gt; 292         return self.env.reset(**kwargs)
    293 
    294     def render(self, mode=&quot;human&quot;, **kwargs):

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    331 class RewardWrapper(Wrapper):
    332     def reset(self, **kwargs):
--&gt; 333         return self.env.reset(**kwargs)
    334 
    335     def step(self, action):

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    317 class ObservationWrapper(Wrapper):
    318     def reset(self, **kwargs):
--&gt; 319         observation = self.env.reset(**kwargs)
    320         return self.observation(observation)
    321 

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
     57 
     58     def reset(self, **kwargs) -&gt; np.ndarray:
---&gt; 59         self.env.reset(**kwargs)
     60         obs, _, done, _ = self.env.step(1)
     61         if done:

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
    104         &quot;&quot;&quot;
    105         if self.was_real_done:
--&gt; 106             obs = self.env.reset(**kwargs)
    107         else:
    108             # no-op step to advance from terminal/lost life state

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
    152 
    153     def reset(self, **kwargs) -&gt; GymObs:
--&gt; 154         return self.env.reset(**kwargs)
    155 
    156 

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
     34             noops = self.override_num_noops
     35         else:
---&gt; 36             noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)
     37         assert noops &gt; 0
     38         obs = np.zeros(0)

AttributeError: 'numpy.random._generator.Generator' object has no attribute 'randint'
</code></pre>
",103,1,0,5,python;numpy;reinforcement-learning;openai-gym;stable-baselines,2022-05-30 09:13:04,2022-05-30 09:13:04,2022-06-05 23:12:39,
352,352,18968336,72506868,Flask machine learning model deployement,"<p>I'm trying to deploy my pretrained model on a flask app but i get this error:</p>
<pre><code>_pickle.UnpicklingError: A load persistent id instruction was encountered,
</code></pre>
<p>but no persistent_load function was specified.</p>
<pre><code>
from flask import Flask, render_template,request,redirect
import pickle


app =Flask(__name__)


@app.route('/')
def Index():
    return render_template('index.html')

@app.route('/aspect',methods=['GET'])
def get():
    print(request.form)
    txt=request.form.get('txt')
    with open('best_model_state.bin', 'rb') as f_in:
       model = pickle.load(f_in)
       prediction= model.predict([txt])
       print(prediction)

if __name__==&quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
",26,0,-2,3,python;flask;machine-learning,2022-06-05 16:46:47,2022-06-05 16:46:47,2022-06-05 22:03:49,i m trying to deploy my pretrained model on a flask app but i get this error  but no persistent_load function was specified 
353,353,19273681,72504902,Jupyter Notebook ModuleNotFound Error with scikit-learn,"<p>Wanted to use scikit-learn for a machine learning project on jupyter notebook but despite downloading it through cmd and satisfying all its dependencies and it showing up in !pip list, when I try to import something from it, jupyter notebook returns ModuleNotFound Error: No module named 'sklearn'. Help! <a href=""https://i.stack.imgur.com/xeUMc.png"" rel=""nofollow noreferrer"">1</a></p>
<p><a href=""https://i.stack.imgur.com/ASfOM.png"" rel=""nofollow noreferrer"">2</a></p>
",24,0,-1,3,python;scikit-learn;jupyter-notebook,2022-06-05 10:27:22,2022-06-05 10:27:22,2022-06-05 19:11:15,wanted to use scikit learn for a machine learning project on jupyter notebook but despite downloading it through cmd and satisfying all its dependencies and it showing up in  pip list  when i try to import something from it  jupyter notebook returns modulenotfound error  no module named  sklearn   help   
354,354,19261923,72498795,How can I visualize this particular EEG dataset?,"<p>I am working on a project and I have been tasked to have more data visualization in my machine learning model. I'm facing a lot of trouble as to how should I visualize this EEG dataset. I have included a snippet of the dataset. Can someone help me with this?
<a href=""https://i.stack.imgur.com/W35i9.png"" rel=""nofollow noreferrer"">EEG dataset with an outcome which is the y variable in my model</a></p>
<p>The dataset snippet :-
<a href=""https://docs.google.com/spreadsheets/d/1FccSDrLvr_Ynda-XkhxCkX9Zj2zakVtBhIINgAdhQiQ/edit#gid=2117604179"" rel=""nofollow noreferrer"">https://docs.google.com/spreadsheets/d/1FccSDrLvr_Ynda-XkhxCkX9Zj2zakVtBhIINgAdhQiQ/edit#gid=2117604179</a></p>
",34,0,-2,5,python;machine-learning;jupyter-notebook;data-visualization;signal-processing,2022-06-04 15:25:54,2022-06-04 15:25:54,2022-06-05 18:14:09,
355,355,4981251,72506870,Pre-processing single feature containing different scales,"<p>How do I preprocess this data containing a single feature with different scales?  This will then be used for supervised machine learning classification.</p>
<p><strong>Data</strong></p>
<pre><code>import pandas as pd
import numpy as np
np.random.seed = 4

df_eur_jpy = pd.DataFrame({&quot;value&quot;: np.random.default_rng().uniform(0.07, 3.85, 50)})
df_usd_cad = pd.DataFrame({&quot;value&quot;: np.random.default_rng().uniform(0.0004, 0.02401, 50)})

df_usd_cad[&quot;ticker&quot;] = &quot;usd_cad&quot;
df_eur_jpy[&quot;ticker&quot;] = &quot;eur_jpy&quot;

df = pd.concat([df_eur_jpy,df_usd_cad],axis=0)

df.head(1)

    value   ticker
0   0.161666    eur_jpy
</code></pre>
<p>We can see the different tickers contain data with a different scale when looking at the max/min of this groupby:</p>
<pre><code>df.groupby(&quot;ticker&quot;)[&quot;value&quot;].agg(['min', 'max'])


         min        max
ticker      
eur_jpy 0.079184    3.837519
usd_cad 0.000405    0.022673
</code></pre>
<p>I have many tickers in my real data and would like to combine all of these in the one feature (pandas column) and use with an estimator in sci-kit learn for supervised machine learning classification.</p>
",30,1,1,3,python;pandas;scikit-learn,2022-06-05 16:47:02,2022-06-05 16:47:02,2022-06-05 17:46:47,how do i preprocess this data containing a single feature with different scales   this will then be used for supervised machine learning classification  data we can see the different tickers contain data with a different scale when looking at the max min of this groupby  i have many tickers in my real data and would like to combine all of these in the one feature  pandas column  and use with an estimator in sci kit learn for supervised machine learning classification 
356,356,17274335,72506288,Prepocessing textual data for Machine Learning,"<pre><code>'''
    import pandas as pd 
    import re
    articles_data = pd.read_csv('C:/Users/amrit/Downloads/data.csv') 
    print(articles_data.apply(lambda x: sum(x.isnull()))) 
    articles_nonNull = articles_data.dropna(subset=['text']) 
    articles_nonNull.reset_index(inplace=True)


    def clean_text(text):



    #Make text lowercase, remove text in square brackets,remove \n,remove punctuation and 
    remove words containing numbers.

    
    text = str(text).lower()
    text = re.sub('&lt;.*?&gt;+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

    articles_nonNull['text_clean’] = articles_nonNull['text']
                                     .apply(lambda x:clean_text(x))

'''
</code></pre>
<p>I am trying to use this code to preprocess my data and keep hitting the &quot;invalid syntax error&quot; on the last two lines from articles_nonNull['text_clean']. Could someone help with this and why this is happening?</p>
<p>P.S. I am new to NLP and this is the first time I am handling an exceptionally large unstructured dataset.</p>
",20,1,-1,3,python;machine-learning;nlp,2022-06-05 15:14:46,2022-06-05 15:14:46,2022-06-05 15:29:45,i am trying to use this code to preprocess my data and keep hitting the  invalid syntax error  on the last two lines from articles_nonnull  text_clean    could someone help with this and why this is happening  p s  i am new to nlp and this is the first time i am handling an exceptionally large unstructured dataset 
357,357,17102070,72498809,Get Colab NoteBook result as iframe,"<p><strong>Hi</strong> ,i'm using colab ,is there any way i can get the result of Machine learning (chart )as iframe</p>
<p>i want to get a chart as iframe , is there any other online way</p>
",18,0,-1,3,python;iframe;google-colaboratory,2022-06-04 15:27:54,2022-06-04 15:27:54,2022-06-05 12:59:01,hi  i m using colab  is there any way i can get the result of machine learning  chart  as iframe i want to get a chart as iframe   is there any other online way
358,358,4169493,71003961,How to Apply customize analytical model in OBIEE12c,"<p>I would like to integrate machine learning model developed by python with obiee 12c dashboard.</p>
",25,1,0,2,oracle;obiee,2022-02-06 08:50:20,2022-02-06 08:50:20,2022-06-05 12:58:37,i would like to integrate machine learning model developed by python with obiee c dashboard 
359,359,17066064,71629070,can not plot a graph using matplotlib showing error,"<p>Exception has occurred: ImportError
dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/PIL/_imaging.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_xcb_connect'
File &quot;/Users/showrov/Desktop/Machine learning/Preprosessing/import_dataset.py&quot;, line 2, in &lt;module&gt;
import matplotlib.pyplot as plt</p>
<p>import matplotlib.pyplot as plt</p>
<p>import pandas as pd</p>
<p>import numpy as np</p>
<p>import sys</p>
<p>print(sys.version)</p>
<p>data=pd.read_csv('Data_customer.csv')</p>
<p>print(data)</p>
<p>plt.plot(data[:2],data[:2])</p>
",32,1,0,1,matplotlib,2022-03-26 20:20:49,2022-03-26 20:20:49,2022-06-05 10:28:43,import matplotlib pyplot as plt import pandas as pd import numpy as np import sys print sys version  data pd read_csv  data_customer csv   print data  plt plot data    data    
360,360,10726764,72480756,How to ensure data imported matches sklearn requirements for processing?,"<p>I am trying to get started on some machine learning on SKlearn I am getting this massive error that I have no idea where to start</p>
<p>*</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_14812/623750250.py in &lt;module&gt;
----&gt; 1 model.fit(X_train,y_train)
      2 predictions = model.predict(X_test)

ValueError: could not convert string to float: '[38.66999816894531, 39.125, 37.0, 37.29999923706055, 31.34000015258789, 31.0, 32.54999923706055, 32.0, 28.64999961853027, 30.5, 30.25, 30.0, 28.68000030517578, 30.13999938964844, 34.5, 34.5, 33.38000106811523, 
</code></pre>
<ul>
<li></li>
</ul>
<p>This is my code:</p>
<pre><code>df = pd.read_excel('C:/Users/radoy/Programming/ML Chart Patterns/xx_combined.xlsx')
X = df.drop(columns=['consolidating'])
y = df['consolidating']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

model = DecisionTreeClassifier()

model.fit(X_train,y_train)
predictions = model.predict(X_test)

score = accuracy_score(y_test, predictions)
score

</code></pre>
<p>And this is an excerpt of the data, a lot of lists of different lengths. I have about 100 lines of data and am aware that this is highly insufficient, but I just want to get started from somewhere.</p>
<p><a href=""https://i.stack.imgur.com/4pW7q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4pW7q.png"" alt=""enter image description here"" /></a></p>
",43,0,2,2,pandas;scikit-learn,2022-06-02 23:52:23,2022-06-02 23:52:23,2022-06-05 02:20:04,i am trying to get started on some machine learning on sklearn i am getting this massive error that i have no idea where to start   this is my code  and this is an excerpt of the data  a lot of lists of different lengths  i have about  lines of data and am aware that this is highly insufficient  but i just want to get started from somewhere  
361,361,2451456,72409120,"Unity&#39;s ml-agents assets throw warnings and errors [PushBlockWithInput, Actuator, Barracuda]","<h2>The Problem</h2>
<p>I'm trying to work with <a href=""https://unity.com/products/machine-learning-agents"" rel=""nofollow noreferrer"">Unity Machine Learning Agents</a> and encountered problems during the setup. When I try to import the assets from <a href=""https://github.com/Unity-Technologies/ml-agents"" rel=""nofollow noreferrer"">Unity's ml-agents git</a> into Unity, I get 999+ warnings and 32 errors.</p>
<p><a href=""https://i.stack.imgur.com/FIVsQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FIVsQ.jpg"" alt=""enter image description here"" /></a></p>
<p>In my case the majority of the warnings seem to come from <code>PushBlockWithInput</code>, <code>PushblockActions</code> and <code>PushBlockWithInputPlayerController</code> missing <code>UnityEngine.InputSystem</code> and <code>Unity.MLAgents.Extensions.Input</code>.</p>
<p><a href=""https://i.stack.imgur.com/dH4Gj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dH4Gj.jpg"" alt=""enter image description here"" /></a></p>
<h2>What I've tried</h2>
<p>I've followed the <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a> from Unity's ml-agents git and was successful in installing a Python 3.7 environment with Anaconda, PyTorch with Cuda, and the Unity's ml-agents python package via pip. When trying to verify the ml-agents python package works with <code>mlagents-learn --help</code>, I first got an exception but could resolve that by updating <code>protobuf==3.20.1</code> as per <a href=""https://discuss.streamlit.io/t/typeerror-descriptors-cannot-not-be-created-directly/25639/3"" rel=""nofollow noreferrer"">suggestion from a forum</a> (just mentioning this in case it is relevant). I downloaded the C# package from Unity's package manager and tried it for several versions (<code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.1.0 (Preview) -- lastest</code>).</p>
<p>Instructions to create a first test scene with assets from Unity's ml-agents git suggest making a new 3D project in Unity and drag and drop the folder <code>projects/assets/ml-agents</code> into the project's assets. At this point, Unity is showing many errors and warnings in the Terminal. It still has the examples in the assets but every element in the scene is full of warnings.</p>
<p>I have also switched from Unity's ml-agents git's main branch to the <a href=""https://github.com/Unity-Technologies/ml-agents/tree/release_19_branch"" rel=""nofollow noreferrer"">release 19 branch</a> and also tried other versions of the Barracuda package, e.g. <code>Version 3.0.0</code>, which seems to remove the warnings, but not the errors and instead gives these notifications:</p>
<p><a href=""https://i.stack.imgur.com/gQZqW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gQZqW.jpg"" alt=""enter image description here"" /></a></p>
<p>However, warnings still show up in the assets' settings:</p>
<p><a href=""https://i.stack.imgur.com/rQWDb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rQWDb.jpg"" alt=""enter image description here"" /></a></p>
<p>I've also tried to create a new Unity project with the <code>ml-agent package 1.9.1 (Preview)</code> with the right Barracuda version, and the release 19 branch of Unity's ml-agents git, without success (now it's 53 warnings and 70 errors). Now also the Actuators are not found, which seems to be a more common problem on its own.</p>
<h2>My setup</h2>
<p>I'm working on a machine with Windows 11</p>
<ul>
<li><code>Unity Version is 2020.3.32f1 Personal &lt;DX11&gt;</code></li>
<li>The Unity <code>ml-agent package</code> was tried with <code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.0.2 (Preview)</code></li>
<li>The Unity <code>ML Agents Extensions</code> package 0.6.1 (preview)</li>
<li>Python Version is, as per <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a>, <code>3.7 with an Anaconda virtual environment</code></li>
<li>Unity's ml-agents git is <code>main</code>, as well as <code>release_19_branch</code></li>
<li>C# editor would be <code>Visual Studio Code 1.67.2</code></li>
</ul>
<h2>Things I found out so far</h2>
<p>This problem seems to be somewhat common, I've found several variations of similar problems over a couple of years, some more specific to the <a href=""https://stackoverflow.com/questions/64085348/unity-ml-agents-package-manager-is-not-importing-actuator-script"">Actuators</a> missing, some more <a href=""https://github.com/Unity-Technologies/ml-agents/issues/3027"" rel=""nofollow noreferrer"">general</a>. Some are posting about <a href=""https://forum.unity.com/threads/cannot-find-unityengine-inputsystem.807645/"" rel=""nofollow noreferrer"">problems with the InputSystem</a> as well, but seemingly different solutions and mixed reactions to the solutions.</p>
<p>There are various suggestions, about version changes for Unity, the ml-agents package, and Visual Studio Code. Other solutions involve downloading additional packages in Visual Studio or Unity. Some suggest editing scripts within the cloned git repository. Although most of these threads are from the past 2 years. I've spent two days trying to set this up and fix this and am just about to give up on the ml-agents package. A shame the installation process for a seemingly great resource seems so infeasible. I'd appreciate further suggestions or directions on additional resources on how to set up this package.</p>
",113,1,2,4,python;c#;unity3d;ml-agent,2022-05-27 23:02:29,2022-05-27 23:02:29,2022-06-05 01:17:48,i m trying to work with  and encountered problems during the setup  when i try to import the assets from  into unity  i get   warnings and  errors   in my case the majority of the warnings seem to come from pushblockwithinput  pushblockactions and pushblockwithinputplayercontroller missing unityengine inputsystem and unity mlagents extensions input   i ve followed the  from unity s ml agents git and was successful in installing a python   environment with anaconda  pytorch with cuda  and the unity s ml agents python package via pip  when trying to verify the ml agents python package works with mlagents learn   help  i first got an exception but could resolve that by updating protobuf     as per   just mentioning this in case it is relevant   i downloaded the c  package from unity s package manager and tried it for several versions      verified       preview       and     preview     lastest   instructions to create a first test scene with assets from unity s ml agents git suggest making a new d project in unity and drag and drop the folder projects assets ml agents into the project s assets  at this point  unity is showing many errors and warnings in the terminal  it still has the examples in the assets but every element in the scene is full of warnings  i have also switched from unity s ml agents git s main branch to the  and also tried other versions of the barracuda package  e g  version     which seems to remove the warnings  but not the errors and instead gives these notifications   however  warnings still show up in the assets  settings   i ve also tried to create a new unity project with the ml agent package     preview  with the right barracuda version  and the release  branch of unity s ml agents git  without success  now it s  warnings and  errors   now also the actuators are not found  which seems to be a more common problem on its own  i m working on a machine with windows  this problem seems to be somewhat common  i ve found several variations of similar problems over a couple of years  some more specific to the  missing  some more   some are posting about  as well  but seemingly different solutions and mixed reactions to the solutions  there are various suggestions  about version changes for unity  the ml agents package  and visual studio code  other solutions involve downloading additional packages in visual studio or unity  some suggest editing scripts within the cloned git repository  although most of these threads are from the past  years  i ve spent two days trying to set this up and fix this and am just about to give up on the ml agents package  a shame the installation process for a seemingly great resource seems so infeasible  i d appreciate further suggestions or directions on additional resources on how to set up this package 
362,362,19131126,72500836,AttributeError: module &#39;tensorflow&#39; has no attribute &#39;to_float&#39;,"<p>I'm trying to train a machine learning model for automatic segmentation of the heart. I use tensorflow 2.8.0. Here is a piece of my code:</p>
<pre><code>for step in range(steps):
    print('\n****** Epoch {} Step {} ******'.format(epoch, step))
    batch_img0, batch_img1, batch_mask0, batch_mask1 = next(train_generator)
    print(model.train_on_batch([batch_img0, batch_img1, batch_mask0], batch_mask1, sample_weight=None, class_weight=None))
</code></pre>
<p>At the level of this for loop, I receive the following error:</p>
<pre><code>  File &quot;C:\Users\talba\Downloads\base de données\CardiacMotionFlow-master\segmentation\finetune_lvrv_net.py&quot;, line 323, in finetune_lvrv_net
    print(model.train_on_batch([batch_img0, batch_img1, batch_mask0], 
  File &quot;C:\Users\talba\Downloads\jupiter\lib\site-packages\keras\engine\training.py&quot;, line 2093, in train_on_batch
    logs = self.train_function(iterator)
  File &quot;C:\Users\talba\Downloads\jupiter\lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;, line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\talba\Downloads\jupiter\lib\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1147, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    File &quot;C:\Users\talba\Downloads\jupiter\lib\site-packages\keras\engine\training.py&quot;, line 1021, in train_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\talba\Downloads\base de données\CardiacMotionFlow-master\segmentation\..\helpers.py&quot;, line 1356, in dice_coef5_0  *
        y_pred = tf.to_float(y_pred)

    AttributeError: module 'tensorflow' has no attribute 'to_float'
</code></pre>
<p>The error is apparently at the level of this line</p>
<pre><code>print(model.train_on_batch([batch_img0, batch_img1, batch_mask0], batch_mask1, sample_weight=None, class_weight=None))
</code></pre>
<p>And I receive the attribute error <code>AttributeError: module 'tensorflow' has no attribute 'to_float'</code>
yet I did not write a line with float in my code. I do not understand. Please can someone help me solve this problem?</p>
",46,0,0,2,python;tensorflow,2022-06-04 20:38:57,2022-06-04 20:38:57,2022-06-04 20:46:21,i m trying to train a machine learning model for automatic segmentation of the heart  i use tensorflow     here is a piece of my code  at the level of this for loop  i receive the following error  the error is apparently at the level of this line
363,363,5013636,72499567,How to use the &#39;adaboost&#39; method to Build Classification Trees wthin the Caret and fastAdaboost Packages in R,"<p><strong>Issue</strong></p>
<p>I'm attempting to use the <code>'adaboost'</code> method within the <code>Caret</code> and <code>fastAdaboost</code> packages. My objective is to build a <code>classification tree</code> using `machine learning techniques in R for an upcoming project at university and I am following this tutorial <a href=""https://www.machinelearningplus.com/machine-learning/caret-package/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>For this model <strong>(see below)</strong>, I have downloaded the libraries <code>caret</code> and <code>fastAdaboost</code> and whenever I try to run my model, I get the message.</p>
<pre><code>Error: object 'model_adaboost' not found
</code></pre>
<p>I don't understand what's wrong with this code <strong>(see below)</strong> as it's identical to my other models and I don't know why R cannot find my model.</p>
<p>Many thanks if anyone can lend a hand.</p>
<p><strong>These models are running just fine:</strong></p>
<pre><code>#Random Forest
**# Train the model using rf
model_rf = train(Country ~., data=train.data, method='rf', metric=metric, tuneLength= tuneLength, trControl = fitControl)

model_rf

#Naive Bayes
nb_tune &lt;- data.frame(usekernel = TRUE, fL = 0, adjust=seq(0, 5, by = 1))

model.nb1 = train(Country ~., data=train.data,'nb', trControl=fitControl, metric=metric, tuneLength=tuneLength, tuneGrid = nb_tune, laplace = 0:3)
model.nb1
</code></pre>
<p><strong>Structure of my data frame</strong></p>
<pre><code>data.frame':    367 obs. of  10 variables:
 $ Country    : Factor w/ 3 levels &quot;Italy&quot;,&quot;Turkey&quot;,..: 2 3 1 3 2 3 3 2 3 3 ...
 $ Low.Freq   : num  -0.1 0.381 0.705 0.441 -0.603 ...
 $ High.Freq  : num  -0.503 0.96 -0.371 0.207 -0.336 ...
 $ Peak.Freq  : num  -0.4751 -0.0966 -0.2089 -0.1952 -0.3184 ...
 $ Delta.Freq : num  -0.334 0.122 -0.567 -0.148 -0.132 ...
 $ Delta.Time : num  -0.445 1.565 -1.145 0.131 0.666 ...
 $ Peak.Time  : num  0.0289 0.1897 -0.4765 -0.029 0.1492 ...
 $ Center.Freq: num  -0.5294 -0.0507 -0.1589 -0.0819 -0.405 ...
 $ Start.Freq : num  0.672 1.787 0.403 0.388 -1.068 ...
 $ End.Freq   : num  -0.5393 -0.8247 -0.0148 -0.9138 0.0482 ...
</code></pre>
<p><strong>R CODE</strong></p>
<pre><code>library(caret)
library(fastAdaboost)

#Data is 'Clusters_Dummy_2'

##Produce a new version of the dataframe 'Clusters_Dummy' with the rows shuffled
NewClusters=Cluster_Dummy_2[sample(1:nrow(Cluster_Dummy_2)),]

#Produce a dataframe
NewCluster&lt;-as.data.frame(NewClusters)

#display
print(NewCluster)

#Check the structure of the data
str(NewCluster)

#Number of rows
nrow(NewCluster)

#Split the data frame into 70% to 30% train and test data
training.parameters &lt;- Cluster_Dummy_2$Country %&gt;% 
createDataPartition(p = 0.7, list = FALSE)
train.data &lt;- NewClusters[training.parameters, ]
test.data &lt;- NewClusters[-training.parameters, ]


##Auxiliary function for controlling model fitting      

fitControl &lt;- trainControl(## 10-fold CV
                          method = &quot;repeatedcv&quot;,
                          number = 10,
                          ## repeated ten times
                          repeats = 10,
                          classProbs = TRUE,
                          verbose = TRUE)


fitGrid_2 &lt;- expand.grid(mfinal = (1:3)*3,         # This is new!
                         maxdepth = c(1, 3),       # ...and this
                         coeflearn = c(&quot;Breiman&quot;),
                         iter=100) # ...and this

model_adaboost = train(Country ~ ., data=train.data, method='adaboost', tuneLength = tuneLength, metric=metric, trControl = fitControl,
                       tuneGrid=fitGrid_2, verbose=TRUE)
model_adaboost
</code></pre>
<p><strong>Data</strong></p>
<pre><code> structure(list(Low.Freq = c(435L, 94103292L, 1L, 2688L, 8471L, 
    28818L, 654755585L, 468628164L, 342491L, 2288474L, 3915L, 411L, 
    267864894L, 3312618L, 5383L, 8989443L, 1894L, 534981L, 9544861L, 
    3437614L, 475386L, 7550764L, 48744L, 2317845L, 5126197L, 2445L, 
    8L, 557450L, 450259742L, 21006647L, 9L, 7234027L, 59L, 9L, 605L, 
    9199L, 3022L, 30218156L, 46423L, 38L, 88L, 396396244L, 28934316L, 
    7723L, 95688045L, 679354L, 716352L, 76289L, 332826763L, 6L, 90975L, 
    83103577L, 9529L, 229093L, 42810L, 5L, 18175302L, 1443751L, 5831L, 
    8303661L, 86L, 778L, 23947L, 8L, 9829740L, 2075838L, 7434328L, 
    82174987L, 2L, 94037071L, 9638653L, 5L, 3L, 65972L, 0L, 936779338L, 
    4885076L, 745L, 8L, 56456L, 125140L, 73043989L, 516476L, 7L, 
    4440739L, 612L, 3966L, 8L, 9255L, 84127L, 96218L, 5690L, 56L, 
    3561L, 78738L, 1803363L, 809369L, 7131L, 0L), High.Freq = c(6071L, 
    3210L, 6L, 7306092L, 6919054L, 666399L, 78L, 523880161L, 4700783L, 
    4173830L, 30L, 811L, 341014L, 780L, 44749L, 91L, 201620707L, 
    74L, 1L, 65422L, 595L, 89093186L, 946520L, 6940919L, 655350L, 
    4L, 6L, 618L, 2006697L, 889L, 1398L, 28769L, 90519642L, 984L, 
    0L, 296209525L, 487088392L, 5L, 894L, 529L, 5L, 99106L, 2L, 926017L, 
    9078L, 1L, 21L, 88601017L, 575770L, 48L, 8431L, 194L, 62324996L, 
    5L, 81L, 40634727L, 806901520L, 6818173L, 3501L, 91780L, 36106039L, 
    5834347L, 58388837L, 34L, 3280L, 6507606L, 19L, 402L, 584L, 76L, 
    4078684L, 199L, 6881L, 92251L, 81715L, 40L, 327L, 57764L, 97668898L, 
    2676483L, 76L, 4694L, 817120L, 51L, 116712L, 666L, 3L, 42841L, 
    9724L, 21L, 4L, 359L, 2604L, 22L, 30490L, 5640L, 34L, 51923625L, 
    35544L), Peak.Freq = c(87005561L, 9102L, 994839015L, 42745869L, 
    32840L, 62737133L, 2722L, 24L, 67404881L, 999242982L, 3048L, 
    85315406L, 703037627L, 331264L, 8403609L, 3934064L, 50578953L, 
    370110665L, 3414L, 12657L, 40L, 432L, 7707L, 214L, 68588962L, 
    69467L, 75L, 500297L, 704L, 1L, 102659072L, 60896923L, 4481230L, 
    94124925L, 60164619L, 447L, 580L, 8L, 172L, 9478521L, 20L, 53L, 
    3072127L, 2160L, 27301893L, 8L, 4263L, 508L, 712409L, 50677L, 
    522433683L, 112844L, 193385L, 458269L, 93578705L, 22093131L, 
    6L, 9L, 1690461L, 0L, 4L, 652847L, 44767L, 21408L, 5384L, 304L, 
    721L, 651147L, 2426L, 586L, 498289375L, 945L, 6L, 816L, 46207L, 
    39135L, 6621028L, 66905L, 26905085L, 4098L, 0L, 14L, 88L, 530L, 
    97809006L, 90L, 6L, 260792844L, 9L, 833205723L, 99467321L, 5L, 
    8455640L, 54090L, 2L, 309L, 299161148L, 4952L, 454824L), Delta.Freq = c(5L, 
    78L, 88553L, 794L, 5L, 3859122L, 782L, 36L, 8756801L, 243169338L, 
    817789L, 8792384L, 7431L, 626921743L, 9206L, 95789L, 7916L, 8143453L, 
    6L, 4L, 6363L, 181125L, 259618L, 6751L, 33L, 37960L, 0L, 2L, 
    599582228L, 565585L, 19L, 48L, 269450424L, 70676581L, 7830566L, 
    4L, 86484313L, 21L, 90899794L, 2L, 72356L, 574280L, 869544L, 
    73418L, 6468164L, 2259L, 5938505L, 31329L, 1249L, 354L, 8817L, 
    3L, 2568L, 82809L, 29836269L, 5230L, 37L, 33752014L, 79307L, 
    1736L, 8522076L, 40L, 2289135L, 862L, 801448L, 8026L, 5L, 15L, 
    4393771L, 405914L, 71098L, 950288L, 8319L, 1396973L, 832L, 70L, 
    1746L, 61907L, 8709547L, 300750537L, 45862L, 91417085L, 79892L, 
    47765L, 5477L, 18L, 4186L, 2860L, 754038591L, 375L, 53809223L, 
    72L, 136L, 509L, 232325L, 13128104L, 1692L, 8581L, 23L), Delta.Time = c(1361082L, 
    7926L, 499L, 5004L, 3494530L, 213L, 64551179L, 70L, 797L, 5L, 
    72588L, 86976L, 5163L, 635080L, 3L, 91L, 919806257L, 81443L, 
    3135427L, 4410972L, 5810L, 8L, 46603718L, 422L, 1083626L, 48L, 
    15699890L, 7L, 90167635L, 446459879L, 2332071L, 761660L, 49218442L, 
    381L, 46L, 493197L, 46L, 798597155L, 45342274L, 6265842L, 6L, 
    3445819L, 351L, 1761227L, 214L, 959L, 908996387L, 6L, 3855L, 
    9096604L, 152664L, 7970052L, 32366926L, 31L, 5201618L, 114L, 
    7806411L, 70L, 239L, 5065L, 2L, 1L, 14472831L, 122042249L, 8L, 
    495604L, 29L, 8965478L, 2875L, 959L, 39L, 9L, 690L, 933626665L, 
    85294L, 580093L, 95934L, 982058L, 65244056L, 137508L, 29L, 7621L, 
    7527L, 72L, 2L, 315L, 6L, 2413L, 8625150L, 51298109L, 851L, 890460L, 
    160736L, 6L, 850842734L, 2L, 7L, 76969113L, 190536L), Peak.Time = c(1465265L, 
    452894L, 545076172L, 8226275L, 5040875L, 700530L, 1L, 3639L, 
    20141L, 71712131L, 686L, 923L, 770569738L, 69961L, 737458636L, 
    122403L, 199502046L, 6108L, 907L, 108078263L, 7817L, 4L, 6L, 
    69L, 721L, 786353L, 87486L, 1563L, 876L, 47599535L, 79295722L, 
    53L, 7378L, 591L, 6607935L, 954L, 6295L, 75514344L, 5742050L, 
    25647276L, 449L, 328566184L, 4L, 2L, 2703L, 21367543L, 63429043L, 
    708L, 782L, 909820L, 478L, 50L, 922L, 579882L, 7850L, 534L, 2157492L, 
    96L, 6L, 716L, 5L, 653290336L, 447854237L, 2L, 31972263L, 645L, 
    7L, 609909L, 4054695L, 455631L, 4919894L, 9L, 72713L, 9997L, 
    84090765L, 89742L, 5L, 5028L, 4126L, 23091L, 81L, 239635020L, 
    3576L, 898597785L, 6822L, 3798L, 201999L, 19624L, 20432923L, 
    18944093L, 930720236L, 1492302L, 300122L, 143633L, 5152743L, 
    417344L, 813L, 55792L, 78L), Center_Freq = c(61907L, 8709547L, 
    300750537L, 45862L, 91417085L, 79892L, 47765L, 5477L, 18L, 4186L, 
    2860L, 754038591L, 375L, 53809223L, 72L, 136L, 4700783L, 4173830L, 
    30L, 811L, 341014L, 780L, 44749L, 91L, 201620707L, 74L, 1L, 65422L, 
    595L, 89093186L, 946520L, 6940919L, 48744L, 2317845L, 5126197L, 
    2445L, 8L, 557450L, 450259742L, 21006647L, 9L, 7234027L, 59L, 
    9L, 651547554L, 45554L, 38493L, 91055218L, 38L, 1116474L, 2295482L, 
    3001L, 9L, 3270L, 141L, 53644L, 667983L, 565598L, 84L, 971L, 
    555498297L, 60431L, 6597L, 856943893L, 607815536L, 4406L, 79L, 
    4885076L, 745L, 8L, 56456L, 125140L, 73043989L, 516476L, 7L, 
    4440739L, 754038591L, 375L, 53809223L, 72L, 136L, 509L, 232325L, 
    13128104L, 1692L, 8581L, 23L, 5874213L, 4550L, 644668065L, 3712371L, 
    5928L, 8833L, 7L, 2186023L, 61627221L, 37297L, 716427989L, 21387L
    ), Start.Freq = c(426355L, 22073538L, 680374L, 41771L, 54L, 6762844L, 
    599171L, 108L, 257451851L, 438814L, 343045L, 4702L, 967787L, 
    1937L, 18L, 89301735L, 366L, 90L, 954L, 7337732L, 70891703L, 
    4139L, 10397931L, 940000382L, 7L, 38376L, 878528819L, 6287L, 
    738366L, 31L, 47L, 5L, 6L, 77848L, 2366508L, 45L, 3665842L, 7252260L, 
    6L, 61L, 3247L, 448348L, 1L, 705132L, 144L, 7423637L, 2L, 497L, 
    844927639L, 78978L, 914L, 131L, 7089563L, 927L, 9595581L, 2774463L, 
    1651L, 73509280L, 7L, 35L, 18L, 96L, 1L, 92545512L, 27354947L, 
    7556L, 65019L, 7480L, 71835L, 8249L, 64792L, 71537L, 349389666L, 
    280244484L, 82L, 6L, 40L, 353872L, 0L, 103L, 1255L, 4752L, 29L, 
    76L, 81185L, 14L, 9L, 470775630L, 818361265L, 57947209L, 44L, 
    24L, 41295L, 4L, 261449L, 9931404L, 773556640L, 930717L, 65007421L
    ), End.Freq = c(71000996L, 11613579L, 71377155L, 1942738L, 8760748L, 
    79L, 455L, 374L, 8L, 5L, 2266932L, 597833L, 155488L, 3020L, 4L, 
    554L, 4L, 16472L, 1945649L, 668181101L, 649780L, 22394365L, 93060602L, 
    172146L, 20472L, 23558847L, 190513L, 22759044L, 44L, 78450L, 
    205621181L, 218L, 69916344L, 23884L, 66L, 312148L, 7710564L, 
    4L, 422L, 744572L, 651547554L, 45554L, 38493L, 91055218L, 38L, 
    1116474L, 2295482L, 3001L, 9L, 3270L, 141L, 55595L, 38451L, 8660867L, 
    14L, 96L, 345L, 6L, 44L, 8235824L, 910517L, 1424326L, 87102566L, 
    53644L, 667983L, 565598L, 84L, 971L, 555498297L, 60431L, 6597L, 
    856943893L, 607815536L, 4406L, 79L, 7L, 28978746L, 7537295L, 
    6L, 633L, 345860066L, 802L, 1035131L, 602L, 2740L, 8065L, 61370968L, 
    429953765L, 981507L, 8105L, 343787257L, 44782L, 64184L, 12981359L, 
    123367978L, 818775L, 123745614L, 25345654L, 3L), Country = c(&quot;Holland&quot;, 
    &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, 
    &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, 
    &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, 
    &quot;France&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;Holland&quot;, 
    &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, 
    &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;Holland&quot;, &quot;France&quot;, &quot;France&quot;, 
    &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, &quot;Spain&quot;, 
    &quot;Spain&quot;, &quot;Spain&quot;, &quot;France&quot;, &quot;France&quot;, &quot;France&quot;)), row.names = c(NA, 
    99L), class = &quot;data.frame&quot;)
    

  [1]: https://www.machinelearningplus.com/machine-learning/caret-package/https://
</code></pre>
",32,1,1,5,r;machine-learning;classification;r-caret;adaboost,2022-06-04 17:31:15,2022-06-04 17:31:15,2022-06-04 20:38:38,issue i m attempting to use the  adaboost  method within the caret and fastadaboost packages  my objective is to build a classification tree using  machine learning techniques in r for an upcoming project at university and i am following this tutorial   for this model  see below   i have downloaded the libraries caret and fastadaboost and whenever i try to run my model  i get the message  i don t understand what s wrong with this code  see below  as it s identical to my other models and i don t know why r cannot find my model  many thanks if anyone can lend a hand  these models are running just fine  structure of my data frame r code data
364,364,11829524,65232290,Change maximum size of AWS lambda,"<p>I tried to deploy my Serverless function on AWS lambda, but I got this error:</p>
<pre><code>Unzipped size must be smaller than 262144000 bytes.
</code></pre>
<p>I can't lower function size, because there is trained machine learning model (about 400Mb).</p>
<p>Is there any way to increase maximum lambda size?</p>
",284,1,1,3,node.js;aws-lambda;serverless,2020-12-10 15:31:00,2020-12-10 15:31:00,2022-06-04 18:22:55,i tried to deploy my serverless function on aws lambda  but i got this error  i can t lower function size  because there is trained machine learning model  about mb   is there any way to increase maximum lambda size 
365,365,10509929,72492300,Python / Fundamental: How do you advise reading varied text input and sorting into standardized sets of data,"<p>I am working on a project that will automate some mundane tasks in the work force.</p>
<p>One of the most mundane, yet seemingly hardest to automate process is reading and sorting e-mail data, allow me to explain...</p>
<p>We get hundreds of e-mails per day, all of which have information like company names, dates and times, ids, etc. Now the issue is there are hundreds of different company names, about 20 different formats these companies use for IDs, several different formats for dates (YYYY-MM-DD, DD-MM-YYYY, MM-DD-YY, etc). I believe this is a calling for some integration of artificial intelligence or machine learning.</p>
<p>Although this seems very interesting to me, this is so out of my specialty, and I am very lost where to begin. I am working in Python. Have no library restrictions. Just need to get it done, efficiently and most importantly, accurately.</p>
<p>How can I move forward on this project?</p>
<p>Below is some examples, to give detail to exactly what is going on, data has been altered for confidentiality, and has been formatted to help readability, original data is in a table format. <code>Information that needs to be grabbed is code formatted</code>.</p>
<p>Field Array currently present, for context:</p>
<p>VNDR = Vendor or Customer, MAINT = Maintenance Event, REC = Recieved, PRIM = Primary</p>
<pre><code>    'VNDR_NAME': &quot;&quot;,
    'MAINT_ID': &quot;&quot;,
    'MAINT_DESC': &quot;&quot;,
    'MAINT_DATE_REC': &quot;&quot;,
    'MAINT_HOUR_REC': &quot;&quot;,
    'MAINT_MIN_REC': &quot;&quot;,
    'MAINT_OUTAGE_NUM': &quot;&quot;,
    'MAINT_OUTAGE_UNT': &quot;&quot;,
    'MAINT_CITY': &quot;&quot;,
    'MAINT_STATE': &quot;&quot;,
    'MAINT_PRIM_START_DATE': &quot;&quot;,
    'MAINT_PRIM_END_DATE': &quot;&quot;,
    'MAINT_PRIM_START_HOUR': &quot;&quot;,
    'MAINT_PRIM_END_HOUR': &quot;&quot;,
    'MAINT_PRIM_START_MIN': &quot;&quot;,
    'MAINT_PRIM_END_MIN': &quot;&quot;,
</code></pre>
<p>Example #1:</p>
<blockquote>
<blockquote>
<p><code>Big Box Store</code> Maintenance Notifications</p>
</blockquote>
<blockquote>
<p>Notification Type</p>
</blockquote>
<blockquote>
<p>Please take note of this maintenance activity being performed on the Big Box Store.</p>
</blockquote>
<blockquote>
<p>Dates and Times (All times are local to the work listed and reflect daylight savings.)</p>
</blockquote>
<blockquote>
<p>Date Sent   <code>09/19/2029 01:02PM CDT</code></p>
</blockquote>
<blockquote>
<p>Customer    INTERNAL COMPANY NAME</p>
</blockquote>
<blockquote>
<p>Description <code>STD | LOS | Reason for work</code></p>
</blockquote>
<blockquote>
<p>Work Location   <code>CITY, ST</code></p>
</blockquote>
<blockquote>
<p>Ticket  <code>CRS000009999999/TAR000009999999</code></p>
</blockquote>
<blockquote>
<p>Time Zone   Central</p>
</blockquote>
<blockquote>
<p>Planned Start   <code>09/12/2029 12:00AM CDT</code></p>
</blockquote>
<blockquote>
<p>Planned End <code>09/12/2029 06:00AM CDT</code></p>
</blockquote>
<blockquote>
<p>Window  The Big Box Store standard maintenance window is Mon-Fri (12:00am-6:00am). For
multi-night maintenances, work will begin and end during this window each day.</p>
</blockquote>
<blockquote>
<p>Impact Details:
Impact Type Loss of Service
Duration    <code>180 (minutes)</code></p>
</blockquote>
<p>Location Requirements: (If there are location specific instructions, they'll be outlined here.)
Big Box Store technicians may on occasion require access to a client location(s). When site access is needed, the client will receive a separate notification from Big Box Store Change Management to coordinate access for that location(s).</p>
<p>Circuits:</p>
<blockquote>
<p>Customer    Location    CircuitID   Address Impact   INTERNAL- LEGACY INTERNAL  Z-side  A-side</p>
<blockquote>
<p>INTERNAL- ADDRESS//ADDRESS <code>99.A9AA.999999..AAAA</code> ADDRESS     Loss of Service</p>
</blockquote>
</blockquote>
</blockquote>
<p>Example #2:</p>
<blockquote>
<blockquote>
<p><code>Small Box Store</code> Maintenance Notification</p>
</blockquote>
<blockquote>
<p>This email serves as official notification that Small Box Store and/or one of its providers will be performing maintenance on its network as described below. This maintenance may affect services you have with us.</p>
</blockquote>
<blockquote>
<p>Maintenance Ticket #: <code>AAA-0001112345</code></p>
</blockquote>
<blockquote>
<p>Urgency: Demand</p>
</blockquote>
<blockquote>
<p>Date Notice Sent: <code>01-Apr-2029</code></p>
</blockquote>
<blockquote>
<p>Customer: INTERNAL</p>
</blockquote>
<blockquote>
<p>Maintenance Window</p>
</blockquote>
<blockquote>
<p>1st Activity Date
<code>08-Apr-2029 07:00 to 08-Apr-2029 19:00 ( Eastern )</code>
08-Apr-2029 11:00 to 08-Apr-2029 23:00 ( GMT )</p>
</blockquote>
<blockquote>
<p>Location of Maintenance: <code>City, ST</code></p>
</blockquote>
<blockquote>
<p>Reason for Maintenance: Small Box Store's service provider will perform demand fiber maintenance. Work will be performed during the daytime due to power safety requirements. As this maintenance is not under the control of Small Box Store, it may not be possible to reschedule it. Small Box Store sincerely apologizes for any inconvenience caused by this maintenance.</p>
</blockquote>
<blockquote>
<p>Expected Impact: Service Affecting Activity: Any Maintenance Activity directly impacting the service(s) of customers. Service(s) are expected to go down as a result of these activities.</p>
</blockquote>
<blockquote>
<p>Circuit(s) Affected:</p>
</blockquote>
<blockquote>
<p>Circuit Id  Expected Impact A Location Address  Z Location Address  Customer Circuit ID</p>
<blockquote>
<p><code>OABC/123456//DEF</code>    <code>Hard Down - Duration of maintenance window</code>    ADDRESS ADDRESS</p>
</blockquote>
<blockquote>
<p><code>OABC/123457//DEF</code>    <code>Hard Down - Duration of maintenance window</code>    ADDRESS ADDRESS</p>
</blockquote>
<blockquote>
<p>OABC/123458//DEF  Switch Hit(s)   ADDRESS ADDRESS</p>
</blockquote>
<blockquote>
<p>OABC/123459//DEF  Switch Hit(s)   ADDRESS ADDRESS</p>
</blockquote>
<blockquote>
<p><code>OABC/123460//DEF</code>    <code>Hard Down - Duration of maintenance window</code>    ADDRESS ADDRESS <code>ETH100-123458</code></p>
</blockquote>
</blockquote>
<blockquote>
<p>Please contact the Small Box Store Maintenance Team with any questions regarding this maintenance event. Please reference the Maintenance Ticket number when calling.</p>
</blockquote>
</blockquote>
",27,0,-1,3,python;machine-learning;artificial-intelligence,2022-06-03 21:33:42,2022-06-03 21:33:42,2022-06-04 16:14:17,i am working on a project that will automate some mundane tasks in the work force  one of the most mundane  yet seemingly hardest to automate process is reading and sorting e mail data  allow me to explain    we get hundreds of e mails per day  all of which have information like company names  dates and times  ids  etc  now the issue is there are hundreds of different company names  about  different formats these companies use for ids  several different formats for dates  yyyy mm dd  dd mm yyyy  mm dd yy  etc   i believe this is a calling for some integration of artificial intelligence or machine learning  although this seems very interesting to me  this is so out of my specialty  and i am very lost where to begin  i am working in python  have no library restrictions  just need to get it done  efficiently and most importantly  accurately  how can i move forward on this project  below is some examples  to give detail to exactly what is going on  data has been altered for confidentiality  and has been formatted to help readability  original data is in a table format  information that needs to be grabbed is code formatted  field array currently present  for context  vndr   vendor or customer  maint   maintenance event  rec   recieved  prim   primary example    big box store maintenance notifications notification type please take note of this maintenance activity being performed on the big box store  dates and times  all times are local to the work listed and reflect daylight savings   date sent       pm cdt customer    internal company name description std   los   reason for work work location   city  st ticket  crs tar time zone   central planned start       am cdt planned end     am cdt circuits  customer    location    circuitid   address impact   internal  legacy internal  z side  a side internal  address  address  aaa   aaaa address     loss of service example    small box store maintenance notification this email serves as official notification that small box store and or one of its providers will be performing maintenance on its network as described below  this maintenance may affect services you have with us  maintenance ticket    aaa  urgency  demand date notice sent   apr  customer  internal maintenance window location of maintenance  city  st reason for maintenance  small box store s service provider will perform demand fiber maintenance  work will be performed during the daytime due to power safety requirements  as this maintenance is not under the control of small box store  it may not be possible to reschedule it  small box store sincerely apologizes for any inconvenience caused by this maintenance  expected impact  service affecting activity  any maintenance activity directly impacting the service s  of customers  service s  are expected to go down as a result of these activities  circuit s  affected  circuit id  expected impact a location address  z location address  customer circuit id oabc   def    hard down   duration of maintenance window    address address oabc   def    hard down   duration of maintenance window    address address oabc   def  switch hit s    address address oabc   def  switch hit s    address address oabc   def    hard down   duration of maintenance window    address address eth  please contact the small box store maintenance team with any questions regarding this maintenance event  please reference the maintenance ticket number when calling 
366,366,14666653,72498634,Multiple iteration returns same data when scraping multiple urls,"<p>I had a set of <strong>urls</strong> around 170. All the urls are from same website <strong><a href=""http://www.blackoffer.com"" rel=""nofollow noreferrer"">www.blackoffer.com</a></strong>, it contains articles on different topics.</p>
<p>My task is to extract <strong>Article Title</strong> and <strong>Paragraphs</strong> from the page.</p>
<p>I am using selenium==3.141.0 to scrape data. When I written the code and run the '.py' file. It returns all the 170 text file (which I wanted), but it returns only same data in all 170 files.</p>
<p>Here is my code</p>
<pre><code># Import required libraries
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time


# Set options to display the window of chrome
options = Options()
options.headless = True
options.add_argument('window-size=1920x1080')

# Multiple urls
urls =      ['https://insights.blackcoffer.com/how-is-login-logout-time-tracking-for-employees-in-office-done-by-ai/',
             'https://insights.blackcoffer.com/how-does-ai-help-to-monitor-retail-shelf-watches/',
             'https://insights.blackcoffer.com/ai-and-its-impact-on-the-fashion-industry/',
             'https://insights.blackcoffer.com/how-do-deep-learning-models-predict-old-and-new-drugs-that-are-successfully-treated-in-healthcare/',
             'https://insights.blackcoffer.com/how-artificial-intelligence-can-boost-your-productivity-level/',
             'https://insights.blackcoffer.com/how-are-genetic-sequencing-maps-affected-by-deep-learning-and-ai/',
             'https://insights.blackcoffer.com/how-is-ai-used-to-solve-traffic-management/',
             'https://insights.blackcoffer.com/benefits-of-big-data-in-different-fields/',
             'https://insights.blackcoffer.com/how-big-data-will-impact-the-future-of-business/',
             'https://insights.blackcoffer.com/how-will-ai-make-decisions-in-tomorrows-wars/',
             'https://insights.blackcoffer.com/which-one-is-better-ai-or-big-data/',
             'https://insights.blackcoffer.com/how-robots-can-help-in-e-learning-platforms/',
             'https://insights.blackcoffer.com/how-does-big-data-help-in-finance-and-the-growth-of-large-firms/',
             'https://insights.blackcoffer.com/future-of-work-robot-ai-and-automation/',
             'https://insights.blackcoffer.com/how-ai-will-help-the-defense-power-of-a-country/',
             'https://insights.blackcoffer.com/future-of-ai-and-machine-roles-in-the-medical-sector/',
             'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/',
             'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/',
             'https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/',
             'https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/',
             'https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/',
             'https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/',
             'https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/',
             'https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/',
             'https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/',
             'https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/',
             'https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/',
             'https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/',
             'https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/',
             'https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/',
             'https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/',
             'https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/',
             'https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/',
             'https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/',
             'https://insights.blackcoffer.com/evolution-of-advertising-industry/',
             'https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/',
             'https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/',
             'https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/',
             'https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/',
             'https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/',
             'https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/',
             'https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/',
             'https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/',
             'https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/',
             'https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/',
             'https://insights.blackcoffer.com/how-we-forecast-future-technologies/',
             'https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/',
             'https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/',
             'https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/',
             'https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/',
             'https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/',
             'https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/',
             'https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/',
             'https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/',
             'https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/',
             'https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/',
             'https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/',
             'https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/',
             'https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/',
             'https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/',
             'https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/',
             'https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/',
             'https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/',
             'https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/',
             'https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/',
             'https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors/',
             'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/',
             'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/',
             'https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/',
             'https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/',
             'https://insights.blackcoffer.com/human-rights-outlook/',
             'https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/',
             'https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/',
             'https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/',
             'https://insights.blackcoffer.com/impacts-of-covid-19-on-streets-sides-food-stalls/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/',
             'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/',
             'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/',
             'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/',
             'https://insights.blackcoffer.com/travel-and-tourism-outlook/',
             'https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/',
             'https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/',
             'https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/',
             'https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/',
             'https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/',
             'https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/',
             'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/',
             'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/',
             'https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-2/',
             'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-3/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-3/',
             'https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work/',
             'https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding-2/',
             'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-4/',
             'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-2/',
             'https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-3/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-4/',
             'https://insights.blackcoffer.com/why-scams-like-nirav-modi-happen-with-indian-banks/',
             'https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy/',
             'https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy-2/',
             'https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy-2/',
             'https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-3/',
             'https://insights.blackcoffer.com/should-celebrities-be-allowed-to-join-politics/',
             'https://insights.blackcoffer.com/how-prepared-is-india-to-tackle-a-possible-covid-19-outbreak/',
             'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work/',
             'https://insights.blackcoffer.com/controversy-as-a-marketing-strategy/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry/',
             'https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets/',
             'https://insights.blackcoffer.com/what-are-the-key-policies-that-will-mitigate-the-impacts-of-covid-19-on-the-world-of-work/',
             'https://insights.blackcoffer.com/marketing-drives-results-with-a-focus-on-problems/',
             'https://insights.blackcoffer.com/continued-demand-for-sustainability/',
             'https://insights.blackcoffer.com/coronavirus-disease-covid-19-effect-the-impact-and-role-of-mass-media-during-the-pandemic/',
             'https://insights.blackcoffer.com/should-people-wear-fabric-gloves-seeking-evidence-regarding-the-differential-transfer-of-covid-19-or-coronaviruses-generally-between-surfaces/',
             'https://insights.blackcoffer.com/why-is-there-a-severe-immunological-and-inflammatory-explosion-in-those-affected-by-sarms-covid-19/',
             'https://insights.blackcoffer.com/what-do-you-think-is-the-lesson-or-lessons-to-be-learned-with-covid-19/',
             'https://insights.blackcoffer.com/coronavirus-the-unexpected-challenge-for-the-european-union/',
             'https://insights.blackcoffer.com/industrial-revolution-4-0-pros-and-cons/',
             'https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy/',
             'https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-2/',
             'https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy/',
             'https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-global-economy/',
             'https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/',
             'https://insights.blackcoffer.com/blockchain-in-fintech/',
             'https://insights.blackcoffer.com/blockchain-for-payments/',
             'https://insights.blackcoffer.com/the-future-of-investing/',
             'https://insights.blackcoffer.com/big-data-analytics-in-healthcare/',
             'https://insights.blackcoffer.com/business-analytics-in-the-healthcare-industry/',
             'https://insights.blackcoffer.com/challenges-and-opportunities-of-big-data-in-healthcare/',
             'https://insights.blackcoffer.com/obstacles-to-data-driven-healthcare/',
             'https://insights.blackcoffer.com/monetization-of-data-innovate-to-harvest-the-full-value-of-data/',
             'https://insights.blackcoffer.com/traceability-of-information-master-your-data-capital/',
             'https://insights.blackcoffer.com/bank-risk-management-india/',
             'https://insights.blackcoffer.com/advance-analytics-for-refocusing-profits/',
             'https://insights.blackcoffer.com/role-of-big-data-analytics-banking-and-finance/',
             'https://insights.blackcoffer.com/mitigating-bank-risk-management/',
             'https://insights.blackcoffer.com/future-of-bank-risk-management/',
             'https://insights.blackcoffer.com/advanced-analytics-redefining-banking/',
             'https://insights.blackcoffer.com/marketing-analytics-needs/',
             'https://insights.blackcoffer.com/big-data-analytics-to-bring-transparency-and-good-governance/',
             'https://insights.blackcoffer.com/how-political-leaders-will-shape-tomorrow-using-big-data-analytics/',
             'https://insights.blackcoffer.com/big-data-and-analytics-to-help-form-political-leaders-win-election/',
             'https://insights.blackcoffer.com/the-emergence-of-data-analytics/',
             'https://insights.blackcoffer.com/how-artificial-intelligence-can-deliver-real-value-to-companies-2/',
             'https://insights.blackcoffer.com/the-prospective-recipe-of-success-in-the-age-of-analytics/',
             'https://insights.blackcoffer.com/using-people-analytics-to-drive-business-performance/',
             'https://insights.blackcoffer.com/how-artificial-intelligence-can-deliver-real-value-to-companies/',
             'https://insights.blackcoffer.com/big-data-analytics-through-iot-in-oil-and-gas-industry/',
             'https://insights.blackcoffer.com/how-big-data-and-analytics-is-helping-marketing-leaders/',
             'https://insights.blackcoffer.com/what-analytics-outsourcing-engagement-model-is-right-for-you/',
             'https://insights.blackcoffer.com/big-data-analytics-solving-problems-banking-and-finance-industry/',
             'https://insights.blackcoffer.com/analytics-healthcare-industry/',
             'https://insights.blackcoffer.com/business-analytics-textile-industry/',
             'https://insights.blackcoffer.com/big-data-marketing-reality/',
             'https://insights.blackcoffer.com/understanding-millennial-market/',
             'https://insights.blackcoffer.com/analytics-helping-fashion-e-tailers-develop-markets-developing-countries/',
             'https://insights.blackcoffer.com/big-data-analytics-change-healthcare-developing-countries/',
             'https://insights.blackcoffer.com/big-data-analytics-help-voters-know-political-leaders/',
             'https://insights.blackcoffer.com/digital-transformation-oil-gas/',
             'https://insights.blackcoffer.com/data-analytics-reduce-cost-production/',
             'https://insights.blackcoffer.com/gaining-insights-internal-data-retail/',
             'https://insights.blackcoffer.com/role-big-data-cyber-security/',
             'https://insights.blackcoffer.com/role-big-data-healthcare/',
             'https://insights.blackcoffer.com/data-driven-dashboards/',
             'https://insights.blackcoffer.com/role-big-data-academia/',
             'https://insights.blackcoffer.com/sales-forecasting-in-retail/',
             'https://insights.blackcoffer.com/detect-data-exfiltration-over-the-network/',
             'https://insights.blackcoffer.com/data-exfiltration/',
             'https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/']

time.sleep(2)
# Pass multiple urls using 'for' loop to 'chrome' driver
for i in range(len(urls)):
    path =r&quot;C:\Users\prvzs\Downloads\chromedriver_win32\chromedriver.exe&quot;
    driver = webdriver.Chrome(path,options=options)
    driver.get(urls[i])
    #driver.maximize_window()


# Get 'article_title' and 'article_paragraph' using xpath
article_title = driver.find_element_by_xpath('//h1').text
article_paragraph = driver.find_element_by_xpath(&quot;//div[contains(@class,'td-post-content')]&quot;).text


time.sleep(2)
# Save as '.txt' by naming 'URL_ID' from 1 to 171
for j in range(1,172,1):
    with open(f'data/{j}.txt','w') as file:
        file.write(article_title+&quot;\n&quot; )
        file.writelines(&quot;% s&quot; %data for data in article_paragraph)

# Quit driver
    driver.quit()
</code></pre>
<p>Here are the sample output files contains all same data across all 170 txt saved files.
<a href=""https://i.stack.imgur.com/wyALo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyALo.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/a47rk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a47rk.png"" alt=""enter image description here"" /></a></p>
",25,1,0,2,python;selenium,2022-06-04 15:05:05,2022-06-04 15:05:05,2022-06-04 15:08:57,i had a set of urls around   all the urls are from same website   it contains articles on different topics  my task is to extract article title and paragraphs from the page  i am using selenium     to scrape data  when i written the code and run the   py  file  it returns all the  text file  which i wanted   but it returns only same data in all  files  here is my code 
367,367,19269021,72497991,OSError: SavedModel file dors not exist at: ./AstBert_bert,"<p>I was working on colab I saved my machine learning model and checked that it was well saved before closing the notebook I come back to open the notebook I can't find my model anymore I'm looking in my drive I still can't find it. Where can it be?</p>
",7,0,1,2,save;google-colaboratory,2022-06-04 13:12:41,2022-06-04 13:12:41,2022-06-04 13:12:41,i was working on colab i saved my machine learning model and checked that it was well saved before closing the notebook i come back to open the notebook i can t find my model anymore i m looking in my drive i still can t find it  where can it be 
368,368,19099750,72497124,SpectrogramGenerator Exception: [Errno 2] No such file or directory:,"<pre><code>    file_name = &quot;data\\temp_folder\\tmp_{}.png&quot;.format(random.randint(0, 100000))
    command = &quot;sox -V0 '{}' -n remix 1 rate 10k spectrogram -y {} -x {} -X {}  -m -r -o {}&quot;.format(file, height, width, pixel_per_sec, file_name)
    p = Popen(command, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)

    output, errors = p.communicate()
    if errors:
        print(errors)

    image = Image.open(file_name)
    #os.remove(file_name)
    return np.array(image)
</code></pre>
<p>hi i am new in machine learning, i have a problem regarding the file cant be find.</p>
<p>this is the error it shows</p>
<p><code>SpectrogramGenerator Exception:  [Errno 2] No such file or directory: 'C:\\Users\\Nur\\Documents\\CRNN\\Language-Identification-Speech\\data\\tmp_46344.png' C:\Users\Nur\Documents\CRNN\Language-Identification-Speech\data\Datasets\segmented\chinese\VOAchina\-1_047.wav</code></p>
",13,0,-1,3,python;machine-learning;keras,2022-06-04 10:10:48,2022-06-04 10:10:48,2022-06-04 10:10:48,hi i am new in machine learning  i have a problem regarding the file cant be find  this is the error it shows spectrogramgenerator exception    errno   no such file or directory   c   users  nur  documents  crnn  language identification speech  data  tmp_ png  c  users nur documents crnn language identification speech data datasets segmented chinese voachina  _ wav
369,369,3164679,68112360,How to change joypy joyplot y-axis labels colors,"<p>How do you change the colors of the y-axis labels in a joyplot using joypy package?</p>
<p>Here is a sample code where i can change the color if the x-axis labels, but not the y-axis.</p>
<pre><code>import joypy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

## DATA
url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;
new_names = ['SepalLength','SepalWidth','PetalLength','PetalWidth','Name']
iris = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')

## PLOT
fig, axes = joypy.joyplot(iris)

## X AXIS
plt.tick_params(axis='x', colors='red') 

## Y AXIS     (NOT WORKING)
plt.tick_params(axis='y', colors='red') 
</code></pre>
<p><a href=""https://i.stack.imgur.com/SSaCw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SSaCw.png"" alt=""enter image description here"" /></a></p>
<p>I'm pretty sure the issue is because there are mutliple sub-y-axis's, one for each density plot, and they are actually hidden already.
Not sure how to access the y-axis that is actually shown (I want to change the color of &quot;SepalLength&quot;)</p>
<p>Joyplot is using Matplotlib</p>
",276,1,1,4,python;matplotlib;data-visualization;joypy,2021-06-24 14:10:09,2021-06-24 14:10:09,2022-06-04 04:06:35,how do you change the colors of the y axis labels in a joyplot using joypy package  here is a sample code where i can change the color if the x axis labels  but not the y axis   joyplot is using matplotlib
370,370,111110,72495381,Missing functions in pytorch 1.11.0+cpu and torchtext 0.12.0,"<p>I'm getting started with machine learning, and I'm trying to run the code <a href=""https://github.com/abhinavtembulkar/coursera-intro-to-machine-learning-duke-university/blob/master/Week%204/4B_Natural_Language_Processing_Assignment.ipynb"" rel=""nofollow noreferrer"">in this link</a> but I got stuck in these lines:</p>
<pre><code>import torchtext

agnews_train, agnews_test = torchtext.datasets.AG_NEWS(root='./datasets', split=('train', 'test')) # this line added for completeness

VOCAB_SIZE = len(trainset.get_vocab())
EMBED_DIME = 100
HIDDEN_DIME = 64
NUM_OUTPUTS = len(trainset.get_labels())
</code></pre>
<p>The error I got is:</p>
<pre><code>AttributeError: 'MapperIterDataPipe' object has no attribute 'get_vocab
</code></pre>
<p>I also get this error when I write the last line at first:</p>
<pre><code>AttributeError: 'MapperIterDataPipe' object has no attribute 'get_labels
</code></pre>
<p>I tried to replace <code>get_vocab()</code> with this code <a href=""https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"" rel=""nofollow noreferrer"">taken from this link</a>:</p>
<pre><code>tokenizer = get_tokenizer(&quot;basic_english&quot;)

def build_vocab(datasets):
    for dataset in datasets:
        for _, text in dataset:
            tok = tokenizer(text)
            yield tok

vocab = build_vocab_from_iterator(build_vocab([agnews_train, agnews_test]), specials=[&quot;&lt;UNK&gt;&quot;])
vocab.set_default_index(vocab[&quot;&lt;UNK&gt;&quot;])

VOCAB_SIZE = len(vocab.get_itos())
</code></pre>
<p>It runs but I don't know if I'm doing it right because I don't know which value <code>get_vocab()</code> returns.
I think the problem is the library version. I have pytorch 1.11.0+cpu and torchtext 0.12.0, while the code I'm trying to reproduce uses torchtext 0.4.</p>
<p>How can I replace <code>get_vocab()</code> and <code>get_labels()</code> functions?</p>
",32,0,0,3,python-3.x;pytorch;torchtext,2022-06-04 03:06:10,2022-06-04 03:06:10,2022-06-04 03:06:10,i m getting started with machine learning  and i m trying to run the code  but i got stuck in these lines  the error i got is  i also get this error when i write the last line at first  i tried to replace get_vocab   with this code   how can i replace get_vocab   and get_labels   functions 
371,371,17109540,72492810,Comparing Predicted Value with actual value,"<p>im new to statistics and R,</p>
<p>Im currently practicing to use GBM model to predict &quot;charges&quot; value from insurance company, with variables of age, bmi, number of children, and smooker.
I managed to use the gbm model,
but I dont know how to compare the predicted value with the actual value here.</p>
<pre><code>insure&lt;-as.tibble(insurance)
insure&lt;-insure %&gt;% 
  mutate(Agegroup=as.factor(findInterval(age,c(18,35,50,80))))
levels(insure$Agegroup)&lt;-c(&quot;Youth&quot;,&quot;Mid Aged&quot;,&quot;Old&quot;)

#Divide the dataset into a training and validation set for some machine learning predictions
trainds&lt;-createDataPartition(insure$Agegroup,p=0.8,list=F)
validate&lt;-insure[-trainds,] 
trainds&lt;-insure[trainds,]  
#Set metric and control
control&lt;-trainControl(method=&quot;cv&quot;,number=10)
metric&lt;-&quot;RMSE&quot; 
#Set up models 
set.seed(233)
summary(fit.gbm&lt;-train(charges~.,data=trainds,method=&quot;gbm&quot;,trControl=control,metric=metric,
               verbose=F) )
</code></pre>
<p>I dont know which data should I use to compare?
since the model used &quot;trainds&quot; data, should i compare it with validate data? or the actual &quot;insure&quot; data?</p>
<p>This is my attempt</p>
<pre><code>plot(predict(fit.gbm),  #should i use the newdata?                         
     validate$charges, #not sure if i should use &quot;validate$charges&quot; or from other data
     xlab = &quot;Predicted Values&quot;,
     ylab = &quot;Observed Values&quot;)
abline(a = 0,                                      
       b = 1,
       col = &quot;red&quot;,
       lwd = 2)
</code></pre>
<p>However, since both data have different length i keep getting error of</p>
<pre><code>'x' and 'y' lengths differ
</code></pre>
",25,0,0,2,r;gbm,2022-06-03 22:20:26,2022-06-03 22:20:26,2022-06-03 22:25:38,im new to statistics and r  this is my attempt however  since both data have different length i keep getting error of
372,372,18426759,71420630,Unable to import SGD and Adam from &#39;tensorflow.python.keras.optimizers&#39;,"<p><strong>Trying to run---</strong></p>
<pre><code>import tensorflow as tf
from tensorflow import keras

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Flatten, Dense
from tensorflow.python.keras.optimizers import SGD, Adam

import numpy as np

print(tf.__version__)
</code></pre>
<p><strong>I get this error---</strong></p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-8-f05f8f753c47&gt; in &lt;module&gt;()
      4 from tensorflow.python.keras.models import Sequential
      5 from tensorflow.python.keras.layers import Flatten, Dense
----&gt; 6 from tensorflow.python.keras.optimizers import SGD, Adam
      7 
      8 import numpy as np

ImportError: cannot import name 'SGD' from 'tensorflow.python.keras.optimizers' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizers.py)

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

---------------------------------------------------------------------------
</code></pre>
<p>I'm studying machine learning in Google Colab.
I pasted the example code and run it, and get error message.</p>
<p>I could find similar errors in Google, but I couldn't find anything to solve this problem.</p>
<p>I tried 'from tensorflow.keras.optimizers import SGD, Adam', 'from tf.keras.optimizers import SGD, Adam', and 'from keras.optimizers import SGD, Adam'.
But everything didn't work.</p>
",429,2,0,5,python;tensorflow;keras;deep-learning;google-colaboratory,2022-03-10 13:33:25,2022-03-10 13:33:25,2022-06-03 21:03:56,trying to run    i get this error    i could find similar errors in google  but i couldn t find anything to solve this problem 
373,373,14714766,72491330,"tensorflow model very wrong after 500 epochs, while still showing high accuracy when training","<p>I'm very new to tensorflow, and ML in general. I downloaded a <a href=""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv"" rel=""nofollow noreferrer"">dataset</a> that claims to be standard. My code is:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras.layers import Dense
from sklearn.metrics import accuracy_score

df = pd.read_csv('testDB.csv')
x = pd.get_dummies(df.drop(['Class'], axis=1))
y = df['Class']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)
model = Sequential()
model.add(Dense(units=32, activation='relu', input_dim=len(x_train.columns)))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=&quot;sgd&quot;, metrics=&quot;accuracy&quot;)
model.fit(x_train,y_train,epochs=500,batch_size=32)
y_hat = model.predict(x_test)
y_hat = [0 if val &lt; 0.5 else 1 for val in y_hat]
print(y_hat)
print(list(y_test))
print(accuracy_score(y_test, y_hat))

model.save(&quot;tfmodel&quot;)
</code></pre>
<p>Where testDB.csv is my dataset.
After running it, it says the loss is about 0.3 and the accuracy is about 0.8 on the training dataset.
When I run it against the test dataset, it has an accuracy of 0.5</p>
<p>I think I might not be understanding how tensorflow works, or maybe I just don't understand machine learning enough. What am I doing wrong?</p>
",20,0,-1,4,python;tensorflow;machine-learning;keras,2022-06-03 20:15:23,2022-06-03 20:15:23,2022-06-03 20:15:23,i m very new to tensorflow  and ml in general  i downloaded a  that claims to be standard  my code is  i think i might not be understanding how tensorflow works  or maybe i just don t understand machine learning enough  what am i doing wrong 
374,374,12877832,72491268,What to do if one feature is highly correlated with the label but most of its values are missing?,"<p>For a Machine-Learning project I have a dataset with 27 features, one label and 10479 samples.</p>
<p>After creating a correlations map I found that all of the features have low correlation with the lable (less than +-0.3), except one that has (-0.77).</p>
<p>The problem is - that higly correlated feature has only 109 values out of 10479 possible (the rest are NaN)..</p>
<p>What should I do with this feature?</p>
<p>Should I fill so many values with mean (or other method - recommendations are welcome)?</p>
<p>Or should I just delete it although it is my most correlated feature with the label (and by far...)?</p>
<p>Many thanks!</p>
",36,0,0,5,pandas;machine-learning;correlation;missing-data;feature-selection,2022-06-03 20:11:47,2022-06-03 20:11:47,2022-06-03 20:11:47,for a machine learning project i have a dataset with  features  one label and  samples  after creating a correlations map i found that all of the features have low correlation with the lable  less than       except one that has       the problem is   that higly correlated feature has only  values out of  possible  the rest are nan    what should i do with this feature  should i fill so many values with mean  or other method   recommendations are welcome   or should i just delete it although it is my most correlated feature with the label  and by far      many thanks 
375,375,10193597,60215601,mkdocs nav title different from page title,"<p>My <code>mkdocs.yml</code> file has <code>nav</code> titles that are shortened to fit onto only 1 line each in the left-side navigation, and I want the <code>.md</code> Markdown <em>page</em> title to be the un-abbreviated full-length title.</p>

<p>For example, while my <code>mkdocs.yml</code> file contains:</p>

<pre><code>nav:
- BD, ML, DS: Big_Data,_Machine_Learning,_Data_Science.md
- AI, VI: Artificial_Intelligence,_Video_Intelligence.md
</code></pre>

<p>I want the <code>.md</code> page title to be:</p>

<pre><code>Big Data, Machine Learning, and Data Science
</code></pre>

<p>...instead of copying/using the <code>mkdocs.yml</code> nav title:</p>

<pre><code>BD, ML, DS
</code></pre>

<p>When I added the Markdown Page Title to the 1st line (<code># Big Data...</code>) of my <code>.md</code> file, I get both the page title I want <em>and</em> the inherited <code>mkdocs.yml</code> <code>nav</code> title:</p>

<pre><code>BD, ML, DS
7 min (1,939 words)
Big Data, Machine Learning, and Data Science
</code></pre>

<p>It seems the answer is in the <code>mkdocs.yml</code> file to specify a different/second ""page display"" name, but many Google searches have turned up nothing. </p>

<p>What other methods have you tried or heard of? Thanks!</p>
",1315,2,3,3,markdown;github-flavored-markdown;mkdocs,2020-02-14 01:31:49,2020-02-14 01:31:49,2022-06-03 19:02:18,my mkdocs yml file has nav titles that are shortened to fit onto only  line each in the left side navigation  and i want the  md markdown page title to be the un abbreviated full length title  for example  while my mkdocs yml file contains  i want the  md page title to be     instead of copying using the mkdocs yml nav title  when i added the markdown page title to the st line    big data     of my  md file  i get both the page title i want and the inherited mkdocs yml nav title  it seems the answer is in the mkdocs yml file to specify a different second page display name  but many google searches have turned up nothing   what other methods have you tried or heard of  thanks 
376,376,7119501,72436750,How to pass the single columnar dependent variable to train the linear regression model?,"<p>I am new to Machine learning and started course on Simple Linear Regression model recently.</p>
<p>I have a dataset where except for a column <code>id</code> (integer type), all the columns are of <code>String</code> datatype.
And I have loaded it into a pandas dataframe and selected indexes out of it as below.</p>
<p>The pandas dataframe has total 32 columns and the 33rd column is the dependent variable column that just says <code>YES</code> or <code>NO</code>.
Using all the independent variables (columns 0 to 31), I am trying to find if I can predict the values in column 32 which is my dependent variable.</p>
<pre><code>data = psyco.read_into_pandas()
X = data.iloc[:, 1:33].values
Y = data.iloc[:, 32].values

# Add missing values
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent', add_indicator=True)

# Fit the rows and columns into the imputer
imputer.fit(X[:, 1:33])

# Transform the data.
X[:, 1:33] = imputer.transform(X[:, 1:33])

# One hot encoding
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

# Label Encoder
le = LabelEncoder()
Y = le.fit_transform(Y)

# Split data into train and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
</code></pre>
<p>Before sending the values of <code>X_train</code> and <code>Y_train</code>, I just printed the values of <code>Y_train</code> and I can see that it contains an array of integers which could be seen in the image below.</p>
<p><a href=""https://i.stack.imgur.com/6sggJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6sggJ.png"" alt=""enter image description here"" /></a></p>
<p>But when I send the data of <code>X_train</code> and <code>Y_train</code> to my <code>LinearRegression()</code> I am facing an error that says:</p>
<pre><code>ValueError: could not convert string to float: 'yes'
</code></pre>
<p>Full error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Some/Path/mltask.py&quot;, line 52, in task_2
    lr.fit(X_train, Y_train)
  File &quot;/Some/Path/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py&quot;, line 684, in fit
    X, y = self._validate_data(
  File &quot;/Some/Path/venv/lib/python3.9/site-packages/sklearn/base.py&quot;, line 596, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File &quot;/Some/Path/venv/lib/python3.9/site-packages/sklearn/utils/validation.py&quot;, line 1074, in check_X_y
    X = check_array(
  File &quot;/Some/Path/venv/lib/python3.9/site-packages/sklearn/utils/validation.py&quot;, line 856, in check_array
    array = np.asarray(array, order=order, dtype=dtype)
ValueError: could not convert string to float: 'yes'
</code></pre>
<p>What I don't understand is when I print <code>Y_train</code> I see integers in the array but the regression says it can't convert String to float.</p>
<p>Could anyone let me know if I missed any step in between and how can I correct my mistake ?
Any help is massively appreciated.</p>
",30,1,2,4,python;pandas;machine-learning;scikit-learn,2022-05-30 21:02:31,2022-05-30 21:02:31,2022-06-03 18:29:59,i am new to machine learning and started course on simple linear regression model recently  before sending the values of x_train and y_train  i just printed the values of y_train and i can see that it contains an array of integers which could be seen in the image below   but when i send the data of x_train and y_train to my linearregression   i am facing an error that says  full error  what i don t understand is when i print y_train i see integers in the array but the regression says it can t convert string to float 
377,377,16982262,72300188,is TFX (Tensorflow Extended) being used in big companies,"<p>I've recently stumbled upon tensorflow extended, and I've finally started to understand the need and uses of it. It makes the whole machine learning pipeline a lot easier and looks like it can be used to automate the tasks. I wanted to ask seniors in this field, are big companies using it, is it extensively being used or will it get obsolete
Thank you</p>
",26,1,0,4,tensorflow;machine-learning;tfx;mlops,2022-05-19 12:46:36,2022-05-19 12:46:36,2022-06-03 18:14:27,
378,378,11483674,72489525,Serving React app from FastAPI: advantages vs disadvantages,"<p>I’m gradually moving away from Flask/Dash for development of my Data Science and Machine Learning apps, and towards a combination of FastAPI + React due to many advantages such as standardized models with pydantic, auto generation of the rest api documentation, async, full-blown React frontend possibilities, etc…</p>
<p>FastAPI can serve any Flask/Dash app I might want to use for a quick prototype development, and it can also serve a compiled React app if I want to. So here comes my question:</p>
<p><strong>Should I serve a compiled React app through FastAPI in production?</strong><br />
What would be the disadvantages of doing so when compared to running the React app from its own, separate server?<br />
Any specific concerns regarding stability, performance or security in such a stack?</p>
",45,1,0,3,reactjs;web-applications;fastapi,2022-06-03 17:54:18,2022-06-03 17:54:18,2022-06-03 18:09:14,i m gradually moving away from flask dash for development of my data science and machine learning apps  and towards a combination of fastapi   react due to many advantages such as standardized models with pydantic  auto generation of the rest api documentation  async  full blown react frontend possibilities  etc  fastapi can serve any flask dash app i might want to use for a quick prototype development  and it can also serve a compiled react app if i want to  so here comes my question 
379,379,16971617,71261347,RuntimeError: DataLoader worker exited unexpectedly,"<p>I am new to PyTorch and Machine Learning so I try to follow the tutorial from here:
<a href=""https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118"" rel=""nofollow noreferrer"">https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118</a></p>
<p>By copying the code step by step I got the following error for no reason. I tried the program on another computer and it gives syntax error. However, my IDE didn't warn my anything about syntax. I am really confused how I can fix the issue. Any help is appreciated.</p>
<pre><code>RuntimeError: DataLoader worker exited unexpectedly
</code></pre>
<p>Here is the code.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torchvision import datasets
from torchvision.transforms import ToTensor
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from torch import optim
from torch.autograd import Variable

train_data = datasets.MNIST(
    root='data',
    train=True,
    transform=ToTensor(),
    download=True,
)
test_data = datasets.MNIST(
    root='data',
    train=False,
    transform=ToTensor()
)
print(train_data)
print(test_data)

print(train_data.data.size())
print(train_data.targets.size())

plt.imshow(train_data.data[0], cmap='gray')
plt.title('%i' % train_data.targets[0])
plt.show()

figure = plt.figure(figsize=(10, 8))
cols, rows = 5, 5
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(train_data), size=(1,)).item()
    img, label = train_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(label)
    plt.axis(&quot;off&quot;)
    plt.imshow(img.squeeze(), cmap=&quot;gray&quot;)
plt.show()

loaders = {
    'train': DataLoader(train_data,
                        batch_size=100,
                        shuffle=True,
                        num_workers=1),

    'test': DataLoader(test_data,
                       batch_size=100,
                       shuffle=True,
                       num_workers=1),
}


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(
                in_channels=1,
                out_channels=16,
                kernel_size=5,
                stride=1,
                padding=2,
            ),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(16, 32, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        # fully connected layer, output 10 classes
        self.out = nn.Linear(32 * 7 * 7, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)
        x = x.view(x.size(0), -1)
        output = self.out(x)
        return output, x  # return x for visualization


cnn = CNN()
print(cnn)

loss_func = nn.CrossEntropyLoss()
print(loss_func)
optimizer = optim.Adam(cnn.parameters(), lr=0.01)
print(optimizer)
num_epochs = 10


def train(num_epochs, cnn, loaders):
    cnn.train()

    # Train the model
    total_step = len(loaders['train'])

    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(loaders['train']):
            # gives batch data, normalize x when iterate train_loader
            b_x = Variable(images)  # batch x
            b_y = Variable(labels)  # batch y

            output = cnn(b_x)[0]
            loss = loss_func(output, b_y)

            # clear gradients for this training step
            optimizer.zero_grad()

            # backpropagation, compute gradients
            loss.backward()
            # apply gradients
            optimizer.step()

            if (i + 1) % 100 == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))
                pass

        pass

    pass


train(num_epochs, cnn, loaders)


def evalFunc():
    # Test the model
    cnn.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in loaders['test']:
            test_output, last_layer = cnn(images)
            pred_y = torch.max(test_output, 1)[1].data.squeeze()
            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))
            pass

        print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)

    pass


evalFunc()

sample = next(iter(loaders['test']))
imgs, lbls = sample

actual_number = lbls[:10].numpy()

test_output, last_layer = cnn(imgs[:10])
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(f'Prediction number: {pred_y}')
print(f'Actual number: {actual_number}')
</code></pre>
",559,2,1,3,python;machine-learning;pytorch,2022-02-25 10:45:24,2022-02-25 10:45:24,2022-06-03 14:12:45,by copying the code step by step i got the following error for no reason  i tried the program on another computer and it gives syntax error  however  my ide didn t warn my anything about syntax  i am really confused how i can fix the issue  any help is appreciated  here is the code 
380,380,19260581,72483832,How to solve the problem &quot;No module named &#39;pandas.core.resample&#39;&quot;?,"<p>I am practicing the coding with &quot;Machine Learning for Financial Risk Management with Python Algorithms for Modeling Risk (Abdullah Karasan)&quot; in Chapter 1. I have successfully accessed the time series data of energy capacity utilization from the FRED for the period of 2010–2020, and followed the codes on book to remove its seasonality. However, One error occurred when I tried to resample the energy series with the following codes, for which I failed to find relevant solutions:</p>
<pre><code>In [10]: from fredapi import Fred

import statsmodels.api as sm

In [11]: fred = Fred(api_key='insert you api key')

In [12]: energy = fred.get_series(&quot;CAPUTLG2211A2S&quot;,observation_start=&quot;2010-01-01&quot;,observation_end=&quot;2020-12-31&quot;)


In [20]: seasonal_index = energy.resample('Q').mean()

ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-44-a3904e416c86&gt; in &lt;module&gt;
----&gt; 1 seasonal_index = energy.resample('Q').mean()
      2 seasonal_index

D:\anaconda\lib\site-packages\pandas\core\generic.py in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base, on, level)

ModuleNotFoundError: No module named 'pandas.core.resample'
</code></pre>
<p>I am a beginner of Python in Finance working on this topic solely, so I have no ideas of this problem.</p>
",40,3,0,2,python;pandas,2022-06-03 06:55:07,2022-06-03 06:55:07,2022-06-03 09:42:09,i am practicing the coding with  machine learning for financial risk management with python algorithms for modeling risk  abdullah karasan   in chapter   i have successfully accessed the time series data of energy capacity utilization from the fred for the period of    and followed the codes on book to remove its seasonality  however  one error occurred when i tried to resample the energy series with the following codes  for which i failed to find relevant solutions  i am a beginner of python in finance working on this topic solely  so i have no ideas of this problem 
381,381,45843,72474771,Why does bfloat16 have so many exponent bits?,"<p>It's clear why a 16-bit floating-point format has started seeing use for machine learning; it reduces the cost of storage and computation, and neural networks turn out to be surprisingly insensitive to numeric precision.</p>
<p>What I find particularly surprising is that practitioners abandoned the already-defined half-precision format in favor of one that allocates only 7 bits to the significand, but 8 bits to the exponent – fully as many as 32-bit FP.  (<a href=""https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format"" rel=""nofollow noreferrer"">wikipedia</a> compares brain-float <code>bfloat16</code> layout against IEEE binary16 and some 24-bit formats.)</p>
<p>Why so many exponent bits? So far, I have only found <a href=""https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus</a></p>
<blockquote>
<p>Based on our years of experience training and deploying a wide variety of neural networks across Google’s products and services, we knew when we designed Cloud TPUs that neural networks are far more sensitive to the size of the exponent than that of the mantissa. To ensure identical behavior for underflows, overflows, and NaNs, bfloat16 has the same exponent size as FP32. However, bfloat16 handles denormals differently from FP32: it flushes them to zero. Unlike FP16, which typically requires special handling via techniques such as loss scaling [Mic 17], BF16 comes close to being a drop-in replacement for FP32 when training and running deep neural networks.</p>
</blockquote>
<p>I haven't run neural network experiments on anything like Google scale, but in such as I have run, a weight or activation with absolute value much greater than 1.0 means it's gone into the weeds, is going to spiral off into infinity, and the computer would be doing you a favor if it were to promptly crash with an error message. I have never seen or heard of any case that needs a dynamic range anything like the 1e38 of single-precision floating point.</p>
<p>So what am I missing?</p>
<p>Are there cases where neural networks really need huge dynamic range? If so, how, why?</p>
<p>Is there some reason why it is considered very beneficial for bfloat16 to use the same exponent as single precision, even though the significand is much smaller?</p>
<p>Or is it the case that the real goal was to shrink the significand to the absolute minimum that would do the job, in order to minimize the chip area and energy cost of the multipliers, being the most expensive part of an FPU; it so happened this turned out to be around 7 bits; the total size should be a power of 2 for alignment reasons; it would not quite fit in 8 bits; going up to 16, left surplus bits that might as well be used for something, and the most elegant solution was to keep the 8-bit exponent?</p>
",62,1,4,5,machine-learning;neural-network;floating-point;cpu-architecture;half-precision-float,2022-06-02 16:03:09,2022-06-02 16:03:09,2022-06-03 06:16:51,it s clear why a  bit floating point format has started seeing use for machine learning  it reduces the cost of storage and computation  and neural networks turn out to be surprisingly insensitive to numeric precision  what i find particularly surprising is that practitioners abandoned the already defined half precision format in favor of one that allocates only  bits to the significand  but  bits to the exponent   fully as many as  bit fp     compares brain float bfloat layout against ieee binary and some  bit formats   why so many exponent bits  so far  i have only found  based on our years of experience training and deploying a wide variety of neural networks across google s products and services  we knew when we designed cloud tpus that neural networks are far more sensitive to the size of the exponent than that of the mantissa  to ensure identical behavior for underflows  overflows  and nans  bfloat has the same exponent size as fp  however  bfloat handles denormals differently from fp  it flushes them to zero  unlike fp  which typically requires special handling via techniques such as loss scaling  mic    bf comes close to being a drop in replacement for fp when training and running deep neural networks  i haven t run neural network experiments on anything like google scale  but in such as i have run  a weight or activation with absolute value much greater than   means it s gone into the weeds  is going to spiral off into infinity  and the computer would be doing you a favor if it were to promptly crash with an error message  i have never seen or heard of any case that needs a dynamic range anything like the e of single precision floating point  so what am i missing  are there cases where neural networks really need huge dynamic range  if so  how  why  is there some reason why it is considered very beneficial for bfloat to use the same exponent as single precision  even though the significand is much smaller  or is it the case that the real goal was to shrink the significand to the absolute minimum that would do the job  in order to minimize the chip area and energy cost of the multipliers  being the most expensive part of an fpu  it so happened this turned out to be around  bits  the total size should be a power of  for alignment reasons  it would not quite fit in  bits  going up to   left surplus bits that might as well be used for something  and the most elegant solution was to keep the  bit exponent 
382,382,1367124,70851048,Does it make sense to use Conda + Poetry?,"<p>Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:</p>
<p>As far as I understand, <strong>Conda</strong> and <strong>Poetry</strong> have different purposes but are largely redundant:</p>
<ul>
<li>Conda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.</li>
<li>Poetry is primarily a Python package manager (say, an upgrade of <strong>pip</strong>), but it can also create and manage Python environments (say, an upgrade of <strong>Pyenv</strong>).</li>
</ul>
<p>My idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.</p>
<p>I've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like <code>poetry shell</code> or <code>poetry run</code>, only <code>poetry init</code>, <code>poetry install</code> etc (after activating the Conda environment).</p>
<p>For full disclosure, my <em>environment.yml</em> file (for Conda) looks like this:</p>
<pre><code>name: N

channels:
  - defaults
  - conda-forge

dependencies:
  - python=3.9
  - cudatoolkit
  - cudnn
</code></pre>
<p>and my <em>poetry.toml</em> file looks like that:</p>
<pre><code>[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>To be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.</p>
<p>Does this project design look reasonable to you?</p>
",7528,1,28,5,python;machine-learning;package;conda;python-poetry,2022-01-25 20:39:43,2022-01-25 20:39:43,2022-06-03 03:20:36,does it make sense to use conda   poetry for a machine learning project  allow me to share my  novice  understanding and please correct or enlighten me  as far as i understand  conda and poetry have different purposes but are largely redundant  my idea is to use both and compartmentalize their roles  let conda be the environment manager and poetry the package manager  my reasoning is that  it sounds like  conda is best for managing environments and can be used for compiling and installing non python packages  especially cuda drivers  for gpu capability   while poetry is more powerful than conda as a python package manager  i ve managed to make this work fairly easily by using poetry within a conda environment  the trick is to not use poetry to manage the python environment  i m not using commands like poetry shell or poetry run  only poetry init  poetry install etc  after activating the conda environment   for full disclosure  my environment yml file  for conda  looks like this  and my poetry toml file looks like that  to be honest  one of the reasons i proceeded this way is that i was struggling to install cuda  for gpu support  without conda  does this project design look reasonable to you 
383,383,19254089,72472063,Importing a json file in pandas dataframe faster,"<p>I have a pandas dataframe (created by appending several CSV files) with more than 5 million records. I need to use the data for a machine learning model.</p>
<p>I would like to convert it to JSON format so that the data loads faster every time I open my ML code. The code below runs fine without any error.</p>
<p>However, it takes a long time to execute. It is taking the same time as it would to read a huge CSV file. I believe one can read a JSON file with millions of records in a few seconds/minutes. Could anyone suggest how that could be done?</p>
<pre class=""lang-py prettyprint-override""><code># creating Json file
result = dfcustdata.to_json('custdata.json', indent= 1, orient= 'records') 

#reading into dataframe
dffinalcustdata = pd.read_json('custdata.json')

</code></pre>
<p>Imp Update - I figured out a way to import huge CSVs very fast without converting to JSON. Here is the code (you can tinker with the chunksize). The code imports in chunks and then appends them all in the final dataframe df:</p>
<pre><code>tp = pd.read_csv('custdata.csv', iterator=True, chunksize=2000)
df = pd.concat(tp, ignore_index=True)
</code></pre>
",44,0,0,4,python;json;pandas;machine-learning,2022-06-02 12:40:35,2022-06-02 12:40:35,2022-06-02 15:59:40,i have a pandas dataframe  created by appending several csv files  with more than  million records  i need to use the data for a machine learning model  i would like to convert it to json format so that the data loads faster every time i open my ml code  the code below runs fine without any error  however  it takes a long time to execute  it is taking the same time as it would to read a huge csv file  i believe one can read a json file with millions of records in a few seconds minutes  could anyone suggest how that could be done  imp update   i figured out a way to import huge csvs very fast without converting to json  here is the code  you can tinker with the chunksize   the code imports in chunks and then appends them all in the final dataframe df 
384,384,44232,4633584,Algorithm to generate all possible arrays of ones and zeros of a given length,"<p>How can I generate all possible bit combinations in an array of bits of length n. If I start with all zeros in my array then there are n possibilities to place the first bit and for these n possibilities there are n-1 possibilities to place the second bit.. unit all n bits are set to one. But so far I didn't manage to program it out.</p>

<p>Also many people pointed out that I can do this by counting from 0 to (2^n)-1 and printing the number in binary. This would be an easy way to solve the problem, however in this case I just let the machine counting instead of telling it where to place ones. I do this for learning, so I would like to know how to program out the ones-placing approach.</p>
",6853,7,8,4,c++;algorithm;language-agnostic;haskell,2011-01-08 16:43:43,2011-01-08 16:43:43,2022-06-02 15:42:45,how can i generate all possible bit combinations in an array of bits of length n  if i start with all zeros in my array then there are n possibilities to place the first bit and for these n possibilities there are n  possibilities to place the second bit   unit all n bits are set to one  but so far i didn t manage to program it out  also many people pointed out that i can do this by counting from  to   n   and printing the number in binary  this would be an easy way to solve the problem  however in this case i just let the machine counting instead of telling it where to place ones  i do this for learning  so i would like to know how to program out the ones placing approach 
385,385,4156957,72473099,DeepLabV3 MLMultiArray output conversion to Image is super slow,"<p>I'm using the DeepLabV3 <code>MLModel</code> provided by Apple from this link: <a href=""https://developer.apple.com/machine-learning/models/"" rel=""nofollow noreferrer"">https://developer.apple.com/machine-learning/models/</a>.</p>
<p>Seems like the actual prediction is quick. I'm running it on a stream of <code>CIImage</code>s which I convert to <code>CVPixelBuffer</code>s (quickly).<br />
After performing the prediction I need to get the actual segmentation mask as <code>CIImage</code>.<br />
This operation runs super slow even on my iPhone 13 Pro.</p>
<p>Code:</p>
<pre><code>let prediction = try! deepLab.prediction(image: pixelBuffer)
let semanticPredictions = prediction.semanticPredictions
</code></pre>
<p>Two methods I tried to fetch back <code>CIImage</code> of the segmentation mask, both super slow:</p>
<ul>
<li><p>Using <code>UIGraphicsImageRenderer</code></p>
<pre><code>func fetchMasKUsingGraphicRenderer(mlMultiArray: MLMultiArray) -&gt; CIImage {

  let aWidth = CGFloat(mlMultiArray.shape[0].intValue)
  let aHeight = CGFloat(mlMultiArray.shape[1].intValue)

  let renderer = UIGraphicsImageRenderer(size: CGSize(width: aWidth, height: aHeight))

  let img = renderer.image(actions: { context in
      let ctx = context.cgContext
      ctx.clear(CGRect(x: 0.0, y: 0.0, width: Double(aWidth), height: Double(aHeight)));
      for j in 0..&lt;Int(aHeight) {
          for i in 0..&lt;Int(aWidth) {

              let aValue =
                  (mlMultiArray[j * Int(aHeight) + i].floatValue &gt; 0.0) ?
                      1.0 : 0.0
              let aRect = CGRect(
                  x: CGFloat(i),
                  y: CGFloat(j),
                  width: 1.0,
                  height: 1.0)

              let aColor: UIColor = UIColor(
                  displayP3Red: 0.0,
                  green: 0.0,
                  blue: 0.0,
                  alpha: CGFloat(aValue))

              aColor.setFill()
              UIRectFill(aRect)
          }
      }
  })

  return CIImage(image: img)!
}
</code></pre>
</li>
<li><p>Using CoreMLHelpers by Hollance: <a href=""https://github.com/hollance/CoreMLHelpers"" rel=""nofollow noreferrer"">https://github.com/hollance/CoreMLHelpers</a>:</p>
<pre><code>let maskedCiImage = CIImage(cgImage: semanticPredictions.cgImage()!)
</code></pre>
</li>
</ul>
<p><strong>What would be the right way to do it? Thanks!</strong></p>
",35,0,2,5,ios;swift;coreml;ciimage;mlmodel,2022-06-02 14:00:20,2022-06-02 14:00:20,2022-06-02 14:09:49,i m using the deeplabv mlmodel provided by apple from this link    code  two methods i tried to fetch back ciimage of the segmentation mask  both super slow  using uigraphicsimagerenderer using coremlhelpers by hollance    what would be the right way to do it  thanks 
386,386,19252414,72469273,How to properly do Matlab string concatenation and save loop over string names of a folder for preprocessing script,"<p>I am doing a machine learning project and need to alter some demo code that stitches images into one individual panorama as a preprocessing step for my dataset. I am not familiar with Matlab and am struggling to do this. This has very little if any theory involved, but I need help moving to the next step in my project due to time constraints.</p>
<p>I am building off of this repository's demo to stitch images into a panoramic image</p>
<p>My dataset folder has several different blocks of 6 images with different names for each block that will, when stitched together, represent a panoramic view of a scene.</p>
<p>I currently am able to stitch 6 images together within my dataset into one panoramic image, but I need it to generalize and loop through the whole dataset</p>
<p>I need the last section in the demo for Matterport to run for the whole dataset not just one block of images.</p>
<p>I am unfamiliar with Matlab so much of my issue is trying to find a way to alter the script to generalize and iterate my whole dataset. I would love to meet and discuss. Thank you so much</p>
<p>This is my current state where I try to loop through files in my folder and stick the names next to the utterable digits which make up the 6 images that will be stitched</p>
<p>Ex: the dataset files in the dataset are organized like this:</p>
<pre><code>0b302846f0994ec9851862b1d317d7f2_skybox0_sami
0b302846f0994ec9851862b1d317d7f2_skybox1_sami
0b302846f0994ec9851862b1d317d7f2_skybox2_sami
0b302846f0994ec9851862b1d317d7f2_skybox3_sami
0b302846f0994ec9851862b1d317d7f2_skybox4_sami
0b302846f0994ec9851862b1d317d7f2_skybox5_sami
1eae38cbbaf744f99de90c99d8c013f1_skybox0_sami
(and this then continues with another name like this)
</code></pre>
<p>Ex: Code to process</p>
<pre class=""lang-matlab prettyprint-override""><code>x = dir;

for i = length(x)/6
    x1 = x(i).name;
    
    name = x1(1:32);

    
    sepImg = [];
    
    vx = [-pi/2 -pi/2 0 pi/2 pi -pi/2];
    vy = [pi/2 0 0 0 0 -pi/2];
    
    for a = 1:6
        sepImg(a).img = im2double(imread(sprintf([name '_skybox%d_sami.jpg'],a-1)));
        sepImg(a).vx = vx(a);
        sepImg(a).vy = vy(a);
        sepImg(a).fov = pi/2+0.001;
        sepImg(a).sz = size(sepImg(a).img);
    end
    
    panoskybox = combineViews( sepImg, 2048, 1024 );
    
    
    figure; imshow(panoskybox);
    
    
    imwrite(panoskybox, [name '_skybox.jpg'])

end
</code></pre>
",33,0,0,2,matlab;data-preprocessing,2022-06-02 05:43:58,2022-06-02 05:43:58,2022-06-02 13:18:47,i am doing a machine learning project and need to alter some demo code that stitches images into one individual panorama as a preprocessing step for my dataset  i am not familiar with matlab and am struggling to do this  this has very little if any theory involved  but i need help moving to the next step in my project due to time constraints  i am building off of this repository s demo to stitch images into a panoramic image my dataset folder has several different blocks of  images with different names for each block that will  when stitched together  represent a panoramic view of a scene  i currently am able to stitch  images together within my dataset into one panoramic image  but i need it to generalize and loop through the whole dataset i need the last section in the demo for matterport to run for the whole dataset not just one block of images  i am unfamiliar with matlab so much of my issue is trying to find a way to alter the script to generalize and iterate my whole dataset  i would love to meet and discuss  thank you so much this is my current state where i try to loop through files in my folder and stick the names next to the utterable digits which make up the  images that will be stitched ex  the dataset files in the dataset are organized like this  ex  code to process
387,387,19239019,72445592,I keep getting this &quot;ValueError: could not convert string to float&quot; error. Whats wrong in my code?,"<p>I am trying to convert IP address into numbers which will then be classified and further used but I keep getting this error <code>ValueError: could not convert string to float: '10.200.7.194-64.233.186.127-35758-19305-6'</code>. How do I fix this? Has it occurred to anyone else too? I'll share more details if needed/required.</p>
<p>Edit: Okay , so here's the code. Basically I am working on a project to classify network traffic as malicious or not using machine learning.</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import socket, struct

import imblearn
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import LabelEncoder
from collections import Counter
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline
from imblearn.under_sampling import NearMiss

filepath = &quot;Dataset-87_Atts.csv&quot;
df = pd.read_csv(filepath)
df = df.replace(r'^\s*$', np.nan, regex=True)
df.head()
df.tail()


# Function that changes IP addresses into numbers
def ip2int(ip):
    packedIP = socket.inet_aton(ip)
    return struct.unpack(&quot;!L&quot;, packedIP)[0]


# Converting IP addresses into numbers
df['Source.IP'] = df['Source.IP'].apply(ip2int)
df['Destination.IP'] = df['Destination.IP'].apply(ip2int)

# Checking if any value in the dataframe is null
df.isnull().values.any()

# Checking types of values
print(df.dtypes)

# Checking columns that have only one unique value
df.columns[df.nunique() &lt;= 1]

# Checking occurance of each application
df['ProtocolName'].value_counts()

# Features that will be removed from dataset because they have low occurances of records in dataset
feats_toDelete = df['ProtocolName'].value_counts()[-25:].index
feats_toDelete

# Plot the number of records for individual applications
target_count = df['ProtocolName'].value_counts()
plt.figure(figsize=(16, 10))
target_count.plot(kind='bar', title='Occurance')

# Removal of the applications saved in feats_toDelete var because they occur in a small amount of records and do not have a major impact on the model
df = df[df.ProtocolName.isin(feats_toDelete)]
df=df.replace(r'^\s*$',np.nan,regex=True)

# Plot the number of records for individual applications one more time after some application removal
target_count2 = df['ProtocolName'].value_counts()
plt.figure(figsize=(16, 10))
target_count2.plot(kind='bar', title='Occurance')
feats = [x for x in df.columns if x != 'ProtocolName']
X = df[feats].astype(float)
Y = df['ProtocolName']

# Convert application names to numbers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
print(sorted(Counter(Y).items()))
print(sorted(Counter(encoded_Y).items()))
dict_nearMiss = {
    &quot;GOOGLE&quot;: 10000,
    &quot;HTTP&quot;: 10001,
    &quot;HTTP_PROXY&quot;: 10002,
    &quot;SSL&quot;: 10003,
    &quot;HTTP_CONNECT&quot;: 10004,
    &quot;YOUTUBE&quot;: 10005,
    &quot;AMAZON&quot;: 10006,
    &quot;MICROSOFT&quot;: 10007,
    &quot;GMAIL&quot;: 10008,
    &quot;WINDOWS_UPDATE&quot;: 10009,
    &quot;SKYPE&quot;: 10010,
    &quot;FACEBOOK&quot;: 10011,
    &quot;DROPBOX&quot;: 10012,
    &quot;YAHOO&quot;: 10013,
    &quot;TWITTER&quot;: 10014,
    &quot;CLOUDFLARE&quot;: 10015,
    &quot;MSN&quot;: 10016,
}
dict_smote = \
{
              &quot;CONTENT_FLASH&quot;: 10017,
              &quot;APPLE&quot;: 10018,
              &quot;OFFICE_365&quot;: 10019,
              &quot;WHATSAPP&quot;: 10020,
              &quot;INSTAGRAM&quot;: 10021,
              &quot;WIKIPEDIA&quot;: 10022,
              &quot;MS_ONE_DRIVE&quot;: 10023,
              &quot;DNS&quot;: 10024,
              &quot;IP_ICMP&quot;: 10025,
              &quot;NETFLIX&quot;: 10026,
              &quot;APPLE_ITUNES&quot;: 10027,
              &quot;SPOTIFY&quot;: 10028,
              &quot;APPLE_ICLOUD&quot;: 10029,
              &quot;EBAY&quot;: 10030,
              &quot;SSL_NO_CERT&quot;: 10031,
              &quot;GOOGLE_MAPS&quot;: 10032,
              &quot;EASYTAXI&quot;: 10033,
              &quot;TEAMVIEWER&quot;: 10034,
              &quot;HTTP_DOWNLOAD&quot;: 10035,
              &quot;MQTT&quot;: 10036,
              &quot;TOR&quot;: 10037,
              &quot;FTP_DATA&quot;: 10038,
              &quot;UBUNTUONE&quot;: 10039,
              &quot;NTP&quot;: 10040,
              &quot;SSH&quot;: 10041
}
print(dict_smote)

# getting rid of the problem of unbalanced data set
pipe = make_pipeline(
    SMOTE(sampling_strategy=dict_smote),
    NearMiss(sampling_strategy=dict_nearMiss)
)
X_resampled, y_resampled = pipe.fit_resample(X, Y)
print(&quot;Shape pierwotnego pliku &quot; + str(df.shape))
print(&quot;Shape X &quot; + str(X.shape))
print(&quot;Shape X_resampled&quot; + str(X_resampled.shape))
print(&quot;Shape Y &quot; + str(Y.shape))
print(&quot;Shape y_resampled&quot; + str(y_resampled.shape))

# creating new dataset
new_dataframe = pd.DataFrame(data=X_resampled, columns=feats)
new_dataframe['ProtocolName'] = y_resampled
new_dataframe.describe()
new_dataframe.to_csv('KaggleImbalanced.csv', index=False)
</code></pre>
",29,0,-1,4,python-3.x;pandas;dataframe;numpy,2022-05-31 15:16:26,2022-05-31 15:16:26,2022-06-02 10:15:00,i am trying to convert ip address into numbers which will then be classified and further used but i keep getting this error valueerror  could not convert string to float                how do i fix this  has it occurred to anyone else too  i ll share more details if needed required  edit  okay   so here s the code  basically i am working on a project to classify network traffic as malicious or not using machine learning 
388,388,13070210,72468087,Retrieve metrics from a saved model machine learning,"<p>I have a question, is it possible to recover the metrics of a saved model  like f1 score, confusion matrix, recall, ... without going through the train and the test?</p>
<p>I use pickle to save my model</p>
<pre><code>with open('SVM_Model.pkl', 'wb') as f:
    pickle.dump(fitted_model, f)

                                    
with open('SVM_Model.pkl', 'rb') as f:
    joblib_LR_model = pickle.load(f)
</code></pre>
",27,1,-1,4,python-3.x;machine-learning;scikit-learn;model,2022-06-02 02:40:21,2022-06-02 02:40:21,2022-06-02 09:13:04,i have a question  is it possible to recover the metrics of a saved model  like f score  confusion matrix  recall      without going through the train and the test  i use pickle to save my model
389,389,2063902,60637170,How to pass arguments to scoring file when deploying a Model in AzureML,"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.
I have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse</code>.
However, I don't seem to find how I can pass arguments
This is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.conda_dependencies import CondaDependencies
from azureml.core.environment import Environment
from azureml.core.model import InferenceConfig

env = Environment('my_hosted_environment')
env.python.conda_dependencies = CondaDependencies.create(
    conda_packages=['scikit-learn'],
    pip_packages=['azureml-defaults'])
scoring_script = 'score.py --model_name ' + model_name
inference_config = InferenceConfig(entry_script=scoring_script, environment=env)
</code></pre>

<p>Adding the score.py for reference on how I'd love to use the arguments in that script:</p>

<pre class=""lang-py prettyprint-override""><code>#removed imports
import argparse

def init():
    global model

    parser = argparse.ArgumentParser(description=""Load sklearn model"")
    parser.add_argument('--model_name', dest=""model_name"", required=True)
    args, _ = parser.parse_known_args()

    model_path = Model.get_model_path(model_name=args.model_name)
    model = joblib.load(model_path)

def run(raw_data):
    try:
        data = json.loads(raw_data)['data']
        data = np.array(data)
        result = model.predict(data)
        return result.tolist()

    except Exception as e:
        result = str(e)
        return result
</code></pre>

<p>Interested to hear your thoughts</p>
",1483,3,4,3,python;azure-machine-learning-service;azureml,2020-03-11 18:57:40,2020-03-11 18:57:40,2022-06-02 09:00:29,adding the score py for reference on how i d love to use the arguments in that script  interested to hear your thoughts
390,390,19242815,72452858,Displaying data from summarization dataset in TensorFlow (using TensorFlow datasets),"<p>I'm new to Machine Learning and a newbie when it comes to utilizing the TensorFlow Module in Python.</p>
<p>I'm currently working with summarization and the dataset library in TensorFlow has many convenient datasets available for training the summarizers. However, I wanted to take a look at their contents before chosing one in particular, does anyone know how to display the dataset as a Table in the Python console?</p>
<p>So far, I have the example code (for the Opinosis dataset) from the TensorFlow website, which is the following:</p>
<pre><code># Copyright 2022 The TensorFlow Datasets Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

&quot;&quot;&quot;Opinosis Opinion Dataset.&quot;&quot;&quot;

import os

import tensorflow as tf
import tensorflow_datasets.public_api as tfds

_CITATION = &quot;&quot;&quot;
@inproceedings{ganesan2010opinosis,
  title={Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions},
  author={Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},
  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics},
  pages={340--348},
  year={2010},
  organization={Association for Computational Linguistics}
}
&quot;&quot;&quot;

_DESCRIPTION = &quot;&quot;&quot;
The Opinosis Opinion Dataset consists of sentences extracted from reviews for 51 topics.
Topics and opinions are obtained from Tripadvisor, Edmunds.com and Amazon.com.
&quot;&quot;&quot;

_URL = &quot;https://github.com/kavgan/opinosis-summarization/raw/master/OpinosisDataset1.0_0.zip&quot;

_REVIEW_SENTS = &quot;review_sents&quot;
_SUMMARIES = &quot;summaries&quot;


class Opinosis(tfds.core.GeneratorBasedBuilder):
  &quot;&quot;&quot;Opinosis Opinion Dataset.&quot;&quot;&quot;

  VERSION = tfds.core.Version(&quot;1.0.0&quot;)

  def _info(self):
    return tfds.core.DatasetInfo(
        builder=self,
        description=_DESCRIPTION,
        features=tfds.features.FeaturesDict({
            _REVIEW_SENTS: tfds.features.Text(),
            _SUMMARIES: tfds.features.Sequence(tfds.features.Text())
        }),
        supervised_keys=(_REVIEW_SENTS, _SUMMARIES),
        homepage=&quot;http://kavita-ganesan.com/opinosis/&quot;,
        citation=_CITATION,
    )

  def _split_generators(self, dl_manager):
    &quot;&quot;&quot;Returns SplitGenerators.&quot;&quot;&quot;
    extract_path = dl_manager.download_and_extract(_URL)
    return [
        tfds.core.SplitGenerator(
            name=tfds.Split.TRAIN,
            gen_kwargs={&quot;path&quot;: extract_path},
        ),
    ]

  def _generate_examples(self, path=None):
    &quot;&quot;&quot;Yields examples.&quot;&quot;&quot;
    topics_path = os.path.join(path, &quot;topics&quot;)
    filenames = tf.io.gfile.listdir(topics_path)
    for filename in filenames:
      file_path = os.path.join(topics_path, filename)
      topic_name = filename.split(&quot;.txt&quot;)[0]
      with tf.io.gfile.GFile(file_path, &quot;rb&quot;) as src_f:
        input_data = src_f.read()
      summaries_path = os.path.join(path, &quot;summaries-gold&quot;, topic_name)
      summary_lst = []
      for summ_filename in sorted(tf.io.gfile.listdir(summaries_path)):
        file_path = os.path.join(summaries_path, summ_filename)
        with tf.io.gfile.GFile(file_path, &quot;rb&quot;) as tgt_f:
          data = tgt_f.read().strip()
          summary_lst.append(data)
      summary_data = summary_lst
      yield filename, {_REVIEW_SENTS: input_data, _SUMMARIES: summary_data}```
</code></pre>
",21,1,0,4,python;tensorflow;tensorflow-datasets;summarization,2022-06-01 00:24:33,2022-06-01 00:24:33,2022-06-02 01:11:44,i m new to machine learning and a newbie when it comes to utilizing the tensorflow module in python  i m currently working with summarization and the dataset library in tensorflow has many convenient datasets available for training the summarizers  however  i wanted to take a look at their contents before chosing one in particular  does anyone know how to display the dataset as a table in the python console  so far  i have the example code  for the opinosis dataset  from the tensorflow website  which is the following 
391,391,19156908,72311274,"iOS - TensorFlow Lite Error: Select TensorFlow op(s), included in the given model is(are) not supported by this interpreter","<p>I am trying to integrate on-device machine learning using react-native. I have converted a transformers model from huggingface to a tensorflow lite file. Doing so, I can get the model to successfully run on the android side of things. When I try to do the same for iOS I am getting this following error:</p>
<p><code>TensorFlow Lite Error: Select TensorFlow op(s), included in the given model is(are) not supported by this interpreter. Make sure you apply/link Flex delegate before inference. For the Android, it can be resolved by adding &quot;org.tensorflow:tensorflow-lite-select-tf-ops&quot; dependency.</code></p>
<p><code>TensorFlow Lite Error: Node number 95 (FlexErf) failed to prepare.</code></p>
<p>I had a similar error on the android side and I solved it using the guide here:
<a href=""https://www.tensorflow.org/lite/guide/ops_select"" rel=""nofollow noreferrer"">https://www.tensorflow.org/lite/guide/ops_select</a></p>
<p>I have followed the steps on the above link for iOS as well and yet I am still getting this error:
This includes:</p>
<ul>
<li>Adding the dependency in the podfile</li>
<li>Running pod install</li>
<li>Adding the other flag links in the build settings.</li>
</ul>
<p>How can I get past this error?</p>
",60,1,0,5,ios;tensorflow;machine-learning;tensorflow-lite;huggingface-transformers,2022-05-20 02:59:18,2022-05-20 02:59:18,2022-06-01 23:25:58,i am trying to integrate on device machine learning using react native  i have converted a transformers model from huggingface to a tensorflow lite file  doing so  i can get the model to successfully run on the android side of things  when i try to do the same for ios i am getting this following error  tensorflow lite error  select tensorflow op s   included in the given model is are  not supported by this interpreter  make sure you apply link flex delegate before inference  for the android  it can be resolved by adding  org tensorflow tensorflow lite select tf ops  dependency  tensorflow lite error  node number   flexerf  failed to prepare  how can i get past this error 
392,392,12772968,72459029,"How to show graphs in Jupyter-notebook, all other solutions did not work","<p>I am taking a machine learning course, the code is being implemented with jupyter notebook.</p>
<p>Problem description:
It does not show the graph, that I intented to see.</p>
<p>Solutions applied:
Using; %matplotlib inline, %matplotlib under the import line, show() is used as it is suggested in this site: <a href=""https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm"" rel=""nofollow noreferrer"">https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm</a></p>
<p>My code: (each line means a different line in the notebook)</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.linear_model import LinearRegression

df = pd.read_csv(&quot;udemy machine learning/homeprices.csv&quot;)
df

plt.xlabel('Area(sqr ft)')
plt.ylabel('Price(US$)')
plt.scatter(df.Area, df.Price, color= 'red', marker = '+')

plt.show()
</code></pre>
<p>Thanks in advance.</p>
<p><strong>Edited:</strong>
Dataframe works fine, the data is ok.</p>
",40,0,0,5,python;matplotlib;graph;scikit-learn;jupyter-notebook,2022-06-01 14:27:25,2022-06-01 14:27:25,2022-06-01 22:09:33,i am taking a machine learning course  the code is being implemented with jupyter notebook  my code   each line means a different line in the notebook  thanks in advance 
393,393,11244722,72464322,Using NLP or other algorithms to mach two strings,"<p>My goal of a project is to correctly assign medications. I have a large catalog at my disposal for this purpose. However, the medications do not appear there in exactly the same spelling. Possibly additional information was added or possible parts of the prescription were abbreviated.
I was already able to implement a possible algorithm using the Levensthein distance (token_set_ratio).
Because of the sometimes long additional information this algorithm assigns wrong medications, I wanted to ask if there are better algorithms for comparing strings. For example, does it make sense to implement machine learning algorithms or NLP technology? This is a relatively new area for me. I would appreciate any ideas or inspiration.</p>
",21,1,-1,2,machine-learning;nlp,2022-06-01 20:47:51,2022-06-01 20:47:51,2022-06-01 21:37:24,
394,394,14050808,72464789,Dealing With Variable Length Audio in Machine Learning,"<p>I'm working on a model for speech emotion recognition, and I'm currently in the pre-processing phase, creating a utility that can transform audio files into a feature space of fixed dimensions. I'm planning on experimenting with spectrograms, mel-spectrograms and mfccs (including deltas and delta-deltas) as input features for convolutional neural networks. One glaring issue is that the audio is of variable length.</p>
<p>Now, I know that the typical method of dealing with this is setting some length and then expanding all the audio files to fit that length, or truncating files, but I imagine the former method is preferable, because truncation loses some data that could be valuable in training. So I intend to pad audio files with 0s to expand them to some fixed length. I found the maximum duration of an audio file in my dataset and then took its ceiling to get the length. Now I intend to add trailing 0s (after resampling to a fixed sampling rate) to expand all the audio files to have a static length.</p>
<p>My question is, do these extra dimensions not potentially confuse the model? I know neural networks automatically handle feature extraction, but are there any potential caveats I should be made aware of, or perhaps some alternative method for going about doing this that may produce better results?</p>
<p>Thanks.</p>
",20,0,1,5,machine-learning;audio;deep-learning;conv-neural-network;signal-processing,2022-06-01 21:21:28,2022-06-01 21:21:28,2022-06-01 21:21:28,i m working on a model for speech emotion recognition  and i m currently in the pre processing phase  creating a utility that can transform audio files into a feature space of fixed dimensions  i m planning on experimenting with spectrograms  mel spectrograms and mfccs  including deltas and delta deltas  as input features for convolutional neural networks  one glaring issue is that the audio is of variable length  now  i know that the typical method of dealing with this is setting some length and then expanding all the audio files to fit that length  or truncating files  but i imagine the former method is preferable  because truncation loses some data that could be valuable in training  so i intend to pad audio files with s to expand them to some fixed length  i found the maximum duration of an audio file in my dataset and then took its ceiling to get the length  now i intend to add trailing s  after resampling to a fixed sampling rate  to expand all the audio files to have a static length  my question is  do these extra dimensions not potentially confuse the model  i know neural networks automatically handle feature extraction  but are there any potential caveats i should be made aware of  or perhaps some alternative method for going about doing this that may produce better results  thanks 
395,395,11707836,72464239,Multiprocessing inside Flask API - not for Browser,"<p>I'm not using this flask API for any UI code,</p>
<p>So, in my team they have designed in this way - Any advice on better alternate ways are welcome.</p>
<p>To run a CPU intensive and time taking huge machine learning process - we broke it down to multiple small processess, some of which are I/O expensive and some are CPU expensive. The plan is we can use HDFS for common storage and run any API in any server (we have 4-6 servers with minimal configuration - so running all of them in one may be more heavy).</p>
<p>Current Process - we are using HDFS for common storage and Kafka for pushing the output for every API to kafka and based on the output next API will be triggered.</p>
<p>Example: Process X - broken down to p1, p2, p3</p>
<p>ConsumeKafka -&gt; Process decider(another api to decide which process of p1,p2,p3 should be triggered) -&gt; p1 (runs and pushes the output to Kafka)</p>
<p>All the 3 process(p1,p2,p3) will push the output to kafka only, only the p3 will end the process by entering the data into DB</p>
<p>One such API - which is CPU expensive - we tried using multiprocessing.
It's running fine for some requests and then hangs without any error. we have to kill the api and rerun it. (Also we need to hit this API parallely - multiple hits at once)</p>
<p>Multithreading is not giving any such error - but not considerable improvement in timing.</p>
<p>Please suggest a better way of doing this.</p>
<p>Above setup is done in Apache Nifi for execution.</p>
",20,0,-1,3,python;flask;multiprocessing,2022-06-01 20:42:04,2022-06-01 20:42:04,2022-06-01 20:42:04,i m not using this flask api for any ui code  so  in my team they have designed in this way   any advice on better alternate ways are welcome  to run a cpu intensive and time taking huge machine learning process   we broke it down to multiple small processess  some of which are i o expensive and some are cpu expensive  the plan is we can use hdfs for common storage and run any api in any server  we have   servers with minimal configuration   so running all of them in one may be more heavy   current process   we are using hdfs for common storage and kafka for pushing the output for every api to kafka and based on the output next api will be triggered  example  process x   broken down to p  p  p consumekafka   gt  process decider another api to decide which process of p p p should be triggered    gt  p  runs and pushes the output to kafka  all the  process p p p  will push the output to kafka only  only the p will end the process by entering the data into db multithreading is not giving any such error   but not considerable improvement in timing  please suggest a better way of doing this  above setup is done in apache nifi for execution 
396,396,12157218,72456723,H2O AI with own python machine learning model integration with snowflake,"<p>i have a few questions regarding H2O AI. As per my understanding, h2o AI powers Auto ML functionality. but need to integrate my own python jupyetr ML model. so my questions are,</p>
<ol>
<li>Can we use H2O AI without Auto ML and with our own python jupyter ML algorithm?</li>
<li>If yes, can we integrate that own manual scripted ML with Snowflake?</li>
<li>If we can integrate our own scripted ml algorithm with snowflake, what are the advantages of doing it that way? instead of an own manually-created python ML algorithm?</li>
</ol>
",33,1,0,5,python;snowflake-cloud-data-platform;h2o;automl;h2o.ai,2022-06-01 10:47:18,2022-06-01 10:47:18,2022-06-01 20:19:21,i have a few questions regarding ho ai  as per my understanding  ho ai powers auto ml functionality  but need to integrate my own python jupyetr ml model  so my questions are 
397,397,18577563,72463350,"convert image from [0.0, 1.0] to [0, 255]","<p>Suppose the image x consists of floats in the range [0, 1],</p>
<ol>
<li>Torchvision adopts the transform of <code>clip(x*255+0.5, 0, 255).as(uint8)</code> .</li>
<li>Skimage seems similar to torch</li>
<li>TensorFlow uses an asymmetric approach</li>
</ol>
<p>Details on the conversion follow below.</p>
<p>However, while investigating a few things, I found that this method gives an unfairly small chance for values of 0 and 255 compared to other values.</p>
<p>Why do these machine learning libraries use these unfair transformations?</p>
<p>pytorch
<a href=""https://pytorch.org/vision/main/_modules/torchvision/utils.html#save_image"" rel=""nofollow noreferrer"">https://pytorch.org/vision/main/_modules/torchvision/utils.html#save_image</a></p>
<pre class=""lang-py prettyprint-override""><code>from collections import Counter, defaultdict
import numpy as np
DICT = defaultdict(list)
def as_uint8(X):
    return np.clip(X * 255 + 0.5, 0, 255).astype(np.uint8)

for K, V in Counter(as_uint8(np.linspace(0/256, 256/256, 32 * 256))).items():
    DICT[V].append(K)
print(DICT)
</code></pre>
<pre><code>defaultdict(&lt;class 'list'&gt;, {17: [0, 255], 32: [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 252, 253, 254], 33: [8, 16, 25, 33, 41, 49, 58, 66, 74, 82, 90, 99, 107, 115, 123, 132, 140, 148, 156, 165, 173, 181, 189, 197, 206, 214, 222, 230, 239, 247]})
</code></pre>
<p>skimage <a href=""https://scikit-image.org/docs/dev/user_guide/data_types.html"" rel=""nofollow noreferrer"">https://scikit-image.org/docs/dev/user_guide/data_types.html</a></p>
<pre class=""lang-py prettyprint-override""><code>from skimage.util import img_as_ubyte
from collections import Counter, defaultdict
import numpy as np
DICT = defaultdict(list)

for K, V in Counter(img_as_ubyte(np.linspace(0/256, 256/256, 32 * 256).reshape(-1, 1, 1)).reshape(-1)).items():
    DICT[V].append(K)
print(DICT)
</code></pre>
<pre><code>defaultdict(&lt;class 'list'&gt;, {17: [0, 255], 32: [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 252, 253, 254], 33: [8, 16, 25, 33, 41, 49, 58, 66, 74, 82, 90, 99, 107, 115, 123, 132, 140, 148, 156, 165, 173, 181, 189, 197, 206, 214, 222, 230, 239, 247]})
</code></pre>
<p>tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype</a></p>
<pre><code>import tensorflow as tf
from collections import Counter, defaultdict
import numpy as np
DICT = defaultdict(list)

img = tf.convert_to_tensor(np.linspace(0/256, 256/256, 32 * 256).reshape(-1, 1, 1))
img = tf.image.convert_image_dtype(img, dtype=tf.uint8, saturate=False)
img = tf.reshape(img, -1).numpy()

for K, V in Counter(img).items():
    DICT[V].append(K)
print(DICT)
</code></pre>
<pre><code>defaultdict(&lt;class 'list'&gt;, {33: [0, 17, 34, 51, 68, 85, 102, 119, 136, 153, 170, 187, 204, 221, 238], 32: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254], 17: [255]})
</code></pre>
<p>my suggestion 1</p>
<pre class=""lang-py prettyprint-override""><code>from collections import Counter, defaultdict
import numpy as np
DICT = defaultdict(list)
def as_uint8(X):
    return np.clip(np.rint(X * 256 - 0.5), 0, 255).astype(np.uint8)
for K, V in Counter(as_uint8(np.linspace(0/256, 256/256, 32 * 256))).items():
    DICT[V].append(K)
print(DICT)
</code></pre>
<pre><code>defaultdict(&lt;class 'list'&gt;, {32: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]})
</code></pre>
<p>my suggestion 2</p>
<pre class=""lang-py prettyprint-override""><code>from collections import Counter, defaultdict
import numpy as np
DICT = defaultdict(list)
def as_uint8(X):
    return np.clip(X * 256, 0, 255).astype(np.uint8)
for K, V in Counter(as_uint8(np.linspace(0/256, 256/256, 32 * 256))).items():
    DICT[V].append(K)
print(DICT)
</code></pre>
<pre><code>defaultdict(&lt;class 'list'&gt;, {32: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]})
</code></pre>
",55,0,0,5,python;tensorflow;pytorch;scikit-image;torchvision,2022-06-01 19:45:23,2022-06-01 19:45:23,2022-06-01 19:58:50,suppose the image x consists of floats in the range       details on the conversion follow below  however  while investigating a few things  i found that this method gives an unfairly small chance for values of  and  compared to other values  why do these machine learning libraries use these unfair transformations  skimage  tensorflow  my suggestion  my suggestion 
398,398,8771368,72463398,Parallel processing on all nodes of Azure Machine Learning compute cluster,"<p>Is there a way to use all nodes of a Azure ML compute cluster for parallel processing, each running the same Python code?</p>
<p>In the example below I want the hostname of the two nodes of the compute cluster to be returned. I does <strong>not</strong> run in parallel, only sequentially. The hostnames returned are equal! Eventually I would like to do distributed hyper parameter tuning with Optuna (can not use HyperDrive).</p>
<p><em>Any ideas on how to solve my problem?</em> The code below is just my latest try.</p>
<p>I create a Azure ML compute cluster like this:</p>
<pre><code>from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core import ScriptRunConfig, Experiment

provisioning_config = AmlCompute.provisioning_configuration(
   vm_size ='STANDARD_DS11_V2', 
   max_nodes = 2, idle_seconds_before_scaledown = 120, vm_priority=&quot;lowpriority&quot;)
cluster = ComputeTarget.create(ws, &quot;mycluster&quot;, provisioning_config)
</code></pre>
<p>then run an experiment</p>
<pre><code>exp = Experiment(name=&quot;myexp_children&quot;, workspace=ws)
src = ScriptRunConfig(source_directory=&quot;.&quot;, script=&quot;train.py&quot;, compute_target=cluster)
run=exp.submit(src)
run.wait_for_completion(show_output=True)
</code></pre>
<p>The code <strong>train.py</strong> to be run on each node is</p>
<pre><code>import socket
import time
from azureml.core import Run

run = Run.get_context()
child_runs = run.create_children(count=2)
for c, child in enumerate(child_runs):
    hostname = socket.gethostname()
    time.sleep(30)
    child.log(name=&quot;Hello from child run &quot;, value=c)
    child.log(name=&quot;Hostname&quot;, value=hostname)
    child.complete()
</code></pre>
",46,0,0,4,python;azure;machine-learning;optuna,2022-06-01 19:48:09,2022-06-01 19:48:09,2022-06-01 19:48:09,is there a way to use all nodes of a azure ml compute cluster for parallel processing  each running the same python code  in the example below i want the hostname of the two nodes of the compute cluster to be returned  i does not run in parallel  only sequentially  the hostnames returned are equal  eventually i would like to do distributed hyper parameter tuning with optuna  can not use hyperdrive   any ideas on how to solve my problem  the code below is just my latest try  i create a azure ml compute cluster like this  then run an experiment the code train py to be run on each node is
399,399,19145967,72463065,Swamped with real time data and tasked with building a database,"<p>I work for a power company, and have been tasked with building a database. I have a pretty beginner/intermediate understanding level of python, and can fuddle decently with MSSQL. They have procured Azure for this project, and I am completely lost of how to start this task.</p>
<p>Here is one of the sources of data that I want to scrape every minute.
<a href=""http://ets.aeso.ca/ets_web/docroot/tradingPage.html"" rel=""nofollow noreferrer"">http://ets.aeso.ca/ets_web/docroot/tradingPage.html</a> - this is a complete overview of the Alberta power market in real time.</p>
<p>Ideally, I would want to be able to scrape this data and other sources, and then modify it to fit into in a certain format and push it onto the SQL server.</p>
<p>Do I need virtual machines that are just looping over python scripts? Or do I need managed instances? This data also then needs to be able to be queried right after it is scraped. Eventually this data may feed machine learning algorithms (I don't know jack about that either but I have been told it should play friendly with that type of enviornment).</p>
<p>Just looking to see if anyone has any insight in how you would approach this, and can tell me what I clearly don't know and haven't thought of. Any insight is truly appreciated.</p>
<p>Thanks!</p>
",25,0,0,4,python;sql;database;bigdata,2022-06-01 19:25:06,2022-06-01 19:25:06,2022-06-01 19:25:06,i work for a power company  and have been tasked with building a database  i have a pretty beginner intermediate understanding level of python  and can fuddle decently with mssql  they have procured azure for this project  and i am completely lost of how to start this task  ideally  i would want to be able to scrape this data and other sources  and then modify it to fit into in a certain format and push it onto the sql server  do i need virtual machines that are just looping over python scripts  or do i need managed instances  this data also then needs to be able to be queried right after it is scraped  eventually this data may feed machine learning algorithms  i don t know jack about that either but i have been told it should play friendly with that type of enviornment   just looking to see if anyone has any insight in how you would approach this  and can tell me what i clearly don t know and haven t thought of  any insight is truly appreciated  thanks 
400,400,10725445,70250296,How to interpret a 4 dimensional contigiency table in pandas,"<p>I am trying to understand this contingency table and I have no luck looking in the documentation of pandas or any other related questions. This is for a personal machine learning project.</p>
<p>I have the following example data:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({&quot;la&quot;:[0,1,1,0,1], &quot;lp1&quot;: [1,0,1,0,0], &quot;lp2&quot;:[1,1,0,0,1], &quot;lp3&quot;:[0,0,1,1,1], &quot;lp4&quot;:[0,1,1,0,0]})
</code></pre>
<p>Hence we will have a Boolean contingency table. If I run:</p>
<pre class=""lang-py prettyprint-override""><code>df.crosstab(index=df['la'], columns=[df['lp1'],df['lp2']])
</code></pre>
<p>I get the output:</p>
<pre class=""lang-py prettyprint-override""><code>lp1  0     1   
lp2  0  1  0  1
la             
0    1  0  0  1
1    0  2  1  0
</code></pre>
<p>This can be better visualised in a table like so:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><strong>lp1</strong>:</td>
<td><strong>0</strong></td>
<td><strong>0</strong></td>
<td><strong>1</strong></td>
<td><strong>1</strong></td>
</tr>
<tr>
<td></td>
<td><strong>lp2</strong>:</td>
<td><strong>0</strong></td>
<td><strong>1</strong></td>
<td><strong>0</strong></td>
<td><strong>1</strong></td>
</tr>
<tr>
<td><strong>la</strong></td>
<td><strong>0</strong></td>
<td><em>1</em></td>
<td><em>0</em></td>
<td><em>0</em></td>
<td><em>1</em></td>
</tr>
<tr>
<td></td>
<td><strong>1</strong></td>
<td><em>0</em></td>
<td><em>2</em></td>
<td><em>1</em></td>
<td><em>0</em></td>
</tr>
</tbody>
</table>
</div>
<p>Which can be better understood as (e.g.) there were <code>2</code> occurrences of <code>lp1=0 lp2=1 and la=0</code> in the dataset. However, if I run:</p>
<pre class=""lang-py prettyprint-override""><code>pd.crosstab(index=df['la'], columns=[df['lp1'],df['lp2'],df['lp3']])
</code></pre>
<p>I expect a table like this:</p>
<pre class=""lang-py prettyprint-override""><code>lp1  0           1   
lp2  0     1     0     1
lp3  0  1  0  1  0  1  0  1 
la                
0    1   #results here
1    0  
</code></pre>
<p>Instead, I get:</p>
<pre class=""lang-py prettyprint-override""><code>lp1  0        1   
lp2  0  1     0  1
lp3  1  0  1  1  0
la                
0    1  0  0  0  1
1    0  1  1  1  0
</code></pre>
<p>I have no idea how to interpret this table. I am unsure why there is a repeating label (the repeating 1 in lp3), why the labels suddenly change order from <code>0,1</code> to <code>1,0</code>. This continues for higher dimensions (e.g. 5 dimensions):</p>
<pre class=""lang-py prettyprint-override""><code>pd.crosstab(index=df['la'], columns=[df['lp1'],df['lp2'],df['lp3'],df['lp4']])
</code></pre>
<p>produces:</p>
<pre class=""lang-py prettyprint-override""><code>lp1  0        1   
lp2  0  1     0  1
lp3  1  0  1  1  0
lp4  0  1  0  1  0
la                
0    1  0  0  0  1
1    0  1  1  1  0
</code></pre>
<p>It all makes sense to me until you have a table &gt; 3 dimensions. I am running Python 3.10 and pandas 1.3.4.</p>
<p>I have tried to go through the source code, docs and related StackOverflow questions and I have not found an answer.</p>
<p>How do I properly interpret this 4 dimensional table, please?</p>
",23,1,0,3,python;pandas;dataframe,2021-12-07 00:08:18,2021-12-07 00:08:18,2022-06-01 18:24:12,i am trying to understand this contingency table and i have no luck looking in the documentation of pandas or any other related questions  this is for a personal machine learning project  i have the following example data  hence we will have a boolean contingency table  if i run  i get the output  this can be better visualised in a table like so  which can be better understood as  e g   there were  occurrences of lp  lp  and la  in the dataset  however  if i run  i expect a table like this  instead  i get  i have no idea how to interpret this table  i am unsure why there is a repeating label  the repeating  in lp   why the labels suddenly change order from   to    this continues for higher dimensions  e g   dimensions   produces  it all makes sense to me until you have a table  gt   dimensions  i am running python   and pandas     i have tried to go through the source code  docs and related stackoverflow questions and i have not found an answer  how do i properly interpret this  dimensional table  please 
401,401,19247831,72461594,KeyError warning - Index problem while concatenating two cols,"<p>im newbie here and also in machine learning. I've got a dataset and try to handle with Ridge Regression.</p>
<p>Dataset of first five:</p>
<pre><code>df_3.head()

departure_state destination_state   passengers numbers  seats numbers   flight_numbers distance(mile)

0   KS  IA  21  30  1   254
1   OR  OR  41  396 22  103
2   OR  OR  88  342 19  103
3   OR  OR  11  72  4   103
4   OR  OR  0   18  1   156
</code></pre>
<p>One-hot encoding for first two categorical variables:</p>
<pre><code>dms = pd.get_dummies(df_3[[&quot;departure_state&quot;, &quot;destination_state&quot;]])

y = df_3[&quot;distance(mile)&quot;] --&gt; dependent variable
</code></pre>
<p>After the dummy process, before creating independent variables,  i've created x_ variable by dropping all dependent and independent variables from the dataset:</p>
<pre><code>x_ = df_3.drop([&quot;departure_state&quot;, &quot;destination_state&quot;, &quot;distance(mile)&quot;], axis = 1) 
</code></pre>
<p>Then, i combined the x_ variable with original dataset. i formed the final version of the independent variables by combining the dummy variables that i've created and the x_ variable:</p>
<pre><code>x = pd.concat([x_, dms[[&quot;departure_state_New&quot;, &quot;destination_state_New&quot;]]], axis = 1)
</code></pre>
<p>At this point, i got an KeyError: (failed to concat)</p>
<p>KeyError: <strong>&quot;None of [Index(['departure_state', 'destination_state'], dtype='object')] are in the [columns]&quot;</strong></p>
<p>How can i add new two cols to existing dataframe in this context?</p>
<p>Thanks so much for kindly help.</p>
",10,0,0,3,indexing;concatenation;keyerror,2022-06-01 17:35:13,2022-06-01 17:35:13,2022-06-01 17:37:48,im newbie here and also in machine learning  i ve got a dataset and try to handle with ridge regression  dataset of first five  one hot encoding for first two categorical variables  after the dummy process  before creating independent variables   i ve created x_ variable by dropping all dependent and independent variables from the dataset  then  i combined the x_ variable with original dataset  i formed the final version of the independent variables by combining the dummy variables that i ve created and the x_ variable  at this point  i got an keyerror   failed to concat  keyerror   none of  index   departure_state    destination_state    dtype  object    are in the  columns   how can i add new two cols to existing dataframe in this context  thanks so much for kindly help 
402,402,18201044,72459433,Machine learning - does the independent variable data need to be balanced as well?,"<p>I know that we need to have balanced data in <code>y</code> to have a better model. However, I'm wondering whether we need to have balanced data in independent variable as well.</p>
<p>In the following dataframe, <code>X3</code> is a category type independent variable.</p>
<pre><code>X1     X2    X3     y

22     67    1      0
33     87    1      0
55     66    1      0
77     12    1      0
28     68    1      1
12     64    2      0
19     17    2      1
10     62    2      1
88     19    2      1
99     20    2      1
</code></pre>
<p>While the data in <code>y</code> is balanced (1:1 distribution), <code>X3</code> has imbalanced data in each category (4:1 distribution).</p>
<p>Do I need to have equal distribution in X3 as well?</p>
",41,1,-1,5,machine-learning;scikit-learn;xgboost;lightgbm;feature-engineering,2022-06-01 14:56:28,2022-06-01 14:56:28,2022-06-01 17:26:06,i know that we need to have balanced data in y to have a better model  however  i m wondering whether we need to have balanced data in independent variable as well  in the following dataframe  x is a category type independent variable  while the data in y is balanced    distribution   x has imbalanced data in each category    distribution   do i need to have equal distribution in x as well 
403,403,18712958,72458567,how to interpret learning curve in machine learning?,"<p>those're the learning curves for each algorithm I used. i'm working on my report and i'm confused how to interpret the curve.
I used multi label classification algorithms.
this is the learning curve of binary relevance the classifier is KNeighborsClassifier.
<a href=""https://i.stack.imgur.com/eupZg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eupZg.png"" alt=""1st learning curve"" /></a></p>
<p>the second one is the curve of classifier chain using DecisionTreeClassifier
<a href=""https://i.stack.imgur.com/c7iFo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c7iFo.png"" alt=""2nd learning curve"" /></a></p>
<p>and the last one is the curve of LabelPowerset using GaussianNB
<a href=""https://i.stack.imgur.com/h5AL8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h5AL8.png"" alt=""3rd learning curve"" /></a></p>
<p>which one is the best? because the accuracy and the F1 score are good results</p>
",43,1,-2,4,python;classification;evaluation;overfitting-underfitting,2022-06-01 13:53:17,2022-06-01 13:53:17,2022-06-01 17:25:50,which one is the best  because the accuracy and the f score are good results
404,404,12705800,72461350,Creating a Graph from a Map,"<p>I have a pdf and png of a map that we are all used to, with roads and cities. I want to convert this into a graph, preferable on networkx. I don't think there's a unique software that does this so maybe I need to create a program by myself. What are some topics and keywords that I should checkout? I am thinking about putting a node on every &quot;circle&quot; that designates a city however I can't think of a better way to do this than using machine learning with synthetic data, but I don't want to do that.</p>
",15,0,-1,2,computer-vision;networkx,2022-06-01 17:17:46,2022-06-01 17:17:46,2022-06-01 17:17:46,i have a pdf and png of a map that we are all used to  with roads and cities  i want to convert this into a graph  preferable on networkx  i don t think there s a unique software that does this so maybe i need to create a program by myself  what are some topics and keywords that i should checkout  i am thinking about putting a node on every  circle  that designates a city however i can t think of a better way to do this than using machine learning with synthetic data  but i don t want to do that 
405,405,17745513,71151176,Link NFT Collection to Opensea,"<p>Hi super basic question as I am new to NFT dev (currently learning solidity).  Looking to make a super basic NFT project with website, art, and link to Opensea in secondary market (no roadmap, just a learning experience for me and probably giving NFTs for free).  I understand that you can make a candy machine that allows users to mint on website, but I'd like them to be able to view and trade their nfts on a secondary marketplace like Opensea.  How would I go about doing this?  Thanks.</p>
",111,2,0,3,solidity;nft;opensea,2022-02-17 06:26:43,2022-02-17 06:26:43,2022-06-01 16:02:21,hi super basic question as i am new to nft dev  currently learning solidity    looking to make a super basic nft project with website  art  and link to opensea in secondary market  no roadmap  just a learning experience for me and probably giving nfts for free    i understand that you can make a candy machine that allows users to mint on website  but i d like them to be able to view and trade their nfts on a secondary marketplace like opensea   how would i go about doing this   thanks 
406,406,14122745,63461131,Deleting environments from azureml studio,"<p>How may I delete an environment from azure machine learning workspace? I can create and list them but could not figure out how I may delete them?</p>
",1937,4,5,1,azure-machine-learning-service,2020-08-18 08:14:38,2020-08-18 08:14:38,2022-06-01 15:29:14,how may i delete an environment from azure machine learning workspace  i can create and list them but could not figure out how i may delete them 
407,407,19232844,72457740,How to convert audio blob recorded using AudioReactRecorder to tensor with the required 4D shape,"<p>I have recorded an audio using an npm library for react, AudioReactRecorder. This npm library gives a blob object which I have to pass it to my Machine Learning Model created using Python Tensorflow. To convert the audio blob to Tensor I first converted the blob to arraybuffer and then passed it to tensor() function of Tensorflow js. Please refer below code for reference.</p>
<pre><code>       new Response(audioData).arrayBuffer().then(val =&gt; {
            const buffer=Buffer.from(val,'binary');
            const x = tf.tensor(buffer, [1, 783, 129], 'float32')
            model2.then((graphModel) =&gt; {
                let prediction = graphModel.predict(x.expandDims())
            })

        })
</code></pre>
<p>Note: Here, the [1, 783, 129] refers to the tensor shape that is expected by the ML Model.</p>
<p>The ArrayBuffer retrieved from the blob contains the following value:</p>
<pre><code>Uint8Array(15) [91, 111, 98, 106, 101, 99, 116, 32, 79, 98, 106, 101, 99, 116, 93, buffer: ArrayBuffer(15), byteLength: 15, byteOffset: 0, length: 15, Symbol(Symbol.toStringTag): 'Uint8Array']
0: 91
1: 111
2: 98
3: 106
4: 101
5: 99
6: 116
7: 32
8: 79
9: 98
10: 106
11: 101
12: 99
13: 116
14: 93
buffer: 
ArrayBuffer(15)
</code></pre>
<p>As the length of ArrayBuffer here is only 15, the tensor is not able to reshape to [1, 783,129] shape. Can anyone tell me how can this audio blob be converted to buffer so that the can be reshaped to this dimension [1,783,129]?</p>
",29,0,0,5,reactjs;machine-learning;blob;tensorflow.js;arraybuffer,2022-06-01 12:46:12,2022-06-01 12:46:12,2022-06-01 12:46:12,i have recorded an audio using an npm library for react  audioreactrecorder  this npm library gives a blob object which i have to pass it to my machine learning model created using python tensorflow  to convert the audio blob to tensor i first converted the blob to arraybuffer and then passed it to tensor   function of tensorflow js  please refer below code for reference  note  here  the        refers to the tensor shape that is expected by the ml model  the arraybuffer retrieved from the blob contains the following value  as the length of arraybuffer here is only   the tensor is not able to reshape to       shape  can anyone tell me how can this audio blob be converted to buffer so that the can be reshaped to this dimension      
408,408,9492890,72456623,Sum of individual labels over a month of granular data,"<p>I have a dataframe which contains life logging data gathered over several years from 44 unique individuals.</p>
<pre><code>Int64Index: 77171 entries, 0 to 4279
Data columns (total 4 columns):
 #   Column     Non-Null Count  Dtype         
---  ------     --------------  -----         
 0   start      77171 non-null  datetime64[ns]
 1   end        77171 non-null  datetime64[ns]
 2   labelName  77171 non-null  category      
 3   id         77171 non-null  int64         
</code></pre>
<p>The <code>start</code> column contains granular datetimes of the format <code>2020-11-01 11:00:00</code>, in  intervals of 30 minutes. The <code>labelName</code> column has 14 different categories.</p>
<pre><code>Categories (14, object): ['COOK', 'EAT', 'GO WALK', 'GO TO BATHROOM', ..., 'DRINK', 'WAKE UP', 'SLEEP', 'WATCH TV']
</code></pre>
<p>Here's a sample user's head, which is <code>[2588 rows x 4 columns]</code> and spans from 2020 to 2021. There are also gaps in the data, occasionally.</p>
<pre><code>                  start                 end       labelName   id
0   2020-08-05 00:00:00 2020-08-05 00:30:00  GO TO BATHROOM  486
1   2020-08-05 06:00:00 2020-08-05 06:30:00         WAKE UP  486
2   2020-08-05 09:00:00 2020-08-05 09:30:00            COOK  486
3   2020-08-05 11:00:00 2020-08-05 11:30:00             EAT  486
4   2020-08-05 12:00:00 2020-08-05 12:30:00             EAT  486
..                  ...                 ...             ...  ...
859 2021-03-10 12:30:00 2021-03-10 13:00:00  GO TO BATHROOM  486
861 2021-03-10 13:30:00 2021-03-10 14:00:00  GO TO BATHROOM  486
862 2021-03-10 18:30:00 2021-03-10 19:00:00            COOK  486
864 2021-03-11 08:00:00 2021-03-11 08:30:00             EAT  486
865 2021-03-11 12:30:00 2021-03-11 13:00:00            COOK  486
</code></pre>
<p>I want a sum of each unique <code>labelNames</code> per user per month, but I'm not sure how to do this.</p>
<p>I would first split the data frame by <code>id</code>, which is easy. But how do you split these <code>start</code> datetimes when it records every 30 minutes over several years of data— and then create 14 new columns which record the sums?</p>
<p>The final data frame might look something like this (with fake values):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>user</th>
<th>month</th>
<th>SLEEP</th>
<th style=""text-align: center;"">...</th>
<th>WATCH TV</th>
</tr>
</thead>
<tbody>
<tr>
<td>486</td>
<td>jun20</td>
<td>324</td>
<td style=""text-align: center;"">...</td>
<td>23</td>
</tr>
<tr>
<td>486</td>
<td>jul20</td>
<td>234</td>
<td style=""text-align: center;"">...</td>
<td>12</td>
</tr>
</tbody>
</table>
</div>
<p>The use-case for this data frame is training a few statistical and machine-learning models.</p>
<p>How do I achieve something like this?</p>
",48,2,2,4,python;pandas;dataframe;datetime,2022-06-01 10:30:08,2022-06-01 10:30:08,2022-06-01 11:29:55,i have a dataframe which contains life logging data gathered over several years from  unique individuals  the start column contains granular datetimes of the format        in  intervals of  minutes  the labelname column has  different categories  here s a sample user s head  which is   rows x  columns  and spans from  to   there are also gaps in the data  occasionally  i want a sum of each unique labelnames per user per month  but i m not sure how to do this  i would first split the data frame by id  which is easy  but how do you split these start datetimes when it records every  minutes over several years of data  and then create  new columns which record the sums  the final data frame might look something like this  with fake values   the use case for this data frame is training a few statistical and machine learning models  how do i achieve something like this 
409,409,19243630,72454313,Disco Diffusion: Error when typing python &quot;prd.py&quot;,"<p>I have Disco Diffusion on my PC, it was fairly straightforward to install with little to no issue.
However, I tried it on my friend's PC following the exact same <a href=""https://www.youtube.com/watch?v=pBS-FzOFbFU&amp;t=7s"" rel=""nofollow noreferrer"">steps</a>.
However, now I'm coming up with a tonne of problems. I have very little experience with programming and machine learning etc. But I'm good at following instructions. If someone can help me understand what this issue is.
I've been searching online forums without any leads.</p>
<p>Please if I haven't supplied enough information, don't hesitate to ask.</p>
<pre><code>##### = my friends username
</code></pre>
<p>(progrockdiffusion) PS C:\Users#####\progrockdiffusion&gt; python prd.py</p>
<blockquote>
<blockquote>
</blockquote>
</blockquote>
<pre><code>Traceback (most recent call last):
  File &quot;prd.py&quot;, line 99, in &lt;module&gt;
    from IPython import display
  File &quot;C:\Users\#####\miniconda3\envs\progrockdiffusion\lib\site-packages\IPython\__init__.py&quot;, line 55, in &lt;module&gt;
    from .core.application import Application
  File &quot;C:\Users\#####\miniconda3\envs\progrockdiffusion\lib\site-packages\IPython\core\application.py&quot;, line 23, in &lt;module&gt;
    from traitlets.config.application import Application, catch_config_error
  File &quot;C:\Users\#####\miniconda3\envs\progrockdiffusion\lib\site-packages\traitlets\__init__.py&quot;, line 5, in &lt;module&gt;
    from ._version import __version__, version_info
  File &quot;C:\Users\#####\miniconda3\envs\progrockdiffusion\lib\site-packages\traitlets\_version.py&quot;, line 8, in &lt;module&gt;
    &quot;.&quot;.join(map(str, version_info)).replace(&quot;.b&quot;, &quot;b&quot;).replace(&quot;.a&quot;, &quot;a&quot;).replace(&quot;.rc&quot;, &quot;rc&quot;)
AssertionError
</code></pre>
",67,0,-1,5,python;machine-learning;artificial-intelligence;miniconda;disco,2022-06-01 02:59:47,2022-06-01 02:59:47,2022-06-01 09:57:55,please if i haven t supplied enough information  don t hesitate to ask   progrockdiffusion  ps c  users      progrockdiffusion gt  python prd py
410,410,1670156,63880205,ffmpeg: Is it possible to replace frames in a variable frame-rate video?,"<p>Machine learning algorithms for video processing typically work on frames (images) rather than video.</p>
<p>In my work, I use ffmpeg to dump a specific scene as a sequence of .png files, process them in some way (denoise, deblur, colorize, annotate, inpainting, etc), output the results into an equal number of .png files, and then update the original video with the new frames.</p>
<p>This works well with constant frame-rate (CFR) video.  I dump the images <a href=""https://stackoverflow.com/a/40090033/1670156"">as so</a> (eg, 50-frame sequence starting at 1:47):</p>
<pre><code>ffmpeg -i input.mp4 -vf &quot;select='gte(t,107)*lt(selected_n,50)'&quot; -vsync passthrough '107+%06d.png'
</code></pre>
<p>And then after editing the images, I replace the originals <a href=""https://stackoverflow.com/a/50257384/1670156"">as so</a> (for a 12.5fps CFR video):</p>
<pre><code>ffmpeg -i input.mp4 -itsoffset 107 -framerate 25/2 -i '107+%06d.png' -filter_complex &quot;[0]overlay=eof_action=pass&quot; -vsync passthrough -c:a copy output.mp4
</code></pre>
<p>However, many of the videos I work with are variable frame-rate (VFR), and this has created some challenges.</p>
<p>A simple solution is to convert VFR video to CFR, which ffmpeg wants to do anyway, but I'm wondering if it's possible to avoid this.  The reason is that CFR requires either dropping frames - since the purpose of ML video processing is usually to improve the output, I'd like to avoid this - or duplicating frames - but an upscaling algorithm that I'm working with right now uses the previous and next frame for data - if the previous or next frame is a duplicate, then ... no data for upscaling.</p>
<p>With <code>-vsync passthrough</code>, I had hoped that I could simply remove the <code>-framerate</code> option, and preserve the original frames as-is, but the resulting command:</p>
<pre><code>ffmpeg -i input.mp4 -itsoffset 107 -i '107+%06d.png' -filter_complex &quot;[0]overlay=eof_action=pass&quot; -vsync passthrough -c:a copy output.mp4
</code></pre>
<p>uses ffmpeg's <a href=""https://video.stackexchange.com/a/13074"">default of 25fps</a>, and drops a lot of frames.  Is there a reliable way to replace frames in VFR video?</p>
",438,1,3,2,ffmpeg;video-editing,2020-09-14 13:25:15,2020-09-14 13:25:15,2022-06-01 08:03:06,machine learning algorithms for video processing typically work on frames  images  rather than video  in my work  i use ffmpeg to dump a specific scene as a sequence of  png files  process them in some way  denoise  deblur  colorize  annotate  inpainting  etc   output the results into an equal number of  png files  and then update the original video with the new frames  this works well with constant frame rate  cfr  video   i dump the images   eg   frame sequence starting at     and then after editing the images  i replace the originals   for a  fps cfr video   however  many of the videos i work with are variable frame rate  vfr   and this has created some challenges  a simple solution is to convert vfr video to cfr  which ffmpeg wants to do anyway  but i m wondering if it s possible to avoid this   the reason is that cfr requires either dropping frames   since the purpose of ml video processing is usually to improve the output  i d like to avoid this   or duplicating frames   but an upscaling algorithm that i m working with right now uses the previous and next frame for data   if the previous or next frame is a duplicate  then     no data for upscaling  with  vsync passthrough  i had hoped that i could simply remove the  framerate option  and preserve the original frames as is  but the resulting command  uses ffmpeg s   and drops a lot of frames   is there a reliable way to replace frames in vfr video 
411,411,12374543,72452136,Passing a global variable though a multiprocess in Python,"<p>I have a sort of strange issue here. I am somewhat new to python and especially parallel TCP sockets in Python. I followed a guide to make a multi-client TCP server that runs using multiprocessing so I can maintain a front end control.</p>
<p>My understanding of passing variables in Python multiprocessing is using the args parameter when calling the multiprocessing.Process function.</p>
<p>However to start my TCP server my target isn't actually a def (function) and I am unsure how to properly pass a boolean value that will allow me to enable a debug mode in my code on the server process.</p>
<p>See code below:</p>
<pre><code>import os
import datetime
import socket
import jetson.inference
import jetson.utils
import sys
import multiprocessing
import socketserver
from cv2 import *

class ThreadedTCPRequestHandler(socketserver.BaseRequestHandler): #Class to process multiple clients if needed, handles sending and receiving messages as well as interperating them    
    def handle(self):
        #cam_port = 0
        if not os.path.exists(&quot;Images&quot;):
            os.makedirs(&quot;Images&quot;)
        net = jetson.inference.detectNet(&quot;ssd-inception-v2&quot;, sys.argv, 0.5)
        inputC = jetson.utils.videoSource(&quot;/dev/video0&quot;, argv=sys.argv)
        # create video sources &amp; outputs
        #input = jetson.utils.videoSource(opt.input_URI, argv=sys.argv)
        while True:
            #print(&quot;Got here&quot;)
            # self.request is the TCP socket connected to the client
            self.data = self.request.recv(250).strip()
            print(&quot;{} wrote:&quot;.format(self.client_address[0]))
            rec = self.data.decode('utf-8')
            print(rec)
            if rec == &quot;TRIGGER&quot;:
                #cam = VideoCapture(cam_port)
                #result, image = cam.read()
                #processTrigger(cam, result, image, net)
                processImage(net, inputC, debug)
            double = &quot;DOUBLE&quot;
            msg = double.encode('utf-8')
            self.request.sendall(msg)
        
class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): #Class to bind each client to a thread
    pass

def openServer(thread, server, isCreate): #function that handles opening and closing the tcp server
    if isCreate:
        with server:
            print(&quot;Server running as &quot;, thread.name, &quot;on&quot;, server.server_address)
            return thread
    if not isCreate:
        with server:
            thread.terminate()
            print(&quot;Server and Process terminated&quot;)

def processImage(network, inputC, debug): 
    # saving image in local storage
    img = inputC.Capture()
    detections = network.Detect(img, overlay=&quot;box,labels,conf&quot;)
    print(&quot;detected {:d} objects in image&quot;.format(len(detections)))
    print(debug)
    for detection in detections:
        print(detection)
    if debug:
        dateNow = datetime.datetime.now()
        currentMSStr = dateNow.strftime(&quot;%f&quot;)
        currentMS = round(int(currentMSStr)/1000)
        outputFile = &quot;Images/Capture_&quot;+dateNow.strftime(&quot;%y_%m_%d_%H_%M_%S&quot;)+&quot;_&quot;+str(currentMS)+&quot;.png&quot;
        output = jetson.utils.videoOutput(outputFile, argv=sys.argv+['--headless'])
        network.PrintProfilerTimes()
        output.Render(img)
        print(&quot;Warning! Debug mode is enabled, this should not be used during normal operation!&quot;)
    
debug = False #global debug boolean

if __name__ == &quot;__main__&quot;:
    enabled = True
    serverOpen = False
    
    HOST, PORT = &quot;&quot;, 3334
    print(&quot;Welcome to the Fives Vision Solution&quot;)
    print(&quot;Press 1 to enable the server&quot;)
    print(&quot;Press 2 to shutdown the server&quot;)
    print(&quot;Press 3 to change server port&quot;)
    print(&quot;Press 4 to enable debug mode. WARNING impacts performance!&quot;)
    print(&quot;Press 5 to exit&quot;)
    while enabled:
        choice = input(&quot;Make a selection... \n&quot;)
        if choice == '1': #Create socket and send to process to run in background
            if serverOpen:
                print(&quot;Server is already active&quot;)
            if not serverOpen:
                try:
                    server = ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler)
                    server_proc = multiprocessing.Process(target=server.serve_forever)
                    server_proc.daemon = True
                    server_proc.start()
                    openServer(server_proc,server,True)
                    serverOpen = True
                except Exception as e:
                    print(&quot;Error occured: &quot;, e)             
        elif choice == '2': #Terminate the server and close process
            openServer(server_proc,server,False)
            serverOpen = False
        elif choice == '3': #Change port
            if serverOpen:
                print(&quot;Please close the server first.&quot;)
            if not serverOpen:
                print(&quot;Please enter a port:&quot;)
                try:
                    port = input()
                    if int(port) &gt; 1024 and int(port) &lt; 65500:
                        PORT = int(port)
                        print(&quot;Port is now set to: &quot;, PORT)
                    if int(port) &lt; 1024 or int(port) &gt; 65500:
                        print(&quot;Invalid port, must be a number between 1023 and 65500&quot;)
                except:
                    print(&quot;Invalid port, must be a number between 1023 and 65500&quot;)
        elif choice == '4': #Enable or disable debug mode
            if debug:
                debug = False
                print(&quot;Debug set to: &quot; ,debug)
                continue
            if not debug:
                debug = True
                print(&quot;Debug set to: &quot; ,debug)
                continue
            
        elif choice == '5': #Terminate server is open and close the program
            enabled = False
            if (serverOpen):
                openServer(server_proc,server,False)
        else:
            print(&quot;Invalid choice&quot;)
</code></pre>
<p>As you can see, line 87 initiates the server, and line 88 calls the actual server to serve in another process:</p>
<pre><code>server = ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler)
server_proc = multiprocessing.Process(target=server.serve_forever)
</code></pre>
<p>How would I go about passing my global variable &quot;debug&quot; into my actual handle(self) function in my server class, which actually does the processing of the incoming message, calls a machine learning model and then returns the result back to the client.</p>
<p>Like I said, I'm pretty new to python so forgive me if there are better ways to do what I'm doing.</p>
<p><strong>About the code:</strong></p>
<p>My main issue is that I need debug mode because I want to be able to enable saving the images I process but doing so takes my total round trip of receiving a message, processing, and returning a result to around 200-250ms, and I need to run in less than 100ms.</p>
<p>When I disable the code within the debug if statement, my total receive/return time is around 40ms which is great, but if the system has issues I want to be able to store the results in image form for debugging camera issues and so on.</p>
<p>Thanks for any help!</p>
",38,0,0,3,python;python-3.x;multiprocessing,2022-05-31 23:15:25,2022-05-31 23:15:25,2022-05-31 23:15:25,i have a sort of strange issue here  i am somewhat new to python and especially parallel tcp sockets in python  i followed a guide to make a multi client tcp server that runs using multiprocessing so i can maintain a front end control  my understanding of passing variables in python multiprocessing is using the args parameter when calling the multiprocessing process function  however to start my tcp server my target isn t actually a def  function  and i am unsure how to properly pass a boolean value that will allow me to enable a debug mode in my code on the server process  see code below  as you can see  line  initiates the server  and line  calls the actual server to serve in another process  how would i go about passing my global variable  debug  into my actual handle self  function in my server class  which actually does the processing of the incoming message  calls a machine learning model and then returns the result back to the client  like i said  i m pretty new to python so forgive me if there are better ways to do what i m doing  about the code  my main issue is that i need debug mode because i want to be able to enable saving the images i process but doing so takes my total round trip of receiving a message  processing  and returning a result to around  ms  and i need to run in less than ms  when i disable the code within the debug if statement  my total receive return time is around ms which is great  but if the system has issues i want to be able to store the results in image form for debugging camera issues and so on  thanks for any help 
412,412,2439278,48398800,Minikube service URL not working,"<p>I'm new to Kubernetes and I'm learning. I have my Windows 8 machine where I installed Vagrant. Using vagrant I'm running ubuntu VM and inside that VM I'm running 3 docker containers.</p>

<p>Vagrant file:</p>

<pre><code>Vagrant.configure(2) do |config|
  config.vm.box = ""test""
  config.vm.network ""public_network""
  config.vm.network ""forwarded_port"", guest: 8080, host: 8080
  config.vm.network ""forwarded_port"", guest: 50000, host: 50000
  config.vm.network ""forwarded_port"", guest: 8081, host: 8089
  config.vm.network ""forwarded_port"", guest: 9000, host: 9000
  config.vm.network ""forwarded_port"", guest: 3306, host: 3306
    config.vm.provider ""virtualbox"" do |v|
        v.memory = 2048
        v.cpus = 2
    end
    config.vm.provider ""virtualbox"" do |v|
    v.customize [""modifyvm"", :id, ""--natdnshostresolver1"", ""on""]
    v.customize [""modifyvm"", :id, ""--natdnsproxy1"", ""on""]
end
end
</code></pre>

<p>Container in Ubuntu VM :</p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# docker images
REPOSITORY                                             TAG                 IMAGE ID            CREATED             SIZE
dockercontainers_jenkins                               latest              bb1142706601        4 days ago          1.03GB
dockercontainers_sonar                                 latest              3f021a73750c        4 days ago          1.61GB
dockercontainers_nexus                                 latest              ddc31d7ad052        4 days ago          1.06GB
jenkins/jenkins                                        lts                 279f21046a63        4 days ago          813MB
openjdk                                                8                   7c57090325cc        5 weeks ago         737MB
</code></pre>

<p>In same VM now I installed minikube and kubectl as mentioned in this <a href=""https://github.com/kubernetes/minikube"" rel=""noreferrer"">link</a></p>

<p>minikube version:</p>

<pre><code>minikube version: v0.24.1
</code></pre>

<p>kubectl version:</p>

<pre><code>Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.1"", GitCommit:""3a1c9449a956b6026f075fa3134ff92f7d55f812"", GitTreeState:""clean"", BuildDate:""2018-01-04T11:52:23Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4"", GitTreeState:""clean"", BuildDate:""2017-11-29T22:43:34Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}
</code></pre>

<p>Minikube successfully started in my ubuntu VM. I have created <code>pod.yml</code> file.</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: testsonaralm
  labels:
    app: sonar_alm
spec:
  containers:
  - name: alm-sonar
    image: dockercontainers_sonar:latest
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 9000
</code></pre>

<p>Using this yml file, I created a pod in minikube</p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# kubectl create -f test_pod.yml
pod ""testsonaralm"" created
</code></pre>

<p>Now I created a service using <code>kubectl</code> command.</p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# kubectl expose pod testsonaralm --port=9000 --target-port=9000 --name almsonar
service ""almsonar"" exposed

root@vagrant-ubuntu-trusty:~/docker-containers# kubectl get service
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
almsonar     ClusterIP   10.102.86.193   &lt;none&gt;        9000/TCP   10s
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    3d
</code></pre>

<p>When I tried to access the URL from my Host machine, I'm getting ""Network Error"".</p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# kubectl describe svc almsonar
Name:              almsonar
Namespace:         default
Labels:            app=sonar_alm
Annotations:       &lt;none&gt;
Selector:          app=sonar_alm
Type:              ClusterIP
IP:                10.101.237.223
Port:              &lt;unset&gt;  9000/TCP
TargetPort:        9000/TCP
Endpoints:         172.17.0.1:9000
Session Affinity:  None
Events:            &lt;none&gt;
root@vagrant-ubuntu-trusty:~/docker-containers# minikube ip
127.0.0.1
</code></pre>

<p>When I execute the <code>minikube service almsonar --url</code> command, I get an Empty response. So I deleted the service and created a new service with modified command.</p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# kubectl expose pod testsonaralm --type=NodePort --name almsonar
service ""almsonar"" exposed
</code></pre>

<p>Now when I run <code>minikube service almsonar --url</code> command,I got an URL as </p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# minikube service almsonar --url
http://127.0.0.1:31209

root@vagrant-ubuntu-trusty:~/docker-containers# kubectl describe svc almsonar
Name:                     almsonar
Namespace:                default
Labels:                   app=sonar_alm
Annotations:              &lt;none&gt;
Selector:                 app=sonar_alm
Type:                     NodePort
IP:                       10.101.192.1
Port:                     &lt;unset&gt;  9000/TCP
TargetPort:               9000/TCP
NodePort:                 &lt;unset&gt;  30600/TCP
Endpoints:                172.17.0.1:9000
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
root@vagrant-ubuntu-trusty:~/docker-containers# minikube ip
127.0.0.1
</code></pre>

<p>I'm unable to access this URL in my Ubuntu VM, </p>

<pre><code>root@vagrant-ubuntu-trusty:~/docker-containers# curl http://127.0.0.1:31209
&lt;HTML&gt;
&lt;HEAD&gt;&lt;TITLE&gt;Redirection&lt;/TITLE&gt;&lt;/HEAD&gt;
&lt;BODY&gt;&lt;H1&gt;Redirect&lt;/H1&gt;&lt;/BODY&gt;
</code></pre>

<p>When I read the Kubernetes document, the minikube service URL will have a vaild IP. But in my case URL contains localhost IP address.</p>
",14334,3,11,5,ubuntu;docker;vagrant;kubernetes;minikube,2018-01-23 15:26:15,2018-01-23 15:26:15,2022-05-31 23:04:30,i m new to kubernetes and i m learning  i have my windows  machine where i installed vagrant  using vagrant i m running ubuntu vm and inside that vm i m running  docker containers  vagrant file  container in ubuntu vm   in same vm now i installed minikube and kubectl as mentioned in this  minikube version  kubectl version  minikube successfully started in my ubuntu vm  i have created pod yml file  using this yml file  i created a pod in minikube now i created a service using kubectl command  when i tried to access the url from my host machine  i m getting network error  when i execute the minikube service almsonar   url command  i get an empty response  so i deleted the service and created a new service with modified command  now when i run minikube service almsonar   url command i got an url as  i m unable to access this url in my ubuntu vm   when i read the kubernetes document  the minikube service url will have a vaild ip  but in my case url contains localhost ip address 
413,413,226473,72385365,How to reshape pandas DataFrame,"<p>I'm following along in the book &quot;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow&quot;. In it, there is a section on Time Series Forecasting. I'm interesting in applying the methodology to intraday stock index price data.
My data looks like this:</p>
<pre><code>In [229]: frame.tail()
Out[229]: 
               O        H        L        C         Day      Time
1472543  4017.50  4018.39  4013.52  4014.38  2022-05-13  15:55:00
1472544  4014.68  4018.05  4014.68  4017.20  2022-05-13  15:56:00
1472545  4017.13  4019.95  4017.01  4019.83  2022-05-13  15:57:00
1472546  4019.86  4021.55  4017.94  4021.32  2022-05-13  15:58:00
1472547  4021.21  4024.77  4020.72  4023.56  2022-05-13  15:59:00
</code></pre>
<p>For each day there are 390 &quot;observations&quot; and there are 3751 days. I would like to reshape this data to the form: (3751, 390, 4).</p>
<p>The reason being that the data in the book has the shape: (7000,50,1). Based on this, it would be easiest to apply the methodology from the book to my dataset if my data were in the same shape.</p>
<p>However, I have tried several different ways (for several days now) without any luck.</p>
<p>I've tried making a <code>numpy.array()</code> of the dates (3751) and one for the times (390) and one for the price values (i.e. each day would have the shape (1,390,4)).
This however did not work:</p>
<pre><code>In [255]: c = []

In [257]: c.append(frame[&quot;Day&quot;].unique())

In [258]: c.append(frame[&quot;Time&quot;].unique())

In [259]: c.append(frame[features])

In [273]: np.array(c, dtype=object).reshape(3751,390,4)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-273-ddd7578e8519&gt; in &lt;module&gt;
----&gt; 1 np.array(c, dtype=object).reshape(3751,390,4)

ValueError: cannot reshape array of size 3 into shape (3751,390,4)

In [248]: x = np.array([frame[&quot;Day&quot;].unique,frame[&quot;Time&quot;].unique(),frame[[&quot;O&quot;,&quot;H&quot;,&quot;L&quot;,&quot;C&quot;]]], dtype=object)

In [249]: x.shape
Out[249]: (3,)

In [250]: frame[&quot;Day&quot;].unique().shape
Out[250]: (3751,)

In [251]: frame[&quot;Time&quot;].unique().shape
Out[251]: (390,)

In [252]: frame[features].shape
Out[252]: (1462890, 4)

In [253]: 390 * 3751
Out[253]: 1462890

In [254]: features
Out[254]: ['O', 'H', 'L', 'C']
</code></pre>
",82,1,0,3,python;pandas;numpy,2022-05-26 05:40:48,2022-05-26 05:40:48,2022-05-31 20:45:07,for each day there are   observations  and there are  days  i would like to reshape this data to the form          the reason being that the data in the book has the shape        based on this  it would be easiest to apply the methodology from the book to my dataset if my data were in the same shape  however  i have tried several different ways  for several days now  without any luck 
414,414,18479801,72449942,Can we accept a machine learning model if it produces the same AUROC value for every iteration in k-fold cross validation?,"<p>I have tried to cross-validate a random forest model with an imbalanced dataset using 10-fold cross-validation and AUROC as the evaluation metric. When perform the cross-validation on the training set, I get the same AUROC value for 9 folds (0.76) and 0.75 for one fold. This cross-validation was performed using the training set. When test the model with test set (which the model has never seen before), it gives 0.76 as the AUROC. Does this indicate any problem of the model or can I accept this model?</p>
",12,0,-1,4,machine-learning;random-forest;cross-validation;roc,2022-05-31 20:18:00,2022-05-31 20:18:00,2022-05-31 20:18:00,i have tried to cross validate a random forest model with an imbalanced dataset using  fold cross validation and auroc as the evaluation metric  when perform the cross validation on the training set  i get the same auroc value for  folds     and   for one fold  this cross validation was performed using the training set  when test the model with test set  which the model has never seen before   it gives   as the auroc  does this indicate any problem of the model or can i accept this model 
415,415,5116207,72438647,How to transparently convert absolute imports to relative inside external module?,"<p>I have a problem: I need to use some external code as part of my own project. This seems to be a relatively common use case in machine learning research when trying to build new experiments on top of previously published solutions/models.</p>
<p>The file structure is as follows:</p>
<pre><code>project_root/
├── external/
│   └── SomeoneElsesCode/
│       └── src/
│           └── dir1/
│               └── subdir1/
│                   ├── codeineed.py
│                   └── anciliarycode.py
└── src/
    └── MyModule/
        └── mycode.py
</code></pre>
<p>When trying to run the line</p>
<pre><code>from external.SomeoneElsesCode.src.dir1.subdir1.codeineed import NeededClass
</code></pre>
<p>in <code>mycode.py</code> I am running into the problem with line</p>
<pre><code>from src.dir1.subdir1.anciliarycode import AncilliaryClass
</code></pre>
<p>in <code>codeineed.py</code> since the external code is using absolute import paths. Since I do not control the code in <code>SomeoneElsesCode</code> I cannot simply adjust all import paths there. Is there any way to tell the Python interpreter to &quot;relativize&quot; all paths below <code>SomeoneElsesCode</code>? If not, is there any recommended way of dealing with including external code in Python projects?</p>
",38,1,0,2,python;python-3.x,2022-05-30 23:57:59,2022-05-30 23:57:59,2022-05-31 19:25:02,i have a problem  i need to use some external code as part of my own project  this seems to be a relatively common use case in machine learning research when trying to build new experiments on top of previously published solutions models  the file structure is as follows  when trying to run the line in mycode py i am running into the problem with line in codeineed py since the external code is using absolute import paths  since i do not control the code in someoneelsescode i cannot simply adjust all import paths there  is there any way to tell the python interpreter to  relativize  all paths below someoneelsescode  if not  is there any recommended way of dealing with including external code in python projects 
416,416,270572,4923836,Generating py.test tests in Python,"<p>Question first, then an explanation if you're interested.</p>
<p>In the context of py.test, how do I generate a large set of test functions from a small set of test-function templates?</p>
<p>Something like:</p>
<pre class=""lang-py prettyprint-override""><code>models = [model1,model2,model3]
data_sets = [data1,data2,data3]

def generate_test_learn_parameter_function(model,data):
    def this_test(model,data):
        param = model.learn_parameters(data)
        assert((param - model.param) &lt; 0.1 )
    return this_test

for model,data in zip(models,data_sets):
    # how can py.test can see the results of this function?
    generate_test_learn_parameter_function(model,data)
</code></pre>
<p>Explanation:</p>
<p>The code I'm writing takes a model structure, some data, and learns the parameters of the model. So my unit testing consists of a bunch of model structures and pre-generated data sets, and then a set of about 5 machine learning tasks to complete on each structure+data.</p>
<p>So if I hand code this I need one test per model per task. Every time I come up with a new model I need to then copy and paste the 5 tasks, changing which pickled structure+data I'm pointing at. This feels like bad practice to me. Ideally what I'd like is 5 template functions that define each of my 5 tasks and then to just spit out test functions for a list of structures that I specify.</p>
<p>Googling about brings me to either a) factories or b) closures, both of which addle my brain and suggest to me that there must be an easier way, as this problem must be faced regularly by proper programmers. So is there?</p>
<hr />
<p>EDIT: So here's how to solve this problem!</p>
<pre class=""lang-py prettyprint-override""><code>def pytest_generate_tests(metafunc):
    if &quot;model&quot; in metafunc.funcargnames:
        models = [model1,model2,model3]
        for model in models:
            metafunc.addcall(funcargs=dict(model=model))

def test_awesome(model):
    assert model == &quot;awesome&quot;
</code></pre>
<p>This will apply the <code>test_awesome</code> test to each model in my list of models! Thanks @dfichter!</p>
<p>(NOTE: that assert always passes, btw)</p>
",9724,3,22,3,python;unit-testing;pytest,2011-02-07 22:10:30,2011-02-07 22:10:30,2022-05-31 17:54:51,question first  then an explanation if you re interested  in the context of py test  how do i generate a large set of test functions from a small set of test function templates  something like  explanation  the code i m writing takes a model structure  some data  and learns the parameters of the model  so my unit testing consists of a bunch of model structures and pre generated data sets  and then a set of about  machine learning tasks to complete on each structure data  so if i hand code this i need one test per model per task  every time i come up with a new model i need to then copy and paste the  tasks  changing which pickled structure data i m pointing at  this feels like bad practice to me  ideally what i d like is  template functions that define each of my  tasks and then to just spit out test functions for a list of structures that i specify  googling about brings me to either a  factories or b  closures  both of which addle my brain and suggest to me that there must be an easier way  as this problem must be faced regularly by proper programmers  so is there  edit  so here s how to solve this problem  this will apply the test_awesome test to each model in my list of models  thanks  dfichter   note  that assert always passes  btw 
417,417,18212417,72447701,Guidance for creating a story generator model,"<p>I want to make some machine learning model that can be feed with a set of books and later return a story of its own. I have never worked with natural language models, so, could anyone guide me what kind of models to study? githubs or articles related are appreciated.</p>
<p>I don't want a markov approach because the lines would not have much coherence.</p>
",7,0,0,2,deep-learning;nlp,2022-05-31 17:50:34,2022-05-31 17:50:34,2022-05-31 17:50:34,i want to make some machine learning model that can be feed with a set of books and later return a story of its own  i have never worked with natural language models  so  could anyone guide me what kind of models to study  githubs or articles related are appreciated  i don t want a markov approach because the lines would not have much coherence 
418,418,13278114,72433640,Azure Machine Learning compute cluster - avoid using docker?,"<p>I would like to use an Azure Machine Learning Compute Cluster as a compute target but do not want it to containerize my project. Is there a way to deactivate this &quot;feature&quot; ?</p>
<p>The main reasons behind this request is that :</p>
<ol>
<li>I already set up a docker-compose file that is used to specify 3 containers for Apache Airflow and want to avoid a Docker-in-Docker situation. Especially that I already tried to do so but failed so far (here's the <a href=""https://stackoverflow.com/questions/72380590/airflow-docker-compose-from-another-docker-container-on-azure-machine-learning-c?noredirect=1#comment127881455_72380590"">link</a> my other related SO question).</li>
<li>I prefer not to use a Compute Instance as it is tied to an Azure account which is not ideal for automation purposes.</li>
</ol>
<p>Thanks in advance !</p>
",48,1,0,5,azure;docker;docker-compose;azure-machine-learning-studio;azure-machine-learning-service,2022-05-30 17:00:19,2022-05-30 17:00:19,2022-05-31 15:34:12,i would like to use an azure machine learning compute cluster as a compute target but do not want it to containerize my project  is there a way to deactivate this  feature    the main reasons behind this request is that   thanks in advance  
419,419,15798673,72443504,Flask - Edit Prefilled Table Value and Resubmit,"<h1>Overview</h1>
<p>I am currently building a Flask application where a user will a upload a book cover and the OCR machine learning model will attempt to read the title and author of the book from the cover. The results are then shown in a html table. If the model is incorrect I want the user to be able to edit the data in the table and send it back to the flask app to be stored in the database.</p>
<p>I have tried a few solution such as placeholders and other solutions provided from stack overflow but none seem to allow me edit the data and submit the updated table. below are the solutions I have tried</p>
<p><a href=""https://stackoverflow.com/questions/49126970/flask-get-and-edit-values"">Flask get and edit values</a>
<a href=""https://stackoverflow.com/questions/51803008/send-table-data-via-form-to-flask-method"">Send table data via form to Flask method</a></p>
<h2>HTML</h2>
<pre><code>&lt;link rel=&quot;stylesheet&quot; href=&quot;{{ url_for('static', filename = 'styles/styles.css') }}&quot;&gt;

&lt;h1&gt;Book Details&lt;/h1&gt;

&lt;body&gt;
&lt;img src=&quot;{{ image }}&quot; alt=&quot;User Image&quot; width=&quot;100%&quot; height=&quot;100%&quot; class=&quot;imagecenter&quot;&gt;
&lt;/body&gt;

&amp;nbsp
&lt;form action=&quot;{{ url_for('book_details') }}&quot; method=&quot;post&quot;&gt;

&lt;table id = &quot;bookDetails&quot;&gt;
   &lt;tr&gt;
      &lt;th&gt; Title &lt;/th&gt;
      &lt;td name=&quot;booktitle&quot; contenteditable='true'&gt;{{bookDetails['Title']}}&lt;/td&gt;
      &lt;input type=&quot;hidden&quot; name=&quot;booktitle&quot; type=&quot;text&quot; value=&quot;{{bookDetails['Title']}}&quot;td&gt;

   &lt;/tr&gt;
  &lt;/table&gt;
  &lt;p&gt;&lt;input type=&quot;submit&quot; value=&quot;Save to Database&quot; class=&quot;uploadButton&quot;&gt;&lt;/p&gt;
&lt;/form&gt;
</code></pre>
<h2>Python</h2>
<pre><code>@app.route('/bookDetails', methods=[&quot;POST&quot;, 'GET'])
def book_details():
    if request.method == &quot;POST&quot;:
        bookid = request.form[&quot;booktitle&quot;]
        print(bookid)
    return render_template('book_details.html', bookDetails=exampleBookDetails, image = 'static/why_nations_fail.jpg')
</code></pre>
<h2>Output</h2>
<p>I am either getting the same value as originally given or the name of the field</p>
<p>Why Nations Fail
or
booktitle</p>
<p>When it needs to be &quot;Why Nations Fail extra text&quot;.</p>
",26,0,0,5,python;html;flask;post;html-table,2022-05-31 12:36:28,2022-05-31 12:36:28,2022-05-31 13:37:11,i am currently building a flask application where a user will a upload a book cover and the ocr machine learning model will attempt to read the title and author of the book from the cover  the results are then shown in a html table  if the model is incorrect i want the user to be able to edit the data in the table and send it back to the flask app to be stored in the database  i have tried a few solution such as placeholders and other solutions provided from stack overflow but none seem to allow me edit the data and submit the updated table  below are the solutions i have tried i am either getting the same value as originally given or the name of the field when it needs to be  why nations fail extra text  
420,420,17979746,72442820,How to make a recomandation machine learning with ML.NET with a tag property,"<p>For my project I would like to integrate a recomendation system. my dataset looks like this.</p>
<pre><code>userId, projectId, projectCategory
1,1, API
1,5, Database
2,6, Arduino
</code></pre>
<p>Each user joins a project with a specific tag. I would like to recommend projects to my users based on the projects they join. Could I do machine learning based on the tags.</p>
",22,0,0,5,c#;machine-learning;artificial-intelligence;recommendation-engine;ml.net,2022-05-31 11:23:36,2022-05-31 11:23:36,2022-05-31 11:23:36,for my project i would like to integrate a recomendation system  my dataset looks like this  each user joins a project with a specific tag  i would like to recommend projects to my users based on the projects they join  could i do machine learning based on the tags 
421,421,17996648,72427900,How can I merge different data sets in R knowing that the variable that I use for matching the two data set are not unique?,"<p>I have two datasets, and I need to merge them by the ID value. The problems are:</p>
<ol>
<li>The ID value can be repeated across the same dataset (no other unique value is available).</li>
<li>The two datasets are not equal in the rows number or the column numbers.</li>
</ol>
<p>Example:</p>
<p>df1</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Gender</th>
</tr>
</thead>
<tbody>
<tr>
<td>99</td>
<td>Male</td>
</tr>
<tr>
<td>85</td>
<td>Female</td>
</tr>
<tr>
<td>7</td>
<td>Male</td>
</tr>
</tbody>
</table>
</div>
<p>df2</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ID</th>
<th style=""text-align: center;"">Body_Temperature</th>
<th style=""text-align: right;"">Body_Temperature_date_time</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">99</td>
<td style=""text-align: center;"">36</td>
<td style=""text-align: right;"">1/1/2020 12:00 am</td>
</tr>
<tr>
<td style=""text-align: left;"">99</td>
<td style=""text-align: center;"">38</td>
<td style=""text-align: right;"">2/1/2020 10:30 am</td>
</tr>
<tr>
<td style=""text-align: left;"">99</td>
<td style=""text-align: center;"">37</td>
<td style=""text-align: right;"">1/1/2020 06:41 am</td>
</tr>
<tr>
<td style=""text-align: left;"">52</td>
<td style=""text-align: center;"">38</td>
<td style=""text-align: right;"">1/2/2020 11:00 am</td>
</tr>
<tr>
<td style=""text-align: left;"">11</td>
<td style=""text-align: center;"">39</td>
<td style=""text-align: right;"">4/5/2020 09:09 pm</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td style=""text-align: center;"">35</td>
<td style=""text-align: right;"">9/8/2020 02:30 am</td>
</tr>
</tbody>
</table>
</div>
<p>How can I turn these two datasets into one single dataset in a way that allows me to apply some machine learning models on it later on?</p>
",63,1,-1,4,r;dataframe;join;dataset,2022-05-30 04:40:35,2022-05-30 04:40:35,2022-05-31 10:19:24,i have two datasets  and i need to merge them by the id value  the problems are  example  df df how can i turn these two datasets into one single dataset in a way that allows me to apply some machine learning models on it later on 
422,422,6316512,72442401,Unable to install mpi4py in MacBook M1,"<p>I am trying to install mpi4py in MacBook M1. I am learning MPI and I wanted to test a basic &quot;hello world&quot; python program on my machine. However, I am facing difficulty installing mpi4py. I have tried solutions from <a href=""https://stackoverflow.com/questions/39213142/installing-mpi4py"">this</a>, <a href=""https://stackoverflow.com/questions/46674330/error-installing-mpi4py"">this</a> and <a href=""https://stackoverflow.com/questions/28440834/error-when-installing-mpi4py"">this</a> answers. Below is the entire error message I get in the terminal:</p>
<pre><code>Collecting mpi4py   Using cached mpi4py-3.1.3.tar.gz (2.5 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done Building wheels for collected packages: mpi4py   Building wheel for mpi4py (pyproject.toml) ... error   error: subprocess-exited-with-error
     × Building wheel for mpi4py (pyproject.toml) did not run successfully.   │ exit code: 1   ╰─&gt; [156 lines of output]
      running bdist_wheel
      running build
      running build_src
      running build_py
      creating build
      creating build/lib.macosx-10.9-x86_64-cpython-39
      creating build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/run.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/bench.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/__main__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      creating build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/_base.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/server.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/_core.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/pool.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/aplus.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/__main__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/_lib.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      creating build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/util/pkl5.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/util/dtlib.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/util/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/py.typed -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/__main__.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/__init__.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/run.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/bench.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/MPI.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/dl.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/__init__.pxd -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/libmpi.pxd -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      copying src/mpi4py/MPI.pxd -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py
      creating build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include
      creating build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/include/mpi4py/mpi4py.MPI.h -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/include/mpi4py/mpi4py.MPI_api.h -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/include/mpi4py/mpi4py.h -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/include/mpi4py/mpi4py.i -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/include/mpi4py/mpi.pxi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/include/mpi4py
      copying src/mpi4py/futures/__main__.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/__init__.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/_core.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/aplus.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/server.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/pool.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/futures/_lib.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/futures
      copying src/mpi4py/util/__init__.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/util/dtlib.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      copying src/mpi4py/util/pkl5.pyi -&gt; build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/util
      running build_clib
      MPI configuration: [mpi] from 'mpi.cfg'
      MPI C compiler:    /Users/sindhuja/opt/anaconda3/bin/mpicc
      MPI C++ compiler:  /Users/sindhuja/opt/anaconda3/bin/mpicxx
      MPI F compiler:    /Users/sindhuja/opt/anaconda3/bin/mpifort
      MPI F90 compiler:  /Users/sindhuja/opt/anaconda3/bin/mpif90
      MPI F77 compiler:  /Users/sindhuja/opt/anaconda3/bin/mpif77
      checking for library 'lmpe' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      building 'mpe' dylib library
      creating build/temp.macosx-10.9-x86_64-cpython-39
      creating build/temp.macosx-10.9-x86_64-cpython-39/src
      creating build/temp.macosx-10.9-x86_64-cpython-39/src/lib-pmpi
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c src/lib-pmpi/mpe.c -o build/temp.macosx-10.9-x86_64-cpython-39/src/lib-pmpi/mpe.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      warning: build_clib: command '/Users/sindhuja/opt/anaconda3/bin/mpicc' failed with exit code 127
      
      warning: build_clib: building optional library &quot;mpe&quot; failed
      
      checking for library 'vt-mpi' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      checking for library 'vt.mpi' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      building 'vt' dylib library
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c src/lib-pmpi/vt.c -o build/temp.macosx-10.9-x86_64-cpython-39/src/lib-pmpi/vt.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      warning: build_clib: command '/Users/sindhuja/opt/anaconda3/bin/mpicc' failed with exit code 127
      
      warning: build_clib: building optional library &quot;vt&quot; failed
      
      checking for library 'vt-mpi' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      checking for library 'vt.mpi' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      building 'vt-mpi' dylib library
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c src/lib-pmpi/vt-mpi.c -o build/temp.macosx-10.9-x86_64-cpython-39/src/lib-pmpi/vt-mpi.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      warning: build_clib: command '/Users/sindhuja/opt/anaconda3/bin/mpicc' failed with exit code 127
      
      warning: build_clib: building optional library &quot;vt-mpi&quot; failed
      
      checking for library 'vt-hyb' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      checking for library 'vt.ompi' ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c _configtest.c -o
_configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      building 'vt-hyb' dylib library
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -c src/lib-pmpi/vt-hyb.c -o build/temp.macosx-10.9-x86_64-cpython-39/src/lib-pmpi/vt-hyb.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      warning: build_clib: command '/Users/sindhuja/opt/anaconda3/bin/mpicc' failed with exit code 127
      
      warning: build_clib: building optional library &quot;vt-hyb&quot; failed
      
      running build_ext
      MPI configuration: [mpi] from 'mpi.cfg'
      MPI C compiler:    /Users/sindhuja/opt/anaconda3/bin/mpicc
      MPI C++ compiler:  /Users/sindhuja/opt/anaconda3/bin/mpicxx
      MPI F compiler:    /Users/sindhuja/opt/anaconda3/bin/mpifort
      MPI F90 compiler:  /Users/sindhuja/opt/anaconda3/bin/mpif90
      MPI F77 compiler:  /Users/sindhuja/opt/anaconda3/bin/mpif77
      checking for dlopen() availability ...
      checking for header 'dlfcn.h' ...
      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include/python3.9 -c _configtest.c -o _configtest.o
      success!
      removing: _configtest.c _configtest.o
      success!
      checking for library 'dl' ...
      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include/python3.9 -c _configtest.c -o _configtest.o
      clang -flat_namespace -undefined suppress _configtest.o -Lbuild/temp.macosx-10.9-x86_64-cpython-39 -ldl -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest
      checking for function 'dlopen' ...
      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include/python3.9 -c _configtest.c -o _configtest.o
      clang _configtest.o -Lbuild/temp.macosx-10.9-x86_64-cpython-39 -ldl -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest
      building 'mpi4py.dl' extension
      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -DHAVE_DLFCN_H=1
-DHAVE_DLOPEN=1 -I/Users/sindhuja/opt/anaconda3/include/python3.9 -c src/dynload.c -o build/temp.macosx-10.9-x86_64-cpython-39/src/dynload.o
      clang -bundle -undefined dynamic_lookup -Wl,-rpath,/Users/sindhuja/opt/anaconda3/lib -L/Users/sindhuja/opt/anaconda3/lib -L/Users/sindhuja/opt/anaconda3/lib -Wl,-rpath,/Users/sindhuja/opt/anaconda3/lib -L/Users/sindhuja/opt/anaconda3/lib build/temp.macosx-10.9-x86_64-cpython-39/src/dynload.o
-Lbuild/temp.macosx-10.9-x86_64-cpython-39 -ldl -o build/lib.macosx-10.9-x86_64-cpython-39/mpi4py/dl.cpython-39-darwin.so
      checking for MPI compile and link ...
      /Users/sindhuja/opt/anaconda3/bin/mpicc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64 -I/Users/sindhuja/opt/anaconda3/include -fPIC -O2 -isystem /Users/sindhuja/opt/anaconda3/include -arch x86_64
-I/Users/sindhuja/opt/anaconda3/include/python3.9 -c _configtest.c -o _configtest.o
      /Users/sindhuja/opt/anaconda3/bin/mpicc: line 301: x86_64-apple-darwin13.4.0-clang: command not found
      failure.
      removing: _configtest.c _configtest.o
      error: Cannot compile MPI programs. Check your configuration!!!
      [end of output]
     note: This error originates from a subprocess, and is likely not a problem with pip.   ERROR: Failed building wheel for mpi4py Failed to build mpi4py ERROR: Could not build wheels for mpi4py, which is required to install pyproject.toml-based projects
</code></pre>
<p>I have downloaded wheel and upgraded pip, but haven't had any luck. Could anyone please tell me where I may be going wrong? Thanks in advance!</p>
",105,0,0,5,python-3.x;installation;mpi;openmpi;mpi4py,2022-05-31 10:18:19,2022-05-31 10:18:19,2022-05-31 10:18:19,i am trying to install mpipy in macbook m  i am learning mpi and i wanted to test a basic  hello world  python program on my machine  however  i am facing difficulty installing mpipy  i have tried solutions from    and  answers  below is the entire error message i get in the terminal  i have downloaded wheel and upgraded pip  but haven t had any luck  could anyone please tell me where i may be going wrong  thanks in advance 
423,423,14095457,72442100,"In machine learning, what is meant by &quot;common features&quot; or &quot;uncommon features&quot;?","<p>My understanding of the term &quot;feature&quot; is that it refers to a given piece of information used as input for a machine learning algorithm. E.g., for a neural network, each input neuron corresponds to one feature.</p>
<p>I have recently read that the AdaGrad algorithm for gradient descent results in a high learning rate for uncommon features and a low learning rate for common ones. But every feature is always there; it just may have a different value, no? So what does it mean for a feature to be uncommon? It is usually zero? It is usually a constant value and doesn't deviate much? Or something else?</p>
",26,0,-1,3,machine-learning;neural-network;gradient-descent,2022-05-31 09:25:25,2022-05-31 09:25:25,2022-05-31 09:25:25,my understanding of the term  feature  is that it refers to a given piece of information used as input for a machine learning algorithm  e g   for a neural network  each input neuron corresponds to one feature  i have recently read that the adagrad algorithm for gradient descent results in a high learning rate for uncommon features and a low learning rate for common ones  but every feature is always there  it just may have a different value  no  so what does it mean for a feature to be uncommon  it is usually zero  it is usually a constant value and doesn t deviate much  or something else 
424,424,7321233,41244421,Linear vs nonlinear neural network?,"<p>I'm new to machine learning and neural networks. I know how to build a nonlinear classification model, but my current problem has a continuous output. I've been searching for information on neural network regression, but all I encounter is information on <strong>linear</strong> regression - nothing about <strong>nonlinear</strong> cases. Which is odd, because why would someone use neural networks to solve a simple linear regression anyway? Isn't that like killing a fly with a nuclear bomb?</p>

<p>So my question is this: what makes a neural network nonlinear? (Hidden layers? Nonlinear activation function?) Or do I have a completely wrong understanding of the word ""linear"" - can a linear regression NN accurately model datasets that are more complex than y=aX+b? Is the word ""linear"" used just as the opposite of ""logistic""?</p>

<p>(I'm planning to use TensorFlow, but the TensorFlow Linear Model Tutorial uses a binary classification problem as an example, so that doesn't help me either.)</p>
",49186,9,34,2,neural-network;tensorflow,2016-12-20 19:47:17,2016-12-20 19:47:17,2022-05-31 07:42:45,i m new to machine learning and neural networks  i know how to build a nonlinear classification model  but my current problem has a continuous output  i ve been searching for information on neural network regression  but all i encounter is information on linear regression   nothing about nonlinear cases  which is odd  because why would someone use neural networks to solve a simple linear regression anyway  isn t that like killing a fly with a nuclear bomb  so my question is this  what makes a neural network nonlinear   hidden layers  nonlinear activation function   or do i have a completely wrong understanding of the word linear   can a linear regression nn accurately model datasets that are more complex than y ax b  is the word linear used just as the opposite of logistic   i m planning to use tensorflow  but the tensorflow linear model tutorial uses a binary classification problem as an example  so that doesn t help me either  
425,425,14808637,66954494,OSError: [Errno 9] Bad file descriptor in tensorflow Estimater When deploying model on Multiple GPUs using tensorflow mirror strategy,"<p>I am trying to deploying deep learning model on two GPUs with single machine. I am utilizing TensorFlow  mirror strategy. I am getting the following error:</p>
<p><strong>Traceback (most recent call last):</strong></p>
<p><a href=""https://i.stack.imgur.com/UI3YQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UI3YQ.png"" alt=""enter image description here"" /></a></p>
<p><strong>Code</strong></p>
<pre><code> from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import tensorflow as tf
import os
import json
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
tf.logging.set_verbosity(tf.logging.INFO)
from tensorflow.keras.datasets import mnist



def cnn_model_fn(features, labels, mode):
  
    input_layer = tf.reshape(features[&quot;x&quot;], [-1, 28, 28, 1])
    input_layer = tf.cast(input_layer, tf.float32)
    labels = tf.cast(labels, tf.int32)
   
    conv1 = tf.layers.conv2d(
        inputs=input_layer,
        filters=32,
        kernel_size=[5, 5],
        padding=&quot;same&quot;,
        activation=tf.nn.relu)


    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

    conv2 = tf.layers.conv2d(
        inputs=pool1,
        filters=64,
        kernel_size=[5, 5],
        padding=&quot;same&quot;,
        activation=tf.nn.relu)

    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])

    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
    dropout = tf.layers.dropout(
        inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

    logits = tf.layers.dense(inputs=dropout, units=10)

    predictions = {
        # Generate predictions (for PREDICT and EVAL mode)
        &quot;classes&quot;: tf.argmax(input=logits, axis=1),
        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
        # `logging_hook`.
        &quot;probabilities&quot;: tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)
    }
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    # Calculate Loss (for both TRAIN and EVAL modes)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    # Add evaluation metrics (for EVAL mode)
    eval_metric_ops = {
        &quot;accuracy&quot;: tf.metrics.accuracy(
            labels=labels, predictions=predictions[&quot;classes&quot;])}
    return tf.estimator.EstimatorSpec(
        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)


def per_device_batch_size(batch_size, num_gpus):
    if num_gpus &lt;= 1:
        return batch_size

    remainder = batch_size % num_gpus
    if remainder:
        err = ('When running with multiple GPUs, batch size '
               'must be a multiple of the number of available GPUs. Found {} '
               'GPUs with a batch size of {}; try --batch_size={} instead.'
               ).format(num_gpus, batch_size, batch_size - remainder)
        raise ValueError(err)
    return int(batch_size / num_gpus)


class InputFnProvider:
    def __init__(self, train_batch_size):
        self.train_batch_size = train_batch_size
        self.__load_data()

    def __load_data(self):
        # Load training and eval data

        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
        self.train_data = X_train # Returns np.array
        self.train_labels = Y_train
        self.eval_data = X_test  # Returns np.array
        self.eval_labels = Y_test

    def train_input_fn(self):
        dataset = tf.data.Dataset.from_tensor_slices(({&quot;x&quot;: self.train_data}, self.train_labels))
        dataset = dataset.shuffle(1000).repeat().batch(self.train_batch_size)
        return dataset

    def eval_input_fn(self):
        &quot;&quot;&quot;An input function for evaluation or prediction&quot;&quot;&quot;
        dataset = tf.data.Dataset.from_tensor_slices(({&quot;x&quot;: self.eval_data}, self.eval_labels))
        dataset = dataset.batch(1)
        return dataset


def main(unused_argv):
    batch_size = 100
    num_gpus = 2

    input_fn_provider = InputFnProvider(per_device_batch_size(batch_size, num_gpus))


    if num_gpus &gt; 1:
        distribution = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;],
                                          cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
    else:
        distribution = None
    # Pass to RunConfig
    config = tf.estimator.RunConfig(
        train_distribute=distribution,
        model_dir=&quot;/tmp/mnist_convnet_model&quot;)

    mnist_classifier = tf.estimator.Estimator(
        model_fn=cnn_model_fn,
        config=config)

    # Train the model
    mnist_classifier.train(
        input_fn=input_fn_provider.train_input_fn,
        steps=1000)

    eval_results = mnist_classifier.evaluate(input_fn=input_fn_provider.eval_input_fn)
    print(eval_results)


if __name__ == &quot;__main__&quot;:
    tf.app.run()
</code></pre>
<p>Surprisingly when I deploy the model on a single GPU using the same code, it works; however, when I try to deploy the model on two GPUs via changing a bit in code, I face the above-given error. I do not know about this error. Can anyone help?</p>
",473,0,0,5,python-3.x;tensorflow;conv-neural-network;tensorflow-estimator;multi-gpu,2021-04-05 19:49:21,2021-04-05 19:49:21,2022-05-31 07:08:33,i am trying to deploying deep learning model on two gpus with single machine  i am utilizing tensorflow  mirror strategy  i am getting the following error  traceback  most recent call last    code surprisingly when i deploy the model on a single gpu using the same code  it works  however  when i try to deploy the model on two gpus via changing a bit in code  i face the above given error  i do not know about this error  can anyone help 
426,426,17178067,69609905,Can&#39;t install numpy on python 3.10,"<p>I'm quite new to machine learning and when I tried to install numpy and this happended Can you guys help me fix this. I'm using python 3.10.0</p>
<p>ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects</p>
",3978,3,2,4,python;version;python-wheel;python-3.10,2021-10-18 07:07:28,2021-10-18 07:07:28,2022-05-31 06:33:05,i m quite new to machine learning and when i tried to install numpy and this happended can you guys help me fix this  i m using python   
427,427,19222695,72440938,Deep Learning Coordinated Beamforming with DeepMIMO,"<p>I'm trying to get figures published in a paper. I have codes about how to do it but it works for only 1 figure but i want to get other figures as well.</p>
<p>Code simply generates data based on a scenario. Then, they developed an integrated machine
learning and coordinated beamforming strategy that enables highly-mobile applications in large antenna array mmWave systems. The key idea of the developed strategy is to leverage a deep learning model that learns the mapping from omni-received uplink pilots and the beam training result.</p>
<p>The code below gives only 1 <a href=""https://ibb.co/qCWgh5B"" rel=""nofollow noreferrer"">figure</a> which is deep learning data size vs Achievable rate(bps/Hz).</p>
<p>When I try to get <a href=""https://ibb.co/1n9dFsP"" rel=""nofollow noreferrer"">figure 9</a>, number of beams vs Effective Achievable rate(bps/Hz), in <a href=""https://ibb.co/1KHgck8"" rel=""nofollow noreferrer"">my figure 9</a>, values in the figure are constant. However, it should not be constant.</p>
<p>In the figure 9, The deep-learning model was trained with a LOS(Line of  Sight) dataset of size 20k samples.</p>
<p>How can i obtain figure 9 correctly? Any hint to get figure 9 would be appreciated.</p>
<pre><code>import DeepMIMO
import os
import numpy as np
from pprint import pprint
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
from tqdm import tqdm
from scipy.io import loadmat, savemat 
import glob
import re 

plt.rcParams['figure.figsize'] = [12, 8] # Set default plot size


def beamforming_codebook(ant_shape = np.array([1, 32, 1]), oversampling_rate = np.array([1, 1, 1]), kd = 0.5):
    
    kd = 2 * np.pi * kd
    codebook_size = ant_shape * oversampling_rate
    
    vecs = []
    for dim in range(3):
        ind = np.arange(ant_shape[dim]).reshape((-1, 1))
        codebook_ang = np.linspace(0, np.pi, codebook_size[dim], endpoint = False).reshape((1, -1))                                                                                                     
        vec = np.sqrt(1./ant_shape[dim]) * np.exp(-1j * kd * ind * np.cos(codebook_ang))
        vecs.append(vec)
        
    F = np.kron(vecs[2], np.kron(vecs[1], vecs[0]))
    
    return F
    
   
#%% # Generate the dataset
# # Load and print the default parameters
parameters = DeepMIMO.default_params()


# # Change parameters for the setup
# Scenario O1_60 extracted at the dataset_folder
parameters['scenario'] = 'O1_60'
parameters['dataset_folder'] = r'C:\Users\Baran\Desktop\DEEPMIMO\Applications\Coordinated Beamforming\scenarios' # Set DeepMIMO dataset folder that has O1_60

parameters['num_paths'] = 5

# User rows 1-100
parameters['user_row_first'] = 1000
parameters['user_row_last'] = 1300

# Activate only the first basestation
parameters['active_BS'] = np.array([3, 4, 5, 6]) 

parameters['OFDM']['bandwidth'] = 0.5 # 50 MHz
parameters['OFDM']['subcarriers'] = 1024 # OFDM with 512 subcarriers
parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers

parameters['enable_BS2BS'] = False

parameters['ue_antenna']['shape'] = np.array([1, 1, 1]) # Single antenna
parameters['bs_antenna']['shape'] = np.array([1, 32, 8]) # ULA of 32 elements
parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'
parameters['ue_antenna']['radiation_pattern'] = 'halfwave-dipole'
pprint(parameters, sort_dicts = False)

dataset = DeepMIMO.generate_data(parameters)

#%% Parameters and Codebook
F = beamforming_codebook(ant_shape = parameters['bs_antenna'][0]['shape'], oversampling_rate = np.array([1, 2, 1]), kd = parameters['bs_antenna'][0]['spacing'])

num_OFDM = int(parameters['OFDM']['subcarriers_limit']/parameters['OFDM']['subcarriers_sampling'])
num_beams = F.shape[1]
num_bs = len(parameters['active_BS'])
num_ue = len(parameters['active_UE'])

NF = 5             # Noise figure at the base station
Process_Gain = 10  # Channel estimation processing gain
BW = parameters['OFDM']['bandwidth'] * 1e9 # System bandwidth in Hz
noise_power_dB = -204 + 10*np.log10(BW/parameters['OFDM']['subcarriers']) + NF - Process_Gain; # Noise power in dB
noise_power = 10**(.1*(noise_power_dB)); # Noise power

#%% DL Input-Output
input_norm = np.zeros((num_bs, num_ue, num_OFDM), dtype=complex)
max_rates = np.zeros((num_bs, num_ue, num_beams))
for bs_idx in tqdm(range(num_bs), desc='Neural Network Input-Output Generation-BS', position=0, leave=True):
    for ue_idx in tqdm(range(num_ue), desc='Neural Network Input-Output Generation-BS-%i'%bs_idx, position=0, leave=True):
        ch = dataset[bs_idx]['user']['channel'][ue_idx].squeeze()
        ch = ch + np.sqrt(noise_power) * (np.random.randn(*(ch.shape)) + 1j * np.random.randn(*(ch.shape)))
        input_norm[bs_idx, ue_idx, :] = ch[0, :]
        max_rates[bs_idx, ue_idx, :] = np.sum(np.log2(1 + np.abs(ch.T.conj() @ F)**2),  axis = 0)/num_OFDM
        
# Input reshape - normalize
input_norm = np.transpose(input_norm, axes=[1, 0, 2])
input_norm = input_norm.reshape((num_ue, -1))
input_norm /=  np.amax(np.abs(input_norm))

# Output reshape - normalize
max_rates_norm_factor = np.amax(max_rates, axis=2, keepdims=True)
max_rates_norm_factor[max_rates_norm_factor== 0] = 1 # Do not normalize if all zeros
max_rates /= max_rates_norm_factor
max_rates = np.transpose(max_rates, axes=[1, 0, 2])
max_rates = max_rates.reshape((num_ue, -1))

if not os.path.exists('./DLCB_dataset'):
                      os.makedirs('DLCB_dataset')
savemat('./DLCB_dataset/DLCB_input.mat', {'DL_input': input_norm})
savemat('./DLCB_dataset/DLCB_output.mat', {'DL_output': max_rates})

# %% Machine Learning
import os
import keras
import keras.models as models
from keras.layers.core import Dense,Dropout
import numpy as np

# Model training function
def train(In_train, Out_train, In_test, Out_test,
          epochs, batch_size,dr,
          num_hidden_layers, nodes_per_layer,
          loss_fn,n_BS,n_beams):
    
    in_shp = list(In_train.shape[1:])

    AP_models = []
    for bs_idx in range(n_BS):
        idx_str = 'BS%i' % bs_idx
        idx = bs_idx*n_beams
        
        model = models.Sequential()
        model.add(Dense(nodes_per_layer, activation='relu', kernel_initializer='he_normal', input_shape=in_shp))
        model.add(Dropout(dr))
        for h in range(num_hidden_layers):
            model.add(Dense(nodes_per_layer, activation='relu', kernel_initializer='he_normal'))
            model.add(Dropout(dr))
        
        model.add(Dense(n_beams, activation='relu', kernel_initializer='he_normal',
                  name=&quot;dense&quot; + idx_str + &quot;o&quot;))
        model.compile(loss=loss_fn, optimizer='adam')
        model.summary()
    
        model.fit(In_train,
                    Out_train[:, idx:idx + n_beams],
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=2,
                    validation_data=(In_test, Out_test[:,idx:idx + n_beams]),
                    callbacks = [
                        #keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),
                        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')
                    ])
        
        AP_models.append(model)
        
        
    return AP_models

# Reading input and output sets generated from MATLAB
In_set_file=loadmat('DLCB_dataset/DLCB_input.mat')
Out_set_file=loadmat('DLCB_dataset/DLCB_output.mat')

In_set=In_set_file['DL_input']
Out_set=Out_set_file['DL_output']

# Parameter initialization
num_user_tot=In_set.shape[0]
n_DL_size=[0.001, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7]
count=0
num_tot_TX=4
num_beams=512

for DL_size_ratio in n_DL_size:
    
    print (DL_size_ratio)
    count=count+1
    DL_size=int(num_user_tot*DL_size_ratio)
    
    np.random.seed(2016)
    n_examples = DL_size
    num_train  = int(DL_size * 0.8)
    num_test   = int(num_user_tot*.2)
    
    train_index = np.random.choice(range(0,num_user_tot), size=num_train, replace=False)
    rem_index = set(range(0,num_user_tot))-set(train_index)
    test_index= list(set(np.random.choice(list(rem_index), size=num_test, replace=False)))
    
    In_train = In_set[train_index]
    In_test =  In_set[test_index] 
        
    Out_train = Out_set[train_index]
    Out_test = Out_set[test_index]
    
    
    # Learning model parameters
    epochs = 10     
    batch_size = 100  
    dr = 0.05                  # dropout rate  
    num_hidden_layers=4
    nodes_per_layer=In_train.shape[1]
    loss_fn='mean_squared_error'
    
    # Model training
    AP_models = train(In_train, Out_train, In_test, Out_test,
                                          epochs, batch_size,dr,
                                          num_hidden_layers, nodes_per_layer,
                                          loss_fn,num_tot_TX,num_beams)

    
    # Model running/testing
    DL_Result={}
    for idx in range(0,num_tot_TX,1): 
        beams_predicted=AP_models[idx].predict( In_test, batch_size=10, verbose=0)
    
        DL_Result['TX'+str(idx+1)+'Pred_Beams']=beams_predicted
        DL_Result['TX'+str(idx+1)+'Opt_Beams']=Out_test[:,idx*num_beams:(idx+1)*num_beams]

    DL_Result['user_index']=test_index
    
    
    if not os.path.exists('./DLCB_code_output'):
                          os.makedirs('DLCB_code_output')
    savemat('DLCB_code_output/DL_Result'+str(count)+'.mat',DL_Result)

#%% Read Results
file_list = sorted(glob.glob('DLCB_code_output/DL_Result*'), key=lambda x: int(re.findall(r'\d+', x)[0]))
num_files = len(file_list)

user_index = []
pred_beams = []
opt_beams = []
for file in tqdm(file_list, desc='Reading DL results'):
    matfile = loadmat(file)
    l1 = []
    l2 = []
    for idx in range(num_bs):
        l1.append(matfile['TX'+str(idx+1)+'Pred_Beams'])
        l2.append(matfile['TX'+str(idx+1)+'Opt_Beams'])
        
    pred_beams.append(l1)
    opt_beams.append(l2)
    user_index.append(matfile['user_index'])


Pn = -204 + 10*np.log10(BW) # Noise power in dB
SNR = 10**(.1*(0-Pn))

ach_rate_DL = np.zeros(num_files)
ach_rate_opt = np.zeros(num_files)

eff_rate = np.zeros(num_files)
opt_rate = np.zeros(num_files)
for file_idx in tqdm(np.arange(num_files), desc = 'Calculating results'):
    user_index_file = user_index[file_idx].flatten()
    for ue_idx in range(len(user_index_file)):
        eff_ch = []
        opt_ch = []
        for bs_idx in range(num_bs):
            if file_idx == 0: # Random BF - 0 Samples
                pred_beam_idx = np.random.randint(num_beams)
            else:
                pred_beam_idx = np.argmax(pred_beams[file_idx][bs_idx][ue_idx])
            opt_beam_idx = np.argmax(opt_beams[file_idx][bs_idx][ue_idx])
            ch_single_bs = dataset[bs_idx]['user']['channel'][user_index_file[ue_idx]].squeeze()
            eff_ch_single_pred = ch_single_bs.T.conj() @ F[:, pred_beam_idx]
            opt_ch_single_pred = ch_single_bs.T.conj() @ F[:, opt_beam_idx]
            eff_ch.append(eff_ch_single_pred)
            opt_ch.append(opt_ch_single_pred)
        eff_ch = np.array(eff_ch)
        opt_ch = np.array(opt_ch)
        eff_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(eff_ch.conj().T @ eff_ch))))
        opt_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(opt_ch.conj().T @ opt_ch))))
    eff_rate[file_idx] /= len(user_index_file)*num_OFDM
    opt_rate[file_idx] /= len(user_index_file)*num_OFDM


# % Eff achievable rate calculations
theta_user=(102/parameters['bs_antenna'][0]['shape'][1])*np.pi/180
alpha=60*np.pi/180
distance_user=10
Tc_const=(distance_user*theta_user)/(2*np.sin(alpha)) # ms
Tt=10*1e-6; # ms

v_mph=50
v=v_mph*1000*1.6/3600 # m/s
Tc=Tc_const/v

overhead_opt=1-(num_beams*Tt)/Tc # overhead of beam training
overhead_DL=1-Tt/Tc # overhead of proposed DL method


#%% Plotting the figure
DL_size_array=np.arange(0, 2.5*(num_files), 2.5);

fig, ax = plt.subplots()
plt.plot(DL_size_array, opt_rate, '--k', label = 'Genie-aided Coordinated Beamforming')
plt.plot(DL_size_array, eff_rate*overhead_DL, '-bo', label = 'Deep Learning Coordinated Beamforming')
plt.plot(DL_size_array, opt_rate*overhead_opt, '-rs', label = 'Baseline Coordinated Beamforming')
plt.ylim([0, 6])
plt.minorticks_on()
plt.grid()
plt.xlabel('Deep Learning Dataset Size (Thousand Samples)')
plt.ylabel('Achievable Rate (bps/Hz)')
plt.legend()
plt.savefig('result.png')
</code></pre>
",15,0,0,3,keras;deep-learning;tf.keras,2022-05-31 05:13:21,2022-05-31 05:13:21,2022-05-31 05:13:21,i m trying to get figures published in a paper  i have codes about how to do it but it works for only  figure but i want to get other figures as well  the code below gives only   which is deep learning data size vs achievable rate bps hz   when i try to get   number of beams vs effective achievable rate bps hz   in   values in the figure are constant  however  it should not be constant  in the figure   the deep learning model was trained with a los line of  sight  dataset of size k samples  how can i obtain figure  correctly  any hint to get figure  would be appreciated 
428,428,16568677,72440428,Value too large for float32,"<p>I am trying to deploy predictive models on big data. The models i have tried to use is random forest and decision tree, but to no avail. Every time I am getting the same error of too large for dtype float32. So, just want to know that Is there any machine learning predictive models that can handle dtype float64?? Please help. Thanks.</p>
",36,1,-2,3,python;random-forest;decision-tree,2022-05-31 03:32:19,2022-05-31 03:32:19,2022-05-31 04:22:01,i am trying to deploy predictive models on big data  the models i have tried to use is random forest and decision tree  but to no avail  every time i am getting the same error of too large for dtype float  so  just want to know that is there any machine learning predictive models that can handle dtype float   please help  thanks 
429,429,13085128,61703398,&#39;Net&#39; object has no attribute &#39;parameters&#39;,"<p>I am fairly new to machine learning. I learned to write this code from youtube tutorials but I keep getting this error</p>

<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/aniket/Desktop/DeepLearning/PythonLearningPyCharm/CatVsDogs.py"", line 109, in &lt;module&gt;
    optimizer = optim.Adam(net.parameters(), lr=0.001) # tweaks the weights from what I understand
AttributeError: 'Net' object has no attribute 'parameters'
</code></pre>

<p>this is the Net class</p>

<pre class=""lang-py prettyprint-override""><code>class Net():
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1,32,5)
        self.conv2 = nn.Conv2d(32,64,5)
        self.conv3 = nn.Conv2d(64,128,5)
        self.to_linear = None
        x = torch.randn(50,50).view(-1,1,50,50)
        self.Conv2d_Linear_Link(x)
        self.fc1 = nn.Linear(self.to_linear, 512)
        self.fc2 = nn.Linear(512, 2)

    def Conv2d_Linear_Link(self , x):
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))
        x = F.max_pool2d(F.relu(self.conv3(x)),(2,2))

        if self.to_linear is None :
            self.to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]
        return x

    def forward(self, x):
        x = self.Conv2d_Linear_Link(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)
</code></pre>

<p>and this is the function train</p>

<pre class=""lang-py prettyprint-override""><code>def train():
    for epoch in range(epochs):
        for i in tqdm(range(0,len(X_train), batch)):
            batch_x = train_X[i:i + batch].view(-1, 1, 50, 50)
            batch_y = train_y[i:i + batch]
            net.zero_grad() # i don't understand why we do this but we do we don't want the probabilites adding up
            output = net(batch_x)
            loss = loss_function(output, batch_y)
            loss.backward()
            optimizer.step()
        print(loss)
</code></pre>

<p>and the optimizer and loss functions and data</p>

<pre class=""lang-py prettyprint-override""><code>optimizer = optim.Adam(net.parameters(), lr=0.001) # tweaks the weights from what I understand
loss_function = nn.MSELoss() # gives the loss
</code></pre>
",6230,3,1,4,deep-learning;neural-network;pytorch;conv-neural-network,2020-05-10 02:06:40,2020-05-10 02:06:40,2022-05-31 02:58:56,i am fairly new to machine learning  i learned to write this code from youtube tutorials but i keep getting this error this is the net class and this is the function train and the optimizer and loss functions and data
430,430,19234264,72438644,"TensorFlow ValueError: Cannot feed value of shape (720, 1280, 3) for Tensor DecodeJpeg/contents:0, which has shape ()","<p>I am new to TensorFlow and machine learning. I have problems combining two programs. One uses images and the other uses files. Is there any way to convert one to the other and solve the problem?</p>
<pre><code>cap = cv2.VideoCapture(0)

label_lines = [line.rstrip() for line
                   in tf.io.gfile.GFile(&quot;logs/trained_labels.txt&quot;)]

with tf.io.gfile.GFile(&quot;logs/trained_graph.pb&quot;, 'rb') as f:
    graph_def = tf.compat.v1.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def, name='')

while True:
    _, frame = cap.read()

    x, y, c = frame.shape

    frame = cv2.flip(frame, 1)
    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    with tf.compat.v1.Session() as sess:
        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')

        predictions = sess.run(softmax_tensor, \
                {'DecodeJpeg/contents:0': framergb})

    classID = np.argmax(predictions)
    className = label_lines[classID]

    cv2.putText(frame, className, (550, 320), cv2.FONT_HERSHEY_SIMPLEX,
                   1, (230,230,250), 2, cv2.LINE_AA)

    cv2.imshow(&quot;Selina&quot;, frame)

    if cv2.waitKey(1) == ord('q'):
        break
</code></pre>
<p>This is the error message:</p>
<pre><code>ValueError: Cannot feed value of shape (720, 1280, 3) for Tensor 
DecodeJpeg/contents:0, which has shape ()
</code></pre>
<p>I have already tried running a 720*1280 file with the original program and it works, so it must be something wrong with my code.</p>
",21,0,0,2,python;tensorflow,2022-05-30 23:57:43,2022-05-30 23:57:43,2022-05-30 23:57:43,i am new to tensorflow and machine learning  i have problems combining two programs  one uses images and the other uses files  is there any way to convert one to the other and solve the problem  this is the error message  i have already tried running a   file with the original program and it works  so it must be something wrong with my code 
431,431,5649301,72436662,Machine Learning using Keras &amp; TensorFlow (Input 2d array deapleanr to predict 2 Length array),"<p><strong>Problem statement</strong></p>
<p>I have a image converted into grayscale and converted into a 2D array of diminution 12x100 values range between 0-255 below is an example what I need to achieve</p>
<p>Input data:</p>
<pre class=""lang-json prettyprint-override""><code>[
  [124,224,211,91,173,25,230,105,221,143,246,6],
  [54,131,204,201,91,102,11,207,53,38,86,114],
  [170,72,190,167,28,121,165,50,27,76,186,41]
  ...100
]
</code></pre>
<p>Expecting Prediction result:</p>
<pre class=""lang-json prettyprint-override""><code>[0,0]
</code></pre>
<p>Prediction in a combination of 0,0|0,1|1,0|1,1</p>
<blockquote>
<p>Before start training i always scale the array by dividing each value by 255 so the input data range become 0-1</p>
</blockquote>
<p>I need a improvization in the code because I have to transform the 2D array to 1D array which is as array of 1200 length</p>
<p>I want a way to pass the 2D array in Tenser and train the model</p>
<p>Any small sample to doing this will help a lot, I am new to python so please anyone trying to help with full runnable code, please pardon my expectations.</p>
<p>Thank You.</p>
",19,0,0,5,python;arrays;numpy;tensorflow;keras,2022-05-30 20:54:52,2022-05-30 20:54:52,2022-05-30 20:54:52,problem statement i have a image converted into grayscale and converted into a d array of diminution x values range between   below is an example what i need to achieve input data  expecting prediction result  prediction in a combination of         before start training i always scale the array by dividing each value by  so the input data range become   i need a improvization in the code because i have to transform the d array to d array which is as array of  length i want a way to pass the d array in tenser and train the model any small sample to doing this will help a lot  i am new to python so please anyone trying to help with full runnable code  please pardon my expectations  thank you 
432,432,7791553,43116825,Dagger 2: Cannot be provided without an @Provides-annotated method,"<p>I just started learning dagger2 and faced a strange issue that looks like a bug to me. Here's the module:</p>
<pre><code>@Module
public class SimpleModule {

    @Provides
    Cooker providerCooker() {

        return new Cooker(&quot;tom&quot;, &quot;natie&quot;);
    }
}
</code></pre>
<p>Component:</p>
<pre><code>@Component(modules = SimpleModule.class)
public interface SimpleComponent {

    void inject(DaggerTestActivity activity);

}
</code></pre>
<p>Interface:</p>
<pre><code>public interface CoffeeMaker {

    String makeCoffee();
}
</code></pre>
<p>Implementation:</p>
<pre><code>  public class SimpleMaker implements CoffeeMaker {
    
        Cooker mCooker;
      
        @Inject
        public SimpleMaker(Cooker cooker) {
    
            this.mCooker = cooker;
    
        }
    
        @Override
        public String makeCoffee() {
    
            return mCooker.makeCoffee();
        }
    }
</code></pre>
<p>Cooker :</p>
<pre><code>public class Cooker {

    String name; 
    String coffeeKind;


    public Cooker(String name, String coffeeKind) {
        this.name = name;
        this.coffeeKind = coffeeKind;
    }
    
   public  String  makeCoffee() {
  
        return name + &quot;make&quot; + coffeeKind; 
    }

}
</code></pre>
<p>Coffee machine:</p>
<pre><code>public class CoffeeMachine {

    CoffeeMaker mMaker;

    @Inject
    public CoffeeMachine(CoffeeMaker coffeeMaker) {

        this.mMaker = coffeeMaker;
    }

    public String makeCoffee() {

        return mMaker.makeCoffee();
    }
}
</code></pre>
<p>Just it. I use it in the activity. Faced strange issue here:</p>
<pre><code>    @Inject
    CoffeeMachine mCoffeeMachine;
</code></pre>
<p>The error I'm getting from the Dagger 2 compiler is the following:</p>
<pre><code>Error:(14, 10) com.wyyc.daggertest.CoffeeMaker cannot be provided without an @Provides-annotated method.
com.wyyc.zqqworkproject.DaggerTestActivity.mCoffeeMachine
[injected field of type: com.wyyc.daggertest.CoffeeMachine mCoffeeMachine]
com.wyyc.daggertest.CoffeeMachine.&lt;init&gt;(com.wyyc.daggertest.CoffeeMaker coffeeMaker)
</code></pre>
<p>All this situation looks very strange, and I'd like to hear some input from more experienced Dagger 2 users.</p>
",61509,2,36,5,java;android;dependency-injection;dagger-2;dagger,2017-03-30 17:08:52,2017-03-30 17:08:52,2022-05-30 20:31:48,i just started learning dagger and faced a strange issue that looks like a bug to me  here s the module  component  interface  implementation  cooker   coffee machine  just it  i use it in the activity  faced strange issue here  the error i m getting from the dagger  compiler is the following  all this situation looks very strange  and i d like to hear some input from more experienced dagger  users 
433,433,12242625,71068392,"Group and create three new columns by condition [Low, Hit, High]","<p>I have a large dataset (~5 Mio rows) with results from a Machine Learning training. Now I want to check to see if the results hit the &quot;target range&quot; or not. Lets say this range contains all values between <code>-0.25</code> and <code>+0.25</code>. If it's inside this range, it's a <code>Hit</code>, if it's below <code>Low</code> and on the other side <code>High</code>.</p>
<p>I now would create this three columns Hit, Low, High and calculate for each row which condition applies and put a <code>1</code> into this col, the other two would become <code>0</code>. After that I would group the values and sum them up. But I suspect there must be a better and faster way, such as calculate it directly while grouping.</p>
<hr />
<h2>Data</h2>
<pre><code>import pandas as pd

df = pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;RF&quot;, &quot;RF&quot;, &quot;MLP&quot;, &quot;MLP&quot;, &quot;MLP&quot;], &quot;Value&quot;:[-1.5,-0.1,1.7,0.2,-0.7,-0.6]})

+----+--------+---------+
|    | Type   |   Value |
|----+--------+---------|
|  0 | RF     |    -1.5 | &lt;- Low
|  1 | RF     |    -0.1 | &lt;- Hit
|  2 | RF     |     1.7 | &lt;- High
|  3 | MLP    |     0.2 | &lt;- Hit
|  4 | MLP    |    -0.7 | &lt;- Low
|  5 | MLP    |    -0.6 | &lt;- Low
+----+--------+---------+
</code></pre>
<hr />
<h2>Expected Output</h2>
<pre><code>pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;MLP&quot;], &quot;Low&quot;:[1,2], &quot;Hit&quot;:[1,1], &quot;High&quot;:[1,0]})

+----+--------+-------+-------+--------+
|    | Type   |   Low |   Hit |   High |
|----+--------+-------+-------+--------|
|  0 | RF     |     1 |     1 |      1 |
|  1 | MLP    |     2 |     1 |      0 |
+----+--------+-------+-------+--------+
</code></pre>
",225,3,8,2,python;pandas,2022-02-10 21:36:06,2022-02-10 21:36:06,2022-05-30 19:57:39,i have a large dataset    mio rows  with results from a machine learning training  now i want to check to see if the results hit the  target range  or not  lets say this range contains all values between    and     if it s inside this range  it s a hit  if it s below low and on the other side high  i now would create this three columns hit  low  high and calculate for each row which condition applies and put a  into this col  the other two would become   after that i would group the values and sum them up  but i suspect there must be a better and faster way  such as calculate it directly while grouping 
434,434,2535521,52012010,Tensorflow Lite - ValueError: Cannot set tensor: Dimension mismatch,"<p>This is probably going to be a stupid question but I am new to machine learning and Tensorflow.
I am trying to run object detection API on Raspberry Pi using <strong>Tensorflow Lite</strong>. I am trying to modify my code with the help of this example</p>

<pre><code>https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection.py
</code></pre>

<p>This piece of code will detect object from a image. But instead of a image I want to detect object on real time through Pi camera.
I tried to modified this code to read input from camera instead of image. Here is my piece of code -</p>

<pre><code>import numpy as np
from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
import cv2

cap = cv2.VideoCapture(0)
ret, image_np = cap.read()

PATH_TO_MODEL = ""ssd_mobilenet_v1_coco.tflite""

interpreter = tf.contrib.lite.Interpreter(model_path=PATH_TO_MODEL)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

while True:
  # NxHxWxC, H:1, W:2
  height = input_details[0]['shape'][1]
  width = input_details[0]['shape'][2]
  ret, image_np = cap.read()

  image_np_expanded = np.expand_dims(image_np, axis=0)
  #if floating_model:
  image_np_expanded = (np.float32(image_np_expanded) - input_mean) / input_std

  #HERE I AM GETTING ERROR
  interpreter.set_tensor(input_details[0]['index'], image_np_expanded)

  if cv2.waitKey(25) &amp; 0xFF == ord('q'):
    cv2.destroyAllWindows()
    break
</code></pre>

<p>but I am getting this error -</p>

<pre><code>Traceback (most recent call last):
  File ""New_object_detection.py"", line 257, in &lt;module&gt;
    interpreter.set_tensor(input_details[0]['index'], image_np_expanded)
  File ""/home/saurabh/.local/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter.py"", line 151, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
  File ""/home/saurabh/.local/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 133, in SetTensor
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)
ValueError: Cannot set tensor: Dimension mismatch
</code></pre>

<p>Can anyone please tell me how to fix this error or suggest a tutorial for the same?</p>
",7623,1,3,5,python;tensorflow;raspberry-pi;object-detection;tensorflow-lite,2018-08-25 03:05:17,2018-08-25 03:05:17,2022-05-30 18:36:00,but i am getting this error   can anyone please tell me how to fix this error or suggest a tutorial for the same 
435,435,8163377,72392440,Keras Agent Training Takes Too Much Time,"<p>I'm fairly new to reinforcement learning and I've built an agent that feeds two inputs to its neural network (first input is a tuple with two numbers representing the agents current position | second input is an array of numbers ranging from 0 to 3 representing what type of requests the agent receives from the environment) and outputs which movement is the best (move forwards, backwards, sideways etc...)</p>
<p>Each episode has 300 steps, the for loop inside the train_pos_nn() takes +5s (each call to predict() takes about 20ms and each call to fit() takes about 7ms), which amounts to +25 minutes per episode, which is too much time. (about 17 days to finish 1000 episodes which is the required number of episodes to converge / it takes the same amount of time on Google Colab (<em>(Edit: even when using the GPU option and gpu cannot be setup to be used on my local machine)</em>)</p>
<p>Is there any way I can reduce the amount of time it takes the agent to train ?</p>
<pre><code>n_possible_movements = 9
MINIBATCH_SIZE = 32

class DQNAgent(object):
    def __init__(self):
        #self.gamma = 0.95 
        self.epsilon = 1.0
        self.epsilon_decay = 0.8
        self.epsilon_min = 0.1
        self.learning_rate = 10e-4 
        self.tau = 1e-3
                        
        # Main models
        self.model_uav_pos = self._build_pos_model()

        # Target networks
        self.target_model_uav_pos = self._build_pos_model()
        # Copy weights
        self.target_model_uav_pos.set_weights(self.model_uav_pos.get_weights())

        # An array with last n steps for training
        self.replay_memory_pos_nn = deque(maxlen=REPLAY_MEMORY_SIZE)
        
    def _build_pos_model(self): # compile the DNN
        # create the DNN model
        dnn = self.create_pos_dnn()
        
        opt = Adam(learning_rate=self.learning_rate) #, decay=self.epsilon_decay)
        dnn.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=opt, metrics=['accuracy'])
        
        return dnn
    
    def create_pos_dnn(self): 
        # initialize the input shape (The shape of an array is the number of elements in each dimension)
        pos_input_shape = (2,)
        requests_input_shape = (len(env.ues),)
        # How many possible outputs we can have
        output_nodes = n_possible_movements
        
        # Initialize the inputs
        uav_current_position = Input(shape=pos_input_shape, name='pos')
        ues_requests = Input(shape=requests_input_shape, name='requests')
        
        # Put them in a list
        list_inputs = [uav_current_position, ues_requests]
        
        # Merge all input features into a single large vector
        x = layers.concatenate(list_inputs)
        
        # Add a 1st Hidden (Dense) Layer
        dense_layer_1 = Dense(512, activation=&quot;relu&quot;)(x)
        
        # Add a 2nd Hidden (Dense) Layer
        dense_layer_2 = Dense(512, activation=&quot;relu&quot;)(dense_layer_1)
        
        # Add a 3rd Hidden (Dense) Layer
        dense_layer_3 = Dense(256, activation=&quot;relu&quot;)(dense_layer_2)
        
        # Output layer
        output_layer = Dense(output_nodes, activation=&quot;softmax&quot;)(dense_layer_3)

        model = Model(inputs=list_inputs, outputs=output_layer)
                        
        # return the DNN
        return model
    
    def remember_pos_nn(self, state, action, reward, next_state, done):
        self.replay_memory_pos_nn.append((state, action, reward, next_state, done)) 
        
    def act_upon_choosing_a_new_position(self, state): # state is a tuple (uav_position, requests_array)
        if np.random.rand() &lt;= self.epsilon: # if acting randomly, take random action
            return random.randrange(n_possible_movements)
        pos =  np.array([state[0]])
        reqs =  np.array([state[1]])
        act_values = self.model_uav_pos.predict(x=[pos, reqs]) # if not acting randomly, predict reward value based on current state
        return np.argmax(act_values[0]) 
        
    def train_pos_nn(self):
        print(&quot;In Training..&quot;)

        # Start training only if certain number of samples is already saved
        if len(self.replay_memory_pos_nn) &lt; MIN_REPLAY_MEMORY_SIZE:
            print(&quot;Exiting Training: Replay Memory Not Full Enough...&quot;)
            return

        # Get a minibatch of random samples from memory replay table
        minibatch = random.sample(self.replay_memory_pos_nn, MINIBATCH_SIZE)

        start_time = time.time()
        # Enumerate our batches
        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):
            print('...Starting Training...')
            target = 0
            pos =  np.array([current_state[0]])
            reqs =  np.array([current_state[1]])
            pos_next = np.array([new_current_state[0]])
            reqs_next = np.array([new_current_state[1]])
    
            if not done:
                target = reward + DISCOUNT * np.amax(self.target_model_uav_pos.predict(x=[pos_next, reqs_next]))
            else:
                target = reward

            # Update Q value for given state
            target_f = self.model_uav_pos.predict(x=[pos, reqs])
            target_f[0][action] = target

            self.model_uav_pos.fit([pos, reqs], \
                                   target_f, \
                                   verbose=2, \
                                   shuffle=False, \
                                   callbacks=None, \
                                   epochs=1 \
                                  )  
        end_time = time.time()
        print(&quot;Time&quot;, end_time - start_time)
        # Update target network counter every episode
        self.target_train()
        
    def target_train(self):
        weights = self.model_uav_pos.get_weights()
        target_weights = self.target_model_uav_pos.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)
        self.target_model_uav_pos.set_weights(target_weights)
</code></pre>
<pre><code># Main 
SIZE = 100 # size of the grid the agent is in
for episode in tqdm(range(1, n_episodes + 1), ascii=True, unit='episodes'):  
    # Reset environment and get initial state
    current_state = env.reset(SIZE)

    # Reset flag and start iterating until episode ends
    done = False
    steps_n = 300

    for t in range(steps_n): 
        # Normalize the input (the current state)
        current_state_normalized = normalize_pos_state(current_state)
        
        # Get new position for the agent
        action_pos = agent_dqn.act_upon_choosing_a_new_position(current_state_normalized)
        
        new_state, reward, done, _ = env.step(action_pos)
        
        agent_dqn.remember_pos_nn(current_state_normalized, action_pos, reward, normalize_pos_state(new_state), done)

        current_state = new_state # not normalized
        
        agent_dqn.train_pos_nn()

    # Decay epsilon
    if episode % 50 == 0:
        if agent_dqn.epsilon &gt; agent_dqn.epsilon_min:
            agent_dqn.epsilon *= agent_dqn.epsilon_decay
            agent_dqn.epsilon = max(agent_dqn.epsilon, agent_dqn.epsilon_min)

</code></pre>
",103,2,2,5,tensorflow;machine-learning;keras;deep-learning;reinforcement-learning,2022-05-26 18:44:37,2022-05-26 18:44:37,2022-05-30 18:09:27,i m fairly new to reinforcement learning and i ve built an agent that feeds two inputs to its neural network  first input is a tuple with two numbers representing the agents current position   second input is an array of numbers ranging from  to  representing what type of requests the agent receives from the environment  and outputs which movement is the best  move forwards  backwards  sideways etc     each episode has  steps  the for loop inside the train_pos_nn   takes  s  each call to predict   takes about ms and each call to fit   takes about ms   which amounts to   minutes per episode  which is too much time   about  days to finish  episodes which is the required number of episodes to converge   it takes the same amount of time on google colab   edit  even when using the gpu option and gpu cannot be setup to be used on my local machine   is there any way i can reduce the amount of time it takes the agent to train  
436,436,4576519,65882709,How to write ndarray to .npy file iteratively with batches,"<p>I am generating large dataset for a machine learning application, which is a numpy array with shape <code>(N,X,Y)</code>. Here <code>N</code> is the number of samples, <code>X</code> is the input of a sample and <code>Y</code> is the target of a sample. I want to save this array in the <code>.npy</code> format. I have many samples (<code>N</code> is very large) so that the final dataset is about 10+ GB. This means that I cannot create the whole dataset and then save it, as it will flood my memory.</p>
<p>Is it possible to instead to write batches of <code>n</code> samples iteratively to this file? So, I want to append for example batches of 256 samples to the file at once (<code>(256,X,Y)</code>).</p>
",457,2,1,3,python;numpy;memory-management,2021-01-25 15:42:58,2021-01-25 15:42:58,2022-05-30 17:31:51,i am generating large dataset for a machine learning application  which is a numpy array with shape  n x y   here n is the number of samples  x is the input of a sample and y is the target of a sample  i want to save this array in the  npy format  i have many samples  n is very large  so that the final dataset is about   gb  this means that i cannot create the whole dataset and then save it  as it will flood my memory  is it possible to instead to write batches of n samples iteratively to this file  so  i want to append for example batches of  samples to the file at once    x y   
437,437,19135059,72272370,Autoencoder for casualty of time series data,"<p>I am trying to figure out the link between 2-time series vectors.<br />
for example :
X = temperature variation for one year;
Y = strain measurement for one year;</p>
<p>I am new to machine learning.
I cannot figure out which unsupervised algorithm to use for identifying possible causality and understanding the link between the 2 vectors?
Can I use autoencoders?</p>
",22,1,-1,4,time-series;autoencoder;unsupervised-learning;causality,2022-05-17 15:57:25,2022-05-17 15:57:25,2022-05-30 16:43:13,
438,438,19058835,72431938,TypeError: &#39;numpy.float32&#39; object is not iterable when logging in mlflow,"<p>I am trying a machine learning model and logging metrics using mlflow. But I am getting <code>TypeError: 'numpy.float32' object is not iterable</code>. I have tried using <code>.tolist()</code> and <code>dict()</code> but nothing seems to work.</p>
<pre class=""lang-py prettyprint-override""><code>def train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name):
    best_val_loss = 100
    for epoch in range(max_epochs):
        model.train()
        running_loss = []
        tq_loader = tqdm(train_loader)
        o = {}
        for samples in tq_loader:
            optimizer.zero_grad()
            outputs, interaction_map = model(
                [samples[0].to(device), samples[1].to(device), torch.tensor(samples[2]).to(device),
                 torch.tensor(samples[3]).to(device)])
            l1_norm = torch.norm(interaction_map, p=2) * 1e-4
            loss = loss_fn(outputs, torch.tensor(samples[4]).to(device).float()) + l1_norm
            loss.backward()
            optimizer.step()
            loss = loss - l1_norm
            running_loss.append(loss.cpu().detach())
            tq_loader.set_description(
                &quot;Epoch: &quot; + str(epoch + 1) + &quot;  Training loss: &quot; + str(np.mean(np.array(running_loss))))
        model.eval()
        val_loss, mae_loss = get_metrics(model, valid_loader)
        scheduler.step(val_loss)
        
        #metrics mlflow
        mlflow.log_metrics('train_loss',(np.mean(np.array(running_loss))).tolist())
        mlflow.log_metrics('validation_loss',(val_loss).tolist())
        mlflow.log_metrics('MAE Val_loss', (mae_loss).tolist())

        print(&quot;Epoch: &quot; + str(epoch + 1) + &quot;  train_loss &quot; + str(np.mean(np.array(running_loss))) + &quot; Val_loss &quot; + str(
            val_loss) + &quot; MAE Val_loss &quot; + str(mae_loss))
        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), &quot;./runs/run-&quot; + str(project_name) + &quot;/models/best_model.tar&quot;)

mlflow.set_experiment('CIGIN_V2')
mlflow.start_run(nested=True)
train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)
mlflow.end_run()
</code></pre>
<p>Error</p>
<pre class=""lang-py prettyprint-override""><code>Epoch: 1  Training loss: 6770.575: 100%|██████████| 1/1 [00:04&lt;00:00,  4.35s/it]
100%|██████████| 1/1 [00:03&lt;00:00,  3.86s/it]

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-96-8c3a6eb822c3&gt; in &lt;module&gt;()
      1 mlflow.set_experiment('CIGIN_V2')
      2 mlflow.start_run(nested=True)
----&gt; 3 train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)
      4 mlflow.end_run()

&lt;ipython-input-95-ab0a6c80b65b&gt; in train(max_epochs, model, optimizer, scheduler, train_loader, valid_loader, project_name)
     55 
     56         #metrics mlflow
---&gt; 57         mlflow.log_metrics('train_loss',dict(np.mean(np.array(running_loss))).tolist())
     58         mlflow.log_metrics('validation_loss',dict(val_loss).tolist())
     59         mlflow.log_metrics('MAE Val_loss', dict(mae_loss).tolist())

TypeError: 'numpy.float32' object is not iterable
</code></pre>
",28,1,0,3,python;pytorch;mlflow,2022-05-30 14:48:30,2022-05-30 14:48:30,2022-05-30 15:14:54,i am trying a machine learning model and logging metrics using mlflow  but i am getting typeerror   numpy float  object is not iterable  i have tried using  tolist   and dict   but nothing seems to work  error
439,439,11575257,62842509,Does model.compile() go inside MirroredStrategy,"<p>I have a network for transfer learning and want to train on two GPUs. I have just trained on one up to this point and am looking for ways to speed things up. I am getting conflicting answers about how to use it most efficiently.</p>
<pre><code>strategy = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;])
with strategy.scope():
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(200,200,3))
    x = base_model.output
    x = GlobalAveragePooling2D(name=&quot;class_pool&quot;)(x)
    x = Dense(1024, activation='relu', name=&quot;class_dense1&quot;)(x)
    types = Dense(20,activation='softmax', name='Class')(x)
    model = Model(inputs=base_model.input, outputs=[types])
</code></pre>
<p>Then I set trainable layers:</p>
<pre><code>for layer in model.layers[:160]:
    layer.trainable=False
for layer in model.layers[135:]:
    layer.trainable=True
</code></pre>
<p>Then I compile</p>
<pre><code>optimizer = Adam(learning_rate=.0000001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy')
</code></pre>
<p>Should everything be nested inside<code>strategy.scope()</code>?</p>
<p>This <a href=""https://www.tensorflow.org/tutorials/distribute/keras#create_the_model"" rel=""nofollow noreferrer"">tutorial</a> shows <code>compile</code> within but this <a href=""https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_tfkerasmodelfit"" rel=""nofollow noreferrer"">tutorial</a> shows it is outside.
Thefirst one shows it outside</p>
<pre><code>mirrored_strategy = tf.distribute.MirroredStrategy()

with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])

model.compile(loss='mse', optimizer='sgd')
</code></pre>
<p>but says this right after</p>
<blockquote>
<p>In this example we used MirroredStrategy so we can run this on a machine with multiple GPUs. strategy.scope() indicates to Keras which strategy to use to distribute the training. Creating models/optimizers/metrics inside this scope allows us to create distributed variables instead of regular variables. Once this is set up, you can fit your model like you would normally. MirroredStrategy takes care of replicating the model's training on the available GPUs, aggregating gradients, and more.</p>
</blockquote>
",405,1,0,4,python;tensorflow;keras;multi-gpu,2020-07-11 02:55:38,2020-07-11 02:55:38,2022-05-30 15:06:05,i have a network for transfer learning and want to train on two gpus  i have just trained on one up to this point and am looking for ways to speed things up  i am getting conflicting answers about how to use it most efficiently  then i set trainable layers  then i compile should everything be nested insidestrategy scope    but says this right after in this example we used mirroredstrategy so we can run this on a machine with multiple gpus  strategy scope   indicates to keras which strategy to use to distribute the training  creating models optimizers metrics inside this scope allows us to create distributed variables instead of regular variables  once this is set up  you can fit your model like you would normally  mirroredstrategy takes care of replicating the model s training on the available gpus  aggregating gradients  and more 
440,440,12415334,72428944,how to unfreeze jupyter notebook cell output?,"<p>I have machine learning going with verbose 1. I run it through the network so I could work even when I'm away. but when I open my jupyter notebook that still has cell running, the output is freeze. The output is not updating. I know it's still running but I want to know the progress. How can I unfreeze it? or can it?</p>
<p><strong>Edit</strong></p>
<p>Let's say I have a program that looping and printing every iteration (because it's the same case, freeze output) and this program take a long time but will give some output, the output cell is not updating but the website icon is still hourglass icon, not the book, and the dot on the right side of &quot;Python 3&quot; is active, which mean there is a process running</p>
<p><a href=""https://i.stack.imgur.com/rpxxI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rpxxI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/9GKvN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9GKvN.png"" alt=""enter image description here"" /></a></p>
<p>I use :</p>
<pre><code>jupyter                   1.0.0            py37h03978a9_7    conda-forge
jupyter_client            7.2.2              pyhd8ed1ab_1    conda-forge
jupyter_console           6.4.3              pyhd8ed1ab_0    conda-forge
jupyter_core              4.9.2            py37h03978a9_0    conda-forge
jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    conda-forge
jupyterlab_widgets        1.1.0              pyhd8ed1ab_0    conda-forge
</code></pre>
",13,0,0,1,jupyter-notebook,2022-05-30 09:16:05,2022-05-30 09:16:05,2022-05-30 09:38:34,i have machine learning going with verbose   i run it through the network so i could work even when i m away  but when i open my jupyter notebook that still has cell running  the output is freeze  the output is not updating  i know it s still running but i want to know the progress  how can i unfreeze it  or can it  edit let s say i have a program that looping and printing every iteration  because it s the same case  freeze output  and this program take a long time but will give some output  the output cell is not updating but the website icon is still hourglass icon  not the book  and the dot on the right side of  python   is active  which mean there is a process running   i use  
441,441,4772210,72194930,How to change OS disk SKU for Virtual Machine Scale Set (VMSS)?,"<p>I create Azure Kubernetes Service by Bicep like below.</p>
<pre><code>param clusterName string = 'kubernetes'
param location string = resourceGroup().location

resource aksCluster 'Microsoft.ContainerService/managedClusters@2022-02-01' = {
  name: clusterName
  location: location
  sku: {
    name: 'Basic'
    tier: 'Free'
  }
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    kubernetesVersion: '1.23.5'
    enableRBAC: true
    agentPoolProfiles: [
      {
        name: 'agentpool'
        mode: 'System'
        type:'VirtualMachineScaleSets'
        orchestratorVersion: '1.23.5'
        enableAutoScaling: false
        enableFIPS: false
        maxPods: 110
        count: 1
        vmSize: 'Standard_B2s'
        osType:'Linux'
        osSKU:'Ubuntu'
        osDiskType: 'Managed'
        osDiskSizeGB: 0
        enableUltraSSD: false
        enableNodePublicIP: false
      }
    ]
    dnsPrefix: 'kubernetes-cluster-dns'
    networkProfile: {
      loadBalancerSku: 'basic'
      networkPlugin: 'kubenet'
    }
  }
}
</code></pre>
<p>Azure Kubernetes Service (AKS) create Virtual Machine Scale Set like below.</p>
<p><a href=""https://i.stack.imgur.com/G6PWB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G6PWB.png"" alt=""VMSS"" /></a></p>
<p>I do not want to use <strong>Premium SSD LRS</strong> OS disk. It is too expensive for me while learning kubernetes. I want to change it to <strong>Standard SSD LRS</strong>.</p>
<p>What should I do?</p>
<hr />
<h3>2022.05.30 Update</h3>
<p>I create an <a href=""https://github.com/Azure/bicep/issues/6974"" rel=""nofollow noreferrer"">issue</a> at <a href=""https://github.com/Azure/bicep"" rel=""nofollow noreferrer"">Azure/bicep</a>.</p>
",166,1,0,5,azure;azure-aks;azure-vm-scale-set;azure-vm;azure-bicep,2022-05-11 08:35:02,2022-05-11 08:35:02,2022-05-30 07:47:53,i create azure kubernetes service by bicep like below  azure kubernetes service  aks  create virtual machine scale set like below   i do not want to use premium ssd lrs os disk  it is too expensive for me while learning kubernetes  i want to change it to standard ssd lrs  what should i do  i create an  at  
442,442,16386146,72428421,Extract backward citations of Google Scholar,"<p>There is any way that i can extract the &quot;reverse&quot; path of citations on Google Scholar?</p>
<p>Exemple:
The article &quot;<a href=""https://scholar.google.com/scholar?hl=pt-BR&amp;as_sdt=0%2C5&amp;q=Machine%20learning%20and%20the%20physical%20sciences&amp;btnG="" rel=""nofollow noreferrer"">(1)Machine learning and the physical sciences</a>&quot; is cited by the article &quot;<a href=""https://scholar.google.com/scholar?hl=pt-BR&amp;as_sdt=0%2C5&amp;q=Artificial%20Intelligence%20%28AI%29%3A%20Multidisciplinary%20perspectives%20on%20emerging%20challenges%2C%20opportunities%2C%20and%20agenda%20for%20research%2C%20practice%20and%20policy&amp;btnG="" rel=""nofollow noreferrer"">(2)Artificial Intelligence (AI): Multidisciplinary...</a>&quot; the only way i know that is beacuse the second article informed that he is citing the first article, so there is a list of articles that the second one is referencing.</p>
<p>Basically i need to know who the article is referencing istead of who is referencing the article</p>
",14,0,0,5,reference;extract;scrape;citations;scholar,2022-05-30 07:18:40,2022-05-30 07:18:40,2022-05-30 07:18:40,there is any way that i can extract the  reverse  path of citations on google scholar  basically i need to know who the article is referencing istead of who is referencing the article
443,443,16800821,72428325,Train a Machine Learning Model for Horizon Detection,"<p>I am trying to train a machine learning model to detect the horizon in an image. For my purposes, the horizon is just a straight line, defined by the angle of the line in radians, and its vertical offset.</p>
<p>The angle labels are all positive numbers, between 0 and 2 pi. The offset values are floats, generally between 0 and 1, but can be lesser or greater than that range.</p>
<p><a href=""https://i.stack.imgur.com/HwuWo.png"" rel=""nofollow noreferrer"">example image</a></p>
<p><a href=""https://i.stack.imgur.com/9weVx.png"" rel=""nofollow noreferrer"">example labels</a></p>
<p>I am struggling to find the right type of machine learning model for this problem. I have tried using the AutoKeras ImageRegressor (<a href=""https://autokeras.com/tutorial/image_regression/"" rel=""nofollow noreferrer"">https://autokeras.com/tutorial/image_regression/</a>) and Res Net 50, but so far, no accurate results have been produced. My data set contains about 2000 sample images.</p>
",32,0,-1,5,tensorflow;machine-learning;keras;computer-vision;conv-neural-network,2022-05-30 06:47:59,2022-05-30 06:47:59,2022-05-30 07:01:00,i am trying to train a machine learning model to detect the horizon in an image  for my purposes  the horizon is just a straight line  defined by the angle of the line in radians  and its vertical offset  the angle labels are all positive numbers  between  and  pi  the offset values are floats  generally between  and   but can be lesser or greater than that range    i am struggling to find the right type of machine learning model for this problem  i have tried using the autokeras imageregressor    and res net   but so far  no accurate results have been produced  my data set contains about  sample images 
444,444,1459266,72425998,MFCC Feature Extractor,"<p>I'm looking for a Mel Frequency Cepstrum Coefficient (MFCC) feature extractor for machine learning. This needs to be compatible with .NET 6.0 and run cross-platform. The only real implementation I can find is in Accord.NET and this project appears to be dead.</p>
<p>Is there some equivalent in the ML.NET or SciSharp ecosystems?</p>
",7,0,1,3,.net;ml.net;scisharp,2022-05-29 23:20:23,2022-05-29 23:20:23,2022-05-29 23:20:23,i m looking for a mel frequency cepstrum coefficient  mfcc  feature extractor for machine learning  this needs to be compatible with  net   and run cross platform  the only real implementation i can find is in accord net and this project appears to be dead  is there some equivalent in the ml net or scisharp ecosystems 
445,445,19225598,72424042,"Image classifier only predicting 2 out of 6 classes and val accuracy is high, what is happening?","<p>I'm new to machine learning and I'm working on a dataset with 14k pictures of sea, forest, glaciers, streets, buildings and mountains (6 classes). I have been training my model with it and achieved a val acc of 91% but for some reason it is biased, when I try to predict new images with my inference code the only classes chosen are glaciers and sea. Here is the <a href=""https://github.com/IguanaAncestral/Intel-"" rel=""nofollow noreferrer"">Github</a> with the model creation code and the inference code.</p>
<pre><code>train_datagen = ImageDataGenerator(
rotation_range= 20,  # Rotate the augmented image by 20 degrees
zoom_range=0.3,  # Zoom by 20% more or less
horizontal_flip=True,  # Allow for horizontal flips of augmented images
vertical_flip=True,  # Allow for vertical flips of augmented images
brightness_range=[0.6, 1.2],  # Lighter and darker images 
fill_mode='nearest', 
preprocessing_function=preprocess_input)

img_data_iterator = train_datagen.flow_from_directory(
# Where to take the data from, the classes are the sub folder names
'../Q2B/archive/seg_train/seg_train/',
class_mode=&quot;categorical&quot;,  # classes are in 2D one hot encoded way 
shuffle=True,  # shuffle the data, default is true but just to point it out
batch_size=32,
target_size=(150, 150),  # This size is the default of mobilenet NN)

validation_generator = ImageDataGenerator(
preprocessing_function=preprocess_input).flow_from_directory(
'../Q2B/archive/seg_test/seg_test/',
class_mode=&quot;categorical&quot;,
shuffle=True,
batch_size=32,
target_size=(150, 150),)
</code></pre>
<p>My guess is that it is related to the way I pre-processed the data.</p>
",19,1,0,3,image-processing;conv-neural-network;image-augmentation,2022-05-29 18:56:27,2022-05-29 18:56:27,2022-05-29 19:23:29,i m new to machine learning and i m working on a dataset with k pictures of sea  forest  glaciers  streets  buildings and mountains   classes   i have been training my model with it and achieved a val acc of   but for some reason it is biased  when i try to predict new images with my inference code the only classes chosen are glaciers and sea  here is the  with the model creation code and the inference code  my guess is that it is related to the way i pre processed the data 
446,446,6388372,54750747,Data Science Case Study,"<p>Please help me to get the right answer of the below question, which is asked in one of the interview. </p>

<p>There is a bank, no of users visit bank for different- different services, but most of the users give bad rating and goes unsatisfied. What should bank do identify the reasons for the bad ratings. Bank capture data like, user info, agent info who deals with users, services offered, and no if things. </p>

<ol>
<li><p>How to identify rules or reasons which are playing an important role in bad rating using machine learning techniques only.</p></li>
<li><p>If we build a classification model, that a user will be unsatisfied/satisfied. Then let say we get list of users who will be unsatisfied. Now what should we do with this data of unsatisfied users to help bank improve rating and business.</p></li>
</ol>
",110,3,-2,3,machine-learning;classification;data-science,2019-02-18 21:11:07,2019-02-18 21:11:07,2022-05-29 18:09:13,please help me to get the right answer of the below question  which is asked in one of the interview   there is a bank  no of users visit bank for different  different services  but most of the users give bad rating and goes unsatisfied  what should bank do identify the reasons for the bad ratings  bank capture data like  user info  agent info who deals with users  services offered  and no if things   how to identify rules or reasons which are playing an important role in bad rating using machine learning techniques only  if we build a classification model  that a user will be unsatisfied satisfied  then let say we get list of users who will be unsatisfied  now what should we do with this data of unsatisfied users to help bank improve rating and business 
447,447,18623377,72422799,ModuleNotFoundError : import tensorflow.compat.v1 as tf,"<pre><code>import tensorflow.compat.v1 as tf
</code></pre>
<p><a href=""https://i.stack.imgur.com/XV6oc.png"" rel=""nofollow noreferrer"">Error displayed in vscode</a>
<a href=""https://i.stack.imgur.com/BZ56r.png"" rel=""nofollow noreferrer"">output in jupyter</a>
Despite trying all the possible options, I am not able to fix this error.
I am trying to learn machine learning to create a simple sign language detection system. In my VSCode, the above image is displayed.</p>
",41,0,0,3,tensorflow;visual-studio-code;modulenotfounderror,2022-05-29 15:48:26,2022-05-29 15:48:26,2022-05-29 16:13:37,
448,448,16770626,72422581,How to convert audio files to pitch data,"<p>I am beginning a machine learning project looking at recorded audio, specifically pitch. I was wondering if there is any way to turn an audio recording into data such like pitch values which can then be analyzed by the model.</p>
<p>I was thinking about something like taking the pitch of the audio recording every 1/100th of a second but I am open to other suggestions. I also thought about looking at apis of music tuners and trying to use those but most use live audio.</p>
<p>Thank you!</p>
",11,0,-1,1,machine-learning,2022-05-29 15:17:36,2022-05-29 15:17:36,2022-05-29 15:17:36,i am beginning a machine learning project looking at recorded audio  specifically pitch  i was wondering if there is any way to turn an audio recording into data such like pitch values which can then be analyzed by the model  i was thinking about something like taking the pitch of the audio recording every  th of a second but i am open to other suggestions  i also thought about looking at apis of music tuners and trying to use those but most use live audio  thank you 
449,449,19223442,72422353,XGBoost error - check failed: auc &lt;= local area,"<p>I was running some code for a machine learning classification project with XGBoost in Jupyter and received this error message. Can someone please clarify what it means and how to avoid?</p>
<p>I checked my data splits and train / test / validation all have examples of both the 0 and 1 class.</p>
<p>I am using BayesSearchCV to tune the model over 5 folds. I am also using n_jobs = 7 to take advantage of parallel processing via joblib.</p>
",18,0,0,5,machine-learning;scikit-learn;xgboost;joblib;bayessearchcv,2022-05-29 14:40:15,2022-05-29 14:40:15,2022-05-29 14:40:15,i was running some code for a machine learning classification project with xgboost in jupyter and received this error message  can someone please clarify what it means and how to avoid  i checked my data splits and train   test   validation all have examples of both the  and  class  i am using bayessearchcv to tune the model over  folds  i am also using n_jobs    to take advantage of parallel processing via joblib 
450,450,19224245,72421770,looping through csv file to create numpy array,"<p>I am working on a machine learning dataset, where each row in the csv represent an image with dimension 28*28:</p>
<pre><code>row[0]=image label
row[1:]=information about pixel in the image
</code></pre>
<p>I need to loop through the CSV file to create 2 np array, 1 containing the label and the 2nd one containing the image.
The image array need to be a 3d numpy array.</p>
<pre><code>csv_reader = csv.reader(file, delimiter=',')
images = np.empty((0,28,28),dtype=np.float64)
labels = np.empty((0,1),dtype=np.float64)

for row in csv_reader:

  image= np.reshape(row[1:],(28,28))
  resize_image =image[np.newaxis,:]
  images = np.append(images,resize_image,axis=0)
  label = np.reshape(row[0],(1))
  resize_label = label[np.newaxis,:]
  labels = np.append(labels,resize_label,axis=0)

return images, labels
</code></pre>
<p>I feel like this is a really slow and not optimized method, is there any more efficient way to do the task?</p>
",23,1,0,3,python;arrays;numpy,2022-05-29 13:06:13,2022-05-29 13:06:13,2022-05-29 13:25:13,i am working on a machine learning dataset  where each row in the csv represent an image with dimension    i feel like this is a really slow and not optimized method  is there any more efficient way to do the task 
451,451,19222343,72419090,Why Bi-LSTM Performed poorly?,"<p>I performed 8 machine learning algorithms on a transliterated English text there bi-lstm accuracy was very poor only 43% can anyone explain the reason behind it?</p>
",16,0,-4,4,text;nlp;lstm;bilstm,2022-05-29 02:03:15,2022-05-29 02:03:15,2022-05-29 04:54:49,i performed  machine learning algorithms on a transliterated english text there bi lstm accuracy was very poor only   can anyone explain the reason behind it 
452,452,18316461,72418015,How to keep the same tab open after refresh javascript / css,"<p>I am trying to keep selected tab active on refresh with css and js. Tried and checked with some question already been asked here but none of work for me. Don't know what to do.
html code:</p>
<pre><code>
&lt;div class=&quot;container&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Machine learning : Prediction ET0 &lt;/div&gt;
      &lt;div class=&quot;tab&quot;&gt;
        &lt;button class=&quot;tablinks&quot; onclick=&quot;openCity(event, 'daily')&quot; id=&quot;defaultOpen&quot;&gt;Journalier&lt;/button&gt;
        &lt;button class=&quot;tablinks&quot; onclick=&quot;openCity(event, 'hourly')&quot;&gt;Horaire&lt;/button&gt;
        &lt;button class=&quot;tablinks&quot; onclick=&quot;openCity(event, 'Exdaily')&quot;&gt;Excel journalier&lt;/button&gt;
        &lt;button class=&quot;tablinks&quot; onclick=&quot;openCity(event, 'Exhourly')&quot;&gt;Excel Horaire&lt;/button&gt;


      &lt;/div&gt;
</code></pre>
<pre><code>
&lt;script&gt;
function openCity(evt, data) {
  var i, tabcontent, tablinks;
  tabcontent = document.getElementsByClassName(&quot;tabcontent&quot;);
  for (i = 0; i &lt; tabcontent.length; i++) {
    tabcontent[i].style.display = &quot;none&quot;;
  }
  tablinks = document.getElementsByClassName(&quot;tablinks&quot;);
  for (i = 0; i &lt; tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(&quot; active&quot;, &quot;&quot;);


  }
  document.getElementById(data).style.display = &quot;block&quot;;
  evt.currentTarget.className += &quot; active&quot;;
}
&lt;/script&gt;
</code></pre>
",46,2,0,3,javascript;html;css,2022-05-28 23:22:13,2022-05-28 23:22:13,2022-05-28 23:57:32,
453,453,19178150,72406761,tune_grid: Error in prep(); Columns must be character or factor,"<p>I am trying to run a penalised multinomial regression. It is my understanding that predictor variables can be numerical. However, when I try to tune my model using a validation set, I get the error that says that all my variables must be character or factor.</p>
<p>This is the output of example_data &lt;- dput(head(my_dataset))</p>
<pre><code>structure(list(...1 = c(1, 2, 3, 4, 5, 6), id = c(10, 15, 49, 
51, 54, 58), npsn = c(&quot;10114357&quot;, &quot;10114353&quot;, &quot;10114234&quot;, &quot;69786607&quot;, 
&quot;10114364&quot;, &quot;69935355&quot;), village = c(&quot;-&quot;, &quot;-&quot;, &quot;ALUE TAMPAK&quot;, 
&quot;Lueng Baro&quot;, &quot;-&quot;, &quot;Suwak Indrapuri&quot;), subdistrict = c(&quot;Kec. Johan Pahlawan&quot;, 
&quot;Kec. Bubon&quot;, &quot;Kec. Kaway XVI&quot;, &quot;Kec. Sungai Mas&quot;, &quot;Kec. Panton Reu&quot;, 
&quot;Kec. Johan Pahlawan&quot;), district = c(&quot;Kab. Aceh Barat&quot;, &quot;Kab. Aceh Barat&quot;, 
&quot;Kab. Aceh Barat&quot;, &quot;Kab. Aceh Barat&quot;, &quot;Kab. Aceh Barat&quot;, &quot;Kab. Aceh Barat&quot;
), public = c(&quot;SWASTA&quot;, &quot;SWASTA&quot;, &quot;SWASTA&quot;, &quot;SWASTA&quot;, &quot;SWASTA&quot;, 
&quot;SWASTA&quot;), morning = c(NA, NA, NA, &quot;Pagi/6 hari&quot;, NA, &quot;Pagi/6 hari&quot;
), level = c(&quot;MTs&quot;, &quot;MTs&quot;, &quot;MA&quot;, &quot;SD&quot;, &quot;MTs&quot;, &quot;SMP&quot;), authority = c(&quot;Kementerian Agama&quot;, 
&quot;Kementerian Agama&quot;, &quot;Kementerian Agama&quot;, &quot;Kementerian Pendidikan dan Kebudayaan&quot;, 
&quot;Kementerian Agama&quot;, &quot;Kementerian Pendidikan dan Kebudayaan&quot;), 
    cert_est = c(&quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;421.2/788/2012&quot;, &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;421.2/1265.a/2015&quot;), date_est = c(&quot;-&quot;, &quot;-&quot;, &quot;-&quot;, &quot;2012-12-31&quot;, 
    &quot;-&quot;, &quot;2015-10-05&quot;), cert_ops = c(&quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;421.2/ 788 / 2013&quot;, &quot;&lt;font color=\&quot;#FF0000\&quot;&gt;Perlu Update&lt;/font&gt;&quot;, 
    &quot;421.2/1360/2015&quot;), date_ops = c(NA, NA, NA, &quot;2013-08-14&quot;, 
    NA, &quot;2015-10-22&quot;), grade = c(&quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;
    ), area = c(0, 0, 0, 1000, 0, 1200), latlon = c(&quot;L.marker([4.1583390,96.1241250]).addTo(mymap);&quot;, 
    &quot;L.marker([4.2839640,96.0855880]).addTo(mymap);&quot;, &quot;L.marker([4.2137130,96.1585580]).addTo(mymap);&quot;, 
    &quot;L.marker([4.5028000,96.0300000]).addTo(mymap);&quot;, &quot;L.marker([4.3833290,96.1981020]).addTo(mymap);&quot;, 
    &quot;L.marker([4.1923750,96.1241380]).addTo(mymap);&quot;), name_clean = c(&quot;MTSS NURUL FALAH&quot;, 
    &quot;MTSS BANDA LAYUNG&quot;, &quot;MAS ALUE TAMPAK&quot;, &quot;SDS LUENG BARO&quot;, 
    &quot;MTSS KRUENG MANGGI&quot;, &quot;SMP ISLAM BAHRUL ULUM ISLAMIC SCHOOL&quot;
    ), name = c(&quot;MTSS NURUL FALAH&quot;, &quot;MTSS BANDA LAYUNG&quot;, &quot;MAS ALUE TAMPAK&quot;, 
    &quot;SDS LUENG BARO&quot;, &quot;MTSS KRUENG MANGGI&quot;, &quot;SMP ISLAM BAHRUL ULUM ISLAMIC SCHOOL&quot;
    ), muh1 = c(0, 0, 0, 0, 0, 0), muh2 = c(0, 0, 0, 0, 0, 0), 
    muh = c(0, 0, 0, 0, 0, 0), chr1 = c(0, 0, 0, 0, 0, 0), chr2 = c(0, 
    0, 0, 0, 0, 0), chr3 = c(NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_), chr = c(0, 0, 0, 0, 0, 0), hindu = c(NA, 
    NA, NA, NA, NA, NA), nu1 = c(0, 0, 0, 0, 0, 0), nu2 = c(0, 
    0, 0, 0, 0, 0), nu_klaten = c(NA, NA, NA, NA, NA, NA), nu_sby = c(NA, 
    NA, NA, NA, NA, NA), nu = c(0, 0, 0, 0, 0, 0), it1 = c(0, 
    0, 0, 0, 0, 0), it = c(0, 0, 0, 0, 0, 0), other_swas_international = c(0, 
    0, 0, 0, 0, 0), afiliasi = c(99, 99, 99, 99, 99, 99), X1 = c(FALSE, 
    FALSE, FALSE, TRUE, FALSE, FALSE), X2 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, TRUE), X3 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X4 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X5 = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE), X6 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, TRUE), X7 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X8 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X9 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X10 = c(TRUE, FALSE, TRUE, FALSE, FALSE, FALSE), 
    X11 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X12 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X13 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X14 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X15 = c(FALSE, FALSE, TRUE, FALSE, 
    FALSE, FALSE), X16 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X17 = c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE), 
    X18 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X19 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X20 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X21 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, TRUE), X22 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X23 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X24 = c(TRUE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X25 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X26 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X27 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X28 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X29 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X30 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X31 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X32 = c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE), X33 = c(TRUE, 
    TRUE, TRUE, TRUE, TRUE, TRUE), X34 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X35 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, TRUE), X36 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X37 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X38 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X39 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, TRUE), X40 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X41 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X42 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X43 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X44 = c(FALSE, FALSE, TRUE, FALSE, TRUE, FALSE), 
    X45 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X46 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X47 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X48 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X49 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X50 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X51 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X52 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X53 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X54 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X55 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X56 = c(TRUE, TRUE, TRUE, TRUE, TRUE, 
    TRUE), X57 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X58 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X59 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X60 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X61 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X62 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X63 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X64 = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE), X65 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X66 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X67 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X68 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X69 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X70 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X71 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X72 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X73 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X74 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X75 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X76 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X77 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X78 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X79 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X80 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X81 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X82 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X83 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    TRUE), X84 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X85 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X86 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X87 = c(TRUE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X88 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X89 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, TRUE), X90 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X91 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), 
    X92 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X93 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X94 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X95 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X96 = c(FALSE, TRUE, FALSE, FALSE, 
    TRUE, FALSE), X97 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X98 = c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE), 
    X99 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X100 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X101 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X102 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X103 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X104 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X105 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X106 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X107 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X108 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X109 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X110 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X111 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X112 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X113 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X114 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X115 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X116 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X117 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X118 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X119 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X120 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X121 = c(TRUE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X122 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X123 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X124 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X125 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X126 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X127 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X128 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X129 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X130 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X131 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X132 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X133 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X134 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X135 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X136 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X137 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X138 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X139 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X140 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X141 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X142 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X143 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X144 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X145 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X146 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X147 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X148 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X149 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X150 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X151 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X152 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X153 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X154 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X155 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X156 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X157 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X158 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X159 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X160 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X161 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X162 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X163 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X164 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X165 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X166 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X167 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X168 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X169 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X170 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X171 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X172 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X173 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X174 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X175 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X176 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X177 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X178 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X179 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X180 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X181 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X182 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X183 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X184 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X185 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X186 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X187 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X188 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X189 = c(FALSE, FALSE, TRUE, FALSE, FALSE, FALSE), 
    X190 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE), X191 = c(FALSE, 
    FALSE, FALSE, FALSE, FALSE, FALSE), X192 = c(FALSE, FALSE, 
    FALSE, FALSE, FALSE, FALSE), X193 = c(FALSE, FALSE, FALSE, 
    FALSE, FALSE, FALSE), X194 = c(FALSE, FALSE, FALSE, FALSE, 
    FALSE, FALSE), X195 = c(FALSE, FALSE, FALSE, FALSE, FALSE, 
    FALSE), X196 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE
    ), X197 = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)), row.names = c(NA, 
-6L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))

</code></pre>
<pre><code>library(tidymodels)  
library(readr)       
library(broom.mixed) 
library(dotwhisker)  
library(skimr)           
library(rpart.plot)  
library(vip)    
library(glmnet)
library(naniar) 
library(tidyr)
library(dplyr)

# Data cleaning

skool &lt;-
  read_csv(&quot;/Users/riddhimaagupta/Desktop/skul_data_word.csv&quot;)

skool_v1 &lt;- 
  select (school, -c(...1, id,  npsn,   public, cert_est,   cert_ops,   name_clean, name,   muh1,   muh2,   muh,    chr1,   chr2,   chr3,   chr,    hindu,  nu1,    nu2,    nu_klaten,  nu_sby, nu, it1,    it, other_swas_international)) 

str(skool_v1)

skool_v2 &lt;- 
  filter(skool_v1, afiliasi != 99)

str(skool_v2)
glimpse(skool_v2)

skool_v2.1 &lt;- replace_with_na(skool_v2,
                                 replace = list(village = c(&quot;-&quot;)))

skool_v2.2 &lt;- replace_with_na(skool_v2.1,
                              replace = list(area = c(&quot;0&quot;)))

skool_v2.3 &lt;- replace_with_na(skool_v2.2,
                              replace = list(date_est = c(&quot;-&quot;)))

skool_v2.3$date_est &lt;- as.Date(skool_v2.3$date_est, format = '%Y-%m-%d')

skool_v2.3$date_ops &lt;- as.Date(skool_v2.3$date_ops, format = '%Y-%m-%d')

skool_v2.3$latlon &lt;- gsub(&quot;.*\\[&quot;, &quot;&quot;, skool_v2.3$latlon)

skool_v2.3$latlon &lt;- gsub(&quot;\\].*&quot;, &quot;&quot;, skool_v2.3$latlon)

skool_v2.4 &lt;- skool_v2.3 %&gt;%
                     separate(latlon, c(&quot;latitude&quot;, &quot;longitude&quot;), &quot;,&quot;)

skool_v2.4$latitude &lt;- as.numeric(skool_v2.4$latitude)

skool_v2.4$longitude &lt;- as.numeric(skool_v2.4$longitude) 


skool_v3 &lt;- skool_v2.4 %&gt;%
            mutate_if(is.character, tolower) %&gt;%
            mutate_if(is.character, as.factor) 


skool_v4 &lt;- skool_v3 %&gt;%
            mutate_if(is.logical, as.factor)

skool_v4 &lt;- skool_v3 %&gt;%
  mutate_if(is.logical, as.factor)


skool_v5 &lt;- skool_v4 %&gt;%
            na.omit()



# Data splitting 

set.seed(123)
splits      &lt;- initial_split(skool_v5 , strata = afiliasi)
school_train &lt;- training(splits)
school_test  &lt;- testing(splits)

set.seed(234)
val_set &lt;- validation_split(skool_v5, 
                            strata = afiliasi, 
                            prop = 0.80)

# Penalised multinomial regression

lr_mod &lt;- 
  multinom_reg(penalty = tune(), mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;)

lr_recipe &lt;- 
  recipe(afiliasi ~ ., data = school_train) %&gt;%  
  step_date(date_est, date_ops) %&gt;% 
  step_rm(date_est, date_ops) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  step_normalize(all_predictors())

lr_workflow &lt;- 
  workflow() %&gt;% 
  add_model(lr_mod) %&gt;% 
  add_recipe(lr_recipe)


lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30))
lr_reg_grid %&gt;% top_n(-5)
lr_reg_grid %&gt;% top_n(5)

lr_res &lt;- 
  lr_workflow %&gt;% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE,  verbose = TRUE),
            metrics = metric_set(roc_auc))

x validation: preprocessor 1/1: Error in `prep()`:
! Columns must be...
Warning message:
All models failed. See the `.notes` column.
</code></pre>
<p>I don't know how to proceed. I am new at both R and machine learning. Any help will be appreciated!</p>
",48,0,-1,4,r;machine-learning;multinomial;r-recipes,2022-05-27 19:42:30,2022-05-27 19:42:30,2022-05-28 23:40:49,i am trying to run a penalised multinomial regression  it is my understanding that predictor variables can be numerical  however  when i try to tune my model using a validation set  i get the error that says that all my variables must be character or factor  this is the output of example_data  lt   dput head my_dataset   i don t know how to proceed  i am new at both r and machine learning  any help will be appreciated 
454,454,4543175,72414899,fastapi prediction with machine learning model from mlflow works in local but not online on Heroku,"<p>When I test my API in local it is working fine:</p>
<p><a href=""https://i.stack.imgur.com/TXvj9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TXvj9.png"" alt=""enter image description here"" /></a></p>
<p>When I test it online. It is still working fine.</p>
<p><a href=""https://i.stack.imgur.com/FntrH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FntrH.png"" alt=""enter image description here"" /></a></p>
<p>When I make a prediction in local; it is still working fine. It uses a model saved online on MLflow to make the prediction.</p>
<p><a href=""https://i.stack.imgur.com/TIjlM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TIjlM.png"" alt=""enter image description here"" /></a></p>
<p>But when I push my API online and try the same prediction, is it not working anymore.</p>
<p>When checking the status_code, I have an error 500:</p>
<p><a href=""https://i.stack.imgur.com/LBeYY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LBeYY.png"" alt=""enter image description here"" /></a></p>
<p>And when trying to print the answer it says:</p>
<p><a href=""https://i.stack.imgur.com/dOxbj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dOxbj.png"" alt=""enter image description here"" /></a></p>
<p>Any idea why?</p>
<p>Thank you</p>
",49,1,0,3,heroku;fastapi;mlflow,2022-05-28 16:03:18,2022-05-28 16:03:18,2022-05-28 22:26:46,when i test my api in local it is working fine   when i test it online  it is still working fine   when i make a prediction in local  it is still working fine  it uses a model saved online on mlflow to make the prediction   but when i push my api online and try the same prediction  is it not working anymore  when checking the status_code  i have an error    and when trying to print the answer it says   any idea why  thank you
455,455,759880,72409210,How to combine two confusion matrices,"<p>In a machine learning context, I am working on a binary classification problem. There is a source of truth T for labels, and a labeling process A which is not perfect and makes errors compared to T, according to a confusion matrix C(T,A). There is then a second labeling process B, and a second confusion matrix C(A,B) <em>between A and B</em>.</p>
<p>Is it possible to calculate C(T,B) from C(T,A) and C(A,B)? If so, what would that calculation be?</p>
",56,1,0,4,machine-learning;statistics;confusion-matrix;contingency,2022-05-27 23:10:40,2022-05-27 23:10:40,2022-05-28 21:24:05,in a machine learning context  i am working on a binary classification problem  there is a source of truth t for labels  and a labeling process a which is not perfect and makes errors compared to t  according to a confusion matrix c t a   there is then a second labeling process b  and a second confusion matrix c a b  between a and b  is it possible to calculate c t b  from c t a  and c a b   if so  what would that calculation be 
456,456,11663956,70828293,Python: SGDClassifier for X without feature names,"<p>I'm following the examples in the book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.</p>
<p>In chapter 3, the example get mnist_784 dataset via fetch_openml while I got the data set via download the .csv file from kaggle.</p>
<p>The code in the book:</p>
<pre><code>from sklearn.datasets import fetch_openml
from sklearn.linear_model import SGDClassifier

mnist = fetch_openml('mnist_784', version=1)
X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]
some_digit = X[0]
some_digit_image = some_digit.reshape(28, 28)
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
y_train_5 = (y_train == 5)  # True for all 5s, False for all other digits
y_test_5 = (y_test == 5)
sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)
</code></pre>
<p>The openml dataset includes the description, data and target but unfortunately the .csv file doesn't have the same information.</p>
<p>And my code is:</p>
<pre><code>file = r&quot;C:\data\cwd\Jupyter\data\mnist_784.csv&quot;
mnist = pd.read_csv(file)
X = mnist.loc[:, mnist.columns != 'class']
y = mnist.loc[:, 'class']
some_digit = X.iloc[0].tolist()
some_digit_image = np.reshape(some_digit,(28, 28))
X_train, X_test, y_train, y_test = X.iloc[:60000], X.iloc[60000:], y.iloc[:60000], y.iloc[60000:]
y_train_5 = (y_train == 5)
y_test_5 = (y_test == 5)
sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)
</code></pre>
<p>So far all good.
However, when I ran below:</p>
<pre><code>sgd_clf.predict([some_digit])
</code></pre>
<p>I got an warning message:</p>
<pre><code>c:\data\tools\Python\sklearn\base.py:445: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names
</code></pre>
<p>Can I supress the message, or can I attach the feature names to my X?</p>
",62,1,1,2,python;machine-learning,2022-01-24 07:40:59,2022-01-24 07:40:59,2022-05-28 20:37:01,i m following the examples in the book  hands on machine learning with scikit learn  keras  and tensorflow  nd edition  in chapter   the example get mnist_ dataset via fetch_openml while i got the data set via download the  csv file from kaggle  the code in the book  the openml dataset includes the description  data and target but unfortunately the  csv file doesn t have the same information  and my code is  i got an warning message  can i supress the message  or can i attach the feature names to my x 
457,457,9421391,53231997,How to print out Structured Stream in Console format,"<p>I am learning Structured Streaming with Databricks and I'm struggling with the DataStreamWriter console mode.</p>

<p>My program: </p>

<ul>
<li>Simulates the streaming arrival of files to the folder ""monitoring_dir"" (one new file is transferred from ""source_dir"" each 10 seconds). </li>
<li>Uses a DataStreamReader to populate the Unbounded DataFrame ""inputUDF"" with the content of each new file.</li>
<li>Uses a DataStreamWriter to output the new rows of ""inputUDF"" to a valid sink.</li>
</ul>

<p>Whereas the program works when choosing to use a File sink (the batches are appended to text-format files in ""result_dir""), I cannot see anything displayed when choosing Console sink. </p>

<p>Moreover, when I run the equivalent version of the program in my local machine (with Spark installed on it) it works fine both for File and Console sinks. </p>

<p>My question is:</p>

<ul>
<li><strong>How can I make this program to output to Console sink and display the results when using Databricks?</strong> </li>
</ul>

<p>Thank you very much in advance! </p>

<p>Best regards,
Nacho</p>

<hr>

<h2>My Program: myTest.py</h2>

<pre><code>import pyspark
import pyspark.sql.functions

import time

#------------------------------------
# FUNCTION get_source_dir_file_names
#------------------------------------ 
def get_source_dir_file_names(source_dir):

    # 1. We create the output variable
    res = []

    # 2. We get the FileInfo representation of the files of source_dir
    fileInfo_objects = dbutils.fs.ls(source_dir)

    # 3. We traverse the fileInfo objects, to get the name of each file
    for item in fileInfo_objects:      
        # 3.1. We get a string representation of the fileInfo
        file_name = str(item)

        # 3.2. We look for the pattern name= to remove all useless info from the start
        lb_index = file_name.index(""name='"")
        file_name = file_name[(lb_index + 6):]

        # 3.3. We look for the pattern ') to remove all useless info from the end
        ub_index = file_name.index(""',"")
        file_name = file_name[:ub_index]

        # 3.4. We append the name to the list
        res.append(file_name)

    # 4. We sort the list in alphabetic order
    res.sort()

    # 5. We return res
    return res

#------------------------------------
# FUNCTION streaming_simulation
#------------------------------------ 
def streaming_simulation(source_dir, monitoring_dir, time_step_interval):
    # 1. We get the names of the files on source_dir
    files = get_source_dir_file_names(source_dir)

    # 2. We get the starting time of the process
    time.sleep(time_step_interval * 0.1)

    start = time.time()

    # 3. We set a counter in the amount of files being transferred
    count = 0

    # 4. We simulate the dynamic arriving of such these files from source_dir to dataset_dir
    # (i.e, the files are moved one by one for each time period, simulating their generation).
    for file in files:
        # 4.1. We copy the file from source_dir to dataset_dir#
        dbutils.fs.cp(source_dir + file, monitoring_dir + file)

        # 4.2. We increase the counter, as we have transferred a new file
        count = count + 1

        # 4.3. We wait the desired transfer_interval until next time slot.
        time.sleep((start + (count * time_step_interval)) - time.time())

    # 5. We wait a last time_step_interval
    time.sleep(time_step_interval)

#------------------------------------
# FUNCTION my_main
#------------------------------------ 
def my_main():
    # 0. We set the mode
    console_sink = True

    # 1. We set the paths to the folders
    source_dir = ""/FileStore/tables/my_dataset/""
    monitoring_dir = ""/FileStore/tables/my_monitoring/""
    checkpoint_dir = ""/FileStore/tables/my_checkpoint/""
    result_dir = ""/FileStore/tables/my_result/""

    dbutils.fs.rm(monitoring_dir, True)
    dbutils.fs.rm(result_dir, True)
    dbutils.fs.rm(checkpoint_dir, True)  

    dbutils.fs.mkdirs(monitoring_dir)
    dbutils.fs.mkdirs(result_dir)
    dbutils.fs.mkdirs(checkpoint_dir)    

    # 2. We configure the Spark Session
    spark = pyspark.sql.SparkSession.builder.getOrCreate()
    spark.sparkContext.setLogLevel('WARN')    

    # 3. Operation C1: We create an Unbounded DataFrame reading the new content copied to monitoring_dir
    inputUDF = spark.readStream.format(""text"")\
                               .load(monitoring_dir)

    myDSW = None
    # 4. Operation A1: We create the DataStreamWritter...

    # 4.1. To either save to result_dir in append mode  
    if console_sink == False:
        myDSW = inputUDF.writeStream.format(""text"")\
                                    .option(""path"", result_dir) \
                                    .option(""checkpointLocation"", checkpoint_dir)\
                                    .trigger(processingTime=""10 seconds"")\
                                    .outputMode(""append"")   
    # 4.2. Or to display by console in append mode    
    else:
        myDSW = inputUDF.writeStream.format(""console"")\
                                    .trigger(processingTime=""10 seconds"")\
                                    .outputMode(""append"")   

    # 5. We get the StreamingQuery object derived from starting the DataStreamWriter
    mySQ = myDSW.start()

    # 6. We simulate the streaming arrival of files (i.e., one by one) from source_dir to monitoring_dir
    streaming_simulation(source_dir, monitoring_dir, 10)

    # 7. We stop the StreamingQuery to finish the application
    mySQ.stop()    

#-------------------------------
# MAIN ENTRY POINT
#-------------------------------strong text
if __name__ == '__main__':
    my_main()
</code></pre>

<hr>

<h2>My Dataset: f1.txt</h2>

<p>First sentence. </p>

<p>Second sentence. </p>

<hr>

<h2>My Dataset: f2.txt</h2>

<p>Third sentence. </p>

<p>Fourth sentence. </p>

<hr>

<h2>My Dataset: f3.txt</h2>

<p>Fifth sentence. </p>

<p>Sixth sentence. </p>
",1256,2,4,4,python;spark-streaming;databricks;spark-structured-streaming,2018-11-10 00:45:20,2018-11-10 00:45:20,2022-05-28 16:49:47,i am learning structured streaming with databricks and i m struggling with the datastreamwriter console mode  my program   whereas the program works when choosing to use a file sink  the batches are appended to text format files in result_dir   i cannot see anything displayed when choosing console sink   moreover  when i run the equivalent version of the program in my local machine  with spark installed on it  it works fine both for file and console sinks   my question is  thank you very much in advance   first sentence   second sentence   third sentence   fourth sentence   fifth sentence   sixth sentence  
458,458,10518324,72414851,scikit-learn) Why are Ridge Regression scores so low?,"<p>I am studying machine learning using scikit-learn.
I'm using scikit-learn's built-in dataset.
I used Ridge Regression to model a dataset on diabetes, but the score is too low.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import load_diabetes 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import Ridge 

diabetes = load_diabetes()

x_train, x_test, y_train, y_test = train_test_split(diabetes[&quot;data&quot;], diabetes[&quot;target&quot;], test_size=0.2, random_state=2022)

r = Ridge()
r.fit(x_train, y_train)
print(f&quot;train score: {r.score(x_train, y_train)}&quot;)
print(f&quot;test score: {r.score(x_test, y_test)}&quot;)

#train score: 0.42721136852031494
#test score: 0.457945815123009
</code></pre>
<p>Even if the regulation is raised by specifying the alpha value, it does not change significantly. How can I get my score to exceed 80? Which parameters of Ridge() should be adjusted?</p>
",16,0,-1,2,machine-learning;scikit-learn,2022-05-28 15:55:40,2022-05-28 15:55:40,2022-05-28 15:55:40,even if the regulation is raised by specifying the alpha value  it does not change significantly  how can i get my score to exceed   which parameters of ridge   should be adjusted 
459,459,14133190,72389432,Difficulty understanding svm and lr algorithms,"<p>I am interested in the field of machine learning, I tried to understand the following code but I could not. Can anyone explain to me simply? What does the scikit-learn library do in this code?</p>
<pre><code>import pandas as pd
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split

df = pd.read_csv('AMZN.csv')

df = df[['Close']]

forcast = 5

df['prediction'] = df[[&quot;Close&quot;]].shift(-forcast)

x = np.array(df.drop(['prediction'], 1))
x = x[:-forcast]

y = np.array(df['prediction'])
y = y[:-forcast]

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)

mysvr = SVR(kernel='rbf', C=1000, gamma=0.1)
mysvr.fit(xtrain, ytrain)
svcconf = mysvr.score(xtrain, ytrain)
print(f&quot;svm confidence : {svcconf}&quot;)

lr = LinearRegression()
lr.fit(xtrain, ytrain)
lrconf = lr.score(xtest, ytest)
print(f&quot;LR confidence : {lrconf}&quot;)

x_forecast = np.array(df.drop(['prediction'], 1))[-forcast:]
lrpred = lr.predict(x_forecast)
svmpred = mysvr.predict(x_forecast)

print(f&quot;LR Method : {lrpred}&quot;)
print(&quot;---------------------&quot;)
print(f&quot;SVM Method : {svmpred}&quot;)
</code></pre>
<p>As far as I know, this code is used to predict the future of amzn share data</p>
",27,0,-1,2,python;scikit-learn,2022-05-26 14:42:23,2022-05-26 14:42:23,2022-05-28 14:47:16,i am interested in the field of machine learning  i tried to understand the following code but i could not  can anyone explain to me simply  what does the scikit learn library do in this code  as far as i know  this code is used to predict the future of amzn share data
460,460,16644912,72052434,I&#39;m getting this error when trying to run the mlagents-learn command. All the correct versions of the correct packages are installed in a venv,"<p>I'm trying to use this toolkit to get the hang of creating machine learning agents in simulations. It seemed like the easiest one out there, so I followed a tutorial from a channel called CodeMonkey. Does anyone know what is going wrong here?
(<a href=""https://www.youtube.com/watchv=zPFU30tbyKs&amp;ab_channel=CodeMonkey"" rel=""nofollow noreferrer"">https://www.youtube.com/watchv=zPFU30tbyKs&amp;ab_channel=CodeMonkey</a>).</p>
<p>'''</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/bin/mlagents-learn&quot;, line 5, in &lt;module&gt;
    from mlagents.trainers.learn import main
  File &quot;/usr/local/lib/python3.9/site-packages/mlagents/trainers/learn.py&quot;, line 2, in &lt;module&gt;
    from mlagents import torch_utils
  File &quot;/usr/local/lib/python3.9/site-packages/mlagents/torch_utils/__init__.py&quot;, line 1, in &lt;module&gt;
    from mlagents.torch_utils.torch import torch as torch  # noqa
  File &quot;/usr/local/lib/python3.9/site-packages/mlagents/torch_utils/torch.py&quot;, line 6, in &lt;module&gt;
    from mlagents.trainers.settings import TorchSettings
  File &quot;/usr/local/lib/python3.9/site-packages/mlagents/trainers/settings.py&quot;, line 644, in &lt;module&gt;
    class TrainerSettings(ExportableSettings):
  File &quot;/usr/local/lib/python3.9/site-packages/mlagents/trainers/settings.py&quot;, line 667, in TrainerSettings
    cattr.register_structure_hook(
  File &quot;/usr/local/lib/python3.9/site-packages/cattr/converters.py&quot;, line 207, in register_structure_hook
    self._structure_func.register_cls_list([(cl, func)])
  File &quot;/usr/local/lib/python3.9/site-packages/cattr/dispatch.py&quot;, line 55, in register_cls_list
    self._single_dispatch.register(cls, handler)
  File &quot;/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/functools.py&quot;, line 855, in register
    raise TypeError(
TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class.
</code></pre>
<p>'''</p>
",89,1,0,2,python;unity3d,2022-04-29 08:46:31,2022-04-29 08:46:31,2022-05-28 12:45:23,       
461,461,8018614,72413567,IoT Edge offline capabilities in VM,"<p>As I am learning, the IoT Edge's main usage is to process the data coming from various child devices and to send the data back upstream which is IoT Hub. In this way, we can reduce the data sent to IoTHub. And the edge device can be any Virtual Machine on the cloud or an On-premise device like Raspberry Pi inside the factory.</p>
<p>If we use an On-premise device we can send sensor data to it, and it can store those data locally if there is no internet.</p>
<p>So my question here is if I use VM as an IoT Edge device, don't I still need the internet to send data to those VMs? In that case, how does the offline functionality works? Will I be able to send the data to the IoT Edge device if there is no internet?</p>
",66,1,0,3,azure;azure-iot-hub;azure-iot-edge,2022-05-28 12:28:59,2022-05-28 12:28:59,2022-05-28 12:36:02,as i am learning  the iot edge s main usage is to process the data coming from various child devices and to send the data back upstream which is iot hub  in this way  we can reduce the data sent to iothub  and the edge device can be any virtual machine on the cloud or an on premise device like raspberry pi inside the factory  if we use an on premise device we can send sensor data to it  and it can store those data locally if there is no internet  so my question here is if i use vm as an iot edge device  don t i still need the internet to send data to those vms  in that case  how does the offline functionality works  will i be able to send the data to the iot edge device if there is no internet 
462,462,16730954,72413118,Does a data scientist have to learn all the machine learning algorithms or just one of each type?,"<p>Does a data scientist have to learn all the machine learning algorithms or just one of each type?
Mostly YouTubers and websites write that data scientists should have to learn all these algorithms that are followed.</p>
<p>Linear regression,
Logistic regression,
Decision tree,
SVM algorithm,
Naive Bayes algorithm,
KNN algorithm,
K-means,
Random forest algorithm,
Dimensionality reduction algorithms,
Gradient boosting algorithm and AdaBoosting algorithm.</p>
<p>However, I'm a bit confused about this when I took the course of data science from team 365 on udemy they touch me only linear regression, logistic regression, K-mean and neural networks. But after some time mostly I heard about these algorithms that are super important for a data scientist. Is that true or Team 365 is right? Please help.&gt;</p>
",35,1,0,1,data-science,2022-05-28 10:59:39,2022-05-28 10:59:39,2022-05-28 11:15:57,however  i m a bit confused about this when i took the course of data science from team  on udemy they touch me only linear regression  logistic regression  k mean and neural networks  but after some time mostly i heard about these algorithms that are super important for a data scientist  is that true or team  is right  please help  gt 
463,463,4281353,67084512,Tensorflow after 1.15 - No need to install tensorflow-gpu package,"<h1>Question</h1>
<p>Please confirm that to use both CPU and GPU with TensorFlow <strong>after 1.15</strong>, install <strong>tensorflow</strong> package is enough and <strong>tensorflow-gpu</strong> is no more required.</p>
<h1>Background</h1>
<p>Still see articles stating to install <strong>tensorflow-gpu</strong> e.g. <code>pip install tensorflow-gpu==2.2.0</code> and the <a href=""https://pypi.org/project/tensorflow-gpu/"" rel=""nofollow noreferrer"">PyPi repository for tensorflow-gpu package</a> is active with the latest <strong>tensorflow-gpu 2.4.1</strong>.</p>
<p>The Annaconda document also refers to tensorflow-gpu package still.</p>
<ul>
<li><a href=""https://docs.anaconda.com/anaconda/user-guide/tasks/gpu-packages/#tensorflow"" rel=""nofollow noreferrer"">Working with GPU packages - Available packages - TensorFlow</a></li>
</ul>
<blockquote>
<p>TensorFlow is a general machine learning library, but most popular for deep learning applications. There are three supported variants of the tensorflow package in Anaconda, one of which is the NVIDIA GPU version. This is selected by installing the meta-package tensorflow-gpu:</p>
</blockquote>
<p>However, according to the TensorFlow v2.4.1 (as of Apr 2021) Core document <a href=""https://www.tensorflow.org/install/gpu"" rel=""nofollow noreferrer"">GPU support - Older versions of TensorFlow</a></p>
<blockquote>
<p>For releases 1.15 and older, CPU and GPU packages are separate:</p>
</blockquote>
<pre><code>pip install tensorflow==1.15      # CPU
pip install tensorflow-gpu==1.15  # GPU
</code></pre>
<p>According to the TensorFlow Core Guide <a href=""https://www.tensorflow.org/guide/gpu"" rel=""nofollow noreferrer"">Use a GPU</a>.</p>
<blockquote>
<p>TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.</p>
</blockquote>
<p>According to <a href=""https://stackoverflow.com/a/52627105/4281353"">Difference between installation libraries of TensorFlow GPU vs CPU</a>.</p>
<blockquote>
<p>Just a quick (unnecessary?) note... from TensorFlow 2.0 onwards these are not separated, and you simply install tensorflow (as this includes GPU support if you have an appropriate card/CUDA installed).</p>
</blockquote>
<p>Hence would like to have a definite confirmation that the tensorflow-gpu package would be for convenience (legacy script which has specified tensorflow-gpu, etc) only and no more required. There is no difference between tensorflow and tensorflow-gpu packages now.</p>
",1824,1,2,1,tensorflow,2021-04-14 07:28:05,2021-04-14 07:28:05,2022-05-28 05:58:26,please confirm that to use both cpu and gpu with tensorflow after    install tensorflow package is enough and tensorflow gpu is no more required  still see articles stating to install tensorflow gpu e g  pip install tensorflow gpu     and the  is active with the latest tensorflow gpu     the annaconda document also refers to tensorflow gpu package still  tensorflow is a general machine learning library  but most popular for deep learning applications  there are three supported variants of the tensorflow package in anaconda  one of which is the nvidia gpu version  this is selected by installing the meta package tensorflow gpu  however  according to the tensorflow v    as of apr   core document  for releases   and older  cpu and gpu packages are separate  according to the tensorflow core guide   tensorflow code  and tf keras models will transparently run on a single gpu with no code changes required  according to   just a quick  unnecessary   note    from tensorflow   onwards these are not separated  and you simply install tensorflow  as this includes gpu support if you have an appropriate card cuda installed   hence would like to have a definite confirmation that the tensorflow gpu package would be for convenience  legacy script which has specified tensorflow gpu  etc  only and no more required  there is no difference between tensorflow and tensorflow gpu packages now 
464,464,3937811,72370250,How to process a CSV based dataset with KerasClassifier from ART,"<p>I am writing a machine learning program that will leverage:</p>
<pre><code>from art.estimators.classification import KerasClassifier
</code></pre>
<p>The goal of this program is to analyze the <a href=""https://www.stratosphereips.org/datasets-iot23"" rel=""nofollow noreferrer"">IoT-23</a> dataset. A sample of the data is shown below:</p>
<pre><code>proto=udp,duration,orig_bytes,resp_bytes,conn_state=SF,conn_state=S0,conn_state=OTH,conn_state=S1,missed_bytes,history=ShADadttfF,history=S,history=ShADadtfF,history=Dd,history=D,history=DdA,history=DdAaFf,history=ShADda,orig_pkts,orig_ip_bytes,resp_pkts,resp_ip_bytes,label
0,1.686784,149,171750,1,0,0,0,11584,1,0,0,0,0,0,0,0,122,7741,122,178102,Malicious
0,3.081233,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,3,180,0,0,Malicious
0,?,?,?,0,1,0,0,0,0,1,0,0,0,0,0,0,1,60,0,0,Malicious
</code></pre>
<p>My implementation is shown below:</p>
<pre><code>import csv
import logging
from pickletools import optimize
from pprint import pprint
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adam
from loggingModule import LoggingHandler

import numpy as np

from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import KerasClassifier
from art.utils import load_mnist

logging.debug(f'Step 1: Load the MNIST dataset')
exampleFile   = open('./data/CTU-44-1.csv')
exampleReader = csv.reader(exampleFile)
exampleData   = list(exampleReader)
(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = exampleData
# Logging out the return values from mnist
# logging.debug(f'(x_train:{x_train})')
# logging.debug(f'(y_train:{y_train})')
# logging.debug(f'(x_test: {x_test}) ')
# logging.debug(f'(x_test: {y_test}) ')
# logging.debug(f'(min_pixel_value: {min_pixel_value})')
# logging.debug(f'(max_pixel_value: {max_pixel_value}')

logging.debug(f'Step 2: Create the model')
model = Sequential()
logging.debug(f'model.add(Conv2D(filters={4}, kernel_size=({5},{5}), strides={1}, activation=\'relu\', input_shape=({28},{28},{1})))')
model.add(Conv2D(filters=4, kernel_size=(5,5), strides=1, activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=10, kernel_size=(5,5), strides=1, activation=&quot;relu&quot;, input_shape=(23,23,4)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(100,activation=&quot;relu&quot;))
model.add(Dense(10,activation=&quot;softmax&quot;))


logging.debug(f'Step 3: Create the ART classifier')
model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.01), metrics=[&quot;accuracy&quot;])

classifier = KerasClassifier(model=model, clip_values=(min_pixel_value,max_pixel_value), use_logits=False)

logging.debug(f'Step 4: Train the ART classifier')
classifier.fit(x_train, y_train, batch_size=64, nb_epochs=3)

logging.debug(f'Step 5: Evaluate the ART classifer on benign test examples')
predictions = classifier.predict(x_test)
accurracy   = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) /len(y_test)

logging.debug('Step 6: Generate adversarial test examples')
attack     = FastGradientMethod(estimator=classifier, eps=0.2)
x_test_adv = attack.generate(x=x_test)
</code></pre>
<p>At each step I log the result.</p>
<p>Actual</p>
<pre><code>python3 keras_example.py 
2022-05-24 18:44:44,861 - INFO - set ART_DATA_PATH to /Users/evangertis/.art/data
2022-05-24 18:44:46,576 - DEBUG - Step 1: Load the MNIST dataset
Traceback (most recent call last):
  File &quot;/Users/evangertis/development/PythonAutomation/ART_examples/keras_example.py&quot;, line 24, in &lt;module&gt;
    (x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = exampleData
ValueError: too many values to unpack (expected 4)
</code></pre>
<p>Expected: the model should return an accuracy score.</p>
",15,0,0,3,python;keras;adversarial-machines,2022-05-25 04:20:49,2022-05-25 04:20:49,2022-05-28 05:36:07,i am writing a machine learning program that will leverage  the goal of this program is to analyze the  dataset  a sample of the data is shown below  my implementation is shown below  at each step i log the result  actual expected  the model should return an accuracy score 
465,465,17558180,72406752,ESP32 MQTT SSL Connection Guidance,"<p>Hope you are doing good,
I'm in need of a guidance of securely connecting a ESP32 to a MQTT Broker with SSL Certificate with a Static IP
Here are the steps i done:
have set an static ip to my ESP32 successfully,
have copied my cert into arduino,
trouble i'm facing is with:
can't connect to MQTTS Broker with port 8883 and username,password
here's the link that i follwed as guidance</p>
<p>IoT Sharing</p>
<p>Demo 30: How to use Arduino ESP32 MQTTS with MQTTS Mosquitto broker (TLS/SSL)
This is a place where I can share my knowledge of: IoT, machine learning self learning and other interetsing topics.</p>
<p>Pls anyone can guide me in this,...
here's the code i used:</p>
<pre><code>#include &lt;WiFi.h&gt;
 #include &lt;WiFiClientSecure.h&gt;
 #include &lt;PubSubClient.h&gt;
 const char* ssid = &quot;xxxxx&quot;;
 const char* password = &quot;xxxxxxx&quot;;
 IPAddress local_IP(xx,xx,xx,xx);
 IPAddress gateway(xx,xx,xx,xx);
 IPAddress subnet(xx,xx,xx,xx);
 const char* mqttServer = &quot;xx.xx.xx.xx&quot;;
 const int mqttPort = 8883;
 const char* mqttUser = &quot;xxxxx&quot;;
 const char* mqttPassword = &quot;xxxxxxxxxx&quot;;
 const char* ca_cert = \
 &quot;-----BEGIN CERTIFICATE-----\n&quot;
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 &quot;-----END CERTIFICATE-----\n&quot;;
WiFiClientSecure client;
PubSubClient mqttclient();
void callback(char* topic, byte* payload, unsigned int length) {
Serial.print(&quot;Message arrived in topic: &quot;);
Serial.println(topic);
Serial.print(&quot;Message:&quot;);
for (int i = 0; i &lt; length; i++) {
Serial.print((char)payload[i]);
 }
Serial.println();
Serial.println(&quot;-----------------------&quot;);
}
void setup() {
Serial.begin(115200);
if (!WiFi.config(local_IP, gateway, subnet)) {
Serial.println(&quot;STA Failed to configure&quot;);
WiFi.begin(ssid, password);
while (WiFi.status() != WL_CONNECTED) {
delay(500);
Serial.println(&quot;Connecting to WiFi..&quot;);
}
Serial.println(&quot;Connected to the WiFi network&quot;);
client.setCACert(ca_cert);
mqttclient.setServer(mqttServer, mqttPort);
mqttclient.setCallback(callback);
while (!mqttclient.connected()) {
Serial.println(&quot;Connecting to MQTT...&quot;);
if (mqttclient.connect(&quot;ESP32Client&quot;(mqttUser,mqttPassword))) {
Serial.println(&quot;connected&quot;);
} else {
Serial.print(&quot;failed with state &quot;);
Serial.print(mqttclient.state());
delay(2000);
}
}
mqttclient.publish(&quot;Sri&quot;,&quot;HelloWorld&quot;);
mqttclient.subscribe(&quot;Sri&quot;);
}
void loop() {
client.loop();
}
</code></pre>
<p>Pls help me in troubleshooting this code to work fine</p>
<p>Kind Regards,
Sri Haran</p>
",51,0,-1,5,ssl;arduino-uno;arduino-ide;arduino-c++;arduino-esp32,2022-05-27 19:42:08,2022-05-27 19:42:08,2022-05-27 19:42:08,iot sharing pls help me in troubleshooting this code to work fine
466,466,4032703,72383850,How to use KNeighborsClassifier user defined weight function based on the predicted class?,"<p>I was trying to analyze the <a href=""https://archive.ics.uci.edu/ml/datasets/Arrhythmia"" rel=""nofollow noreferrer"">UCI Machine Learning Repository Arrhythmia data set</a> using the K-Nearest Neighbors algorithm. In particular, I am doing exercise 1 from chapter 6 of <em>Outlier Analysis</em> by Charu C. Aggarwal for self-learning (<em>not</em> class homework):</p>
<blockquote>
<ul>
<li>Implement a 20-nearest neighbor classifier which classifies the majority class as the primary label. use a 3 : 1 ratio of costs between the normal class, and any other minority cost. Determine the overall accuracy and the cost-weighted accuracy.<br></li>
<li>Implement the same algorithm as above, except that each data point is given a weight, which is proportional to its cost. Determine the overall accuracy and the cost-weighted accuracy.</li>
</ul>
</blockquote>
<p>The text gives the following directions for implementing this assignment:</p>
<blockquote>
<p><strong>Proximity-based Classifiers.</strong> In nearest neighbor classifiers, the classification label of a test instance is defined to be the majority class from its k nearest neighbors. In the context of <em>cost-sensitive</em> classification, the <em>weighted</em> majority label is reported as the relevant one, where the weight of an instance from class <em>i</em> is denoted by <em>c_i</em>. Thus, fewer examples of the rare class need to be present in a neighborhood of a test instance, in order for it to be reported as the relevant one. In a practical implementation, the number of <em>k</em>-nearest neighbors for each class can be multiplied with the corresponding cost for that class.</p>
</blockquote>
<p>I was trying to implement this with SciKit-Learn's <code>KNeighborsClassifier</code> class. This class has several choices for a weight function, including a callable user-defined weight function. I was trying to use this, but I got stuck.</p>
<p>I tried reading the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier"" rel=""nofollow noreferrer"">documentation</a>, but it unfortunately contains almost no details about how the callable weight function is supposed to actually work and I've been struggling to find any other documentation or examples that show more details about this (even after searching for several hours).</p>
<p>I tried following <a href=""https://www.tutorialspoint.com/scikit_learn/scikit_learn_kneighbors_classifier.htm"" rel=""nofollow noreferrer"">this tutorial</a>; it helped me understand how to use this class &quot;in general&quot; but it unfortunately didn't explain how to use the callable weight function.</p>
<p>I read <a href=""https://stackoverflow.com/questions/17327880/scikit-learn-user-defined-weights-function-for-kneighborsclassifier"">this Q&amp;A</a> about user defined weight functions, but it unfortunately didn't explain how to change the weights based on the class; rather, that Q&amp;A used hardcoded weights. (I don't understand the point of doing that, but that's a different question).</p>
<p>Here's the code I tried:</p>
<pre><code>import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

df = pd.read_csv('C:\\data\\arrhythmia.csv')

for column in df.columns:
    df[column] = df[column].fillna(df[column].mean())

X = df.loc[:, df.columns[0:-2]]
Y = df.loc[:, df.columns[-1]]

# 136 testing examples, 316 training examples, 13 distinct classes
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# This is the part I'm stuck on
def get_weights(distances):
    print(distances)
    return distances

knn = KNeighborsClassifier(n_neighbors = 20, weights = get_weights)

# Strangely enough, this does NOT appear to result in get_weights being called
knn.fit(X_train, y_train)

# This DOES result in get_weights being called
# It results in the function being called a single time with 136 rows and 20 columns
y_pred = knn.predict(X_test)

accuracy = metrics.accuracy_score(y_test, y_pred)

confusion = metrics.confusion_matrix(y_test, y_pred)

print(accuracy)
print(confusion)
</code></pre>
<p>When I stepped through he code with a debugger, I found that the <code>get_weights</code> function gets called a single time when I do a prediction (but <em>not</em> when I fit). The <code>distances</code> parameter contains a 136x20 array (because I have 136 testing examples and specified 20 nearest neighbors). However, it contains only the pre-computed distances to the nearest neighbors that the algorithm selected, with no indication as to what the neighbors are or which class the selected neighbors are, so this function doesn't help me in my task at all.</p>
<p>Unless I'm misreading the directions in the textbook, my understanding of what I need to do is to be able to compute the pairwise distance to each potential neighbor; if it's an infrequent class, I would just use 1/3 of the Euclidean Distance (to make the distant classes appear closer); otherwise, I would like to just use the Euclidean Distance.</p>
<p>One other possible interpretation (and this is where the directions seemed a bit ambiguous to me) is that a vote for an infrequent class &quot;count&quot; 3 times as much as votes for frequent class. So, 6 votes for an infrequent class and 14 for a frequent class would actually result in the infrequent class &quot;winning.&quot;</p>
<p>Can someone explain how I can use the callable function like this? Or am I misunderstanding what I need to do entirely?</p>
",23,0,0,5,python;machine-learning;scikit-learn;nearest-neighbor;outliers,2022-05-26 02:03:49,2022-05-26 02:03:49,2022-05-27 18:28:52,i was trying to analyze the  using the k nearest neighbors algorithm  in particular  i am doing exercise  from chapter  of outlier analysis by charu c  aggarwal for self learning  not class homework   the text gives the following directions for implementing this assignment  proximity based classifiers  in nearest neighbor classifiers  the classification label of a test instance is defined to be the majority class from its k nearest neighbors  in the context of cost sensitive classification  the weighted majority label is reported as the relevant one  where the weight of an instance from class i is denoted by c_i  thus  fewer examples of the rare class need to be present in a neighborhood of a test instance  in order for it to be reported as the relevant one  in a practical implementation  the number of k nearest neighbors for each class can be multiplied with the corresponding cost for that class  i was trying to implement this with scikit learn s kneighborsclassifier class  this class has several choices for a weight function  including a callable user defined weight function  i was trying to use this  but i got stuck  i tried reading the   but it unfortunately contains almost no details about how the callable weight function is supposed to actually work and i ve been struggling to find any other documentation or examples that show more details about this  even after searching for several hours   i tried following   it helped me understand how to use this class  in general  but it unfortunately didn t explain how to use the callable weight function  i read  about user defined weight functions  but it unfortunately didn t explain how to change the weights based on the class  rather  that q amp a used hardcoded weights   i don t understand the point of doing that  but that s a different question   here s the code i tried  when i stepped through he code with a debugger  i found that the get_weights function gets called a single time when i do a prediction  but not when i fit   the distances parameter contains a x array  because i have  testing examples and specified  nearest neighbors   however  it contains only the pre computed distances to the nearest neighbors that the algorithm selected  with no indication as to what the neighbors are or which class the selected neighbors are  so this function doesn t help me in my task at all  unless i m misreading the directions in the textbook  my understanding of what i need to do is to be able to compute the pairwise distance to each potential neighbor  if it s an infrequent class  i would just use   of the euclidean distance  to make the distant classes appear closer   otherwise  i would like to just use the euclidean distance  one other possible interpretation  and this is where the directions seemed a bit ambiguous to me  is that a vote for an infrequent class  count   times as much as votes for frequent class  so   votes for an infrequent class and  for a frequent class would actually result in the infrequent class  winning   can someone explain how i can use the callable function like this  or am i misunderstanding what i need to do entirely 
467,467,19213312,72403849,Do we need To do Feature Scaling In Flask App,"<p>Hi i know it can sound a bit dumb question but I am new in Machine Learning and want to know is it required to do standardization of inputs from forms in a flask app before giving variables to model.predict() if my model is trained with scaled data.</p>
",17,0,-2,1,python,2022-05-27 15:45:50,2022-05-27 15:45:50,2022-05-27 18:16:16,hi i know it can sound a bit dumb question but i am new in machine learning and want to know is it required to do standardization of inputs from forms in a flask app before giving variables to model predict   if my model is trained with scaled data 
468,468,13214782,72399746,How to check the model prediction with original values in machine learning,"<p>I have trained my model using machine learning and want to check with original value. Is i am doing it right?
As whenever I change the numbers in 'value' getting the same result.</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(normalize(df4), y, test_size=0.2, random_state=0)

rfc1=RandomForestClassifier( random_state=0, max_features='auto', n_estimators= 90, max_depth=8, criterion='gini' )
rfc1.fit(X_train, y_train)

value=[2.60,1.0,3.0,19.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0]
print(rfc1.predict([value]))

y_pred=rfc1.predict(X_test)

print(metrics.classification_report(y_test,y_pred))
df_cm = pd.DataFrame(confusion_matrix(y_test,y_pred), index = [i for i in ['Avg Marks','Good Marks','Bad Marks']],
                  columns = [i for i in ['Avg Marks','Good Marks','Bad Marks']])
plt.figure(figsize = (4,2))
sn.heatmap(df_cm, annot=True, fmt='g')
</code></pre>
<p>Model Accuracy is good but still getting always &quot;Hight chances of Good Marks&quot;</p>
<pre><code>['High Chances of Good Marks']
                            precision    recall  f1-score   support

             Average Marks       0.80      0.83      0.82        59
 High Chances of Bad Marks       0.81      0.72      0.76        18
High Chances of Good Marks       0.86      0.86      0.86        50

                  accuracy                           0.83       127
                 macro avg       0.83      0.80      0.81       127
              weighted avg       0.83      0.83      0.83       127

</code></pre>
<p>Original data looks like this
<a href=""https://i.stack.imgur.com/i0f6x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i0f6x.png"" alt=""enter image description here"" /></a></p>
",34,2,0,5,python;pandas;machine-learning;data-science;random-forest,2022-05-27 07:39:03,2022-05-27 07:39:03,2022-05-27 18:07:42,model accuracy is good but still getting always  hight chances of good marks 
469,469,14282714,72403090,Losing variables while fixing runtime in Google Colab,"<p>I am training a machine learning model in google collab and fixed the automatically disconnecting by using this code in the console inspector view of this question (<a href=""https://stackoverflow.com/questions/54057011/google-colab-session-timeout"">Google Colab session timeout</a>):</p>
<pre><code>function ClickConnect(){
console.log(&quot;Working&quot;); 
document.querySelector(&quot;#top-toolbar &gt; colab-connect-button&quot;).shadowRoot.querySelector(&quot;#connect&quot;).click();
}
var clicker = setInterval(ClickConnect,60000);
</code></pre>
<p>However, after a given amount of time, all of my variables become undefined, which I require after the model has been trained. Is there any way to avoid this?</p>
",67,0,0,1,google-colaboratory,2022-05-27 14:42:44,2022-05-27 14:42:44,2022-05-27 14:42:44,i am training a machine learning model in google collab and fixed the automatically disconnecting by using this code in the console inspector view of this question     however  after a given amount of time  all of my variables become undefined  which i require after the model has been trained  is there any way to avoid this 
470,470,13900384,72389771,Confusion matrix for portion of data,"<p>I am running several machine learning models using Python and can get accuracy and confusion matrix (CM) for a given training data.
However, I am interested if there is a way to get CM for a portion of test data?</p>
<p>For example, if I am classifying 'cats' and 'dogs', and only want to see what 'small animals' were correctly classified?</p>
<p>So running model on train and checking CM just for part of test data.
Here is a rough visualisation, tried creating new df for just 'small' animals but CM is still for the whole thing.</p>
<p>Many thanks!</p>
<p>Please check image, it illustrates it better than my description!
<a href=""https://i.stack.imgur.com/sc6g5.jpg"" rel=""nofollow noreferrer"">classification example</a></p>
",23,0,1,3,python;class;text-classification,2022-05-26 15:10:27,2022-05-26 15:10:27,2022-05-27 13:32:47,for example  if i am classifying  cats  and  dogs   and only want to see what  small animals  were correctly classified  many thanks 
471,471,12051503,72399408,How to get list of compute instance size under Azure Machine Learning and Azure Databricks?,"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=""https://docs.microsoft.com/en-us/azure/governance/resource-graph/reference/supported-tables-resources#resources"" rel=""nofollow noreferrer"">documentation</a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices/</code>(not classic studio) and <code>Microsoft.Databricks/workspaces</code>.</p>
<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning/Azure Databricks.</p>
<pre><code>Resources
| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)
| where type =~ 'Microsoft.Compute/virtualMachines'
| order by name desc
</code></pre>
",48,1,0,5,azure;size;azure-databricks;azure-machine-learning-service;azure-resource-graph,2022-05-27 06:25:49,2022-05-27 06:25:49,2022-05-27 10:12:21,goal here is to query a list of frequently used compute instance size under azure machine learning and azure databricks using azure resource graph explorer from azure portal using kusto query  from the  here  there is a list of resources can be queried but there isn t any compute under microsoft machinelearningservices  not classic studio  and microsoft databricks workspaces  below is what was tried  to get vm instance size but not showing what we have under azure machine learning azure databricks 
472,472,19209890,72398476,Python Meteostat running on SQL Server keeps throwing errors,"<p>I am trying to get historical wheather data from Meteostat using Python in SQL Server. When executing the code I am getting the same error.</p>
<p>This code in pycharm works properly.</p>
<p>I have tried solutions from this site but none of them worked
<a href=""https://docs.microsoft.com/en-us/sql/machine-learning/install/sql-server-machine-learning-services-2019?view=sql-server-ver15#file-permissions"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/machine-learning/install/sql-server-machine-learning-services-2019?view=sql-server-ver15#file-permissions</a></p>
<p>Windows 10 64bit
-Microsoft SQL Server Management Studio 18
-Python 3.7</p>
<p>CODE</p>
<pre><code> EXECUTE sp_execute_external_script 
  @language = N'Python',
  @script = N'

from datetime import datetime
from meteostat import Point, Daily

start = datetime(2008, 11, 1)
end = datetime(2008, 12, 31)

location = Point(49.2497, -123.1193)

data = Daily(location, start, end)
data = data.fetch()

print(data)
'
</code></pre>
<p>ERROR:</p>
<pre><code>Msg 39004, Level 16, State 20, Line 0
A 'Python' script error occurred during execution of 'sp_execute_external_script' with HRESULT 0x80004004.
Msg 39019, Level 16, State 2, Line 0
An external script error occurred: 

Error in execution.  Check the output for more information.
Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 5, in &lt;module&gt;
  File &quot;C:\ProgramData\MSSQLSERVER\Temp-PY\Appcontainer1\9B2DF73F-42E0-4108-A253-7CA33EA0AA58\sqlindb_0.py&quot;, line 49, in transform
    data = Daily(location, start, end)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\interface\daily.py&quot;, line 100, in __init__
    self._init_time_series(loc, start, end, model, flags)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\interface\timeseries.py&quot;, line 167, in _init_time_series
    stations = loc.get_stations('daily', start, end, model)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\interface\point.py&quot;, line 76, in get_stations
    stations = Stations()

Msg 39019, Level 16, State 2, Line 0
An external script error occurred: 
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\interface\stations.py&quot;, line 111, in __init__
    self._load()
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\interface\stations.py&quot;, line 96, in _load
    True)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\meteostat\core\loader.py&quot;, line 90, in load_handler
    parse_dates=parse_dates
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\io\parsers.py&quot;, line 678, in parser_f
    return _read(filepath_or_buffer, kwds)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\io\parsers.py&quot;, line 440, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)

Msg 39019, Level 16, State 2, Line 0
An external script error occurred: 
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\io\parsers.py&quot;, line 787, in __init__
    self._make_engine(self.engine)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\io\parsers.py&quot;, line 1014, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\io\parsers.py&quot;, line 1708, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File &quot;pandas\_libs\parsers.pyx&quot;, line 487, in pandas._libs.parsers.TextReader.__cinit__
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\pandas\core\dtypes\common.py&quot;, line 2013, in pandas_dtype
    npdtype = np.dtype(dtype)
TypeError: data type &quot;string&quot; not understood


Msg 39019, Level 16, State 2, Line 0
An external script error occurred: 
SqlSatelliteCall error: Error in execution.  Check the output for more information.
SqlSatelliteCall function failed. Please see the console output for more information.
Traceback (most recent call last):
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\revoscalepy\computecontext\RxInSqlServer.py&quot;, line 605, in rx_sql_satellite_call
    rx_native_call(&quot;SqlSatelliteCall&quot;, params)
  File &quot;C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\revoscalepy\RxSerializable.py&quot;, line 375, in rx_native_call
    ret = px_call(functionname, params)
RuntimeError: revoscalepy function failed.
</code></pre>
",26,0,0,5,python;sql;sql-server;sp-executesql;external-script,2022-05-27 03:33:29,2022-05-27 03:33:29,2022-05-27 03:37:21,i am trying to get historical wheather data from meteostat using python in sql server  when executing the code i am getting the same error  this code in pycharm works properly  code error 
473,473,16588472,72398020,iteration per second attribute in tqdm library,"<p>this is the first time I'm working with tqdm library in a machine learning project on google colab.
I'm using Transformer's trainer and I'm getting a progress bar for my training process with tqdm but I want to know what does it/sec depend on?
because each time I start training I get different it/sec which gradually changes through the training process and sometimes it's very small so it can take 1-2 hours up to 24 hours depending on what it/sec I got!</p>
<p>is there a way to improve it/sec so it can always be bigger than 1 it/sec?</p>
<p>appreciate any clarification.</p>
",46,0,-1,3,python;google-colaboratory;tqdm,2022-05-27 02:38:18,2022-05-27 02:38:18,2022-05-27 02:48:48,is there a way to improve it sec so it can always be bigger than  it sec  appreciate any clarification 
474,474,1639594,68386650,Can one use Terraform templates with SageMaker ML Pipelines?,"<p>SageMaker has ML Pipelines that come with “ML templates”, which I assume are Cloud Formation templates for machine learning pipelines.</p>
<p>Can one use custom Terraform templates instead of Cloud Formation? Where does one place the Terraform templates? Can this be done through the SageMaker UI?</p>
",1062,3,0,5,machine-learning;terraform;amazon-cloudformation;amazon-sagemaker;aws-codepipeline,2021-07-15 06:33:00,2021-07-15 06:33:00,2022-05-27 02:07:14,sagemaker has ml pipelines that come with  ml templates   which i assume are cloud formation templates for machine learning pipelines  can one use custom terraform templates instead of cloud formation  where does one place the terraform templates  can this be done through the sagemaker ui 
475,475,6024683,72396872,How to forecast macro trend by multiple index by LSTM model?,"<p>I just start exploring machine learning world. I want to try predicting the macro economic trend by grouping different index futures by LSTM model. After reading many article, I have came up 2 approaches below. May I ask what is the best approach?</p>
<pre><code>1. In the pre-processing stage, group the Index futures (e.g. S&amp;P 500, Dow Jones, Nasdaq 100, FTSE 100 etc) and get the average price. Adding a extra column holding the average price of 2 days after.
data structure: 
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>avg price</th>
<th>T+2 avg price</th>
</tr>
</thead>
</table>
</div>
<pre><code>2. Simply random pick one index futures and adding a extra column holding its average price of 2 days after.
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date</th>
<th>S&amp;P</th>
<th>RTY</th>
<th>DJ</th>
<th>FESX</th>
<th>NK</th>
<th>S&amp;P +2</th>
</tr>
</thead>
</table>
</div>",7,0,0,5,machine-learning;deep-learning;lstm;prediction;trend,2022-05-27 00:40:50,2022-05-27 00:40:50,2022-05-27 00:40:50,i just start exploring machine learning world  i want to try predicting the macro economic trend by grouping different index futures by lstm model  after reading many article  i have came up  approaches below  may i ask what is the best approach 
476,476,16128336,72395877,"SGDClassifier with no featured names, hands on ml","<p>I was reading the chapter 3 of the book &quot;Hands-On Machine Learning with Scikit-Learn and TensorFlow&quot; and i'm having a lot of bugs in my code, i'm using google colab but i'm getting an error when i try predicting to see if my training was successful</p>
<pre><code>from sklearn.datasets import fetch_openml
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import SGDClassifier

mnist = fetch_openml(&quot;mnist_784&quot;,version=1)

x,y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]

some_digit = x.to_numpy()[0] # i had to use that otherwise the x remains as a string

y = y.astype(np.uint8)

x_train,x_test,y_train,y_test = x[:60000], x[60000:], y[:60000], y[60000:]

y_train_5 = (y_train == 5)
y_test_5 = (y_test == 5)


sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(x_train,y_train_5)

sgd_clf.predict([some_digit]) # the line that returns me the error
</code></pre>
<p>The last line returns me the following error in the image. <a href=""https://i.stack.imgur.com/cd4QA.png"" rel=""nofollow noreferrer"">image showing the prediction error</a></p>
<p>ERROR:
<code>/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names &quot;X does not have valid feature names, but&quot; array([ True])</code></p>
",19,0,0,3,python;tensorflow;scikit-learn,2022-05-26 23:05:52,2022-05-26 23:05:52,2022-05-26 23:08:04,i was reading the chapter  of the book  hands on machine learning with scikit learn and tensorflow  and i m having a lot of bugs in my code  i m using google colab but i m getting an error when i try predicting to see if my training was successful the last line returns me the following error in the image  
477,477,8515317,65447548,"Installing Graphlab create on Anaconda on Windows 10 (Python, Machine Learning)","<p>I'm following <a href=""https://youtu.be/aXec_IqKA74"" rel=""nofollow noreferrer"">this tutorial</a> on youtube to install graphlab creat, but when I try to go to <strong><a href=""https://turi.com/download/install-graphlab-create-command-line.html"" rel=""nofollow noreferrer"">https://turi.com/download/install-graphlab-create-command-line.html</a></strong> and create an account on graphlab create and get a license to complete the installation I get this github page!</p>
<p>What is wrong? Is graphlab create no loger used?
What are the new steps I need to follow to install graphlab create?</p>
",453,1,3,4,python;anaconda;graphlab;turi,2020-12-25 16:42:31,2020-12-25 16:42:31,2022-05-26 22:56:19,i m following  on youtube to install graphlab creat  but when i try to go to  and create an account on graphlab create and get a license to complete the installation i get this github page 
478,478,19206631,72392069,Validate first name and last name using a deep learning or machine learning model,"<p>I have around 1 million data points of first name and last name.
These names could be valid ones , for example : 'David Beckham' or invalid like - 'rockstar123' or 'new mutant'. Is there any deep learning / ML model which will allow me to differentiate amongst the 2 ?</p>
",24,1,0,3,machine-learning;deep-learning;nlp,2022-05-26 18:17:29,2022-05-26 18:17:29,2022-05-26 21:28:20,
479,479,3607698,52567821,spark assign column name for withColumn function from variable fields,"<p>I have some json data like below, I need to create new columns based on the some Jason values</p>
<pre class=""lang-json prettyprint-override""><code>{ &quot;start&quot;: &quot;1234567679&quot;, &quot;test&quot;: [&quot;abc&quot;], &quot;value&quot;: 324, &quot;end&quot;: &quot;1234567689&quot; }

{ &quot;start&quot;: &quot;1234567679&quot;, &quot;test&quot;: [&quot;xyz&quot;], &quot;value&quot;: &quot;Near&quot;, &quot;end&quot;: &quot;1234567689&quot;}

{ &quot;start&quot;: &quot;1234568679&quot;, &quot;test&quot;: [&quot;pqr&quot;], &quot;value&quot;: [&quot;Attr&quot;,&quot; &quot;], &quot;end&quot;:&quot;1234568679&quot;}  

{ &quot;start&quot;: &quot;1234568997&quot;, &quot;test&quot;: [&quot;mno&quot;], &quot;value&quot;: [&quot;{\&quot;key\&quot;: \&quot;1\&quot;, \&quot;value\&quot;: [\&quot;789\&quot;]}&quot; ], &quot;end&quot;: &quot;1234568999&quot;} 
</code></pre>
<p>above is the json example</p>
<p>I want to create a column like below</p>
<pre><code> start      abc     xyz    pqr     mno    end
 1234567679 324     null   null    null   1234567689
 1234567889 null    Near   null    null   1234567989
 1234568679 null    null   attr    null   1234568679
 1234568997 null    null   null    789    1234568999
</code></pre>
<pre class=""lang-scala prettyprint-override""><code>def getValue1(s1: Seq[String], v: String) = {
     if (s1(0)==&quot;abc&quot;))  v else null
} 
 
def getValue2(s1: Seq[String], v: String) = {
    if (s1(0)==&quot;xyz&quot;))  v else null
}  

val df = spark.read.json(&quot;path to json&quot;)

val tdf = df.withColumn(&quot;abc&quot;,getValue1($&quot;test&quot;, $&quot;value&quot;)).withColumn(&quot;xyz&quot;,getValue2($&quot;test&quot;, $&quot;value&quot;))
</code></pre>
<p>But this  i dont want to use because my test values are more, I want some function  do something like this</p>
<pre class=""lang-scala prettyprint-override""><code>def getColumnname(s1: Seq[String]) = {
    return s1(0)
}  

val tdf = df.withColumn(getColumnname($&quot;test&quot;),$&quot;value&quot;))
</code></pre>
<p>is it good idea to change the values to columns, I want like this because I need to apply this on some Machine learning code which needs plain columns</p>
",832,1,1,2,apache-spark;apache-spark-sql,2018-09-29 16:46:09,2018-09-29 16:46:09,2022-05-26 20:32:22,i have some json data like below  i need to create new columns based on the some jason values above is the json example i want to create a column like below but this  i dont want to use because my test values are more  i want some function  do something like this is it good idea to change the values to columns  i want like this because i need to apply this on some machine learning code which needs plain columns
480,480,19206156,72392579,SciKeras - RandomizedSearchCV for best hyper-parameters,"<p>I'm trying to follow the example on chapter 10 of the book <em>Hands-On Machine Learning with SciKit-Learn, Keras and TensorFlow</em> which regard the optimization of the hyperparameters of a DNN model.</p>
<p>The dataset is the MNIST fashion model and the goal of the project is the classification of the images in 10 classes.</p>
<pre><code>(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()
</code></pre>
<p>There is no validation, so I'm going to create that set using the first 5k elements:</p>
<pre><code>X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
</code></pre>
<p>A possible implementation of a simple DNN model is the following, using sequential Keras API:</p>
<pre><code>model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation=&quot;relu&quot;))
model.add(keras.layers.Dense(200, activation=&quot;relu&quot;))
model.add(keras.layers.Dense(100, activation=&quot;relu&quot;))
model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;))
</code></pre>
<p>The book then suggests to study the hyper-parameter space to found the best ones, using <code>RandomizedSearchCV</code>. The example uses <code>keras.wrappers.scikit_learn.KerasRegressor</code> which is now deprecated in favor of <code>KerasRegressor</code> by SciKeras.</p>
<p>I created a function containing the ML model:</p>
<pre><code>input_shape=X_train[0].shape
def build_model(n_hidden = 1, n_neurons = 30, learning_rate=3e-3, input_shape=input_shape):
    grid_model = keras.models.Sequential()
    grid_model.add(keras.layers.Flatten(input_shape=input_shape))
    for layer in range(n_hidden):
        grid_model.add(keras.layers.Dense(n_neurons, activation=&quot;relu&quot;))
    grid_model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;))
    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    grid_model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;])
    return grid_model
</code></pre>
<p>Then I defined the model hyperparameters to explore:</p>
<pre><code>param_distribs = {
    &quot;n_hidden&quot;: [0, 1, 2, 3, 4, 5],
    &quot;n_neurons&quot;: np.arange(1, 300),
    &quot;learning_rate&quot;: reciprocal(3e-4, 3e-2)
}
</code></pre>
<p>I then used SciKeras to create a wrapper around the Keras model, feeding the parameter space:</p>
<pre><code>keras_reg = KerasRegressor(build_model, n_hidden=param_distribs[&quot;n_hidden&quot;], n_neurons=param_distribs[&quot;n_neurons&quot;], learning_rate=param_distribs[&quot;learning_rate&quot;], verbose=1)
</code></pre>
<p>The last step is to define a <code>RandomizedSearchCV</code> object and start the research using the <code>fit</code> method:</p>
<pre><code>rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)

rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])
</code></pre>
<p>This last row gives me the following error, for each epoch:</p>
<pre><code>/home/docker_user/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py&quot;, line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/sklearn/metrics/_scorer.py&quot;, line 429, in _passthrough_scorer
    return estimator.score(*args, **kwargs)
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/scikeras/wrappers.py&quot;, line 1100, in score
    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/scikeras/wrappers.py&quot;, line 1697, in scorer
    return sklearn_r2_score(y_true, y_pred, **kwargs)
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py&quot;, line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/sklearn/metrics/_regression.py&quot;, line 100, in _check_reg_targets
    check_consistent_length(y_true, y_pred)
  File &quot;/home/docker_user/.local/lib/python3.8/site-packages/sklearn/utils/validation.py&quot;, line 387, in check_consistent_length
    raise ValueError(
ValueError: Found input variables with inconsistent numbers of samples: [18334, 183340]
</code></pre>
<p>A factor 10 on the second dimension makes me thinking... I also checked the shapes of  the data and they are fine...</p>
<pre><code>print(X_train.shape, y_train.shape)
(55000, 28, 28) (55000,)
</code></pre>
<p>Can you please help me dealing with this error?</p>
",50,1,1,3,python;tensorflow;keras,2022-05-26 18:55:49,2022-05-26 18:55:49,2022-05-26 19:26:34,i m trying to follow the example on chapter  of the book hands on machine learning with scikit learn  keras and tensorflow which regard the optimization of the hyperparameters of a dnn model  the dataset is the mnist fashion model and the goal of the project is the classification of the images in  classes  there is no validation  so i m going to create that set using the first k elements  a possible implementation of a simple dnn model is the following  using sequential keras api  the book then suggests to study the hyper parameter space to found the best ones  using randomizedsearchcv  the example uses keras wrappers scikit_learn kerasregressor which is now deprecated in favor of kerasregressor by scikeras  i created a function containing the ml model  then i defined the model hyperparameters to explore  i then used scikeras to create a wrapper around the keras model  feeding the parameter space  the last step is to define a randomizedsearchcv object and start the research using the fit method  this last row gives me the following error  for each epoch  a factor  on the second dimension makes me thinking    i also checked the shapes of  the data and they are fine    can you please help me dealing with this error 
481,481,0,72392108,How to skip Not Null values in dataframe column,"<p>I have the following dataset
<a href=""https://i.stack.imgur.com/gO15v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gO15v.png"" alt=""enter image description here"" /></a>
I'm extracting results from the machine learning model and storing the result in the column 'new_locations_names'. <strong>I want to skip the storing if the column value is already filled!</strong> I mean I want it to store only in the NULL values rows in 'new_locations_names'.
<strong>I want to do what's inside the (try) only for nan values.</strong></p>
<p>I'm trying using the following code but it is not working:</p>
<pre><code>for index, row in data.iterrows():
    warnings.filterwarnings('ignore')
    if row['new_locations_names'] is not None:
        continue
    try:
        print(&quot;INDEX AT &quot;,index)
        result = extract_location_with_xlm_roberta(row['translated_title'])
        print(result)
        data.at[index, 'new_locations_names'] = {'new_locations_names':result['text_locations_names']}
    except:
        print(&quot;EXCEPT &quot;,index)
</code></pre>
<p>Any thoughts, please?</p>
",61,0,0,4,python;python-3.x;pandas;dataframe,2022-05-26 18:20:23,2022-05-26 18:20:23,2022-05-26 18:20:23,i m trying using the following code but it is not working  any thoughts  please 
482,482,19204801,72388777,API Key for Azure Machine Learning Endpoint,"<p>I am using Azure ML, I made my models and now I want to connect them to Data Factory to run some process.</p>
<p>I implement an endpoint, but I can't find the API key for the endpoints. Right now, I have the REST endpoint, but not in key-based authentication enabled, it's false. Do you know how to generate the API key?</p>
",65,1,0,5,azure;machine-learning;endpoint;azure-machine-learning-service;automl,2022-05-26 13:45:24,2022-05-26 13:45:24,2022-05-26 17:35:19,i am using azure ml  i made my models and now i want to connect them to data factory to run some process  i implement an endpoint  but i can t find the api key for the endpoints  right now  i have the rest endpoint  but not in key based authentication enabled  it s false  do you know how to generate the api key 
483,483,9088176,72376401,Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe),"<p>My question is somehow related to <a href=""https://docs.microsoft.com/en-us/answers/questions/217305/data-input-format-call-the-service-for-azure-ml-ti.html"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/answers/questions/217305/data-input-format-call-the-service-for-azure-ml-ti.html</a> - however, the provided solution does not seem to work.</p>
<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import pickle

# data input
df = pd.read_csv('heart.csv')

# numerical variables
num_cols = ['age',
            'trestbps',
            'chol',
            'thalach',
            'oldpeak'
]

# categorical variables
cat_cols = ['sex',
            'cp',
            'fbs',
            'restecg',
            'exang',
            'slope',
            'ca',
            'thal']

# changing format of the categorical variables
df[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))

# target variable
y = df['target']

# features
X = df.drop(['target'], axis=1)

# data split:

# random seed
np.random.seed(42)

# splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    stratify=y)

# double check
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# pipeline for numerical data
num_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean
                                                   ('minmaxscaler', MinMaxScaler())]) # scaling

# pipeline for categorical data
cat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values
                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding

# preprocessor - combining pipelines
preprocessor = ColumnTransformer([
                                  ('categorical', cat_preprocessing, cat_cols),
                                  ('numerical', num_preprocessing, num_cols)
                                                           ])

# initial model parameters
log_ini_params = {'penalty': 'l2', 
                  'tol': 0.0073559740277086005, 
                  'C': 1.1592424247511928, 
                  'fit_intercept': True, 
                  'solver': 'liblinear'}

# model - Pipeline
log_clf = Pipeline([('preprocessor', preprocessor),
                  ('clf', LogisticRegression(**log_ini_params))])

log_clf.fit(X_train, y_train)

# dumping the model
f = 'model/log.pkl'
with open(f, 'wb') as file:
    pickle.dump(log_clf, file)

# loading it
loaded_model = joblib.load(f)

# double check on a single datapoint
new_data = pd.DataFrame({'age': 71,
                         'sex': 0,
                         'cp': 0,
                         'trestbps': 112,
                         'chol': 203,
                         'fbs': 0,
                         'restecg': 1,
                         'thalach': 185,
                         'exang': 0,
                         'oldpeak': 0.1,
                         'slope': 2,
                         'ca': 0,
                          'thal': 2}, index=[0])

loaded_model.predict(new_data)

</code></pre>
<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:</p>
<ol>
<li>I create the score.py file</li>
</ol>
<pre><code>import joblib
from azureml.core.model import Model
import json

def init():
    global model
    model_path = Model.get_model_path('log') # logistic
    print('Model Path is  ', model_path)
    model = joblib.load(model_path)


def run(data):
    try:
        data = json.loads(data)
        result = model.predict(data['data'])
        # any data type, as long as it is JSON serializable.
        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}
    except Exception as e:
        error = str(e)
        return {'data' : error , 'message' : 'Failed to classify heart diseases'}
</code></pre>
<ol>
<li>I deploy the model:</li>
</ol>
<pre><code>from azureml.core import Workspace
from azureml.core.webservice import AciWebservice
from azureml.core.webservice import Webservice
from azureml.core.model import InferenceConfig
from azureml.core.environment import Environment
from azureml.core import Workspace
from azureml.core.model import Model
from azureml.core.conda_dependencies import CondaDependencies

ws = Workspace.from_config()

model = Model.register(workspace = ws,
              model_path ='model/log.pkl',
              model_name = 'log',
              tags = {'version': '1'},
              description = 'Heart disease classification',
              )

# to install required packages
env = Environment('env')
cd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])
env.python.conda_dependencies = cd

# Register environment to re-use later
env.register(workspace = ws)
print('Registered Environment')

myenv = Environment.get(workspace=ws, name='env')

myenv.save_to_directory('./environ', overwrite=True)

aciconfig = AciWebservice.deploy_configuration(
            cpu_cores=1,
            memory_gb=1,
            tags={'data':'heart disease classifier'},
            description='Classification of heart diseases',
            )

inference_config = InferenceConfig(entry_script='score.py', environment=myenv)

service = Model.deploy(workspace=ws,
                name='hd-model-log',
                models=[model],
                inference_config=inference_config,
                deployment_config=aciconfig, 
                overwrite = True)

service.wait_for_deployment(show_output=True)
url = service.scoring_uri
print(url)
</code></pre>
<p>The deployment is fine:</p>
<blockquote>
<p>Succeeded
ACI service creation operation finished, operation &quot;Succeeded&quot;</p>
</blockquote>
<p>But I can not make any predictions with the new data. I try to use:</p>
<pre><code>import pandas as pd

new_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],
                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],
                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])
</code></pre>
<p>Following the answer from this topic (<a href=""https://docs.microsoft.com/en-us/answers/questions/217305/data-input-format-call-the-service-for-azure-ml-ti.html"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/answers/questions/217305/data-input-format-call-the-service-for-azure-ml-ti.html</a>) I transform the data:</p>
<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})
</code></pre>
<p>And try to make some predictions:</p>
<pre><code>import json
import requests
data = test_sample
headers = {'Content-Type':'application/json'}
r = requests.post(url, data=data, headers = headers)
print(r.status_code)
print(r.json())
</code></pre>
<p>However, I encounter an error:</p>
<blockquote>
<p>200
{'data': &quot;Expected 2D array, got 1D array instead:\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}</p>
</blockquote>
<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?</p>
<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.</p>
<p>Would really appreciate some help.</p>
",89,2,1,5,json;python-3.x;machine-learning;azure-machine-learning-studio;azure-machine-learning-service,2022-05-25 16:30:27,2022-05-25 16:30:27,2022-05-26 15:58:07,my question is somehow related to    however  the provided solution does not seem to work  i am constructing a simple model with heart disease dataset but i wrap it into pipeline as i use some featurization steps  scaling  encoding etc   the full script below     and it works just fine   then i deploy the model to the azure web service using these steps  the deployment is fine  but i can not make any predictions with the new data  i try to use  following the answer from this topic    i transform the data  and try to make some predictions  however  i encounter an error  how is it possible to adjust the input data to this form of predictions and add other output like predict_proba so i could store them in a separate output dataset  i know this error is somehow related either with the  run  part of the score py file or the last code cell that calls the webservice  but i m unable to find it  would really appreciate some help 
484,484,8867480,47052996,how to upload multiple files to azure blob storage using azure machine Learning,"<p>I have come across a requirement wherein the first part involves reading a blob file (i.e. .csv) present in Azure blob storage and splitting the file data into multiple files, based on the distinct combination of few columns. The second part of the requirement involves writing/uploading the multiple files to Azure blob Storage at a separate destination folder. </p>

<p>I am able to split the blob file into multiple files, but am not able to write/upload the partitioned files on to azure blob storage. Is there any possibility to write the files to blob storage. Any help will be highly appreciated.</p>
",1264,1,0,3,file;azure-blob-storage;azure-machine-learning-studio,2017-11-01 16:11:05,2017-11-01 16:11:05,2022-05-26 14:44:01,i have come across a requirement wherein the first part involves reading a blob file  i e   csv  present in azure blob storage and splitting the file data into multiple files  based on the distinct combination of few columns  the second part of the requirement involves writing uploading the multiple files to azure blob storage at a separate destination folder   i am able to split the blob file into multiple files  but am not able to write upload the partitioned files on to azure blob storage  is there any possibility to write the files to blob storage  any help will be highly appreciated 
485,485,5123111,72381910,How to update libraries in runtime on Databricks?,"<p>I have certain code snippets that need to be run using library versions that are higher than the versions currently shipped within the Databricks Runtime version.</p>
<p>I have an external file logged by mlflow which contains a collection of Python libraries &amp; versions, which are required for running certain code, and I need to ensure that those are properly installed before running the cells in the Databricks notebook.</p>
<p>I'm aware that it's possible to just do <code>%pip install</code> in a cell to upgrade to the required versions, but I want to avoid hardcoding packages and package versions, that's why I am attempting to rely on the external file logged by mlflow.</p>
<p>I am currently using Databricks Runtime 9.1 LTS for Machine Learning which comes with <code>pandas==1.2.4</code> pre-installed. However, for my workflow I would need a more up-to-date package, <code>pandas==1.3.5</code>.</p>
<p>My approach for ensuring that the version seen by the notebook is indeed the newer 1.3.5 one, and not the default 1.2.4 that comes pre-installed, was to use the Libraries API of Databricks:</p>
<pre><code>import requests

import pandas as pd 
print(f&quot;pandas version BEFORE installation with Libraries API:&quot;, pd.__version__)

ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()
host_name  = ctx.tags().get(&quot;browserHostName&quot;).get()
host_token = ctx.apiToken().get()
cluster_id = ctx.tags().get(&quot;clusterId&quot;).get()

response = requests.post(
    f&quot;https://{host_name}/api/2.0/libraries/install&quot;,
    headers = {'Authorization': f'Bearer {host_token}'},
    json = {
        &quot;cluster_id&quot;: cluster_id,
        &quot;libraries&quot;: [
            {
                &quot;pypi&quot;: {
                    &quot;package&quot;: &quot;pandas==1.3.5&quot;,
                }
            }
        ]
    }
)
assert response.status_code == 200, f&quot;Package installation might have failed, status code: {response.status_code}!&quot;

import pandas as pd 
print(f&quot;pandas version AFTER installation with Libraries API:&quot;, pd.__version__)
</code></pre>
<p>The output of the above cell is:</p>
<pre><code>pandas version BEFORE installation with Libraries API: 1.2.4
pandas version AFTER installation with Libraries API: 1.2.4
</code></pre>
<p>While the intended output which I want to achieve is:</p>
<pre><code>pandas version BEFORE installation with Libraries API: 1.2.4
pandas version AFTER installation with Libraries API: 1.3.5
</code></pre>
<p>How could such library upgrade/overwrite over the Databricks Runtime be achieved?</p>
",38,1,1,4,python;pip;python-requests;databricks,2022-05-25 22:54:19,2022-05-25 22:54:19,2022-05-26 14:38:10,i have certain code snippets that need to be run using library versions that are higher than the versions currently shipped within the databricks runtime version  i have an external file logged by mlflow which contains a collection of python libraries  amp  versions  which are required for running certain code  and i need to ensure that those are properly installed before running the cells in the databricks notebook  i m aware that it s possible to just do  pip install in a cell to upgrade to the required versions  but i want to avoid hardcoding packages and package versions  that s why i am attempting to rely on the external file logged by mlflow  i am currently using databricks runtime   lts for machine learning which comes with pandas     pre installed  however  for my workflow i would need a more up to date package  pandas      my approach for ensuring that the version seen by the notebook is indeed the newer    one  and not the default    that comes pre installed  was to use the libraries api of databricks  the output of the above cell is  while the intended output which i want to achieve is  how could such library upgrade overwrite over the databricks runtime be achieved 
486,486,17240155,72388768,how to do clustering or classification for multiple string feratures data in python machine learning,"<p>I want to do clustering data which has multiple categorical featueres.
the data consists of :
{
&quot;fieldA&quot;: &quot;valueA&quot;,
&quot;fieldB&quot;: &quot;valueB&quot;,
&quot;fieldC&quot;: &quot;valueC&quot;,
&quot;fieldD&quot;: &quot;valueD&quot;,
&quot;fieldE&quot;: &quot;valueE&quot;,
&quot;fieldF&quot;: &quot;valueF&quot;,
&quot;fieldG&quot;: &quot;valueG&quot;,
}</p>
<p>usually, the human can cluster the data manually(some fields in each data can be different, but human can cluster same data by whose knowledge.), but I want to implement automatic system.</p>
<p>I've tried to apply some ML algorithms of clusterung for categorical features (like KModes), but it's not clear.</p>
<p>could you please help ?
have a good day, thank you</p>
",17,0,0,2,python;cluster-analysis,2022-05-26 13:44:40,2022-05-26 13:44:40,2022-05-26 13:51:43,usually  the human can cluster the data manually some fields in each data can be different  but human can cluster same data by whose knowledge    but i want to implement automatic system  i ve tried to apply some ml algorithms of clusterung for categorical features  like kmodes   but it s not clear 
487,487,10650689,72388267,Best way to segment a data set with binary columns using excel or Pandas,"<p>. <a href=""https://i.stack.imgur.com/EELSN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EELSN.png"" alt=""enter image description here"" /></a></p>
<p>I have a CSV with 101 rows and 150 columns. I need to find a way to segment/cluster the ID's using the Column values. It can be a machine learning approach or just using Excel techniques. At the end what I need is ID's with similar behaviors group together. Thanks in advance.</p>
",20,0,-1,4,excel;dataframe;machine-learning;unsupervised-learning,2022-05-26 12:58:19,2022-05-26 12:58:19,2022-05-26 12:58:19,   i have a csv with  rows and  columns  i need to find a way to segment cluster the id s using the column values  it can be a machine learning approach or just using excel techniques  at the end what i need is id s with similar behaviors group together  thanks in advance 
488,488,2707770,72388165,Pair between lists of descriptions,"<p>I have a list of short descriptions (in english), call it &quot;list_a&quot;, and I want to create a map between each item of list_a to one item of a second list, call it &quot;list_b&quot;. The latter is a list containing some other short descriptions.</p>
<p>It sounds like it can be faced with a machine learning approach (e.g. natural language processing), but I don't know how to start. Could you please suggest a method or give me a hint what path can I follow? I prefer an implementation in python.</p>
<p>In other words, the problem is to pair each item of list_a with one item of list_b that has the same meaning, or at least to find all possible items of list_b that matches to the item's short description of list_a.</p>
<p>Thank you.</p>
",20,0,0,5,nlp;mapping;match;matching;pairing,2022-05-26 12:47:36,2022-05-26 12:47:36,2022-05-26 12:47:36,i have a list of short descriptions  in english   call it  list_a   and i want to create a map between each item of list_a to one item of a second list  call it  list_b   the latter is a list containing some other short descriptions  it sounds like it can be faced with a machine learning approach  e g  natural language processing   but i don t know how to start  could you please suggest a method or give me a hint what path can i follow  i prefer an implementation in python  in other words  the problem is to pair each item of list_a with one item of list_b that has the same meaning  or at least to find all possible items of list_b that matches to the item s short description of list_a  thank you 
489,489,19200596,72387566,Not Able to Use Keras Package in R on M1 MacBook,"<p>So I was able download the M1 compatible version of TensorFlow on my M1 MacBook using the instructions here: <a href=""https://github.com/mrdbourke/m1-machine-learning-test#how-to-setup-a-tensorflow-environment-on-m1-m1-pro-m1-max-using-miniforge-shorter-version"" rel=""nofollow noreferrer"">https://github.com/mrdbourke/m1-machine-learning-test#how-to-setup-a-tensorflow-environment-on-m1-m1-pro-m1-max-using-miniforge-shorter-version</a></p>
<p>I then defined a variable in the R environment:</p>
<pre><code>RETICULATE_PYTHON = &quot;~/miniforge3/bin/python&quot;

</code></pre>
<p>as per the instructions under the &quot;Apple Silicon&quot; section here <a href=""https://www.rdocumentation.org/packages/tensorflow/versions/2.6.0/topics/install_tensorflow"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/tensorflow/versions/2.6.0/topics/install_tensorflow</a>.</p>
<p>However, I'm still getting &quot;R session aborted&quot; error every time I try to use a Keras function. Any suggestions as to what to try next? Would appreciate any help.</p>
",31,0,0,3,r;tensorflow;keras,2022-05-26 11:52:15,2022-05-26 11:52:15,2022-05-26 11:52:15,so i was able download the m compatible version of tensorflow on my m macbook using the instructions here   i then defined a variable in the r environment  as per the instructions under the  apple silicon  section here   however  i m still getting  r session aborted  error every time i try to use a keras function  any suggestions as to what to try next  would appreciate any help 
490,490,16000016,72386451,"My training and testing graph remains constant, can anyone help me interpret it or explain where have I gone wrong?","<p>I'm doing a simple machine learning project. At initial model, my model was over fitting, as I understood by googling and learning about what over fitting is and how to detect it. Then I used SMOTE to reduce over fitting and tried to find if it still over fits. I'm getting a graph that I'm unable to interpret and tried several links to understand what is happening but failed.
Can anyone please tell me if this graph is okay or there is something wrong in it? (The picture and code is given below)
<a href=""https://i.stack.imgur.com/GNs35.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GNs35.png"" alt=""enter image description here"" /></a></p>
<pre><code>def EF_final(x_train, y_train, x_test, y_test):
  train_scores, test_scores = [], []
  values = [i for i in range(1, 21)]
# evaluate a decision tree for each depth
  for i in values:
    # configure the model
      model_ef = ExtraTreesClassifier(n_estimators = 80, random_state=42, min_samples_split = 2, min_samples_leaf= 1, max_features = 'sqrt', max_depth= 24, bootstrap=False)
    # fit model on the training dataset
      model_ef.fit(x_train, y_train)
    # evaluate on the train dataset
      train_yhat = model_ef.predict(x_train)
      train_acc = accuracy_score(y_train, train_yhat)
      train_scores.append(train_acc)
    # evaluate on the test dataset
      test_yhat = model_ef.predict(x_test)
      test_acc = accuracy_score(y_test, test_yhat)
      test_scores.append(test_acc)
    # summarize progress
      print('&gt;%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))
# plot of train and test scores vs tree depth
  plt.plot(values, train_scores, '-o', label='Train')
  plt.plot(values, test_scores, '-o', label='Test')
  plt.legend()
  plt.show()
</code></pre>
",19,1,0,4,python;machine-learning;graph;overfitting-underfitting,2022-05-26 09:13:45,2022-05-26 09:13:45,2022-05-26 10:12:04,
491,491,16826890,69051891,R C5.0: Error while including minCases in tunegrid (caret),"<p>i am trying to implement the minCases-argument into my tuning process of a c5.0 model.
As i am using the caret package i am trying to get that argument into the &quot;tuneGrid&quot;.
For that purpose i found the following Tutorial.
<a href=""https://www.euclidean.com/machine-learning-in-practice/2015/6/12/r-caret-and-parameter-tuning-c50"" rel=""nofollow noreferrer"">https://www.euclidean.com/machine-learning-in-practice/2015/6/12/r-caret-and-parameter-tuning-c50</a></p>
<p>After implementing the code into my syntax i get the following error:</p>
<pre><code>**Error: The tuning parameter grid should have columns NA, NA, NA, splits**
</code></pre>
<p>Anyone knows where there is a mistake?
The error occurs as soon as i am building my model &quot;mdl&quot; in the last line of the code.</p>
<p>With regard to the Tutorial mentionend above my current code is the following:</p>
<pre><code>library(datasets)
data(iris)

library('gmodels')
library(&quot;RcppCNPy&quot;)
library(&quot;class&quot;)
library(&quot;C50&quot;)
library('caret')
library('mlbench')


####Customizing the C5.0

C5CustomSort &lt;- function(x) {
  
  x$model &lt;- factor(as.character(x$model), levels = c(&quot;rules&quot;,&quot;tree&quot;))
  x[order(x$trials, x$model, x$splits, !x$winnow),]
  
}

C5CustomLoop &lt;- function (grid) 
{
  loop &lt;- ddply(grid, c(&quot;model&quot;, &quot;winnow&quot;,&quot;splits&quot;), function(x) c(trials = max(x$trials)))
  submodels &lt;- vector(mode = &quot;list&quot;, length = nrow(loop))
  for (i in seq(along = loop$trials)) {
    index &lt;- which(grid$model == loop$model[i] &amp; grid$winnow == loop$winnow[i] &amp; grid$splits == loop$splits[i])
    trials &lt;- grid[index, &quot;trials&quot;]
    submodels[[i]] &lt;- data.frame(trials = trials[trials != loop$trials[i]])
  }
  list(loop = loop, submodels = submodels)
}

C5CustomGrid &lt;- function(x, y, len = NULL) {
  c5seq &lt;- if(len == 1)  1 else  c(1, 10*((2:min(len, 11)) - 1))
  expand.grid(trials = c5seq, splits = c(2,10,20,50), winnow = c(TRUE, FALSE), model = c(&quot;tree&quot;,&quot;rules&quot;))
}

C5CustomFit &lt;- function(x, y, wts, param, lev, last, classProbs, ...) {
  # add the splits parameter to the fit function
  # minCases is a function of splits
  
  theDots &lt;- list(...)
  
  splits   &lt;- param$splits
  minCases &lt;- floor( length(y)/splits ) - 1
  
  if(any(names(theDots) == &quot;control&quot;))
  {
    theDots$control$winnow        &lt;- param$winnow
    theDots$control$minCases      &lt;- minCases
    theDots$control$earlyStopping &lt;- FALSE
  }
  else
    theDots$control &lt;- C5.0Control(winnow = param$winnow, minCases = minCases, earlyStopping=FALSE )
  
  argList &lt;- list(x = x, y = y, weights = wts, trials = param$trials, rules = param$model == &quot;rules&quot;)
  
  argList &lt;- c(argList, theDots)
  
  do.call(&quot;C5.0.default&quot;, argList)
  
}

GetC5Info &lt;- function() {
  
  # get the default C5.0 model functions
  c5ModelInfo &lt;- getModelInfo(model = &quot;C5.0&quot;, regex = FALSE)[[1]]
  
  # modify the parameters data frame so that it includes splits
  c5ModelInfo$parameters$parameter &lt;- factor(c5ModelInfo$parameters$parameter,levels=c(levels(c5ModelInfo$parameters$parameter),'splits'))
  c5ModelInfo$parameters$label &lt;- factor(c5ModelInfo$parameters$label,levels=c(levels(c5ModelInfo$parameters$label),'Splits'))
  c5ModelInfo$parameters &lt;- rbind(c5ModelInfo$parameters,c('splits','numeric','Splits'))
  
  # replace the default c5.0 functions with ones that are aware of the splits parameter
  c5ModelInfo$fit  &lt;- C5CustomFit
  c5ModelInfo$loop &lt;- C5CustomLoop
  c5ModelInfo$grid &lt;- C5CustomGrid
  c5ModelInfo$sort &lt;- C5CustomSort
  
  return (c5ModelInfo)
  
}

c5info &lt;- GetC5Info()

#Building the actual model
x_a &lt;- iris[c(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;,&quot;Petal.Length&quot;,&quot;Petal.Width&quot;)]
y_a &lt;-as.factor(iris[,c(&quot;Species&quot;)])

fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10)
grida &lt;- expand.grid( .winnow = &quot;FALSE&quot;, .trials=c(1,5,10,15,20), .model=&quot;tree&quot;, .splits=c(2,5,10,15,20,25,50,100) )
mdl&lt;- train(x=x_a,y=y_a,tuneGrid=grida,trControl=fitControl,method=c5info)
</code></pre>
",96,1,0,4,r;decision-tree;r-caret;c5.0,2021-09-04 07:25:31,2021-09-04 07:25:31,2022-05-26 03:38:35,after implementing the code into my syntax i get the following error  with regard to the tutorial mentionend above my current code is the following 
492,492,18912529,72379718,Install and load Tidymodels package in AML,"<p>I'm trying install and load some R packages in the Execute R Script in Azure Machine Learning for to run models, such as <strong>tidymodels, timetk, modeltime, modeltime.ensemble</strong>.</p>
<pre><code>library(forecast)
library(tidyverse)
library(lubridate)
install.packages(&quot;quantdates&quot;,repos = &quot;https://cloud.r-project.org&quot;)
install.packages(&quot;tidymodels&quot;,repos = &quot;https://cloud.r-project.org&quot;)
library(quantdates)
library(tidymodels) 
library(timetk) 
library(modeltime) 
library(modeltime.resample) 
library(modeltime.ensemble)
</code></pre>
<p>However I get the following error:</p>
<pre><code>Error: package or namespace load failed for ‘tidymodels’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
namespace ‘rlang’ 0.4.5 is already loaded, but &gt;= 1.0.2 is required

azureml_main(input_dataframe_1), library(tidymodels), tryCatch({
    attr(package, &quot;LibPath&quot;) &lt;- which.lib.loc
    ns &lt;- loadNamespace(package, lib.loc)
    env &lt;- attachNamespace(ns, pos = pos, deps)
}, error = function(e) {
    P &lt;- if (!is.null(cc &lt;- conditionCall(e))) 
        paste(&quot; in&quot;, deparse(cc)[1])
    else &quot;&quot;
    msg &lt;- gettextf(&quot;package or namespace load failed for %s%s:\n %s&quot;, sQuote(package), P, conditionMessage(e))
    if (logical.return) 
        message(paste(&quot;Error:&quot;, msg), domain = NA)
    else stop(msg, call. = FALSE, domain = NA)
}), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(expr, names, parentenv, handlers[[1]]), value[[3]](cond), stop(msg, call. = FALSE, domain = NA), .handleSimpleError(function (e) 
{
    error_msg &lt;&lt;- paste(toString(e), toString(sys.calls()[-c(1:3)]), sep = &quot;\n&quot;)
    stop(e)
}, &quot;package or namespace load failed for ‘tidymodels’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace ‘rlang’ 0.4.5 is already loaded, but &gt;= 1.0.2 is required&quot;, quote(NULL)), h(simpleError(msg, call))
'.
---------- End of error message from R  interpreter  ----------
</code></pre>
<p>I have also tried with devtools package for install a particular version but I keep getting the same error with the <strong>rlang package</strong>. Sometimes, I get the same error with the <strong>cli package</strong>.</p>
<p>In my local machine, the R code runs fine. I have the R version 4.1.3 and the Azure Machine Learning has the R version 3.5.1.</p>
<p>Does anyone know how I can solve this problem?</p>
",64,1,0,5,r;azure;r-package;azure-machine-learning-service;tidymodels,2022-05-25 20:11:58,2022-05-25 20:11:58,2022-05-26 02:03:41,i m trying install and load some r packages in the execute r script in azure machine learning for to run models  such as tidymodels  timetk  modeltime  modeltime ensemble  however i get the following error  i have also tried with devtools package for install a particular version but i keep getting the same error with the rlang package  sometimes  i get the same error with the cli package  in my local machine  the r code runs fine  i have the r version    and the azure machine learning has the r version     does anyone know how i can solve this problem 
493,493,13278114,72380590,Airflow docker-compose from another docker container on Azure Machine Learning Compute Cluster,"<p>I’m attempting to run Airflow in docker with docker-compose from inside another container. This container is created by default by an Azure Machine Learning compute cluster that I want to use to run my Airflow DAG.</p>
<p>The problem is that I have the following error when I try to execute <code>docker-compose build</code> through the ScriptRunConfig class (see script below).</p>
<pre><code>error creating aufs mount to /var/lib/docker/aufs/mnt/3d54c61133023e1d3700ce9746529dc8328e739346d4123d632951f11acdf122-init: mount target=/var/lib/docker/aufs/mnt/3d54c61133023e1d3700ce9746529dc8328e739346d4123d632951f11acdf122-init data=br:/var/lib/docker/aufs/diff/3d54c61133023e1d3700ce9746529dc8328e739346d4123d632951f11acdf122-init=rw:/var/lib/docker/aufs/diff/d6e643e28d8729f8972b78bedd2de1de602879b65ff56e07e76a236d3096b709=ro+wh:/var/lib/docker/aufs/diff/8d3dde63564e16508dbf64e7baae79a6b2913b3262cb6dba7027e1fdf8bb6f8f=ro+wh:/var/lib/docker/aufs/diff/fa2e9c9ac8ff7af5f5d35e4d2081fac4bb5139d760675e9608d2b8d04f096837=ro+wh:/var/lib/docker/aufs/diff/d34747b3a1646124aa7a7a0fed4790e1264667aa58fc4de2288a10e4b68673ce=ro+wh:/var/lib/docker/aufs/diff/ea46ddfc022eb268aa428376132bc52b00b317e747d63078a38d044bae5d48ec=ro+wh:/var/lib/docker/aufs/diff/b3c155d801b49d8252a3926493328d471c8f4cfd72c553771374e3952a999d95=ro+wh:/var/lib/docker/aufs/diff/a067d04104f7a70c6194a3e742be72fc221759b18628285e7fd16a2d678120f3=ro+wh:/var/lib/docker/aufs/diff/3db994298571c09ee3d40abf21f657f9c8650a6fe0ea2c6c6c7590eb7c6c712f=ro+wh:/var/lib/docker/aufs/diff/273fc331f9ebae1d0a01963d46bf9edca6d91a089d4791066cb170954eb7609c=ro+wh:/var/lib/docker/aufs/diff/419a894bbee2b9b8ec05deed26dcfc21f234276e06d765d03ed939b918d3908f=ro+wh:/var/lib/docker/aufs/diff/e91d472c4e53f2a2eae97aca59f7dcacdf57a4b22d64991c348528e4081500d6=ro+wh:/var/lib/docker/aufs/diff/c23cc64e903b5254c19446e8ddc6db0253cbd19e239860c1dc95440ca65aae94=ro+wh:/var/lib/docker/aufs/diff/5794fefdefed444bf17de20f6c3ecf93743cccef83169c94aba62ec902c8380f=ro+wh,dio,xino=/dev/shm/aufs.xino: invalid argument

</code></pre>
<p>What I tried so far:</p>
<ol>
<li><strong>Discarded option</strong> : Using a <code>docker:dind</code> custom image where I installed <em>curl</em> and <em>docker-compose</em>. The problem is that <strong>it fails when building the image</strong> because:
<ul>
<li>it seems that Azure Machine Learning adds other steps to the Dockerfile to setup the conda environment needed for my project … but conda is not installed by default in the docker:dind image.</li>
<li>The Official docker image is based on Alpine Linux but Azure Machine Learning is only compatible with systems specifications including Ubuntu, Conda … as per the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-with-custom-image#use-a-custom-dockerfile-optional"" rel=""nofollow noreferrer"">documentation</a></li>
</ul>
</li>
<li><strong>The option presented in this question</strong>: Using an image already available to Azure Machine Learning where I installed the <em>Docker engine</em> and <em>docker-compose</em>. <strong>The image is built successfully but I get the error shown above during the execution of my script by the compute cluster</strong></li>
</ol>
<p>Here's the Dockerfile for the custom Image (ubuntu os) used by the compute cluster to setup the environment. It is referred to as <em>Dockerfile_cluster</em> in the python script below.</p>
<pre><code>FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1

USER root

# install docker
RUN apt-get update -y \
 &amp;&amp; apt-get install -y \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
 &amp;&amp; mkdir -p /etc/apt/keyrings \
 &amp;&amp; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg \
 &amp;&amp; echo \
  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null \
 &amp;&amp; apt-get update -y \
 &amp;&amp; apt-get install -y docker-ce docker-ce-cli containerd.io

# install docker-compose
RUN curl -L https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
RUN chmod +x /usr/local/bin/docker-compose
RUN ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
</code></pre>
<p>And my docker-compose.yml file (Different dockerfile than the first one) :</p>
<pre><code>services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - &quot;5434:5432&quot;
  init_db:
    build:
      context: .
      dockerfile: Dockerfile
    command: bash -c &quot;airflow db init &amp;&amp; airflow db upgrade&quot;
    env_file: .env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  
    depends_on:
      - postgres
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    restart: on-failure
    command:  bash -c &quot;airflow scheduler&quot;
    env_file: .env
    depends_on:
      - postgres
    ports:
      - &quot;8080:8793&quot;
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./.git:/opt/airflow/.git
      - ./conf:/opt/airflow/conf
      - ./airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, &quot;[ -f /usr/local/airflow/airflow-webserver.pid ]&quot;]
      interval: 30s
      timeout: 30s
      retries: 3
  webserver:
    build:
      context: .
      dockerfile: Dockerfile
    hostname: webserver
    restart: always
    env_file: .env
    depends_on:
      - postgres
    command: bash -c &quot;airflow users create -r Admin -u admin -e admin@example.com -f admin -l user -p admin &amp;&amp; airflow webserver&quot;
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./.git:/opt/airflow/.git
      - ./conf:/opt/airflow/conf
      - ./airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - &quot;5000:8080&quot;
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, &quot;[ -f /usr/local/airflow/airflow-webserver.pid ]&quot;]
      interval: 30s
      timeout: 30s
      retries: 32
</code></pre>
<p>And finally the script I use to submit my experiment to the compute cluster.</p>
<pre><code>from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig
from azureml.core.authentication import ServicePrincipalAuthentication

# Create an environment from conda reqs
env = Environment.from_conda_specification(name = &quot;env_name&quot;, file_path = &quot;./src/conda.yml&quot;)              

# Use custom image from Dockerfile
env.docker.base_image = None
env.docker.base_dockerfile = &quot;./Dockerfile_cluster&quot;


# Instantiate ServicePrincipalAuth object
svc_pr = ServicePrincipalAuthentication(tenant_id=tenant_id,
       service_principal_id=sp_id,
       service_principal_password=sp_pwd)

# Instantiate AML Workspace object
ws = Workspace(
       subscription_id=sub_id,
       resource_group=rg_name,
       workspace_name=ws_name,
       auth=svc_pr
       )

command =&quot;bash -c 'service docker start &amp;&amp; docker-compose build --no-cache'&quot;.split()
experiment = Experiment(workspace=ws, name='exp-test')

config = ScriptRunConfig(source_directory='.', command = command, compute_target='cpt-cluster', environment=env)
# Submit experiment

run = experiment.submit(config)
aml_url = run.get_portal_url()
print(aml_url)
run.wait_for_completion(show_output=True)
</code></pre>
<p>In the script above, I only tried to build first before running the services.</p>
<p>Normally, the commands I use to run my Airflow DAG locally or using a compute instance are:</p>
<ol>
<li><code>docker-compose build –no-cache</code></li>
<li><code>docker-compose up postgres</code></li>
</ol>
<p>In another terminal:</p>
<ol start=""3"">
<li><code>docker-compose up init_db</code></li>
<li><code>docker-compose up scheduler webserver</code></li>
</ol>
<p>Thank you very much for your help.</p>
",60,0,0,5,python;azure;docker;docker-compose;airflow,2022-05-25 21:09:46,2022-05-25 21:09:46,2022-05-25 21:09:46,i m attempting to run airflow in docker with docker compose from inside another container  this container is created by default by an azure machine learning compute cluster that i want to use to run my airflow dag  the problem is that i have the following error when i try to execute docker compose build through the scriptrunconfig class  see script below   what i tried so far  here s the dockerfile for the custom image  ubuntu os  used by the compute cluster to setup the environment  it is referred to as dockerfile_cluster in the python script below  and my docker compose yml file  different dockerfile than the first one    and finally the script i use to submit my experiment to the compute cluster  in the script above  i only tried to build first before running the services  normally  the commands i use to run my airflow dag locally or using a compute instance are  in another terminal  thank you very much for your help 
494,494,14740097,68848892,difference between cross_val_score and KFold,"<p>I am learning Machine learning and I am having this doubt. Can anyone tell me what is the difference between:-</p>
<pre><code>from sklearn.model_selection import cross_val_score
</code></pre>
<p>and</p>
<pre><code>from sklearn.model_selection import KFold
</code></pre>
<p>I think both are used for k fold cross validation, but I am not sure why to use two different code for same function.
If there is something I am missing please do let me know. ( If possible please explain difference between these two methods)</p>
<p>Thanks,</p>
",1726,2,0,5,python;machine-learning;scikit-learn;cross-validation;k-fold,2021-08-19 19:04:23,2021-08-19 19:04:23,2022-05-25 19:54:28,i am learning machine learning and i am having this doubt  can anyone tell me what is the difference between   and thanks 
495,495,13571224,72378611,What algorithm does core ml use?,"<p>I have searched it everywhere but couldn’t end up with an solid answer. My question is what machine learning algorithm does core ml use? How do I answer to that question?</p>
",36,1,0,3,swift;xcode;coreml,2022-05-25 19:02:21,2022-05-25 19:02:21,2022-05-25 19:48:33,i have searched it everywhere but couldn t end up with an solid answer  my question is what machine learning algorithm does core ml use  how do i answer to that question 
496,496,7373787,62409303,How to handle missing values (NaN) in categorical data when using scikit-learn OneHotEncoder?,"<p>I have recently started learning python to develop a predictive model for a research project using machine learning methods. I have a large dataset comprised of both numerical and categorical data. The dataset has lots of missing values. I am currently trying to encode the categorical features using OneHotEncoder. When I read about OneHotEncoder, my understanding was that for a missing value (NaN), OneHotEncoder would assign 0s to all the feature's categories, as such:</p>

<pre><code>0     Male 
1     Female
2     NaN
</code></pre>

<p>After applying OneHotEncoder:</p>

<pre><code>0     10 
1     01
2     00
</code></pre>

<p>However, when running the following code:</p>

<pre class=""lang-py prettyprint-override""><code>    # Encoding categorical data
    from sklearn.compose import ColumnTransformer
    from sklearn.preprocessing import OneHotEncoder


    ct = ColumnTransformer([('encoder', OneHotEncoder(handle_unknown='ignore'), [1])],
                           remainder='passthrough')
    obj_df = np.array(ct.fit_transform(obj_df))
    print(obj_df)

</code></pre>

<p>I am getting the error <strong>ValueError: Input contains NaN</strong></p>

<p>So I am guessing my previous understanding of how OneHotEncoder handles missing values is wrong. 
Is there a way for me to get the functionality described above? I know imputing the missing values before encoding will resolve this issue, but I am reluctant to do this as I am dealing with medical data and fear that imputation may decrease the predictive accuracy of my model. </p>

<p>I found this <a href=""https://stackoverflow.com/questions/58222008/nan-giving-valueerror-in-onehotencoder-in-scikit-learn"">question</a> that is similar but the answer doesn't offer a detailed enough solution on how to deal with the NaN values.</p>

<p>Let me know what your thoughts are, thanks.</p>
",11436,4,15,3,python;machine-learning;scikit-learn,2020-06-16 18:39:16,2020-06-16 18:39:16,2022-05-25 19:48:22,i have recently started learning python to develop a predictive model for a research project using machine learning methods  i have a large dataset comprised of both numerical and categorical data  the dataset has lots of missing values  i am currently trying to encode the categorical features using onehotencoder  when i read about onehotencoder  my understanding was that for a missing value  nan   onehotencoder would assign s to all the feature s categories  as such  after applying onehotencoder  however  when running the following code  i am getting the error valueerror  input contains nan i found this  that is similar but the answer doesn t offer a detailed enough solution on how to deal with the nan values  let me know what your thoughts are  thanks 
497,497,6702746,72101715,During thread contention how can I speed up this ConcurrentQueue implementation which uses ReaderWriterLockSlim over a regular Queue&lt;T&gt;,"<p><strong>Question:</strong> How can I implement a faster thread safe queue to support an object pool when under heavy thread contention?</p>
<p><strong>Scenario:</strong> My overall final objective is a pure Dot Net implementation of a Micro Services host with performance as a feature: <a href=""https://github.com/tcwicks/ChillX"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX</a></p>
<p><strong>Example Use Case:</strong> Unity ML Agents using TorchSharp but also able to utilize the processing power of multiple NVidia cards (in my case 4X 3090 cards). Performance is critical because PPO machine learning requires many millions of iterations before it even begins to learn. In other words very tight loops serializing data messages hundreds of millions of times as fast as possible. With this current code depending on the model I get between 2 to 6 million iterations per day (depending on the model) and it is only enough data to utilize one of the four video cards at around 60%.</p>
<p>Custom Implementation of <code>ConcurrentQueue&lt;T&gt;</code> using <code>ReaderWriterLockSlim</code>
<a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/ThreadSafeQueue.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/ThreadSafeQueue.cs</a></p>
<p>Turns out it is slightly faster than using <code>System.Collections.Concurrent.ConcurrentQueue&lt;T&gt;</code></p>
<p>Note: both <code>ConcurrentQueue&lt;T&gt;</code> and ThreadSafeQueue.cs above are extremely fast when there is NO thread contention. However when there is high thread contention (example 4 threads in tight loops) it tends to struggle.</p>
<p><strong>Objective:</strong> I am looking for a way to create a queue with faster performance than this during thread contention. Attempts at a lock free queue (see below) did not perform any better.</p>
<p>I am aware that this may be considered a Micro Optimization.
The reason I need this to run faster is because this scenario is for machine learning (or other high volume transaction processing) where I have multiple producers (agents) feeding requests (messages) or training data to a central instance (consumer). For the machine learning scenario the central instance is executing these requests against Tensorflow.Net / TorchSharp running across 4 X 3090 video cards. the Tensorflow.Net / TorchSharp model is Proximal Policy optimization and therefore the result of each request then needs to be sent back to the producer (agent). I need to be able to serialize well over 200K messages per second to fully utilize the four video cards. However I am only able to get a throughput of around 43K messages per second. Each message is multiple float[] arrays plus control parameters such as score (float) step (int) etc... The overall reason why I need this performance is because I am trying to cut down 1 month for a training run down to about 1 week.</p>
<p>I'm using the Thread safe queue as a singleton to provide a backing store for object pooling for serialization across TCP sockets in a custom message queue implementation. The objects being pooled are management objects which wrap <code>ArrayPool&lt;byte&gt;</code> buffers in order to:</p>
<p><strong>A)</strong> manage the renting and returning of <code>ArrayPool&lt;T&gt;</code> buffers
<a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/RentedBuffer.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/RentedBuffer.cs</a></p>
<p><strong>B)</strong> Guarantee the returning of rented buffers in order to prevent memory leaks
<a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/RentedBufferContract.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/RentedBufferContract.cs</a></p>
<p>Minimal GC collection overheads is also a very important factor.</p>
<p>Object Pooling code is here:
<a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/ManagedPool.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/ManagedPool.cs</a></p>
<p>Consider that serializing an object with say 30 array properties / fields marked for serialization results in not one buffer but multiple. This is because the buffers are used for the array properties themselves as well as for the byte buffers used in serialization. Plus packing an object which has other object properties means nested levels of serialization which requires even more buffers.
The Serializer itself is here:</p>
<p><a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Serialization/TypedSerializer.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Serialization/TypedSerializer.cs</a></p>
<p>I'm using a custom extended version of BitConverter which reads from and writes to afore mentioned (pooled) rented buffers:</p>
<p><a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Serialization/BitConverterExtended.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Serialization/BitConverterExtended.cs</a></p>
<p>Note: I realize I could use <code>MemoryPool&lt;T&gt;</code> however that does not solve my issue because using <code>MemoryPool&lt;T&gt;</code> would require creating yet another class to manage its renting and returning as it does NOT autoreturn when it goes out of scope.</p>
<p>Note: For the end solution (Message Queue) Already looked at ZeroMQ, RabitMQ etc... but I need faster speeds. In fact I was previously using ZeroMQ with MessagePack but I need faster speeds. The issue with MessagePack + ZeroMQ is not their native speed itself. Rather its to do with the amount of allocations and therefore GC Collection overheads.</p>
<p>Hence I've written from scratch serializer, socket transport, and object pooling. For which a thread safe queue is a basic building block. In my current implementation when under heavy thread contention the performance gains from the serializer are negated by the queue implementation where I am using it for object pooling of wrappers around ArrayPool&lt; T &gt; for managing their return and preventing memory leaks. These objects must be pooled because otherwise we are back to the issue of GC collection overheads.</p>
<p>Tried Julian M Bucknall's Lock Free Queue.
<a href=""https://secondboyet.com/Articles/LockfreeQueue.html"" rel=""nofollow noreferrer"">https://secondboyet.com/Articles/LockfreeQueue.html</a>
However his implementation generates an allocation for each queue insert which creates a massive amount of garbage. End result is it ends up with 60% time spent in GC.</p>
<p>Tried extending it using object pooling however since the object pool uses the above ThreadSafeQueue implementation it is bottlenecked on the performance of the same.</p>
<p>Extended (optional object pooling) implementation of Julian M Bucknall's Lock Free Queue is here:</p>
<p><a href=""https://github.com/tcwicks/ChillX/blob/master/src/TestApps/ChillX.MQServer.Benchmark/LockFreeQueue.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/TestApps/ChillX.MQServer.Benchmark/LockFreeQueue.cs</a></p>
<p>Next implemented a lock free structure from scratch without pooling but using a queue of ring buffers to minimize allocations. Its performance is twice as fast as the LockFreeQueue above. 367ms vs 607ms for 1000000 queue / dequeue operations across 8 threads. however it is still 50% slower than ThreadSafeQueue which uses ReaderWriterLockSlim. Note LockFreeRingBufferQueue is a 99% lock free implementation:</p>
<p><a href=""https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/LockFreeRingBufferQueue.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/ChillX.Core/Structures/LockFreeRingBufferQueue.cs</a></p>
<p>Any ideas or suggestions would be highly appreciated.</p>
<p>Benchmark Code is here: <a href=""https://github.com/tcwicks/ChillX/blob/master/src/TestApps/ChillX.MQServer.Benchmark/Bench_Queue.cs"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/blob/master/src/TestApps/ChillX.MQServer.Benchmark/Bench_Queue.cs</a></p>
<p>If running the benchmark App (link below) choose option 2 from the menu.
<a href=""https://github.com/tcwicks/ChillX/tree/master/src/TestApps/ChillX.MQServer.Benchmark"" rel=""nofollow noreferrer"">https://github.com/tcwicks/ChillX/tree/master/src/TestApps/ChillX.MQServer.Benchmark</a></p>
<p>Here are the benchmark results using Benchmark.Net comparing above mentioned LockFreeQueue (Lock Free) versus ThreadSafeQueue (ReaderWriterLockSlim) versus the standard ConcurrentQueue:</p>
<p>Note: ConcurrentQueueCount method is checking the count before TryDequeue. ConcurrentQueueTry is doing a TryDequeue without checking the count.</p>
<pre><code>/*
BenchmarkDotNet=v0.13.1, OS=Windows 10.0.19044.1645 (21H2)
AMD Ryzen Threadripper 3970X, 1 CPU, 64 logical and 32 physical cores
.NET SDK=6.0.202
  [Host]   : .NET 6.0.4 (6.0.422.16404), X64 RyuJIT  [AttachedDebugger]
  .NET 6.0 : .NET 6.0.4 (6.0.422.16404), X64 RyuJIT

Job=.NET 6.0  Runtime=.NET 6.0

|                 Method |           m_TestMode | numRepititions | numThreads |     Mean |    Error |   StdDev |     Gen 0 | Completed Work Items | Lock Contentions |     Gen 1 |     Gen 2 |    Allocated |
|----------------------- |--------------------- |--------------- |----------- |---------:|---------:|---------:|----------:|---------------------:|-----------------:|----------:|----------:|-------------:|
| Bench_QueuePerformance |        LockFreeQueue |        1000000 |          4 | 607.8 ms | 11.97 ms | 15.14 ms | 5000.0000 |                    - |                - | 3000.0000 | 1000.0000 | 40,002,976 B |
| Bench_QueuePerformance |      ThreadSafeQueue |        1000000 |          4 | 322.0 ms | 19.68 ms | 58.02 ms |         - |                    - |                - |         - |         - |        992 B |
| Bench_QueuePerformance | ConcurrentQueueCount |        1000000 |          4 | 421.2 ms |  8.38 ms | 14.90 ms |         - |                    - |                - |         - |         - |      9,360 B |
| Bench_QueuePerformance |   ConcurrentQueueTry |        1000000 |          4 | 330.1 ms |  6.82 ms | 20.01 ms |         - |                    - |          11.5000 |         - |         - |  1,116,912 B |
 */
</code></pre>
<p>Here are the benchmark results for LockFreeRingBufferQueue  versus ThreadSafeQueue (ReaderWriterLockSlim) versus the standard ConcurrentQueue</p>
<pre><code>OS=Windows 10.0.19044.1645 (21H2)
AMD Ryzen Threadripper 3970X, 1 CPU, 64 logical and 32 physical cores
.NET SDK=6.0.202
  [Host]   : .NET 6.0.4 (6.0.422.16404), X64 RyuJIT  [AttachedDebugger]
  .NET 6.0 : .NET 6.0.4 (6.0.422.16404), X64 RyuJIT

Job=.NET 6.0  Runtime=.NET 6.0

| Method                 | m_TestMode           | numRepititions | numThreads | Mean      | Error      | StdDev     | Lock Contentions  |     Gen 0  |     Gen 1  |    Gen 2  | Allocated  |
|----------------------- |--------------------- |--------------- |----------- |----------:| ----------:| ----------:| -----------------:| ----------:| ----------:| ---------:| ----------:|
| Bench_QueuePerformance | RingBufferQueue      | 1000000        | 1          | 80.31 ms  | 3.194 ms   | 8.742 ms   | -                 | 3400.0000  | 1400.0000  | -         | 28,566 KB  |
| Bench_QueuePerformance | ThreadSafeQueue      | 1000000        | 1          | 96.60 ms  | 1.910 ms   | 3.901 ms   | -                 | -          | -          | -         | 1 KB       |
| Bench_QueuePerformance | ConcurrentQueueCount | 1000000        | 1          | 70.94 ms  | 1.410 ms   | 2.750 ms   | -                 | 500.0000   | 500.0000   | 500.0000  | 2,948 KB   |
| Bench_QueuePerformance | ConcurrentQueueTry   | 1000000        | 1          | 49.13 ms  | 2.274 ms   | 6.704 ms   | 0.1250            | -          | -          | -         | 283 KB     |
| Bench_QueuePerformance | RingBufferQueue      | 1000000        | 4          | 367.08 ms | 10.954 ms  | 32.128 ms  | -                 | 3000.0000  | 1000.0000  | -         | 28,566 KB  |
| Bench_QueuePerformance | ThreadSafeQueue      | 1000000        | 4          | 253.59 ms | 12.344 ms  | 36.398 ms  | -                 | -          | -          | -         | 1 KB       |
| Bench_QueuePerformance | ConcurrentQueueCount | 1000000        | 4          | 341.09 ms | 12.249 ms  | 36.116 ms  | 18.5000           | -          | -          | -         | 1,540 KB   |
| Bench_QueuePerformance | ConcurrentQueueTry   | 1000000        | 4          | 266.15 ms | 13.365 ms  | 39.408 ms  | 11.6667           | 333.3333   | 333.3333   | 333.3333  | 2,223 KB   |
</code></pre>
<p>Code implementation for each of the benchmarks:</p>
<p>4 threads are queueing items:</p>
<pre><code>switch (m_TestMode)
{
    case TestMode.LockFreeQueue:
        for (int I = 0; I &lt; numReps; I++)
        {
            m_queueLockFree.Enqueue(I);
            Interlocked.Increment(ref QueueSize);
        }
        break;
    case TestMode.ThreadSafeQueue:
        for (int I = 0; I &lt; numReps; I++)
        {
            m_queueThreadSafe.Enqueue(I);
            Interlocked.Increment(ref QueueSize);
        }
        break;
    case TestMode.RingBufferQueue:
        for (int I = 0; I &lt; numReps; I++)
        {
            m_queueThreadSafeRingBuffer.Enqueue(I);
            Interlocked.Increment(ref QueueSize);
        }
        break;
    case TestMode.ConcurrentQueueCount:
        for (int I = 0; I &lt; numReps; I++)
        {
            m_queueConcurrent.Enqueue(I);
            Interlocked.Increment(ref QueueSize);
        }
        break;
    case TestMode.ConcurrentQueueTry:
        for (int I = 0; I &lt; numReps; I++)
        {
            m_queueConcurrent.Enqueue(I);
            Interlocked.Increment(ref QueueSize);
        }
        break;
}
</code></pre>
<p>Simultaneously 4 threads are dequeuing items</p>
<pre><code>switch (m_TestMode)
{
    case TestMode.LockFreeQueue:
        while (ThreadsIsRunning)
        {
            success = false;
            while (!success)
            {
                if (m_queueLockFree.Count &gt; 0)
                {
                    m_queueLockFree.DeQueue();
                    success = true;
                }
                else
                {
                    if (!ThreadsIsRunning) { break; }
                }
            }
            Interlocked.Decrement(ref QueueSize);
        }
        break;
    case TestMode.ThreadSafeQueue:
        while (ThreadsIsRunning)
        {
            success = false;
            while (!success)
            {
                if (m_queueThreadSafe.Count &gt; 0)
                {
                    m_queueThreadSafe.DeQueue();
                    success = true;
                }
                else
                {
                    if (!ThreadsIsRunning) { break; }
                }
            }
            Interlocked.Decrement(ref QueueSize);
        }
        break;
    case TestMode.RingBufferQueue:
        while (ThreadsIsRunning)
        {
            success = false;
            while (!success)
            {
                m_queueThreadSafeRingBuffer.DeQueue(out success);
                if (!ThreadsIsRunning) { break; }
            }
            Interlocked.Decrement(ref QueueSize);
        }
        break;
    case TestMode.ConcurrentQueueCount:
        while (ThreadsIsRunning)
        {
            success = false;
            while (!success)
            {
                // In order to keep this fair we are also checking .Count property
                if (m_queueConcurrent.Count &gt; 0 &amp;&amp; m_queueConcurrent.TryDequeue(out item))
                {
                    success = true;
                }
                else
                {
                    if (!ThreadsIsRunning) { break; }
                }
            }
            Interlocked.Decrement(ref QueueSize);
        }
        break;
    case TestMode.ConcurrentQueueTry:
        while (ThreadsIsRunning)
        {
            success = false;
            while (!success)
            {
                if (m_queueConcurrent.TryDequeue(out item))
                {
                    success = true;
                }
                else
                {
                    if (!ThreadsIsRunning) { break; }
                }
            }
            Interlocked.Decrement(ref QueueSize);
        }
        break;
}
</code></pre>
",178,2,3,5,c#;multithreading;performance;queue;micro-optimization,2022-05-03 20:51:50,2022-05-03 20:51:50,2022-05-25 18:14:02,question  how can i implement a faster thread safe queue to support an object pool when under heavy thread contention  scenario  my overall final objective is a pure dot net implementation of a micro services host with performance as a feature   example use case  unity ml agents using torchsharp but also able to utilize the processing power of multiple nvidia cards  in my case x  cards   performance is critical because ppo machine learning requires many millions of iterations before it even begins to learn  in other words very tight loops serializing data messages hundreds of millions of times as fast as possible  with this current code depending on the model i get between  to  million iterations per day  depending on the model  and it is only enough data to utilize one of the four video cards at around    turns out it is slightly faster than using system collections concurrent concurrentqueue lt t gt  note  both concurrentqueue lt t gt  and threadsafequeue cs above are extremely fast when there is no thread contention  however when there is high thread contention  example  threads in tight loops  it tends to struggle  objective  i am looking for a way to create a queue with faster performance than this during thread contention  attempts at a lock free queue  see below  did not perform any better  i m using the thread safe queue as a singleton to provide a backing store for object pooling for serialization across tcp sockets in a custom message queue implementation  the objects being pooled are management objects which wrap arraypool lt byte gt  buffers in order to  minimal gc collection overheads is also a very important factor   i m using a custom extended version of bitconverter which reads from and writes to afore mentioned  pooled  rented buffers   note  i realize i could use memorypool lt t gt  however that does not solve my issue because using memorypool lt t gt  would require creating yet another class to manage its renting and returning as it does not autoreturn when it goes out of scope  note  for the end solution  message queue  already looked at zeromq  rabitmq etc    but i need faster speeds  in fact i was previously using zeromq with messagepack but i need faster speeds  the issue with messagepack   zeromq is not their native speed itself  rather its to do with the amount of allocations and therefore gc collection overheads  hence i ve written from scratch serializer  socket transport  and object pooling  for which a thread safe queue is a basic building block  in my current implementation when under heavy thread contention the performance gains from the serializer are negated by the queue implementation where i am using it for object pooling of wrappers around arraypool lt  t  gt  for managing their return and preventing memory leaks  these objects must be pooled because otherwise we are back to the issue of gc collection overheads  tried extending it using object pooling however since the object pool uses the above threadsafequeue implementation it is bottlenecked on the performance of the same  extended  optional object pooling  implementation of julian m bucknall s lock free queue is here   next implemented a lock free structure from scratch without pooling but using a queue of ring buffers to minimize allocations  its performance is twice as fast as the lockfreequeue above  ms vs ms for  queue   dequeue operations across  threads  however it is still   slower than threadsafequeue which uses readerwriterlockslim  note lockfreeringbufferqueue is a   lock free implementation   any ideas or suggestions would be highly appreciated  benchmark code is here   here are the benchmark results using benchmark net comparing above mentioned lockfreequeue  lock free  versus threadsafequeue  readerwriterlockslim  versus the standard concurrentqueue  note  concurrentqueuecount method is checking the count before trydequeue  concurrentqueuetry is doing a trydequeue without checking the count  here are the benchmark results for lockfreeringbufferqueue  versus threadsafequeue  readerwriterlockslim  versus the standard concurrentqueue code implementation for each of the benchmarks   threads are queueing items  simultaneously  threads are dequeuing items
498,498,14133190,72375738,What does the scikit-learn library do in this code?,"<p>I am interested in the field of machine learning, I tried to understand the following code but I could not. Can anyone explain to me simply?</p>
<pre><code>from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split  # This module divides our data into two parts, train and test
import sklearn.metrics as met
from sklearn.datasets import load_boston


boston = load_boston()
x = boston.data
y = boston.target

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(xtrain, ytrain)

ypredict = model.predict(xtest)

plt.scatter(ytest, ypredict)
plt.show()
print(met.mean_squared_error(ytest, ypredict))
</code></pre>
",40,2,0,3,python;machine-learning;scikit-learn,2022-05-25 15:44:52,2022-05-25 15:44:52,2022-05-25 16:32:50,i am interested in the field of machine learning  i tried to understand the following code but i could not  can anyone explain to me simply 
499,499,12692772,72371446,Trouble Deploying Docker image Flask AWS Beanstalker,"<p>I am trying to deploy a machine learning model using flask, then I used docker to build an image, and finally Travis for CI\CD and deploy it on AWS Beanstalk.
the Flask application runs well on localhost and docker image run command.</p>
<p>My entire project is here: <a href=""https://github.com/raudez77/end2end-machine-learning"" rel=""nofollow noreferrer"">end2endML</a></p>
<p>I am having the following problem &gt; <em><strong>502 Bad Gateway</strong></em></p>
<p>Until now :</p>
<ul>
<li>It is running on 64bit Amazon Linux 2/3.4.15 -I don't find any other option-</li>
<li>I have read about the port 80 or 5000 I have tried both and they failed</li>
<li>I read the Log and received this message <a href=""https://i.stack.imgur.com/tqIJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tqIJJ.png"" alt=""Error"" /></a></li>
</ul>
<p>I have a Dockerfile and Docker-compose.yml for deployment. and they look like this:</p>
<pre><code>FROM python:3.7-slim-bullseye

WORKDIR /app

ENV VIRTUAL_ENV=/opt/venv
RUN python3 -m venv $VIRTUAL_ENV
ENV PATH=&quot;$VIRTUAL_ENV/bin:$PATH&quot;

# Install dependencies:
COPY . /app
RUN pip install -r /app/requirements/requirements.txt

EXPOSE 5000

# Run the application:
CMD [&quot;python&quot;, &quot;/app/classification_model/application.py&quot;] 


 ---&gt; My docker Compose

# Docker compose for Amazon
version: '3'
services:
  aws_deployment:
    # This function forces Dockerfile
    build:
      # Files (look at the current dir)
      context: .

      # Look for this file
      dockerfile: Dockerfile
    ports:
      - &quot;80:5000&quot;

--&gt; My Travis file 

#autom install a copy of docker 
services:
  - docker

# Do before : -f force -t anynameyouwantforyourimage
before_install: 
  - docker build -t raudez77/end2end-machine-learning -f Dockerfile .

# Run test
# Script contents all script to run
script:
  - docker run raudez77/end2end-machine-learning python test.py -- --coverage

#Deploy into Amazon ElasticBeanstalk
deploy:
  # Use this Saas for deployment
  provider: elasticbeanstalk
  region: &quot;us-west-2&quot;
  # Application name
  app: &quot;docker_titanic&quot;
  # Enviroment
  env: &quot;Dockertitanic-env&quot;
  # Go Services- S3 - look for yout us-west-2 elastickbeanstalk
  bucket_name: &quot;elasticbeanstalk-us-west-2-082124557004&quot;
  bucket_path : &quot;docker_titanic&quot;

  # Create Acces key in Travis
  access_key_id: $AWS_ACCESS_KEY
  secret_access_key: &quot;$AWS_SECRET_KEY&quot;
  on:
    branch: master
  skip_cleanup: true ```
</code></pre>
",19,0,1,4,machine-learning;dockerfile;amazon-elastic-beanstalk;continuous-deployment,2022-05-25 08:13:06,2022-05-25 08:13:06,2022-05-25 09:37:20,my entire project is here   i am having the following problem  gt   bad gateway until now   i have a dockerfile and docker compose yml for deployment  and they look like this 
500,500,14378043,72371822,"R squared value is negative , while the model is actually predicting data quiet well","<p>Hey everyone I'm quite new to machine learning so please bare with me.
I'm using machine learning to try and predict sound levels in different neighborhoods . My Data set is as follows : a column for the neighborhood where the sample was taken , a column for the street it was taken in (Highway/ a street in a residential area / Main street meaning it has a certain amount of cars going through it each hour i.e minimum of 300 cars) , a column for the time where the sample was taken (morning , afternoon , night and dawn ) and of course the dependent variable which is the sound level in Decibels. Now i one hot encoded all the independent variables and fed the data to a support vector regression with the kernel set to 'poly'. After splitting the data set into a training and test set the Mean absolute error i got was 3.9 which is amazing , and the mean absolute squared error was 7.5 which means that my model can accurately predict sound levels to 7.5 decibels (if i understood that correctly) .After a bunch of research i stumbled upon R squared values and adjusted R squared values and when it came to that my R2 values was a negative value(-0.067) which means that my model fits terribly. My problem lies in these R values. Are they an accurate metric to measure my model by ? and how can my model have such horrible values even though it predicts the data quite well when comparing the test set to the predicted set. I've attached an image that shows some of the y_test values compared to the y_predicted. N.B the value 15 was an outlier and was removed from the dataset later on. <a href=""https://i.stack.imgur.com/P8GqD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P8GqD.png"" alt=""enter image description here"" /></a></p>
",23,0,0,3,python;machine-learning;svm,2022-05-25 09:30:25,2022-05-25 09:30:25,2022-05-25 09:30:25,
501,501,19067174,72360956,Q: ValueError: substring not found,"<pre><code>pygame 2.1.2 (SDL 2.0.18, Python 3.8.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
Traceback (most recent call last):
  File &quot;C:\Users\Dell\AppData\Local\Programs\Python\Python38\lib\runpy.py&quot;, line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;C:\Users\Dell\AppData\Local\Programs\Python\Python38\lib\runpy.py&quot;, line 85, 
in _run_code
    exec(code, run_globals)
  File &quot;c:\Users\Dell\.vscode\extensions\ms-python.python-2022.6.2\pythonFiles\lib\python\debugpy\__main__.py&quot;, line 45, in &lt;module&gt;
    cli.main()
  File &quot;c:\Users\Dell\.vscode\extensions\ms-python.python-2022.6.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py&quot;, line 444, in main
    run()
  File &quot;c:\Users\Dell\.vscode\extensions\ms-python.python-2022.6.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py&quot;, line 285, in run_file
    runpy.run_path(target_as_str, run_name=compat.force_str(&quot;__main__&quot;))
  File &quot;C:\Users\Dell\AppData\Local\Programs\Python\Python38\lib\runpy.py&quot;, line 262, in run_path
    return _run_module_code(code, init_globals, run_name,
  File &quot;C:\Users\Dell\AppData\Local\Programs\Python\Python38\lib\runpy.py&quot;, line 95, 
in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File &quot;C:\Users\Dell\AppData\Local\Programs\Python\Python38\lib\runpy.py&quot;, line 85, 
in _run_code
    exec(code, run_globals)
  File &quot;c:\Users\Dell\Documents\My beatMaker\My beats app.py&quot;, line 228, in &lt;module&gt; 
    exit_button, loading_button, delete_button, load_rectangle = draw_load_menu()    
  File &quot;c:\Users\Dell\Documents\My beatMaker\My beats app.py&quot;, line 157, in draw_load_menu
    name_index_start = saved_beats[beat].index('name: ') + 6
ValueError: substring not found
</code></pre>
<p>Hi, I am coding a drum machine. I am making it through pygame in python.</p>
<p>I am learning pygame through freecodecamp.org.</p>
<p>I don't know where this error came from. It says 'substring not found'. See if there is problem.</p>
<p>There is my code -</p>
<pre><code>import pygame
from pygame import mixer
pygame.init()

WIDTH = 1300
HEIGHT = 650

black = (0, 0, 0)
white = (255, 255, 255)
gray = (128, 128, 128)
dark_gray = (50, 50, 50)
green = (0, 255, 0)
gold = (212, 175, 55)
blue = (0, 255, 255)

screen = pygame.display.set_mode((WIDTH, HEIGHT))
pygame.display.set_caption(&quot;Beat Maker&quot;)
label_font = pygame.font.Font(&quot;Roboto-Bold.ttf&quot;, 32)
medium_font = pygame.font.Font(&quot;Roboto-Bold.ttf&quot;, 24)

fps = 60
timer = pygame.time.Clock()
beats = 8
instruments = 5
boxes = []
clicked = [[-1 for _ in range(beats)] for _ in range(instruments)]
active_list = [1 for _ in range(instruments)]
bpm = 240
playing = True
active_length = 0
active_beat = 0
beat_changed = True
save_menu = False
load_menu = False
saved_beats = []
file = open('saved_beats.txt', 'r')
for line in file:
    saved_beats.append(line)
beat_name = ''
typing = False

# load in sounds
hi_hat = mixer.Sound('sounds\hi hat.wav')
snare = mixer.Sound('sounds\snare.wav')
drum = mixer.Sound('sounds\kick.wav')
crash = mixer.Sound('sounds\crash.wav')
clap = mixer.Sound('sounds\clap.wav')
pygame.mixer.set_num_channels(instruments * 3)


def play_notes():
    for i in range(len(clicked)):
        if clicked[i][active_beat] == 1 and active_list[i] == 1:
            if i == 0:
                hi_hat.play()
            if i == 1:
                snare.play()
            if i == 2:
                drum.play()
            if i == 3:
                crash.play()
            if i == 4:
                clap.play()



def draw_grid(clicks, beat, actives):
    left_box = pygame.draw.rect(screen, gray, (0,0, 206, HEIGHT - 150), 4)
    bottom_box = pygame.draw.rect(screen, gray, (0, HEIGHT - 150, WIDTH, 150), 4)
    boxes = []
    colors = [gray, white, gray]

    hi_hat_text = label_font.render('Hi Hat', True, colors[actives[0]])
    screen.blit(hi_hat_text, (30, 40))

    snare_text = label_font.render('Snare', True, colors[actives[1]])
    screen.blit(snare_text, (30, 145))

    drum_text = label_font.render('Bass Drum', True, colors[actives[2]])
    screen.blit(drum_text, (30, 240))

    crash_text = label_font.render('Crash', True, colors[actives[3]])
    screen.blit(crash_text, (30, 340))

    clap_text = label_font.render('Clap', True, colors[actives[4]])
    screen.blit(clap_text, (30, 435))

    for i in range(instruments):
        pygame.draw.line(screen, gray, (0, (i*100)+1), (200, (i*100)+1), 3)

    for i in range(beats):
        for j in range(instruments):
            if clicks[j][i] == -1:
                color = gray
            else:
                if active_list[j] == 1:
                    color = green
                else:
                    color = dark_gray
            rect = pygame.draw.rect(screen, color, 
                                                  ((i * ((WIDTH-200) // beats) + 200), (j*100), 
                                                   ((WIDTH-200) // beats), ((HEIGHT-150)//instruments)
                                                  ), 0, 3
                                   )
            pygame.draw.rect(screen, gold, 
                                          ((i * ((WIDTH-200) // beats) + 200), (j*100), 
                                          ((WIDTH-200) // beats), ((HEIGHT-150)//instruments)
                                          ), 5, 5
                                   )
            pygame.draw.rect(screen, black, 
                                                  ((i * ((WIDTH-200) // beats) + 200), (j*100), 
                                                   ((WIDTH-200) // beats), ((HEIGHT-150)//instruments)
                                                  ), 2, 5
                                   )
            boxes.append((rect, (i, j)))

        active = pygame.draw.rect(screen, blue, (beat*((WIDTH-200)//beats)+200, 0, ((WIDTH-200)//beats), instruments*100), 5, 3)
    return boxes


def draw_save_menu(beat_name, typing):
    pygame.draw.rect(screen, black, (0, 0, WIDTH, HEIGHT))
    menu_text = label_font.render('SAVE MENU: Enter a Name for Current beat', True, white)
    saving_btn = pygame.draw.rect(screen, gray, (WIDTH//2 - 166, HEIGHT*0.75, 400, 100), 0, 5)
    saving_txt = label_font.render('Save beat', True, white)
    screen.blit(saving_txt, (WIDTH//2-50, HEIGHT*0.75+30))
    screen.blit(menu_text, (400, 40))
    exit_btn = pygame.draw.rect(screen, gray, (WIDTH-200, HEIGHT-100, 180, 90), 0, 5)
    exit_text = label_font.render('Close', True, white)
    screen.blit(exit_text, (WIDTH-150, HEIGHT-70))
    if typing:
        pygame.draw.rect(screen, dark_gray, (400, 200, 600, 200), 0, 5)
    entry_rect = pygame.draw.rect(screen, gray, (400, 200, 600, 200), 5, 5)
    entry_text = label_font.render(f'{beat_name}', True, white)
    screen.blit(entry_text, (430, 250))
    return exit_btn, saving_btn, entry_rect

def draw_load_menu():
    pygame.draw.rect(screen, black, (0, 0, WIDTH, HEIGHT))
    menu_text = label_font.render('LOAD MENU: Select a Beat a Load', True, white)
    loading_btn = pygame.draw.rect(screen, gray, (WIDTH//2 - 166, HEIGHT*0.87, 400, 100), 0, 5)
    loading_txt = label_font.render('Load beat', True, white)
    delete_btn = pygame.draw.rect(screen, gray, ((WIDTH//2)-600, HEIGHT*0.87, 200, 100), 0, 5)
    delete_text = label_font.render('Delete Beat', True, white)
    screen.blit(delete_text, ((WIDTH//2)-583, HEIGHT*0.87+30))
    screen.blit(loading_txt, (WIDTH//2-50, HEIGHT*0.87+30))
    screen.blit(menu_text, (400, 40))
    exit_btn = pygame.draw.rect(screen, gray, (WIDTH-200, HEIGHT-100, 180, 90), 0, 5)
    exit_text = label_font.render('Close', True, white)
    screen.blit(exit_text, (WIDTH-150, HEIGHT-70))
    loaded_rectangle = pygame.draw.rect(screen, gray, (190, 90, 1000, 450), 3, 5)
    for beat in range(len(saved_beats)):
        if beat &lt; 10:
            beat_clicked = []
            row_text = medium_font.render(f'{beat + 1}', True, white)
            screen.blit(row_text, (200, 100 + beat * 50))
            name_index_start = saved_beats[beat].index('name: ') + 6
            name_index_end = saved_beats[beat].index(', beats:')
            name_text = medium_font.render(saved_beats[beat][name_index_start:name_index_end], True, white)
            screen.blit(name_text, (240, 100 + beat * 50))
    return exit_btn, loading_btn, delete_btn, loaded_rectangle



run = True
while run:
    timer.tick(fps)
    screen.fill(black)

    boxes = draw_grid(clicked, active_beat, active_list)
    # lower menu buttons
    play_pause = pygame.draw.rect(screen, gray, (50, HEIGHT-100, 200, 100), 0, 5)
    play_text = label_font.render('Play/Pause', True, white)
    screen.blit(play_text, (70, HEIGHT-80))
    if playing:
        play_text2 = medium_font.render('Playing', True, dark_gray)
    else:
        play_text2 = medium_font.render('Paused', True, dark_gray)
    screen.blit(play_text2, (70, HEIGHT-40))
    # bpm stuff
    bpm_rect = pygame.draw.rect(screen, gray, (300, HEIGHT - 100, 250, 100), 5, 5)
    bpm_text = medium_font.render('Beats Per Minute', True, white)
    screen.blit(bpm_text, (330, HEIGHT-80))
    bpm_text2 = medium_font.render(f'{bpm}', True, dark_gray)
    screen.blit(bpm_text2, (330, HEIGHT-40))
    bpm_add_rect = pygame.draw.rect(screen, gray, (560, HEIGHT-100, 45, 45), 0, 5)
    bpm_sub_rect = pygame.draw.rect(screen, gray, (560, HEIGHT-50, 45, 45), 0, 5)
    add_text = medium_font.render('+5', True, white)
    sub_text = medium_font.render('-5', True, white)
    screen.blit(add_text, (565, HEIGHT-89))
    screen.blit(sub_text, (567, HEIGHT-40))

    # beats stuff
    beats_rect = pygame.draw.rect(screen, gray, (700, HEIGHT - 100, 250, 100), 5, 5)
    beats_text = medium_font.render('Beats Per Loop', True, white)
    screen.blit(beats_text, (730, HEIGHT-80))
    beats_text2 = medium_font.render(f'{beats}', True, dark_gray)
    screen.blit(beats_text2, (730, HEIGHT-40))
    beats_add_rect = pygame.draw.rect(screen, gray, (960, HEIGHT-100, 45, 45), 0, 5)
    beats_sub_rect = pygame.draw.rect(screen, gray, (960, HEIGHT-50, 45, 45), 0, 5)
    add_text2 = medium_font.render('+1', True, white)
    sub_text2 = medium_font.render('-1', True, white)
    screen.blit(add_text2, (965, HEIGHT-89))
    screen.blit(sub_text2, (967, HEIGHT-40))

    # instruments rects
    instruments_rects = []
    for i in range(instruments):
        rect = pygame.rect.Rect((0, i * 100), (200, 100))
        instruments_rects.append(rect)
    
    # save and load stuff
    save_button = pygame.draw.rect(screen, gray, (1050, HEIGHT-100, 200, 40), 0, 5)
    save_text = label_font.render('Save Beat', True, white)
    screen.blit(save_text, (1077, HEIGHT - 100))
    load_button = pygame.draw.rect(screen, gray, (1050, HEIGHT-50, 200, 40), 0, 5)
    load_text = label_font.render('Load beat', True, white)
    screen.blit(load_text, (1077, HEIGHT - 50))

    # clear board
    clear_button = pygame.draw.rect(screen, gray, (570, HEIGHT - 150, 200, 40), 0, 5)
    clear_text = label_font.render('Clear', True, white)
    screen.blit(clear_text, (635, HEIGHT-147))

    if save_menu:
        exit_button, saving_button, entry_rectangle = draw_save_menu(beat_name, typing)
    if load_menu:
        exit_button, loading_button, delete_button, load_rectangle = draw_load_menu()

    # sounds stuff
    if beat_changed:
        play_notes()
        beat_changed = False

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            run = False
        if event.type == pygame.MOUSEBUTTONDOWN:
            for i in range(len(boxes)):
                if boxes[i][0].collidepoint(event.pos) and not save_menu and not load_menu:
                    coords = boxes[i][1]
                    clicked[coords[1]][coords[0]] *= -1
        if event.type == pygame.MOUSEBUTTONUP and not save_menu and not load_menu:
            if play_pause.collidepoint(event.pos) and playing:
                playing = False
            elif not playing:
                playing = True
            elif bpm_add_rect.collidepoint(event.pos):
                bpm += 5
            elif bpm_sub_rect.collidepoint(event.pos):
                bpm -= 5
            elif beats_add_rect.collidepoint(event.pos):
                beats += 1
                for i in range(len(clicked)):
                    clicked[i].append(-1)
            elif beats_sub_rect.collidepoint(event.pos):
                beats -= 1
                for i in range(len(clicked)):
                    clicked[i].pop(-1)
            elif clear_button.collidepoint(event.pos):
                clicked = [[-1 for _ in range(beats)] for _ in range(instruments)]
            elif save_button.collidepoint(event.pos):
                save_menu = True
            elif load_button.collidepoint(event.pos):
                load_menu = True
            for i in range(len(instruments_rects)):
                if instruments_rects[i].collidepoint(event.pos):
                    active_list[i] *= -1
        elif event.type == pygame.MOUSEBUTTONUP:
            if exit_button.collidepoint(event.pos):
                save_menu = False
                load_menu = False
                playing = True
                beat_name = ''
                typing = False
            elif entry_rectangle.collidepoint(event.pos):
                if typing:
                    typing = False
                elif not typing:
                    typing = True
            elif saving_button.collidepoint(event.pos):
                file = open('saved_beats.txt', 'w')
                saved_beats.append(f'\nname: {beat_name}, beats: {beats}, bpm: {bpm}, selected: {clicked}')
                for i in range(len(saved_beats)):
                    file.write(str(saved_beats[i]))
                file.close()
                save_menu = False
                typing = False
                beat_name = ''
        if event.type == pygame.TEXTINPUT and typing:
            beat_name += event.text
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_BACKSPACE and len(beat_name) &gt; 0 and typing:
                beat_name = beat_name[:-1]

    beat_length = 3600 // bpm

    if playing:
        if active_length &lt; beat_length:
            active_length += 1
        else:
            active_length = 0
            if active_beat &lt; beats - 1:
                active_beat += 1
                beat_changed = True
            else:
                active_beat = 0
                beat_changed = True

    pygame.display.flip()

pygame.quit()

</code></pre>
<p>This code is 113 lines long. See if there is anything wrong. If you find an anwer, please answer me that.</p>
<p>Thanks</p>
",57,1,-1,5,python;python-3.x;string;visual-studio-code;pygame,2022-05-24 15:33:36,2022-05-24 15:33:36,2022-05-25 08:41:14,hi  i am coding a drum machine  i am making it through pygame in python  i am learning pygame through freecodecamp org  i don t know where this error came from  it says  substring not found   see if there is problem  there is my code   this code is  lines long  see if there is anything wrong  if you find an anwer  please answer me that  thanks
502,502,7397098,72371282,Convert tensorflow 1.x model architecture into tensorflow 2.x compatable code,"<p>I've been having trouble implementing a tensorflow 1.x model arch in tensorflow 2.x. Here is the v1 code</p>
<pre><code>class ZicoMNIST(object):
    def __init__(self, images):
        self.x_image = images
        W0 = tf.get_variable('W0', dtype=tf.float32, shape=(4, 4, 1, 16))
        B0 = tf.get_variable('B0', dtype=tf.float32, shape=(16,))
        W2 = tf.get_variable('W2', dtype=tf.float32, shape=(4, 4, 16, 32))
        B2 = tf.get_variable('B2', dtype=tf.float32, shape=(32,))
        W5 = tf.get_variable('W5', dtype=tf.float32, shape=(1568, 100))
        B5 = tf.get_variable('B5', dtype=tf.float32, shape=(100,))
        W7 = tf.get_variable('W7', dtype=tf.float32, shape=(100, 10))
        B7 = tf.get_variable('B7', dtype=tf.float32, shape=(10,))

        y = tf.pad(self.x_image, [[0, 0], [1, 1], [1, 1], [0, 0]])
        y = tf.nn.conv2d(y, W0, strides=[1, 2, 2, 1], padding='VALID')
        y = tf.nn.bias_add(y, B0)
        y = tf.nn.relu(y)
        y = tf.pad(y, [[0, 0], [1, 1], [1, 1], [0, 0]])
        y = tf.nn.conv2d(y, W2, strides=[1, 2, 2, 1], padding=&quot;VALID&quot;)
        y = tf.nn.bias_add(y, B2)
        y = tf.nn.relu(y)
        y = tf.transpose(y, [0, 3, 1, 2])
        y = tf.reshape(y, [tf.shape(y)[0], -1])
        y = y @ W5 + B5
        y = tf.nn.relu(y)
        y = y @ W7 + B7

        self.logits = y
</code></pre>
<p>It's a class from a machine learning experiment I've been trying to modify. Thanks for all the help!</p>
",17,0,0,5,python;tensorflow;machine-learning;artificial-intelligence;mnist,2022-05-25 07:38:06,2022-05-25 07:38:06,2022-05-25 07:38:06,i ve been having trouble implementing a tensorflow  x model arch in tensorflow  x  here is the v code it s a class from a machine learning experiment i ve been trying to modify  thanks for all the help 
503,503,16644401,70426153,How to resolve problems with pandas in juypter notebook?,"<p>I am working my way down through the chapter 3 of book hands on machine learning with scikit learn. I have imported the mnist dataset, but when I am trying to get some image it is showing me an error: <a href=""https://i.stack.imgur.com/242Zt.jpg"" rel=""nofollow noreferrer"">screenshot of the error and code!</a> I have import every libraries which I need to import, I have also pushed the code to my GitHub where you can see entire juypter notebook. <a href=""https://github.com/isauravmanitripathi/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow-2nd-Edition/blob/main/chaptrer-3-%20classification.ipynb"" rel=""nofollow noreferrer"">link to GitHub notebook</a> can anyone help me with this?</p>
<p>the code which I wrote is:</p>
<pre><code>some_digit = X[0]
some_digit_image = some_digit.reshape(28, 28)
plt.imshow(some_digit_image, cmap=mpl.cm.binary)
plt.axis(&quot;off&quot;)

save_fig(&quot;some_digit_plot&quot;)
plt.show()
</code></pre>
<p>the error I am getting</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>KeyError                                  Traceback (most recent call last)
c:\python3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   3360             try:
-&gt; 3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:

c:\python3\lib\site-packages\pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

c:\python3\lib\site-packages\pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 0

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_5048/2618338264.py in &lt;module&gt;
----&gt; 1 some_digit = X[0]
      2 some_digit_image = some_digit.reshape(28, 28)
      3 plt.imshow(some_digit_image, cmap=mpl.cm.binary)
      4 plt.axis(""off"")
      5 

c:\python3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   3453             if self.columns.nlevels &gt; 1:
   3454                 return self._getitem_multilevel(key)
-&gt; 3455             indexer = self.columns.get_loc(key)
   3456             if is_integer(indexer):
   3457                 indexer = [indexer]

c:\python3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:
-&gt; 3363                 raise KeyError(key) from err
   3364 
   3365         if is_scalar(key) and isna(key) and not self.hasnans:

KeyError: 0</code></pre>
</div>
</div>
</p>
",235,2,0,4,python;pandas;scikit-learn;mnist,2021-12-20 23:33:21,2021-12-20 23:33:21,2022-05-25 06:55:50,i am working my way down through the chapter  of book hands on machine learning with scikit learn  i have imported the mnist dataset  but when i am trying to get some image it is showing me an error   i have import every libraries which i need to import  i have also pushed the code to my github where you can see entire juypter notebook   can anyone help me with this  the code which i wrote is  the error i am getting
504,504,15581170,67000519,Pycaret - Stuck on Setup(),"<p>I'm using Pycaret classification to do some machine learning with my &gt;1 million of data (this includes 18 categorical and 1 numerical features). Pandas Dataframe is storing the data pulled from Oracle database. These steps take about 2-3 minutes. When my data is being preprocessed, it's taking &gt;7 hours. Is there a way to improve the speed?</p>
<p>Here's python SQL code:</p>
<pre><code>from pycaret.classification import *
# init setup
clfl = setup(data=SQL_Query, target = 'cat_ind',silent = True, html = False,categorical_features= [cat1,cat2,cat3,cat4,cat5,cat6,cat7,cat8,cat9,cat10,cat11,cat12,cat13,cat14,cat15,cat16,cat17],numeric_features=['amt'],ignore_features=['paid','catignore']remove_outliers=True,train_size=0.9,handle_unknown_categorical=True, unknown_categorical_method='most_frequent'))
</code></pre>
",824,2,3,2,python;pycaret,2021-04-08 14:23:05,2021-04-08 14:23:05,2022-05-25 05:53:16,i m using pycaret classification to do some machine learning with my  gt  million of data  this includes  categorical and  numerical features   pandas dataframe is storing the data pulled from oracle database  these steps take about   minutes  when my data is being preprocessed  it s taking  gt  hours  is there a way to improve the speed  here s python sql code 
505,505,11402435,72365314,"SVM classifier n_samples, n_splits problem sklearn Python","<p>I'm trying to predict volatility one step ahead with an SVM model based on O'Reilly book example (Machine Learning for Financial Risk Management with Python). When I copy exactly the example (with S&amp;P500 data) it works well but now I'm having troubles with this chunk of code with a particular fund returns data:</p>
<pre><code># returns
r = np.array([        nan,  0.0013933 ,  0.00118874,  0.00076462,  0.00168565,
       -0.00018507, -0.00390753,  0.00307275, -0.00351472])

# horizon
t = 252

# mean of returns
mu = r.mean()

# critical value
z = norm.ppf(0.95)

# realized volatility
vol = r.rolling(5).std()
vol = pd.DataFrame(vol)
vol.reset_index(drop=True, inplace=True)

# SVM GARCH
r_svm = r ** 2
r_svm = r_svm.reset_index()

# inputs X (returns and realized volatility)
X = pd.concat([vol, r_svm], axis=1, ignore_index=True)
X = X.dropna().copy()
X = X.reset_index()
X.drop([1, 'index'], axis=1, inplace=True)

# labels y realized volatility shifted 1 period onward
vol = vol.dropna().reset_index()
vol.drop('index', axis=1, inplace=True)

# linear kernel
svr_lin = SVR(kernel='linear')

# hyperparameters grid
para_grid = {'gamma': sp_rand(),
'C': sp_rand(),
'epsilon': sp_rand()}

# svm classifier (regression?)
clf = RandomizedSearchCV(svr_lin, para_grid)
clf.fit(X[:-1].dropna().values,
vol[1:].values.reshape(-1,))

# prediction
n_vol = clf.predict(X.iloc[-1:])
</code></pre>
<p>The raised error is:</p>
<pre><code>ValueError: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=3.
</code></pre>
<p>The code works with longer returns series so I assume that the problem is the length of the array but I can't figure out how to solve it. can someone help me with that?</p>
",40,1,0,5,python;scikit-learn;svm;forecasting;volatility,2022-05-24 20:39:23,2022-05-24 20:39:23,2022-05-25 03:37:12,i m trying to predict volatility one step ahead with an svm model based on o reilly book example  machine learning for financial risk management with python   when i copy exactly the example  with s amp p data  it works well but now i m having troubles with this chunk of code with a particular fund returns data  the raised error is  the code works with longer returns series so i assume that the problem is the length of the array but i can t figure out how to solve it  can someone help me with that 
506,506,13214782,72369371,Getting error when I apply this condition to my output in my Classification model of Machine Learning,"<p>When I apply this condition to my output (y). It is giving me this error in Jupyter Notebook.</p>
<p><strong>Condition</strong></p>
<pre><code>conditions = [
    (df['Please mention your Previous Semester GPA?'] &lt; 2.3),
    (df['Please mention your Previous Semester GPA?'] &gt;= 2.3) &amp; (df['Please mention your Previous Semester GPA?'] &lt; 3),
    (df['Please mention your Previous Semester GPA?'] &gt;= 3)
]

values = ['High Chances of Bad Marks', 'Average Marks', 'High Chances of Good Marks']

df['Results'] = np.select(conditions, values)

y=df['Results']
y
</code></pre>
<p><strong>Error</strong></p>
<pre><code>SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df['Results'] = np.select(conditions, values)
</code></pre>
",16,0,0,2,python;pandas,2022-05-25 02:26:01,2022-05-25 02:26:01,2022-05-25 02:26:01,when i apply this condition to my output  y   it is giving me this error in jupyter notebook  condition error
507,507,15203917,72347420,Implementing Machine learning model into flutter app,"<p>i have created a machine learning model that can predict laptop prices with python using GridSearchCV algorithm, i want to implement it into my flutter app, so that the user when he chooses his laptop specifications and hit &quot;tap to predict&quot; button, the estimated price will be shown. i don't know backend very well.
My model with GridSearchCV
<a href=""https://i.stack.imgur.com/dC2pQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dC2pQ.png"" alt=""My model with Lasso regression"" /></a></p>
<p>my flutter app</p>
<p><a href=""https://i.stack.imgur.com/MgqJ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MgqJ7.png"" alt=""enter image description here"" /></a></p>
",98,2,1,4,python;flutter;machine-learning;gridsearchcv,2022-05-23 16:28:41,2022-05-23 16:28:41,2022-05-25 01:53:04,my flutter app 
508,508,5048277,72060001,Detect predefined topics in a text,"<p>I would like to find in a text corpus (with long boring text), allusions about some pre-defined topic (let's say i am interested in the 2 topic: &quot;Remuneration&quot; and &quot;Work condition&quot;).
For exemple finding in my corpus where (the specific paragraph) it is pointing problems about &quot;remuneration&quot;.</p>
<p>To accomplish that i first thought about a deterministic approach: building a big dictionary, and thanks to regex maybe flagging those words in the text corpus. It is a very basic idea but i do not know how i could build efficiently my dictionary (i need a lot of words in the lexical field of remuneration). Do you know some website in french which could help me to build this dictionary ?</p>
<p>Perhaps can you think about a more clever approach based on some Machine Learning algorithm which could realize this task (i know about topic modelling but the difference here is that i am focusing on pre-determines subject/topic like &quot;Remuneration&quot;). I need a simple approach :)</p>
",62,3,0,1,text-mining,2022-04-29 20:36:03,2022-04-29 20:36:03,2022-05-24 23:25:25,to accomplish that i first thought about a deterministic approach  building a big dictionary  and thanks to regex maybe flagging those words in the text corpus  it is a very basic idea but i do not know how i could build efficiently my dictionary  i need a lot of words in the lexical field of remuneration   do you know some website in french which could help me to build this dictionary   perhaps can you think about a more clever approach based on some machine learning algorithm which could realize this task  i know about topic modelling but the difference here is that i am focusing on pre determines subject topic like  remuneration    i need a simple approach   
509,509,11160487,55026818,AMAZON Lex Missing credentials in config,"<p>I use the html code from ""<a href=""https://aws.amazon.com/tw/blogs/machine-learning/greetings-visitor-engage-your-web-users-with-amazon-lex/"" rel=""nofollow noreferrer"">https://aws.amazon.com/tw/blogs/machine-learning/greetings-visitor-engage-your-web-users-with-amazon-lex/</a>"" and I modify it to my amazon lex bots. But always reponose ""Error: Missing credentials in config (see console for details)"". Please tell me where should I correct? Thank you.</p>

<p>
    </p>

<pre><code>&lt;head&gt;
    &lt;title&gt;Amazon Lex for test&lt;/title&gt;
    &lt;script src=""https://sdk.amazonaws.com/js/aws-sdk-2.41.0.min.js""&gt;&lt;/script&gt;
    &lt;style language=""text/css""&gt;
        input#wisdom {
            padding: 4px;
            font-size: 1em;
            width: 400px
        }

        input::placeholder {
            color: #ccc;
            font-style: italic;
        }

        p.userRequest {
            margin: 4px;
            padding: 4px 10px 4px 10px;
            border-radius: 4px;
            min-width: 50%;
            max-width: 85%;
            float: left;
            background-color: #7d7;
        }

        p.lexResponse {
            margin: 4px;
            padding: 4px 10px 4px 10px;
            border-radius: 4px;
            text-align: right;
            min-width: 50%;
            max-width: 85%;
            float: right;
            background-color: #bbf;
            font-style: italic;
        }

        p.lexError {
            margin: 4px;
            padding: 4px 10px 4px 10px;
            border-radius: 4px;
            text-align: right;
            min-width: 50%;
            max-width: 85%;
            float: right;
            background-color: #f77;
        }
    &lt;/style&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;h1 style=""text-align:  left""&gt;Easy Join Chat Bot&lt;/h1&gt;
    &lt;div id=""conversation"" style=""width: 400px; height: 400px; border: 1px solid #ccc; background-color: #eee; padding: 4px; overflow: scroll""&gt;&lt;/div&gt;
    &lt;form id=""chatform"" style=""margin-top: 10px"" onsubmit=""return pushChat();""&gt;
        &lt;input type=""text"" id=""wisdom"" size=""80"" value="""" placeholder=""Please enter a question or keyword.""&gt;
    &lt;/form&gt;
    &lt;script type=""text/javascript""&gt;
        // set the focus to the input box
        document.getElementById(""wisdom"").focus();

        AWS.config.region = 'us-east-1'; // Region

        var lexruntime = new AWS.LexRuntime();

        var lexUserId = 'chatbot-demo' + Date.now();
        var sessionAttributes = {};

        function pushChat() {

            // if there is text to be sent...
            var wisdomText = document.getElementById('wisdom');
            if (wisdomText &amp;&amp; wisdomText.value &amp;&amp; wisdomText.value.trim().length &gt; 0) {

                // disable input to show we're sending it
                var wisdom = wisdomText.value.trim();
                wisdomText.value = '...';
                wisdomText.locked = true;

                // send it to the Lex runtime
                var params = {
                    botAlias: 'EasyJoinChatBot',
                    botName: 'EasyJoinChatBot',
                    inputText: wisdom,
                    userId: lexUserId,
                    sessionAttributes: sessionAttributes
                };
                showRequest(wisdom);
                lexruntime.postText(params, function(err, data) {
                    if (err) {
                        console.log(err, err.stack);
                        showError('Error:  ' + err.message + ' (see console for details)')
                    }
                    if (data) {
                        // capture the sessionAttributes for the next cycle
                        sessionAttributes = data.sessionAttributes;
                        // show response and/or error/dialog status
                        showResponse(data);
                    }
                    // re-enable input
                    wisdomText.value = '';
                    wisdomText.locked = false;
                });
            }
            // we always cancel form submission
            return false;
        }

        function showRequest(daText) {

            var conversationDiv = document.getElementById('conversation');
            var requestPara = document.createElement(""P"");
            requestPara.className = 'userRequest';
            requestPara.appendChild(document.createTextNode(daText));
            conversationDiv.appendChild(requestPara);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }

        function showError(daText) {

            var conversationDiv = document.getElementById('conversation');
            var errorPara = document.createElement(""P"");
            errorPara.className = 'lexError';
            errorPara.appendChild(document.createTextNode(daText));
            conversationDiv.appendChild(errorPara);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }

        function showResponse(lexResponse) {

            var conversationDiv = document.getElementById('conversation');
            var responsePara = document.createElement(""P"");
            responsePara.className = 'lexResponse';
            if (lexResponse.message) {
                responsePara.appendChild(document.createTextNode(lexResponse.message));
                responsePara.appendChild(document.createElement('br'));
            }
            if (lexResponse.dialogState === 'ReadyForFulfillment') {
                responsePara.appendChild(document.createTextNode(
                    'Ready for fulfillment'));
                // TODO:  show slot values
            } else {
                responsePara.appendChild(document.createTextNode(
                    '(' + lexResponse.dialogState + ')'));
            }
            conversationDiv.appendChild(responsePara);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }
    &lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</code></pre>

<p><br>
<a href=""https://docs.google.com/document/d/1MV05gMQMnZnIiO5NUeIv1rMLp5PYSuvMDAlBqFfc0jM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/document/d/1MV05gMQMnZnIiO5NUeIv1rMLp5PYSuvMDAlBqFfc0jM/edit?usp=sharing</a></p>
",374,2,2,3,javascript;html;amazon-web-services,2019-03-06 21:06:02,2019-03-06 21:06:02,2022-05-24 16:54:46,i use the html code from  and i modify it to my amazon lex bots  but always reponose error  missing credentials in config  see console for details   please tell me where should i correct  thank you 
510,510,4964095,72360984,Time series forecast with scarce data,"<p>Lets say i want to get a weekly forecast of y (e.g. consumption) for 1 year ahead, but i only got training data of y for the last year, seeing a clear pattern during the year.</p>
<p>I know that y depends on x (population), which was linear monotonic increasing in my data.</p>
<p>My first naive approach is:</p>
<ul>
<li>forecast x with linear regression to get predicted x values for the future</li>
<li>forecast y with $$y_{t1}=(y_{t0} / x_{t0})*x_{t1}$$</li>
</ul>
<p>What would be a more &quot;machine learning&quot; approach to this problem?
Is this even feasible without any additional data (more than one cycle of the timeframe to forecast)?</p>
",10,0,0,2,prediction;forecast,2022-05-24 15:35:38,2022-05-24 15:35:38,2022-05-24 15:47:09,lets say i want to get a weekly forecast of y  e g  consumption  for  year ahead  but i only got training data of y for the last year  seeing a clear pattern during the year  i know that y depends on x  population   which was linear monotonic increasing in my data  my first naive approach is 
511,511,17390412,72359550,Port scan returning all ports open on non-existent network machines,"<p>I'm doing a project for my course that involves creating a port scanner for IP addresses. It involves a user input for a network address that will then generate a list of addresses and a txt file for the ports. I've got it more or less working, but have noticed something strange that I can't explain or find an answer for.</p>
<p>Basically, when I'm testing the network I'm currently on (standard 192.168 home network) it seems to work as intended. It takes time to scan the port and reports whether they're open or closed. For example, my machine will return the correct ports being open while a non-existent machine will list all its ports as closed.</p>
<p>However, if I put in a different network (say 192.169.0.0) it instantly loops through all of the addresses reporting all ports are open, seemingly without scanning them. And that's what doesn't make sense to me, especially since the earlier non-existent machine returns closed as I'd expect.</p>
<p>I've tried googling it and searching around here but couldn't find an answer (one question went unanswered). This is the specific function I'm calling;</p>
<pre><code>def sockCheck(x):
    for y in portList:
        try:
            location = (str(hosts[x]), int(y))
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            c = s.connect_ex(location)
            if c == 0:
                print(&quot;Port &quot;+y+&quot; open&quot;)
            else:
                print(&quot;Port &quot;+y+&quot; closed&quot;)
            print(&quot; &quot;)
            s.close()
        except:
            print(&quot;Port &quot;+y+&quot; unavailable&quot;)
            s.close()
</code></pre>
<p>I know it's not great (still learning). For reference, <code>portList</code> is the list of ports from the imported <code>port.txt</code> file and <code>hosts[x]</code> is how I'm looping through my generated IP addresses (that I need to individually check their ports).</p>
<p>I'll post other code if requested.</p>
",26,0,1,5,python;sockets;ip;ip-address;port-scanning,2022-05-24 13:54:23,2022-05-24 13:54:23,2022-05-24 14:51:50,i m doing a project for my course that involves creating a port scanner for ip addresses  it involves a user input for a network address that will then generate a list of addresses and a txt file for the ports  i ve got it more or less working  but have noticed something strange that i can t explain or find an answer for  basically  when i m testing the network i m currently on  standard   home network  it seems to work as intended  it takes time to scan the port and reports whether they re open or closed  for example  my machine will return the correct ports being open while a non existent machine will list all its ports as closed  however  if i put in a different network  say      it instantly loops through all of the addresses reporting all ports are open  seemingly without scanning them  and that s what doesn t make sense to me  especially since the earlier non existent machine returns closed as i d expect  i ve tried googling it and searching around here but couldn t find an answer  one question went unanswered   this is the specific function i m calling  i know it s not great  still learning   for reference  portlist is the list of ports from the imported port txt file and hosts x  is how i m looping through my generated ip addresses  that i need to individually check their ports   i ll post other code if requested 
512,512,18059670,72358635,How do I convert values of data frame to string type and how do I use train_test_split to generate the two arrays with same dimensions?,"<p>I am trying to learn more about machine learning. I have this data of spam/non-spam emails and trying to build the classifier. to use &quot;CountVectorizer&quot;, I need to convert data frame values (emails) to the string type but for some reason, after looping it and converting, values still remain into a pandas series. 1. How would I fix that ? p.s I will put the code as well.</p>
<p>'''</p>
<pre><code>import re
def preprocessor(e):
    e = re.sub(&quot;[^a-zA-Z0-9]+&quot;, &quot; &quot;,e)
    return e.lower()
indexes = list(df['content'].index)
for i in indexes:
    df['content'][i] = preprocessor(str(df['content'][i]))
    df['name'][i] = preprocessor(str(df['name'][i]))
    df['category'][i] = preprocessor(str(df['category'][i]))   
</code></pre>
<p>'''
code for converting to string types</p>
<ol start=""2"">
<li>secondly, assuming that I did it and somehow worked, next I need to apply CountVectorizer which should generate two arrays, for X --&gt; email texts and for y --&gt; category (spar or not spam). it does generate the arrays but after I apply train_test_split and then later try to fit my model, I get error &quot; y should be a 1d array, got an array of shape (3455, 1483) instead.&quot; --&gt; I try to reshape but then the dimensions get all messed up.</li>
</ol>
<p>'''</p>
<pre><code>[vectorizer = CountVectorizer()
x = vectorizer.fit_transform(df\['content'\])
x = x.toarray()

y = vectorizer.fit_transform(df\['category'\])
y = y.toarray()


x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33, random_state=42)

model = LogisticRegression(random_state=0)

model.fit(x_train,y_train)][1]
</code></pre>
<p>'''</p>
",17,0,0,3,python;arrays;machine-learning,2022-05-24 12:43:09,2022-05-24 12:43:09,2022-05-24 12:43:09,i am trying to learn more about machine learning  i have this data of spam non spam emails and trying to build the classifier  to use  countvectorizer   i need to convert data frame values  emails  to the string type but for some reason  after looping it and converting  values still remain into a pandas series    how would i fix that   p s i will put the code as well             
513,513,16961408,72341819,Machine Learning Question on missing values in training and test data,"<p>I'm training a text classifier for binary classification.  In my training data, there are null values in the .csv file in the text portion, and there are also null values in my test file.  I have converted both files to a dataframe (Pandas).  This is a small percentage of the overall data (less than 0.01).</p>
<p>Knowing this - is it better to replace the null text fields with an empty string or leave it as as empty?  And if the answer is replace with empty string, is it &quot;acceptable&quot; to do the same for the test csv file <em>before</em> running it against the model?</p>
",23,1,0,3,python;sentiment-analysis;naivebayes,2022-05-23 04:25:21,2022-05-23 04:25:21,2022-05-24 02:44:39,i m training a text classifier for binary classification   in my training data  there are null values in the  csv file in the text portion  and there are also null values in my test file   i have converted both files to a dataframe  pandas    this is a small percentage of the overall data  less than     knowing this   is it better to replace the null text fields with an empty string or leave it as as empty   and if the answer is replace with empty string  is it  acceptable  to do the same for the test csv file before running it against the model 
514,514,11860556,72351077,Python (Scikeras) - ValueError: Invalid parameter layers for estimator KerasClassifier,"<p>I am trying to create a convolutional neural network using GridSearchCV and the scikeras wrapper, but I have been receiving an error that I cannot figure out the cause of.</p>
<p>The core of the error is this:</p>
<blockquote>
<p>ValueError: Invalid parameter layers for estimator KerasClassifier.
This issue can likely be resolved by setting this parameter in the
KerasClassifier constructor: <code>KerasClassifier(layers=[128])</code> Check the
list of available parameters with <code>estimator.get_params().keys()</code></p>
</blockquote>
<p>Please find the full error after the code. I have tried changing a few line, or adding different parameters, however nothing seems to change the error I am receiving. Here is the code:</p>
<pre><code># first model using the chosen parameters

# Part 1: Create the model
def cnn_model0(layers):
  cnn = tf.keras.models.Sequential() # initialising the CNN
  
  # model layers
  cnn.add( # Step 1 - Convolution
      tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=&quot;same&quot;, activation=&quot;relu&quot;, input_shape=[50, 50, 3]))
  cnn.add( # Step 2 - Pooling
      tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))
  cnn.add( # Second convolutional layer
      tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=&quot;same&quot;, activation=&quot;relu&quot;))
  cnn.add( # Second pooling layer 
      tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))
  cnn.add( # Step 3 - Flattening
      tf.keras.layers.Flatten())

  # Step 4 - Full connection (FC)
  for i, nodes in enumerate(layers):
      cnn.add(tf.keras.layers.Dense(units = nodes, activation = 'relu'))
  cnn.add(tf.keras.layers.Dense(units = 43, activation = 'softmax'))

  # Compiling the CNN
  cnn.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) 
  return cnn


# Part 2: Fitting the CNN model
model = KerasClassifier(build_fn = cnn_model0, verbose = 1)

# establish the grid parameters
layers = [[128], (256, 128), (200, 150, 120)]
param_grid = dict(layers = layers)

# fit GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, verbose = 1)
grid_results = grid.fit(X_train, y_train, validation_data = (X_val, y_val))

# Part 3: Printing the results
print(&quot;Best: {0}, using {1}&quot;.format(grid_results.best_score_, grid_results.best_params_))

# result values
means = grid_results.cv_results_['mean_test_score']
stds = grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{0} ({1}) with: {2}'.format(mean, stdev, param))
</code></pre>
<p>The main section that seems to be the cause of the error is:</p>
<pre><code>model = KerasClassifier(build_fn = cnn_model0, verbose = 1)
layers = [[128], (256, 128), (200, 150, 120)]
param_grid = dict(layers = layers)
grid = GridSearchCV(estimator = model, param_grid = param_grid, verbose = 1)
grid_results = grid.fit(X_train, y_train, validation_data = (X_val, y_val))
</code></pre>
<p>From the message I am receiving, I am thinking there is something incorrect within the model, and the layers being established. Any help on narrowing down the cause would be greatly appreciated. I am still unfamiliar with a lot of machine learning.</p>
<p>Thanks in advance.</p>
<p>Full Error Message:</p>
<pre><code>Fitting 5 folds for each of 3 candidates, totalling 15 fits

---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-33-b6a6389b51ee&gt; in &lt;module&gt;()
     35 # fit GridSearchCV
     36 grid = GridSearchCV(estimator = model, param_grid = param_grid, verbose = 1)
---&gt; 37 grid_results = grid.fit(X_train, y_train, validation_data = (X_val, y_val))
     38 
     39 # Part 3: Printing the results

12 frames

/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py in set_params(self, **params)
   1153                         &quot;\nCheck the list of available parameters with&quot;
   1154                         &quot; `estimator.get_params().keys()`&quot;
-&gt; 1155                     ) from None
   1156         return self
   1157 

ValueError: Invalid parameter layers for estimator KerasClassifier.
This issue can likely be resolved by setting this parameter in the KerasClassifier constructor:
`KerasClassifier(layers=[128])`
Check the list of available parameters with `estimator.get_params().keys()`
</code></pre>
",52,1,1,3,python;tensorflow;gridsearchcv,2022-05-23 20:55:55,2022-05-23 20:55:55,2022-05-24 00:14:00,i am trying to create a convolutional neural network using gridsearchcv and the scikeras wrapper  but i have been receiving an error that i cannot figure out the cause of  the core of the error is this  please find the full error after the code  i have tried changing a few line  or adding different parameters  however nothing seems to change the error i am receiving  here is the code  the main section that seems to be the cause of the error is  from the message i am receiving  i am thinking there is something incorrect within the model  and the layers being established  any help on narrowing down the cause would be greatly appreciated  i am still unfamiliar with a lot of machine learning  thanks in advance  full error message 
515,515,19182701,72352849,"Fold1: preprocessor 1/1, model 1/6: Error in if (!all(o)) : missing value where TRUE/FALSE needed","<p>I'm a student fairly new to r and machine learning, and I'm trying to finish my final report.
I want to use multinominal logistic regression through cross-validation to tune my parameters, but I keep receiving this error :  Fold1: preprocessor 1/1, model 1/6: Error in if (!all(o)) : missing value where TRUE/FALSE needed</p>
<p>When trying to fix it, I think the error is from the NAs that haven't been replaced by means and modes. I have used step_impute_median and step_impute_mode to replace my NAs, but when viewing it, all the NAs are still there.  Why didn't the step function work? what can I do to fix this?</p>
<p>the following is my code, thank you!</p>
<pre><code>
library(tidymodels)
young &lt;- read.csv(&quot;https://kirk.cdkm.com/convert/file/st4u2ucw5qg4f4lrfrfkswjfetflr66d/responses.html&quot;)
young &lt;- young %&gt;%
mutate(Group=ifelse(Friends.versus.money&gt;3,&quot;friend&quot;,ifelse(Friends.versus.money==3,&quot;mo_fr&quot;,&quot;money&quot;))) %&gt;% 
  select(-Friends.versus.money)

glmnet_model &lt;- 
  multinom_reg(
    penalty=tune(), mixture = tune()) %&gt;% 
  set_engine(&quot;glmnet&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)

glmnet_param &lt;- parameters(glmnet_model)
glmnet_param &lt;- glmnet_param %&gt;% 
  update(penalty=penalty(c(-4,0)),
         mixture = mixture(c(0,1)))
glmnet_grid &lt;- glmnet_param %&gt;% 
  grid_regular(levels = c(penalty = 50, mixture = 6))

glmnet_rec &lt;- 
  recipe(Group ~., data = young) %&gt;% 
  step_impute_median(all_numeric_predictors()) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;%
  step_impute_mode(all_nominal_predictors()) %&gt;% 
  step_dummy(all_nominal_predictors())

glmnet_wflow &lt;- 
  workflow() %&gt;% 
  add_model(glmnet_model) %&gt;% 
  add_recipe(glmnet_rec)

fvm_class_metrics &lt;- metric_set(accuracy, f_meas)

fvm_folds &lt;- 
  vfold_cv(young, v = 5, repeats = 1)

glmnet_tune &lt;- 
  glmnet_wflow %&gt;% 
  tune_grid(
    fvm_folds,
    grid = glmnet_grid,
    metrics = fvm_class_metrics  )
</code></pre>
",27,0,0,4,r;machine-learning;cross-validation;glmnet,2022-05-23 23:19:13,2022-05-23 23:19:13,2022-05-23 23:19:13,when trying to fix it  i think the error is from the nas that haven t been replaced by means and modes  i have used step_impute_median and step_impute_mode to replace my nas  but when viewing it  all the nas are still there   why didn t the step function work  what can i do to fix this  the following is my code  thank you 
516,516,19017313,72351897,Plotting Legend with Multi-Class Y values (Matplotlib),"<p>Will be a juvenile question but just getting accustom to matplotlib &amp; plotting in python!</p>
<p>I've ran the below code to get my machine learning SVC model plotted - it works perfectly however im wanting to plot the multiclass Y values on the legend.</p>
<p>The output (Y) values are either 0,1,2. These were mapped through 'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2</p>
<p><strong>couple of questions</strong></p>
<ol>
<li><p>How do I plot the legend so it specifies the Y class in the plot (0,1,2)?</p>
</li>
<li><p>if possible, how do I plot the legend so it specifies the old Y class before remapping ('Iris etc.)?</p>
<pre><code> import pandas as pd
 import numpy as np
 from sklearn import svm, datasets
 import matplotlib.pyplot as plt
 from sklearn.metrics import classification_report

 ##load dataset
 url = url = &quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&quot;
 names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
 df = pd.read_csv(url, names=names)

 ## creating a new column from our dict mapping
 mapping = {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}
 df['class_coded'] = df['class'].map(mapping)

 ## X &amp; Y values
 X = df[['sepal-width', 'sepal-length']].values
 y = df[['class_coded']].values

 ## creating our mesh
 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 ## x axis min and max
 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 ## y axis min and max
 h = (x_max / x_min)/100
 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
 np.arange(y_min, y_max, h))    
 X_plot = np.c_[xx.ravel(), yy.ravel()]

 ## create our model &amp; reshape 
 svc_classifier = svm.SVC(kernel='linear', C=1.0).fit(X, y)
 Model = svc_classifier.predict(X_plot)
 Model_new = Model.reshape(xx.shape)

 ## plotting our graph
 plt.figure(figsize=(15, 5))
 plt.subplot(121)
 plt.contourf(xx, yy, Model_new, cmap=plt.cm.tab10, alpha=0.3)
 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)
 plt.xlabel('Sepal length')   
 plt.ylabel('Sepal width')
 plt.xlim(xx.min(), xx.max())
 plt.title('Support Vector Classifier with linear kernel')
</code></pre>
</li>
</ol>
",13,0,0,4,python;pandas;matplotlib;scikit-learn,2022-05-23 21:58:28,2022-05-23 21:58:28,2022-05-23 21:58:28,will be a juvenile question but just getting accustom to matplotlib  amp  plotting in python  i ve ran the below code to get my machine learning svc model plotted   it works perfectly however im wanting to plot the multiclass y values on the legend  the output  y  values are either     these were mapped through  iris setosa     iris versicolor     iris virginica   couple of questions how do i plot the legend so it specifies the y class in the plot       if possible  how do i plot the legend so it specifies the old y class before remapping   iris etc   
517,517,15379178,72339128,Cython: error while building extension: Microsoft Visual C++ 14.0 or greater is required,"<h3>Short Description:</h3>
<p>I'm trying to build an example cython script, but when I run the <code>python setup.py build_ext --inplace</code> command, I get an error saying that I need MS Visual C++ version 14.0 or greater. I've tried a lot of the things on related SO threads and other forums but to no avail in resolving the issue.</p>
<h3>Longer Description:</h3>
<p>The specific cython script:</p>
<p><code>test.pyx</code>:</p>
<pre class=""lang-py prettyprint-override""><code>cpdef int test(int n):
    cdef int sum_ = 0, i = 0
    while i &lt; n:
        sum_ += i
        i += 1

    return sum_
</code></pre>
<p><code>setup.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code># from setuptools import setup
from distutils.core import setup
from Cython.Build import cythonize

setup(
    name = &quot;test&quot;,
    ext_modules = cythonize('test.pyx'), # accepts a glob pattern
)
</code></pre>
<p>I'm on <code>python 3.10.0</code> and <code>cython 0.29.30</code> and am using <code>Windows 10</code></p>
<p>And here is the error that I get:</p>
<pre><code>C:\Users\LENOVO PC\PycharmProjects\MyProject\cython_src&gt;py setup.py build_ext --inplace
Compiling test.pyx because it changed.
[1/1] Cythonizing test.pyx
C:\Users\LENOVO PC\AppData\Local\Programs\Python\Python310\lib\site-packages\Cython\Compiler\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\Users\LENOVO PC\PycharmProjects\MyProject\cython_src\test.pyx
  tree = Parsing.p_module(s, pxd, full_module_name)
running build_ext
building 'test' extension
error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/

C:\Users\LENOVO PC\PycharmProjects\MyProject\cython_src&gt;
</code></pre>
<p>I've tried numerous different things:</p>
<ol>
<li>Visited the link in the error and downloaded and installed the build tools</li>
<li>Installed multiple versions of Visual Studio (2022, 2019, 2017) CE and Build Tools</li>
<li>Uninstalled all of the above and reinstalling MSVC 2019 CE and Build Tools from scratch</li>
<li>Browsed through a lot of other related SO threads about this error and none of the solutions presented in them have worked for me so far, they have broadly included:
<ul>
<li>Building the script from the developer console</li>
<li>Updating setuptools</li>
<li>Installing numerous different components in MSVC</li>
<li>Installing numerous vc redistributables</li>
</ul>
</li>
</ol>
<p>But none of these have worked for me unfortunately, and I keep getting the same error.</p>
<p>I personally think the cause might be related to missing registry keys, or missing path variables, because the MSVC tools are definitely installed on my machine, but the setup script is unable to find them, but I do not know how to find out for sure.</p>
<p>Some additional info that might be relevant(?):</p>
<p>I've used Cython on the same machine before, and it used to work just fine, I had Visual Studio 2019 at this time. At some point though, I uninstalled it and upgraded to Visual Studio 2022 because I was learning C++ and wanted to use a newer C++ standard. Oddly enough, when I did this, the IDE that I use for C++ (CLion) stopped detecting the MSVC toolchain as well, and I never got it to correctly detect it again (I've been using WSL toolchain on CLion since)</p>
<p>Recently when I tried to use Cython again and got this error, and did a lot of digging, I realised that the two incidents might be related, so I thought that it may be worth mentioning here.</p>
",52,1,0,3,python;visual-studio;cython,2022-05-22 21:23:53,2022-05-22 21:23:53,2022-05-23 21:38:12,i m trying to build an example cython script  but when i run the python setup py build_ext   inplace command  i get an error saying that i need ms visual c   version   or greater  i ve tried a lot of the things on related so threads and other forums but to no avail in resolving the issue  the specific cython script  test pyx  setup py  i m on python    and cython    and am using windows  and here is the error that i get  i ve tried numerous different things  but none of these have worked for me unfortunately  and i keep getting the same error  i personally think the cause might be related to missing registry keys  or missing path variables  because the msvc tools are definitely installed on my machine  but the setup script is unable to find them  but i do not know how to find out for sure  some additional info that might be relevant     i ve used cython on the same machine before  and it used to work just fine  i had visual studio  at this time  at some point though  i uninstalled it and upgraded to visual studio  because i was learning c   and wanted to use a newer c   standard  oddly enough  when i did this  the ide that i use for c    clion  stopped detecting the msvc toolchain as well  and i never got it to correctly detect it again  i ve been using wsl toolchain on clion since  recently when i tried to use cython again and got this error  and did a lot of digging  i realised that the two incidents might be related  so i thought that it may be worth mentioning here 
518,518,11611246,67557515,InvalidArgumentError: required broadcastable shapes at loc(unknown),"<p><strong>Background</strong></p>
<p>I am totally new to Python and to machine learning. I just tried to set up a UNet from code I found on the internet and wanted to adapt it to the case I'm working on bit for bit. When trying to <code>.fit</code> the UNet to the training data, I received the following error:</p>
<pre><code>InvalidArgumentError:  required broadcastable shapes at loc(unknown)
     [[node Equal (defined at &lt;ipython-input-68-f1422c6f17bb&gt;:1) ]] [Op:__inference_train_function_3847]
</code></pre>
<p>I get a lot of results when I search for it, but mostly they are different errors.</p>
<p>What does this mean? And, more importantly, how can I fix it?</p>
<p><strong>The code that caused the error</strong></p>
<p>The context of this error is as follows:
I want to segment images and label the different classes.
I set up directories &quot;trn&quot;, &quot;tst&quot; and &quot;val&quot; for training, test, and validation data. The <code>dir_dat()</code> function applies <code>os.path.join()</code> to get the full path to the respective <a href=""https://www.dropbox.com/sh/g4yr4qjgjtsytmi/AABJXVehoCw5In9wQvL7Llu2a?dl=0"" rel=""noreferrer"">data set</a>. Each of the 3 folders has sub directories for each class labeled with integers. In each folder, there are some <code>.tif</code> images for the respective class.</p>
<p>I defined the following image data generators (training data is sparse, hence the augmentation):</p>
<pre><code>classes = np.array([ 0,  2,  4,  6,  8, 11, 16, 21, 29, 30, 38, 39, 51])
bs = 15 # batch size

augGen = ks.preprocessing.image.ImageDataGenerator(rotation_range = 365,
                                                   width_shift_range = 0.05,
                                                   height_shift_range = 0.05,
                                                   horizontal_flip = True,
                                                   vertical_flip = True,
                                                   fill_mode = &quot;nearest&quot;) \
    .flow_from_directory(directory = dir_dat(&quot;trn&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs, seed = 42)
    
tst_batches = ks.preprocessing.image.ImageDataGenerator() \
    .flow_from_directory(directory = dir_dat(&quot;tst&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs, shuffle = False)

val_batches = ks.preprocessing.image.ImageDataGenerator() \
    .flow_from_directory(directory = dir_dat(&quot;val&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs)
</code></pre>
<p>Then I set up the UNet based on <a href=""https://github.com/bnsreenu/python_for_microscopists/blob/master/074-Defining%20U-net%20in%20Python%20using%20Keras.py"" rel=""noreferrer"">this example</a>. Here, I altered a few parameters to adapt the UNet to the situation (multiple classes), namely activation in the last layer and the loss function:</p>
<pre><code>layer_in = ks.layers.Input(shape = (imgr, imgc, imgdim))
# convert pixel integer values to float
inVals = ks.layers.Lambda(lambda x: x / 255)(layer_in)

# Contraction path
c1 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(inVals)
c1 = ks.layers.Dropout(0.1)(c1)
c1 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c1)
p1 = ks.layers.MaxPooling2D((2, 2))(c1)

c2 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p1)
c2 = ks.layers.Dropout(0.1)(c2)
c2 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c2)
p2 = ks.layers.MaxPooling2D((2, 2))(c2)
 
c3 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p2)
c3 = ks.layers.Dropout(0.2)(c3)
c3 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c3)
p3 = ks.layers.MaxPooling2D((2, 2))(c3)
 
c4 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p3)
c4 = ks.layers.Dropout(0.2)(c4)
c4 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c4)
p4 = ks.layers.MaxPooling2D(pool_size = (2, 2))(c4)
 
c5 = ks.layers.Conv2D(256, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p4)
c5 = ks.layers.Dropout(0.3)(c5)
c5 = ks.layers.Conv2D(256, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c5)

# Expansive path 
u6 = ks.layers.Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c5)
u6 = ks.layers.concatenate([u6, c4])
c6 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u6)
c6 = ks.layers.Dropout(0.2)(c6)
c6 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c6)
 
u7 = ks.layers.Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c6)
u7 = ks.layers.concatenate([u7, c3])
c7 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u7)
c7 = ks.layers.Dropout(0.2)(c7)
c7 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c7)
 
u8 = ks.layers.Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c7)
u8 = ks.layers.concatenate([u8, c2])
c8 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u8)
c8 = ks.layers.Dropout(0.1)(c8)
c8 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c8)
 
u9 = ks.layers.Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c8)
u9 = ks.layers.concatenate([u9, c1], axis = 3)
c9 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u9)
c9 = ks.layers.Dropout(0.1)(c9)
c9 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c9)
 
out = ks.layers.Conv2D(1, (1, 1), activation = &quot;softmax&quot;)(c9)
 
model = ks.Model(inputs = layer_in, outputs = out)
model.compile(optimizer = &quot;adam&quot;, loss = &quot;sparse_categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;])
model.summary()
</code></pre>
<p>Finally, I defined callbacks and ran the training, which produced the error:</p>
<pre><code>cllbs = [
    ks.callbacks.EarlyStopping(patience = 4),
    ks.callbacks.ModelCheckpoint(dir_out(&quot;Checkpoint.h5&quot;), save_best_only = True),
    ks.callbacks.TensorBoard(log_dir = './logs'),# log events for TensorBoard
    ]

model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)
</code></pre>
<p><strong>Full console output</strong></p>
<p>This is the full output when running the last line (in case it helps solving the issue):</p>
<pre><code>trained = model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)
Epoch 1/5
Traceback (most recent call last):

  File &quot;&lt;ipython-input-68-f1422c6f17bb&gt;&quot;, line 1, in &lt;module&gt;
    trained = model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\keras\engine\training.py&quot;, line 1183, in fit
    tmp_logs = self.train_function(iterator)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 889, in __call__
    result = self._call(*args, **kwds)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 950, in _call
    return self._stateless_fn(*args, **kwds)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 3023, in __call__
    return graph_function._call_flat(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1960, in _call_flat
    return self._build_call_outputs(self._inference_function.call(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 591, in call
    outputs = execute.execute(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

InvalidArgumentError:  required broadcastable shapes at loc(unknown)
     [[node Equal (defined at &lt;ipython-input-68-f1422c6f17bb&gt;:1) ]] [Op:__inference_train_function_3847]

Function call stack:
train_function
</code></pre>
",21524,5,8,5,python;tensorflow;neural-network;conv-neural-network;tf.keras,2021-05-16 19:29:01,2021-05-16 19:29:01,2022-05-23 21:34:17,background i am totally new to python and to machine learning  i just tried to set up a unet from code i found on the internet and wanted to adapt it to the case i m working on bit for bit  when trying to  fit the unet to the training data  i received the following error  i get a lot of results when i search for it  but mostly they are different errors  what does this mean  and  more importantly  how can i fix it  the code that caused the error i defined the following image data generators  training data is sparse  hence the augmentation   then i set up the unet based on   here  i altered a few parameters to adapt the unet to the situation  multiple classes   namely activation in the last layer and the loss function  finally  i defined callbacks and ran the training  which produced the error  full console output this is the full output when running the last line  in case it helps solving the issue  
519,519,13499265,72326132,Unable to connect worker node to master using K3S,"<p>I am trying to setup a K3S cluster for learning purposes but I am having trouble connecting the master node with agents. I have looked several tutorials and discussions on this but I can't find a solution. I know I am probably missing something obvious (due to my lack of knowledge), but still help would be much appreciated.</p>
<p>I am using two AWS t2.micro instances with default configuration.</p>
<p>When ssh into the master and installed K3S using</p>
<pre><code>curl -sfL https://get.k3s.io | sh -s - --no-deploy traefik --write-kubeconfig-mode 644 --node-name k3s-master-01
</code></pre>
<p>with kubectl get nodes, I am able to see the master</p>
<pre><code>NAME            STATUS   ROLES                  AGE   VERSION
k3s-master-01   Ready    control-plane,master   13s   v1.23.6+k3s1
</code></pre>
<p>So far it seems I am doing things right. From what I understand, I am supposed to configure the kubeconfig file. So, I accessed it by using</p>
<pre><code>cat /etc/rancher/k3s/k3s.yaml
</code></pre>
<p>I copied the configuration file and the server info to match the private IP I took from AWS console, resulting in something like this</p>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;lots_of_info&gt;
    server: https://&lt;master_private_IP&gt;:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: &lt;my_certificate_data&gt;
    client-key-data: &lt;my_key_data&gt;
</code></pre>
<p>Then, I ran <code>vi ~/.kube/config</code>, and there I pasted the kubeconfig file</p>
<p>Finally, I grabbed the token with <code>cat /var/lib/rancher/k3s/server/node-token</code>, ssh into the other machine and then run the following</p>
<pre><code>curl -sfL https://get.k3s.io | K3S_NODE_NAME=k3s-worker-01 K3S_URL=https://&lt;master_private_IP&gt;:6443 K3S_TOKEN=&lt;master_token&gt; sh -
</code></pre>
<p>The output is</p>
<pre><code>[INFO]  Finding release for channel stable
[INFO]  Using v1.23.6+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.23.6+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.23.6+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Creating /usr/local/bin/ctr symlink to k3s
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service
[INFO]  systemd: Enabling k3s-agent unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.
[INFO]  systemd: Starting k3s-agent
</code></pre>
<p>By this output, it looks like I have created an agent. However, when I run kubectl get nodes in the master, I still get</p>
<pre><code>NAME            STATUS   ROLES                  AGE   VERSION
k3s-master-01   Ready    control-plane,master   12m   v1.23.6+k3s1
</code></pre>
<p>What is the thing I was supposed to do in order to get the agent connected to the master? I am guess I am probably missing something simple, but I just can't seem to find the solution. I've read all the documentation but it is still not clear to me where I am making the mistake. I've tried saving the private master IP and token into the agent as environmental variables with export K3S_TOKEN=master_token and K3S_URL=master_private_IP and then simply running <code>curl -sfL https://get.k3s.io | sh -</code>  but I still can't see the worker nodes when running <code>kubectl get nodes</code></p>
<p>Any help would be appreciated.</p>
",143,1,0,1,k3s,2022-05-21 07:28:38,2022-05-21 07:28:38,2022-05-23 21:30:30,i am trying to setup a ks cluster for learning purposes but i am having trouble connecting the master node with agents  i have looked several tutorials and discussions on this but i can t find a solution  i know i am probably missing something obvious  due to my lack of knowledge   but still help would be much appreciated  i am using two aws t micro instances with default configuration  when ssh into the master and installed ks using with kubectl get nodes  i am able to see the master so far it seems i am doing things right  from what i understand  i am supposed to configure the kubeconfig file  so  i accessed it by using i copied the configuration file and the server info to match the private ip i took from aws console  resulting in something like this then  i ran vi    kube config  and there i pasted the kubeconfig file finally  i grabbed the token with cat  var lib rancher ks server node token  ssh into the other machine and then run the following the output is by this output  it looks like i have created an agent  however  when i run kubectl get nodes in the master  i still get what is the thing i was supposed to do in order to get the agent connected to the master  i am guess i am probably missing something simple  but i just can t seem to find the solution  i ve read all the documentation but it is still not clear to me where i am making the mistake  i ve tried saving the private master ip and token into the agent as environmental variables with export ks_token master_token and ks_url master_private_ip and then simply running curl  sfl https   get ks io   sh    but i still can t see the worker nodes when running kubectl get nodes any help would be appreciated 
520,520,10489954,72349775,How to get AmlDatastore image url from Azure,"<p>I'm trying to use auto ML to train an object detection model. I have all of my images uploaded to azure and am trying to create my jsonl file for the tabular dataset. I'm using this document: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models</a></p>
<p>This is their example of an image url: AmlDatastore://image_data/Image_01.png</p>
<p>How do I get that url for my images in my datastore? When I view the image in the datastore, it shows me an https url. I tried using that one but I get an error that the file is not found. I've also tried making an AmlDatastore url with a path to my image with no luck there either.</p>
",21,0,0,2,azure;automl,2022-05-23 19:24:50,2022-05-23 19:24:50,2022-05-23 19:24:50,i m trying to use auto ml to train an object detection model  i have all of my images uploaded to azure and am trying to create my jsonl file for the tabular dataset  i m using this document   this is their example of an image url  amldatastore   image_data image_ png how do i get that url for my images in my datastore  when i view the image in the datastore  it shows me an https url  i tried using that one but i get an error that the file is not found  i ve also tried making an amldatastore url with a path to my image with no luck there either 
521,521,11962765,57610129,Do Spark-NLP pretrained pipelines only work on linux systems?,"<p>I am trying to set up a simple code where I pass a dataframe and test it with the pretrained explain pipeline provided by johnSnowLabs Spark-NLP library. 
I am using jupyter notebooks from anaconda and have a spark scala kernet setup using apache toree. Everytime I run the step where it should load the pretrained pipeline, it throws a tensorflow error. Is there a way we can run this on windows locally?</p>

<pre><code>I was trying this in a maven project earlier and the same error had happened. Another colleague tried it on a linux system and it worked. Below is the code I have tried and the error that it gave.


import org.apache.spark.ml.PipelineModel
import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline
import com.johnsnowlabs.nlp.SparkNLP
import org.apache.spark.sql.SparkSession

val spark: SparkSession = SparkSession
    .builder()
    .appName(""test"")
    .master(""local[*]"")
    .config(""spark.driver.memory"", ""4G"")
    .config(""spark.kryoserializer.buffer.max"", ""200M"")
    .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
    .getOrCreate()

val testData = spark.createDataFrame(Seq(
    (1, ""Google has announced the release of a beta version of the popular TensorFlow machine learning library""),
    (2, ""Donald John Trump (born June 14, 1946) is the 45th and current president of the United States""))).toDF(""id"", ""text"")
val pipeline = PretrainedPipeline(""explain_document_dl"", lang = ""en"") //this is where it gives error
val annotation = pipeline.transform(testData)

  annotation.show()

  annotation.select(""entities.result"").show(false)
</code></pre>

<p>Below error occurs:</p>

<blockquote>
  <p>Name: java.lang.UnsupportedOperationException Message: Spark NLP tried
  to load a Tensorflow Graph using Contrib module, but failed to load it
  on this system. If you are on Windows, this operation is not
  supported. Please try a noncontrib model. If not the case, please
  report this issue. Original error message:</p>
  
  <p>Op type not registered 'BlockLSTM' in binary running on
  'MyMachine'. Make sure the Op and Kernel are registered in the
  binary running in this process. Note that if you are loading a saved
  graph which used ops from tf.contrib, accessing (e.g.)
  <code>tf.contrib.resampler</code> should be done before importing the graph, as
  contrib ops are lazily registered when the module is first accessed.
  StackTrace:  Op type not registered 'BlockLSTM' in binary running on
  'MyMachine'. Make sure the Op and Kernel are registered in the
  binary running in this process. Note that if you are loading a saved
  graph which used ops from tf.contrib, accessing (e.g.)
  <code>tf.contrib.resampler</code> should be done before importing the graph, as
  contrib ops are lazily registered when the module is first accessed.<br>
  at
  com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.readGraph(TensorflowWrapper.scala:163)
  at
  com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.read(TensorflowWrapper.scala:202)
  at
  com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel$class.readTensorflowModel(TensorflowSerializeModel.scala:73)
  at
  com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel$.readTensorflowModel(NerDLModel.scala:134)
  at
  com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$class.readNerGraph(NerDLModel.scala:112)
  at
  com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel$.readNerGraph(NerDLModel.scala:134)
  at
  com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$$anonfun$2.apply(NerDLModel.scala:116)
  at
  com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$$anonfun$2.apply(NerDLModel.scala:116)
  at
  com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:31)
  at
  com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:30)
  at
  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) 
  at
  com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$class.com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead(ParamsAndFeaturesReadable.scala:30)
  at
  com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
  at
  com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
  at
  com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:19)
  at
  com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:8)
  at
  org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:652)
  at
  org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:274)
  at
  org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:272)
  at
  scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at
  scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at
  scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at
  scala.collection.TraversableLike$class.map(TraversableLike.scala:234) 
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)<br>
  at
  org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:272)
  at
  org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:348)
  at
  org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:342)
  at
  com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:135)
  at
  com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:129)
  at com.johnsnowlabs.nlp.pretrained.PretrainedPipelin<code>enter code
  here</code>e.(PretrainedPipeline.scala:14)</p>
</blockquote>
",1454,1,8,1,johnsnowlabs-spark-nlp,2019-08-22 18:39:13,2019-08-22 18:39:13,2022-05-23 19:20:50,below error occurs 
522,522,13223282,62279160,Predicting values in time series for future periods with windowed dataset,"<p>I have the data till 3650 time steps, but I want to make future predictions i.e. data after 3650 time steps. I am new to machine learning, and apparently can't figure it out. How can I do it?
For reference,
<a href=""https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%204%20Exercise%20Answer.ipynb"" rel=""nofollow noreferrer"">Colab Notebook</a></p>
",342,1,-2,5,tensorflow;machine-learning;time-series;data-science;prediction,2020-06-09 14:51:59,2020-06-09 14:51:59,2022-05-23 18:30:12,
523,523,19151089,72300717,UWP application use &quot;CustomDevice.FromIdAsync&quot; access KMDF driver is denied,"<blockquote>
<p>I found a similar <a href=""https://stackoverflow.com/questions/59047977/access-is-denied-when-trying-to-customdevice-fromidasync-using-a-software-kmdf-d"">question</a>, but it probably didn't help me much.</p>
</blockquote>
<p>I'm learning to access drive devices via UWP, in the first two days, the program was <strong>working fine</strong>, but today, UWP error close when open device, the error message is <strong>&quot;Access is denied. Exception from HRESULT: 0x80070005&quot;</strong></p>
<p>I don't know if something was modified during the update process that caused this problem. I only have a snapshot of the virtual machine that can open the drive device. On this snapshot, when I update the UWP application, I can also open the drive device, but after update the driver, the UWP application can no longer open the driver device.</p>
<p>Does this mean the problem is with the driver?</p>
<p>After that, I created a new KMDF project and a UWP project, only printed some information, and the same error occurred after installing to the virtual machine.</p>
<p>It is worth noting that when I saw the UWP access to the drive device in DbgView, the <code>EvtDeviceFileCreate</code> event callback was triggered, but the <code>EvtDeviceClose</code> event callback was triggered immediately, I think, there should be no problem with the device interface.</p>
<p>Also, in the beginning, I used the device manager to &quot;Add legacy hardware&quot; to load the driver, then I tried to install the driver using <code>devcon</code>, both can install the driver, but it seems to be slightly different, I am not sure if this also It will have an impact, although both installation methods currently do not solve the problem I am encountering.</p>
<p>Below are some settings for UWP app and drivers.</p>
<p>Register device interface and add custom capability:</p>
<p>In this part, I refer to Microsoft's official sample <a href=""https://github.com/microsoft/Windows-driver-samples/tree/master/usb/kmdf_fx2"" rel=""nofollow noreferrer"">kmdf_fx2</a>.</p>
<pre><code>    WDFSTRING symbolicLinkString = NULL;
    UNICODE_STRING symbolicLinkName = SymbolicName;
    DEVPROP_BOOLEAN isRestricted;
    status = WdfDeviceCreateDeviceInterface(
        hDevice,
        (LPGUID)&amp;GUID_INTERFACE_HSADRVSAMPLE1,
        NULL); // Reference String
    if (!NT_SUCCESS(status)) {
        KdPrint((&quot;WdfDeviceCreateDeviceInterface Fail\n&quot;));
        goto Error;
    }
    if (g_pIoSetDeviceInterfacePropertyData != NULL) {
        status = WdfStringCreate(NULL,
            WDF_NO_OBJECT_ATTRIBUTES,
            &amp;symbolicLinkString);
        if (!NT_SUCCESS(status)) {
            KdPrint((&quot;WdfStringCreate Fail\n&quot;));
            goto Error;
        }
        status = WdfDeviceRetrieveDeviceInterfaceString(
            hDevice,
            (LPGUID)&amp;GUID_INTERFACE_HSADRVSAMPLE1,
            NULL,
            symbolicLinkString);
        if (!NT_SUCCESS(status)) {
            KdPrint((&quot;WdfDeviceRetrieveDeviceInterfaceString Fail\n&quot;));
            goto Error;
        }
        WdfStringGetUnicodeString(symbolicLinkString, &amp;symbolicLinkName);
        isRestricted = DEVPROP_TRUE;
        status = g_pIoSetDeviceInterfacePropertyData(
            &amp;symbolicLinkName,
            &amp;DEVPKEY_DeviceInterface_Restricted,
            0,
            0,
            DEVPROP_TYPE_BOOLEAN,
            sizeof(isRestricted),
            &amp;isRestricted);
        if (!NT_SUCCESS(status)) {
            KdPrint((&quot;g_pIoSetDeviceInterfacePropertyData Fail\n&quot;));
            goto Error;
        }

#if defined(NTDDI_WIN10_RS2) &amp;&amp; (NTDDI_VERSION &gt;= NTDDI_WIN10_RS2)
        static const wchar_t customCapabilities[] = L&quot;ColinTest.HSADrvSample_2022051717171\0&quot;;
        status = g_pIoSetDeviceInterfacePropertyData(
            &amp;symbolicLinkName,
            &amp;DEVPKEY_DeviceInterface_UnrestrictedAppCapabilities,
            0,
            0,
            DEVPROP_TYPE_STRING_LIST,
            sizeof(customCapabilities),
            (PVOID)&amp;customCapabilities);
        if (!NT_SUCCESS(status)) {
            KdPrint((&quot;g_pIoSetDeviceInterfacePropertyData Fail\n&quot;));
            goto Error;
        }
#endif

        WdfObjectDelete(symbolicLinkString);
    }
Error:
    if (symbolicLinkString != NULL) {
        WdfObjectDelete(symbolicLinkString);
    }
</code></pre>
<p>Driver INF：</p>
<pre><code>;
; HSADrvSample1.inf
;

[Version]
Signature=&quot;$WINDOWS NT$&quot;
Class=Test ; TODO: edit Class
ClassGuid={160303BD-D84C-4819-B962-9B1BDBB52310} ; TODO: edit ClassGuid
Provider=%ManufacturerName%
CatalogFile=HSADrvSample1.cat
DriverVer = 05/19/2022,9.21.50.151
PnpLockDown=1

[DestinationDirs]
DefaultDestDir = 12
HSADrvSample1_Device_CoInstaller_CopyFiles = 11

; ================= Class section =====================

[ClassInstall32]
Addreg=SampleClassReg

[SampleClassReg]
HKR,,,0,%ClassName%
HKR,,Icon,,-5

[SourceDisksNames]
1 = %DiskName%,,,&quot;&quot;

[SourceDisksFiles]
HSADrvSample1.sys  = 1,,
WdfCoInstaller01011.dll=1 ; make sure the number matches with SourceDisksNames

;*****************************************
; Install Section
;*****************************************

[Manufacturer]
%ManufacturerName%=Standard,NTamd64

[Standard.NTamd64]
%HSADrvSample1.DeviceDesc%=HSADrvSample1_Device, Root\HSADrvSample1 ; TODO: edit hw-id

[HSADrvSample1_Device.NT]
CopyFiles=Drivers_Dir

[Drivers_Dir]
HSADrvSample1.sys

;-------------- Service installation
[HSADrvSample1_Device.NT.Services]
AddService = HSADrvSample1,%SPSVCINST_ASSOCSERVICE%, HSADrvSample1_Service_Inst

; -------------- HSADrvSample1 driver install sections
[HSADrvSample1_Service_Inst]
DisplayName    = %HSADrvSample1.SVCDESC%
ServiceType    = 1               ; SERVICE_KERNEL_DRIVER
StartType      = 3               ; SERVICE_DEMAND_START
ErrorControl   = 1               ; SERVICE_ERROR_NORMAL
ServiceBinary  = %12%\HSADrvSample1.sys

;
;--- HSADrvSample1_Device Coinstaller installation ------
;

[HSADrvSample1_Device.NT.CoInstallers]
AddReg=HSADrvSample1_Device_CoInstaller_AddReg
CopyFiles=HSADrvSample1_Device_CoInstaller_CopyFiles

[HSADrvSample1_Device_CoInstaller_AddReg]
HKR,,CoInstallers32,0x00010000, &quot;WdfCoInstaller01011.dll,WdfCoInstaller&quot;

[HSADrvSample1_Device_CoInstaller_CopyFiles]
WdfCoInstaller01011.dll

[HSADrvSample1_Device.NT.Wdf]
KmdfService =  HSADrvSample1, HSADrvSample1_wdfsect
[HSADrvSample1_wdfsect]
KmdfLibraryVersion = 1.11

[Strings]
SPSVCINST_ASSOCSERVICE= 0x00000002
ManufacturerName=&quot;Colin&quot; ;TODO: Replace with your manufacturer name
ClassName=&quot;Test&quot; ; TODO: edit ClassName
DiskName = &quot;HSADrvSample1 Installation Disk&quot;
HSADrvSample1.DeviceDesc = &quot;HSADrvSample1 Device&quot;
HSADrvSample1.SVCDESC = &quot;HSADrvSample1 Service&quot;
</code></pre>
<p>SCCD File：</p>
<p>The file is configured as per the official documentation<a href=""https://docs.microsoft.com/en-us/windows-hardware/drivers/devapps/hardware-support-app--hsa--steps-for-app-developers"" rel=""nofollow noreferrer"">Hardware Support App (HSA): Steps for App Developers</a>, the file is already set to <code>content</code>.</p>
<pre class=""lang-html prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;CustomCapabilityDescriptor xmlns=&quot;http://schemas.microsoft.com/appx/2018/sccd&quot; xmlns:s=&quot;http://schemas.microsoft.com/appx/2018/sccd&quot;&gt;
    &lt;CustomCapabilities&gt;
        &lt;CustomCapability Name=&quot;ColinTest.HSADrvSample_2022051717171&quot;&gt;&lt;/CustomCapability&gt;
    &lt;AuthorizedEntities&gt;
        &lt;AuthorizedEntity AppPackageFamilyName=&quot;Colin.HSA.APP_4a8dxf1xdwf12&quot; CertificateSignatureHash=&quot;2d6e4f1418bc13447bb8c10067d0af418cb8a2f9e14c014e03a1e0ec811735af&quot;&gt;&lt;/AuthorizedEntity&gt;
    &lt;/AuthorizedEntities&gt;
    &lt;Catalog&gt;FFFF&lt;/Catalog&gt;
&lt;/CustomCapabilityDescriptor&gt;
</code></pre>
<p>Part of Package.appxmanifest：</p>
<pre class=""lang-html prettyprint-override""><code>  &lt;Capabilities&gt;
    &lt;Capability Name=&quot;internetClient&quot; /&gt;
      &lt;uap4:CustomCapability Name=&quot;ColinTest.HSADrvSample_2022051717171&quot;/&gt;
  &lt;/Capabilities&gt;
</code></pre>
<p>Part of UWP Code：</p>
<pre><code>string selector = CustomDevice.GetDeviceSelector(DeviceInterfaceGuid);
            DeviceInformationCollection enumDevice = await DeviceInformation.FindAllAsync(selector);
            string DeviceID = enumDevice[0].Id;
            try
            {
                CustomDevice Device = await CustomDevice.FromIdAsync(DeviceID, DeviceAccessMode.ReadWrite, DeviceSharingMode.Shared);
                if (null != Device)
                {
                    Note.Text = &quot;Open successfully&quot;;
                }
            }
            catch (Exception ex)
            {
                Note.Text = &quot;Failed to open!&quot; + ex.Message;
            }
</code></pre>
<p>I would like to know at which step am I getting the error? How to solve?</p>
<hr />
<blockquote>
<p>At the suggestion of <strong>@Nico Zhu - MSFT</strong>, I checked the <code>CustomCapabilityName</code> again.</p>
</blockquote>
<p>There are three places in my code related to <code>CustomCapabilityName</code>, but they are all the <strong>same</strong>.
To test if the problem lies here, I <strong>reset</strong> the name.</p>
<ol>
<li>Driver Code</li>
</ol>
<pre><code>static const wchar_t customCapabilities[] = L&quot;Test.hsaTestCustomCapability_1653011401000\0&quot;;
</code></pre>
<ol start=""2"">
<li>CustomCapability.SCCD</li>
</ol>
<pre class=""lang-html prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;CustomCapabilityDescriptor xmlns=&quot;http://schemas.microsoft.com/appx/2018/sccd&quot; xmlns:s=&quot;http://schemas.microsoft.com/appx/2018/sccd&quot;&gt;
    &lt;CustomCapabilities&gt;
        &lt;CustomCapability Name=&quot;Test.hsaTestCustomCapability_1653011401000&quot;&gt;&lt;/CustomCapability&gt;
    &lt;/CustomCapabilities&gt;
    &lt;AuthorizedEntities AllowAny=&quot;true&quot;/&gt;
    &lt;Catalog&gt;0000&lt;/Catalog&gt;
&lt;/CustomCapabilityDescriptor&gt;
</code></pre>
<ol start=""3"">
<li>Package.appxmanifest</li>
</ol>
<pre class=""lang-html prettyprint-override""><code>  &lt;Capabilities&gt;
    &lt;Capability Name=&quot;internetClient&quot; /&gt;
      &lt;uap4:CustomCapability Name=&quot;Test.hsaTestCustomCapability_1653011401000&quot;/&gt;
  &lt;/Capabilities&gt;
</code></pre>
<p>After that, I <strong>uninstalled</strong> the previously installed apps and drivers, <strong>restarted</strong> and installed the latest version of the program, but I still <strong>can't access</strong> it.</p>
<p>My SDK and WDK version is <strong>10.0.19041.0</strong>, test machine OS version is <strong>Windows 10 Pro Build 19041.vb_release.191206-1406</strong></p>
",37,1,0,2,c#;uwp,2022-05-19 13:26:17,2022-05-19 13:26:17,2022-05-23 16:25:13,i found a similar   but it probably didn t help me much  i m learning to access drive devices via uwp  in the first two days  the program was working fine  but today  uwp error close when open device  the error message is  access is denied  exception from hresult  x  i don t know if something was modified during the update process that caused this problem  i only have a snapshot of the virtual machine that can open the drive device  on this snapshot  when i update the uwp application  i can also open the drive device  but after update the driver  the uwp application can no longer open the driver device  does this mean the problem is with the driver  after that  i created a new kmdf project and a uwp project  only printed some information  and the same error occurred after installing to the virtual machine  it is worth noting that when i saw the uwp access to the drive device in dbgview  the evtdevicefilecreate event callback was triggered  but the evtdeviceclose event callback was triggered immediately  i think  there should be no problem with the device interface  also  in the beginning  i used the device manager to  add legacy hardware  to load the driver  then i tried to install the driver using devcon  both can install the driver  but it seems to be slightly different  i am not sure if this also it will have an impact  although both installation methods currently do not solve the problem i am encountering  below are some settings for uwp app and drivers  register device interface and add custom capability  in this part  i refer to microsoft s official sample   driver inf  sccd file  the file is configured as per the official documentation  the file is already set to content  part of package appxmanifest  part of uwp code  i would like to know at which step am i getting the error  how to solve  at the suggestion of  nico zhu   msft  i checked the customcapabilityname again  after that  i uninstalled the previously installed apps and drivers  restarted and installed the latest version of the program  but i still can t access it  my sdk and wdk version is      test machine os version is windows  pro build  vb_release  
524,524,18798680,71959823,Scaling pixels between -1 and 1 using cv2.Normalize(),"<pre><code>def preprocess(self):

    # Import image
    pic1 = self.path
    raw_image = cv2.imread(pic1)
    #cv2.imshow('Raw image',raw_image)
    #cv2.waitKey(0)

    # Resize image
    dim = (320,180)
    resized = cv2.resize(raw_image, dim)
    #cv2.imshow('Resized Image',resized)
    #cv2.waitKey(0)

    # Scale image
    scaled = cv2.normalize(resized, None, alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX,dtype=cv2.CV_32F)
    #cv2.imshow('Scaled Image',scaled)
    #cv2.waitKey(0)

    return scaled
</code></pre>
<p>I'm trying to scale the pixel values of &quot;raw_image&quot; to within the range -1 to 1 as part of a pre-process for identifying an object using machine learning. Essentially, a camera takes a picture, resizes and scales the image to the same size as the images within a dataset used for training and validating. Then that image is inferred by the model generated using model.fit() to detect what the object in the image actually is.</p>
<p>The question here is: &quot; Is this scaling function correct for putting the pixel values in the range of -1 to 1?&quot; It appears SUPER dark when I use cv2.imshow and I'm afraid the model isn't recognizing it properly.</p>
",25,0,0,3,python;opencv;normalize,2022-04-22 00:44:41,2022-04-22 00:44:41,2022-05-23 15:59:40,i m trying to scale the pixel values of  raw_image  to within the range   to  as part of a pre process for identifying an object using machine learning  essentially  a camera takes a picture  resizes and scales the image to the same size as the images within a dataset used for training and validating  then that image is inferred by the model generated using model fit   to detect what the object in the image actually is  the question here is    is this scaling function correct for putting the pixel values in the range of   to    it appears super dark when i use cv imshow and i m afraid the model isn t recognizing it properly 
525,525,19178233,72344597,Django validation errors not showing up,"<p>So, I was learning Django from a tutorial and came across form validation. The tutor's version had errors pop-up on screen when validation failed but nothing shows up on my form.</p>
<p>Here is my forms.py.</p>
<pre><code>from django import forms
from django.core import validators


def check_for_z(value):
    if value[0].lower() != 'z':
        raise forms.ValidationError('Name should start with z')

class FormName(forms.Form):

    name = forms.CharField(validators = [check_for_z])
    email =forms.EmailField()
    text = forms.CharField(widget=forms.Textarea)
</code></pre>
<p>This is my views.py file.</p>
<pre><code>from django.shortcuts import render
from myformapp import forms

def form_name_view(request):
    form = forms.FormName()

    if request.method == 'POST':
        filled_form = forms.FormName(request.POST)

        if filled_form.is_valid():
            # print the form data to terminal
            print(&quot;Validation success&quot;)
            print('Name: ' + filled_form.cleaned_data['name'])
            print('Email: ' + filled_form.cleaned_data['email'])
            print('Text: ' + filled_form.cleaned_data['text'])

         
    return render(request, 'myformapp/formpage.html', {'form' : form})
</code></pre>
<p>And this is my template for the page.</p>
<pre><code>&lt;!DOCTYPE html&gt;
{% load static %}
&lt;html lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
    &lt;title&gt;Form Page&lt;/title&gt;
    &lt;!-- Latest compiled and minified CSS --&gt;
   &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css&quot; integrity=&quot;sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu&quot; crossorigin=&quot;anonymous&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;

    &lt;div class=&quot;container&quot;&gt;

      
      &lt;form method=&quot;post&quot;&gt;

          {{form.as_p}}
          {% csrf_token %}
          &lt;input type=&quot;submit&quot; class=&quot;btn btn-primary&quot; value=&quot;Submit&quot;&gt;


      &lt;/form&gt;

    &lt;/div&gt;

  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>So whenever i enter a name that's not beginning with z i am supposed to get an exception on the screen but nothing shows up. My code is pretty similar to what the tutor is showing on his machine. Can someone point me in the right direction as to what i am doing wrong.
Thanks</p>
",30,1,0,3,python;django;django-forms,2022-05-23 12:47:18,2022-05-23 12:47:18,2022-05-23 13:06:13,so  i was learning django from a tutorial and came across form validation  the tutor s version had errors pop up on screen when validation failed but nothing shows up on my form  here is my forms py  this is my views py file  and this is my template for the page 
526,526,19178150,72344357,x Fold01: preprocessor 1/1: Error in `prep()`: in R,"<p>I am new to creating machine learning models. I am trying to create a penalised logistic model based on <a href=""https://www.tidymodels.org/start/case-study/"" rel=""nofollow noreferrer"">https://www.tidymodels.org/start/case-study/</a> steps here.</p>
<pre><code>library(readr)       
library(broom.mixed) 
library(dotwhisker)  
library(skimr)           
library(rpart.plot)  
library(vip)    
library(glmnet)

#data cleaning
school &lt;-
read_csv(&quot;/Users/riddhimaagupta/Desktop/skul_data_word.csv&quot;)

schools &lt;- 
 select (school, -c(muh1, muh2, muh, chr1, chr2, chr3, chr, hindu, nu1, nu2, nu_klaten, nu_sby, nu, it1, it, id, npsn))

new_school &lt;- 
filter(schools, afiliasi != 99)


#data splitting
set.seed(123)
splits      &lt;- initial_split(new_school, strata = afiliasi)
school_train &lt;- training(splits)
school_test  &lt;- testing(splits)

set.seed(345)
folds &lt;- vfold_cv(school_train, v = 10)
folds

# penalised logistic regression
lr_mod &lt;- 
logistic_reg(penalty = tune(), mixture = 1) %&gt;% 
set_engine(&quot;glmnet&quot;)

lr_recipe &lt;- 
recipe(afiliasi ~ ., data = school_train) %&gt;%
step_novel(all_predictors()) %&gt;%
step_unknown(all_nominal_predictors()) %&gt;%
step_dummy(all_nominal_predictors()) %&gt;% 
step_zv(all_predictors()) %&gt;% 
step_normalize(all_predictors())


lr_workflow &lt;- 
workflow() %&gt;% 
add_model(lr_mod) %&gt;% 
add_recipe(lr_recipe)

lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_res &lt;- 
lr_workflow %&gt;% 
tune_grid(folds,
          grid = lr_reg_grid,
          control = control_grid(save_pred = TRUE),
          metrics = metric_set(roc_auc))
</code></pre>
<p>I keep running across the error:</p>
<hr />
<p>x Fold01: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold02: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold03: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold04: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold05: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold06: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold07: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold08: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold09: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
x Fold10: preprocessor 1/1: Error in <code>prep()</code>:
! Columns must be c...
Warning message:
All models failed. See the <code>.notes</code> column.</p>
<hr />
<p>I don't know what to do. Any help will be greatly appreciated, thank you!</p>
",41,0,0,2,r;machine-learning,2022-05-23 12:27:55,2022-05-23 12:27:55,2022-05-23 12:33:11,i am new to creating machine learning models  i am trying to create a penalised logistic model based on  steps here  i keep running across the error  i don t know what to do  any help will be greatly appreciated  thank you 
527,527,17934590,72278349,error when running streamline as frontend and FastAPI as backend,"<p>I have developed a machine learning app with fastapi as the backend, and streamlit as the frontend, it works on my local machine when I run it, i am abe to generate th predictions using the streamlit running on fastapi backend. But using docker , individually they work, I can generate predictions using fast API, and I can view the UI using streamlit, but if I want to generate the predictions on the streamlit UI, it generates this error:</p>
<p><code>ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with URL: /predict (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0xffffa808a0d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</code></p>
<p>Where could the error be coming from?</p>
",63,1,0,4,python;docker;fastapi;streamlit,2022-05-17 22:58:55,2022-05-17 22:58:55,2022-05-23 09:18:23,i have developed a machine learning app with fastapi as the backend  and streamlit as the frontend  it works on my local machine when i run it  i am abe to generate th predictions using the streamlit running on fastapi backend  but using docker   individually they work  i can generate predictions using fast api  and i can view the ui using streamlit  but if i want to generate the predictions on the streamlit ui  it generates this error  connectionerror  httpconnectionpool host  localhost   port    max retries exceeded with url   predict  caused by newconnectionerror   lt urllib connection httpconnection object at xffffaad gt   failed to establish a new connection   errno   connection refused    where could the error be coming from 
528,528,12227391,71553543,How to perform template matching between matrix and image?,"<p>I've been working on a simple machine learning on Matlab<br />
My dataset looks like:</p>
<p><a href=""https://i.stack.imgur.com/IzdPp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzdPp.png"" alt=""enter image description here"" /></a></p>
<p>There are 10 pictures in a folder, 5 for training, and 5 for testing.<br />
Totally 40 folders, or 400 pictures in my dataset.</p>
<p>First, I read 200 images(32*32pixel) into a matrix called <strong>FFace</strong> which size is 200x1024.<br />
Then, perform PCA on <strong>FFace</strong> and get <strong>pcaTotalFace</strong>(200x50).<br />
Next, perform LDA and get <strong>prototypeFace</strong>(200x50).<br />
I've successfully reduce <strong>FFace</strong>(200x1024) to <strong>prototypeFace</strong>(200x50).</p>
<p>My question is : How to do template matching between remaining 200 images and <strong>prototypeFace</strong>?</p>
<p>Below are my PCA and LDA code for reference.
PCA:</p>
<pre><code>function [FFace, TotalMeanFace, pcaTotalFace, projectPCA, eigvector, 
prototypeFace]=PCALDA_Train

people=40; 
withinsamp=5; 
principlenum=50; %reduction to dimension of 50
FFace=[]; %store every traning picture(200*1024)

for k=1:1:people
    for m=1:2:10
        matchstring=['dataset' '\' num2str(k) '\' num2str(m) '.bmp'];
        matchX=imread(matchstring);
        matchX=double(matchX);
        if(k==1 &amp;&amp; m==1)
            [row, col]=size(matchX);
        end
        matchtmpF=[];
        % arrange the image into a vector
        for n=1:row
            matchtmpF=[matchtmpF, matchX(n,:)]; %1*32 row concat 32 times
        end
        FFace=[FFace;matchtmpF]; % col concat 
    end
end
    TotalMeanFace=mean(FFace);
    FFaceNor=FFace-TotalMeanFace;
    covPCA=FFaceNor'*FFaceNor; 
    [Vec, Val]=eig(covPCA);
    eigval=diag(Val); 
    [junk, index]=sort(eigval, 'descend');
    PCA=Vec(:,index); 
    eigval=eigval(index);
    projectPCA=PCA(:,1:principlenum); %extract the principle component
    pcaTotalFace=[];
    for i=1:1:withinsamp*people
        tmpFace=FFaceNor(i,:);
        tmpFace=tmpFace*projectPCA;  
        pcaTotalFace=[pcaTotalFace; tmpFace];  
    end
</code></pre>
<p>LDA:</p>
<pre><code>classMean=[];
SW=[];
for i=1:withinsamp:withinsamp*people
    withinFace=pcaTotalFace(i:i+withinsamp-1,:);
    if(i==1)
        meanwithinFace=mean(withinFace);
        withinFace=withinFace-meanwithinFace;
        SW=withinFace'*withinFace %cov(withinFace)
        classMean=mean(withinFace);
    end
    if(i&gt;1)
        meanwithinFace=mean(withinFace);
        withinFace=withinFace-meanwithinFace;
        SW=SW+withinFace'*withinFace;
        classMean=[classMean;mean(withinFace)];
    end

end
pcaTotalMean=mean(pcaTotalFace);
classMean=classMean-pcaTotalMean;
SB=classMean'*classMean;
[eigvector, eigvalue]=eig(inv(SW)*SB);
eigvalue=diag(eigvalue);
[junk, index]=sort(eigvalue, 'descend');
eigvalue=eigvalue(index);
eigvector=eigvector(:,index);
prototypeFace=pcaTotalFace*eigvector;

end
</code></pre>
",64,1,0,5,matlab;machine-learning;pca;lda;template-matching,2022-03-21 11:56:16,2022-03-21 11:56:16,2022-05-23 07:31:55, my question is   how to do template matching between remaining  images and prototypeface  lda 
529,529,13214782,72341470,Should we apply normalization on whole data set or only X,"<p>I am doing a project based on Machine learning (Python) and trying all models on my data.
Really confused in</p>
<p>For Classification and For Regression</p>
<ol>
<li>If I have to apply normalization, Z Score or Standard deviation on whole data set and then set the values of Features(X) and output(y).</li>
</ol>
<pre><code>    def normalize(df):
        from sklearn.preprocessing import MaxAbsScaler
        scaler = MaxAbsScaler()
        scaler.fit(df)
        scaled = scaler.transform(df)
        scaled_df = pd.DataFrame(scaled, columns=df.columns)
        return scaled_df
    
data=normalize(data)
X=data.drop['col']
y=data['col']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

</code></pre>
<ol start=""2"">
<li>Or only have to apply on features(X)</li>
</ol>
<pre><code>X=data.drop['col']
y=data['col']

def normalize(df):
    from sklearn.preprocessing import MaxAbsScaler
    scaler = MaxAbsScaler()
    scaler.fit(df)
    scaled = scaler.transform(df)
    scaled_df = pd.DataFrame(scaled, columns=df.columns)
    return scaled_df

X=normalize(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

</code></pre>
",59,1,-3,5,python;machine-learning;jupyter-notebook;data-science;normalization,2022-05-23 03:10:47,2022-05-23 03:10:47,2022-05-23 06:06:50,for classification and for regression
530,530,17056524,72341967,Trackback Error with Scipy Optimize Jac: IndexError: index 1 is out of bounds for axis 0 with size 1,"<p>I'm taking the Coursera Machine Learning course by Andrew Ng. I'm working on this exercise to optimize the cost function for the logistic regression. I was able to do it correctly on Octave but I got errors when implementing in Python.</p>
<p>Here's my code:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># Define Sigmoid Function
def sig(z):
return 1/(1+np.exp(-z))

# Initialize Theta
theta = np.zeros((X.shape[1], 1))

# Find Gradient
def find_cost_grad(theta, X, y):
    h = sig(X.dot(theta))
    m = X.shape[0]
    theta_s = theta[1:]
    J = (-1/m) * (y.T.dot(np.log(h)) + (1-y).T.dot(np.log(1-h))) + lamb/(2*m) * np.sum(theta_s**2)
    
    grad1 = (1/m)*((h-y).T.dot(X)).flatten()
    grad2 = (theta*lamb/m).flatten()
    grad2[0] = 0
    
    grad = grad1+grad2

    return J, grad

find_cost_grad(theta, X, y)

## Use BFGS and L-BFGS optimization
from scipy.optimize import minimize


bfgs = minimize(find_cost, theta, (X, y), method='L-BFGS-B', jac=True)
final_theta2 = bfgs['x']
bfgs</code></pre>
</div>
</div>
</p>
<p>I got the following error:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>    621                                   **options)
    622     elif meth == 'l-bfgs-b':
--&gt; 623         return _minimize_lbfgsb(fun, x0, args, jac, bounds,
    624                                 callback=callback, **options)
    625     elif meth == 'tnc':

~\anaconda3\lib\site-packages\scipy\optimize\lbfgsb.py in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)
    304             iprint = disp
    305 
--&gt; 306     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,
    307                                   bounds=new_bounds,
    308                                   finite_diff_rel_step=finite_diff_rel_step)

~\anaconda3\lib\site-packages\scipy\optimize\optimize.py in _prepare_scalar_function(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)
    259     # ScalarFunction caches. Reuse of fun(x) during grad
    260     # calculation reduces overall function evaluations.
--&gt; 261     sf = ScalarFunction(fun, x0, args, grad, hess,
    262                         finite_diff_rel_step, bounds, epsilon=epsilon)
    263 

~\anaconda3\lib\site-packages\scipy\optimize\_differentiable_functions.py in __init__(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)
    138 
    139         self._update_fun_impl = update_fun
--&gt; 140         self._update_fun()
    141 
    142         # Gradient evaluation

~\anaconda3\lib\site-packages\scipy\optimize\_differentiable_functions.py in _update_fun(self)
    231     def _update_fun(self):
    232         if not self.f_updated:
--&gt; 233             self._update_fun_impl()
    234             self.f_updated = True
    235 

~\anaconda3\lib\site-packages\scipy\optimize\_differentiable_functions.py in update_fun()
    135 
    136         def update_fun():
--&gt; 137             self.f = fun_wrapped(self.x)
    138 
    139         self._update_fun_impl = update_fun

~\anaconda3\lib\site-packages\scipy\optimize\_differentiable_functions.py in fun_wrapped(x)
    132             # Overwriting results in undefined behaviour because
    133             # fun(self.x) will change self.x, with the two no longer linked.
--&gt; 134             return fun(np.copy(x), *args)
    135 
    136         def update_fun():

~\anaconda3\lib\site-packages\scipy\optimize\optimize.py in __call__(self, x, *args)
     72     def __call__(self, x, *args):
     73         """""" returns the the function value """"""
---&gt; 74         self._compute_if_needed(x, *args)
     75         return self._value
     76 

~\anaconda3\lib\site-packages\scipy\optimize\optimize.py in _compute_if_needed(self, x, *args)
     67             self.x = np.asarray(x).copy()
     68             fg = self.fun(x, *args)
---&gt; 69             self.jac = fg[1]
     70             self._value = fg[0]
     71 

IndexError: index 1 is out of bounds for axis 0 with size 1</code></pre>
</div>
</div>
</p>
<p>I checked that the gradient returned is a row vector (1x28). The find_cost_grad function runs fine when I ran it itself.</p>
<p>However, it caused errors when I ran it within the minimize function. Been looking for a solution but can't find it anywhere.</p>
<p>Please help.</p>
<p>Thanks!</p>
",18,0,0,4,python;arrays;scipy;data-science,2022-05-23 05:04:23,2022-05-23 05:04:23,2022-05-23 05:04:23,i m taking the coursera machine learning course by andrew ng  i m working on this exercise to optimize the cost function for the logistic regression  i was able to do it correctly on octave but i got errors when implementing in python  here s my code  i got the following error  i checked that the gradient returned is a row vector  x   the find_cost_grad function runs fine when i ran it itself  however  it caused errors when i ran it within the minimize function  been looking for a solution but can t find it anywhere  please help  thanks 
531,531,19175992,72341377,Attempting to generate a note sequence from MIDI files but am continuing to get [Errno 21] Is a directory: &#39;/Output&#39;,"<p>this is my first post here. I'm still quite new to this so bear with me.</p>
<p>For a machine learning project, I'm attempting to generate a note sequence from midi files so that I can begin generating music with an LSTM. I've cleared a lot of errors from the original script, but I'm now having issues getting the script to find my input and output directories.</p>
<p>I'm working on this project in Colab. Everything seems to be working as intended leading up to the final cell where I get the directory error. I've attached the code for my final cell below.</p>
<pre><code>output_file = '/Output'
input_folder = '/Input'
recursive = False

with note_seq.midi_file_to_note_sequence(output_file) as writer:
convert_files(input_folder, '', writer, recursive)
</code></pre>
<p>The error given is:</p>
<pre><code>IsADirectoryError                         Traceback (most recent call last)
&lt;ipython-input-29-cdd01c8d5498&gt; in &lt;module&gt;()
----&gt; 6 with note_seq.midi_file_to_note_sequence(output_file) as writer:
      7       convert_files(input_folder, '', writer, recursive)

/usr/local/lib/python3.7/dist-packages/note_seq/midi_io.py in midi_file_to_note_sequence(midi_file)
    183     MIDIConversionError: Invalid midi_file.
    184   &quot;&quot;&quot;
--&gt; 185   with open(midi_file, 'rb') as f:
    186     midi_as_string = f.read()
    187     return midi_to_note_sequence(midi_as_string)`
</code></pre>
<p>IsADirectoryError: [Errno 21] Is a directory: '/Output'`</p>
<p>As I said, I'm still very new to coding, so I expect the answer is painfully simple as with most tech issues. I've tried this same cell with /content/Output as well as without. The error persists in both cases. I've also tried directory commands to set them up, but I suspect there are aspects of those commands I still don't understand.</p>
<p>Thank you!</p>
",26,0,0,3,google-colaboratory;midi;magenta,2022-05-23 02:52:46,2022-05-23 02:52:46,2022-05-23 02:52:46,this is my first post here  i m still quite new to this so bear with me  for a machine learning project  i m attempting to generate a note sequence from midi files so that i can begin generating music with an lstm  i ve cleared a lot of errors from the original script  but i m now having issues getting the script to find my input and output directories  i m working on this project in colab  everything seems to be working as intended leading up to the final cell where i get the directory error  i ve attached the code for my final cell below  the error given is  isadirectoryerror   errno   is a directory    output   as i said  i m still very new to coding  so i expect the answer is painfully simple as with most tech issues  i ve tried this same cell with  content output as well as without  the error persists in both cases  i ve also tried directory commands to set them up  but i suspect there are aspects of those commands i still don t understand  thank you 
532,532,6565511,72341121,"How do I open a `.pb` file? Ideally, how to import it into Meta Quest 2?","<p>I'm using Matterport's free scanning software for Android and it spits out 2 files called <code>2b7cb66c-4c9e-408d-88c5-90b8c0322c13_sweep_features.pb</code> and <code>2b7cb66c-4c9e-408d-88c5-90b8c0322c13_sweep_cloud.pb</code> (the important thing is that they're of the forms <code>[nums]_sweep_features.pb</code> and <code>[nums]_sweep_cloud.pb</code>).</p>
<p>:^)</p>
<h6>How can I open <code>.pb</code> files?</h6>
<p>.</p>
<p>=======================================================</p>
<p>=======================================================</p>
<h5>My ultimate goal: the room in VR</h5>
<p>I need to know how to open the file so I can walk around the rooms I scan in my Meta Quest 2 VR Headset.  Ideally, I'd get it working in Unity, but UnrealEngine would be fine too.  The most important thing is that I get the numerical and/or color information about the scanned room.  Besides <code>sweep_cloud.pb</code>, there is also a file called <code>sweep_features.pb</code>.</p>
<ol>
<li><p>Maybe <code>.pb</code> is a Tensorflow format?  I say that because most of the <code>.pb</code> files I have found through Google for are <a href=""https://developers.google.com/protocol-buffers"" rel=""nofollow noreferrer"">Protocol Buffers</a>, many of which store Neural Network (Tensorflow) files, and &quot;<code>feature</code>&quot; is jargon in Neural Networks and Machine Learning more generally!</p>
</li>
<li><p>Another guess would be something like a <code>.obj</code> file, a numpy (<code>.npy</code>) file or other point cloud format that probably has some color information in it!</p>
</li>
</ol>
<p>Thanks!</p>
",26,0,1,5,unity3d;point-cloud-library;point-clouds;oculus;matterport,2022-05-23 02:05:02,2022-05-23 02:05:02,2022-05-23 02:05:02,i m using matterport s free scanning software for android and it spits out  files called bcbc ce d c bcc_sweep_features pb and bcbc ce d c bcc_sweep_cloud pb  the important thing is that they re of the forms  nums _sweep_features pb and  nums _sweep_cloud pb                                                                                                                         i need to know how to open the file so i can walk around the rooms i scan in my meta quest  vr headset   ideally  i d get it working in unity  but unrealengine would be fine too   the most important thing is that i get the numerical and or color information about the scanned room   besides sweep_cloud pb  there is also a file called sweep_features pb  maybe  pb is a tensorflow format   i say that because most of the  pb files i have found through google for are   many of which store neural network  tensorflow  files  and  feature  is jargon in neural networks and machine learning more generally  another guess would be something like a  obj file  a numpy   npy  file or other point cloud format that probably has some color information in it  thanks 
533,533,19071056,72339175,MinMax function crashes on depth = 3,"<p>I'm writing a chess AI by using a convolutional neural network to evaluate a specific board state, and then I'm using that evaluation to apply a min_max algorithm to get the AI's move. When I go passed a depth of 2 on my algorithm I get an error about comparing tuples to scalars.</p>
<pre><code>def NN_evaluate(board):
    board3d = split_dims(board)
    board3d = np.expand_dims(board3d, 0)
    return model.predict(board3d)[0][0]


def minimax(board, depth, alpha, beta, maximizing_player):
    if depth == 0 or board.is_game_over(): 
        return NN_evaluate(board)
    moves = board.legal_moves
    
    if maximizing_player:
        max_eval = -np.Inf
        for move in moves:
            board.push(move)
            current_eval = minimax(board, depth-1, alpha, beta, False)
            board.pop()
            max_eval = max(max_eval, current_eval)
            best_move = move
            alpha = max(alpha, current_eval)
            if beta &lt;= alpha:
                break
            return max_eval
    else: 
        min_eval = np.Inf
        for move in moves:
            board.push(move)
            current_eval = minimax(board, depth-1, alpha, beta, True)
            board.pop()
            min_eval = min(min_eval, current_eval)
            best_move = move
            beta = min(beta, current_eval) 
            if beta &lt;= alpha:
                break 
        return min_eval
    
def get_ai_move(board, depth, maximizing_player):
    max_move = None
    max_eval = -np.inf  
    for move in board.legal_moves:
        board.push(move)
        current_eval = minimax(board, depth-1, -np.inf, np.inf, False)
        board.pop()
        if current_eval &gt; max_eval:
            max_eval = current_eval
            max_move = move
    return max_move


board = chess.Board()

with chess.engine.SimpleEngine.popen_uci('C:\\Users\\coope\\Downloads\\Python\\Machine Learning\\Chess AI\\stockfish_15_win_x64_avx2\\stockfish_15_x64_avx2.exe') as engine:
    while True:
        move = get_ai_move(board, 3, True)
        board.push(move)
        print(f'\n{board}')
        if board.is_game_over():
            break

        move = engine.analyse(board, chess.engine.Limit(time=1), info=chess.engine.INFO_PV)['pv'][0]
        board.push(move)
        print(f'\n{board}')
        if board.is_game_over():
            break
</code></pre>
<p>The error goes as</p>
<pre><code>TypeError                                 Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_1108/561116885.py in &lt;module&gt;
      3 with chess.engine.SimpleEngine.popen_uci('C:\\Users\\coope\\Downloads\\Python\\Machine Learning\\Chess AI\\stockfish_15_win_x64_avx2\\stockfish_15_x64_avx2.exe') as engine:
      4     while True:
----&gt; 5         move = get_ai_move(board, 3, True)
      6         board.push(move)
      7         print(f'\n{board}')

~\AppData\Local\Temp/ipykernel_1108/3382320008.py in get_ai_move(board, depth, maximizing_player)
     40     for move in board.legal_moves:
     41         board.push(move)
---&gt; 42         current_eval = minimax(board, depth-1, -np.inf, np.inf, False)
     43         board.pop()
     44         if current_eval &gt; max_eval:

~\AppData\Local\Temp/ipykernel_1108/3382320008.py in minimax(board, depth, alpha, beta, maximizing_player)
     28             current_eval = minimax(board, depth-1, alpha, beta, True)
     29             board.pop()
---&gt; 30             min_eval = min(min_eval, current_eval)
     31             best_move = move
     32             beta = min(beta, current_eval)

TypeError: '&gt;' not supported between instances of 'float' and 'NoneType'
</code></pre>
<p>This seems to be an issue with my <code>min(min_eval, current_eval)</code> but I'm unsure how to fix it if<code>get_ai_move(board, 2, True)</code> doesn't crash.</p>
",21,1,0,3,machine-learning;tuples;minmax,2022-05-22 21:28:54,2022-05-22 21:28:54,2022-05-22 22:02:27,i m writing a chess ai by using a convolutional neural network to evaluate a specific board state  and then i m using that evaluation to apply a min_max algorithm to get the ai s move  when i go passed a depth of  on my algorithm i get an error about comparing tuples to scalars  the error goes as this seems to be an issue with my min min_eval  current_eval  but i m unsure how to fix it ifget_ai_move board    true  doesn t crash 
534,534,16378913,72338061,ModuleNotFoundError: No module named &#39;cleverhans.future&#39;,"<p>I'm trying to install cleverhans package on Google Colab and currently getting an error for ModuleNotFoundError: No module named 'cleverhans.future'</p>
<p>Code Attempted:</p>
<pre><code>!pip install -qq -e git+http://github.com/tensorflow/cleverhans.git#egg=cleverhans
import sys
sys.path.append('/content/src/cleverhans')
import cleverhans
from cleverhans.future.tf2.attacks import fast_gradient_method, \
    basic_iterative_method, momentum_iterative_method
</code></pre>
<p>I followed this initial snippet from Bob Smith, but still getting an error: <a href=""https://stackoverflow.com/questions/47525550/installing-cleverhans-on-colaboratory"">Installing cleverhans on Colaboratory</a></p>
<p>More info on cleverhans: <a href=""https://pypi.org/project/cleverhans/#history"" rel=""nofollow noreferrer"">https://pypi.org/project/cleverhans/#history</a></p>
<p>Code tutorial that I'm trying to run: <a href=""https://towardsdatascience.com/adversarial-machine-learning-mitigation-adversarial-learning-9ae04133c137"" rel=""nofollow noreferrer"">https://towardsdatascience.com/adversarial-machine-learning-mitigation-adversarial-learning-9ae04133c137</a></p>
",56,1,0,3,python;cleverhans;adversarial-attack,2022-05-22 19:07:01,2022-05-22 19:07:01,2022-05-22 19:26:38,i m trying to install cleverhans package on google colab and currently getting an error for modulenotfounderror  no module named  cleverhans future  code attempted  i followed this initial snippet from bob smith  but still getting an error   more info on cleverhans   code tutorial that i m trying to run  
535,535,15673855,72332126,Variable input and output size for Keras,"<p>Before I start, I am quite new to Keras and machine learning. I know the theory quite well but the syntax less so.</p>
<p>I am trying to create a reinforcement learning neural network using Keras. The problem to be solved is essentially the travelling salesman problem. The problem is, is that the network is fed in its location and the environment, which is a randomly created network of points such as [[0,5],[30,17],[19,83]..., and as the agent travels through this network, it changes as a point cannot be visited again. So if the agent goes from [0,0] to [0,5] then [30,17], the input would look like [0,5],[30,17],[19,83] to [30,17],[19,83] to [19,83]. There is a similar issue with the output, which is just the index of the possible locations to move to. This means that there could be any number of outputs.</p>
<p>The size of the input is initially 100, and the output could also be anywhere between 0 and 100. Methods like padding the inputs with a number would not work as the network would be fed a location impossible to get to, and there is a similar problem with padding the output with a number - the network can just stay in the same position whilst 'moving' ([0,0] to [0,0] etc). The agent also has limited time, so even with filling with random numbers it could just travel to locations which don't actually exist which doesn't solve the problem at hand.</p>
<p>How would I dynamically change the input and output sizes? Is it even possible, and if not, how should it be done?</p>
<p>edit: code because someone wanted it. Quite unintelligible but in essence a class containing the actions able to be done, the input in the form of self.state, and the enviroment in self.point_space. Reward is calculated as the distance travelled at each step and when complete, the distance compared to a random loop. The more important thing is if i can change the input and output sizes.</p>
<pre><code>class GraphEnv(Env):
  def __init__(self):
      self.point_space = createpoints()
      self.action_space = Discrete(len(self.point_space))
      self.observation_space = self.point_space.copy()
      self.state = [[0,0]]
     
      for i in self.observation_space:
        self.state.append(i)
      self.length = len(self.point_space)
      self.totallen = 0
      self.unchangedpoint_space = self.point_space.copy()
  def step(self, action):
    oldstate = self.state[0]
    self.state = []
    self.state.append(list(self.point_space[action-1]))
    try:
      del self.point_space[action-1]
    except:
      pass
    self.observation_space = self.point_space.copy()
    for i in self.observation_space:
        self.state.append(i)
    self.action_space = Discrete(len(self.point_space))
    #print(&quot;self.state = &quot;, self.state)

    reward = int(-math.sqrt((oldstate[0] - self.state[0][0])**2 + (oldstate[1] - self.state[0][1])**2))
    self.totallen += reward

    self.length -= 1
    if self.length &lt;= 0:
      #print(&quot;unchanged =&quot;, self.unchangedpoint_space)
      randomscore = scoreforrandom(self.unchangedpoint_space)
      reward = self.totallen - randomscore
      #print(&quot;totallen =&quot;,self.totallen)
      #print(&quot;randomscore =&quot;, randomscore)
      #print(&quot;reward&quot;, reward)
      done = True
    else:
      done = False


    info = {}

    return(self.state,reward,done,info)
  def render(self):
    
    pass
  def reset(self):
    self.state = [[0,0]]
    self.length = 100
    self.point_space = createpoints()
    self.observation_space = self.point_space.copy()
    self.state.append(self.observation_space)
    self.unchangedpoint_space = self.point_space.copy()
    #print(&quot;unchanged on init&quot;, self.unchangedpoint_space)
    self.action_space = Discrete(len(self.point_space))
    self.totallen = 0
    pass
</code></pre>
<p>The video i used as help: <a href=""https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte</a></p>
",38,1,0,5,machine-learning;keras;deep-learning;neural-network;reinforcement-learning,2022-05-21 23:41:40,2022-05-21 23:41:40,2022-05-22 16:14:03,before i start  i am quite new to keras and machine learning  i know the theory quite well but the syntax less so  i am trying to create a reinforcement learning neural network using keras  the problem to be solved is essentially the travelling salesman problem  the problem is  is that the network is fed in its location and the environment  which is a randomly created network of points such as                  and as the agent travels through this network  it changes as a point cannot be visited again  so if the agent goes from     to     then      the input would look like             to         to      there is a similar issue with the output  which is just the index of the possible locations to move to  this means that there could be any number of outputs  the size of the input is initially   and the output could also be anywhere between  and   methods like padding the inputs with a number would not work as the network would be fed a location impossible to get to  and there is a similar problem with padding the output with a number   the network can just stay in the same position whilst  moving       to     etc   the agent also has limited time  so even with filling with random numbers it could just travel to locations which don t actually exist which doesn t solve the problem at hand  how would i dynamically change the input and output sizes  is it even possible  and if not  how should it be done  edit  code because someone wanted it  quite unintelligible but in essence a class containing the actions able to be done  the input in the form of self state  and the enviroment in self point_space  reward is calculated as the distance travelled at each step and when complete  the distance compared to a random loop  the more important thing is if i can change the input and output sizes  the video i used as help  
536,536,18358769,72336590,one hot encoding classification,"<p>I am having a CSV file like this</p>
<pre><code>F1  |  F2  |  F3  |  F4  |  Label  
</code></pre>
<p>I used the <code>get_dummies</code> to change the label to a one-hot encoding representation, the data contains 3 different labels, so the file now looks like</p>
<pre><code>F1  |  F2  |  F3  |  F4  |  Label1  |  Label2  |  Label3
</code></pre>
<p>let's say I want to use this data to train a machine learning model. I have to determine the features and label columns
can I set it to:</p>
<pre class=""lang-py prettyprint-override""><code>Features, x = [0:3]
Labels, y = [4:6]
</code></pre>
<p>Is it right? I am thinking, by doing this way, maybe this could be understood as a multi-label problem since this is not! originally it was a multi-class classification.</p>
<p>Any help will be so much appreciated.</p>
",28,1,0,4,python;pandas;numpy;one-hot-encoding,2022-05-22 15:40:41,2022-05-22 15:40:41,2022-05-22 15:55:06,i am having a csv file like this i used the get_dummies to change the label to a one hot encoding representation  the data contains  different labels  so the file now looks like is it right  i am thinking  by doing this way  maybe this could be understood as a multi label problem since this is not  originally it was a multi class classification  any help will be so much appreciated 
537,537,19172452,72335727,model.predict gives an error could not convert string to float,"<p>I'm using Streamlit to build a machine learning model to predict the price of a car.
After splitting the data into train and test and choosing the best model,
I need to predict the price of a car based on the user input</p>
<pre><code>ax1,ax2 = st.columns(2)
with ax1:
    year = st.slider('Year The Car Manufatured', 1996,2020,1996)
    model = st.selectbox('model',('Fiesta'  ,'Focus','Kuga','EcoSport','C-MAX','Ka+','Mondeo','B-MAX','S-MAX','Grand C-MAX','Galaxy','Edge','KA','Puma','Tourneo Custom','Grand Tourneo Connect','Mustang','Tourneo Connect','Fusion','Streetka','Ranger','Transit Tourneo','Escort','Focus','Transit Tourneo'))
    transmission = st.selectbox('transmission', ('Manual','Automatic','Semi-Auto'))
    
    mileage = st.slider('How many mileage?', 0, 18000) 
with ax2:
    fuelType= st.selectbox('fuelType', ('Petrol','Diesel','Hybrid','Electric','Other'))
    tax= st.slider('Tax Amount', 0, 600)
    mpg = st.slider('MPG', 0, 250)
    engineSize =  st.slider('engineSize', 0, 5)
    
data = {'year':[year],'model':[model],'transmission':[transmission], 
            'fuelType':[fuelType],'mileage':[mileage],'mpg':[mpg],'engineSize':[engineSize], 
            'tax':[tax],}
features = pd.DataFrame(data)
cars = pd.read_csv(&quot;ford.csv&quot;)
cars.columns = cars.columns.str.strip()

cars.fillna(0, inplace=True)
cars = cars.drop(columns=['price'])

df = pd.concat([features,cars],axis=0)

df.columns = df.columns.str.strip()
df = pd.get_dummies(df, columns =  ['model','transmission', 'fuelType'],drop_first=True)

df = df[:1]
df.fillna(0, inplace=True)

def RFR():
    RFR= RandomForestRegressor(n_estimators=10, min_samples_leaf=0.05)
    RFR= RFR.fit(x, y)

prediction = RFR().predict(df)
</code></pre>
<p>i'm getting the below error:</p>
<p>ValueError: could not convert string to float: ' Fiesta'</p>
<p>Any Idea what might be the problem ?</p>
",65,1,0,4,python;machine-learning;dummy-variable;streamlit,2022-05-22 13:19:04,2022-05-22 13:19:04,2022-05-22 13:24:11,i m getting the below error  valueerror  could not convert string to float    fiesta  any idea what might be the problem  
538,538,19172394,72335741,Error in adorn_ns in while doing target segmentation of clusters in R,"<p>Am just starting out in R and machine learning.
Am working on a hierarchical clustering problem where after getting clusters of attitudinal variables (e.g preference for a certain choice), I want to do target segmentation by combining the cluster information with demographic data ,I did not include in the clustering such as marital status to check how cluster groups behave in married vs non married people.  my code is as seen below.</p>
<pre><code> data %&gt;%
  tabyl(clusters, marital) %&gt;% 
  adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% 
  adorn_percentages(&quot;row&quot;) %&gt;%
  adorn_pct_formatting(digits = 2) %&gt;%
  adorn_ns()
  Error in adorn_ns(.) : could not find function &quot;adorn_ns&quot;
</code></pre>
<p>The problem is the code throws this error,  Error in adorn_ns(.) : could not find function &quot;adorn_ns&quot;</p>
<p>I have tried all that I know , but I have not been able to solve this .</p>
<p>Thanks in advance for your help.</p>
",16,0,0,3,r;machine-learning;hierarchical-clustering,2022-05-22 13:21:51,2022-05-22 13:21:51,2022-05-22 13:21:51,the problem is the code throws this error   error in adorn_ns      could not find function  adorn_ns  i have tried all that i know   but i have not been able to solve this   thanks in advance for your help 
539,539,7404838,72320472,How do WebHooks maps to a domain service in DDD?,"<p>I have a question about web-hooks and how they map to domain driven events.</p>
<p>I am modelling a system using domain-driven design. Without going into the details, there is a domain service (a machine-learning prediction) which takes some time to run. The user would like therefore to send &quot;batches&quot; in an asynchronous manner. At a high level, I have a REST API (a controller), which sends commands to my application (use cases), which manipulates my domain model.</p>
<p>On the modelling side, I think of the completion of a prediction and of a batch as <em>domain events</em>. On the API side, I am considering <a href=""https://resthooks.org/docs/"" rel=""nofollow noreferrer"">a &quot;subscriber&quot; pattern</a>, where the API user could register an endpoint and choose which events she would like to receive. For example, she could subscribe only to the &quot;Batch Done&quot; event, or to both &quot;prediction done&quot; and &quot;batch done&quot;.</p>
<p>My question is whether the concept of subscription/subscriber should be part of my domain model. In other words, where should I store the &quot;endpoint&quot; associated with a subscriber? In the domain model or in API-level storage?</p>
<p>On the one hand, I feel it does not belong to my domain, because it is a technical concern that only relates to my REST/HTTP API. A different API (say a synchronous gRPC for example) would not have that info. Does it make sense to store it into a &quot;api-level&quot; storage? On the other hand, the same argument applies for a user email, which I could see as a email-protocol identifier (still I directly put it into my domain model).</p>
<p>Am I splitting hairs here? Is there a &quot;common&quot; way to do that?</p>
<p>Many thanks in advance :-)</p>
<p>Franck</p>
",68,1,0,3,rest;domain-driven-design;webhooks,2022-05-20 19:49:23,2022-05-20 19:49:23,2022-05-22 13:17:52,i have a question about web hooks and how they map to domain driven events  i am modelling a system using domain driven design  without going into the details  there is a domain service  a machine learning prediction  which takes some time to run  the user would like therefore to send  batches  in an asynchronous manner  at a high level  i have a rest api  a controller   which sends commands to my application  use cases   which manipulates my domain model  on the modelling side  i think of the completion of a prediction and of a batch as domain events  on the api side  i am considering   where the api user could register an endpoint and choose which events she would like to receive  for example  she could subscribe only to the  batch done  event  or to both  prediction done  and  batch done   my question is whether the concept of subscription subscriber should be part of my domain model  in other words  where should i store the  endpoint  associated with a subscriber  in the domain model or in api level storage  on the one hand  i feel it does not belong to my domain  because it is a technical concern that only relates to my rest http api  a different api  say a synchronous grpc for example  would not have that info  does it make sense to store it into a  api level  storage  on the other hand  the same argument applies for a user email  which i could see as a email protocol identifier  still i directly put it into my domain model   am i splitting hairs here  is there a  common  way to do that  many thanks in advance     franck
540,540,11052706,72141756,Forecasting out of sample with Fourier regressors,"<p>I'm trying to create a multivariate multi-step-ahead forecast using machine learning (weekly and yearly seasonality).
I use some exogenous variables, including Fourier terms. I'm happy with the results of testing the model with in sample data, but now I want to go for production and make real forecasts on completely unseen data. While I can update the other regressors (variables) since they are dummy variables and related to time, I don't know how I will generate new Fourier terms for the N steps ahead.
I have an understanding problem here and what to check it with you: when you generate the fourier terms based on periodicity and the number of sin/cos used to decompose the time serie you want to forecast this process should be independent on that values of the time series. Is that right?
If so, how do you extend the terms for the N steps?
Just for the sake of completeness, I use R.</p>
<p>Thank you</p>
",25,1,0,2,fft;forecast,2022-05-06 18:21:26,2022-05-06 18:21:26,2022-05-22 07:06:43,thank you
541,541,7438090,41750346,"R caret package, how do I tune the intercept and slope in lm?","<p>I'm attempting to use the R caret package to perform 5-fold cross-validation of a linear regression model. I'm new to machine learning, but I expected that with each ""repeat"", a new slope and intercept would be fit to the ""training"" data set. However by default,  the slope and intercept are held constant for all repeats, and the testing just appears to be putting out new RMSE and Rsquared with each repeat. Is there a way to allow tuning of the intercept?</p>

<p>Here is my code:</p>

<pre><code>regressControl  &lt;- trainControl(method=""repeatedcv"",
                            number = 5,
                            repeats = 5)    

regress         &lt;- train(y ~ x,
                   data = myData,
                   method  = ""lm"",
                   trControl = regressControl)
regress
</code></pre>

<p>The output looks like this:</p>

<pre><code>Linear Regression 

54 samples
 1 predictor

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 5 times) 
Summary of sample sizes: 45, 44, 42, 42, 43, 43, ... 
Resampling results:

  RMSE        Rsquared 
  0.01162334  0.9614908

Tuning parameter 'intercept' was held constant at a value of TRUE

regress$finalModel
Call:
lm(formula = .outcome ~ ., data = dat)

Coefficients:
(Intercept)     x  
   -0.03054      0.01690  
</code></pre>
",1326,1,1,3,r;r-caret;lm,2017-01-20 01:15:31,2017-01-20 01:15:31,2022-05-22 06:38:21,i m attempting to use the r caret package to perform  fold cross validation of a linear regression model  i m new to machine learning  but i expected that with each repeat  a new slope and intercept would be fit to the training data set  however by default   the slope and intercept are held constant for all repeats  and the testing just appears to be putting out new rmse and rsquared with each repeat  is there a way to allow tuning of the intercept  here is my code  the output looks like this 
542,542,19170581,72333498,I&#39;m having trouble working on 2 dataframes,"<p>I'm super new to python programming and I'm pursuing an MBA in data science and machine learning. I'm stuck on a problem that I can't solve, I have these 2 tables in CSV, one with some data on purchases of public papers here in my country, and the other is the interest rate of each month of the years that I managed to get.</p>
<p>What I would like to do is pass the value of these fees to the transaction dataframe grouped by month, I'm really not able to do that, if anyone can help me on how to do this I will be grateful.</p>
<p>I put the 2 csv in the drive for anyone:
<a href=""https://drive.google.com/drive/folders/18_sOSIZZw9DCW7ftEKuOG4aIzGXoasFe?usp=sharing"" rel=""nofollow noreferrer"">2 csv files</a></p>
<p>Some code I managed to do to clean the data frames:</p>
<pre><code>import pandas as pd
df = pd.read_csv('VendasTesouroDireto3.csv', sep=';')
df['Data Venda'] = pd.to_datetime(df['Data Venda'])
df.sort_values(by='Data Venda', ascending=True)
df2 = pd.read_csv('historico.csv', sep = ';')
df2['Data'] = pd.to_datetime(df2['Data'])
df2.dropna()
</code></pre>
<p>Just to contextualize: I want to pass the values of the column &quot;Taxa Selic (ao ano)&quot; of the table &quot;historico&quot;, to the table &quot;VendasTesouroDireto3&quot; grouped by months and year.</p>
<p>This topic will be made public once posted.</p>
",34,1,-1,2,python;pandas,2022-05-22 03:30:04,2022-05-22 03:30:04,2022-05-22 04:51:57,i m super new to python programming and i m pursuing an mba in data science and machine learning  i m stuck on a problem that i can t solve  i have these  tables in csv  one with some data on purchases of public papers here in my country  and the other is the interest rate of each month of the years that i managed to get  what i would like to do is pass the value of these fees to the transaction dataframe grouped by month  i m really not able to do that  if anyone can help me on how to do this i will be grateful  some code i managed to do to clean the data frames  just to contextualize  i want to pass the values of the column  taxa selic  ao ano   of the table  historico   to the table  vendastesourodireto  grouped by months and year  this topic will be made public once posted 
543,543,10994166,66594056,Linear Regression on each grid cell across time dim,"<p>I'm new to xarray and machine learning stuff.</p>
<p>So I have xarray dataset as follows:</p>
<pre><code>&lt;xarray.Dataset&gt;
Dimensions:    (latitude: 721, longitude: 1440, time: 72)
Coordinates:
  * time       (time) datetime64[ns] 1950-01-01 1951-01-01 ... 2021-01-01
  * longitude  (longitude) float32 0.0 0.25 0.5 0.75 ... 359.25 359.5 359.75
  * latitude   (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0
Data variables:
    z          (time, latitude, longitude) float32 49654.793 49654.793 ... 49654.793
</code></pre>
<p>Now I want to apply Linear Regression on each grid across time dimension, then I want to remove the regression value from the original value to remove the trend. Below is an example of one sample grid.</p>
<pre><code>y = np.array(jan.z[:, 700, 700]) #single grid with all time
x = (np.arange(1950, len(y)+1949)).reshape(-1, 1) #72 time for x axis which will remain same for all grid

reg = LinearRegression().fit(x, y)

pred = reg.predict(x)
y = (y - (pred - y))
</code></pre>
<p>Now this is for just one grid I have such <code>721*14000</code> grid so for loop won't be the most optimized way to do it, is there more optimized way or some direct function to do so in xarray? I tried looking for similar thing but I'm not able to find which can solve my problem.</p>
",189,1,3,5,python;python-3.x;machine-learning;linear-regression;python-xarray,2021-03-12 09:37:30,2021-03-12 09:37:30,2022-05-22 03:10:40,i m new to xarray and machine learning stuff  so i have xarray dataset as follows  now i want to apply linear regression on each grid across time dimension  then i want to remove the regression value from the original value to remove the trend  below is an example of one sample grid  now this is for just one grid i have such   grid so for loop won t be the most optimized way to do it  is there more optimized way or some direct function to do so in xarray  i tried looking for similar thing but i m not able to find which can solve my problem 
544,544,2135504,72331301,How are codebases for large data science teams organized?,"<p>Given a company with a large data science or machine learning team, what does the codebase typically look like?</p>
<p>I imagine that some code is only relevant for a specific project (e.g. a special kind of data parser or plotting function) and some code (e.g. validation strategies, losses, etc.) is worth sharing and maintaining. On top of that, people might use incompatible frameworks (e.g. pytorch vs tensorflow) and versions, so full compatibility cannot be achieved.</p>
<p>Any specific examples and/or experiences would be very much appreciated.</p>
",10,0,0,3,machine-learning;data-science;codebase,2022-05-21 21:47:20,2022-05-21 21:47:20,2022-05-21 21:47:20,given a company with a large data science or machine learning team  what does the codebase typically look like  i imagine that some code is only relevant for a specific project  e g  a special kind of data parser or plotting function  and some code  e g  validation strategies  losses  etc   is worth sharing and maintaining  on top of that  people might use incompatible frameworks  e g  pytorch vs tensorflow  and versions  so full compatibility cannot be achieved  any specific examples and or experiences would be very much appreciated 
545,545,7893889,44989785,Running MS-MPI application on two Windows machines,"<p>I am learning MPI. I have two Windows machines on each I have VS2015 and I have installed Microsoft HPC Pack 2012, Microsoft HPC Pack 2012 SDK 8.1, Microsoft MPI 8.1 and I am able to run the following code on each machine separately.</p>

<p>I want to connect the two machines to communicate through MPI and run this code while having the ability to specify which process ID runs on which machine. What is the next step to achieve this? </p>

<pre><code>#include&lt;iostream&gt;
#include ""mpi.h""
#include &lt;omp.h&gt;

using namespace std;

int numOfProc, id, array_size, portion;
int *arr = NULL;
MPI_Status status;
const static int tag = 1;


int main(int argc, char *argv[])
{
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;numOfProc);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;id);

    cout &lt;&lt; ""Hello from Process # "" &lt;&lt; id &lt;&lt; '\n';

    if (id == 0)//master
    {
        cin &gt;&gt; array_size;
        arr = new int[array_size];

        for (int i = 0; i &lt; array_size; i++)
        {
            arr[i] = i + 1;
        }

        portion = array_size / numOfProc;

        for (int p = 1; p &lt; numOfProc; p++)
        {

            MPI_Send(&amp;portion, 1, MPI_INT, p, tag, MPI_COMM_WORLD);
            MPI_Send(&amp;arr[(p - 1)*portion], portion, MPI_INT, p, tag, MPI_COMM_WORLD);
        }
    }
    else // slaves 
    {

        //cout &lt;&lt; ""Hello from Process # "" &lt;&lt; id &lt;&lt; '\n';

        MPI_Recv(&amp;portion, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &amp;status);

        arr = new int[portion];

        MPI_Recv(arr, portion, MPI_INT, 0, tag, MPI_COMM_WORLD, &amp;status);


    omp_set_num_threads(2);
#pragma omp parallel for
        for (int i = 0; i &lt; portion; i++)
        {
            cout &lt;&lt; ""Thread ["" &lt;&lt; omp_get_thread_num() &lt;&lt; ""] is printing number "" &lt;&lt; arr[i] &lt;&lt; ""."" &lt;&lt; endl;
        }

    }

    MPI_Finalize();

}
</code></pre>
",532,0,0,4,c++;mpi;hpc;ms-mpi,2017-07-09 00:31:53,2017-07-09 00:31:53,2022-05-21 21:11:26,i am learning mpi  i have two windows machines on each i have vs and i have installed microsoft hpc pack   microsoft hpc pack  sdk    microsoft mpi   and i am able to run the following code on each machine separately  i want to connect the two machines to communicate through mpi and run this code while having the ability to specify which process id runs on which machine  what is the next step to achieve this  
546,546,11788164,72297937,How to pass the variable from Python and receive it in PHP,"<p><strong>Background</strong></p>
<p>I see similar questions in Stack Overflow, but I didn't get the answer I want. I am trying to develop a project which contains a upload page where users upload the image to it which is saved on server.</p>
<p>Server runs the Machine learning algorithm to detect the objects and send the detected object back to the server and displays in same webpage (upload page).</p>
<p><strong>What I did</strong></p>
<p>Currently I finished the upload page and Python ML code.</p>
<p>I need to send the detected objects from Python back to the PHP server, for that I user Python request module and successfully post the message to the server.</p>
<pre><code>//obj detection code here
detected_obj= {&quot;value&quot;: &quot;This image contains car cycle&quot;)}     // Have to send this data
import requests
url = &quot;http://localhost/index.php&quot;
r = requests.post(url, data=detected_obj)
print(r.text)                               // I got the entire html code of the index.php
</code></pre>
<p>But in PHP, I can't able to get the response back.</p>
<pre><code>//PHP code
//rest of the file upload code
&lt;?php
$value = $_POST[&quot;value&quot;];
echo $value;
?&gt;
</code></pre>
<p><strong>What I want</strong></p>
<p>I think I made some mistake in PHP code, I'm new to PHP. How can I get the value from Python request and update in PHP?</p>
<p><em>i.e PHP has to wait for any POST message and when it received it has to print the posted data in the webpage.</em></p>
<p>I searched in internet, where they get the post message from the form and validate it using isset() method and print the response. In my case I had no form, PHP listens for any post message, once it receives, it has to print the message in the same page.</p>
<p>On internet, I found that it can be done by ajax, but I didn't know how to do this and how it fits in this case.</p>
",37,1,0,5,python;php;ajax;post;file-upload,2022-05-19 07:49:22,2022-05-19 07:49:22,2022-05-21 18:34:11,background i see similar questions in stack overflow  but i didn t get the answer i want  i am trying to develop a project which contains a upload page where users upload the image to it which is saved on server  server runs the machine learning algorithm to detect the objects and send the detected object back to the server and displays in same webpage  upload page   what i did currently i finished the upload page and python ml code  i need to send the detected objects from python back to the php server  for that i user python request module and successfully post the message to the server  but in php  i can t able to get the response back  what i want i think i made some mistake in php code  i m new to php  how can i get the value from python request and update in php  i e php has to wait for any post message and when it received it has to print the posted data in the webpage  i searched in internet  where they get the post message from the form and validate it using isset   method and print the response  in my case i had no form  php listens for any post message  once it receives  it has to print the message in the same page  on internet  i found that it can be done by ajax  but i didn t know how to do this and how it fits in this case 
547,547,19164814,72325769,Java Error Eclipse 2022 after uninstalling java old version,"<p>I am learning Java and I had unintall old version and install new
I can not open eclipse,</p>
<p>After I click on eclipse I see following error:</p>
<p>A Java Runtime Environment (JRE) or Java Development Kit (JDK) must be available in order to run Eclipse. No Java Virtual Machine was found after searching the following locations:
C:\Program Files \Java\jdk-15\bin/javaw.exe</p>
",14,0,0,1,java,2022-05-21 05:41:39,2022-05-21 05:41:39,2022-05-21 05:41:39,after i click on eclipse i see following error 
548,548,3897315,72325264,Emacs Tramp access to AWS SageMaker instance,"<p>I do machine learning code development on an AWS SageMaker instance, and would like to make use of Emacs/Tramp<sup>†</sup>. My question: how, if at all, could that be done? I strongly suspect that some knowledge about security protocols / IAM roles, etc., would be important, but I am a mere SageMaker end-user.</p>
<p>A co-worker may be able to assist on some of these questions, but he is way over-subscribed. It is a big ask of him to indulge my personal preferences (however strong), so I start by posing the question, has anybody else has already solved this problem, or are there good starting points for consideration?</p>
<p>Already seen:</p>
<ul>
<li>Martin Baillie's <a href=""https://martin.baillie.id/wrote/emacs-tramp-over-aws-ssm-apis/"" rel=""nofollow noreferrer"">Emacs TRAMP over AWS SSM APIs</a>, which seems light on key details, and may not be applicable for SageMaker environments</li>
<li>AWS's own <a href=""https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-pycharm.html"" rel=""nofollow noreferrer"">Tutorial: Set Up PyCharm Professional with a Development Endpoint</a>, which seems to be pretty specific to PyCharm, and which again may not be suitable for SageMaker environments.</li>
</ul>
<p><sup>†</sup>Why? Because I have a long lifetime of using emacs key bindings and macros, etc., that improve my efficiency greatly. Why not emacs within a terminal running on a SageMaker instance? That's what I'm doing now, but it leaves out important flexibility compared with a local windowed emacs client; the latter can be as tall or wide as my pixels permit, can have multiple frames simultaneously, wouldn't have as many networking latencies, etc.</p>
",21,0,0,4,amazon-web-services;emacs;amazon-sagemaker;tramp,2022-05-21 03:48:44,2022-05-21 03:48:44,2022-05-21 03:59:13,i do machine learning code development on an aws sagemaker instance  and would like to make use of emacs tramp   my question  how  if at all  could that be done  i strongly suspect that some knowledge about security protocols   iam roles  etc   would be important  but i am a mere sagemaker end user  a co worker may be able to assist on some of these questions  but he is way over subscribed  it is a big ask of him to indulge my personal preferences  however strong   so i start by posing the question  has anybody else has already solved this problem  or are there good starting points for consideration  already seen   why  because i have a long lifetime of using emacs key bindings and macros  etc   that improve my efficiency greatly  why not emacs within a terminal running on a sagemaker instance  that s what i m doing now  but it leaves out important flexibility compared with a local windowed emacs client  the latter can be as tall or wide as my pixels permit  can have multiple frames simultaneously  wouldn t have as many networking latencies  etc 
549,549,19146682,72324365,Column in dataframe erroring due to object format,"<p>I am working with labeled data where the ID numbers are in a dataframe such as: “AQG123” “012AGD”</p>
<p>When running a supervised machine learning method it states it could not convert string to float and erroring the ID column.
How do I work around this? The ID column is an object.</p>
",16,0,0,3,pandas;string;object,2022-05-21 01:43:35,2022-05-21 01:43:35,2022-05-21 01:43:35,i am working with labeled data where the id numbers are in a dataframe such as   aqg   agd 
550,550,19139716,72315885,How to deploy a machine learning model in a django web app?,"<p>My project is the conception and realization of a web application for the detection of ransomwares based on machine learning.
For the realization, I made a web application with python, Django, HTML and CSS.
On the other hand i have to create a machine learning code that makes the detection of these ransomware viruses.</p>
<p>Now what I have to do is deploy the machine learning code in my web app.
I'll explain a little how it works, the user first registers and then he chooses a csv file to scan, he uploads it and then he chooses the machine learning model he wants  use and then he clicks on scan, when he clicks on scan the file will be scanned by the model he has chosen and a result will be returned to him which is either 1: the file is a ransomware or 0: it is not a ransomware.
So,
I built the web app, I built the model
Now my problem is how to pass user input to the model
And than take the model's out put to the user.</p>
<p>Need help please.</p>
",51,2,0,3,python;django;machine-learning,2022-05-20 14:05:28,2022-05-20 14:05:28,2022-05-20 23:03:26,need help please 
551,551,4451521,72320359,Jupyter Notebook RuntimeError: Permissions assignment failed for secure file: &#39;. Got &#39;0o655&#39; instead of &#39;0o0600&#39;,"<p>I am reading the book <a href=""https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter01"" rel=""nofollow noreferrer"">Machine Learning Engineering with MLflow</a> and in the most basic of examples I got the error of the title.</p>
<p>There is a very simple docker file</p>
<pre><code>FROM jupyter/scipy-notebook
RUN pip install mlflow
RUN pip install sklearn
</code></pre>
<p>and I build it with <code>docker build -t chapter_1_homlflow .</code></p>
<p>However I am working on a windows 10 machine so I run the docker with:</p>
<pre><code>docker run -p 8888:8888 -p 5000:5000 -v %cd%:/home/jovyan/ -it chapter_1_homlflow
</code></pre>
<p>The jupyter notebook starts running but then I got the error</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/conda/bin/jupyter-lab&quot;, line 10, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.9/site-packages/jupyter_server/extension/application.py&quot;, line 594, in launch_instance
    serverapp.start()
  File &quot;/opt/conda/lib/python3.9/site-packages/jupyter_server/serverapp.py&quot;, line 2757, in start
    self.start_app()
  File &quot;/opt/conda/lib/python3.9/site-packages/jupyter_server/serverapp.py&quot;, line 2687, in start_app
    self.write_server_info_file()
  File &quot;/opt/conda/lib/python3.9/site-packages/jupyter_server/serverapp.py&quot;, line 2503, in write_server_info_file
    with secure_write(self.info_file) as f:
  File &quot;/opt/conda/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
    return next(self.gen)
  File &quot;/opt/conda/lib/python3.9/site-packages/jupyter_core/paths.py&quot;, line 903, in secure_write
    raise RuntimeError(
RuntimeError: Permissions assignment failed for secure file: '/home/jovyan/.local/share/jupyter/runtime/jpserver-7.json'. Got '0o655' instead of '0o0600'.
</code></pre>
<p>I tried advice found in SO so I changed</p>
<pre><code>docker run -p 8888:8888 -p 5000:5000 -v /users/user/ML_MLflow/Machine-Learning-Engineering-with-MLflow/Chapter01:/home/jovyan/ -it chapter_1_homlflow
</code></pre>
<p>and the error went away but now I got the error</p>
<pre><code> [Errno 13] Permission denied: '/home/jovyan/.local'
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.9/site-packages/traitlets/traitlets.py&quot;, line 642, in get
    value = obj._trait_values[self.name]
KeyError: 'runtime_dir'
</code></pre>
<p>Is this a product of using windows? How can the jupyter from inside the container can be run?</p>
<p>EDIT:
I could overcome the problem thanks to the advice from <a href=""https://github.com/jupyter/docker-stacks/issues/1003#issuecomment-583768897"" rel=""nofollow noreferrer"">here</a> so I did</p>
<pre><code>docker run --user root -e CHOWN_HOME=yes -e CHOWN_HOME_OPTS=-R -p 8888:8888 -p 5000:5000 -v /users/user/ML_MLflow/Machine-Learning-Engineering-with-MLflow/Chapter01:/home/jovyan/ -it --rm chapter_1_homlflow
</code></pre>
<p>and now I got the jupyter lab running (although with a password so I had to access it with the url that is output in the command window)</p>
<p>Now the problem is that the mapping (the -v option above) does not happen and <code>/home/jovyan</code> is empty.</p>
<p>EDIT2: Now I found that I made a spelling mistake and I should have put Users with capital U.
But with this now I get the error</p>
<pre><code>Error while creating the mounting path. mkdir /Users file exists. 
</code></pre>
<p>Similar error to <a href=""https://stackoverflow.com/questions/50817985/docker-tries-to-mkdir-the-folder-that-i-mount"">this question</a></p>
<p>The only reason I can find is that the Users folder actually is in japanese, in file explorer</p>
",33,0,0,3,python;docker;jupyter-notebook,2022-05-20 19:42:28,2022-05-20 19:42:28,2022-05-20 20:30:51,i am reading the book  and in the most basic of examples i got the error of the title  there is a very simple docker file and i build it with docker build  t chapter__homlflow   however i am working on a windows  machine so i run the docker with  the jupyter notebook starts running but then i got the error i tried advice found in so so i changed and the error went away but now i got the error is this a product of using windows  how can the jupyter from inside the container can be run  and now i got the jupyter lab running  although with a password so i had to access it with the url that is output in the command window  now the problem is that the mapping  the  v option above  does not happen and  home jovyan is empty  similar error to  the only reason i can find is that the users folder actually is in japanese  in file explorer
552,552,2543582,17431712,SoX How to add noise to a file?,"<p>I would like to ADD noise to audio files.  Is there a way to accomplish this in SoX (or other tool)?</p>

<p>I am performing machine learning research, and need to test my algorithms in the presence of noise.  Ideally, I would like to specify a signal to noise ratio and have noise added to reach that target SNR.</p>

<p>I know that I could generate a noise file and subsequently mix it with each of my source files.  As I need to do this in a batch mode over thousands of files, I seek simpler more elegant method.</p>

<p>Any ideas or suggestions most welcome.  Thank you. </p>
",6064,2,4,2,noise;sox,2013-07-02 22:40:23,2013-07-02 22:40:23,2022-05-20 20:16:41,i would like to add noise to audio files   is there a way to accomplish this in sox  or other tool   i am performing machine learning research  and need to test my algorithms in the presence of noise   ideally  i would like to specify a signal to noise ratio and have noise added to reach that target snr  i know that i could generate a noise file and subsequently mix it with each of my source files   as i need to do this in a batch mode over thousands of files  i seek simpler more elegant method  any ideas or suggestions most welcome   thank you  
553,553,13527873,72319891,Application and Deployment of K-Fold Cross-Validation,"<p>K-Fold Cross Validation is a technique applied for splitting up the data into K number of Folds for testing and training. The goal is to estimate the generalizability of a machine learning model. The model is trained K times, once on each train fold and then tested on the corresponding test fold.</p>
<p>Suppose I want to compare a Decision Tree and a Logistic Regression model on some arbitrary dataset with 10 Folds. Suppose after training each model on each of the 10 folds and obtaining the corresponding test accuracies, Logistic Regression has a higher mean accuracy across the test folds, indicating that it is the better model for the dataset.</p>
<p>Now, for application and deployment. Do I retrain the Logistic Regression model on all the data, or do I create an ensemble from the 10 Logistic Regression models that were trained on the K-Folds?</p>
",48,2,2,4,machine-learning;scikit-learn;cross-validation;k-fold,2022-05-20 19:09:25,2022-05-20 19:09:25,2022-05-20 19:51:45,k fold cross validation is a technique applied for splitting up the data into k number of folds for testing and training  the goal is to estimate the generalizability of a machine learning model  the model is trained k times  once on each train fold and then tested on the corresponding test fold  suppose i want to compare a decision tree and a logistic regression model on some arbitrary dataset with  folds  suppose after training each model on each of the  folds and obtaining the corresponding test accuracies  logistic regression has a higher mean accuracy across the test folds  indicating that it is the better model for the dataset  now  for application and deployment  do i retrain the logistic regression model on all the data  or do i create an ensemble from the  logistic regression models that were trained on the k folds 
554,554,18051160,70980156,NLP Classification on a dataset,"<p>I am trying to learned NLP. I understand the basic concepts from Text Preprocessing to td-idf, and Word Embedding. How do I apply this learning? I have a Data set with two columns: Answer and Gender. I want to use NLP to transform the Answer column to vectors and then use supervised machine learning to train a model that predict where a certain type of answer was given by male or a female.
I dont know how to process after I Pre_processed the text.</p>
",36,1,0,2,nlp;text-classification,2022-02-04 06:05:03,2022-02-04 06:05:03,2022-05-20 14:37:35,
555,555,19083869,72312484,ValueError: inconsistent shapes after using MultiLabelBinarizer,"<p>I'm trying to create a Performance Evaluation Results for my CRF model, which depicts what part of speech does the word belong to. I've created a function to transform the data in a more 'datasetish' format. This function returns the data as two lists, one of Dicts of features and the other with the labels.</p>
<pre class=""lang-py prettyprint-override""><code>def transform_to_dataset(tagged_sentences):
    X, y = [], []
    for sentence, tags in tagged_sentences:
        sent_word_features, sent_tags = [], []
        for index in range(len(sentence)):
            sent_word_features.append(extract_features(sentence, index)),
            sent_tags.append(tags[index])
        X.append(sent_word_features)
        y.append(sent_tags)
    return X, y
</code></pre>
<p>then I divide the set BEFORE encoding to have full sentences in training/testing sets.</p>
<pre><code>penn_train_size = int(0.8*len(penn_treebank))
penn_training = penn_treebank[:penn_train_size]
penn_testing = penn_treebank[penn_train_size:]
X_penn_train, y_penn_train = transform_to_dataset(penn_training)
X_penn_test, y_penn_test = transform_to_dataset(penn_testing)
</code></pre>
<p>and then I load the model for it to train and test my data</p>
<pre><code>penn_crf = CRF(
    algorithm='lbfgs',
    c1=0.01,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=True
)
#The fit method is the default name used by Machine Learning algorithms to start training.
print(&quot;Started training on Penn Treebank corpus!&quot;)
penn_crf.fit(X_penn_train, y_penn_train)
print(&quot;Finished training on Penn Treebank corpus!&quot;)
</code></pre>
<p>and then I test it with</p>
<pre><code>y_penn_pred=penn_crf.predict(X_penn_test)
</code></pre>
<p>But when I try</p>
<pre><code>from sklearn.metrics import accuracy_score

print(&quot;Accuracy: &quot;, accuracy_score(y_penn_test, y_penn_pred))
</code></pre>
<p>It gives off an error:</p>
<blockquote>
<p>ValueError: You appear to be using a legacy multi-label data
representation. Sequence of sequences are no longer supported; use a
binary array or sparse matrix instead - the MultiLabelBinarizer
transformer can convert to this format.</p>
</blockquote>
<p>But when I try to use MultiLabelBinarizer;</p>
<pre><code>from sklearn.preprocessing import MultiLabelBinarizer

bin_y_penn_test = MultiLabelBinarizer().fit_transform(y_penn_test)
bin_y_penn_pred = MultiLabelBinarizer().fit_transform(y_penn_pred)
</code></pre>
<p>It gives me an error of:</p>
<blockquote>
<p>ValueError: inconsistent shapes</p>
</blockquote>
<p>Here's the full traceback</p>
<blockquote>
<p>--------------------------------------------------------------------------- ValueError Traceback (most recent call last)
/tmp/ipykernel_5694/856179584.py in 
1 from sklearn.metrics import accuracy_score
2
----&gt; 3 print(&quot;Accuracy: &quot;, accuracy_score(bin_y_penn_test, bin_y_penn_pred))</p>
<p>~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py in
inner_f(*args, **kwargs)
61             extra_args = len(args) - len(all_args)
62             if extra_args &lt;= 0:
---&gt; 63                 return f(*args, **kwargs)
64
65             # extra_args &gt; 0</p>
<p>~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py
in accuracy_score(y_true, y_pred, normalize, sample_weight)
203     check_consistent_length(y_true, y_pred, sample_weight)
204     if y_type.startswith('multilabel'):
--&gt; 205         differing_labels = count_nonzero(y_true - y_pred, axis=1)
206         score = differing_labels == 0
207     else:</p>
<p>~/.local/lib/python3.8/site-packages/scipy/sparse/base.py in
<strong>sub</strong>(self, other)
431         elif isspmatrix(other):
432             if other.shape != self.shape:
--&gt; 433                 raise ValueError(&quot;inconsistent shapes&quot;)
434             return self._sub_sparse(other)
435         elif isdense(other):</p>
<p>ValueError: inconsistent shapes</p>
</blockquote>
<p>What should I do so that I could produce the confusion matrix of my model?</p>
",52,1,1,5,python;machine-learning;scikit-learn;confusion-matrix;crf,2022-05-20 06:23:19,2022-05-20 06:23:19,2022-05-20 12:47:16,i m trying to create a performance evaluation results for my crf model  which depicts what part of speech does the word belong to  i ve created a function to transform the data in a more  datasetish  format  this function returns the data as two lists  one of dicts of features and the other with the labels  then i divide the set before encoding to have full sentences in training testing sets  and then i load the model for it to train and test my data and then i test it with but when i try it gives off an error  but when i try to use multilabelbinarizer  it gives me an error of  valueerror  inconsistent shapes here s the full traceback valueerror  inconsistent shapes what should i do so that i could produce the confusion matrix of my model 
556,556,16007337,70196515,Getting the shape of the array from a PKL file,"<p>I have this machine learning model that I have saved into a PKL file. In &quot;MC_features&quot; you can see that I have assigned the 21 values in the model to be 0 by default. I am trying to see if there is a way to get the shape of the array from the PKL file so I would not need to manually change the &quot;21&quot; and it will automatically register it from the shape or the number of elements in array.</p>
<p>This is how I saved the PKL file.</p>
<pre><code>Dtree = LinearRegression()
Dtree.fit(X_train, y_train)

import joblib
joblib.dump(Dtree,'MC_Model.pkl')
</code></pre>
<p>This is my flask code.</p>
<pre><code>from flask import Flask, request, render_template
import numpy as np
import joblib

app = Flask(__name__)
model = joblib.load('MC_Model.pkl')

@app.route('/', methods=[&quot;GET&quot;, &quot;POST&quot;])
def home():
    # default values for `GET`
    cluster = None
    material = None
    MC_features = [np.zeros((21,), dtype=int)]
    MC_text = 'Usage Amount: ???'
    
    if request.method == 'POST':
        cluster = request.form.get('cluster')
        print('cluster:', cluster)
        
        material = request.form.get('material')
        print('material:', material)
                  
        if cluster:
            index = int(cluster)
            MC_features[0][index] = 1.0

        if material:
            index = int(material)
            MC_features[0][index] = 1.0

        print(MC_features)
        
        MC_prediction = model.predict(MC_features)
        #prediction = [np.random.randint(0, 100)]
        
        MC_text = 'Usage Amount: {}'.format(MC_prediction[0])
                
    return render_template('BOM.html',MC_prediction=MC_text, MC_features=MC_features[0], cluster=cluster, material=material)
</code></pre>
",51,1,0,3,python;arrays;joblib,2021-12-02 14:24:57,2021-12-02 14:24:57,2022-05-20 11:11:30,i have this machine learning model that i have saved into a pkl file  in  mc_features  you can see that i have assigned the  values in the model to be  by default  i am trying to see if there is a way to get the shape of the array from the pkl file so i would not need to manually change the    and it will automatically register it from the shape or the number of elements in array  this is how i saved the pkl file  this is my flask code 
557,557,5645165,65108421,Google Colab keeps crashing when trying to run keras tuner,"<p>This is my first machine learning project, working with a dataset i have created on my own.</p>
<p>Unfortunately Google Colab keeps crashing.
And it seems to have something to do with keras tuner, but i am not sure.</p>
<p>It actually worked for a while.
But now it is crashing immediately when i run it.</p>
<p>edit: it is when i run the tuner.search that Colab crashes.</p>
<p>The log. (read from the bottom and up)</p>
<pre><code>Dec 2, 2020, 12:53:12 PM    WARNING 
WARNING:root:kernel e615fcc9-5bdc-44af-ad35-ee2a772f131f restarted
Dec 2, 2020, 12:53:12 PM    INFO    KernelRestarter: 
restarting kernel (1/5), keep random ports
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.006902: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] 
Created TensorFlow device 
(/job:localhost/replica:0/task:0/device:GPU:0 with 10630 MB memory) 
-&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, 
compute capability: 3.7)
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.006032: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1), 
but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.004903: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1), 
but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.004580: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.004559: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0
Dec 2, 2020, 12:53:11 PM    WARNING 2020-12-02 11:53:11.004497: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] 
Device interconnect StreamExecutor with strength 1 edge matrix:
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.529441: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcudart.so.10.1
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.529298: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] 
Adding visible gpu devices: 0
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.528166: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1), 
but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526440: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1), 
but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526344: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcudnn.so.7
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526305: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcusparse.so.10
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526268: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcusolver.so.10
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526227: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcurand.so.10
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526186: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcufft.so.10
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.526125: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcublas.so.10
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.525706: 
I tensorflow/stream_executor/platform/default/dso_loader.cc:48] 
Successfully opened dynamic library libcudart.so.10.1
Dec 2, 2020, 12:53:10 PM    WARNING coreClock: 0.8235GHz coreCount: 
13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
Dec 2, 2020, 12:53:10 PM    WARNING pciBusID: 0000:00:04.0 name: 
Tesla K80 computeCapability: 3.7
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.525625: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] 
Found device 0 with properties:
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.524630: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1)
, but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.523938: 
I tensorflow/compiler/xla/service/service.cc:176] 
StreamExecutor device (0): Tesla K80, Compute Capability 3.7
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.523902: 
I tensorflow/compiler/xla/service/service.cc:168] 
XLA service 0x7a39500 initialized for platform CUDA 
(this does not guarantee that XLA will be used). Devices:
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.522755: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] 
successful NUMA node read from SysFS had negative value (-1), 
but there must be at least one NUMA node, so returning NUMA node zero
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.467341: 
I tensorflow/compiler/xla/service/service.cc:176] 
StreamExecutor device (0): Host, Default Version
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.467308: 
I tensorflow/compiler/xla/service/service.cc:168] 
XLA service 0x2383480 initialized for platform Host 
(this does not guarantee that XLA will be used). Devices:
Dec 2, 2020, 12:53:10 PM    WARNING 2020-12-02 11:53:10.466693: 
I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] 
CPU Frequency: 2300000000 Hz

</code></pre>
<p>My code</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import kerastuner
from tensorflow import keras
from kerastuner.tuners import RandomSearch
from kerastuner.engine.hypermodel import HyperModel
from kerastuner.engine.hyperparameters import HyperParameters
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.losses import sparse_categorical_crossentropy

!unzip -q /content/paintings.zip

data_dir = &quot;/content/paintings&quot;

#Theese three rows of code is only here because i read somewhere 
#that it would help solve the problem, but it does not.
gpu_devices = tf.config.experimental.list_physical_devices('GPU')
for device in gpu_devices:
    tf.config.experimental.set_memory_growth(device, True)

num_classes = 50
nb_epochs = 10
batch_size = 16
img_height = 128
img_width = 128

train_datagen = ImageDataGenerator(rescale=1./255,
    validation_split=0.2) 

train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle = True,
    class_mode=&quot;sparse&quot;,
    subset='training') 

validation_generator = train_datagen.flow_from_directory(
    data_dir, 
    target_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle = True,
    class_mode=&quot;sparse&quot;,
    subset='validation') 

hp = HyperParameters()
hp.Choice('learning_rate', [0.005, 1e-4])
hp.Int('num_layers_conv', 1, 5)
hp.Int('num_layers_dense', 1, 3)
hp.Int('dense_n',
        min_value=0,
        max_value=500,
        step=50)
hp.Choice(
        'activation',
        values=['relu', 'tanh'],
        default='relu')
hp.Float('dropout',
          min_value=0.0,
          max_value=0.5,
          default=0.25,
          step=0.05)

def build_model(hp):
    model = keras.Sequential()

    for i in range(hp.get('num_layers_conv')): 
        model.add(layers.Conv2D
            (filters=hp.Int('filters_' + str(i), 0, 512, step=32),
            kernel_size=hp.Int('kernel_size_' + str(i), 3, 5), padding=&quot;same&quot;, 
            activation=hp.get('activation')))

    model.add(layers.MaxPooling2D(pool_size=(2,2)))
  
    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))
    
    model.add(layers.MaxPooling2D(pool_size=(2,2)))

    model.add(layers.Flatten())
    
    for i in range(hp.get('num_layers_dense')): 
        model.add(layers.Dense(units=hp.get('dense_n'), 
        activation=hp.get('activation')))
        model.add(layers.BatchNormalization())
        model.add(layers.Dropout(rate=hp.get('dropout')))

    model.add(layers.Dense(num_classes, activation='softmax'))
    
    model.compile(
        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])
    
    return model

tuner = RandomSearch(
    build_model,
    max_trials=100,
    executions_per_trial=1,
    hyperparameters=hp,
    directory = &quot;output&quot;,
    project_name = &quot;ArtNet&quot;,
    objective='val_accuracy')

tuner.search(train_generator,
             epochs=10,
             validation_data=validation_generator)

</code></pre>
<p>Any help would be really appreciated!</p>
",1334,2,1,5,tensorflow;keras;deep-learning;google-colaboratory;keras-tuner,2020-12-02 18:17:40,2020-12-02 18:17:40,2022-05-20 10:41:39,this is my first machine learning project  working with a dataset i have created on my own  edit  it is when i run the tuner search that colab crashes  the log   read from the bottom and up  my code any help would be really appreciated 
558,558,15846242,72311894,Feature preprocessing - Large categorial multivalue feature,"<p>I'm barely scratching the surface of Machine learning. I have gotten my hands on some real data from a hospital where I want to predict a score between 1-6 based on various data in the medical chart. Based on research I found, multiple others are suggesting SVM for this task, so that is what i'm going for as well.</p>
<p>One of the features are Diagnosis. This feature contains a delimited list of diagnosis codes. Each patient can have a list between 1 and aprrox. 20 diagnosis codes. Some of them should in theory have a strong impact on the score the patient gets. lets just say it will be in the format DE280,BA234,DG4234 etc.
With 30.000 patients, this could lead to an immense set of features if i would headless try to OneHotEncode it (8759 to be exact). So what would my best option be to fit a Linear SVC model without getting hit by a &quot;curse of dimensionality&quot;?</p>
",22,1,0,4,pandas;dataframe;machine-learning;feature-selection,2022-05-20 04:26:06,2022-05-20 04:26:06,2022-05-20 05:16:31,i m barely scratching the surface of machine learning  i have gotten my hands on some real data from a hospital where i want to predict a score between   based on various data in the medical chart  based on research i found  multiple others are suggesting svm for this task  so that is what i m going for as well 
559,559,14890809,72238283,"Is there a way to list all resources/components (Models, endpoints, TrainingJobs, etc) associated with a AWS SageMaker Notebook/ Studio Project?","<p>If I write a script to train and deploy a machine learning model on AWS SageMaker notebook or create a Project using AWS SageMaker Studio, when I try to list all the resources used, using boto3, the information extracted has no pattern linking the resource information to the notebook/project.</p>
<p><a href=""https://i.stack.imgur.com/y5G1B.png"" rel=""nofollow noreferrer"">boto3.client('sagemaker').list_models()</a></p>
<p>The boto3 api returns all resource information associated with my account and region. Is there a way to link these info (Models, endpoints, TrainingJobs, ProcessingJobs etc) to the Sagemaker Notebook/ Studio Project they were created in?</p>
",72,1,-1,3,amazon-web-services;amazon-ec2;amazon-sagemaker,2022-05-14 13:17:16,2022-05-14 13:17:16,2022-05-20 03:58:10,if i write a script to train and deploy a machine learning model on aws sagemaker notebook or create a project using aws sagemaker studio  when i try to list all the resources used  using boto  the information extracted has no pattern linking the resource information to the notebook project   the boto api returns all resource information associated with my account and region  is there a way to link these info  models  endpoints  trainingjobs  processingjobs etc  to the sagemaker notebook  studio project they were created in 
560,560,19155180,72307891,"From an input vector of parabolic shape values, predict a scalar value using machine learning","<p>I was wondering if you could train a neural network model where from a vector a parabolic shape values you could predict a scalar value.</p>
<p>For example :
let's say the input vector is  [5, 10, 15, 20, 22, 25, 22, 15, 10, 5] then the ouput should be 23.
And to train it I just give the model lots of input vector like the one in the example and the values that should be returned for each one of these vectors.
I looked it up on the internet but didn't find anything that was matching my case but I'm a newbie in this so maybe I just don't understand certain algorithms.</p>
",14,0,0,2,machine-learning;neural-network,2022-05-19 21:45:24,2022-05-19 21:45:24,2022-05-19 21:45:24,i was wondering if you could train a neural network model where from a vector a parabolic shape values you could predict a scalar value 
561,561,19083869,72183136,Saved Machine Learning Model using Pickle won&#39;t predict text values properly,"<p>I currently have a Machine Learning model which would predict what part of speech does a current word belong to</p>
<pre><code>penn_results = penn_crf.predict_single(features)
</code></pre>
<p>and then, I made a code wherein it makes a print making a (word, POS) style print;</p>
<pre><code>penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]
</code></pre>
<p>and when I try to run this, it gives me this output.</p>
<p><strong>[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN')] [('The', 'DET'), ('quick', 'NOUN'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP')]</strong></p>
<p>and so  I saved this model using</p>
<pre><code>penn_filename = 'ptcp.sav'
pickle.dump(penn_crf, open(penn_filename, 'wb'))
</code></pre>
<p>Upon trying to run the model by loading hte saved pickle file with this</p>
<pre><code>test = &quot;The quick brown fox jumps over the head&quot;
pickled_model = pickle.load(open('penn_treebank_crf_postagger.sav', 'rb'))
pickled_model.predict(test)
print(pickled_model.predict(test))
</code></pre>
<p>It prints this
<strong>[['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP'], ['NNP']]</strong></p>
<p>How can I make it print the accurate predicted values like this
<strong>[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN')] [('The', 'DET'), ('quick', 'NOUN'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP')]</strong></p>
",49,2,0,4,python;machine-learning;pickle;text-classification,2022-05-10 14:08:40,2022-05-10 14:08:40,2022-05-19 21:42:49,i currently have a machine learning model which would predict what part of speech does a current word belong to and then  i made a code wherein it makes a print making a  word  pos  style print  and when i try to run this  it gives me this output     the    dt      quick    jj      brown    nn      fox    nn      jumps    nns      over    in       the    det      quick    noun      brown    adj      fox    noun      jumps    noun      over    adp    and so  i saved this model using upon trying to run the model by loading hte saved pickle file with this
562,562,19153602,72305000,Tensorflow keras model.predict() fails,"<p>I am doing my machine learning project in Keras.</p>
<p>I am using a k-fold cross-validation and later I am using the built-in model.predict().</p>
<pre><code>model = tf.keras.Sequential()

model.add(layers.Dense(w, input_dim=w, activation='relu'))
model.add(layers.Dense(lw1, activation='relu'))
#model.add(layers.Dense(round(lw1/2), activation='relu'))
model.add(layers.Dense(1, activation='linear'))

opt = keras.optimizers.Adam(learning_rate=lr)
model.compile(optimizer=opt, loss=&quot;mae&quot;)

print('------------------------------------------------------------------------')
print(f'Training for fold {fn} ...')

history = model.fit(X[train], Y[train], epochs=epoc,batch_size=bs)

scores = model.evaluate(X[test], Y[test], verbose=0)
</code></pre>
<p>The fitting procedure for every fold is ending fine, with mae=0.1. The model.evaluate() method gives as well reasonable error.</p>
<p>The problems start when I am calling the model.predict():</p>
<pre><code>b=X[11]

print(b)

n=model.predict(np.array([b]))

print(n)
</code></pre>
<p>The prediction runs, but the result is the same for different inputs. Thinking that the problem is in poor ANN design, I tried a simple thing - putting in the same values used in fitting - the X[train] set. By logic, it should give the same difference, because the weights were set exactly for it.</p>
<p>This is how it looks for different inputs (v is the generated output):</p>
<pre><code>line in x[test]
[[6.17734280e-02 4.86320246e-03 1.42857143e-01 4.39196341e-02
  2.00000000e-01 5.26184000e+02]]
v=
[[0.17459638]]
line in x[test]
[[1.23547013e-01 1.94528483e-02 1.42857143e-01 8.78393456e-02
  2.00000000e-01 3.68779000e+02]]
v=
[[0.17459638]]
line in x[test]
[[1.23547013e-01 1.94528483e-02 1.42857143e-01 8.78393456e-02
  2.00000000e-01 2.09692000e+02]]
v=
[[0.17459638]]
line in x[test]
[[1.23547013e-01 1.94528483e-02 1.42857143e-01 8.78393456e-02
  2.00000000e-01 9.30220000e+01]]
v=
[[0.17459638]]
line in x[test]
[[0.32942755 0.30746921 0.28571429 0.34921939 0.45       8.055     ]]
v=
[[0.17459638]]
line in x[test]
[[0.32942755 0.30746921 0.28571429 0.34921939 0.45       4.12681   ]]
v=
[[0.17459638]]
line in x[test]
[[ 0.16471299  0.0768674   0.28571429  0.17460985  0.45       65.9551    ]]
v=
[[0.17459638]]
line in x[test]
[[ 0.16471299  0.0768674   0.28571429  0.17460985  0.45       34.052     ]]
v=
[[0.17459638]]
</code></pre>
<p>I strongly believe that the problem is in the model.predict(); possibly the way I give the inputs is wrong. Any other way of calling the model.predict() gives me this error:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 6) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name='dense_27_input'), name='dense_27_input', description=&quot;created by layer 'dense_27_input'&quot;), but it was called on an input with incompatible shape (None,).
</code></pre>
<p>Thank you in advance for your help.</p>
",36,0,0,2,machine-learning;tf.keras,2022-05-19 18:27:21,2022-05-19 18:27:21,2022-05-19 21:20:17,i am doing my machine learning project in keras  i am using a k fold cross validation and later i am using the built in model predict    the fitting procedure for every fold is ending fine  with mae    the model evaluate   method gives as well reasonable error  the problems start when i am calling the model predict    the prediction runs  but the result is the same for different inputs  thinking that the problem is in poor ann design  i tried a simple thing   putting in the same values used in fitting   the x train  set  by logic  it should give the same difference  because the weights were set exactly for it  this is how it looks for different inputs  v is the generated output   i strongly believe that the problem is in the model predict    possibly the way i give the inputs is wrong  any other way of calling the model predict   gives me this error  thank you in advance for your help 
563,563,1714920,17312499,Giving public access to chat server on node.js,"<p>Now I am learning node.js and created a simple chat server using node.js.</p>

<p>My code:</p>

<pre><code>net = require('net');

var sockets = [];

var s = net.Server(function(socket) {
    sockets.push(socket);

    socket.on('data', function(d) {
        for (var i = 0; i &lt; sockets.length; i++) {
            if(sockets[i] == socket) continue;
            sockets[i].write(d);
        }
    });

    socket.on('end', function() {
        var i = sockets.indexOf(socket);
        sockets.splice(i, 1);
    });
});

s.listen(8000);
</code></pre>

<p>How can I share this chat server on the Internet, so other people can use it?</p>

<p>On my local machine, I have an access through telnet: 
<code>telnet localhost 8000</code></p>
",355,2,0,1,node.js,2013-06-26 11:25:13,2013-06-26 11:25:13,2022-05-19 19:31:01,now i am learning node js and created a simple chat server using node js  my code  how can i share this chat server on the internet  so other people can use it 
564,564,16222634,72305954,Make fitted machine learning model predict values in th&#233; future instead of actual values,"<p>I'm having a machine learning model which i fitted with xgboost régressor and linear regression, is there any way to make thé model predict values in thé future
It's a dataset with 13 columns and Price target value , i have date Time as one of thé variables
By searching on internet. I learned about fb prophet, and that this is a Time séries problème.
But if my xgboost is doing well , is there a way to make it predict future values ? Or maybe combiné xgboost and fb prophet ? Can anyone please explain what shroul i do in this case ?</p>
",22,0,0,5,python;machine-learning;scikit-learn;linear-regression;xgboost,2022-05-19 19:29:45,2022-05-19 19:29:45,2022-05-19 19:29:45,
565,565,18577803,72300205,How to upload image file to heroku,"<p>I am currently doing android app for plant recognition using machine learning model which is deployed in Heroku. I capture image using camera implemented in my app and I want to send that image into api deployed in heroku, which will then predict the image and send me back the response in a string format which tells us that to which species it belongs to.</p>
<pre><code>public class MainActivity extends AppCompatActivity {
private final int CAMERA_REQ_CODE=100;
private final int GALLERY_REQ_CODE=1000;
ImageView imgCam;

Bitmap img;
String url = &quot;https://medkit.herokuapp.com/predict&quot;;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);
    imgCam=findViewById(R.id.imgCamera);
    Button btnCamera=findViewById(R.id.btnCamera);
    Button btnGallery=findViewById(R.id.btnGallery);
    Button btnPredict=findViewById(R.id.btnPredict);
    TextView result=findViewById(R.id.resultText);


    btnCamera.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            Intent iCamera=new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            startActivityForResult(iCamera,CAMERA_REQ_CODE);
        }
    });
    btnGallery.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            Intent iGallery=new Intent(Intent.ACTION_PICK);
            iGallery.setData(MediaStore.Images.Media.EXTERNAL_CONTENT_URI);
            startActivityForResult(iGallery,GALLERY_REQ_CODE);

        }
    });
    btnPredict.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            StringRequest stringRequest = new StringRequest(Request.Method.POST, url, new Response.Listener&lt;String&gt;() {
                @Override
                public void onResponse(String response) {
                    try {
                        JSONObject jsonObject = new JSONObject(response);
                        String data = jsonObject.getString(&quot;y_pred_mobile&quot;);
                        if (data.equals(&quot;1&quot;)) {
                            result.setText(&quot;Hibiscus&quot;);
                        } else if (data.equals(&quot;2&quot;)) {
                            result.setText(&quot;Aloevera&quot;);
                        } else if (data.equals(&quot;3&quot;)) {
                            result.setText(&quot;Brahmi&quot;);
                        } else if (data.equals(&quot;4&quot;)) {
                            result.setText(&quot;Neem&quot;);
                        } else {
                            result.setText(&quot;not in the selected species&quot;);
                        }


                    } catch (JSONException e) {
                        e.printStackTrace();
                    }


                }
            }, new Response.ErrorListener() {
                @Override
                public void onErrorResponse(VolleyError error) {
                    Toast.makeText(MainActivity.this, error.getMessage(), Toast.LENGTH_SHORT).show();

                }
            }) {@override
                 protected Map&lt;String,String&gt; getParams(){
                    Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;();
                    params.put(&quot;cgpa&quot;,cgpa.getText().toString());
                    params.put(&quot;iq&quot;,iq.getText().toString());
                    params.put(&quot;profile_score&quot;,profile_score.getText().toString());

                    return params;

                }

            };
            RequestQueue queue = Volley.newRequestQueue(MainActivity.this);
            queue.add(stringRequest);




        }
    });
}
@Override
protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
    super.onActivityResult(requestCode, resultCode, data);
    if(resultCode==RESULT_OK)
    {
        if(requestCode==CAMERA_REQ_CODE)
        {
            //for camera
            Bitmap img=(Bitmap)(data.getExtras().get(&quot;data&quot;));
            imgCam.setImageBitmap(img);

        }
        if(requestCode==GALLERY_REQ_CODE)
        {
            //for Gallery
            imgCam.setImageURI(data.getData());
        }
    }
}
</code></pre>
<p>}
`
above is my app code. Rest all works fine but i need a code that will put image instead of string for example</p>
<pre><code>@Override
                protected Map&lt;String,String&gt; getParams(){
                    Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;();
                    params.put(&quot;cgpa&quot;,cgpa.getText().toString());
                    params.put(&quot;iq&quot;,iq.getText().toString());
                    params.put(&quot;profile_score&quot;,profile_score.getText().toString());

                    return params;
</code></pre>
<p>this code is to send string as a data , I need a code that can send image as data to the api instead of string like in the above code</p>
<p>Can anyone please help me to fix this ?</p>
",44,1,1,5,java;android;heroku;android-volley;mobile-application,2022-05-19 12:47:29,2022-05-19 12:47:29,2022-05-19 17:29:07,i am currently doing android app for plant recognition using machine learning model which is deployed in heroku  i capture image using camera implemented in my app and i want to send that image into api deployed in heroku  which will then predict the image and send me back the response in a string format which tells us that to which species it belongs to  this code is to send string as a data   i need a code that can send image as data to the api instead of string like in the above code can anyone please help me to fix this  
566,566,17305608,72254141,Long waiting when running training model with ML,"<p>I have trouble for long waiting when I run my training model with Machine Learning using CNNs. Maybe this because my pc has such a bad specs for machine learning.
I have 50000 images for my X_training and must wait up to 1 hours more until it's done.
I think maybe that someone can solve my problem. Thanks a lot</p>
",32,1,-1,5,tensorflow;machine-learning;conv-neural-network;tensorboard;tf.keras,2022-05-16 09:57:04,2022-05-16 09:57:04,2022-05-19 15:40:40,
567,567,19101939,72230205,"ValueError when fitting my model. (ValueError: could not broadcast input array from shape (224,224,3) into shape (224,224,3,3))","<p>I am new to machine learning and I am using kaggle's notebook to code. I am making a classification model with multiple categories. I used efficientnet to make my model's architecture but the issue happens with every other model I've tried. The images to be classified are divided in train and val folders in the dataset. In those folders they are in their respective class's folder.</p>
<p>The code runs fine till the fit_generator, it gives me a valueError &quot;ValueError: could not broadcast input array from shape (224,224,3) into shape (224,224,3,3)&quot;</p>
<p>I have attached the full code, the dataset and an image of the error message.</p>
<p>I have no idea what is wrong in the code or the data? Please help me and thank you for reading this question and I apologize if there is any more context missing.</p>
<pre><code>#!pip install -U efficientnet
import pandas as pd
import numpy as np
import efficientnet.tfkeras as efn  # Convolutional Neural Network architecture
import IPython.display as ipd
import librosa.display
import matplotlib.pyplot as plt
from efficientnet.keras import preprocess_input
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from sklearn.utils import class_weight
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os

IM_SIZE = (224, 224, 3)
train=pd.read_csv(&quot;../input/birdclef-2022/train_metadata.csv&quot;)
BIRDS = os.listdir(&quot;../input/mel-split-mark17/mel_spectrogram/train&quot;)
BATCH_SIZE = 16
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.1,
    fill_mode=&quot;nearest&quot;,
)
train_batches = train_datagen.flow_from_directory(
    &quot;../input/mel-split-mark17/mel_spectrogram/train&quot;,
    classes=BIRDS,
    target_size=IM_SIZE,
    class_mode=&quot;categorical&quot;,
    shuffle=True,
    batch_size=BATCH_SIZE,
)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(
    &quot;../input/mel-split-mark17/mel_spectrogram/val&quot;,
    classes=BIRDS,
    target_size=IM_SIZE,
    class_mode=&quot;categorical&quot;,
    shuffle=False,
    batch_size=BATCH_SIZE,
)
# Define CNN's architecture
net = efn.EfficientNetB3(
    include_top=False, weights=&quot;imagenet&quot;, input_tensor=None, input_shape=IM_SIZE
)
x = net.output
x = Flatten()(x)
x = Dropout(0.5)(x)
output_layer = Dense(len(BIRDS), activation=&quot;softmax&quot;, name=&quot;softmax&quot;)(x)
net_final = Model(inputs=net.input, outputs=output_layer)
net_final.compile(
    optimizer=Adam(), loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)

print(net_final.summary())

# Estimate class weights for unbalanced dataset
class_weights = class_weight.compute_class_weight(
    class_weight = &quot;balanced&quot;,
    classes= np.unique(train_batches.classes),
    y=train_batches.classes
)

# Define callbacks
ModelCheck = ModelCheckpoint(
    &quot;models/efficientnet_checkpoint.h5&quot;,
    monitor=&quot;val_loss&quot;,
    verbose=0,
    save_best_only=True,
    save_weights_only=True,
    mode=&quot;auto&quot;,
    period=1,
)

ReduceLR = ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.2, patience=5, min_lr=3e-4)
# Train the model
net_final.fit_generator(
    train_batches,
    validation_data=valid_batches,
    epochs=30,
    steps_per_epoch=1596,
    class_weight=class_weights,
    callbacks=[ModelCheck, ReduceLR],
)
</code></pre>
<p><a href=""https://i.stack.imgur.com/LNjOS.png"" rel=""nofollow noreferrer"">I get this error when I run the code</a></p>
<p><a href=""https://www.kaggle.com/datasets/bluetriad/mel-split-mark17"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/bluetriad/mel-split-mark17</a></p>
",55,1,0,5,python;pandas;tensorflow;machine-learning;keras,2022-05-13 18:45:18,2022-05-13 18:45:18,2022-05-19 15:39:43,i am new to machine learning and i am using kaggle s notebook to code  i am making a classification model with multiple categories  i used efficientnet to make my model s architecture but the issue happens with every other model i ve tried  the images to be classified are divided in train and val folders in the dataset  in those folders they are in their respective class s folder  the code runs fine till the fit_generator  it gives me a valueerror  valueerror  could not broadcast input array from shape      into shape        i have attached the full code  the dataset and an image of the error message  i have no idea what is wrong in the code or the data  please help me and thank you for reading this question and i apologize if there is any more context missing   
568,568,10763076,53680913,TypeError: cannot unpack non-iterable NoneType object,"<p>I know this question has been asked before but I can't seem to get mine to work.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np


def load_dataset():
    def download(filename, source=&quot;http://yaan.lecun.com/exdb/mnist/&quot;):
        print (&quot;Downloading &quot;,filename)
        import urllib
        urllib.urlretrieve(source+filename,filename)

    import gzip
    
    def load_mnist_images(filename):
        if not os.path.exists(filename):
            download(filename)
        with gzip.open(filename,&quot;rb&quot;) as f:
            data=np.frombuffer(f.read(), np.uint8, offset=16)
            
            data = data.reshape(-1,1,28,28)
            
            return data/np.float32(256)

        def load_mnist_labels(filename):
            if not os.path.exists(filename):
                download(filename)
            with gzip.open(filename,&quot;rb&quot;) as f:
                data = np.frombuffer(f.read(), np.uint8, offset=8)
            return data

        X_train = load_mnist_images(&quot;train-images-idx3-ubyte.gz&quot;)
        y_train = load_mnist_labels(&quot;train-labels-idx1-ubyte.gz&quot;)
        X_test = load_mnist_images(&quot;t10k-images-idx3-ubyte.gz&quot;)
        y_test = load_mnist_labels(&quot;t10k-labels-idx1-ubyte.gz&quot;)

        return X_train, y_train, X_test, y_test


X_train, y_train, X_test, y_test = load_dataset()


import matplotlib
matplotlib.use(&quot;TkAgg&quot;)

import matplotlib.pyplot as plt
plt.show(plt.imshow(X_train[3][0]))
</code></pre>
<p>This is the error I am getting:</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File &quot;C:\Users\nehad\Desktop\Neha\Non-School\Python\Handwritten Digits 
Recognition.py&quot;, line 38, in &lt;module&gt;
    X_train, y_train, X_test, y_test = load_dataset()
TypeError: cannot unpack non-iterable NoneType object
</code></pre>
<p>I am new to machine learning. Did I just miss something simple? I am trying a Handwritten Digit Recognition project for my school Science Exhibition.</p>
",174428,5,39,2,python;python-3.x,2018-12-08 14:11:30,2018-12-08 14:11:30,2022-05-19 15:11:19,i know this question has been asked before but i can t seem to get mine to work  this is the error i am getting  i am new to machine learning  did i just miss something simple  i am trying a handwritten digit recognition project for my school science exhibition 
569,569,3734905,72280422,Is TensorFlow the way to go for this optimization problem?,"<p>I have to optimize the result of a process that depends on a large number of variables, i.e. a laser engraving system where the engraving depth depends on the laser speed, distance, power and so on.</p>
<p>The final objective is the minimization of the engraving time, or the maximization of the laser speed. All the other parameters can vary, but must stay within safe bounds.</p>
<p>I have never used any machine learning tools, but to my very limited knowledge this seems like a good use case for TensorFlow or any other machine learning library.</p>
<p>I would experimentally gather data points to train the algorithm, test it and then use a gradient descent optimizer to find the parameters (within bounds) that maximize the laser travel velocity.</p>
<p>Does this sound feasible? How would you approach such a problem? Can you link to any examples available online?</p>
<p>Thank you,<br />
Riccardo</p>
",32,1,-1,3,tensorflow;machine-learning;optimization,2022-05-18 02:07:40,2022-05-18 02:07:40,2022-05-19 15:09:18,i have to optimize the result of a process that depends on a large number of variables  i e  a laser engraving system where the engraving depth depends on the laser speed  distance  power and so on  the final objective is the minimization of the engraving time  or the maximization of the laser speed  all the other parameters can vary  but must stay within safe bounds  i have never used any machine learning tools  but to my very limited knowledge this seems like a good use case for tensorflow or any other machine learning library  i would experimentally gather data points to train the algorithm  test it and then use a gradient descent optimizer to find the parameters  within bounds  that maximize the laser travel velocity  does this sound feasible  how would you approach such a problem  can you link to any examples available online 
570,570,5765649,72280867,Clustering method for three-dimensional vectors,"<p>I have <code>N</code> three-dimensional vectors</p>
<pre><code>(x,y,z)
</code></pre>
<p>I want a simple yet effective approach for  clustering these vectors (I do not know a priori the number of clusters, nor can I guess a valid number). I am not familiar with classical machine learning so any advice would be  helpful.</p>
",36,2,1,3,python;machine-learning;scikit-learn,2022-05-18 02:59:00,2022-05-18 02:59:00,2022-05-19 14:55:19,i have n three dimensional vectors i want a simple yet effective approach for  clustering these vectors  i do not know a priori the number of clusters  nor can i guess a valid number   i am not familiar with classical machine learning so any advice would be  helpful 
571,571,1273879,72297600,Text extraction and text recognition with AI,"<p>Starting from text I'd like to be able to identify specific informations.</p>
<p>Example :</p>
<p>Input texts : &quot;The invoice number is 18&quot;, &quot;Inv : 75&quot;, &quot;Inv N. : 84&quot;</p>
<p>Identified invoice numbers : &quot;18&quot;, &quot;75&quot;, &quot;84&quot;</p>
<p>The concrete problem is I have a lot of documents containing lots of this information and I would like to use an algorithm to identify and extract various type of fields.</p>
<p>I thought that in theory I'll use some kind of framework / algorithm, input all of my documents and train the algorithm by approving or not the results, but I don't know where to start.</p>
<p>I looked into deep learning for unstructured text, machine learning, Stanford NER, Named Entity Recognition as a general concept etc.</p>
<p>I would appreciate some guidance on where to start implementing such a solution.</p>
<p>Thanks</p>
",34,1,0,3,python;machine-learning;deep-learning,2022-05-19 06:45:54,2022-05-19 06:45:54,2022-05-19 14:46:29,starting from text i d like to be able to identify specific informations  example   input texts    the invoice number is     inv       inv n      identified invoice numbers              the concrete problem is i have a lot of documents containing lots of this information and i would like to use an algorithm to identify and extract various type of fields  i thought that in theory i ll use some kind of framework   algorithm  input all of my documents and train the algorithm by approving or not the results  but i don t know where to start  i looked into deep learning for unstructured text  machine learning  stanford ner  named entity recognition as a general concept etc  i would appreciate some guidance on where to start implementing such a solution  thanks
572,572,11267281,72294879,"Azure ML Workspace missing secrets of associated keyvault, no way to access its datastores","<p>by mistake I deleted the secrets from my keyvault, which were associated to my default Azure Machine Learning datastores (workspacefilestore, workspaceartifactstore and workspaceblobstore).
As a result, when I click on one of these datastores (or a dataset depending on them), I get the error &quot;<strong>Please make sure that you are passing valid secret names and that the keyvault https://mauromikv00.v.</strong>&quot;, where mauromikv00 (without &quot;.v.&quot;) is my associated keyvault.
<a href=""https://i.stack.imgur.com/SNnBl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SNnBl.png"" alt=""enter image description here"" /></a>
I tried to recover the deleted secrets, but they aren't in the recycle bin anymore.</p>
<p>If I were able to click on the datastore, I would be able to use the &quot;updated authentication&quot; button to re-associate the blob storage to the keyvault. But the error above happens as soon as I click on the datastore, preventing me from updating its authentication credentials.</p>
",57,1,0,4,azure;azure-machine-learning-studio;azure-machine-learning-service;azuremlsdk,2022-05-19 00:52:49,2022-05-19 00:52:49,2022-05-19 13:38:31,if i were able to click on the datastore  i would be able to use the  updated authentication  button to re associate the blob storage to the keyvault  but the error above happens as soon as i click on the datastore  preventing me from updating its authentication credentials 
573,573,0,72291655,ML Models results in `AttributeError: &#39;OneHotEncoder&#39; object has no attribute &#39;_infrequent_enabled&#39;`,"<p>I am trying to run the <a href=""https://github.com/jonathanreadshaw/ServingMLFastCelery"" rel=""nofollow noreferrer"">ServingMLFastCelery</a>, which is also available and explained on the <a href=""https://towardsdatascience.com/deploying-ml-models-in-production-with-fastapi-and-celery-7063e539a5db"" rel=""nofollow noreferrer"">Towards Data Science website</a>.</p>
<p>The machine learning model is working perfectly, but when I test the complete project the error appears:</p>
<pre><code>[2022-05-18 11:37:45,306: ERROR/MainProcess] Task celery_task_app.tasks.Churn raised unexpected: AttributeError(&quot;'OneHotEncoder' object has no attribute '_infrequent_enabled'&quot;)
Traceback (most recent call last):
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\celery\app\trace.py&quot;, line 405, in trace_task
    R = retval = fun(*args, **kwargs)
  File &quot;C:\Users\diego\codes\ServingMLFastCelery\celery_task_app\tasks.py&quot;, line 30, in __call__
    return self.run(*args, **kwargs)
  File &quot;C:\Users\diego\codes\ServingMLFastCelery\celery_task_app\tasks.py&quot;, line 42, in predict_churn_single
    pred_array = self.model.predict([data])
  File &quot;C:\Users\diego\codes\ServingMLFastCelery\celery_task_app\ml\model.py&quot;, line 27, in predict
    predictions = self.model.predict_proba(df)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\pipeline.py&quot;, line 523, in predict_proba
    Xt = transform.transform(Xt)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\compose\_column_transformer.py&quot;, line 746, in transform
    Xs = self._fit_transform(
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\compose\_column_transformer.py&quot;, line 604, in _fit_transform
    return Parallel(n_jobs=self.n_jobs)(
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\parallel.py&quot;, line 1044, in __call__
    while self.dispatch_one_batch(iterator):
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\parallel.py&quot;, line 859, in dispatch_one_batch
    self._dispatch(tasks)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\parallel.py&quot;, line 777, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\_parallel_backends.py&quot;, line 208, in apply_async
    result = ImmediateResult(func)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\_parallel_backends.py&quot;, line 572, in __init__
    self.results = batch()
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\parallel.py&quot;, line 262, in __call__
    return [func(*args, **kwargs)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\joblib\parallel.py&quot;, line 262, in &lt;listcomp&gt;
    return [func(*args, **kwargs)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\utils\fixes.py&quot;, line 117, in __call__
    return self.function(*args, **kwargs)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\pipeline.py&quot;, line 853, in _transform_one
    res = transformer.transform(X)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 888, in transform
    self._map_infrequent_categories(X_int, X_mask)
  File &quot;c:\users\diego\anaconda3\envs\k38\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 726, in _map_infrequent_categories
    if not self._infrequent_enabled:
AttributeError: 'OneHotEncoder' object has no attribute '_infrequent_enabled'
</code></pre>
<p>The part of the prediction model that uses <code>OneHotEnconder</code> is:</p>
<pre class=""lang-py prettyprint-override""><code>preprocessing_pipeline = ColumnTransformer(transformers=[
    ('num', StandardScaler(), NUMERICAL_FEATURES),
    ('cat', OneHotEncoder(sparse=False), CATEGORICAL_FEATURES)
])

df_new = pd.DataFrame(preprocessing_pipeline.fit_transform(df))
</code></pre>
<p>I tried some solutions available on the internet, but none worked for this case.</p>
",917,0,2,5,python;machine-learning;scikit-learn;celery;fastapi,2022-05-18 20:35:06,2022-05-18 20:35:06,2022-05-19 01:55:42,i am trying to run the   which is also available and explained on the   the machine learning model is working perfectly  but when i test the complete project the error appears  the part of the prediction model that uses onehotenconder is  i tried some solutions available on the internet  but none worked for this case 
574,574,5211657,72279741,How to import modules in Azure Machine Learning run script?,"<p>I am new to Azure Machine Learning and have been struggling with importing modules into my run script. I am using the AzureML SDK for Python. I think I somehow have to append the script location to PYTHONPATH, but have been unable to do so.</p>
<p>To illustrate the problem, assume I have the following project directory:</p>
<pre><code>project/
   src/
      utilities.py
      test.py
   run.py
   requirements.txt
</code></pre>
<p>I want to run test.py on a compute instance on AzureML and I submit the run via run.py.
A simple version of run.py looks as follows:</p>
<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.compute import ComputeInstance
ws = Workspace.get(...) # my credentials here
env = Environment.from_pip_requirements(name='test-env', file_path='requirements.txt')
instance = ComputeInstance(ws, '&lt;instance-name&gt;')
config = ScriptRunConfig(source_directory='./src', script='test.py', environment=env, compute_target=instance)
run = exp.submit(config)
run.wait_for_completion()
</code></pre>
<p>Now, test.py imports functions from utilities.py, e.g.:</p>
<pre><code>from src.utilities import test_func
test_func()
</code></pre>
<p>Then, when I submit a run, I get the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;src/test.py&quot;, line 13, in &lt;module&gt;
    from src.utilities import test_func
ModuleNotFoundError: No module named 'src.utilities'; 'src' is not a package
</code></pre>
<p>This looks like a standard error where the directory is not appended to the Python path. I tried two things to get rid of it:</p>
<ol>
<li>include an <code>__init__.py</code> file in src. This didn't work and I would also for various reasons prefer not to use <code>__init__.py</code> files anyways.</li>
<li>fiddle with the environment_variables passed to AzureML like so
<code>env.environment_variables={'PYTHONPATH': f'./src:${{PYTHONPATH}}'</code> but that didn't really work either and I assume that is simply not the correct way to append the PYTHONPATH</li>
</ol>
<p>I would greatly appreciate any suggestions on extending PYTHONPATH or any other ways to import modules when running a script in AzureML.</p>
",68,1,2,3,python;azureml;azureml-python-sdk,2022-05-18 01:04:11,2022-05-18 01:04:11,2022-05-19 00:09:15,i am new to azure machine learning and have been struggling with importing modules into my run script  i am using the azureml sdk for python  i think i somehow have to append the script location to pythonpath  but have been unable to do so  to illustrate the problem  assume i have the following project directory  now  test py imports functions from utilities py  e g   then  when i submit a run  i get the error  this looks like a standard error where the directory is not appended to the python path  i tried two things to get rid of it  i would greatly appreciate any suggestions on extending pythonpath or any other ways to import modules when running a script in azureml 
575,575,10796077,53966504,Py4JJavaError: An error occurred while calling o37.showString. Spark &amp; anaconda3,"<p>I am a student I am really stuck with this problem of <strong>Py4JJavaError</strong> for two weeks, on the internet there is not much; I really need help:</p>

<p>I follow this tutorial :<a href=""https://docs.microsoft.com/fr-fr/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython"" rel=""nofollow noreferrer""><code>https://docs.microsoft.com/fr-fr/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython</code></a></p>

<p>when I retrieve a line from the RDD in order to be able to observe the data schema like <code>inspections.take(1)</code> or <code>df.show(5)</code> I come across this error</p>

<pre><code>&gt; Py4JJavaError                             Traceback (most recent call
&gt; last) &lt;ipython-input-13-eb589bae8d4b&gt; in &lt;module&gt;()
&gt; ----&gt; 1 df.show(5)
&gt; 
&gt; ~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py in
&gt; show(self, n, truncate, vertical)
&gt;     376         """"""
&gt;     377         if isinstance(truncate, bool) and truncate:
&gt; --&gt; 378             print(self._jdf.showString(n, 20, vertical))
&gt;     379         else:
&gt;     380             print(self._jdf.showString(n, int(truncate), vertical))
&gt; 
&gt; ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in
&gt; __call__(self, *args)    1255         answer = self.gateway_client.send_command(command)    1256         return_value
&gt; = get_return_value(
&gt; -&gt; 1257             answer, self.gateway_client, self.target_id, self.name)    1258     1259         for temp_arg in temp_args:
&gt; 
&gt; ~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py in
&gt; deco(*a, **kw)
&gt;      61     def deco(*a, **kw):
&gt;      62         try:
&gt; ---&gt; 63             return f(*a, **kw)
&gt;      64         except py4j.protocol.Py4JJavaError as e:
&gt;      65             s = e.java_exception.toString()
&gt; 
&gt; ~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py in
&gt; get_return_value(answer, gateway_client, target_id, name)
&gt;     326                 raise Py4JJavaError(
&gt;     327                     ""An error occurred while calling {0}{1}{2}.\n"".
&gt; --&gt; 328                     format(target_id, ""."", name), value)
&gt;     329             else:
&gt;     330                 raise Py4JError(
&gt; 
&gt; Py4JJavaError: An error occurred while calling o37.showString. :
&gt; org.apache.spark.SparkException: Job aborted due to stage failure:
&gt; Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0
&gt; in stage 0.0 (TID 0, localhost, executor driver):
&gt; org.apache.spark.api.python.PythonException: Traceback (most recent
&gt; call last):   File
&gt; ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"",
&gt; line 372, in main
&gt;     process()   File ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"",
&gt; line 367, in process
&gt;     serializer.dump_stream(func(split_index, iterator), outfile)   File
&gt; ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py"",
&gt; line 390, in dump_stream
&gt;     vs = list(itertools.islice(iterator, batch))   File ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py"",
&gt; line 100, in wrapper
&gt;     return f(*args, **kwargs)   File ""&lt;ipython-input-10-9aa45565a8c1&gt;"", line 3, in csvParse
&gt; ModuleNotFoundError: No module named 'StringIO'
&gt; 
&gt;   at
&gt; org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
&gt;   at
&gt; org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
&gt;   at
&gt; org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
&gt;   at
&gt; org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
&gt;   at
&gt; org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
&gt;   at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)   at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown
&gt; Source)   at
&gt; org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&gt;   at
&gt; org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
&gt;   at
&gt; org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
&gt;   at
&gt; org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
&gt;   at
&gt; org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&gt;   at
&gt; org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&gt;   at
&gt; org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&gt;   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
&gt;   at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)     at
&gt; org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&gt;   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
&gt;   at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)     at
&gt; org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at
&gt; org.apache.spark.scheduler.Task.run(Task.scala:121)   at
&gt; org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
&gt;   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
&gt;   at
&gt; org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
&gt;   at
&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   at java.lang.Thread.run(Thread.java:745)
&gt; 
&gt; Driver stacktrace:    at
&gt; org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
&gt;   at
&gt; scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
&gt;   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
&gt;   at scala.Option.foreach(Option.scala:257)   at
&gt; org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
&gt;   at
&gt; org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
&gt;   at
&gt; org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
&gt;   at
&gt; org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
&gt;   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
&gt;   at
&gt; org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
&gt;   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)    at
&gt; org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)     at
&gt; org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)     at
&gt; org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
&gt;   at
&gt; org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
&gt;   at
&gt; org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)
&gt;   at
&gt; org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
&gt;   at
&gt; org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)
&gt;   at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)
&gt;   at
&gt; org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
&gt;   at
&gt; org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
&gt;   at
&gt; org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
&gt;   at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)  at
&gt; org.apache.spark.sql.Dataset.head(Dataset.scala:2545)     at
&gt; org.apache.spark.sql.Dataset.take(Dataset.scala:2759)     at
&gt; org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)   at
&gt; org.apache.spark.sql.Dataset.showString(Dataset.scala:292)    at
&gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at
&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
&gt;   at
&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
&gt;   at java.lang.reflect.Method.invoke(Method.java:483)     at
&gt; py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at
&gt; py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at
&gt; py4j.Gateway.invoke(Gateway.java:282)     at
&gt; py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
&gt;   at py4j.commands.CallCommand.execute(CallCommand.java:79)   at
&gt; py4j.GatewayConnection.run(GatewayConnection.java:238)    at
&gt; java.lang.Thread.run(Thread.java:745) Caused by:
&gt; org.apache.spark.api.python.PythonException: Traceback (most recent
&gt; call last):   File
&gt; ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"",
&gt; line 372, in main
&gt;     process()   File ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"",
&gt; line 367, in process
&gt;     serializer.dump_stream(func(split_index, iterator), outfile)   File
&gt; ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py"",
&gt; line 390, in dump_stream
&gt;     vs = list(itertools.islice(iterator, batch))   File ""/Users/sabbar/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py"",
&gt; line 100, in wrapper
&gt;     return f(*args, **kwargs)   File ""&lt;ipython-input-10-9aa45565a8c1&gt;"", line 3, in csvParse
&gt; ModuleNotFoundError: No module named 'StringIO'
&gt; 
&gt;   at
&gt; org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
&gt;   at
&gt; org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
&gt;   at
&gt; org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
&gt;   at
&gt; org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
&gt;   at
&gt; org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
&gt;   at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)   at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)    at
&gt; org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown
&gt; Source)   at
&gt; org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&gt;   at
&gt; org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
&gt;   at
&gt; org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
&gt;   at
&gt; org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
&gt;   at
&gt; org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&gt;   at
&gt; org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&gt;   at
&gt; org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&gt;   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
&gt;   at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)     at
&gt; org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&gt;   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
&gt;   at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)     at
&gt; org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)    at
&gt; org.apache.spark.scheduler.Task.run(Task.scala:121)   at
&gt; org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
&gt;   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
&gt;   at
&gt; org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
&gt;   at
&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;   at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;   ... 1 more
</code></pre>

<p>here is the code : </p>

<pre><code>from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.sql import Row
from pyspark.sql.functions import UserDefinedFunction
from pyspark.sql.types import *
import pyspark 
#from pyspark import SparkContext
#sc = SparkContext(""local"", ""Simple App"")
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from py4j.protocol import Py4JJavaError

def csvParse(s):
    import csv
    from StringIO import StringIO
    sio = StringIO(s)
    value = csv.reader(sio).next()
    sio.close()
    return value

inspections = sc.textFile('Chicago_Street_Names.csv').map(csvParse)

inspections.take(1)
</code></pre>

<p><strong>Please help me this is project to make next week</strong></p>
",7899,3,3,4,python-3.x;pyspark;anaconda;bigdata,2018-12-29 09:10:45,2018-12-29 09:10:45,2022-05-18 23:04:09,i am a student i am really stuck with this problem of pyjjavaerror for two weeks  on the internet there is not much  i really need help  i follow this tutorial   when i retrieve a line from the rdd in order to be able to observe the data schema like inspections take   or df show   i come across this error here is the code    please help me this is project to make next week
576,576,13914268,72293203,How to choose optimal circuit breaker parameters for microservices?,"<p>I was watching this video by java brains (<a href=""https://www.youtube.com/watch?v=CSqxIKJhFRI&amp;list=PLqq-6Pq4lTTbXZY_elyGv7IkKrfkSrX5e&amp;index=14"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=CSqxIKJhFRI&amp;list=PLqq-6Pq4lTTbXZY_elyGv7IkKrfkSrX5e&amp;index=14</a>). At the timestamp 3:48, he states the parameters for microservice circuit breakers, in his experience, are best chosen by trial and error.</p>
<p>I was wondering if anyone could provide any resources on how to choose optimal circuit breaker parameters (ex. the parameters of hystrix for a spring boot application). Also, is there any room for using some algorithm like machine learning to predict these optimal parameters for you? I would love to know your thoughts on this subject. Thanks!</p>
",30,0,0,5,spring-boot;machine-learning;microservices;hystrix;circuit-breaker,2022-05-18 22:27:51,2022-05-18 22:27:51,2022-05-18 22:27:51,i was watching this video by java brains     at the timestamp    he states the parameters for microservice circuit breakers  in his experience  are best chosen by trial and error  i was wondering if anyone could provide any resources on how to choose optimal circuit breaker parameters  ex  the parameters of hystrix for a spring boot application   also  is there any room for using some algorithm like machine learning to predict these optimal parameters for you  i would love to know your thoughts on this subject  thanks 
577,577,18320654,72291046,How to change function name in the display of a pivot table?,"<p>I am making a machine learning model on the titanic dataset. Click <a href=""https://www.kaggle.com/competitions/titanic/data"" rel=""nofollow noreferrer"">here</a> to access the data.</p>
<p>For starters, I started off by creating a pivot table to understand the differences in Central Tendencies for people who survived the disaster, and for the people who didn't.</p>
<p>I use the code below to get the <code>pivot_table</code>.</p>
<pre><code>pd.pivot_table(
    data = train_df, 
    values = [&quot;Pclass&quot;, &quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;, &quot;Cabin&quot;, &quot;Embarked&quot;], 
    index = &quot;Survived&quot;, 
    aggfunc = [&quot;mean&quot;, &quot;median&quot;, lambda series:series.mode()[0]]
    )
</code></pre>
<p>Output -</p>
<p><a href=""https://i.stack.imgur.com/TJVBN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TJVBN.png"" alt=""enter image description here"" /></a></p>
<p>Because I can't just pass in &quot;mode&quot; in the <code>aggfunc</code> parameter like I can pass &quot;mean&quot;, I made a lambda function to do the job. But because of it &quot;&lt; lambda &gt;&quot; shows up in the pivot table. How can I change it to display something like &quot;mode&quot;?</p>
<p>I can definitely create a function by the name <code>mode</code> but is there some other way?</p>
",33,2,0,4,python;pandas;pandas-groupby;pivot-table,2022-05-18 19:56:54,2022-05-18 19:56:54,2022-05-18 20:30:40,i am making a machine learning model on the titanic dataset  click  to access the data  for starters  i started off by creating a pivot table to understand the differences in central tendencies for people who survived the disaster  and for the people who didn t  i use the code below to get the pivot_table  output    because i can t just pass in  mode  in the aggfunc parameter like i can pass  mean   i made a lambda function to do the job  but because of it   lt  lambda  gt   shows up in the pivot table  how can i change it to display something like  mode   i can definitely create a function by the name mode but is there some other way 
578,578,18159919,72290141,Accuracy not increasing when running multiple LinearRegressions tests,"<p>I made a very simple program, that takes columns of data from a csv file, here is a short preview of the file data:</p>
<pre><code>,matchId,blue_win,blueGold,blueMinionsKilled,blueJungleMinionsKilled,blueAvgLevel,redGold,redMinionsKilled,redJungleMinionsKilled,redAvgLevel,blueChampKills,blueHeraldKills,blueDragonKills,blueTowersDestroyed,redChampKills,redHeraldKills,redDragonKills,redTowersDestroyed
0,3493250918.0,0,24575.0,349.0,89.0,8.6,25856.0,346.0,80.0,9.2,6.0,1.0,0.0,1.0,12.0,2.0,0.0,1.0
1,3464936341.0,0,27210.0,290.0,36.0,9.0,28765.0,294.0,92.0,9.4,20.0,0.0,0.0,0.0,19.0,2.0,0.0,0.0
2,3428425921.0,1,32048.0,346.0,92.0,9.4,25305.0,293.0,84.0,9.4,17.0,3.0,0.0,0.0,11.0,0.0,0.0,4.0
3,3428347390.0,0,20261.0,223.0,60.0,8.2,30429.0,356.0,107.0,9.4,7.0,0.0,0.0,3.0,16.0,3.0,0.0,0.0
4,3428350940.0,1,30217.0,376.0,110.0,9.8,23889.0,334.0,60.0,8.8,16.0,3.0,0.0,0.0,8.0,0.0,0.0,2.0
5,3494458885.0,1,25470.0,362.0,82.0,9.2,22856.0,319.0,86.0,8.8,9.0,1.0,0.0,0.0,7.0,1.0,0.0,0.0
6,3463320642.0,1,25391.0,350.0,96.0,9.2,23236.0,345.0,80.0,8.6,8.0,2.0,0.0,0.0,5.0,1.0,0.0,1.0
...
</code></pre>
<p>I drop the unnecessary columns and run tests with 30% data used as test data to predict the accuracy of blue team winning the game:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import sklearn
from sklearn import linear_model

df = pd.read_csv('MatchTimelinesFirst15.csv', delimiter=',')

predict = &quot;blue_win&quot;

df = df.drop('Unnamed: 0', axis=1)
df = df.drop('redDragonKills', axis=1)
df = df.drop('blueDragonKills', axis=1)
# print(df.describe())

x = np.array(df.drop([predict], axis=1))
y = np.array(df[predict])


for _ in range(500):
    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.30)

    # print('{0}, {1}'.format(type(x_train), x_train))

    linear = linear_model.LinearRegression()

    # trains model
    linear.fit(x_train, y_train)

    acc = linear.score(x_test, y_test)

    print('Accuracy: {0}'.format(acc))
</code></pre>
<p>But my accuracy wont increase even tho training it through a loop 500 times? I keep getting the same range of results:</p>
<pre><code>Accuracy: 0.39030223064480596
Accuracy: 0.3980014684661366
Accuracy: 0.3840247556358104
Accuracy: 0.3939949181269252
Accuracy: 0.38657487661026535
Accuracy: 0.3950506154649621
Accuracy: 0.3925506648304995
...
</code></pre>
<p>Any help will be greatly appreciated, also on improvements since i am very new to python and machine learning.</p>
",34,1,3,5,python;pandas;numpy;machine-learning;linear-regression,2022-05-18 18:59:49,2022-05-18 18:59:49,2022-05-18 19:15:16,i made a very simple program  that takes columns of data from a csv file  here is a short preview of the file data  i drop the unnecessary columns and run tests with   data used as test data to predict the accuracy of blue team winning the game  but my accuracy wont increase even tho training it through a loop  times  i keep getting the same range of results  any help will be greatly appreciated  also on improvements since i am very new to python and machine learning 
579,579,16029031,67902541,How could I save data in mongoDB from watson studio using jupyter notebook?,"<p>I have a machine learning project in IBM Watson Studio which should save data in a mongoDB (as IBM Service). How can I connect from my notebook in IBM Studio to the mongoDB and save data there using python environnement or Spark &amp; python environnement?</p>
",50,1,0,5,python;mongodb;apache-spark;jupyter-notebook;watson-studio,2021-06-09 16:10:53,2021-06-09 16:10:53,2022-05-18 18:19:43,i have a machine learning project in ibm watson studio which should save data in a mongodb  as ibm service   how can i connect from my notebook in ibm studio to the mongodb and save data there using python environnement or spark  amp  python environnement 
580,580,17772652,71719992,What other programming languages besides Dart compile to native machine code across many platforms,"<p>I am learning Dart and was wondering if there are many more programming languages that compile to native machine code across many platforms like IOS, Android, Windows, web etc...</p>
<p>Am I correct in assuming that C, C++ and Go compile directly into native machine code rather than being interpreted?
If those languages count, what about languages like Rust that compile to clang, do they count?</p>
",44,1,1,2,dart;cross-platform,2022-04-02 23:38:24,2022-04-02 23:38:24,2022-05-18 18:01:01,i am learning dart and was wondering if there are many more programming languages that compile to native machine code across many platforms like ios  android  windows  web etc   
581,581,6391766,72279327,Ideas for model selection for predicting sales at locations based on time component and class column,"<p>I am trying to build a model for sales prediction out of three different storages based on previous sales. However there is an extra (and very important) component to this which is a column with the values of A and B. These letters indicate a price category, where A siginifies a comparetively cheaper price compared to similar products. Here is a mock example of the table</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>week</th>
<th>Letter</th>
<th>Storage1 sales</th>
<th>Storage2 sales</th>
<th>Storage3 sales</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A</td>
<td>50</td>
<td>28</td>
<td>34</td>
</tr>
<tr>
<td>2</td>
<td>A</td>
<td>47</td>
<td>29</td>
<td>19</td>
</tr>
<tr>
<td>3</td>
<td>B</td>
<td>13</td>
<td>11</td>
<td>19</td>
</tr>
<tr>
<td>4</td>
<td>B</td>
<td>14</td>
<td>19</td>
<td>8</td>
</tr>
<tr>
<td>5</td>
<td>B</td>
<td>21</td>
<td>13</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>A</td>
<td>39</td>
<td>25</td>
<td>23</td>
</tr>
</tbody>
</table>
</div>
<p>I have previously worked with both types of prediction problems seperately, namely time series analysis and regression problems, using classical methods and using machine learning but I have not built a model which can take both predicition types into account.</p>
<p>I am writing this to hear any suggestions as how to tackle such a prediction problem. I am thinking of converting the three storage sale columns into one, in order to have one feature column, and having three one-hot encoder columns to indicate the storage. However I am not sure how to tackle this problem with a machine learning approach and would like to hear if anyone knows where to start with such a prediction problem.</p>
",17,0,0,5,machine-learning;time-series;regression;prediction;forecasting,2022-05-18 00:26:34,2022-05-18 00:26:34,2022-05-18 17:10:23,i am trying to build a model for sales prediction out of three different storages based on previous sales  however there is an extra  and very important  component to this which is a column with the values of a and b  these letters indicate a price category  where a siginifies a comparetively cheaper price compared to similar products  here is a mock example of the table i have previously worked with both types of prediction problems seperately  namely time series analysis and regression problems  using classical methods and using machine learning but i have not built a model which can take both predicition types into account  i am writing this to hear any suggestions as how to tackle such a prediction problem  i am thinking of converting the three storage sale columns into one  in order to have one feature column  and having three one hot encoder columns to indicate the storage  however i am not sure how to tackle this problem with a machine learning approach and would like to hear if anyone knows where to start with such a prediction problem 
582,582,19099750,72287351,Speech Recognition on multiple language in a single audio file as an input,"<p>i'am completely new to machine learning, i am currently working on speech recognition. Is there anyway that i could recognize multiple language e.g( english, chinese, spanish) in a single audio file as an input?</p>
<p>Example:</p>
<p>3 nationality talking in a single audio file.</p>
<pre><code>Language predicted: english, spanish, russian.

</code></pre>
<p>If there is any resources or repositories you know please do share i would really be grateful 😊</p>
",57,0,0,5,python;tensorflow;machine-learning;deep-learning;pytorch,2022-05-18 15:51:56,2022-05-18 15:51:56,2022-05-18 15:51:56,i am completely new to machine learning  i am currently working on speech recognition  is there anyway that i could recognize multiple language e g  english  chinese  spanish  in a single audio file as an input  example   nationality talking in a single audio file  if there is any resources or repositories you know please do share i would really be grateful  
583,583,13921845,62903214,"Why is my binary classification model not learning, even to overfit?","<p>I have the following model, using tensorflow 2.2.0 with keras:</p>
<pre><code>def get_model(input_shape):
  model = keras.Sequential()
  
  model.add(Conv2D(32, input_shape=input_shape, kernel_size=(3, 3)))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(64, kernel_size=(3, 3)))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Conv2D(64, kernel_size=(3, 3)))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Flatten())

  model.add(Dense(64, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))

  return model
</code></pre>
<p>The shape of the input is <code>(25, 25, 4)</code> - a 3-dimensional image, 25x25px, with 4 channels. The model does not learn - it won't even overfit! I am trying to fit using the following incantation:</p>
<pre><code>model.compile(optimizer='sgd', metrics=['accuracy'], loss='binary_crossentropy')
model.fit(trainX, trainY, validation_split=0.2, epochs=10, batch_size=50)
</code></pre>
<p>I have also tried changing the optimizer to be <code>sgd</code> with the same results and have tried varying batch sizes (including 1). An example of training for 10 epochs:</p>
<pre><code>Epoch 1/10
763/763 [==============================] - 4s 5ms/step - loss: 0.6935 - accuracy: 0.5045 - val_loss: 0.6937 - val_accuracy: 0.5031
Epoch 2/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6935 - accuracy: 0.5020 - val_loss: 0.6946 - val_accuracy: 0.4972
Epoch 3/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6935 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4984
Epoch 4/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6934 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.4986
Epoch 5/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6933 - accuracy: 0.5027 - val_loss: 0.6934 - val_accuracy: 0.4972
Epoch 6/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6932 - accuracy: 0.5051 - val_loss: 0.6946 - val_accuracy: 0.5019
Epoch 7/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.4959
Epoch 8/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6934 - val_accuracy: 0.5056
Epoch 9/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5009
Epoch 10/10
763/763 [==============================] - 3s 4ms/step - loss: 0.6933 - accuracy: 0.5018 - val_loss: 0.6931 - val_accuracy: 0.5020
&lt;tensorflow.python.keras.callbacks.History at 0x7f761a0856d8&gt;
</code></pre>
<p>For what it's worth, the data is almost certainly not the problem - I have tried other machine learning methods such as random forests and gradient boosting and they are able to overfit just fine.</p>
<p>Am I missing something fundamental here?</p>
<p>Edit: setting the activation of conv layers to <code>relu</code> does not help. The below output is with <code>relu</code>:</p>
<pre><code>
Epoch 1/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6936 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 2/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6933 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5043
Epoch 3/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4971
Epoch 4/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 5/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5029
Epoch 6/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 7/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 8/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 9/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 10/10
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5029
&lt;tensorflow.python.keras.callbacks.History at 0x7f29766804a8&gt;
</code></pre>
<p>I also tried changing the labels to be categorical and using <code>categorical_crossentropy</code>, to no avail.</p>
<p>Edit 2: The same behaviour persists over more epochs, with activation correctly set.</p>
<p>Model:</p>
<pre><code>def get_model(input_shape):
  model = keras.Sequential()
  
  model.add(Conv2D(32, input_shape=input_shape, kernel_size=(3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Flatten())

  model.add(Dense(64, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))

  return model
</code></pre>
<p>Output:</p>
<pre><code>Epoch 1/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6937 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.5008
...
Epoch 243/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 244/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 245/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 246/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 247/250
1907/1907 [==============================] - 7s 4ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5029
Epoch 248/250
1907/1907 [==============================] - 7s 4ms/step - loss: 0.6932 - accuracy: 0.5026 - val_loss: 0.6932 - val_accuracy: 0.4971
Epoch 249/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5018 - val_loss: 0.6932 - val_accuracy: 0.4971
Epoch 250/250
1907/1907 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5029
</code></pre>
<p>Data sample:</p>
<pre><code>display(trainX[0])
display(trainX[0].shape)
---
array([[[-0.81307793, -0.80876915, -0.80270227, -0.81340067],
        [-0.81323822, -0.80901267, -0.80424022, -0.81004681],
        [-0.80974839, -0.80952621, -0.80894936, -0.81924987],
        [-0.81901061, -0.81892894, -0.8198063 , -0.82950191],
        [-0.82926863, -0.82535357, -0.81962295, -0.82940024],
        [-0.82911602, -0.82669005, -0.81815252, -0.82725751],
        [-0.82717653, -0.82594539, -0.81691338, -0.82605227],
        [-0.82584266, -0.82452835, -0.81556359, -0.82556375],
        [-0.82525089, -0.82266387, -0.8177839 , -0.82243512],
        [-0.82222369, -0.82112803, -0.82649334, -0.83150323]],

       [[-0.81323822, -0.80901267, -0.80424022, -0.81004681],
        [-0.81339844, -0.80925606, -0.80577279, -0.80666623],
        [-0.80990994, -0.8097693 , -0.81046532, -0.81594339],
        [-0.81916858, -0.81916656, -0.82128286, -0.82628101],
        [-0.8294225 , -0.82558735, -0.82110019, -0.82617847],
        [-0.82926995, -0.82692302, -0.81963519, -0.82401759],
        [-0.82733125, -0.82617881, -0.8184006 , -0.82280219],
        [-0.82599791, -0.82476263, -0.81705573, -0.82230957],
        [-0.82540639, -0.82289927, -0.81926792, -0.81915487],
        [-0.8223804 , -0.82136435, -0.82794484, -0.82829943]],

       [[-0.80974839, -0.80952621, -0.80894936, -0.81924987],
        [-0.80990994, -0.8097693 , -0.81046532, -0.81594339],
        [-0.80639256, -0.81028192, -0.81510641, -0.82501505],
        [-0.81572868, -0.81966765, -0.82580199, -0.83511534],
        [-0.82607158, -0.82608032, -0.82562142, -0.8350152 ],
        [-0.82591768, -0.82741428, -0.8241732 , -0.83290467],
        [-0.8239619 , -0.82667103, -0.82295269, -0.83171742],
        [-0.82261689, -0.82525665, -0.82162309, -0.83123616],
        [-0.82202021, -0.82339567, -0.82381013, -0.82815378],
        [-0.818968  , -0.82186268, -0.83238638, -0.83708633]],

       [[-0.81901061, -0.81892894, -0.8198063 , -0.82950191],
        [-0.81916858, -0.81916656, -0.82128286, -0.82628101],
        [-0.81572868, -0.81966765, -0.82580199, -0.83511534],
        [-0.82485699, -0.82883834, -0.8362085 , -0.84494163],
        [-0.83496124, -0.83509975, -0.83603291, -0.84484426],
        [-0.83481096, -0.83640177, -0.83462448, -0.84279185],
        [-0.83290099, -0.83567633, -0.83343734, -0.84163708],
        [-0.83158729, -0.83429571, -0.83214391, -0.84116895],
        [-0.83100444, -0.83247886, -0.83427135, -0.83817006],
        [-0.82802254, -0.830982  , -0.84260906, -0.84685789]],

       [[-0.82926863, -0.82535357, -0.81962295, -0.82940024],
        [-0.8294225 , -0.82558735, -0.82110019, -0.82617847],
        [-0.82607158, -0.82608032, -0.82562142, -0.8350152 ],
        [-0.83496124, -0.83509975, -0.83603291, -0.84484426],
        [-0.84479157, -0.84125479, -0.83585723, -0.84474687],
        [-0.84464544, -0.84253437, -0.83444812, -0.84269387],
        [-0.84278804, -0.84182145, -0.8332604 , -0.84153877],
        [-0.84151027, -0.84046456, -0.83196635, -0.84107051],
        [-0.84094331, -0.83867874, -0.83409482, -0.83807077],
        [-0.83804212, -0.83720728, -0.84243663, -0.84676108]],

       [[-0.82911602, -0.82669005, -0.81815252, -0.82725751],
        [-0.82926995, -0.82692302, -0.81963519, -0.82401759],
        [-0.82591768, -0.82741428, -0.8241732 , -0.83290467],
        [-0.83481096, -0.83640177, -0.83462448, -0.84279185],
        [-0.84464544, -0.84253437, -0.83444812, -0.84269387],
        [-0.84449925, -0.84380921, -0.83303354, -0.84062854],
        [-0.84264105, -0.84309893, -0.83184123, -0.83946655],
        [-0.84136274, -0.84174705, -0.8305422 , -0.83899551],
        [-0.84079554, -0.83996778, -0.83267886, -0.83597806],
        [-0.83789312, -0.83850169, -0.84105351, -0.84472027]],

       [[-0.82717653, -0.82594539, -0.81691338, -0.82605227],
        [-0.82733125, -0.82617881, -0.8184006 , -0.82280219],
        [-0.8239619 , -0.82667103, -0.82295269, -0.83171742],
        [-0.83290099, -0.83567633, -0.83343734, -0.84163708],
        [-0.84278804, -0.84182145, -0.8332604 , -0.84153877],
        [-0.84264105, -0.84309893, -0.83184123, -0.83946655],
        [-0.84077276, -0.84238718, -0.83064506, -0.83830071],
        [-0.83948755, -0.84103251, -0.82934186, -0.83782811],
        [-0.8389173 , -0.83924958, -0.83148541, -0.83480076],
        [-0.8359994 , -0.8377805 , -0.83988758, -0.84357199]],

       [[-0.82584266, -0.82452835, -0.81556359, -0.82556375],
        [-0.82599791, -0.82476263, -0.81705573, -0.82230957],
        [-0.82261689, -0.82525665, -0.82162309, -0.83123616],
        [-0.83158729, -0.83429571, -0.83214391, -0.84116895],
        [-0.84151027, -0.84046456, -0.83196635, -0.84107051],
        [-0.84136274, -0.84174705, -0.8305422 , -0.83899551],
        [-0.83948755, -0.84103251, -0.82934186, -0.83782811],
        [-0.83819763, -0.83967254, -0.82803413, -0.83735488],
        [-0.8376253 , -0.83788269, -0.83018513, -0.83432354],
        [-0.83469681, -0.83640794, -0.83861716, -0.84310649]],

       [[-0.82525089, -0.82266387, -0.8177839 , -0.82243512],
        [-0.82540639, -0.82289927, -0.81926792, -0.81915487],
        [-0.82202021, -0.82339567, -0.82381013, -0.82815378],
        [-0.83100444, -0.83247886, -0.83427135, -0.83817006],
        [-0.84094331, -0.83867874, -0.83409482, -0.83807077],
        [-0.84079554, -0.83996778, -0.83267886, -0.83597806],
        [-0.8389173 , -0.83924958, -0.83148541, -0.83480076],
        [-0.8376253 , -0.83788269, -0.83018513, -0.83432354],
        [-0.83705204, -0.83608379, -0.83232385, -0.83126675],
        [-0.83411887, -0.83460162, -0.8407067 , -0.84012427]],

       [[-0.82222369, -0.82112803, -0.82649334, -0.83150323],
        [-0.8223804 , -0.82136435, -0.82794484, -0.82829943],
        [-0.818968  , -0.82186268, -0.83238638, -0.83708633],
        [-0.82802254, -0.830982  , -0.84260906, -0.84685789],
        [-0.83804212, -0.83720728, -0.84243663, -0.84676108],
        [-0.83789312, -0.83850169, -0.84105351, -0.84472027],
        [-0.8359994 , -0.8377805 , -0.83988758, -0.84357199],
        [-0.83469681, -0.83640794, -0.83861716, -0.84310649],
        [-0.83411887, -0.83460162, -0.8407067 , -0.84012427],
        [-0.83116192, -0.83311339, -0.84889275, -0.84876322]]])
(10, 10, 4)

display(trainY[0:5])
display(trainY.shape)
---
array([0, 1, 0, 1, 0], dtype=int64)
(47666,)

</code></pre>
",431,2,0,5,python;tensorflow;machine-learning;keras;deep-learning,2020-07-15 01:36:35,2020-07-15 01:36:35,2022-05-18 13:41:53,i have the following model  using tensorflow    with keras  the shape of the input is          a  dimensional image  xpx  with  channels  the model does not learn   it won t even overfit  i am trying to fit using the following incantation  i have also tried changing the optimizer to be sgd with the same results and have tried varying batch sizes  including    an example of training for  epochs  for what it s worth  the data is almost certainly not the problem   i have tried other machine learning methods such as random forests and gradient boosting and they are able to overfit just fine  am i missing something fundamental here  edit  setting the activation of conv layers to relu does not help  the below output is with relu  i also tried changing the labels to be categorical and using categorical_crossentropy  to no avail  edit   the same behaviour persists over more epochs  with activation correctly set  model  output  data sample 
584,584,4263759,72285325,Is Selenium a security vulnerability?,"<p>I'm learning Selenium but I am concerned that the Webdriver (Chromedriver.exe, Geckodriver.exe) seems to open a port for communication functioning as a proxy between the selenium code and the browser. I don't want to expose myself to any vulnerabilities and would prefer some form of direct browser communication.  I've viewed several solutions, but selenium just seems to be the reigning champion.</p>
<p>It appears that some form of what I am looking for was present with PhantomJS, but that seems to be no longer supported.  This leaves me two options:</p>
<p>-Find a browser that selenium can communicate directly with without the aid of Webdriver.</p>
<p>-Find a way to completely restrict that port that it seems to open up so it does not allow connections from anybody on the network and no one can connect to my local machine.</p>
<p>Any suggestions folks?</p>
",54,0,0,3,selenium;security;automation,2022-05-18 13:32:15,2022-05-18 13:32:15,2022-05-18 13:32:15,i m learning selenium but i am concerned that the webdriver  chromedriver exe  geckodriver exe  seems to open a port for communication functioning as a proxy between the selenium code and the browser  i don t want to expose myself to any vulnerabilities and would prefer some form of direct browser communication   i ve viewed several solutions  but selenium just seems to be the reigning champion  it appears that some form of what i am looking for was present with phantomjs  but that seems to be no longer supported   this leaves me two options   find a browser that selenium can communicate directly with without the aid of webdriver   find a way to completely restrict that port that it seems to open up so it does not allow connections from anybody on the network and no one can connect to my local machine  any suggestions folks 
585,585,0,63952338,how to save best weights and best model using keras,"<p>Experts, I'm new to the machine learning and using Keras API with TensorFlow back-end to train a machine learning model. I'm using Model-checkpoint to save best weights and best model in .json and .h5 file independently.in so far I tried to write a code as below but i am not getting any model or weights saved. Hope i will get good solution.Thanks in advance.</p>
<pre><code>    filepath1=&quot;best_weights.h5&quot;
    filepath2=&quot;best_model.json&quot;
    checkpoint = ModelCheckpoint(filepath1, monitor='val_acc', verbose=1, save_best_only=True,  mode='max')
    callbacks_list = [checkpoint]
    history = model.fit_generator(train_generator,steps_per_epoch=nb_train_samples // batch_size, epochs=epochs, validation_data=validation_generator, callbacks=callbacks_list, validation_steps=nb_validation_samples // batch_size, verbose=1)
    
</code></pre>
",1346,1,2,5,python;tensorflow;keras;deep-learning;autoencoder,2020-09-18 14:15:35,2020-09-18 14:15:35,2022-05-18 13:15:37,experts  i m new to the machine learning and using keras api with tensorflow back end to train a machine learning model  i m using model checkpoint to save best weights and best model in  json and  h file independently in so far i tried to write a code as below but i am not getting any model or weights saved  hope i will get good solution thanks in advance 
586,586,9728121,71378280,Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo,"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.</p>
<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull/update&quot;) on my local project of repository A.</p>
<p>The problem comes when I use colab to run my project. So what I did was the following:</p>
<ol>
<li>Created a new notebook on colab</li>
<li>Successfully git-cloned my machine learning project (repository A)</li>
<li>Ran &quot;!pip install dvc&quot;</li>
<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)</li>
</ol>
<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)</p>
<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '/content/&lt;my_project_A&gt;/.dvc/config.local' to gitignore file.
2022-03-08 08:53:31,866 DEBUG: Adding '/content/&lt;my_project_A&gt;/.dvc/tmp' to gitignore file.
2022-03-08 08:53:31,866 DEBUG: Adding '/content/&lt;my_project_A&gt;/.dvc/cache' to gitignore file.
2022-03-08 08:53:31,916 DEBUG: Creating external repo https://gitlab.com/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75
2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https://gitlab.com/&lt;my-dataset-repo-B&gt;.git' to a temporary dir
Everything is up to date.
2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https://gitlab.com/&lt;my-dataset-repo-B&gt;.git' to '/tmp/tmp2x7y7xgedvc-clone'
------------------------------------------------------------
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/scmrepo/git/backend/gitpython.py&quot;, line 185, in clone
    tmp_repo = clone_from()
  File &quot;/usr/local/lib/python3.7/dist-packages/git/repo/base.py&quot;, line 1148, in clone_from
    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/git/repo/base.py&quot;, line 1079, in _clone
    finalize_process, decode_streams=False)
  File &quot;/usr/local/lib/python3.7/dist-packages/git/cmd.py&quot;, line 176, in handle_process_output
    return finalizer(process)
  File &quot;/usr/local/lib/python3.7/dist-packages/git/util.py&quot;, line 386, in finalize_process
    proc.wait(**kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/git/cmd.py&quot;, line 502, in wait
    raise GitCommandError(remove_password_if_present(self.args), status, errstr)
git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)
  cmdline: git clone -v --no-single-branch --progress https://gitlab.com/&lt;my-dataset-repo-B&gt;.git /tmp/tmp2x7y7xgedvc-clone

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/scm.py&quot;, line 104, in clone
    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/scmrepo/git/__init__.py&quot;, line 121, in clone
    backend.clone(url, to_path, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/scmrepo/git/backend/gitpython.py&quot;, line 190, in clone
    raise CloneError(url, to_path) from exc
scmrepo.exceptions.CloneError: Failed to clone repo 'https://gitlab.com/&lt;my-dataset-repo-B&gt;.git' to '/tmp/tmp2x7y7xgedvc-clone'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/command/data_sync.py&quot;, line 41, in run
    glob=self.args.glob,
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/__init__.py&quot;, line 49, in wrapper
    return f(repo, *args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/pull.py&quot;, line 38, in pull
    run_cache=run_cache,
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/__init__.py&quot;, line 49, in wrapper
    return f(repo, *args, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/fetch.py&quot;, line 50, in fetch
    revs=revs,
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/__init__.py&quot;, line 437, in used_objs
    with_deps=with_deps,
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/repo/index.py&quot;, line 190, in used_objs
    filter_info=filter_info,
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/stage/__init__.py&quot;, line 660, in get_used_objs
    for odb, objs in out.get_used_objs(*args, **kwargs).items():
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/output.py&quot;, line 918, in get_used_objs
    return self.get_used_external(**kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/output.py&quot;, line 973, in get_used_external
    return dep.get_used_objs(**kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/dependency/repo.py&quot;, line 94, in get_used_objs
    used, _ = self._get_used_and_obj(**kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/dependency/repo.py&quot;, line 108, in _get_used_and_obj
    locked=locked, cache_dir=local_odb.cache_dir
  File &quot;/usr/lib/python3.7/contextlib.py&quot;, line 112, in __enter__
    return next(self.gen)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/external_repo.py&quot;, line 35, in external_repo
    path = _cached_clone(url, rev, for_write=for_write)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/external_repo.py&quot;, line 155, in _cached_clone
    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)
  File &quot;/usr/local/lib/python3.7/dist-packages/funcy/decorators.py&quot;, line 45, in wrapper
    return deco(call, *dargs, **dkwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/funcy/flow.py&quot;, line 274, in wrap_with
    return call()
  File &quot;/usr/local/lib/python3.7/dist-packages/funcy/decorators.py&quot;, line 66, in __call__
    return self._func(*self._args, **self._kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/external_repo.py&quot;, line 220, in _clone_default_branch
    git = clone(url, clone_path)
  File &quot;/usr/local/lib/python3.7/dist-packages/dvc/scm.py&quot;, line 106, in clone
    raise CloneError(str(exc))
dvc.scm.CloneError: Failed to clone repo 'https://gitlab.com/&lt;my-dataset-repo-B&gt;.git' to '/tmp/tmp2x7y7xgedvc-clone'
------------------------------------------------------------
2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.
2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '/tmp/tmp4x5js0dk']'
2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '/tmp/tmp4x5js0dk']'
</code></pre>
<p>And btw this is how I cloned my git repository (repo A)</p>
<pre><code>!git config - global user.name &quot;Zharfan&quot;
!git config - global user.email &quot;zharfan@myemail.com&quot;
!git clone https://&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git
</code></pre>
<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!</p>
",431,1,1,4,git;dataset;google-colaboratory;dvc,2022-03-07 14:02:28,2022-03-07 14:02:28,2022-05-18 12:22:58,i m having a problem trying to run  dvc pull  on google colab  i have two repositories  let s call them a and b  where repository a is for my machine learning codes and repository b is for my dataset  i ve successfully pushed my dataset to repository b with dvc  using gdrive as my remote storage  and i also managed to successfully run  dvc import   as well as  dvc pull update   on my local project of repository a  the problem comes when i use colab to run my project  so what i did was the following  on step   i got the error  this is the full stack trace  note that i changed the repo url in the stack trace for confidentiality reasons  and btw this is how i cloned my git repository  repo a  does anyone know why  any help would be greatly appreciated  thank you in advance 
587,587,13985387,72282927,Model val_accuracy higher than test accuracy without dropout regularization,"<p>I recently created a machine learning of 810 training and 810 test images (27 classes) in order to identify ASL hand signs. I trained this model using an SGD optimizer with a 0.001 learning rate, 5 epochs, and categorical cross entropy loss. However, my validation accuracy is around 20% higher than my model test accuracy, and I'm not sure why. I've tried adjusting my model structure, optimizers, learning rate, and epochs - this never changes.
Anyone have any ideas? Here's my model code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
model = tf.keras.models.Sequential([
    
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    
    tf.keras.layers.Dense(27, activation='softmax')
])

model.summary()
from tensorflow.keras.optimizers import SGD
sgd = SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)

model.compile(loss = 'categorical_crossentropy',
              optimizer = sgd,
              metrics = ['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy')&gt;0.95):
            print(&quot;\nReached &gt;95% accuracy so cancelling training!&quot;)
            self.model.stop_training = True
        
callbacks = myCallback()
train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=40,
      width_shift_range=0.2, # Shifting image width by 20%
      height_shift_range=0.2,# Shifting image height by 20%
      shear_range=0.2,       # Shearing across X-axis by 20%
      zoom_range=0.2,        # Image zooming by 20%
      horizontal_flip=True,
      fill_mode='nearest')

train_generator = train_datagen.flow_from_directory(
    &quot;/content/drive/MyDrive/train_asl&quot;,
    target_size = (150, 150),
    class_mode = 'categorical',
    batch_size = 5)
validation_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = validation_datagen.flow_from_directory(
    &quot;/content/drive/MyDrive/test_asl&quot;,
    target_size = (150, 150),
    class_mode = 'categorical',
    batch_size = 5
)
import numpy as np
history = model.fit_generator(
      train_generator,
      steps_per_epoch = np.ceil(810/5),  # 2520 images = batch_size * steps
      epochs = 100,
      validation_data=validation_generator,
      validation_steps = np.ceil(810/5),  # 372 images = batch_size * steps
      callbacks=[callbacks],
      verbose = 2)
</code></pre>
",24,1,-1,5,python;tensorflow;machine-learning;keras;deep-learning,2022-05-18 09:06:15,2022-05-18 09:06:15,2022-05-18 09:39:36,
588,588,19139313,72279452,i want to integrate an Machine Learning system in my web app using Django Back-end from Nyckel Machine Learning Service?,"<p>i send the request to Nyckel Service and get the response back as text but the response i get is invalid format
i am working on implement image classification :</p>
<p>the result i need to get is this depends on the image i send it through the POST request for Nyckel Service :</p>
<p>{</p>
<p>&quot;labelName&quot;: &quot;harissa&quot;,</p>
<p>&quot;labelId&quot;: &quot;label_684pbumtvbzp3k9q&quot;,</p>
<p>&quot;confidence&quot;: 0.76</p>
<p>}</p>
<p>@api_view(['POST','GET'])</p>
<p>def addProductByNyckel(request):</p>
<pre><code>data = request.data


image = data['image']
url = 'https://www.nyckel.com/v1/functions/7aaigszss2ejx7t8/invoke/?format=json'
header = {
'Content-type': 'application/json', 'Accept': 'text/plain'
}

urlimg = 'http://127.0.0.1:8000/media/images/chamia_2lJVXBC.jpg'


img = requests.get(urlimg,params=request.GET)
m = img.content

result = requests.post(url,m, headers=header)
dict = result.json() 

labelName= dict.get(&quot;labelName&quot;),
labelId = dict.get(&quot;labelId&quot;),
confidence = dict.get(&quot;confidence&quot;)

return Response(dict )
`
</code></pre>
<p>the error message is :</p>
<p>**{</p>
<pre><code>&quot;type&quot;: &quot;https://tools.ietf.org/html/rfc7231#section-6.5.1&quot;,


&quot;title&quot;: &quot;One or more validation errors occurred.&quot;,


&quot;status&quot;: 400,


&quot;traceId&quot;: &quot;00-983824a0cb2b204855f4387cf92f2dca-780a95630f9dc3d6-00&quot;,


&quot;errors&quot;: {

    &quot;$&quot;: [

        &quot;'0x89' is an invalid start of a value. Path: $ | LineNumber: 0 | BytePositionInLine: 0.&quot;

    ],


    &quot;input&quot;: [

        &quot;The input field is required.&quot;

    ]

}
</code></pre>
<p>}**</p>
",21,1,0,5,django;machine-learning;post;postman;response,2022-05-18 00:37:28,2022-05-18 00:37:28,2022-05-18 06:02:22,the result i need to get is this depends on the image i send it through the post request for nyckel service      labelname    harissa    labelid    label_pbumtvbzpkq    confidence        api_view   post   get    def addproductbynyckel request   the error message is          
589,589,13852756,72276134,Improving performance result of classification for severely imbalance data having abnormal skewed distribution,"<p>I have a large dataset D  which I balanced using under sampling method called RandomUnderSampler from imblearn package which reduce the class data with majority. The data have three classes: Yes (1), No (0), Unfinished (2).</p>
<p>This is the minimal 3d projection of the dataset after perform PCA on the balanced version of the dataset:<a href=""https://i.stack.imgur.com/VXfJs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXfJs.png"" alt=""enter image description here"" /></a></p>
<p>I tried RandomForestClassifier,BaggingClassifier, KNN and MLP Classifier etc with some hyperparameter tuning even LocalOutlierFactor via sklearn package but I cannot get desired precision even if I am using balanced data.</p>
<p>I am sharing the HTML version of the code I have so far in the following link : <a href=""https://imbalance-data-classification.netlify.app/"" rel=""nofollow noreferrer"">https://imbalance-data-classification.netlify.app/</a></p>
<p>All I see from google search is to balance and try different classifier while tweaking the hyperparameter which is not working. I atleast need precision of 80% rather than existing 44% I got in some instance of MLPClassifier.</p>
<p>What can be done given this situation to improve the precision for predicting data which should be labelled as Yes or No.  My knowledge related to Machine and Deep Learning is limited.</p>
",43,1,0,5,python;machine-learning;deep-learning;classification;imbalanced-data,2022-05-17 20:17:12,2022-05-17 20:17:12,2022-05-18 01:32:49,i have a large dataset d  which i balanced using under sampling method called randomundersampler from imblearn package which reduce the class data with majority  the data have three classes  yes     no     unfinished     this is the minimal d projection of the dataset after perform pca on the balanced version of the dataset  i tried randomforestclassifier baggingclassifier  knn and mlp classifier etc with some hyperparameter tuning even localoutlierfactor via sklearn package but i cannot get desired precision even if i am using balanced data  i am sharing the html version of the code i have so far in the following link    all i see from google search is to balance and try different classifier while tweaking the hyperparameter which is not working  i atleast need precision of   rather than existing   i got in some instance of mlpclassifier  what can be done given this situation to improve the precision for predicting data which should be labelled as yes or no   my knowledge related to machine and deep learning is limited 
590,590,4442478,72262608,Steps for Machine Learning in Pytorch,"<p>When we define our model in <code>PyTorch</code>. We run through different <code>#epochs</code>. I want to know that in the iteration of epochs.
What is the difference between the two following snippets of code in which the order is different? These two snippet versions are:</p>
<ol>
<li>I found over tutorials</li>
<li>The code provided by my supervisor for the project.</li>
</ol>
<p>Tutorial Version</p>
<pre><code>for i in range(epochs):
    logits = model(x)    
    loss = loss_fcn(logits,lables)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>
<p>Supervisor Version</p>
<pre><code>for i in range(epochs):
    logits = model(x)
    loss = loss_fcn(logits,lables)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
",33,3,2,3,python;machine-learning;pytorch,2022-05-16 22:04:21,2022-05-16 22:04:21,2022-05-18 00:37:34,tutorial version supervisor version
591,591,19122949,72272532,module &#39;keras.api._v2.keras&#39; has no attribute &#39;model&#39;,"<p>I tried to create a nueral network by:</p>
<p><code> import tensorflow as tf</code></p>
<p><code>from tensorflow import keras</code></p>
<p><code>model = keras.model.Sequential([</code></p>
<pre><code>keras.layers.Flatten(), # the input layer... why it has no size???
keras.layers.Dense(128, activation = 'relu'), #the hidden
keras.layers.Dense(10) #The output layer
</code></pre>
<p><code>])</code></p>
<p>But it yeild at me:</p>
<p><strong>module 'keras.api._v2.keras' has no attribute 'model'</strong></p>
<p>Here is the tensorflow package info:</p>
<p>Name: tensorflow</p>
<p>Version: 2.8.0</p>
<p>Summary: TensorFlow is an open source machine learning framework for everyone.</p>
<p>Home-page: <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/</a></p>
<p>Author: Google Inc.</p>
<p>Author-email: packages@tensorflow.org</p>
<p>License: Apache 2.0</p>
<p>Location: c:\users\lior\appdata\roaming\python\python39\site-packages</p>
<p>Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, tf-estimator-nightly, typing-extensions, wrapt</p>
<p>Required-by:</p>
",81,1,-1,5,python;tensorflow;keras;module;neural-network,2022-05-17 16:06:53,2022-05-17 16:06:53,2022-05-17 23:12:02,i tried to create a nueral network by   import tensorflow as tf from tensorflow import keras model   keras model sequential      but it yeild at me  module  keras api _v keras  has no attribute  model  here is the tensorflow package info  name  tensorflow version     summary  tensorflow is an open source machine learning framework for everyone  home page   author  google inc  author email  packages tensorflow org license  apache   location  c  users lior appdata roaming python python site packages requires  absl py  astunparse  flatbuffers  gast  google pasta  grpcio  hpy  keras  keras preprocessing  libclang  numpy  opt einsum  protobuf  setuptools  six  tensorboard  tensorflow io gcs filesystem  termcolor  tf estimator nightly  typing extensions  wrapt required by 
592,592,11014465,54560993,How to split a dataset (CSV) into training and test data,"<p>How to split a dataset (CSV) into training and test data in Python programming language if there are no dependent variables in it?</p>
<p>The project I am currently working on is machine learning based and the dataset does not contain any dependent data. The following code works only if the dataset contains a dependent data-</p>
<pre><code>from sklearn.model_selection import train_test_split
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)
</code></pre>
<p>I expect the split to happen without any <code>y</code> variable.
Is it possible?</p>
",1042,2,1,4,python-3.x;machine-learning;scikit-learn;train-test-split,2019-02-07 00:45:58,2019-02-07 00:45:58,2022-05-17 17:52:46,how to split a dataset  csv  into training and test data in python programming language if there are no dependent variables in it  the project i am currently working on is machine learning based and the dataset does not contain any dependent data  the following code works only if the dataset contains a dependent data 
593,593,19135855,72273757,Adding a new column to existing dataframe using a library with values,"<p>I have two lists with different sizes, I create a library using zip, and after that I created a function, but when I'm running the code I'm getting a new column without values. The code is below:</p>
<pre><code>key_list = [&quot;Business&quot;, &quot;Clients support&quot;, &quot;Developers&quot;, &quot;Sales&quot;, &quot;HR&quot;, &quot;Marketing&quot;, &quot;Finance&quot;, &quot;Algorithms&quot;, &quot;Leads&quot;, &quot;Managment&quot;, &quot;Unknown&quot;]
value_list = [[&quot;Business Development Representative&quot;, &quot;Partnerships Manager&quot;, &quot;Partnerships Development Manager&quot;, &quot;Strategy Partnership&quot;, &quot;Salesforce Admin&quot;],
              [&quot;Senior Customer Success Manager&quot;, &quot;Technical Account Manager&quot;, &quot;Technical Support Tier 1&quot;, &quot;Technical Support Engineer&quot;], 
              [&quot;Junior Front End Engineer&quot;, &quot;Implementation Specialist&quot;, &quot;Back End Engineer&quot;, &quot;Customer Success Manager&quot;, &quot;Junior Full Stack Engineer&quot;, 
               &quot;Senior Full Stack Developer&quot;, &quot;Senior Full Stack Engineer&quot;, &quot;Full Stack Engineer&quot;, &quot;QA Engineer&quot;, &quot;Junior Back End Engineer&quot;, &quot;Dev Ops Engineer&quot;, 
               &quot;BI Developer&quot;, &quot;Senior Dev Ops Engineer&quot;, &quot;Senior Back End Engineer&quot;, &quot;Full Stack Developer&quot;, &quot;QA Specialist&quot;, &quot;IT Specialist&quot;, &quot;Senior SEO Expert&quot;], 
              [&quot;Account Development Representative&quot;, &quot;Enterprise Account Executive&quot;, &quot;Pre Sales Consultant&quot;], 
              [&quot;Talent Acquisition Specialist&quot;, &quot;HR Business Partner&quot;, &quot;Executive Assistant&quot;, &quot;Employee Experience Specialist &amp; Office Manager&quot;], 
              [&quot;Social Media Expert&quot;, &quot;Senior Content Marketer&quot;, &quot;Graphic Marketing Designer&quot;], 
              [&quot;Bookkeeper&quot;, &quot;Controller&quot;, &quot;Assistant Controller&quot;], 
              [&quot;Data Specialist&quot;, &quot;Data Scientist / Machine Learning Engineer&quot;, &quot;Junior BI Analyst&quot;, &quot;Data Scientist&quot;, &quot;Junior BI &amp; Data Analyst&quot;, 
               &quot;Machine Learning Engineer&quot;, &quot;BI &amp; Data Analyst&quot;, &quot;Lead Product Analyst&quot;, &quot;Machine Learning Data Analyst&quot;], 
              [&quot;Tech Lead&quot;, &quot;Team Lead - SyteApp Customizations&quot;, &quot;Team Lead - Self-Service &amp; Experimentation&quot;, &quot;Team Lead - Deep Tag&quot;, &quot;Team Lead - Ingestion&quot;, 
               &quot;Team Lead - Architecture&quot;, &quot;Team Lead - Technical Accounts&quot;, &quot;Team Leader - Visual Squad&quot;, &quot;Team Lead - Stories&quot;, &quot;Team Lead - Implementation&quot;, 
               &quot;Team Lead - Salesforce&quot;, &quot;Team Lead - Dev Ops&quot;, &quot;QA Team Lead&quot;, &quot;Team Lead - Augmented Search&quot;, &quot;Account Development Team Lead&quot;, 
               &quot;Team Lead - SyteApp&quot;, &quot;Team Lead - Solution Engineering&quot;], 
              [&quot;VP Finance&quot;, &quot;Product manager&quot;, &quot;Director of Customer Success&quot;, &quot;CISO&quot;, &quot;Director of Enterprise Sales&quot;, &quot;Product Marketing Manager&quot;, &quot;Project Manager&quot;, 
               &quot;Co-Founder / CTO&quot;, &quot;Co-Founder/ COO&quot;, &quot;Director of Back End &amp; Dev Ops&quot;, &quot;VP Partnerships&quot;, &quot;Head of Delivery&quot;, &quot;VP Sales&quot;, &quot;Director of Machine Learning&quot;, 
               &quot;Director of Account Development&quot;, &quot;Director of Presales&quot;, &quot;Head of Content &amp; Social&quot;, &quot;VP Product&quot;, &quot;Program Manager&quot;, &quot;Director of BI &amp; Analytics&quot;, 
               &quot;Director of Full Stack Engineering&quot;, &quot;Director of Product&quot;, &quot;CEO&quot;, &quot;VP R&amp;D&quot;, &quot;VP Customer Success&quot;, &quot;Art Director&quot;, &quot;VP People Operations&quot;,
               &quot;Salesforce Project Manager&quot;, &quot;Solution Architect&quot;], 
              [&quot;202683231&quot;, &quot;201390700&quot;, &quot;199565334&quot;]]

uniq_title = dict(zip(key_list, value_list))

def add_title_cathegory_colomn(df):
    df[&quot;Title cathegory&quot;] = uniq_title
    return df 
update_df = df
uniq_title.update({k: add_title_cathegory_colomn(v) for k, v in uniq_title.items() if k in update_df})
</code></pre>
",24,0,0,4,python;pandas;dataframe;data-science,2022-05-17 17:35:04,2022-05-17 17:35:04,2022-05-17 17:35:04,i have two lists with different sizes  i create a library using zip  and after that i created a function  but when i m running the code i m getting a new column without values  the code is below 
594,594,7281675,72258521,Tuning XGBRanker produces error for groups,"<p>I have a simple ranking problem, and i use:</p>
<pre><code>from xgboost import XGBRanker

model = XGBRanker(
    min_child_weight=10,
    subsample=0.5,
    tree_method='hist',
)
model.fit(X_train, y_train, group=groups)
</code></pre>
<p>Works fine. As an step of machine learning flow, now I want to tune the hyperparameters of the model as usual. I tried:</p>
<pre><code>from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer


opt = BayesSearchCV(
    model,
    {
        'min_child_weight': Real(.05, .5, prior='log-uniform'),
        'subsample': Real(.05, .5, prior='log-uniform'),
        #'n_estimators ': Integer(1,50),
    },
    n_iter=32,
    random_state=0,
    scoring='accuracy'
)

# executes bayesian optimization
_ = opt.fit(X_train, y_train, group=groups)
</code></pre>
<p>and I receive the following error:</p>
<pre><code>Check failed: group_ptr_.back() == num_row_ (5740832 vs. 4592665) : Invalid group structure.  Number of rows obtained from groups doesn't equal to actual number of rows given by data.
</code></pre>
<p>I tried this with <code>RandomizedSearchCV</code> of <code>scikit</code> and the same error occurred.</p>
",41,0,0,5,python;machine-learning;scikit-learn;xgboost;skopt,2022-05-16 17:01:01,2022-05-16 17:01:01,2022-05-17 17:22:45,i have a simple ranking problem  and i use  works fine  as an step of machine learning flow  now i want to tune the hyperparameters of the model as usual  i tried  and i receive the following error  i tried this with randomizedsearchcv of scikit and the same error occurred 
595,595,15564067,72272193,"Why is the model getting 100% accuracy for SVM, Random-forest Classifier and Logistic Regression?","<blockquote>
<p>I'm using an existing disease prediction model to build a chatbot. While I was referring to the model I realized that it has an accuracy of 100%. I'm not quite sure how and why the accuracy is 100%. I've attached herewith the link to the code I'm referring to - <a href=""https://www.geeksforgeeks.org/disease-prediction-using-machine-learning/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/disease-prediction-using-machine-learning/</a></p>
</blockquote>
<blockquote>
<p>I would be glad if someone could give me some insight on how the accuracy is 100%. Below is the code</p>
</blockquote>
<pre><code>from cgi import test
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import mode
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
import pickle
import requests
import json
import seaborn as sns

# Reading the train.csv by removing the last column since it's an empty column
DATA_PATH = &quot;D:/Diabetdeck-V3/flask-server/dataset/Training.csv&quot;
data = pd.read_csv(DATA_PATH).dropna(axis = 1)

# Checking whether the dataset is balanced or not
disease_counts = data[&quot;prognosis&quot;].value_counts()
temp_df = pd.DataFrame({
&quot;Disease&quot;: disease_counts.index,
&quot;Counts&quot;: disease_counts.values
})

plt.figure(figsize = (18,8))
sns.barplot(x = &quot;Disease&quot;, y = &quot;Counts&quot;, data = temp_df)
plt.xticks(rotation=90)
plt.show()

# Encoding the target value into numerical value using LabelEncoder
encoder = LabelEncoder()
data[&quot;prognosis&quot;] = encoder.fit_transform(data[&quot;prognosis&quot;])

X = data.iloc[:,:-1]
y = data.iloc[:, -1]
X_training_data, X_testing_data, y_training_data, y_testing_data =train_test_split(
X, y, test_size = 0.2, random_state = 24)

print(f&quot;Train: {X_training_data.shape}, {y_training_data.shape}&quot;)
print(f&quot;Test: {X_testing_data.shape}, {y_testing_data.shape}&quot;)

# Defining scoring metric for k-fold cross validation
def cv_scoring(estimator, X, y):
return accuracy_score(y, estimator.predict(X))

# Initializing Models
models = {
&quot;SVC&quot;:SVC(),
&quot;Logistic Regression&quot;:LogisticRegression(),
&quot;Random Forest&quot;:RandomForestClassifier(random_state=18)
}

# Producing cross validation score for the models
for model_name in models:
model = models[model_name]
scores = cross_val_score(model, X, y, cv = 10,
                         n_jobs = -1,
                         scoring = cv_scoring)

print(&quot;==&quot;*30)
print(model_name)
print(f&quot;Scores: {scores}&quot;)
print(f&quot;Mean Score: {np.mean(scores)}&quot;)

# Training and testing SVM Classifier
svm_model = SVC()
svm_model.fit(X_training_data, y_training_data)
preds = svm_model.predict(X_testing_data)
# pickle.dump(svm_model, open('model.pkl','wb'))

print(f&quot;Accuracy on train data by SVM Classifier\
: {accuracy_score(y_training_data, 
svm_model.predict(X_training_data))*100}&quot;)

print(f&quot;Accuracy on test data by SVM Classifier\
: {accuracy_score(y_testing_data, preds)*100}&quot;)
cf_matrix = confusion_matrix(y_testing_data, preds)
plt.figure(figsize=(12,8))
sns.heatmap(cf_matrix, annot=True)
plt.title(&quot;Confusion Matrix for SVM Classifier on Test Data&quot;)
plt.show()

# Training and testing Logistic Regression
lr_model = LogisticRegression(C=0.1, penalty='l2', 
solver='liblinear')
lr_model.fit(X_training_data, y_training_data)
lr_model.score(X_training_data, y_training_data)
preds = lr_model.predict(X_testing_data)
# pickle.dump(lr_model, open('model.pkl','wb'))

print(f&quot;Accuracy on train data by Logistic Regression\
: {accuracy_score(y_training_data, 
lr_model.predict(X_training_data))*100}&quot;)

print(f&quot;Accuracy on test data by Logistic Regression\
: {accuracy_score(y_testing_data, preds)*100}&quot;)
cf_matrix = confusion_matrix(y_testing_data, preds)
plt.figure(figsize=(12,8))
sns.heatmap(cf_matrix, annot=True)
plt.title(&quot;Confusion Matrix for Logistic Regression on Test Data&quot;)
plt.show()

# Training and testing Random Forest Classifier
rf_model = RandomForestClassifier(random_state=18)
rf_model.fit(X_training_data, y_training_data)
preds = rf_model.predict(X_testing_data)
# pickle.dump(rf_model, open('model.pkl','wb'))

print(f&quot;Accuracy on train data by Random Forest Classifier\
: {accuracy_score(y_training_data, 
rf_model.predict(X_training_data))*100}&quot;)

print(f&quot;Accuracy on test data by Random Forest Classifier\
: {accuracy_score(y_testing_data, preds)*100}&quot;)

cf_matrix = confusion_matrix(y_testing_data, preds)
plt.figure(figsize=(12,8))
sns.heatmap(cf_matrix, annot=True)
plt.title(&quot;Confusion Matrix for Random Forest Classifier on Test Data&quot;)
plt.show()

# Training the models on whole data
final_svm_model = SVC()
final_lr_model = LogisticRegression()
final_rf_model = RandomForestClassifier(random_state=18)
final_svm_model.fit(X, y)
pickle.dump(final_svm_model, open('D:/Diabetdeck-V3/flask-server/Model/save/svm/finalsvmmodel.h5', 'wb'))
final_lr_model.fit(X, y)
pickle.dump(final_lr_model, open('D:/Diabetdeck-V3/flask-server/Model/save/lr/finallrmodel.h5', 'wb'))
final_rf_model.fit(X, y)
pickle.dump(final_rf_model, open('D:/Diabetdeck-V3/flask-server/Model/save/rf/finalrfmodel.h5', 'wb'))

# Reading the test data
test_data = pd.read_csv(&quot;D:/Diabetdeck-V3/flask-server/dataset/Testing.csv&quot;).dropna(axis=1)

test_X = test_data.iloc[:, :-1]
test_Y = encoder.transform(test_data.iloc[:, -1])

# Making prediction by take mode of predictions made by all the classifiers
svm_preds = final_svm_model.predict(test_X)
lr_preds = final_lr_model.predict(test_X)
rf_preds = final_rf_model.predict(test_X)

final_preds = [mode([i,j,k])[0][0] for i,j,
           k in zip(svm_preds, lr_preds, rf_preds)]

print(f&quot;Accuracy on Test dataset by the combined model\
: {accuracy_score(test_Y, final_preds)*100}&quot;)

cf_matrix = confusion_matrix(test_Y, final_preds)
plt.figure(figsize=(12,8))

sns.heatmap(cf_matrix, annot = True)
plt.title(&quot;Confusion Matrix for Combined Model on Test Dataset&quot;)
plt.show()

symptoms = X.columns.values

# Creating a symptom index dictionary to encode the input symptoms into numerical form
symptom_index = {}
for index, value in enumerate(symptoms):
symptom = &quot; &quot;.join([i.capitalize() for i in value.split(&quot;_&quot;)])
symptom_index[symptom] = index

data_dict = {
&quot;symptom_index&quot;:symptom_index,
&quot;predictions_classes&quot;:encoder.classes_
}
pickle.dump(data_dict, open('D:/Diabetdeck-V3/flask-server/Model/save/data_dictionary/datadictionary.h5', 'wb'))
</code></pre>
<blockquote>
<p>Link to dataset <a href=""https://www.kaggle.com/datasets/kaushil268/disease-prediction-using-machine-learning"" rel=""nofollow noreferrer"">here</a></p>
</blockquote>
<blockquote>
<p>Below is a screenshot of the accuracy
<a href=""https://i.stack.imgur.com/cWocP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cWocP.png"" alt=""Screenshot of accuracy and output"" /></a></p>
</blockquote>
",96,3,-1,4,python;machine-learning;svm;random-forest,2022-05-17 15:47:19,2022-05-17 15:47:19,2022-05-17 16:00:10,i m using an existing disease prediction model to build a chatbot  while i was referring to the model i realized that it has an accuracy of    i m not quite sure how and why the accuracy is    i ve attached herewith the link to the code i m referring to    i would be glad if someone could give me some insight on how the accuracy is    below is the code link to dataset 
596,596,11154841,67852675,PostgreSQL 13 + Python 3.7.9 + plpython3u: &#39;psql: server closed the connection unexepectedly.&#39; + &#39;The application has lost the database connection.&#39;,"<p>I have added all of the details I could find, with all of the links, and there is no way to get plpython3u to work on Windows in PostgreSQL 13, it seems.</p>
<p><em>OLD, since the accepted answer shows that v3.7.0 solves it:</em>
<s>Better do not read through this long question and rather just jump to the answer: not to use Windows PostgreSQL when you need plpython3u. This question has been opened long enough, no solution in sight.</p>
<p>Perhaps a higher PostgreSQL version for Windows will solve this, then please answer.</s></p>
<hr />
<h2>Spin-off</h2>
<p>This is a spin-off from</p>
<p><a href=""https://stackoverflow.com/a/62275974/11154841"">Can't “install” plpython3u - postgresql</a> and all of its comments</p>
<p>and from</p>
<p><a href=""https://stackoverflow.com/a/53472460/11154841"">PosgreSQL 11 lost connection when i'm trying to create function with plpython3u [WIN10, pgAdmin4 3.5]</a>.</p>
<h2>Steps of errors and solutions up to now</h2>
<p>I have taken these steps which were totally scattered across Stack Overflow:</p>
<h3>Step 0</h3>
<p>If you run a sql that uses the language plpython3u without it being installed, you get</p>
<blockquote>
<p>ERROR:  language &quot;plpython3u&quot; does not exist HINT:  Use CREATE
EXTENSION to load the language into the database.</p>
<p>SQL state: 42704</p>
</blockquote>
<p>Related:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/62274910/cant-install-plpython3u-postgresql?noredirect=1&amp;lq=1"">Can't “install” plpython3u - postgresql</a></li>
</ul>
<h3>Step 1</h3>
<p>At error</p>
<blockquote>
<p>ERROR: could not load library &quot;C:/Program Files
(x86)/PostgreSQL/13/lib/plpython3u.dll&quot;: The specified module could
not be found.</p>
<p>SQL state: 58P01</p>
</blockquote>
<p>look up <code>C:\Program Files\PostgreSQL\13\doc\installation-notes.html</code> to find the needed Python version to be installed for the installed PostgreSQL version.</p>
<blockquote>
<p>PostgreSQL 13</p>
<p>Installation Notes</p>
<p>Welcome to the PostgreSQL 13 Installation Wizard.</p>
<p>Procedural Languages</p>
<p>The procedural languages pl/Perl, pl/Python and pl/Tcl are included in
this distribution of PostgreSQL. The server has been built using the
LanguagePack community distributions of those language interpreters.
To use any of the these languages from within PostgreSQL, download and
install the appropriate interpreters and ensure they are included in
the PATH variable under which the database server will be started. The
versions used are shown below - newer minor (bugfix) releases may also
work, but have not been tested:</p>
<pre><code>Perl 5.26
Python 3.7
Tcl 8.6
</code></pre>
</blockquote>
<p><a href=""https://i.stack.imgur.com/l3Qxc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3Qxc.png"" alt=""enter image description here"" /></a></p>
<p>Thus, Python 3.7 is needed.</p>
<p>Credits go to:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/47907232/could-not-load-library-plpython3-dll/62955618#62955618"">could not load library plpython3.dll --&gt; comment: Where to find this information ? Like for plpython3u which python version is required ?</a>, answered Jul 17 '20</li>
<li><a href=""https://stackoverflow.com/a/63374406/11154841"">Error during: CREATE EXTENSION plpython3u; on PostgreSQL 9.6.0</a>, edited Oct 2 '20</li>
</ul>
<p>Related:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/21001890/installing-plpythonu-on-windows"">Installing plpythonu on Windows</a></li>
<li><a href=""https://stackoverflow.com/questions/60683776/install-pl-python-on-windows-for-postgresql-12"">Install PL/Python on Windows for PostgreSQL 12</a></li>
</ul>
<h3>Step 2</h3>
<p>Install Python version using the webinstaller of <a href=""https://www.python.org/downloads/windows/"" rel=""nofollow noreferrer"">Python Releases for Windows</a></p>
<p>The most recent sub-version 3.7.10 does not have any files in the list of stable releases and I am too lazy to install Python from source on Windows. The source code of v3.7.10 is available here <a href=""https://www.python.org/downloads/release/python-3710/"" rel=""nofollow noreferrer"">Looking for a specific release?</a>, for anyone who wants to try):</p>
<blockquote>
<p>Python 3.7.10 - Feb. 15, 2021</p>
<p>Note that Python 3.7.10 cannot be used on Windows XP or earlier.</p>
<pre><code>No files for this release.
</code></pre>
</blockquote>
<p><a href=""https://i.stack.imgur.com/HC7VM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HC7VM.png"" alt=""enter image description here"" /></a></p>
<p>Explanation copied from <a href=""https://stackoverflow.com/a/44019990/11154841"">How to build Python 3.4.6 from source?</a></p>
<blockquote>
<p>The Python 3.7 branch is in security fixes only mode. This means that
only security fixes will be accepted on this branch, no more
non-critical bug fixes. New releases on this branch are source-only,
no binaries will be provided.</p>
<p><a href=""https://mail.python.org/pipermail/python-committers/2017-January/004128.html"" rel=""nofollow noreferrer"">See the official
announcement</a>.</p>
<p>If you really need a python 3.7.10 binary for windows, you will have to
compile it yourself.</p>
</blockquote>
<p><a href=""https://stackoverflow.com/a/64176192/11154841"">Cannot install plpython for postgres 12</a> recommends to install a specific version from source:</p>
<blockquote>
<p>you want to use a specific python version &gt; use source and compile it</p>
</blockquote>
<p>Again, since I am lazy, I take the most recent stable release of 3.7, which is sub-version 3.7.9, and this should be no problem following the remark, as you seem to be free to choose the sub-version:</p>
<blockquote>
<p>Try version python-3.4.0.amd64 for windows 64bit <strong>or other versions</strong>
from this Python 3.4.0 downloads Link</p>
</blockquote>
<p>From: <a href=""https://stackoverflow.com/a/48542889/11154841"">could not load library plpython3.dll</a></p>
<p>As I said, I am too lazy to take the effort of compiling the binaries of v3.7.10 on Windows when v3.7.9 is available, thus:</p>
<blockquote>
<p>Python 3.7.9 - Aug. 17, 2020</p>
<p>Note that Python 3.7.9 cannot be used on Windows XP or earlier.</p>
<pre><code>Download Windows help file
Download Windows x86-64 embeddable zip file
Download Windows x86-64 executable installer
Download Windows x86-64 web-based installer
Download Windows x86 embeddable zip file
Download Windows x86 executable installer
Download Windows x86 web-based installer
</code></pre>
</blockquote>
<p><a href=""https://i.stack.imgur.com/AepGM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AepGM.png"" alt=""enter image description here"" /></a></p>
<p>I install &quot;Download Windows x86-64 web-based installer&quot; (side-note: you cannot change the installation path, they seem to force you to use this; to reach it quickly, in Windows Explorer, type in the path %appdata% --&gt; go to parent folder &quot;appdata&quot; --&gt; then to &quot;local&quot; --&gt; &quot;programs&quot; --&gt; &quot;python&quot; to quickly get there) and check the box for adding the PATH variables as well.</p>
<p>You will have a new entry in your user environment variable &quot;PATH&quot; and you may check this, but you do not need to:</p>
<pre><code>C:\Users\MY_USER\AppData\Local\Programs\Python\Python37\Scripts\
</code></pre>
<p>and</p>
<pre><code>C:\Users\MY_USER\AppData\Local\Programs\Python\Python37\
</code></pre>
<p><a href=""https://i.stack.imgur.com/vDqSu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDqSu.png"" alt=""enter image description here"" /></a></p>
<p>Credits go to:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/48542889/11154841"">could not load library plpython3.dll</a>, answered Jan 31 '18</li>
</ul>
<h3>Step 3</h3>
<p>When executing</p>
<pre><code>CREATE EXTENSION plpython3u;
</code></pre>
<p>in the query tool of PostgreSQL pgAdmin4, I get the error:</p>
<blockquote>
<p>could not load library &quot;C:/Program
Files/PostgreSQL/13/lib/plpython3u.dll&quot;: The specified module could not
be found</p>
</blockquote>
<p>Go to your Python 3.7 installation folder, in my case</p>
<pre><code>C:\Users\MY_USER\AppData\Local\Programs\Python\Python37
</code></pre>
<p>and copy &quot;python37.dll&quot; from there to</p>
<pre><code>C:\Windows\System32
</code></pre>
<p>by confirming that you have admin rights.</p>
<p>Now execute again and it will work:</p>
<pre><code>CREATE EXTENSION plpython3u;
</code></pre>
<p>Credits go to:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/46281240/11154841"">Error during: CREATE EXTENSION plpython3u; on PostgreSQL 9.6.0</a>, answered Sep 18 '17</li>
<li><a href=""https://stackoverflow.com/a/21641465/11154841"">Is there any recipe to successfully install PLPython in Postgresql 9.3 64bit or 32bit on Windows 64 bits?</a>, answered Feb 8 '14</li>
<li><a href=""https://stackoverflow.com/a/64176192/11154841"">Cannot install plpython for postgres 12</a>, answered Oct 2 '20</li>
</ul>
<p>Related questions:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/35210979/postgresql-unable-to-create-plpythonu-extension"">PostgreSQL unable to create plpythonu extension</a></li>
<li><a href=""https://stackoverflow.com/questions/24216627/how-to-install-pl-python-on-postgresql-9-3-x64-windows-7/24218449#24218449"">How to install PL/Python on PostgreSQL 9.3 x64 Windows 7?</a></li>
<li><a href=""https://stackoverflow.com/questions/56358952/im-facing-issues-to-create-a-postgresql-plpython3u-extension"">i'm facing issues to create a postgresql plpython3u extension</a></li>
</ul>
<h3>Step 4 (optional)</h3>
<pre><code>SELECT * FROM pg_extension;
</code></pre>
<p>Output:</p>
<pre><code>old    | extname       | extowner | extrelocatable | extversion | extversion | extconfig | extcondition
&quot;13428&quot;| &quot;plpgsql&quot;     | &quot;10&quot;     | &quot;11&quot;           | false      | &quot;1.0&quot;      | [null]    | [null]
&quot;16776&quot;| &quot;plpython3u&quot;  | &quot;10&quot;     | &quot;11&quot;           | false      | &quot;1.0&quot;      | [null]    | [null]
</code></pre>
<p><a href=""https://i.stack.imgur.com/fdK7a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdK7a.png"" alt=""enter image description here"" /></a></p>
<p>And another check with:</p>
<pre><code>SELECT * FROM pg_language;
</code></pre>
<p>Output:</p>
<pre><code>  lanname   | lanowner | lanispl | lanpltrusted | lanplcallfoid | laninline | lanvalidator | lanacl
------------+----------+---------+--------------+---------------+-----------+--------------+--------
 internal   |       10 | f       | f            |             0 |         0 |         2246 |
 c          |       10 | f       | f            |             0 |         0 |         2247 |
 sql        |       10 | f       | t            |             0 |         0 |         2248 |
 plpgsql    |       10 | t       | t            |         12279 |     12280 |        12281 |
 plpython3u |       10 | t       | f            |         40963 |     40964 |        40965 |
(5 rows)
</code></pre>
<p>Now the available extensions (that is, all possible extensions that can be installed) also show <code>installed_version</code> = <code>1.0</code> for the plpython3u extension:</p>
<pre><code>SELECT * FROM pg_available_extensions WHERE name LIKE '%python%' ORDER BY name;
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/GmbRw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GmbRw.png"" alt=""enter image description here"" /></a></p>
<p>or the output when running the same in psql:</p>
<pre><code>    name    | default_version | installed_version |                  comment
------------+-----------------+-------------------+-------------------------------------------
 plpython3u | 1.0             | 1.0               | PL/Python3U untrusted procedural language
(1 Zeile)
</code></pre>
<p>We see here probably one of the main reasons why there is no recent image that offers plpython extensions: <code>PL/Python3U untrusted procedural language</code>.</p>
<p>And another query which shows the same:</p>
<pre><code>SELECT * FROM pg_pltemplate;
</code></pre>
<p>Output:</p>
<pre><code>  tmplname  | tmpltrusted | tmpldbacreate |      tmplhandler       |        tmplinline        |    tmplvalidator    |    tmpllibrary    | tmplacl
------------+-------------+---------------+------------------------+--------------------------+---------------------+-------------------+---------
 plpgsql    | t           | t             | plpgsql_call_handler   | plpgsql_inline_handler   | plpgsql_validator   | $libdir/plpgsql   |
 pltcl      | t           | t             | pltcl_call_handler     |                          |                     | $libdir/pltcl     |
 pltclu     | f           | f             | pltclu_call_handler    |                          |                     | $libdir/pltcl     |
 plperl     | t           | t             | plperl_call_handler    | plperl_inline_handler    | plperl_validator    | $libdir/plperl    |
 plperlu    | f           | f             | plperlu_call_handler   | plperlu_inline_handler   | plperlu_validator   | $libdir/plperl    |
 plpythonu  | f           | f             | plpython_call_handler  | plpython_inline_handler  | plpython_validator  | $libdir/plpython2 |
 plpython2u | f           | f             | plpython2_call_handler | plpython2_inline_handler | plpython2_validator | $libdir/plpython2 |
 plpython3u | f           | f             | plpython3_call_handler | plpython3_inline_handler | plpython3_validator | $libdir/plpython3 |
(8 rows)
</code></pre>
<p>For the plpython extensions, we see <code>False</code> in the <code>tmpltrusted</code> column and <code>False</code> in the <code>tmpdbacreate</code> column, while the three trusted extensions &quot;plpgsql&quot;, &quot;pltcl&quot; and &quot;plperl&quot; are <code>True</code> in the same columns.</p>
<p>Credits go to:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/21799995/11154841"">Using psql how do I list extensions installed in a database?</a></li>
<li><a href=""https://stackoverflow.com/a/62150830/11154841"">PostgreSQL: how to install plpythonu extension</a></li>
<li><a href=""https://stackoverflow.com/questions/61541532/run-python-script-from-postgresql-function"">Run python script from PostgreSQL function</a></li>
<li><a href=""https://stackoverflow.com/a/62275974/11154841"">Can't “install” plpython3u - postgresql</a>, commented Jun 9 '20</li>
</ul>
<h3>Step 5</h3>
<p>Now run a general test query like this:</p>
<pre><code>CREATE OR REPLACE FUNCTION return_version()
  RETURNS VARCHAR
AS $$
    import sys
    return sys.version
$$ LANGUAGE plpython3u;
</code></pre>
<p>If this worked, you would be able to run the SQL query <code>SELECT return_version()</code> and get</p>
<p>Output:</p>
<pre><code>CREATE FUNCTION
</code></pre>
<p>Test:</p>
<pre><code>postgres=# SELECT return_version();
              return_version
------------------------------------------
 3.8.10 (default, Jun  2 2021, 10:49:15) +
 [GCC 9.4.0]
(1 row)
</code></pre>
<p><em>Of course, we cannot see this, elsewise the question would be solved. It would be <code>3.7.9</code> in this case, I used the Linux installation where <code>plpython3u</code> works, see the Linux hint in the answer.</em></p>
<hr />
<p><em>Side note: a more complicated test with loaded modules</em></p>
<p><strong>Normally, you can ignore this second test and stop at the <code>return_version()</code> function test.</strong></p>
<p>Of course, if the creating the function <code>return_version()</code> fails, the following will also fail. This second test is just to check whether you can also load modules as soon as plpython3u can be used. You will need to install the needed Python packages which must be compatible with Python 3.7, in this case. It seems that one has to use pip and not conda since Python was meant to be downloaded from the official website. To avoid dependency conflicts, it might be good to use <a href=""https://python-poetry.org/docs/"" rel=""nofollow noreferrer"">Poetry</a> as a package manager (similar to conda, just for pip).</p>
<p>When executing this PostgreSQL query of <a href=""https://www.cybertec-postgresql.com/en/machine-learning-in-postgresql-part-1-kmeans-clustering/"" rel=""nofollow noreferrer"">Machine Learning in PostgreSQL Part 1: Kmeans clustering</a>, using the language plpython3u (the needed packages &quot;pandas&quot; and &quot;sklearn&quot; are installed in the base environment of Python3.7, that is, no virtual environment is used to avoid the unsolved <a href=""https://stackoverflow.com/questions/62551455/can-python-venv-be-used-with-plpython3u-for-postgresql"">Can python venv be used with plpython3u for postgresql?</a>, which is absolutely not what I expected from a standard setter like PostgreSQL):</p>
<pre><code>CREATE OR replace FUNCTION kmeans(input_table text, columns text[], clus_num int) RETURNS bytea AS
 
$$
 
from pandas import DataFrame
from sklearn.cluster import KMeans
from pickle import dumps
 
all_columns = &quot;,&quot;.join(columns)
if all_columns == &quot;&quot;:
    all_columns = &quot;*&quot;
 
rv = plpy.execute('SELECT %s FROM %s;' % (all_columns, plpy.quote_ident(input_table)))
 
frame = []
 
for i in rv:
    frame.append(i)
df = DataFrame(frame).astype('float')
kmeans = KMeans(n_clusters=clus_num, random_state=0).fit(df._get_numeric_data())
return dumps(kmeans)
 
$$ LANGUAGE plpython3u;
</code></pre>
<p><em>End of the sidenote</em></p>
<hr />
<p>Any test query using <code>LANGUAGE plpython3u</code> will cause the error:</p>
<pre><code>ERROR: server closed the connection unexpectedly

This probably means the server terminated abnormally before or while processing the request.
</code></pre>
<p><a href=""https://i.stack.imgur.com/O5Xi3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O5Xi3.png"" alt=""enter image description here"" /></a></p>
<p>and when I run another query after this, it runs, but before clicking &quot;Continue&quot;, I get:</p>
<pre><code>The application has lost the database connection.

- If the connection was idle, it may have been forcibly disconnected.
- The application server or database server may have been restarted.
- The user session may have timed out.

Do you want to continue and establish a new session?
</code></pre>
<p><a href=""https://i.stack.imgur.com/S24jQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S24jQ.png"" alt=""enter image description here"" /></a></p>
<p>This might be solved by the thread <a href=""https://stackoverflow.com/a/53472460/11154841"">PosgreSQL 11 lost connection when i'm trying to create function with plpython3u [WIN10, pgAdmin4 3.5]</a>. Such an answer shows that the <strong>sub-version of v3.7.9 or v3.7.10 or another could indeed matter!</strong> Do I need to install version 3.7.10 from source just to have the most recent version?</p>
<p>I do not want to take the effort of installing Python 3.7.10 from source just to check this out. Who says that changing from v3.6.5 to v3.6.7 has really solved it in the link above, and that it was not rather something happening just because of a new install?</p>
<p>I could also try out v3.7.0.</p>
<blockquote>
<p>Python 3.7.0 - June 27, 2018</p>
<p>Note that Python 3.7.0 cannot be used on Windows XP or earlier.</p>
<pre><code>Download Windows help file
Download Windows x86-64 embeddable zip file
Download Windows x86-64 executable installer
Download Windows x86-64 web-based installer
Download Windows x86 embeddable zip file
Download Windows x86 executable installer
Download Windows x86 web-based installer
</code></pre>
</blockquote>
<p><a href=""https://i.stack.imgur.com/dvI7Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dvI7Q.png"" alt=""enter image description here"" /></a></p>
<p>But since version v3.6.7 once seems to have worked, I do not see a reason why I should invest into this.</p>
<p>Credits go to:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/53468284/posgresql-11-lost-connection-when-im-trying-to-create-function-with-plpython3u/53472460#53472460"">PosgreSQL 11 lost connection when i'm trying to create function with plpython3u [WIN10, pgAdmin4 3.5]</a>, answered Nov 25 '18</li>
<li><a href=""https://stackoverflow.com/a/62275974/11154841"">Can't “install” plpython3u - postgresql --&gt; comments</a>, commented Jun 9 '20</li>
</ul>
<p>Related:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/15934364/psql-server-closed-the-connection-unexepectedly"">psql: server closed the connection unexepectedly</a></li>
</ul>
<h3>Windows installation with EDB and Stack Builder</h3>
<p>EDB and Stack Builder is the installation method that is recommended by the PostgreSQL website. I found this at <a href=""https://stackoverflow.com/questions/63764511/cannot-install-plpython-for-postgres-12"">Cannot install plpython for postgres 12</a> (a thread which just deals with not being able to create the extension at all and therefore cannot help out).
I installed PostgreSQL 10 since plpython3u works with that in the timescaleDB Linux container (see &quot;Docker&quot; below) and my hope was that this lower PostgreSQL version would solve it. But with this official installation method, using EDB and then the Stack Builder for the additional &quot;pl/python language pack&quot;, I still get the same error.</p>
<h3>Question</h3>
<p>Which sub-version of Python 3.7 (v3.7.10, v3.7.0, or another; perhaps my v3.7.9 is also already right since plpython extension could be created with that) is surely working together with PostgreSQL13, and how would this have to be found out if not just by testing around? And if choosing the right Python sub-version is not the issue here (which is more likely), how else can I fix the <strong>Step 5</strong> errors that pop up from using the <code>LANGUAGE plpython3u</code>:</p>
<pre><code>ERROR: server closed the connection unexpectedly

This probably means the server terminated abnormally before or while processing the request.
</code></pre>
<p><em>(which is a question at <a href=""https://stackoverflow.com/questions/15934364/psql-server-closed-the-connection-unexepectedly"">psql: server closed the connection unexepectedly</a> but is not focused on this Python extension problem)</em></p>
<p>and</p>
<pre><code>The application has lost the database connection.
</code></pre>
<p><em>(which is a question at <a href=""https://stackoverflow.com/questions/53468284/posgresql-11-lost-connection-when-im-trying-to-create-function-with-plpython3u/53472460#53472460"">PosgreSQL 11 lost connection when i'm trying to create function with plpython3u [WIN10, pgAdmin4 3.5]</a> but would mean installing v3.7.10 from source only to have the most recent sub-version, and I try to find out the right sub-version or another trick to get it run before doing so)</em></p>
",3109,3,1,5,windows;postgresql;python-3.7;plpython;postgresql-13,2021-06-05 23:48:52,2021-06-05 23:48:52,2022-05-17 15:30:33,i have added all of the details i could find  with all of the links  and there is no way to get plpythonu to work on windows in postgresql   it seems  perhaps a higher postgresql version for windows will solve this  then please answer  this is a spin off from  and all of its comments and from   i have taken these steps which were totally scattered across stack overflow  if you run a sql that uses the language plpythonu without it being installed  you get sql state   related  at error sql state  p look up c  program files postgresql  doc installation notes html to find the needed python version to be installed for the installed postgresql version  postgresql  installation notes welcome to the postgresql  installation wizard  procedural languages  thus  python   is needed  credits go to  related  install python version using the webinstaller of  the most recent sub version    does not have any files in the list of stable releases and i am too lazy to install python from source on windows  the source code of v   is available here   for anyone who wants to try   python      feb     note that python    cannot be used on windows xp or earlier   explanation copied from   recommends to install a specific version from source  you want to use a specific python version  gt  use source and compile it again  since i am lazy  i take the most recent stable release of    which is sub version     and this should be no problem following the remark  as you seem to be free to choose the sub version  from   as i said  i am too lazy to take the effort of compiling the binaries of v   on windows when v   is available  thus  python      aug     note that python    cannot be used on windows xp or earlier   i install  download windows x  web based installer   side note  you cannot change the installation path  they seem to force you to use this  to reach it quickly  in windows explorer  type in the path  appdata     gt  go to parent folder  appdata     gt  then to  local     gt   programs     gt   python  to quickly get there  and check the box for adding the path variables as well  you will have a new entry in your user environment variable  path  and you may check this  but you do not need to  and  credits go to  when executing in the query tool of postgresql pgadmin  i get the error  go to your python   installation folder  in my case and copy  python dll  from there to by confirming that you have admin rights  now execute again and it will work  credits go to  related questions  output   and another check with  output  now the available extensions  that is  all possible extensions that can be installed  also show installed_version     for the plpythonu extension  output   or the output when running the same in psql  we see here probably one of the main reasons why there is no recent image that offers plpython extensions  pl pythonu untrusted procedural language  and another query which shows the same  output  for the plpython extensions  we see false in the tmpltrusted column and false in the tmpdbacreate column  while the three trusted extensions  plpgsql    pltcl  and  plperl  are true in the same columns  credits go to  now run a general test query like this  if this worked  you would be able to run the sql query select return_version   and get output  test  of course  we cannot see this  elsewise the question would be solved  it would be    in this case  i used the linux installation where plpythonu works  see the linux hint in the answer  side note  a more complicated test with loaded modules normally  you can ignore this second test and stop at the return_version   function test  of course  if the creating the function return_version   fails  the following will also fail  this second test is just to check whether you can also load modules as soon as plpythonu can be used  you will need to install the needed python packages which must be compatible with python    in this case  it seems that one has to use pip and not conda since python was meant to be downloaded from the official website  to avoid dependency conflicts  it might be good to use  as a package manager  similar to conda  just for pip   when executing this postgresql query of   using the language plpythonu  the needed packages  pandas  and  sklearn  are installed in the base environment of python   that is  no virtual environment is used to avoid the unsolved   which is absolutely not what i expected from a standard setter like postgresql   end of the sidenote any test query using language plpythonu will cause the error   and when i run another query after this  it runs  but before clicking  continue   i get   this might be solved by the thread   such an answer shows that the sub version of v   or v   or another could indeed matter  do i need to install version    from source just to have the most recent version  i do not want to take the effort of installing python    from source just to check this out  who says that changing from v   to v   has really solved it in the link above  and that it was not rather something happening just because of a new install  i could also try out v    python      june    note that python    cannot be used on windows xp or earlier   but since version v   once seems to have worked  i do not see a reason why i should invest into this  credits go to  related  which sub version of python    v    v    or another  perhaps my v   is also already right since plpython extension could be created with that  is surely working together with postgresql  and how would this have to be found out if not just by testing around  and if choosing the right python sub version is not the issue here  which is more likely   how else can i fix the step  errors that pop up from using the language plpythonu   which is a question at  but is not focused on this python extension problem  and  which is a question at  but would mean installing v   from source only to have the most recent sub version  and i try to find out the right sub version or another trick to get it run before doing so 
597,597,3848207,42621190,Display this decision tree with Graphviz,"<p>I am following a tutorial on using python v3.6 to do decision tree with machine learning using scikit-learn.</p>

<p>Here is the code;</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import mglearn
import graphviz

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)

tree = DecisionTreeClassifier(max_depth=4, random_state=0)
tree.fit(X_train, y_train)

from sklearn.tree import export_graphviz
export_graphviz(tree, out_file=""tree.dot"", class_names=[""malignant"", ""benign""],feature_names=cancer.feature_names, impurity=False, filled=True)

import graphviz
with open(""tree.dot"") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
</code></pre>

<p>How do I use Graphviz to see what is inside dot_graph? Presumably, it should look something like this;</p>

<p><a href=""https://i.stack.imgur.com/WNZ8q.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WNZ8q.jpg"" alt=""enter image description here""></a></p>
",22578,5,2,5,python;python-3.x;scikit-learn;graphviz;decision-tree,2017-03-06 14:20:49,2017-03-06 14:20:49,2022-05-17 13:53:02,i am following a tutorial on using python v  to do decision tree with machine learning using scikit learn  here is the code  how do i use graphviz to see what is inside dot_graph  presumably  it should look something like this  
598,598,17203565,72268164,Does GPU accelerate data preprocessing in ML tasks?,"<p>I am doing a machine learning (value prediction) task. While I am preprocessing data, it takes a very long time. I have a csv file with around 640000 rows, and I am trying to subtract the dates of consecutive rows and calculate the time duration. The csv file looks as attached. For example, 2011-08-17 to 2011-08-19 takes 2 days, and I would like to write 2 to the &quot;time duration&quot; column. I've used the python datetime function to do this. And it costs a lot of time.</p>
<pre><code>data = pd.read_csv(f'{proj_dir}/raw data/measures.csv', encoding=&quot;cp1252&quot;) 

file = data[['ID', 'date', 'value1', 'value2', 'duration']]

def time_subtraction(date, prev_date):
  diff = datetime.strptime(date, '%Y-%m-%d') - datetime.strptime(prev_date, '%Y-%m-%d')
  diff_days = diff.days
  return diff_days

def calculate_time_duration(dataframe, set_0_indices):
  for i in range(dataframe.shape[0]):
    # For each patient, sets &quot;Time Duration&quot; at the first measurement to be 0
    if i in set_time_0_indices.values:
      dataframe.iloc[i, 4] = 0 # set time duration to 0 (beginning of this patient)
    else: # time subtraction
      dataframe.iloc[i, 4] = time_subtraction(date=dataframe.iloc[i, 1], prev_date=dataframe.iloc[i-1, 1])
  return dataframe

# I am running on Google Colab. This line takes very long.
result = calculate_time_duration(dataframe = file, set_0_indices = set_time_0_indices)
</code></pre>
<p>I wonder if there are any ways to accelerate this process. Does using a GPU help? I have access to a remote GPU, but I don't know if using a GPU helps with data preprocessing. By the way, under what scenario can GPUs really make things faster? Thanks in advance!</p>
<p><a href=""https://i.stack.imgur.com/hNbsL.png"" rel=""nofollow noreferrer"">what my data looks like</a></p>
",35,1,0,3,machine-learning;gpu;data-science,2022-05-17 09:49:55,2022-05-17 09:49:55,2022-05-17 12:42:09,i am doing a machine learning  value prediction  task  while i am preprocessing data  it takes a very long time  i have a csv file with around  rows  and i am trying to subtract the dates of consecutive rows and calculate the time duration  the csv file looks as attached  for example     to    takes  days  and i would like to write  to the  time duration  column  i ve used the python datetime function to do this  and it costs a lot of time  i wonder if there are any ways to accelerate this process  does using a gpu help  i have access to a remote gpu  but i don t know if using a gpu helps with data preprocessing  by the way  under what scenario can gpus really make things faster  thanks in advance  
599,599,16336222,72269058,Decision Tree Classifier took 16min to fit,"<p><a href=""https://i.stack.imgur.com/tDYMy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tDYMy.png"" alt=""enter image description here"" /></a></p>
<p>So, for some reasons, It took my laptop to 16min to fit data into DecisionTreeClassifier. It usually take like 1 sec to fit into other type of machine learning model. Anyone can help me with what is happening here? I am not sure what information should I provide to help with this. Feel free to ask away!</p>
<p>My guess is it has to do with encoder transform syntax, which I have no idea how to fix from many online searches. It shows that my approach will lead to poor performance, but this syntax is from the library itself, so I do not know how to change the code inside.
<a href=""https://i.stack.imgur.com/63PvW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/63PvW.png"" alt=""enter image description here"" /></a></p>
",57,1,0,5,python;machine-learning;scikit-learn;decision-tree;sklearn-pandas,2022-05-17 11:53:17,2022-05-17 11:53:17,2022-05-17 12:08:41, so  for some reasons  it took my laptop to min to fit data into decisiontreeclassifier  it usually take like  sec to fit into other type of machine learning model  anyone can help me with what is happening here  i am not sure what information should i provide to help with this  feel free to ask away 
600,600,402093,4178614,Suppressing output of module calling outside library,"<p>I have an annoying problem when using machine learning library <a href=""http://pyml.sourceforge.net"" rel=""noreferrer"">PyML</a>. PyML uses <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""noreferrer"">libsvm</a> to train the SVM classifier. The problem is that libsvm outputs some text to standard output. But because that is outside of Python I cannot intercept it. I tried using methods described in problem <a href=""https://stackoverflow.com/questions/2828953/silent-the-stdout-of-a-function-in-python-without-trashing-sys-stdout-and-restori"">Silence the stdout of a function in Python without trashing sys.stdout and restoring each function call</a> but none of those help.</p>

<p>Is there any way how to do this. Modifying PyML is not an option.</p>
",5848,4,10,3,python;libsvm;pyml,2010-11-14 22:44:16,2010-11-14 22:44:16,2022-05-17 09:11:09,i have an annoying problem when using machine learning library   pyml uses  to train the svm classifier  the problem is that libsvm outputs some text to standard output  but because that is outside of python i cannot intercept it  i tried using methods described in problem  but none of those help  is there any way how to do this  modifying pyml is not an option 
601,601,19131016,72266011,XGBoost decision tree machine learning,"<p>I got this fig when I used the XGBoost regressor (the decision tree has too many details &amp; not clear)for a large dataset (3 MB)
<a href=""https://i.stack.imgur.com/yZ6M3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Please what is the solution?</p>
",14,0,0,1,machine-learning,2022-05-17 03:22:13,2022-05-17 03:22:13,2022-05-17 03:22:13,please what is the solution 
602,602,7544564,42154481,How to start a machine learning course of Udacity on Anaconda Jupyter notebook and Python 2.7?,"<p>I want to start a machine learning course of udacity. So I downloaded ud120-projects-master.zip file and extracted it in my downloads folder. I have installed anaconda jupyter notebook (python 2.7).</p>

<p>First mini project is Naïve-Bayes ,so I opened the jupyter notebook and the %load nb_author_id.py to convert into .ipynb
But I think I have to first run the startup.py in tools folder to extract the data.</p>

<p>So I ran the startup.ipynb.</p>

<pre><code># %load startup.py
print
print ""checking for nltk""
try:
    import nltk
except ImportError:
    print ""you should install nltk before continuing""

print ""checking for numpy""
try:
    import numpy
except ImportError:
    print ""you should install numpy before continuing""

print ""checking for scipy""
try:
    import scipy
except:
    print ""you should install scipy before continuing""

print ""checking for sklearn""
try:
    import sklearn
except:
    print ""you should install sklearn before continuing""

print
print ""downloading the Enron dataset (this may take a while)""
print ""to check on progress, you can cd up one level, then execute &lt;ls -lthr&gt;""
print ""Enron dataset should be last item on the list, along with its current size""
print ""download will complete at about 423 MB""
import urllib
url = ""https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz""
urllib.urlretrieve(url, filename=""../enron_mail_20150507.tgz"") 
print ""download complete!""


print
print ""unzipping Enron dataset (this may take a while)""
import tarfile
import os
os.chdir("".."")
tfile = tarfile.open(""enron_mail_20150507.tgz"", ""r:gz"")
tfile.extractall(""."")

print ""you're ready to go!""
</code></pre>

<p>But getting an error....</p>

<pre><code>checking for nltk
checking for numpy
checking for scipy
checking for sklearn

downloading the Enron dataset (this may take a while)
to check on progress, you can cd up one level, then execute &lt;ls -lthr&gt;
Enron dataset should be last item on the list, along with its current size
download will complete at about 423 MB




---------------------------------------------------------------------------
IOError                                   Traceback (most recent call last)
&lt;ipython-input-1-c30fe1ced56a&gt; in &lt;module&gt;()
     32 import urllib
     33 url = ""https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz""
---&gt; 34 urllib.urlretrieve(url, filename=""../enron_mail_20150507.tgz"")
     35 print ""download complete!""
     36 
</code></pre>

<p>This is for nb_author_id.py :</p>

<pre><code># %load nb_author_id.py
#!/usr/bin/python

"""""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors

    authors and labels:
    Sara has label 0
    Chris has label 1
""""""

import sys
from time import time
sys.path.append(""../tools/"")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###


#########################################################
</code></pre>

<p>error/warning</p>

<pre><code>C:\Users\jr31964\AppData\Local\Continuum\Anaconda2\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)




no. of Chris training emails: 7936
no. of Sara training emails: 7884
</code></pre>

<p>How to I start with Naïve Bayes mini project and what are the prerequisites action needed.</p>
",1910,2,0,5,python-2.7;github;machine-learning;anaconda;jupyter-notebook,2017-02-10 13:42:27,2017-02-10 13:42:27,2022-05-17 01:30:44,i want to start a machine learning course of udacity  so i downloaded ud projects master zip file and extracted it in my downloads folder  i have installed anaconda jupyter notebook  python     so i ran the startup ipynb  but getting an error     this is for nb_author_id py   error warning how to i start with naïve bayes mini project and what are the prerequisites action needed 
603,603,18782495,72264509,IndexError Machine-Learning Python,"<p>I am trying to do a ML algorithm on a netflix movies data set. One movie from the data set looks like this (it's csv):</p>
<p>Atrributes:</p>
<ul>
<li><p>show_id, type, title, director, cast, country, date_added,
release_year, rating, duration, listed_in, description
Movie example:</p>
</li>
<li><p>s1,Movie,Dick Johnson Is Dead,Kirsten Johnson,,United
States,&quot;September 25, 2021&quot;,2020,PG-13,90 min,Documentaries,&quot;As her
father nears the end of his life, filmmaker Kirsten Johnson stages
his death in inventive and comical ways to help them both face the
inevitable.&quot;</p>
</li>
</ul>
<p>Here is my code by now:</p>
<pre><code>import numpy as np
import pandas as pd
import difflib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#data collection + pre-processing
movies_data = pd.read_csv('netflix_titles.csv')

# printing the first 5 rows of the dataframe
#print(movies_data.head());
#print(movies_data.shape);


# selecting the relevant features for recommendation

selected_features = ['type','title','description','cast','director']
#print(selected_features);

# replacing the null valuess with null string

for feature in selected_features:
  movies_data[feature] = movies_data[feature].fillna('');

# combining all the 5 selected features

combined_features = movies_data['type']+' '+movies_data['title']+' '+movies_data['description']+' '+movies_data['cast']+' '+movies_data['director']
#print(combined_features)

# converting the text data to feature vectors
vectorizer = TfidfVectorizer()
#
feature_vectors = vectorizer.fit_transform(combined_features)
#
#print(feature_vectors);
#
# # getting the similarity scores using cosine similarity
#
similarity = cosine_similarity(feature_vectors)
#print(similarity.shape);
#
# # getting the movie name from the user
#
movie_name = input(' Enter your favourite movie name : ')
#
# # creating a list with all the movie names given in the dataset
#
list_of_all_titles = movies_data['title'].tolist()
#print(list_of_all_titles)
#
# # finding the close match for the movie name given by the user
#
find_close_match = difflib.get_close_matches(movie_name, list_of_all_titles)
#print(find_close_match)
#
close_match = find_close_match[0]
#print(close_match)
#
# # finding the index of the movie with title
#
index_of_the_movie = movies_data[movies_data.title == close_match]['show_id'].values[0]
index_replace=index_of_the_movie.replace('s', '');
index_integer=type(int(index_replace));
print(index_integer);
#
# # getting a list of similar movies
#
similarity_score = list(enumerate(similarity[index_integer]))
print(similarity_score)
</code></pre>
<p>and I am getting this exact error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/andre/PycharmProjects/MLProject/main.py&quot;, line 68, in &lt;module&gt;
    similarity_score = list(enumerate(similarity[index_integer]))
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
</code></pre>
<p>Any ideas on how can I solve it? Thanks.</p>
",21,1,0,2,python;machine-learning,2022-05-17 00:46:36,2022-05-17 00:46:36,2022-05-17 00:52:12,i am trying to do a ml algorithm on a netflix movies data set  one movie from the data set looks like this  it s csv   atrributes  here is my code by now  and i am getting this exact error  any ideas on how can i solve it  thanks 
604,604,13346228,72258087,unexpected keyword argument &#39;tenant_id&#39; while accessing Azure Key Vault in Python,"<p>I was trying to accessing my key vault, but I got always the same error:</p>
<pre><code>AppServiceCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'
ManagedIdentityCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'
</code></pre>
<p>This was the code I used in an Azure Machine Learning notebook, copied from the docs:</p>
<pre class=""lang-py prettyprint-override""><code>from azure.identity import ManagedIdentityCredential
from azure.keyvault.secrets import SecretClient

credential = ManagedIdentityCredential()
secret_client = SecretClient(vault_url=&quot;https://XXXX.vault.azure.net/&quot;, credential=credential)

secretName = 'test'
retrieved_secret = secret_client.get_secret(secretName) # here's the error
retrieved_secret
</code></pre>
<p>What is wrong? Could you help me?
Thank you in advance.</p>
",152,1,4,3,python;azure;azure-keyvault,2022-05-16 16:25:34,2022-05-16 16:25:34,2022-05-16 22:10:38,i was trying to accessing my key vault  but i got always the same error  this was the code i used in an azure machine learning notebook  copied from the docs 
605,605,8366805,64547303,How to retrieve the random_state of sklearn.model_selection.train_test_split?,"<p>How to retrieve the random state of <code>sklearn.model_selection.train_test_split</code>?</p>
<p>Without setting the <code>random_state</code>, I split my dataset with <code>train_test_split</code>. Because the machine learning model trained on the split dataset performs quite well, I want to retrieve the <code>random_state</code> that was used to split the dataset. Is there something like <code>numpy.random.get_state()</code></p>
",554,3,1,3,python;python-3.x;scikit-learn,2020-10-27 07:22:58,2020-10-27 07:22:58,2022-05-16 22:08:56,how to retrieve the random state of sklearn model_selection train_test_split  without setting the random_state  i split my dataset with train_test_split  because the machine learning model trained on the split dataset performs quite well  i want to retrieve the random_state that was used to split the dataset  is there something like numpy random get_state  
606,606,10989735,56449031,How to handle columns like &#39;country&#39; and &#39;age groups&#39; while making a prediction model in python?,"<p>I am much new to machine learning and while I was working on this specific data-frame, I found it difficult to handle important columns like age groups and country. </p>

<p>Here is a link to the data-set I am using:</p>

<p><a href=""https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016"" rel=""nofollow noreferrer"">https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016</a></p>

<p>In the more precise prediction of the data, the columns 'country' and 'age group' are pretty much important. But I am constantly getting the errors like:</p>

<pre><code>{ 
  could not convert string to float: '15-24 years' 
}
</code></pre>

<p>And similar for the country column.</p>

<p>What could I do to make them suitable for the model?</p>
",1059,2,-1,5,python;machine-learning;data-science;data-analysis;data-cleaning,2019-06-04 23:22:05,2019-06-04 23:22:05,2022-05-16 20:57:33,i am much new to machine learning and while i was working on this specific data frame  i found it difficult to handle important columns like age groups and country   here is a link to the data set i am using   in the more precise prediction of the data  the columns  country  and  age group  are pretty much important  but i am constantly getting the errors like  and similar for the country column  what could i do to make them suitable for the model 
607,607,17589917,72261547,How do sklearn StandardScaler works inside a Pipeline?,"<p>I'm trying to make my first machine learning exercise with the <a href=""https://www.kaggle.com/datasets/kumargh/pimaindiansdiabetescsv"" rel=""nofollow noreferrer"">Pima Diabetes</a> dataset.
I tried to use KNN model two times, one manually, and another inside a Pipeline. There is the results:</p>
<p><strong>Just getting the samples and the labels</strong></p>
<pre class=""lang-py prettyprint-override""><code>X_train = np.asarray(train.iloc[:, :8])
X_test = np.asarray(test.iloc[:, :8])
y_train = np.asarray(train['Outcome'])
y_test = np.asarray(test['Outcome'])
</code></pre>
<p><strong>Without Pipeline</strong></p>
<pre><code>scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

mod = KNeighborsClassifier()

mod.fit(X_train, y_train)
</code></pre>
<p><em>Results:</em></p>
<blockquote>
<p>Precision =  0.4835164835164835</p>
</blockquote>
<blockquote>
<p>Recall =  0.8979591836734694</p>
</blockquote>
<blockquote>
<p>F1 =  0.6285714285714287</p>
</blockquote>
<blockquote>
<p>Accuracy =  0.48</p>
</blockquote>
<p><strong>With Pipeline</strong></p>
<pre><code>mod = Pipeline([
                ('scaler', StandardScaler()),
                ('model', KNeighborsClassifier())
])

mod.fit(X_train, y_train)
</code></pre>
<p><em>Results:</em></p>
<blockquote>
<p>Precision =  0.5517241379310345</p>
</blockquote>
<blockquote>
<p>Recall =  0.6530612244897959</p>
</blockquote>
<blockquote>
<p>F1 =  0.5981308411214952</p>
</blockquote>
<blockquote>
<p>Accuracy =  0.57</p>
</blockquote>
<p>Why are the results going different? That's how I print the results:</p>
<pre><code>y_pred = mod.predict(X_test)

print('Precision = ', precision_score(y_test, y_pred))
print('Recall = ', recall_score(y_test, y_pred))
print('F1 = ', f1_score(y_test, y_pred))
print('Accuracy = ', mod.score(X_test, y_test))
</code></pre>
",54,0,0,4,python;machine-learning;scikit-learn;dataset,2022-05-16 20:45:25,2022-05-16 20:45:25,2022-05-16 20:45:25,just getting the samples and the labels without pipeline results  precision      recall      f      accuracy      with pipeline results  precision      recall      f      accuracy      why are the results going different  that s how i print the results 
608,608,18771355,72255640,"Keras VGG19: Node: &#39;Equal&#39; Incompatible shapes: [64,7,7] vs. [64,1]","<p>I am trying to implement a VGG19 model for image classification. To be honnest, I am not this confident about what I am doing. Whenever I try to run I have the following error (here full output):</p>
<pre><code>D:\logiciels\pycharm\IDH_hiv\hiv_venv\Scripts\python.exe D:/logiciels/pycharm/IDH_hiv/main.py
2022-05-16 09:27:30.951597: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-05-16 09:27:30.951716: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
loading images...
2022-05-16 09:27:44.410794: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-05-16 09:27:44.411737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2022-05-16 09:27:44.412877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2022-05-16 09:27:44.414421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2022-05-16 09:27:44.415896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found
2022-05-16 09:27:44.416760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found
2022-05-16 09:27:44.417606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2022-05-16 09:27:44.418467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2022-05-16 09:27:44.418593: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-05-16 09:27:44.419047: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Training the network...
Epoch 1/100
2022-05-16 09:27:47.130316: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 822083584 exceeds 10% of free system memory.
2022-05-16 09:27:47.371899: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 822083584 exceeds 10% of free system memory.
Traceback (most recent call last):
  File &quot;D:\logiciels\pycharm\IDH_hiv\main.py&quot;, line 21, in &lt;module&gt;
    procedures.vgg19(&quot;VGG19_test&quot;)
  File &quot;D:\logiciels\pycharm\IDH_hiv\procedures.py&quot;, line 228, in vgg19
    H = model.fit(aug.flow(trainX, trainY, batch_size=BS),
  File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node 'Equal' defined at (most recent call last):
    File &quot;D:\logiciels\pycharm\IDH_hiv\main.py&quot;, line 21, in &lt;module&gt;
      procedures.vgg19(&quot;VGG19_test&quot;)
    File &quot;D:\logiciels\pycharm\IDH_hiv\procedures.py&quot;, line 228, in vgg19
      H = model.fit(aug.flow(trainX, trainY, batch_size=BS),
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 1384, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 1021, in train_function
      return step_function(self, iterator)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 1010, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 1000, in run_step
      outputs = model.train_step(data)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 864, in train_step
      return self.compute_metrics(x, y, y_pred, sample_weight)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\training.py&quot;, line 957, in compute_metrics
      self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\engine\compile_utils.py&quot;, line 459, in update_state
      metric_obj.update_state(y_t, y_p, sample_weight=mask)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\utils\metrics_utils.py&quot;, line 70, in decorated
      update_op = update_state_fn(*args, **kwargs)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\metrics.py&quot;, line 178, in update_state_fn
      return ag_update_state(*args, **kwargs)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\metrics.py&quot;, line 729, in update_state
      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;D:\logiciels\pycharm\IDH_hiv\hiv_venv\lib\site-packages\keras\metrics.py&quot;, line 4086, in sparse_categorical_accuracy
      return tf.cast(tf.equal(y_true, y_pred), backend.floatx())
Node: 'Equal'
Incompatible shapes: [64,7,7] vs. [64,1]
     [[{{node Equal}}]] [Op:__inference_train_function_2058]
2022-05-16 09:27:58.097496: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.
     [[{{node PyFunc}}]]

Process finished with exit code 1
</code></pre>
<p>Here is my full code:</p>
<pre><code># the data is located in this data_dir
data_dir = &quot;E:/HIV-AI Project/20220330 Trial Images/PNG&quot;

# the output model and the graph is saved in this 'output_dir'
output_dir = &quot;Output/&quot;

print(&quot;loading images...&quot;)

data = []
labels = []

# grab the image paths and shuffle them
imagePaths = sorted(list(paths.list_images(data_dir)))
random.seed(2)
random.shuffle(imagePaths)

IMAGE_WIDTH, IMAGE_HEIGHT = 224, 224

for imagePath in imagePaths:
    image = cv2.imread(imagePath)
    image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))

    # append the image to the data list
    data.append(image)
    # extract label from the image path and update the labels list

    title = imagePath.split(os.path.sep)[1]
    label = &quot;&quot;
    if &quot;NI&quot; in title:
        label = &quot;NI&quot;
    else:
        label = &quot;INF&quot;

    labels.append(label)

# scale the raw pixel intensities to the range [0, 1]
data = np.array(data, dtype=&quot;float&quot;) / 255.0
labels = np.array(labels)
# Binarize labels
lb = LabelBinarizer()
labels = lb.fit_transform(labels)

# save the encoder to output directory
with open(os.path.join(output_dir, 'labels'), 'wb') as f:
    pickle.dump(lb, f)

# Randomly split the data into test and train sets (15% test and 85% train)
trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.15, random_state=42)

# construct the image generator for data augmentation
aug = ImageDataGenerator(rotation_range=45, width_shift_range=0.1,
                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
                         horizontal_flip=True, fill_mode=&quot;nearest&quot;)

# initialize our VGG-like Convolutional Neural Network
model = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    classes=2,
    classifier_activation='softmax')

# initialize our initial learning rate, # of epochs to train for,and batch size
INIT_LR = 0.0007
EPOCHS = 100
BS = 64


# initialize the model and optimizers
opt = Adam(learning_rate=INIT_LR, beta_1=0.9, beta_2=0.999, amsgrad=False)

# compile the model with loss function, optimizer and the evaluating metrics
model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=opt,
              metrics=[&quot;accuracy&quot;])

# train the network
print('Training the network...', np.shape(trainX), np.shape(trainY), np.shape(testX), np.shape(testY))
H = model.fit(aug.flow(trainX, trainY, batch_size=BS),
              validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,
              epochs=EPOCHS)

# Save the model locally for use later
model_path = os.path.join(output_dir, figname + '.h5')
model.save(model_path)

# evaluation
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))

# plot the training loss and accuracy
N = np.arange(0, EPOCHS)
plt.figure()
plt.plot(N, H.history[&quot;loss&quot;], label=&quot;train_loss&quot;)
plt.plot(N, H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;)
plt.plot(N, H.history[&quot;acc&quot;], label=&quot;train_acc&quot;)
plt.plot(N, H.history[&quot;val_acc&quot;], label=&quot;val_acc&quot;)
plt.title(&quot;Training/Validation Loss and Accuracy&quot;)
plt.xlabel(&quot;Epoch #&quot;)
plt.ylabel(&quot;Loss/Accuracy&quot;)
plt.legend()
plt.savefig(os.path.join(output_dir, figname + '.png'))
</code></pre>
<p>I do understand that there is a problem with the shape of the batches, but I don't know how to solve this.</p>
<p>Here are the shapes of trainX, trainY, testX, testY :
(408, 224, 224, 3) (408, 1) (72, 224, 224, 3) (72, 1)</p>
",48,1,0,5,python;machine-learning;keras;vgg-net;image-classification,2022-05-16 13:07:38,2022-05-16 13:07:38,2022-05-16 16:20:53,i am trying to implement a vgg model for image classification  to be honnest  i am not this confident about what i am doing  whenever i try to run i have the following error  here full output   here is my full code  i do understand that there is a problem with the shape of the batches  but i don t know how to solve this 
609,609,13345077,72210373,Nginx cofig can&#39;t read the static with 404 ERR_ABORTED,"<ul>
<li>system: ubuntu 18.04.5 LTS</li>
<li>nginx: nginx/1.14.0 (Ubuntu)</li>
</ul>
<p>Hi, I am trying Nginx to proxy reverse a service(doccano) to my prod path and need some help, below is the config file</p>
<pre><code>log_format  main_ext '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                    '$status $body_bytes_sent &quot;$http_referer&quot; '
                    '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; '
                     '&quot;host&quot; sn=&quot;$server_name&quot; ';

server {
  listen 8080 default_server;
  listen [::]:8080 default_server;

  access_log /var/log/nginx/access.log main_ext;
  error_log /var/log/nginx/error.log crit;

  
  index index.html index.htm index.nginx-debian.html;
  server_name &lt;my prod name&gt;;
  
  location / {
    try_files $uri $uri/ $uri/index.html $uri.html =404;
  }

  location /audience {
    ...
  }

  location /doccano {
    stub_status on;
    proxy_set_header        Host $host;
    proxy_set_header        X-Real-IP $remote_addr;
    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header        X-Forwarded-Proto $scheme;
    proxy_pass  http://127.0.0.1;
    proxy_connect_timeout   1800s;
    proxy_send_timeout      1800s;
    proxy_read_timeout      1800s;
    send_timeout            1800s;
  }
}

</code></pre>
<p>and when I open the the link on chrome it shows 404 with the browser console error message
<code>GET https://&lt;my prod name&gt;/_nuxt/08b13b1.js net::ERR_ABORTED 404</code></p>
<p>the curl seems OK to get the content of the service:</p>
<pre><code> ➜ curl http://127.0.0.1

&lt;!doctype html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;doccano - doccano&lt;/title&gt;&lt;meta data-n-head=&quot;1&quot; charset=&quot;utf-8&quot;&gt;&lt;meta data-n-head=&quot;1&quot; name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot;&gt;&lt;meta data-n-head=&quot;1&quot; data-hid=&quot;description&quot; name=&quot;description&quot; content=&quot;doccano is an open source annotation tools for machine learning practitioner.&quot;&gt;&lt;link data-n-head=&quot;1&quot; rel=&quot;icon&quot; type=&quot;image/x-icon&quot; href=&quot;/favicon.ico&quot;&gt;&lt;link rel=&quot;preload&quot; href=&quot;/_nuxt/14b976a.js&quot; as=&quot;script&quot;&gt;&lt;link rel=&quot;preload&quot; href=&quot;/_nuxt/08b13b1.js&quot; as=&quot;script&quot;&gt;&lt;link rel=&quot;preload&quot; href=&quot;/_nuxt/ac0f9d4.js&quot; as=&quot;script&quot;&gt;&lt;link rel=&quot;preload&quot; href=&quot;/_nuxt/7d3a60f.js&quot; as=&quot;script&quot;&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div id=&quot;__nuxt&quot;&gt;&lt;style&gt;#nuxt-loading{background:#fff;visibility:hidden;opacity:0;position:absolute;left:0;right:0;top:0;bottom:0;display:flex;justify-content:center;align-items:center;flex-direction:column;animation:nuxtLoadingIn 10s ease;-webkit-animation:nuxtLoadingIn 10s ease;animation-fill-mode:forwards;overflow:hidden}@keyframes nuxtLoadingIn{0%{visibility:hidden;opacity:0}20%{visibility:visible;opacity:0}100%{visibility:visible;opacity:1}}@-webkit-keyframes nuxtLoadingIn{0%{visibility:hidden;opacity:0}20%{visibility:visible;opacity:0}100%{visibility:visible;opacity:1}}#nuxt-loading&gt;div,#nuxt-loading&gt;div:after{border-radius:50%;width:5rem;height:5rem}#nuxt-loading&gt;div{font-size:10px;position:relative;text-indent:-9999em;border:.5rem solid #f5f5f5;border-left:.5rem solid #fff;-webkit-transform:translateZ(0);-ms-transform:translateZ(0);transform:translateZ(0);-webkit-animation:nuxtLoading 1.1s infinite linear;animation:nuxtLoading 1.1s infinite linear}#nuxt-loading.error&gt;div{border-left:.5rem solid #ff4500;animation-duration:5s}@-webkit-keyframes nuxtLoading{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes nuxtLoading{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}&lt;/style&gt;&lt;script&gt;window.addEventListener(&quot;error&quot;,function(){var e=document.getElementById(&quot;nuxt-loading&quot;);e&amp;&amp;(e.className+=&quot; error&quot;)})&lt;/script&gt;&lt;div id=&quot;nuxt-loading&quot; aria-live=&quot;polite&quot; role=&quot;status&quot;&gt;&lt;div&gt;Loading...&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;window.__NUXT__={config:{_app:{basePath:&quot;/&quot;,assetsPath:&quot;/_nuxt/&quot;,cdnURL:null}}}&lt;/script&gt;
  &lt;script src=&quot;/_nuxt/14b976a.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/_nuxt/08b13b1.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/_nuxt/ac0f9d4.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/_nuxt/7d3a60f.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>have anyone get the same error before?</p>
",54,1,0,3,nginx;nginx-reverse-proxy;doccano,2022-05-12 10:39:16,2022-05-12 10:39:16,2022-05-16 15:24:38,hi  i am trying nginx to proxy reverse a service doccano  to my prod path and need some help  below is the config file the curl seems ok to get the content of the service  have anyone get the same error before 
610,610,19123253,72255934,How to feed multiple training sets (which are with the same time series) into a machine learning model for training,"<p>Machine learning is completely new to me. I now have two targets which should be achieved with machine learning and some time-varying parameter of the district heating system (like mass flow, temperature difference, etc.):</p>
<ul>
<li>A. the fault types happen to the district heating network</li>
<li>B. How many buildings (percentage) failed</li>
</ul>
<p>(Where come from the Datasets: Suppose that different proportions of buildings have a certain fault. For example, 10% of buildings have leakage → get a dataset. 50% of buildings have fouling get another new data set ... (So all the datasets are labeled))(The time series is the same for all datasets - the full year of 2020 - in 10 minute intervals)</p>
<p>For these targets, I have two questions:</p>
<ul>
<li>question 1. <strong>How do I feed multiple datasets to the estimator for training?</strong></li>
<li>question 2. I think I can attach two labels to each dataset (as shown in the following table: 501 represents the buildings percentage, 510 represents the fault type) , and then use classification  at first and regression later as estimator, but <strong>how should these two methods be connected, and how should the model effectively be evaluated?  (since the evaluation method of classification and regression are totally different, should it be evaluated in two parts?) Maybe there are more suitable algorithms for my targets?</strong></li>
</ul>
<p>Beside, for question 1, I tried using <strong>pandas.concat()</strong> to concat all the datasets, but this method will repeat the same time series n times (n = number of datasets)in the first column, <strong>I am not sure if there is any potential problem? Is there a better way to feed multiple datasets to estimator?</strong></p>
<p>Looking forward to your answers (it would be great if you could attach the code), thank you very much for your time.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>                         m          Ta          Tb         dT        P   T     Tamb
2020-01-01 00:10:00 0.474645    368.140961  315.991455  52.149506   501 510 282.866669
2020-01-01 00:20:00 0.068980    368.116608  315.895203  52.221405   501 510 282.883331
2020-01-01 00:30:00 0.568058    368.088501  330.033783  38.054718   501 510 282.899994
2020-01-01 00:40:00 2.201882    368.028473  360.710754  7.317719    501 510 282.916656
2020-01-01 00:50:00 0.690575    367.948517  366.315552  1.632965    501 510 282.933319
... ... ... ... ... ... ... ...
2020-12-30 23:20:00 3.965592    350.273804  341.046326  9.227478    501 510 280.450012
2020-12-30 23:30:00 3.823962    350.284180  339.876129  10.408051   501 510 280.450012
2020-12-30 23:40:00 3.609059    350.288849  339.110016  11.178833   501 510 280.450012
2020-12-30 23:50:00 3.729008    350.377655  341.847198  8.530457    501 510 280.450012
2020-12-31 00:00:00 3.289396    350.510712  341.748199  8.762512    501 510 280.450012

52560 rows × 7 columns</code></pre>
</div>
</div>
</p>
",41,0,0,3,python;machine-learning;time-series,2022-05-16 13:34:13,2022-05-16 13:34:13,2022-05-16 13:39:07,machine learning is completely new to me  i now have two targets which should be achieved with machine learning and some time varying parameter of the district heating system  like mass flow  temperature difference  etc     where come from the datasets  suppose that different proportions of buildings have a certain fault  for example    of buildings have leakage   get a dataset    of buildings have fouling get another new data set      so all the datasets are labeled   the time series is the same for all datasets   the full year of    in  minute intervals  for these targets  i have two questions  beside  for question   i tried using pandas concat   to concat all the datasets  but this method will repeat the same time series n times  n   number of datasets in the first column  i am not sure if there is any potential problem  is there a better way to feed multiple datasets to estimator  looking forward to your answers  it would be great if you could attach the code   thank you very much for your time 
611,611,1040915,72254545,"`kubectl` almost-always hangs from controller node, but works reliably from dev laptop","<p><strong>TL;DR <code>kubectl</code> hangs indefinitely when calling controller node from itself, but operates normally when calling from another machine</strong></p>
<p>I'm getting started with learning Kubernetes. I've installed Raspbian 64-bit OS on a Raspberry Pi 4 8Gb RAM, and then installed <a href=""https://rancher.com/docs/k3s/latest/en/"" rel=""nofollow noreferrer"">k3s</a> (and nothing else) onto it as the master node (and installed k3s onto a 4Gb Pi4 as an worker node). Both Pis are fast and responsive when executing non-Kubernetes commands (and are connected to the same network via Ethernet), but <code>kubectl get nodes</code> from the controller node will almost-always (but not <em>always</em>-always) hang indefinitely (with no output).</p>
<p>Strangely, though, when I copy the configuration file from the controller node to my dev laptop and change the <code>clusters[0]cluster.server</code> address from using <code>127.0.0.1</code> to the appropriate domain name (as advised in &quot;<em>Accessing K3s from my dev laptop</em>&quot;, <a href=""https://anthonynsimon.com/blog/kubernetes-cluster-raspberry-pi/"" rel=""nofollow noreferrer"">here</a>), <code>kubectl</code> commands from my dev laptop complete reliably and quickly.</p>
<p>Is this expected performance of k8s/k3s? If not, what are some areas I can look into to make <code>kubectl</code> complete reliably from my controller node? I've tried replacing <code>127.0.0.1</code> in the controller node's kubeconfig with <code>localhost</code> or with the machine's assigned domain name - neither made a difference.</p>
",21,0,0,2,kubernetes;k3s,2022-05-16 11:05:34,2022-05-16 11:05:34,2022-05-16 11:05:34,tl dr kubectl hangs indefinitely when calling controller node from itself  but operates normally when calling from another machine i m getting started with learning kubernetes  i ve installed raspbian  bit os on a raspberry pi  gb ram  and then installed   and nothing else  onto it as the master node  and installed ks onto a gb pi as an worker node   both pis are fast and responsive when executing non kubernetes commands  and are connected to the same network via ethernet   but kubectl get nodes from the controller node will almost always  but not always always  hang indefinitely  with no output   strangely  though  when i copy the configuration file from the controller node to my dev laptop and change the clusters  cluster server address from using     to the appropriate domain name  as advised in  accessing ks from my dev laptop      kubectl commands from my dev laptop complete reliably and quickly  is this expected performance of ks ks  if not  what are some areas i can look into to make kubectl complete reliably from my controller node  i ve tried replacing     in the controller node s kubeconfig with localhost or with the machine s assigned domain name   neither made a difference 
612,612,1444859,14058985,Parsing a lisp file with Python,"<p>I have the following lisp file, which is from the <a href=""http://archive.ics.uci.edu/ml/datasets/Bach+Chorales"" rel=""noreferrer"">UCI machine learning database</a>.  I would like to convert it into a flat text file using python. A typical line looks like this:</p>

<pre><code>(1 ((st 8) (pitch 67) (dur 4) (keysig 1) (timesig 12) (fermata 0))((st 12) (pitch 67) (dur 8) (keysig 1) (timesig 12) (fermata 0)))
</code></pre>

<p>I would like to parse this into a text file like:</p>

<pre><code>time pitch duration keysig timesig fermata
8    67    4        1      12      0
12   67    8        1      12      0
</code></pre>

<p>Is there a python module to intelligently parse this? This is my first time seeing lisp.</p>
",4450,4,8,2,python;parsing,2012-12-27 23:24:38,2012-12-27 23:24:38,2022-05-16 05:20:47,i have the following lisp file  which is from the    i would like to convert it into a flat text file using python  a typical line looks like this  i would like to parse this into a text file like  is there a python module to intelligently parse this  this is my first time seeing lisp 
613,613,14404904,72248375,How to write excel formula to return column header name,"<p>I am looking to allow users to select the modules. Diving deeper, the users can select base on syllabus. Thereafter, it will tell them which courses (aka the column header), which matches the condition base on the rows with &quot;Y&quot;.</p>
<blockquote>
<blockquote>
<pre><code>Example  If the following is selected:
- Data Governance (Policy and Process)
- Data Analytics Project (Problem Statement Formulation)
- Data Exploration (Exploratory Analysis)
- Data Exploration (Machine Learning) 
 It will all return both CourseID1 &amp; CourseID2,  If Data Analytics Project (Data Handling &amp; Ethics) selected it will not return CourseID1. Only CourseID2 will be returned
</code></pre>
</blockquote>
</blockquote>
<p>I have attached a screen shot of the Excel Table. The flow is as follows:
step 1: user selects module
step 2: user select the syllabus that's tied to the module (a dependent selection from step 1)
step3: Output for the suitable courses is shown!</p>
<p><a href=""https://i.stack.imgur.com/6EHv3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6EHv3.png"" alt=""enter image description here"" /></a></p>
<p>How to write such formula?</p>
<p>The formula I used was wrong</p>
<pre><code>=INDEX($A$1:$E$1,SUMPRODUCT(MAX(($A$2:$E$72=&quot;Y&quot;)*($B$2:$B$72=B2)*(COLUMN($A$2:$E$72))))-COLUMN($A$1)+1) 
</code></pre>
<p>It does not return both CourseID1 and CourseID2 when the right criteria is being selected.</p>
",69,1,2,3,excel;excel-formula;vlookup,2022-05-15 18:14:12,2022-05-15 18:14:12,2022-05-15 22:29:49,i am looking to allow users to select the modules  diving deeper  the users can select base on syllabus  thereafter  it will tell them which courses  aka the column header   which matches the condition base on the rows with  y    how to write such formula  the formula i used was wrong it does not return both courseid and courseid when the right criteria is being selected 
614,614,16430823,72248726,How do I execute my TensorFlow Realtime Object Detection model on a browser,"<p>I'm a Machine learning beginner, working on my first ML project, I made a program that detects if people are wearing helmets in realtime (Using TensorFlow and OpenCV), now when I execute it, Python displays a new window using my camera, but now I want to execute it on a browser, just locally, I don't want to deploy it</p>
<p>This is the part of the code that does the execution and the display of the program :</p>
<pre><code>while True: 
    ret, frame = cap.read()
    image_np = np.array(frame)
    
    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections = detect_fn(input_tensor)
    
    num_detections = int(detections.pop('num_detections'))
    detections = {key: value[0, :num_detections].numpy()
                  for key, value in detections.items()}
    detections['num_detections'] = num_detections

    # detection_classes should be ints.
    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)

    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
                image_np_with_detections,
                detections['detection_boxes'],
                detections['detection_classes']+label_id_offset,
                detections['detection_scores'],
                category_index,
                use_normalized_coordinates=True,
                max_boxes_to_draw=5,
                min_score_thresh=.5,
                agnostic_mode=False)

    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))
    
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        cap.release()
        break
</code></pre>
<p>I don't know how to proceed from here, it works perfectly on my computer, but I don't know how I could switch to a browser, I don't know where to start from and would appreciate some help if possible</p>
",44,2,1,5,python;tensorflow;opencv;machine-learning;object-detection,2022-05-15 19:04:06,2022-05-15 19:04:06,2022-05-15 20:15:18,i m a machine learning beginner  working on my first ml project  i made a program that detects if people are wearing helmets in realtime  using tensorflow and opencv   now when i execute it  python displays a new window using my camera  but now i want to execute it on a browser  just locally  i don t want to deploy it this is the part of the code that does the execution and the display of the program   i don t know how to proceed from here  it works perfectly on my computer  but i don t know how i could switch to a browser  i don t know where to start from and would appreciate some help if possible
615,615,8358447,56772967,Converting ImageProxy to Bitmap,"<p>So, I wanted to explore new Google's Camera API - <code>CameraX</code>.
What I want to do, is take an image from camera feed every second and then pass it into a function that accepts bitmap for machine learning purposes. </p>

<p>I read the documentation on <code>Camera X</code> Image Analyzer:</p>

<blockquote>
  <p>The image analysis use case provides your app with a CPU-accessible
  image to perform image processing, computer vision, or machine
  learning inference on. The application implements an Analyzer method
  that is run on each frame.</p>
</blockquote>

<p>..which basically is what I need. So, I implemented this image analyzer like this:</p>

<pre><code>imageAnalysis.setAnalyzer { image: ImageProxy, _: Int -&gt;
    viewModel.onAnalyzeImage(image)
}
</code></pre>

<p>What I get is <code>image: ImageProxy</code>. How can I transfer this <code>ImageProxy</code> to <code>Bitmap</code>?</p>

<p>I tried to solve it like this:</p>

<pre><code>fun decodeBitmap(image: ImageProxy): Bitmap? {
    val buffer = image.planes[0].buffer
    val bytes = ByteArray(buffer.capacity()).also { buffer.get(it) }
    return BitmapFactory.decodeByteArray(bytes, 0, bytes.size)
}
</code></pre>

<p>But it returns <code>null</code> - because <code>decodeByteArray</code> does not receive valid (?) bitmap bytes. Any ideas?</p>
",27845,9,38,2,android;android-camerax,2019-06-26 18:04:35,2019-06-26 18:04:35,2022-05-15 16:34:24,i read the documentation on camera x image analyzer    which basically is what i need  so  i implemented this image analyzer like this  what i get is image  imageproxy  how can i transfer this imageproxy to bitmap  i tried to solve it like this  but it returns null   because decodebytearray does not receive valid     bitmap bytes  any ideas 
616,616,156458,29051564,Learning a regex from strings?,"<p>Given some strings, is there some algorithm (and program that implement such an algorithm) that can create a regex which matches some of the given strings and not the other given strings?  </p>

<p>I like the regex can generalize well on other strings not given (just like machine learning algorithms)</p>

<p>I am not restricted to any particular regex flavor.  Thanks.</p>
",81,1,-1,2,regex;machine-learning,2015-03-14 22:21:28,2015-03-14 22:21:28,2022-05-15 16:31:56,given some strings  is there some algorithm  and program that implement such an algorithm  that can create a regex which matches some of the given strings and not the other given strings    i like the regex can generalize well on other strings not given  just like machine learning algorithms  i am not restricted to any particular regex flavor   thanks 
617,617,3536583,27847565,basic feature selection or dimensionality reduction previous to machine learning,"<p>I am analyzing a group of stocks which share many intrinsic features and also adding external datasets that could expand data points in the original dataset. I have the following dataframe, using a made up example in Pandas:</p>

<pre><code>%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#A = INTEL, #B = IBM, #C = MSFT, #D = AAPL, #E=AIG, #F=GS
df = pd.DataFrame({'A' : ['IBM', 'INTEL', 'MSFT', 'INTEL',
                         'AAPL', 'INTEL', 'MSFT', 'IBM','INTEL','AAPL'],
                    'B' : np.random.randn(10),
                    'C' : np.random.randn(10),
                    'D' : np.random.randn(10),
                    'E' : np.random.randn(10)})
</code></pre>

<p>which produces the following dataset:</p>

<p><img src=""https://i.stack.imgur.com/iQvGI.png"" alt=""enter image description here""></p>

<p>My real dataset might contain >100 features (columns). The question: Is there a pythonic way to visualize salient features of the dataset so I work with a reduced matrix?</p>
",583,1,0,4,python;numpy;matrix;pandas,2015-01-09 00:29:08,2015-01-09 00:29:08,2022-05-15 16:31:41,i am analyzing a group of stocks which share many intrinsic features and also adding external datasets that could expand data points in the original dataset  i have the following dataframe  using a made up example in pandas  which produces the following dataset   my real dataset might contain   features  columns   the question  is there a pythonic way to visualize salient features of the dataset so i work with a reduced matrix 
618,618,14225979,72191149,How do I install coremltools version 4.0 using pip without install errors on my mac?,"<p>I have used pyenv to create a shell environment of python 2.7.13 but when I do <code>pip install 'coremltools==4.0'</code> I get five error messages as below;</p>
<pre><code>ERROR: Exception:
Traceback (most recent call last):
  File &quot;/Users/MyUsername/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pip/_internal/cli/base_command.py&quot;, line 223, in _main
    status = self.run(options, args)
  File &quot;/Users/MyUsername/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pip/_internal/cli/req_command.py&quot;, line 180, in wrapper
    return func(self, options, args)
  File &quot;/Users/MyUsername/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pip/_internal/commands/install.py&quot;, line 321, in run
    reqs, check_supported_wheels=not options.target_dir
  File &quot;/Users/MyUsername/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pip/_internal/resolution/legacy/resolver.py&quot;, line 180, in resolve
    discovered_reqs.extend(self._resolve_one(requirement_set, req))
  File &quot;/Users/MyUsername/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pip/_internal/resolution/legacy/resolver.py&quot;, line 419, in _resolve_one
    assert req_to_install.user_supplied
AssertionError

</code></pre>
<p>I think there is some compatibility issue between Python 2.7.13 and coremltools 4.0.</p>
<p>I am following Angela Yu's course on iOS Development and I am at the section on machine learning where she uses Python 2.7 and coremltools to convert a Caffe model into a .mlmodel file. But the course is nearly 3 years old and my mac doesn't have the same version of Python and coreltools.</p>
<p>I am completely unfamiliar with Python and I would really appreciate some help.</p>
",91,2,0,3,python-2.7;pip;coremltools,2022-05-10 23:39:24,2022-05-10 23:39:24,2022-05-15 12:38:32,i have used pyenv to create a shell environment of python    but when i do pip install  coremltools     i get five error messages as below  i think there is some compatibility issue between python    and coremltools    i am following angela yu s course on ios development and i am at the section on machine learning where she uses python   and coremltools to convert a caffe model into a  mlmodel file  but the course is nearly  years old and my mac doesn t have the same version of python and coreltools  i am completely unfamiliar with python and i would really appreciate some help 
619,619,14923227,72240965,Conversion between binary vector and 128 bit number,"<p>Is there a way to convert back and forth between a binary vector and a 128-bit number? I have the following binary vector:</p>
<pre><code>import numpy as np

bits = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
                 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
                 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
                 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
                 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
                 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1], dtype=np.uint8)
</code></pre>
<p>which is a MD5 hash that I am trying use as a feature for a scikit-learn machine learning classifier (I need to represent the hash as a single feature).</p>
",53,1,0,3,python;numpy;scikit-learn,2022-05-14 19:35:18,2022-05-14 19:35:18,2022-05-15 02:31:55,is there a way to convert back and forth between a binary vector and a  bit number  i have the following binary vector  which is a md hash that i am trying use as a feature for a scikit learn machine learning classifier  i need to represent the hash as a single feature  
620,620,14738799,72242636,How do engineered features help when they are not present in the test data,"<p>I am trying to classify between drones and birds using machine learning. I have got a big number of samples of feature vectors from a  radar which generally consists of position(x,y,z), velocity(vx,vy,vz), acceleration(ax,ay,az), Noise, SNR etc plus some more features. Actual classes are known for the samples. However, These basic features are not able to distinguish between drones and birds for new(out of bag) samples. so I am going to try feature engineering to generate new features like standard deviation of speed calculated using mean-speed and then uses the difference between mean-speed and speeds obtained from individual samples(of the same track) to calculate standard deviation by averaging out the differences . Similarly, I generate new features using some other formula by using sum or difference or deviation from average(of different samples from same track) etc.</p>
<p>After obtaining these features we will use the same to create a trained model which will be used for classification.</p>
<p>However, I can apply all these feature engineering on the training dataset whereas the same engineered features will not be present in the test dataset obtained in the operational scenario where we get one sample after another. Also in operational scenario we do not know how many samples we will be getting for a track.
So, how can these engineered features be obtained so as to create a test feature vector with the same in actual operational scenario.</p>
<p><em>If these cannot be obtained while testing ,then how will the same engineered features (used for model training) be able to solve the classification problem when we do not have these in the test data?</em></p>
",19,0,0,2,machine-learning;feature-engineering,2022-05-14 23:14:19,2022-05-14 23:14:19,2022-05-14 23:14:19,i am trying to classify between drones and birds using machine learning  i have got a big number of samples of feature vectors from a  radar which generally consists of position x y z   velocity vx vy vz   acceleration ax ay az   noise  snr etc plus some more features  actual classes are known for the samples  however  these basic features are not able to distinguish between drones and birds for new out of bag  samples  so i am going to try feature engineering to generate new features like standard deviation of speed calculated using mean speed and then uses the difference between mean speed and speeds obtained from individual samples of the same track  to calculate standard deviation by averaging out the differences   similarly  i generate new features using some other formula by using sum or difference or deviation from average of different samples from same track  etc  after obtaining these features we will use the same to create a trained model which will be used for classification  if these cannot be obtained while testing  then how will the same engineered features  used for model training  be able to solve the classification problem when we do not have these in the test data 
621,621,12989056,71212588,Video datasets in Python,"<p>I am a new to deep learning algorithms and Machine learning as well as working with data. I am currently trying to work with annotated video dataset, I tried to have a simple example on How I should get started. I am aware that to work with video dataset, we will first need to extract the images from videos and then do the image processing. However, as I am new it is still difficult for me to understand the steps. I came accross this link, it is great but the data is really large and it cannot be downloaded on my computer.
<a href=""https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/</a></p>
<p>Any suggestions to a walk through examples I can use to build my understanding and Know how to deal with these datasets</p>
",94,1,0,3,python;video;dataset,2022-02-22 01:54:17,2022-02-22 01:54:17,2022-05-14 20:33:42,any suggestions to a walk through examples i can use to build my understanding and know how to deal with these datasets
622,622,14923227,72241144,RandomForestClassifer with large feature datatypes,"<p>Is it possible to mix small datatypes (such as bits) and long datatypes (such as 256-bit hashes) when using a machine learning model in scikit-learn such as the RandomForestClassifier?</p>
<p>I have the following scenario:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier()

X = [[1, 2, 3, 'verylongfeature1'], [1, 1, 2, 'verylongfeature2']]
y = [1, 0]

clf.fit(X,y)
</code></pre>
<p>which gives the following error:</p>
<pre><code>ValueError: could not convert string to float: 'verylongfeature1'
</code></pre>
<p>Is the RandomForestClassifier limited to 64-bit float input features?</p>
",17,0,0,1,scikit-learn,2022-05-14 20:01:21,2022-05-14 20:01:21,2022-05-14 20:01:21,is it possible to mix small datatypes  such as bits  and long datatypes  such as  bit hashes  when using a machine learning model in scikit learn such as the randomforestclassifier  i have the following scenario  which gives the following error  is the randomforestclassifier limited to  bit float input features 
623,623,14979393,71365407,How to extract a set of *.tar.gz.(letters) files?,"<p>I have downloaded a medical data set to use in machine learning and the files are like this:</p>
<pre><code>dicom_v1.tar.gz.aa
dicom_v1.tar.gz.ab
dicom_v1.tar.gz.ac
dicom_v1.tar.gz.ad
</code></pre>
<p>I don't know how to extract these files. When I use <em>WinRAR</em> or <em>7-Zip</em>, it doesn't work.</p>
<p>It is written in the attached <code>ReadMe</code> file:</p>
<blockquote>
<p>To decompress this data set execute in a Unix command line:<br />
<code>cat dicom_archive.tar.* | tar -xzvf</code></p>
</blockquote>
<p>How can I do this in Windows 10 as I am a beginner?</p>
",150,1,0,2,dataset;extract,2022-03-06 01:05:14,2022-03-06 01:05:14,2022-05-14 17:26:22,i have downloaded a medical data set to use in machine learning and the files are like this  i don t know how to extract these files  when i use winrar or  zip  it doesn t work  it is written in the attached readme file  how can i do this in windows  as i am a beginner 
624,624,2722870,64543886,Text recommendation based on keywords,"<p>I need some advice on the following problem.</p>
<p>I'm given a set of weighted keywords (by percentage) and need to find a text in a database that best matches those keywords. I will give an example.</p>
<p>I'm presented with these keywords</p>
<ul>
<li>Sun(90%)</li>
<li>National Park(85% some keywords contain 2 words)</li>
<li>Landmark(60%)</li>
</ul>
<p>Now lets say my database contains 3 entries of texts e.g</p>
<ol>
<li>Going-to-the-Sun Road is a scenic mountain road in the Rocky Mountains of the western United States, in Glacier National Park in Montana.</li>
<li>Everybody has a little bit of the sun and moon in them. Everybody has a little bit of man, woman, and animal in them.</li>
<li>A hybrid car is one that uses more than one means of propulsion - that means combining a petrol or diesel engine with an electric motor.</li>
</ol>
<p>Obviously the first text is the one that best describes the given set of keywords so this is what I want to recommend to the user. Following the second text that somewhat relates with the &quot;sun&quot; keyword and that could be an acceptable choice too.</p>
<p>The 3rd text is totally irrelevant and shall only be recommended as a last resort when everything else fails.</p>
<p>I'm totally new to that kind of stuff so I need some advice as to which technologies/algorithms I should use. Seems like there is some machine learning (nlp) involved or some kind of fuzzy logic. I'm not really sure.</p>
",53,1,0,4,machine-learning;nlp;text-mining;fuzzy-search,2020-10-27 01:07:02,2020-10-27 01:07:02,2022-05-14 16:26:06,i need some advice on the following problem  i m given a set of weighted keywords  by percentage  and need to find a text in a database that best matches those keywords  i will give an example  i m presented with these keywords now lets say my database contains  entries of texts e g obviously the first text is the one that best describes the given set of keywords so this is what i want to recommend to the user  following the second text that somewhat relates with the  sun  keyword and that could be an acceptable choice too  the rd text is totally irrelevant and shall only be recommended as a last resort when everything else fails  i m totally new to that kind of stuff so i need some advice as to which technologies algorithms i should use  seems like there is some machine learning  nlp  involved or some kind of fuzzy logic  i m not really sure 
625,625,18953434,72186711,How could I know the code in the bob like bob.pipelines.CheckpointWrapper?,"<p>How to access the code in the bob? Like,I want to know the return of <em>bob.pipelines.CheckpointWrapper</em>. But I just find the Parameters Interpretation.</p>
<p>Note: Bob is a free signal-processing and machine learning toolbox</p>
<p><a href=""https://i.stack.imgur.com/USerm.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
",17,1,0,1,python-bob,2022-05-10 18:24:50,2022-05-10 18:24:50,2022-05-14 16:19:16,how to access the code in the bob  like i want to know the return of bob pipelines checkpointwrapper  but i just find the parameters interpretation  note  bob is a free signal processing and machine learning toolbox 
626,626,7208958,72239395,Keras FER-2013 model predict for a single image,"<p>i'm pretty new to machine learning. I followed a tutorial to classify if the user is similing or not. I created this code:</p>
<pre><code>def get_model(input_size, classes=7):
model = Sequential()   

model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape =input_size))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2, 2))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(classes, activation='softmax'))

#Compliling the model
model.compile(optimizer=Adam(lr=0.0001, decay=1e-6), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
return model
</code></pre>
<p>if i try to predict an array from flow_from_directory its working fine but i would like to predict it using the following code:</p>
<pre><code>final_image = cv2.imread('./tesimg.jpeg')
final_image = np.expand_dims(final_image, axis=0)
final_image = final_image/255.0
</code></pre>
<p>The problem is that i'm getting this error:</p>
<blockquote>
<p>UnimplementedError: Graph execution error:</p>
</blockquote>
",25,0,0,2,tensorflow;keras,2022-05-14 16:04:13,2022-05-14 16:04:13,2022-05-14 16:04:13,i m pretty new to machine learning  i followed a tutorial to classify if the user is similing or not  i created this code  if i try to predict an array from flow_from_directory its working fine but i would like to predict it using the following code  the problem is that i m getting this error  unimplementederror  graph execution error 
627,627,19084912,72185226,How can I convert this lakh into actual price with int datatype,"<p>I was trying to convert this column values into actual numbers so that I can used this number for machine learning algorithm.
This label is actually what I want to predict from my machine learning algorithm, so I wanted to give this as input to my model to train them before the actual price prediction but here the range of price is given which is what I am finding difficult to convert, Can you help me how can I convert this combination of number and text to actual proper number with int data type(currently having object as data type)</p>
<p>About this Dataset: This is the dataset of all the used car which was sold to the customer at what price and what is the same car price if you buy a new car. so I wanted to create a model in which user give data about new-car price of that range, car-company name and many more other fields of label from which my model give expected price of Used car.</p>
<p>But I am stuck what can I do with this field of data as this is the range and I cannot drop it as it is one of the main factor to decide used car price.</p>
<p>Rs means Indian Rupees(similar to Dollar)</p>
<p>10 Lakh=1 million
OR</p>
<p>1 Lakh = 100 Thousands</p>
<p><a href=""https://i.stack.imgur.com/1WlLG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1WlLG.png"" alt=""image of two column I am talking about"" /></a></p>
",48,1,0,1,python,2022-05-10 16:35:42,2022-05-10 16:35:42,2022-05-14 08:33:51,about this dataset  this is the dataset of all the used car which was sold to the customer at what price and what is the same car price if you buy a new car  so i wanted to create a model in which user give data about new car price of that range  car company name and many more other fields of label from which my model give expected price of used car  but i am stuck what can i do with this field of data as this is the range and i cannot drop it as it is one of the main factor to decide used car price  rs means indian rupees similar to dollar   lakh    thousands 
628,628,18900893,72236690,extract attributes from list,"<p>I am a novice in python and I treat to extract emotions from list and put each attribute to its corresponding series to train machine learning algorithms
for example this one instance in list and I want put anger in new series with its value= 0.013736263736263736 and so on.</p>
<p>&quot;{'anger':&quot;,
'0.013736263736263736,',
&quot;'anticipation':&quot;,
'0.0027472527472527475,',
&quot;'disgust':&quot;,
'0.03296703296703297,',
&quot;'fear':&quot;,
'0.0027472527472527475,',
&quot;'joy':&quot;,
'0.0,',
&quot;'negative':&quot;,
'0.06043956043956044,',
&quot;'positive':&quot;,
'0.019230769230769232,',
&quot;'sadness':&quot;,
'0.0027472527472527475,',
&quot;'surprise':&quot;,
'0.008241758241758242,',
&quot;'trust':&quot;,
'0.019230769230769232}']
thanks in advance</p>
",26,0,0,3,python;list;extract,2022-05-14 06:55:45,2022-05-14 06:55:45,2022-05-14 06:55:45,
629,629,14252319,72234795,Extract multiple start date and end date from a string in python?,"<p>I am making a resume parser but I want to know the years of experience of the person from the experience section and want results like if there are 3 years of experience is mentioned and there are 3 companies the person worked in those 3 years and there are the start and end date mentioned on all of them so is there any way to know this is the start date and this is an end date and also can I calculate the total years of experience mentioned in the experience section by adding all those ranges.</p>
<p>Example field</p>
<blockquote>
<p><strong>Experience</strong></p>
</blockquote>
<blockquote>
<p>AI and Machine learning Intern, Dawn DigiTech (04/2022 -present), ❖, This company digitally transform multiple front- and back-office business, processes, SCM, ERP and Manufacturing Excellence., -, SpiceJet(08/2020 - 10/2021), ❖, Leading Indian airlines company worked and Developedy of 30%, Machine learning Intern, TutorBin(02/2022 - 05/2022 ), ❖, Tutorbin is an integrated online tutoring platform serving as a one-stop, solution for students and online tutors. work on Ai and Machine learning, tasks provided by the client, 60%,</p>
</blockquote>
<p>This is the parsed experience section so in this I want to extract dates ranges which should know the start date and end date and also return the total experience mentioned which is :</p>
<pre><code>    start date      end date
    (04/2022   -     present)    =  2 months
    (08/2020   -     10/2021)    =  1 year 2months
   (02/2022    -     04/2022)    =  2 months
Total experience  =  1 year 6 months
</code></pre>
<p>So, is there any way to get this output in total years of experience and get to know in date range which is the start date and which is the end date?</p>
<p>Thanks in advance.</p>
",42,1,0,4,python;django;machine-learning;nlp,2022-05-14 01:23:37,2022-05-14 01:23:37,2022-05-14 01:37:44,i am making a resume parser but i want to know the years of experience of the person from the experience section and want results like if there are  years of experience is mentioned and there are  companies the person worked in those  years and there are the start and end date mentioned on all of them so is there any way to know this is the start date and this is an end date and also can i calculate the total years of experience mentioned in the experience section by adding all those ranges  example field experience ai and machine learning intern  dawn digitech     present      this company digitally transform multiple front  and back office business  processes  scm  erp and manufacturing excellence      spicejet            leading indian airlines company worked and developedy of    machine learning intern  tutorbin             tutorbin is an integrated online tutoring platform serving as a one stop  solution for students and online tutors  work on ai and machine learning  tasks provided by the client     this is the parsed experience section so in this i want to extract dates ranges which should know the start date and end date and also return the total experience mentioned which is   so  is there any way to get this output in total years of experience and get to know in date range which is the start date and which is the end date  thanks in advance 
630,630,19111008,72231803,Instruct WebApp using Django to run the code locally and return results,"<p>I am a newbie to web development and python scripts. I am learning to host python scripts using Django. I would like to know how to instruct Django to run the script locally instead of running it on the server.</p>
<p>For e.g. User visits the web app &gt; enters parameters and submits the form &gt; script should run locally from the client machine and return results back on the web app.</p>
<p>The purpose of my python script is to test the network port status locally on the user machine's private network and return the results.</p>
<p>Appreciate your advise.</p>
",18,0,0,2,python-3.x;django,2022-05-13 20:41:20,2022-05-13 20:41:20,2022-05-13 20:41:20,i am a newbie to web development and python scripts  i am learning to host python scripts using django  i would like to know how to instruct django to run the script locally instead of running it on the server  for e g  user visits the web app  gt  enters parameters and submits the form  gt  script should run locally from the client machine and return results back on the web app  the purpose of my python script is to test the network port status locally on the user machine s private network and return the results  appreciate your advise 
631,631,18621668,72160742,Sklearn can&#39;t convert string to float,"<p>I'm using Sklearn as a machine learning tool, but every time I run my code, it gives this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\FakeUserMadeUp\Desktop\Python\Machine Learning\MachineLearning.py&quot;, line 12, in &lt;module&gt;
    model.fit(X_train, Y_train)
  File &quot;C:\Users\FakeUserMadeUp\AppData\Roaming\Python\Python37\site-packages\sklearn\tree\_classes.py&quot;, line 942, in fit
    X_idx_sorted=X_idx_sorted,
  File &quot;C:\Users\FakeUserMadeUp\AppData\Roaming\Python\Python37\site-packages\sklearn\tree\_classes.py&quot;, line 166, in fit
    X, y, validate_separately=(check_X_params, check_y_params)
  File &quot;C:\Users\FakeUserMadeUp\AppData\Roaming\Python\Python37\site-packages\sklearn\base.py&quot;, line 578, in _validate_data
    X = check_array(X, **check_X_params)
  File &quot;C:\Users\FakeUserMadeUp\AppData\Roaming\Python\Python37\site-packages\sklearn\utils\validation.py&quot;, line 746, in check_array
    array = np.asarray(array, order=order, dtype=dtype)
  File &quot;C:\Users\FakeUserMadeUp\AppData\Roaming\Python\Python37\site-packages\pandas\core\generic.py&quot;, line 1993, in __ array __
    return np.asarray(self._values, dtype=dtype)
ValueError: could not convert string to float: 'Paris'
</code></pre>
<p>Here is the code, and down below there's my dataset:</p>
<p>(I've tried multiple different datasets, also, this dataset is a txt because I made it myself and am to dumb to convert it to csv.)</p>
<pre><code>    import pandas as pd
    from sklearn.tree import DecisionTreeClassifier as dtc
    from sklearn.model_selection import train_test_split as tts

    city_data = pd.read_csv('TimeZoneTable.txt')
    X = city_data.drop(columns=['Country'])
    Y = city_data['Country']

    X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2)

    model = dtc()
    model.fit(X_train, Y_train)
    predictions = model.predict(X_test)

    print(Y_test)
    print(predictions)
</code></pre>
<p>Dataset:</p>
<pre><code>CityName,Country,Latitude,Longitude,TimeZone

Moscow,Russia,55.45'N,37.37'E,3

Vienna,Austria,48.13'N,16.22'E,2

Barcelona,Spain,41.23'N,2.11'E,2

Madrid,Spain,40.25'N,3.42'W,2

Lisbon,Portugal,38.44'N,9.09'W,1

London,UK,51.30'N,0.08'W,1

Cardiff,UK,51.29'N,3.11'W,1

Edinburgh,UK,55.57'N,3.11'W,1

Dublin,Ireland,53.21'N,6.16'W,1

Paris,France,48.51'N,2.21'E,2
</code></pre>
",57,1,-1,4,python;csv;artificial-intelligence;sklearn-pandas,2022-05-08 17:25:10,2022-05-08 17:25:10,2022-05-13 20:08:20,i m using sklearn as a machine learning tool  but every time i run my code  it gives this error  here is the code  and down below there s my dataset   i ve tried multiple different datasets  also  this dataset is a txt because i made it myself and am to dumb to convert it to csv   dataset 
632,632,8103611,72167773,How to create one PredictEnginePool which serves multiple models？,"<p>My project is an online foods order app, the key feature of this app is the &quot;Daily nutrients intake monitor&quot;. This monitor shows the differences of daily intake recommendation values of 30 types of nutrients vs the actual nutrients contains from the foods in user's shoppingcart.</p>
<p>I created 30 models base on those nutrients and each one of them has an InputData which inherits from a base class - NutrientInputDataBase, below is the example of Added sugar InputData class and the base class:</p>
<pre><code>public class AddedSugarUlInputData : NutrientInputDataBase
{
    [ColumnName(@&quot;AddedSugar-AMDR-UL&quot;)]
    public float AddedSugar_AMDR_UL { get; set; }
}

public class NutrientInputDataBase
{
    [ColumnName(@&quot;Sex&quot;)]
    public float Sex { get; set; }

    [ColumnName(@&quot;Age&quot;)]
    public float Age { get; set; }

    [ColumnName(@&quot;Activity&quot;)]
    public float Activity { get; set; }

    [ColumnName(@&quot;BMI&quot;)]
    public float BMI { get; set; }

    [ColumnName(@&quot;Disease&quot;)]
    public float Disease { get; set; }
}
</code></pre>
<p>From the official documents:
<a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/serve-model-web-api-ml-net"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/serve-model-web-api-ml-net</a>
i understood that i need to create a 'PredictionEnginePool'  and i already know how to register the PredictionEnginePool in the application startup file.
My app logic is when user added or removed an item from the shoppingcart, the front end will request the api, the backend will get the user profile first(to obtain the input data for the prediction), then return a packaged objects which contains all 30 types of nutrients prediction results.</p>
<p>My question is, should i register the PredictionEnginePool for each one of the nutrient model individually in the Startup file? or in anyother effecient way which i haven't be awared of?</p>
",29,1,0,4,c#;model;webapi;ml.net,2022-05-09 12:01:59,2022-05-09 12:01:59,2022-05-13 18:44:29,my project is an online foods order app  the key feature of this app is the  daily nutrients intake monitor   this monitor shows the differences of daily intake recommendation values of  types of nutrients vs the actual nutrients contains from the foods in user s shoppingcart  i created  models base on those nutrients and each one of them has an inputdata which inherits from a base class   nutrientinputdatabase  below is the example of added sugar inputdata class and the base class  my question is  should i register the predictionenginepool for each one of the nutrient model individually in the startup file  or in anyother effecient way which i haven t be awared of 
633,633,8393866,71576026,Running keras model on ubuntu VM (UTM) on Mac with M1 chip,"<p>I have a Mac with an M1 Pro chip. I was able to install keras/tensorflow with tensorflow-metal PluggableDevice. My image classification model runs smoothly on my M1.
<img src=""https://i.stack.imgur.com/IE2dn.png"" alt=""enter image description here"" /></p>
<p>Because of some reasons I also need to run code on an Ubuntu VM machine - I used UTM and install ubuntu 20.04 with the UTM guide. In that VM I successfully installed keras/tensorflow.</p>
<p>When I run exactly the same code with exactly the same data as on my M1 inside Ubuntu I don't get any errors but the loss is exploding:
<img src=""https://i.stack.imgur.com/gFjsR.jpg"" alt=""enter image description here"" /></p>
<p>I tried every technique that I know for loss explosion:</p>
<ul>
<li>gradient by norm clipping</li>
<li>smaller learning rate</li>
<li>normalization of network outputs</li>
<li>different optimizer</li>
</ul>
<p>None of the above helped me and on Ubuntu, this model is just not learning. Does anyone properly set Ubuntu on their Mac with M1 that keras can properly run?</p>
",114,1,0,5,python;keras;virtual-machine;apple-m1;ubuntu-20.04,2022-03-22 22:28:43,2022-03-22 22:28:43,2022-05-13 18:29:01,because of some reasons i also need to run code on an ubuntu vm machine   i used utm and install ubuntu   with the utm guide  in that vm i successfully installed keras tensorflow  i tried every technique that i know for loss explosion  none of the above helped me and on ubuntu  this model is just not learning  does anyone properly set ubuntu on their mac with m that keras can properly run 
634,634,9010784,72228743,Partial Derivative term in the Gradient Descent Algorithm,"<p>I'm learning the &quot;Machine Learning - Andrew Ng&quot; course from Coursera. In the lesson called &quot;Gradient Descent&quot;, I've found the formula a bit complicated. The theorem is consist of &quot;<strong>partial derivative</strong>&quot; term. <br/>
The problem for me to understand the calculation of partial derivative term. Thus, later the term is calculated as  ​</p>
<blockquote>
<p><strong>1/m *
∑
​
(h
θ
​
(x) − y(i)
)²</strong></p>
</blockquote>
<br/>
<p>My question is, &quot;How did the <strong>1/2m</strong> from the 'Cost Function' becomes <strong>1/m</strong> while calculating the partial derivative inside the <strong>Gradient Descent</strong> theorem?&quot;</p>
",25,1,-1,2,machine-learning;gradient-descent,2022-05-13 16:51:18,2022-05-13 16:51:18,2022-05-13 17:04:57,my question is   how did the  m from the  cost function  becomes  m while calculating the partial derivative inside the gradient descent theorem  
635,635,9441168,72132392,feature importance in mlr3 with iml for classification forests,"<p>I calculate feature importance for 2 different types of machine learning models (SVM and Classification Forest). I cannot post the data here, but I describe what I do:</p>
<p>My (classification) task has about 400 observations of 70 variables. Some of them are highly, but nor perfectly correlated</p>
<ol>
<li>I fit the models with</li>
</ol>
<pre><code>learner_1$train(task)
learner_2$train(task)
</code></pre>
<p>where learner1 is a svm and learner 2 is a classification forest.</p>
<ol start=""2"">
<li>Now, I want to calculate feature importance with iml, so for each of the learners I use the following code (here the code for learner_1)</li>
</ol>
<pre><code>  
  model_analyzed=Predictor$new(learner_1,
                               data=dplyr::select(task$data(), task$feature_names), 
                               y=dplyr::select(task$data(), task$target_names))
  
  used_features &lt;- task$feature_names
  
  
  effect = FeatureImp$new(model_analyzed, loss=&quot;ce&quot;, n.repetitions=10, compare=&quot;ratio&quot;)
  print(effect$plot(features=used_features))
</code></pre>
<p>My results are the following</p>
<p>a) For the SVM</p>
<p><a href=""https://i.stack.imgur.com/YFC10.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YFC10.png"" alt=""SVM"" /></a></p>
<p>b) For the classification forest
<a href=""https://i.stack.imgur.com/iedxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iedxY.png"" alt=""classification forest"" /></a></p>
<p>I do not understand the second picture:</p>
<p>a) should the &quot;anchor&quot; point not be around 1, as I observe for the SVM? If the ce is not made worse by shuffling for any feature, then the graph shoud show a 1 and not a 0?</p>
<p>b) If all features show a value very close to zero, as I see in the second graph, does it mean that the classification error is zero, if the feature is shuffled? So for each single feature, I would get a perfect model if just this one feature is omitted or shuffled?</p>
<p>I am really confused here, can someone help me understand what happens?</p>
",42,0,1,5,r;machine-learning;ensemble-learning;mlr3;iml,2022-05-06 00:33:13,2022-05-06 00:33:13,2022-05-13 15:12:17,i calculate feature importance for  different types of machine learning models  svm and classification forest   i cannot post the data here  but i describe what i do  my  classification  task has about  observations of  variables  some of them are highly  but nor perfectly correlated where learner is a svm and learner  is a classification forest  my results are the following a  for the svm  i do not understand the second picture  a  should the  anchor  point not be around   as i observe for the svm  if the ce is not made worse by shuffling for any feature  then the graph shoud show a  and not a   b  if all features show a value very close to zero  as i see in the second graph  does it mean that the classification error is zero  if the feature is shuffled  so for each single feature  i would get a perfect model if just this one feature is omitted or shuffled  i am really confused here  can someone help me understand what happens 
636,636,15783456,70009338,Resources$NotFoundException - Failed to open file &#39;...fragment_list.xml&#39;: No such file or directory,"<p>I am learning MVVM and have run into an issue, and I am getting the following errors:</p>
<pre><code>Resources$NotFoundException: File res/layout/fragment_list.xml from xml type layout resource ID #0x7f0b002f
</code></pre>
<p>and</p>
<pre><code>Failed to open file '/data/data/software.genau.dogs/code_cache/.overlay/base.apk/res/layout/fragment_list.xml': No such file or directory
</code></pre>
<p>I know the file exists in my layout, and there are no errors or warnings listed for the fragment_list.xml file. In the design view, everything is laid out the way I want and it looks correct.</p>
<p>To try and correct the issue I've tried some of the basic things like clean project and rebuilding, and restarted the program (and my machine). Also, I've tried different ways to override fun onCreateView() with and without binding.</p>
<p>I would appreciate some help.</p>
<p>Here is the MVVM layout:
<a href=""https://i.stack.imgur.com/wkUzs.png"" rel=""nofollow noreferrer"">pic from Android studio folder layout</a></p>
<p>Here is my code for ListFragment.kt:</p>
<pre><code>package software.genau.dogs.view

    import android.os.Bundle
    import androidx.fragment.app.Fragment
    import android.view.LayoutInflater
    import android.view.View
    import android.view.ViewGroup
    import androidx.lifecycle.Observer
    import androidx.lifecycle.ViewModelProviders
    import androidx.navigation.Navigation
    import androidx.recyclerview.widget.LinearLayoutManager
    import software.genau.dogs.R
    import software.genau.dogs.databinding.FragmentListBinding
    import software.genau.dogs.viewmodel.ListViewModel
    
    class ListFragment : Fragment() {
        private lateinit var binding: FragmentListBinding
        private lateinit var viewModel: ListViewModel
        private val dogsListAdapter = DogsListAdapter(arrayListOf())
    
    
    
        override fun onCreateView(
            inflater: LayoutInflater,
            container: ViewGroup?,
            savedInstanceState: Bundle?
        ): View? {
            return inflater.inflate(R.layout.fragment_list, container, false)
        }
    
        override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
            super.onViewCreated(view, savedInstanceState)
    
            viewModel = ViewModelProviders.of(this).get(ListViewModel::class.java)
            viewModel.refresh()
    
            binding.dogsList.apply {
                layoutManager = LinearLayoutManager(context)
                adapter = dogsListAdapter
            }
            observeViewModel()
        }
    
        fun observeViewModel() {
            viewModel.dogs.observe(viewLifecycleOwner, Observer { dogs -&gt;
                dogs?.let {
                    binding.dogsList.visibility = View.VISIBLE
                    dogsListAdapter.updateDogList(dogs)
                }
            })
    
            viewModel.dogsLoadError.observe(viewLifecycleOwner, Observer { isError -&gt;
                isError?.let {
                    binding.listError.visibility = View.VISIBLE
                }
            })
    
            viewModel.loading.observe(viewLifecycleOwner, Observer { isLoading -&gt;
                isLoading?.let {
                    binding.progressBar.visibility = if (it) View.VISIBLE else View.INVISIBLE
                    if (it) {
                        binding.listError.visibility = View.INVISIBLE
                        binding.dogsList.visibility = View.INVISIBLE
                    }
                }
            })
        }
    
    }
</code></pre>
<p>Here is the code for fragment_list.xml:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;layout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;
    xmlns:tools=&quot;http://schemas.android.com/tools&quot;&gt;

    &lt;androidx.swiperefreshlayout.widget.SwipeRefreshLayout
        android:id=&quot;@+id/refreshLayout&quot;
        android:layout_width=&quot;match_parent&quot;
        android:layout_height=&quot;match_parent&quot;&gt;

        &lt;androidx.constraintlayout.widget.ConstraintLayout
            android:layout_width=&quot;match_parent&quot;
            android:layout_height=&quot;match_parent&quot;&gt;

            &lt;androidx.recyclerview.widget.RecyclerView
                android:id=&quot;@+id/dogsList&quot;
                android:layout_width=&quot;0dp&quot;
                android:layout_height=&quot;0dp&quot;
                android:layout_marginStart=&quot;1dp&quot;
                android:layout_marginTop=&quot;1dp&quot;
                android:layout_marginEnd=&quot;1dp&quot;
                android:layout_marginBottom=&quot;1dp&quot;
                app:layout_constraintBottom_toBottomOf=&quot;parent&quot;
                app:layout_constraintEnd_toEndOf=&quot;parent&quot;
                app:layout_constraintStart_toStartOf=&quot;parent&quot;
                app:layout_constraintTop_toTopOf=&quot;parent&quot;&gt;

            &lt;/androidx.recyclerview.widget.RecyclerView&gt;

            &lt;TextView
                android:id=&quot;@+id/listError&quot;
                android:layout_width=&quot;wrap_content&quot;
                android:layout_height=&quot;wrap_content&quot;
                android:text=&quot;@string/list_fragment_error_msg&quot;
                app:layout_constraintBottom_toBottomOf=&quot;@+id/dogsList&quot;
                app:layout_constraintEnd_toEndOf=&quot;parent&quot;
                app:layout_constraintStart_toStartOf=&quot;parent&quot;
                app:layout_constraintTop_toTopOf=&quot;@+id/dogsList&quot; /&gt;

            &lt;ProgressBar
                android:id=&quot;@+id/progressBar&quot;
                style=&quot;?android:attr/progressBarStyle&quot;
                android:layout_width=&quot;wrap_content&quot;
                android:layout_height=&quot;wrap_content&quot;
                app:layout_constraintBottom_toBottomOf=&quot;@+id/dogsList&quot;
                app:layout_constraintEnd_toEndOf=&quot;parent&quot;
                app:layout_constraintStart_toStartOf=&quot;parent&quot;
                app:layout_constraintTop_toTopOf=&quot;@+id/dogsList&quot; /&gt;
        &lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;

    &lt;/androidx.swiperefreshlayout.widget.SwipeRefreshLayout&gt;
&lt;/layout&gt;
</code></pre>
<p>Here is MainActivity.kt:</p>
<pre><code>package software.genau.dogs.view

import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import androidx.navigation.NavAction
import androidx.navigation.NavController
import androidx.navigation.Navigation
import androidx.navigation.fragment.NavHostFragment
import androidx.navigation.ui.NavigationUI
import software.genau.dogs.R
import software.genau.dogs.databinding.ActivityMainBinding

class MainActivity : AppCompatActivity() {
    private lateinit var binding: ActivityMainBinding
    private lateinit var navController: NavController

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        binding = ActivityMainBinding.inflate(layoutInflater)
        setContentView(binding.root)

        val navHostFragment = supportFragmentManager.findFragmentById(R.id.fragmentContainerView) as NavHostFragment
        navController = navHostFragment.navController

        NavigationUI.setupActionBarWithNavController(this, navController)
    }

    override fun onSupportNavigateUp(): Boolean {
        return NavigationUI.navigateUp(navController, null)
    }
}
</code></pre>
<p>Here is activity_main.xml:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;

&lt;layout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;
    xmlns:tools=&quot;http://schemas.android.com/tools&quot;&gt;

    &lt;androidx.constraintlayout.widget.ConstraintLayout
        android:layout_width=&quot;match_parent&quot;
        android:layout_height=&quot;match_parent&quot;
        tools:context=&quot;.view.MainActivity&quot;&gt;


        &lt;androidx.fragment.app.FragmentContainerView
            android:id=&quot;@+id/fragmentContainerView&quot;
            android:name=&quot;androidx.navigation.fragment.NavHostFragment&quot;
            android:layout_width=&quot;0dp&quot;
            android:layout_height=&quot;0dp&quot;
            android:layout_marginStart=&quot;1dp&quot;
            android:layout_marginTop=&quot;1dp&quot;
            android:layout_marginEnd=&quot;1dp&quot;
            android:layout_marginBottom=&quot;1dp&quot;
            app:defaultNavHost=&quot;true&quot;
            app:layout_constraintBottom_toBottomOf=&quot;parent&quot;
            app:layout_constraintEnd_toEndOf=&quot;parent&quot;
            app:layout_constraintStart_toStartOf=&quot;parent&quot;
            app:layout_constraintTop_toTopOf=&quot;parent&quot;
            app:navGraph=&quot;@navigation/dog_navigation&quot;
            tools:layout=&quot;@layout/fragment_list&quot; /&gt;
    &lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;
&lt;/layout&gt;
</code></pre>
<p>Here are the errors from logcat:</p>
<pre><code>2021-11-17 12:29:08.420 12809-12809/software.genau.dogs E/ware.genau.dog: Failed to open file '/data/data/software.genau.dogs/code_cache/.overlay/base.apk/res/layout/fragment_list.xml': No such file or directory
2021-11-17 12:29:08.420 12809-12809/software.genau.dogs D/AndroidRuntime: Shutting down VM
2021-11-17 12:29:08.425 12809-12809/software.genau.dogs E/AndroidRuntime: FATAL EXCEPTION: main
    Process: software.genau.dogs, PID: 12809
    android.content.res.Resources$NotFoundException: File res/layout/fragment_list.xml from xml type layout resource ID #0x7f0b002f
        at android.content.res.ResourcesImpl.loadXmlResourceParser(ResourcesImpl.java:1264)
        at android.content.res.Resources.loadXmlResourceParser(Resources.java:2426)
        at android.content.res.Resources.loadXmlResourceParser(Resources.java:2402)
        at android.content.res.Resources.getLayout(Resources.java:1252)
        at android.view.LayoutInflater.inflate(LayoutInflater.java:530)
        at software.genau.dogs.view.ListFragment.onCreateView(ListFragment.kt:35)
        at androidx.fragment.app.Fragment.performCreateView(Fragment.java:2963)
        at androidx.fragment.app.FragmentStateManager.createView(FragmentStateManager.java:518)
        at androidx.fragment.app.FragmentStateManager.moveToExpectedState(FragmentStateManager.java:282)
        at androidx.fragment.app.FragmentManager.executeOpsTogether(FragmentManager.java:2189)
        at androidx.fragment.app.FragmentManager.removeRedundantOperationsAndExecute(FragmentManager.java:2106)
        at androidx.fragment.app.FragmentManager.execPendingActions(FragmentManager.java:2002)
        at androidx.fragment.app.FragmentManager.dispatchStateChange(FragmentManager.java:3138)
        at androidx.fragment.app.FragmentManager.dispatchViewCreated(FragmentManager.java:3065)
        at androidx.fragment.app.Fragment.performViewCreated(Fragment.java:2988)
        at androidx.fragment.app.FragmentStateManager.createView(FragmentStateManager.java:546)
        at androidx.fragment.app.FragmentStateManager.moveToExpectedState(FragmentStateManager.java:282)
        at androidx.fragment.app.FragmentStore.moveToExpectedState(FragmentStore.java:112)
        at androidx.fragment.app.FragmentManager.moveToState(FragmentManager.java:1647)
        at androidx.fragment.app.FragmentManager.dispatchStateChange(FragmentManager.java:3128)
        at androidx.fragment.app.FragmentManager.dispatchActivityCreated(FragmentManager.java:3072)
        at androidx.fragment.app.FragmentController.dispatchActivityCreated(FragmentController.java:251)
        at androidx.fragment.app.FragmentActivity.onStart(FragmentActivity.java:502)
        at androidx.appcompat.app.AppCompatActivity.onStart(AppCompatActivity.java:246)
        at android.app.Instrumentation.callActivityOnStart(Instrumentation.java:1435)
        at android.app.Activity.performStart(Activity.java:8018)
        at android.app.ActivityThread.handleStartActivity(ActivityThread.java:3475)
        at android.app.servertransaction.TransactionExecutor.performLifecycleSequence(TransactionExecutor.java:221)
        at android.app.servertransaction.TransactionExecutor.cycleToPath(TransactionExecutor.java:201)
        at android.app.servertransaction.TransactionExecutor.executeLifecycleState(TransactionExecutor.java:173)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:97)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:223)
        at android.app.ActivityThread.main(ActivityThread.java:7656)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)
     Caused by: java.io.FileNotFoundException: res/layout/fragment_list.xml
        at android.content.res.AssetManager.nativeOpenXmlAsset(Native Method)
        at android.content.res.AssetManager.openXmlBlockAsset(AssetManager.java:1092)
        at android.content.res.ResourcesImpl.loadXmlResourceParser(ResourcesImpl.java:1248)
        at android.content.res.Resources.loadXmlResourceParser(Resources.java:2426) 
        at android.content.res.Resources.loadXmlResourceParser(Resources.java:2402) 
        at android.content.res.Resources.getLayout(Resources.java:1252) 
        at android.view.LayoutInflater.inflate(LayoutInflater.java:530) 
        at software.genau.dogs.view.ListFragment.onCreateView(ListFragment.kt:35) 
        at androidx.fragment.app.Fragment.performCreateView(Fragment.java:2963) 
        at androidx.fragment.app.FragmentStateManager.createView(FragmentStateManager.java:518) 
        at androidx.fragment.app.FragmentStateManager.moveToExpectedState(FragmentStateManager.java:282) 
        at androidx.fragment.app.FragmentManager.executeOpsTogether(FragmentManager.java:2189) 
        at androidx.fragment.app.FragmentManager.removeRedundantOperationsAndExecute(FragmentManager.java:2106) 
        at androidx.fragment.app.FragmentManager.execPendingActions(FragmentManager.java:2002) 
        at androidx.fragment.app.FragmentManager.dispatchStateChange(FragmentManager.java:3138) 
        at androidx.fragment.app.FragmentManager.dispatchViewCreated(FragmentManager.java:3065) 
        at androidx.fragment.app.Fragment.performViewCreated(Fragment.java:2988) 
        at androidx.fragment.app.FragmentStateManager.createView(FragmentStateManager.java:546) 
        at androidx.fragment.app.FragmentStateManager.moveToExpectedState(FragmentStateManager.java:282) 
        at androidx.fragment.app.FragmentStore.moveToExpectedState(FragmentStore.java:112) 
        at androidx.fragment.app.FragmentManager.moveToState(FragmentManager.java:1647) 
        at androidx.fragment.app.FragmentManager.dispatchStateChange(FragmentManager.java:3128) 
        at androidx.fragment.app.FragmentManager.dispatchActivityCreated(FragmentManager.java:3072) 
        at androidx.fragment.app.FragmentController.dispatchActivityCreated(FragmentController.java:251) 
        at androidx.fragment.app.FragmentActivity.onStart(FragmentActivity.java:502) 
        at androidx.appcompat.app.AppCompatActivity.onStart(AppCompatActivity.java:246) 
        at android.app.Instrumentation.callActivityOnStart(Instrumentation.java:1435) 
        at android.app.Activity.performStart(Activity.java:8018) 
        at android.app.ActivityThread.handleStartActivity(ActivityThread.java:3475) 
        at android.app.servertransaction.TransactionExecutor.performLifecycleSequence(TransactionExecutor.java:221) 
        at android.app.servertransaction.TransactionExecutor.cycleToPath(TransactionExecutor.java:201) 
        at android.app.servertransaction.TransactionExecutor.executeLifecycleState(TransactionExecutor.java:173) 
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:97) 
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066) 
        at android.os.Handler.dispatchMessage(Handler.java:106) 
        at android.os.Looper.loop(Looper.java:223) 
        at android.app.ActivityThread.main(ActivityThread.java:7656) 
        at java.lang.reflect.Method.invoke(Native Method) 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592) 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947) 
</code></pre>
",493,2,1,2,android;android-jetpack,2021-11-17 23:26:44,2021-11-17 23:26:44,2022-05-13 14:55:14,i am learning mvvm and have run into an issue  and i am getting the following errors  and i know the file exists in my layout  and there are no errors or warnings listed for the fragment_list xml file  in the design view  everything is laid out the way i want and it looks correct  to try and correct the issue i ve tried some of the basic things like clean project and rebuilding  and restarted the program  and my machine   also  i ve tried different ways to override fun oncreateview   with and without binding  i would appreciate some help  here is my code for listfragment kt  here is the code for fragment_list xml  here is mainactivity kt  here is activity_main xml  here are the errors from logcat 
637,637,19103665,72218769,Using multiple input images in Tensorflow for object recognition,"<p>I'm new to Tensorflow and machine learning in general so please forgive my ignorance.</p>
<p>I'm designing a mechanical process that will separate various objects and take photos/webcam stream of each at different angles (e.g. by rotating them).</p>
<p>I can find many tutorials around object detection and classification, but they all seem to be centred on a single image or snapshot from a webcam. I can't find anything that uses multiple photos of the same object, e.g. at different angles, to improve the recognition process.</p>
<p>To justify my approsch - certain objects might look the same from one angle, but if you rotate them they can be separately identified - in the same way you might look at something in real life and rotate it in your hand.</p>
<p>Can anyone point to tutorials that take multiple image inputs?</p>
<p>Many thanks!</p>
",67,1,0,3,tensorflow;label;object-detection,2022-05-12 21:29:35,2022-05-12 21:29:35,2022-05-13 02:19:38,i m new to tensorflow and machine learning in general so please forgive my ignorance  i m designing a mechanical process that will separate various objects and take photos webcam stream of each at different angles  e g  by rotating them   i can find many tutorials around object detection and classification  but they all seem to be centred on a single image or snapshot from a webcam  i can t find anything that uses multiple photos of the same object  e g  at different angles  to improve the recognition process  to justify my approsch   certain objects might look the same from one angle  but if you rotate them they can be separately identified   in the same way you might look at something in real life and rotate it in your hand  can anyone point to tutorials that take multiple image inputs  many thanks 
638,638,2778860,72219304,Eliminating sign flips in quaternion data from sensors,"<p>I obtain sensor data from an AR headset, which outputs the user's head orientation as quaternions. When I inspected the raw data, I saw that there are several significant jumps in the consecutive quaternion samples, especially for qw and qy components as shown below.</p>
<p><a href=""https://i.stack.imgur.com/9uYEJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9uYEJ.png"" alt=""enter image description here"" /></a></p>
<p>Since I want to use some machine learning algorithms on this data, continuity is important. To get rid of the flips, I followed the advice in <a href=""https://stackoverflow.com/questions/42428136/quaternion-is-flipping-sign-for-very-similar-rotations"">this</a> answer and flipped the sign of all quaternion components, if qw &lt; 0. This is valid because q and -q denote the same rotation, assuming q is a unit quaternion.
With this approach, most of the flips are gone:</p>
<p><a href=""https://i.stack.imgur.com/DZYrP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZYrP.png"" alt=""enter image description here"" /></a></p>
<p>However, I noticed that this created another jump for qy at around t=25000 where the magnitude of qy is very close to 1.0. Checking the specific samples where the jump occurs, I converted the quaternions values to Euler angles (yaw, pitch, roll) to get a better understanding:</p>
<pre><code>e1 = [175.84487617, 4.24149047, 170.7215615]
e2 = [175.0441748, -0.47157242, 169.98347392]
</code></pre>
<p>It is clear that the angles are very similar except for the zero-crossing in the pitch value which seems to cause the flip in qy. Do I have to live with these discontinuities that occur at the borders of the range or is there a way to make quaternions fully continuous?</p>
",30,1,0,4,signal-processing;sensors;quaternions;euler-angles,2022-05-12 22:10:27,2022-05-12 22:10:27,2022-05-12 23:20:01,i obtain sensor data from an ar headset  which outputs the user s head orientation as quaternions  when i inspected the raw data  i saw that there are several significant jumps in the consecutive quaternion samples  especially for qw and qy components as shown below    however  i noticed that this created another jump for qy at around t  where the magnitude of qy is very close to    checking the specific samples where the jump occurs  i converted the quaternions values to euler angles  yaw  pitch  roll  to get a better understanding  it is clear that the angles are very similar except for the zero crossing in the pitch value which seems to cause the flip in qy  do i have to live with these discontinuities that occur at the borders of the range or is there a way to make quaternions fully continuous 
639,639,17571258,72217366,Gathering data with the same ID from across multiple rows into one row by adding columns,"<p>I am learning to use R/Rstudio on a Windows machine and have not found a solution to my problem.
I have a dataframe with thousands of observations and 10 columns (numeric, factors and dates).
IDs in my first column often occur many times in different rows.</p>
<p>I would like to get all values of all the rows with the same ID into one row.</p>
<p>I have tried loops, across, merge, gather, spread, etc. but  I have not yet found a way.</p>
<p>Here is a made-up example of the data I have:</p>
<pre><code>id&lt;-rep(c(1,2,3,4),times=5)
df1&lt;-data.frame(id)
df1$type &lt;- rep(c(22, 50, 49, 33), times = 5) 
df1$begin &lt;- rep(c(2020-05-23, 2020-06-24, 2020-04-18, 2020-09-07), times = 5) 
df1$end &lt;- rep(c(2021-07-23, 2021-04-24, 2021-03-18, 2021-10-07), times = 5)
</code></pre>
<p><code>df1</code></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th>id</th>
<th>type</th>
<th>begin</th>
<th style=""text-align: right;"">end</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;"">1991</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td>2</td>
<td>50</td>
<td>1990</td>
<td style=""text-align: right;"">1993</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td>3</td>
<td>49</td>
<td>1998</td>
<td style=""text-align: right;"">2000</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td>4</td>
<td>33</td>
<td>2004</td>
<td style=""text-align: right;"">2004</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;"">1991</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td>2</td>
<td>50</td>
<td>1990</td>
<td style=""text-align: right;"">1993</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td>3</td>
<td>49</td>
<td>1998</td>
<td style=""text-align: right;"">2000</td>
</tr>
<tr>
<td style=""text-align: left;"">8</td>
<td>4</td>
<td>33</td>
<td>2004</td>
<td style=""text-align: right;"">2004</td>
</tr>
<tr>
<td style=""text-align: left;"">9</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;"">1991</td>
</tr>
<tr>
<td style=""text-align: left;"">10</td>
<td>2</td>
<td>50</td>
<td>1990</td>
<td style=""text-align: right;"">1993</td>
</tr>
<tr>
<td style=""text-align: left;"">11</td>
<td>3</td>
<td>49</td>
<td>1998</td>
<td style=""text-align: right;"">2000</td>
</tr>
<tr>
<td style=""text-align: left;"">12</td>
<td>4</td>
<td>33</td>
<td>2004</td>
<td style=""text-align: right;"">2004</td>
</tr>
<tr>
<td style=""text-align: left;"">13</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;"">1991</td>
</tr>
<tr>
<td style=""text-align: left;"">14</td>
<td>2</td>
<td>50</td>
<td>1990</td>
<td style=""text-align: right;"">1993</td>
</tr>
<tr>
<td style=""text-align: left;"">15</td>
<td>3</td>
<td>49</td>
<td>1998</td>
<td style=""text-align: right;"">2000</td>
</tr>
<tr>
<td style=""text-align: left;"">16</td>
<td>4</td>
<td>33</td>
<td>2004</td>
<td style=""text-align: right;"">2004</td>
</tr>
<tr>
<td style=""text-align: left;"">17</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;"">1991</td>
</tr>
<tr>
<td style=""text-align: left;"">18</td>
<td>2</td>
<td>50</td>
<td>1990</td>
<td style=""text-align: right;"">1993</td>
</tr>
<tr>
<td style=""text-align: left;"">19</td>
<td>3</td>
<td>49</td>
<td>1998</td>
<td style=""text-align: right;"">2000</td>
</tr>
<tr>
<td style=""text-align: left;"">20</td>
<td>4</td>
<td>33</td>
<td>2004</td>
<td style=""text-align: right;"">2004</td>
</tr>
</tbody>
</table>
</div>
<p>My end result should look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id</th>
<th>type</th>
<th>begin</th>
<th>end</th>
<th>tpye2</th>
<th>begin2</th>
<th>end2</th>
<th>type3</th>
<th>begin3</th>
<th>end3</th>
<th style=""text-align: right;"">etc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td>22</td>
<td>1992</td>
<td>1991</td>
<td>22</td>
<td>1991</td>
<td>1992</td>
<td>1</td>
<td>22</td>
<td>1992</td>
<td style=""text-align: right;""></td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td>50</td>
<td>1990</td>
<td>1993</td>
<td>50</td>
<td>1990</td>
<td>1993</td>
<td>50</td>
<td>1990</td>
<td>1993</td>
<td style=""text-align: right;""></td>
</tr>
</tbody>
</table>
</div>
<p>I hope this makes sense. Any help is greatly appreciated.
Many thanks!!</p>
",49,1,1,4,r;merge;row;multiple-columns,2022-05-12 19:53:08,2022-05-12 19:53:08,2022-05-12 20:52:50,i would like to get all values of all the rows with the same id into one row  i have tried loops  across  merge  gather  spread  etc  but  i have not yet found a way  here is a made up example of the data i have  df my end result should look something like this 
640,640,9182231,72205219,Scikitlearn machine learning pipeline with passthrough parameters,"<p>I have implemented 3 <code>TransformerMixin</code> classes in an attempt to make my own scikitlearn <code>Pipeline</code>. However, I am unable to combine them since <code>PrepareModel</code> object uses information from <code>FeatureEngineering</code> object. In particular, consider:</p>
<pre><code>cleaner = DataCleaner()
df_clean = cleaner.fit_transform(df)
engineering = FeatureEngineering()
df_engineered = engineering.fit_transform(df_clean)
modelprep = PrepareModel(engineering.des_features)
X = modelprep.fit_transform(df_engineered)
</code></pre>
<p>Note that each of <code>DataCleaner</code>, <code>FeatureEngineering</code>, <code>PrepareModel</code> are child classes of <code>TransformerMixin</code>.</p>
<p>How would I make a <code>Pipeline</code> with this setup?</p>
<pre><code>from sklearn.pipeline import Pipeline  
full_pipeline = Pipeline([('cleaner', DataCleaner()), 
                          ('engineering', FeatureEngineering()),
                          ('prepare', PrepareModel())])
</code></pre>
<p>The issue I have is that the third step needs the <code>des_features</code> from the second step? <strong>So this does not work.</strong> How would I make this work?</p>
",36,1,1,5,python;dataframe;machine-learning;scikit-learn;pipeline,2022-05-11 22:55:22,2022-05-11 22:55:22,2022-05-12 20:27:23,i have implemented  transformermixin classes in an attempt to make my own scikitlearn pipeline  however  i am unable to combine them since preparemodel object uses information from featureengineering object  in particular  consider  note that each of datacleaner  featureengineering  preparemodel are child classes of transformermixin  how would i make a pipeline with this setup  the issue i have is that the third step needs the des_features from the second step  so this does not work  how would i make this work 
641,641,10319358,52682607,which Deep learning method is best for non linear data(for classification),"<p>i have a dataset with variables 60 and observations 150,000, dataset is completly  non linear, and i know there are few machine learning algortihms which are good for non linear data such as SVM, but i would like to know which deep learning method is best for non linear data, any suggestion or tips would be helpful. thanks </p>
",50,2,-2,2,machine-learning;deep-learning,2018-10-07 00:56:02,2018-10-07 00:56:02,2022-05-12 20:00:43,i have a dataset with variables  and observations    dataset is completly  non linear  and i know there are few machine learning algortihms which are good for non linear data such as svm  but i would like to know which deep learning method is best for non linear data  any suggestion or tips would be helpful  thanks 
642,642,7963888,72217286,Basic KNN code returning input contains NaN despite using df.replace,"<p>This code was working before, but has randomly (as far as I can tell atleast) stopped working. I am running the code on jupyter-lab, and am following sentdex's Machine Learning with Python series (the current video is pt14).</p>
<p>I am using train_test_split instead of cross_validation like sentdex as it has since been deprecated.</p>
<p>I am using df.replace to replace '?' values in the dataset (wisconsin breast cancer from UCI repository) with '-99999'. I have added in keywords into df.replace() parameters to make sure it's not caused by the pandas futurewarning warning.</p>
<p>However I still get this error:</p>
<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>
<p><strong>Code:</strong></p>
<pre><code>import numpy as np
import pandas as pd
from sklearn import preprocessing, neighbors
from sklearn.model_selection import train_test_split

df=pd.read_csv('breast-cancer-wisconsin.data')
df.replace(to_replace='?', value=-99999, inplace=True)
df.drop(['id'], axis =1, inplace=True)

X=np.array(df.drop(['class'],1))
y=np.array(df['class'])
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)

clf=neighbors.KNeighborsClassifier()

clf.fit(X_train,y_train)
pred=clf.predict(X_test)
accuracy=clf.score(X_test,y_test)

print(accuracy)
</code></pre>
",14,0,0,5,python;machine-learning;classification;jupyter;knn,2022-05-12 19:47:45,2022-05-12 19:47:45,2022-05-12 19:47:45,this code was working before  but has randomly  as far as i can tell atleast  stopped working  i am running the code on jupyter lab  and am following sentdex s machine learning with python series  the current video is pt   i am using train_test_split instead of cross_validation like sentdex as it has since been deprecated  i am using df replace to replace     values in the dataset  wisconsin breast cancer from uci repository  with      i have added in keywords into df replace   parameters to make sure it s not caused by the pandas futurewarning warning  however i still get this error  code 
643,643,14667788,72214896,Falcon API - check if the server is running,"<p>I have built a very simple API in Falcon that post results of my machine learning model. See below:</p>
<pre class=""lang-py prettyprint-override""><code>
import falcon
import heureka_predikce


class MLmodel:
    def on_post(self, req, resp):

        reg_list = req.media
        response_list = []

        for vstup in reg_list:
            response_list.append(heureka_predikce.heureka_predikuj(vstup))

        resp.text = response_list
        resp.status = falcon.HTTP_OK


my_model = MLmodel()

app = application = falcon.API()
app.add_route(&quot;/predictionApi/v1/predict&quot;, my_model)

</code></pre>
<p>But I would like to check if the server is running. That is, I would like to include some new method into <code>MLmodel</code> that does a &quot;health check&quot;. I am a beginner with APIs, but is there a recommended way, how to do this? I think that it should be relatively easy, but I cannot find anything myself... Thanks</p>
",19,1,0,2,python;falcon,2022-05-12 17:10:58,2022-05-12 17:10:58,2022-05-12 18:30:12,i have built a very simple api in falcon that post results of my machine learning model  see below  but i would like to check if the server is running  that is  i would like to include some new method into mlmodel that does a  health check   i am a beginner with apis  but is there a recommended way  how to do this  i think that it should be relatively easy  but i cannot find anything myself    thanks
644,644,14125436,72215109,How to train a machine learning model in python including several target variables,"<p>I am trying to build a machine learning model in python. I used <code>pytorch</code> and <code>sklearn</code> to make the model. My model is a bit complicated: I have one input feature but several target variables. My target variables are values making a curve and I used each value of the curve as a different feature. I showed five different curves in the upladed figure.</p>
<p>I used algorithms like <code>DecisionTreeRegressor</code> and <code>RandomeForestRegressor</code> to fit the only input variable to several target variables. But the prediction of trained model is not so well for extrapolation. The trained model can create the a series of data but not so accure. Does anyone know such trained model in Python? I tried hyperparameter tuning using <code>GridSearchCV</code> but it did not help me.
In advance I do appreciate your help and feedback.</p>
<p><a href=""https://i.stack.imgur.com/22E95.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/22E95.jpg"" alt=""enter image description here"" /></a></p>
",50,0,0,5,python;machine-learning;scikit-learn;pytorch;extrapolation,2022-05-12 17:25:05,2022-05-12 17:25:05,2022-05-12 17:25:05,i am trying to build a machine learning model in python  i used pytorch and sklearn to make the model  my model is a bit complicated  i have one input feature but several target variables  my target variables are values making a curve and i used each value of the curve as a different feature  i showed five different curves in the upladed figure  
645,645,2602477,24758218,Time-series - data splitting and model evaluation,"<p>I've tried to use machine learning to make prediction based on time-series data. In one of the stackoverflow question (<a href=""https://stackoverflow.com/questions/22334561/createtimeslices-function-in-caret-package-in-r"">createTimeSlices function in CARET package in R</a>) is an example of using createTimeSlices to cross-validation for model training and parameter tuning:</p>

<pre><code>    library(caret)
    library(ggplot2)
    library(pls)
    data(economics)
    myTimeControl &lt;- trainControl(method = ""timeslice"",
                                  initialWindow = 36,
                                  horizon = 12,
                                  fixedWindow = TRUE)

    plsFitTime &lt;- train(unemploy ~ pce + pop + psavert,
                        data = economics,
                        method = ""pls"",
                        preProc = c(""center"", ""scale""),
                        trControl = myTimeControl)
</code></pre>

<p>My understanding is:</p>

<ol>
<li>I need to split may data to training and test set.</li>
<li>Use training set for parameters tuning.</li>
<li>Evaluate obtained model on the test set (using R2, RMSE, etc.)</li>
</ol>

<p>Because my data is time-series, I suppose that I cannot use bootstraping for spliting data into training and test set. So, my questions are: Am I right? And If so - How to use createTimeSlices for model evaluation?</p>
",31643,3,20,3,r;time-series;r-caret,2014-07-15 17:57:32,2014-07-15 17:57:32,2022-05-12 16:30:44,i ve tried to use machine learning to make prediction based on time series data  in one of the stackoverflow question    is an example of using createtimeslices to cross validation for model training and parameter tuning  my understanding is  because my data is time series  i suppose that i cannot use bootstraping for spliting data into training and test set  so  my questions are  am i right  and if so   how to use createtimeslices for model evaluation 
646,646,13516151,72213117,How to connect Google colab with gitlab?,"<p>I'm new to the Google colab and Gitlab. I am working on a machine learning project on google colab and I want to push my project into a <strong>Gitlab</strong> repository. I searched for some tutorials but all of them are only for github. Does anyone know about the workflow for connecting google colab with gitlab?</p>
<p>Any help you can give would be greatly appreciated and thanks for your valuable time.</p>
",60,0,0,4,git;version-control;gitlab;google-colaboratory,2022-05-12 14:56:49,2022-05-12 14:56:49,2022-05-12 15:02:29,i m new to the google colab and gitlab  i am working on a machine learning project on google colab and i want to push my project into a gitlab repository  i searched for some tutorials but all of them are only for github  does anyone know about the workflow for connecting google colab with gitlab  any help you can give would be greatly appreciated and thanks for your valuable time 
647,647,5793353,71808879,Can I feed intermediate result back into the CNN and get my final result? (update),"<p>I am new to machine learning.</p>
<p>I got the intermediate result of layer 31 of my CNN using the following code:</p>
<pre><code>conv2d = Model(inputs = self.model_ori.input, outputs= self.model_ori.layers[31].output)
intermediateResult = conv2d.predict(img)
</code></pre>
<p>I am trying to load this intermediate result back into the neural network.</p>
<p>Can this be done?</p>
<p>I tried doing the following:</p>
<pre><code>newmodel = keras.Sequential(self.model_ori.layers[32:])
newmodel = newmodel.build(intermediateResult.shape)
</code></pre>
<p>I did the above but I got the following error:</p>
<pre><code>A merge layer should be called on a list of inputs. Received: inputs=Tensor(&quot;up_sampling2d_2/resize/ResizeNearestNeighbor:0&quot;, shape=(1, 26, 26, 128), dtype=float32) (not a list of tensors)
</code></pre>
<p>Here is my model summary:</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, None, None,  0           []
                                 3)]

 conv2d (Conv2D)                (None, None, None,   432         ['input_1[0][0]']
                                16)

 batch_normalization (BatchNorm  (None, None, None,   64         ['conv2d[0][0]']
 alization)                     16)

 leaky_re_lu (LeakyReLU)        (None, None, None,   0           ['batch_normalization[0][0]']
                                16)

 max_pooling2d (MaxPooling2D)   (None, None, None,   0           ['leaky_re_lu[0][0]']
                                16)

 conv2d_1 (Conv2D)              (None, None, None,   4608        ['max_pooling2d[0][0]']
                                32)

 batch_normalization_1 (BatchNo  (None, None, None,   128        ['conv2d_1[0][0]']
 rmalization)                   32)

 leaky_re_lu_1 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_1[0][0]']
                                32)

 max_pooling2d_1 (MaxPooling2D)  (None, None, None,   0          ['leaky_re_lu_1[0][0]']
                                32)

 conv2d_2 (Conv2D)              (None, None, None,   18432       ['max_pooling2d_1[0][0]']
                                64)

 batch_normalization_2 (BatchNo  (None, None, None,   256        ['conv2d_2[0][0]']
 rmalization)                   64)

 leaky_re_lu_2 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_2[0][0]']
                                64)

 max_pooling2d_2 (MaxPooling2D)  (None, None, None,   0          ['leaky_re_lu_2[0][0]']
                                64)

 conv2d_3 (Conv2D)              (None, None, None,   73728       ['max_pooling2d_2[0][0]']
                                128)

 batch_normalization_3 (BatchNo  (None, None, None,   512        ['conv2d_3[0][0]']
 rmalization)                   128)

 leaky_re_lu_3 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_3[0][0]']
                                128)

 max_pooling2d_3 (MaxPooling2D)  (None, None, None,   0          ['leaky_re_lu_3[0][0]']
                                128)

 conv2d_4 (Conv2D)              (None, None, None,   294912      ['max_pooling2d_3[0][0]']
                                256)

 batch_normalization_4 (BatchNo  (None, None, None,   1024       ['conv2d_4[0][0]']
 rmalization)                   256)

 leaky_re_lu_4 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_4[0][0]']
                                256)

 max_pooling2d_4 (MaxPooling2D)  (None, None, None,   0          ['leaky_re_lu_4[0][0]']
                                256)

 conv2d_5 (Conv2D)              (None, None, None,   1179648     ['max_pooling2d_4[0][0]']
                                512)

 batch_normalization_5 (BatchNo  (None, None, None,   2048       ['conv2d_5[0][0]']
 rmalization)                   512)

 leaky_re_lu_5 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_5[0][0]']
                                512)

 max_pooling2d_5 (MaxPooling2D)  (None, None, None,   0          ['leaky_re_lu_5[0][0]']
                                512)

 conv2d_6 (Conv2D)              (None, None, None,   4718592     ['max_pooling2d_5[0][0]']
                                1024)

 batch_normalization_6 (BatchNo  (None, None, None,   4096       ['conv2d_6[0][0]']
 rmalization)                   1024)

 leaky_re_lu_6 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_6[0][0]']
                                1024)

 conv2d_7 (Conv2D)              (None, None, None,   262144      ['leaky_re_lu_6[0][0]']
                                256)

 batch_normalization_7 (BatchNo  (None, None, None,   1024       ['conv2d_7[0][0]']
 rmalization)                   256)

 leaky_re_lu_7 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_7[0][0]']
                                256)

 conv2d_10 (Conv2D)             (None, None, None,   32768       ['leaky_re_lu_7[0][0]']
                                128)

 batch_normalization_9 (BatchNo  (None, None, None,   512        ['conv2d_10[0][0]']
 rmalization)                   128)

 leaky_re_lu_9 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_9[0][0]']
                                128)

 up_sampling2d (UpSampling2D)   (None, None, None,   0           ['leaky_re_lu_9[0][0]']
                                128)

 concatenate (Concatenate)      (None, None, None,   0           ['up_sampling2d[0][0]',
                                384)                              'leaky_re_lu_4[0][0]']

 conv2d_8 (Conv2D)              (None, None, None,   1179648     ['leaky_re_lu_7[0][0]']
                                512)

 conv2d_11 (Conv2D)             (None, None, None,   884736      ['concatenate[0][0]']
                                256)

 batch_normalization_8 (BatchNo  (None, None, None,   2048       ['conv2d_8[0][0]']
 rmalization)                   512)

 batch_normalization_10 (BatchN  (None, None, None,   1024       ['conv2d_11[0][0]']
 ormalization)                  256)

 leaky_re_lu_8 (LeakyReLU)      (None, None, None,   0           ['batch_normalization_8[0][0]']
                                512)

 leaky_re_lu_10 (LeakyReLU)     (None, None, None,   0           ['batch_normalization_10[0][0]']
                                256)

 conv2d_9 (Conv2D)              (None, None, None,   130815      ['leaky_re_lu_8[0][0]']
                                255)

 conv2d_12 (Conv2D)             (None, None, None,   65535       ['leaky_re_lu_10[0][0]']
                                255)

==================================================================================================
Total params: 8,858,734
Trainable params: 8,852,366
Non-trainable params: 6,368
__________________________________________________________________________________________________
None
</code></pre>
<p>Can someone kindly help me out?</p>
<p>Sincerely,
Lolcocks.</p>
",52,1,0,3,tensorflow;keras;tf.keras,2022-04-09 19:47:25,2022-04-09 19:47:25,2022-05-12 14:07:53,i am new to machine learning  i got the intermediate result of layer  of my cnn using the following code  i am trying to load this intermediate result back into the neural network  can this be done  i tried doing the following  i did the above but i got the following error  here is my model summary  can someone kindly help me out 
648,648,3448011,71721694,tensorflow2.x keras Embedding layer process tf.dataset error,"<p>This question is a follow-up of <a href=""https://stackoverflow.com/questions/71714299/tensorflow-2-textvectorization-process-tensor-and-dataset-error"">tensorflow 2 TextVectorization process tensor and dataset error</a></p>
<p>I would like to make do a word embedding for the processed text with tnesorflow 2.8 on Jupyter.</p>
<pre><code>def standardize(input_data):

    input_data = tf.strings.lower(input_data)
    input_data = tf.strings.regex_replace(input_data, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;)
    return input_data

# the input data loaded from text files by TfRecordDataset(file_paths, &quot;GZIP&quot;)
# each file can be 200+MB, totally about 300 files
# each file hold the data with multiple columns
# some columns are text
# after loading, the dataset will be accessed by column name 
# e.g. one column is &quot;sports&quot;, so the input_dataset[&quot;sports&quot;] 
# return a tensor, which is like the following example

input_data = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]], shape=(2, 1), dtype=tf.string)

text_layer = tf.keras.layers.TextVectorization( standardize = standardize, max_tokens = 10, output_mode = 'int', output_sequence_length=10 )

dataset = tf.data.Dataset.from_tensors( input_data )

dataset = dataset.batch(2)

text_layer.adapt(dataset)

process_text = dataset.map(text_layer)

emb_layer = layers.Embedding(10, 10)

emb_layer(process_text) # error 
</code></pre>
<p>error:</p>
<pre><code> AttributeError: Exception encountered when calling layer &quot;embedding_7&quot; (type Embedding).

'MapDataset' object has no attribute 'dtype'

Call arguments received:

 • inputs=&lt;MapDataset element_spec=TensorSpec(shape=(None, 2, 10), dtype=tf.int64, name=None)&gt;
</code></pre>
<p>How can I convert a tf.dataset to tf.tensor ?</p>
<p>This <a href=""https://stackoverflow.com/questions/64497977/tensorflow-convert-tf-dataset-to-tf-tensor"">TensorFlow: convert tf.Dataset to tf.Tensor</a> does not help me.</p>
<p>The above layers will be implemented in a machine learning neural network model.</p>
<pre><code>loading data --&gt; processing features (multiple text columns) --&gt; tokens --&gt; embedding --&gt; average pooling --&gt; some dense layers --&gt; output layer
</code></pre>
<p>thanks</p>
",89,2,2,5,python;tensorflow;keras;text-processing;word-embedding,2022-04-03 04:15:07,2022-04-03 04:15:07,2022-05-12 14:05:39,this question is a follow up of  i would like to make do a word embedding for the processed text with tnesorflow   on jupyter  error  how can i convert a tf dataset to tf tensor   this  does not help me  the above layers will be implemented in a machine learning neural network model  thanks
649,649,18756539,72211837,Fit - Transform,"<p>We wrote such code at a place where I was trained in machine learning.</p>
<p>My question is:
Why do we transform X_test without fitting while fitting X_train at the bottom of the code?</p>
<pre><code>hit = pd.read_csv(&quot;./xxx/xxx.csv&quot;)
df = hit.copy()
df = df.dropna()
y = df[&quot;Salary&quot;]
X_ = df.drop([&quot;Salary&quot;,&quot;League&quot;,&quot;Division&quot;,&quot;NewLeague&quot;],axis=1).astype(&quot;float64&quot;)
dms = pd.get_dummies(df[[&quot;League&quot;,&quot;Division&quot;,&quot;NewLeague&quot;]])
X = pd.concat([X_ , dms[[&quot;League_N&quot;,&quot;Division_W&quot;,&quot;NewLeague_N&quot;]]],axis=1)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
</code></pre>
",43,1,-3,3,python;machine-learning;scikit-learn,2022-05-12 13:18:08,2022-05-12 13:18:08,2022-05-12 13:48:05,we wrote such code at a place where i was trained in machine learning 
650,650,19099750,72212185,Spoken Language Identification,"<pre><code>```import numpy as np
import glob
import os
from keras.models import Model
from keras.layers import Input, Dense, GRU, CuDNNGRU, CuDNNLSTM
from keras import optimizers
import h5py
from sklearn.model_selection import train_test_split
from keras.models import load_model


def language_name(index):
    if index == 0:
        return &quot;English&quot;
    elif index == 1:
        return &quot;Hindi&quot;
    elif index == 2:
        return &quot;Mandarin&quot;

# ---------------------------BLOCK 1------------------------------------
# COMMENT/UNCOMMENT BELOW CODE BLOCK -
# Below code extracts mfcc features from the files provided into a dataset
codePath = './train/'
num_mfcc_features = 64

english_mfcc = np.array([]).reshape(0, num_mfcc_features)
for file in glob.glob(codePath + 'english/*.npy'):
    current_data = np.load(file).T
    english_mfcc = np.vstack((english_mfcc, current_data))

hindi_mfcc = np.array([]).reshape(0, num_mfcc_features)
for file in glob.glob(codePath + 'hindi/*.npy'):
    current_data = np.load(file).T
    hindi_mfcc = np.vstack((hindi_mfcc, current_data))

mandarin_mfcc = np.array([]).reshape(0, num_mfcc_features)
for file in glob.glob(codePath + 'mandarin/*.npy'):
    current_data = np.load(file).T
    mandarin_mfcc = np.vstack((mandarin_mfcc, current_data))

# Sequence length is 10 seconds
sequence_length = 1000
list_english_mfcc = []
num_english_sequence = int(np.floor(len(english_mfcc)/sequence_length))
for i in range(num_english_sequence):
    list_english_mfcc.append(english_mfcc[sequence_length*i:sequence_length*(i+1)])
list_english_mfcc = np.array(list_english_mfcc)
english_labels = np.full((num_english_sequence, 1000, 3), np.array([1, 0, 0]))

list_hindi_mfcc = []
num_hindi_sequence = int(np.floor(len(hindi_mfcc)/sequence_length))
for i in range(num_hindi_sequence):
    list_hindi_mfcc.append(hindi_mfcc[sequence_length*i:sequence_length*(i+1)])
list_hindi_mfcc = np.array(list_hindi_mfcc)
hindi_labels = np.full((num_hindi_sequence, 1000, 3), np.array([0, 1, 0]))

list_mandarin_mfcc = []
num_mandarin_sequence = int(np.floor(len(mandarin_mfcc)/sequence_length))
for i in range(num_mandarin_sequence):
    list_mandarin_mfcc.append(mandarin_mfcc[sequence_length*i:sequence_length*(i+1)])
list_mandarin_mfcc = np.array(list_mandarin_mfcc)
mandarin_labels = np.full((num_mandarin_sequence, 1000, 3), np.array([0, 0, 1]))

del english_mfcc
del hindi_mfcc
del mandarin_mfcc

total_sequence_length = num_english_sequence + num_hindi_sequence + num_mandarin_sequence
Y_train = np.vstack((english_labels, hindi_labels))
Y_train = np.vstack((Y_train, mandarin_labels))

X_train = np.vstack((list_english_mfcc, list_hindi_mfcc))
X_train = np.vstack((X_train, list_mandarin_mfcc))

del list_english_mfcc
del list_hindi_mfcc
del list_mandarin_mfcc

X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)

with h5py.File(&quot;mfcc_dataset.hdf5&quot;, 'w') as hf:
    hf.create_dataset('X_train', data=X_train)
    hf.create_dataset('Y_train', data=Y_train)
    hf.create_dataset('X_val', data=X_val)
    hf.create_dataset('Y_val', data=Y_val)
# ---------------------------------------------------------------


# --------------------------BLOCK 2-------------------------------------
# Load MFCC Dataset created by the code in the previous steps
with h5py.File(&quot;mfcc_dataset.hdf5&quot;, 'r') as hf:
    X_train = hf['X_train'][:]
    Y_train = hf['Y_train'][:]
    X_val = hf['X_val'][:]
    Y_val = hf['Y_val'][:]
# ---------------------------------------------------------------


# ---------------------------BLOCK 3------------------------------------
# Setting up the model for training
DROPOUT = 0.3
RECURRENT_DROP_OUT = 0.2
optimizer = optimizers.Adam(decay=1e-4)
main_input = Input(shape=(sequence_length, 64), name='main_input')

# ### main_input = Input(shape=(None, 64), name='main_input')
# ### pred_gru = GRU(4, return_sequences=True, name='pred_gru')(main_input)
# ### rnn_output = Dense(3, activation='softmax', name='rnn_output')(pred_gru)

layer1 = CuDNNLSTM(64, return_sequences=True, name='layer1')(main_input)
layer2 = CuDNNLSTM(32, return_sequences=True, name='layer2')(layer1)
layer3 = Dense(100, activation='tanh', name='layer3')(layer2)
rnn_output = Dense(3, activation='softmax', name='rnn_output')(layer3)

model = Model(inputs=main_input, outputs=rnn_output)
print('\nCompiling model...')
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])
model.summary()
history = model.fit(X_train, Y_train, batch_size=32, epochs=75, validation_data=(X_val, Y_val), shuffle=True, verbose=1)
model.save('sld.hdf5')
# ---------------------------------------------------------------

# --------------------------BLOCK 4-------------------------------------
# Inference Mode Setup
streaming_input = Input(name='streaming_input', batch_shape=(1, 1, 64))
pred_layer1 = CuDNNLSTM(64, return_sequences=True, name='layer1', stateful=True)(streaming_input)
pred_layer2 = CuDNNLSTM(32, return_sequences=True, name='layer2')(pred_layer1)
pred_layer3 = Dense(100, activation='tanh', name='layer3')(pred_layer2)
pred_output = Dense(3, activation='softmax', name='rnn_output')(pred_layer3)
streaming_model = Model(inputs=streaming_input, outputs=pred_output)
streaming_model.load_weights('sld.hdf5')
# streaming_model.summary()
# ---------------------------------------------------------------

# ---------------------------BLOCK 5------------------------------------
# Language Prediction for a random sequence from the validation data set
random_val_sample = np.random.randint(0, X_val.shape[0])
random_sequence_num = np.random.randint(0, len(X_val[random_val_sample]))
test_single = X_val[random_val_sample][random_sequence_num].reshape(1, 1, 64)
val_label = Y_val[random_val_sample][random_sequence_num]
true_label = language_name(np.argmax(val_label))
print(&quot;***********************&quot;)
print(&quot;True label is &quot;, true_label)
single_test_pred_prob = streaming_model.predict(test_single)
pred_label = language_name(np.argmax(single_test_pred_prob))
print(&quot;Predicted label is &quot;, pred_label)
print(&quot;***********************&quot;)
# ---------------------------------------------------------------

# ---------------------------BLOCK 6------------------------------------
## COMMENT/UNCOMMENT BELOW
# Prediction for all sequences in the validation set - Takes very long to run
print(&quot;Predicting labels for all sequences - (Will take a lot of time)&quot;)
list_pred_labels = []
for i in range(X_val.shape[0]):
    for j in range(X_val.shape[1]):
        test = X_val[i][j].reshape(1, 1, 64)
        seq_predictions_prob = streaming_model.predict(test)
        predicted_language_index = np.argmax(seq_predictions_prob)
        list_pred_labels.append(predicted_language_index)
pred_english = list_pred_labels.count(0)
pred_hindi = list_pred_labels.count(1)
pred_mandarin = list_pred_labels.count(2)
print(&quot;Number of English labels = &quot;, pred_english)
print(&quot;Number of Hindi labels = &quot;, pred_hindi)
print(&quot;Number of Mandarin labels = &quot;, pred_mandarin)
# ---------------------------------------------------------------
```

```Traceback (most recent call last):
  File &quot;C:\Users\SKYLAND-2\Documents\nipunmanral SLR\language_identification.py&quot;, line 79, in &lt;module&gt;
    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)
  File &quot;C:\Users\SKYLAND-2\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_split.py&quot;, line 2417, in train_test_split
    arrays = indexable(*arrays)
  File &quot;C:\Users\SKYLAND-2\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py&quot;, line 378, in indexable
    check_consistent_length(*result)
  File &quot;C:\Users\SKYLAND-2\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py&quot;, line 332, in check_consistent_length
    raise ValueError(
ValueError: Found input variables with inconsistent numbers of samples: [3, 0]```
</code></pre>
<p>hi, i am trying to run the code which belong to nipunmanral spoken language identification and i received this error. this is my first time learning machine learning, i am trying to learn spoken language identification which classify what type of language from an audio. i hope someone can share some tutorial or fix the error.</p>
",25,0,0,3,python;speech-recognition;spoken-language,2022-05-12 13:46:49,2022-05-12 13:46:49,2022-05-12 13:46:49,hi  i am trying to run the code which belong to nipunmanral spoken language identification and i received this error  this is my first time learning machine learning  i am trying to learn spoken language identification which classify what type of language from an audio  i hope someone can share some tutorial or fix the error 
651,651,3448011,71714299,tensorflow 2 TextVectorization process tensor and dataset error,"<p>I would like to process text with tensorflow 2.8 on Jupyter notebook.</p>
<p>my code:</p>
<pre><code>import re
import string
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_text as tf_text

def standardize(input_data):
    lowercase_str = tf.strings.lower(input_data)
    a_str = tf.strings.regex_replace(lowercase_str, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot;&quot;)
    tokenizer = tf_text.WhitespaceTokenizer()
    tokens = tokenizer.tokenize(a_str)
    return tokens

# the input data loaded from text files by TfRecordDataset(file_paths, &quot;GZIP&quot;)
# each file can be 200+MB, totally about 300 files
# each file hold the data with multiple columns
# some columns are text
# after loading, the dataset will be accessed by column name 
# e.g. one column is &quot;sports&quot;, so the input_dataset[&quot;sports&quot;] 
# return a tensor, which is like the following example
my_data_tensor = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]])

tf.print(my_data_tensor)
tf.print(my_data_tensor.shape)
tf.print(f&quot;type is {type(my_data_tensor)}&quot;)
text_layer = layers.TextVectorization(
                        standardize = standardize,
                        max_tokens = 10,
                        output_mode = 'int',
                        output_sequence_length=10
                       )

my_dataset = tf.data.Dataset.from_tensor_slices(my_data_tensor)
text_layer.adapt(my_dataset.batch(2)) # error         
processed_text = text_layer(my_dataset)

error:
 ValueError: Exception encountered when calling layer &quot;query_tower&quot; (type QueryTower).
 When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(2, 1, None) with rank=3
</code></pre>
<p>I have tried tf.unstack() and tf.reshape, tf.unbatch, but none of them work.
For the given example:</p>
<pre><code>[[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]]
</code></pre>
<p>what I need:</p>
<pre><code>[[&quot;swim 200807 baseball&quot;], [&quot;football&quot;]]
then
it will be encoded as int by the &quot;text_layer&quot;
</code></pre>
<p>these data  (bach_size=2) will be used for a machine learning model as features.</p>
<p>Did I do something wrong ? thanks</p>
",368,2,2,5,tensorflow;dataset;tensorflow2.0;tensor;text-processing,2022-04-02 07:27:03,2022-04-02 07:27:03,2022-05-12 13:32:09,i would like to process text with tensorflow   on jupyter notebook  my code  what i need  these data   bach_size   will be used for a machine learning model as features  did i do something wrong   thanks
652,652,14252319,72200316,How to parse years of experience from resume from Experience field present in resume?,"<p>I am working with a resume parser in Django that can parse all the data but I want to calculate years of experience from the dates mentioned in the resume in the experience field I have come up with a strategy that we can parse the experience section and parse all the dates but I am facing a hard time to implement it. Is there any other way to calculate the experience from different dates and add all the dates?</p>
<pre><code>def extract_experience(resume_text):
'''
Helper function to extract experience from resume text

:param resume_text: Plain resume text
:return: list of experience
'''
    wordnet_lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

# word tokenization 
    word_tokens = nltk.word_tokenize(resume_text)

# remove stop words and lemmatize  
    filtered_sentence = [w for w in word_tokens if not w in stop_words and 
    wordnet_lemmatizer.lemmatize(w) not in stop_words] 
    sent = nltk.pos_tag(filtered_sentence)

# parse regex
   cp = nltk.RegexpParser('P: {&lt;NNP&gt;+}')
   cs = cp.parse(sent)

# for i in cs.subtrees(filter=lambda x: x.label() == 'P'):
#     print(i)

   test = []

   for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):
    test.append(&quot; &quot;.join([i[0] for i in vp.leaves() if len(vp.leaves()) &gt;= 2]))

# Search the word 'experience' in the chunk and then print out the text after it
    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 
    'experience' in x.lower()]
    return x
</code></pre>
<p>This function extracts the experience from previous companies which includes text so I want to extract years or months of experience from it and store it in a field.</p>
<p><strong>This is the main question</strong>
This is the text filed and I want to extract years from this and add all the years and months in any format? Is there any way to specify start and end dates and add them and return total years of experience?</p>
<blockquote>
<p>SpiceJet(08/2021 - 10/2021), ❖, Leading Indian airlines company worked and Developed an analytical tool that, can regulate data  .(02/2022 - 03/2022 ), ❖, Tutorbin is an integrated online tutoring platform serving as a one-stop, solution AI and Machine learning Intern, Dawn DigiTech (04/2022 -present), ❖, This company digitally transform multiple front- and back-office</p>
</blockquote>
<p>I have another sub-question in this if we calculate total experience how can we calculate relevant experience if I need only those dates which match the Job description and add only those dates is there any way we can calculate those years of experience.
For example: if I have 3 years of hotel management experience and 1 year of software developer experience and I want to take only relevant software developer experience and count those years only how we can do that. and store in relevant experience field.</p>
<p>This is the current output but the experience field function is not there .</p>
<p>How can we make a function to extract years of experience and relevant experience from resume?</p>
<p><a href=""https://i.stack.imgur.com/VkTJj.png"" rel=""nofollow noreferrer"">Current output</a></p>
",95,0,0,5,python;django;machine-learning;deep-learning;nlp,2022-05-11 17:02:58,2022-05-11 17:02:58,2022-05-12 11:42:34,i am working with a resume parser in django that can parse all the data but i want to calculate years of experience from the dates mentioned in the resume in the experience field i have come up with a strategy that we can parse the experience section and parse all the dates but i am facing a hard time to implement it  is there any other way to calculate the experience from different dates and add all the dates  this function extracts the experience from previous companies which includes text so i want to extract years or months of experience from it and store it in a field  spicejet            leading indian airlines company worked and developed an analytical tool that  can regulate data                tutorbin is an integrated online tutoring platform serving as a one stop  solution ai and machine learning intern  dawn digitech     present      this company digitally transform multiple front  and back office this is the current output but the experience field function is not there   how can we make a function to extract years of experience and relevant experience from resume  
653,653,19099101,72210450,Is there any way to create or delete workspaces in AML studio using powershell?,"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?</p>
",72,1,0,3,azure;powershell;azure-machine-learning-studio,2022-05-12 10:51:34,2022-05-12 10:51:34,2022-05-12 11:02:02,i am working on a prediction model and am about to use the azure machine learning studio resources  the main operation is to create a workspace on azure ml studio through powershell  i would like to operate my workspace through the command line  is there any way to develop and operate the ml studio workspace through powershell 
654,654,4961730,72205448,"C++ Arduino KNN Split matrix, merge results","<p>I am new in Machine learning but I decided to develop a small program for Arduino. The idea is that we have two microcontrollers that communicate via the I2C protocol and some kind of data set that we break, for example, into two parts, then we use the KNN algorithm for each divided data set and then combine the result. I would like to know how correct my approach is in terms of machine learning.</p>
<p>This is the KNN algorithm:</p>
<pre><code>  struct Point
  {
    int val;
    int x, y;
    int distance;
  };

  bool comparison(Point a, Point b)
  {
    return a.distance &lt; b.distance;
  }

  int classifyAPoint(Point arr[], int n, int k, Point p)
  {
    for(int i = 0; i &lt; n; ++i)
    {
      arr[i].distance = sqrt((arr[i].x - p.x) * (arr[i].x - p.x) + (arr[i].y + p.y) * (arr[i].y - p.y));
    }

    int lt_length = sizeof(arr) / sizeof(arr[0]);
    
    Core::Sort::sortArr(arr, lt_length+n, comparison);

    int freq1 = 0;
    int freq2 = 0;

    for (int i = 0; i &lt; k; i++)
    {
      if (arr[i].val == 0)
      {
        freq1++;
      }
      else if (arr[i].val == 1)
      {
        freq2++;
      }
    }

    return (freq1 &gt; freq2 ? 0 : 1);
  }
</code></pre>
<p>That's how I devide data</p>
<pre><code>template&lt;typename T, typename V&gt;
void get_slice_(T arr[], V size, T arr1[], T arr2[], V pos, V k1 = 0, V k2 = 0)
{
  for(int i = 0; i &lt; size; i++)
  {
    if(i &lt; pos)
    {
      arr1[k1++] = arr[i];
    }
    else
    {
      arr2[k2++] = arr[i];
    }
  }
}
</code></pre>
<p>And in main function I have this part:</p>
<pre><code>Point arr[n];

arr[0].x = 1;
arr[0].y = 12;
arr[0].val = 0;

arr[1].x = 2;
arr[1].y = 5;
arr[1].val = 0;

arr[2].x = 5;
arr[2].y = 3;
arr[2].val = 1;

arr[3].x = 3;
arr[3].y = 2;
arr[3].val = 1;

arr[4].x = 3;
arr[4].y = 6;
arr[4].val = 0;

arr[5].x = 1.5;
arr[5].y = 9;
arr[5].val = 1;

arr[6].x = 7;
arr[6].y = 2;
arr[6].val = 1;

arr[7].x = 6;
arr[7].y = 1;
arr[7].val = 1;

arr[8].x = 3.8;
arr[8].y = 3;
arr[8].val = 1;

arr[9].x = 3;
arr[9].y = 10;
arr[9].val = 0;

arr[10].x = 5.6;
arr[10].y = 4;
arr[10].val = 1;

arr[11].x = 4;
arr[11].y = 2;
arr[11].val = 1;

arr[12].x = 3.5;
arr[12].y = 8;
arr[12].val = 0;

arr[13].x = 2;
arr[13].y = 11;
arr[13].val = 0;

arr[14].x = 2;
arr[14].y = 5;
arr[14].val = 1;

arr[15].x = 2;
arr[15].y = 9;
arr[15].val = 0;

arr[16].x = 1;
arr[16].y = 7;
arr[16].val = 0;

/*Testing Point*/
Core::AI::KNN::Point p;
p.x = 2.5;
p.y = 7;

// Parameter to decide group of the testing point
int k = 3;

// Distribution
Core::AI::KNN::Point block[8];
Core::AI::KNN::Point block_[8];

Core::DivideMatrix::get_slice_(arr, 16, block, block_, 8);

Serial.println(&quot;Start classify using KNN&quot;);
Serial.println(Core::AI::KNN::classifyAPoint(block, 9, k, p));
</code></pre>
<p>So is it correct in terms of dividing into some parts of data, if yes then I have another question, how to merge the final result?</p>
<p><strong>P.S.</strong></p>
<p>This applies not only to the KNN algorithm but also to other machine learning algorithms such as LinearRegression, etc.</p>
",21,0,0,4,c++;algorithm;machine-learning;arduino,2022-05-11 23:17:06,2022-05-11 23:17:06,2022-05-11 23:27:22,i am new in machine learning but i decided to develop a small program for arduino  the idea is that we have two microcontrollers that communicate via the ic protocol and some kind of data set that we break  for example  into two parts  then we use the knn algorithm for each divided data set and then combine the result  i would like to know how correct my approach is in terms of machine learning  this is the knn algorithm  that s how i devide data and in main function i have this part  so is it correct in terms of dividing into some parts of data  if yes then i have another question  how to merge the final result  p s  this applies not only to the knn algorithm but also to other machine learning algorithms such as linearregression  etc 
655,655,17855046,72161405,"Input 0 of layer &quot;global_average_pooling1d&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 16)","<p>I just started to learn Tensorflow and got an error. I watched a <a href=""https://www.youtube.com/watch?v=IpYmz3_BUM0&amp;list=PLzMcBGfZo4-lak7tiFDec5_ZMItiIIfmj&amp;index=7"" rel=""nofollow noreferrer"">Neural Network Tutorial of Tech with Tim</a>. I finished this episode and got a problem to the end. I couldn't predict the value. when i try it this error shows up:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/havil/Documents/GitHub/Python/Machine-Learning-Tutorial/Neural Network Tutorial/Tutorial 2.py&quot;, line 56, in &lt;module&gt;
    predict = model.predict([test_review])
  File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1147, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\engine\training.py&quot;, line 1801, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\engine\training.py&quot;, line 1790, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\engine\training.py&quot;, line 1783, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\engine\training.py&quot;, line 1751, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;C:\Users\havil\anaconda3\envs\tf_3.7\lib\site-packages\keras\engine\input_spec.py&quot;, line 214, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;sequential&quot; (type Sequential).
    
    Input 0 of layer &quot;global_average_pooling1d&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 16)
    
    Call arguments received:
      • inputs=('tf.Tensor(shape=(None,), dtype=int32)',)
      • training=False
      • mask=None
</code></pre>
<p>My whole code looks like this:</p>
<pre><code>    import tensorflow as tf
    from tensorflow import keras
    import numpy as np
    
    data = keras.datasets.imdb
    
    (train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=10000)
    
    print(train_data[0])
    
    # decode Data
    word_index = data.get_word_index()
    
    word_index = {key: (value+3) for key, value in word_index.items()}
    word_index[&quot;&lt;PAD&gt;&quot;] = 0
    word_index[&quot;&lt;START&gt;&quot;] = 1
    word_index[&quot;&lt;UNK&gt;&quot;] = 2
    word_index[&quot;&lt;UNUSED&gt;&quot;] = 3
    
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    
    
    train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[&quot;&lt;PAD&gt;&quot;], padding=&quot;post&quot;, maxlen=250)
    test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[&quot;&lt;PAD&gt;&quot;], padding=&quot;post&quot;, maxlen=250)
    
    
    def decode_review(text):
        return &quot; &quot;.join([reverse_word_index.get(i, &quot;?&quot;) for i in text])
    
    
    print(decode_review(test_data[0]))
    
    model = keras.Sequential()
    model.add(keras.layers.Embedding(10000, 16))
    model.add(keras.layers.GlobalAveragePooling1D())
    model.add(keras.layers.Dense(16, activation=&quot;relu&quot;))
    model.add(keras.layers.Dense(1, activation=&quot;sigmoid&quot;))
    
    model.summary()
    
    model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
    
    x_val = train_data[:10000]
    x_train = train_data[10000:]
    
    y_val = train_labels[:10000]
    y_train = train_labels[10000:]
    
    fitModel = model.fit(x_train, y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), verbose=1)
    
    results = model.evaluate(test_data, test_labels)
    
    print(results)
    
    test_review = test_data[0]
    predict = model.predict([test_review])
    print(&quot;Review: &quot;)
    print(decode_review(test_review))
    print(&quot;Prediction: &quot; + str(predict[0]))
    print(&quot;Actual: &quot; + str(test_labels[0]))
    print(results)
</code></pre>
<p>Does anyone have an idea how to fix this error?</p>
",111,2,0,5,tensorflow;keras;layer;predict;sequential,2022-05-08 18:48:41,2022-05-08 18:48:41,2022-05-11 21:44:44,i just started to learn tensorflow and got an error  i watched a   i finished this episode and got a problem to the end  i couldn t predict the value  when i try it this error shows up  my whole code looks like this  does anyone have an idea how to fix this error 
656,656,19071056,72165193,How do I fix this InvalidArgumentError?,"<p>I'm working on this chess algorithm for my machine learning class, but I'm not sure what's going wrong. I'm following a video <a href=""https://www.youtube.com/watch?v=ffzvhe97J4Q"" rel=""nofollow noreferrer"">Here</a>, but it seems everything goes wrong when I try to fit my model. I've attached the code below, which sets up a chess board and then a convolutional network. I keep getting the error:</p>
<pre><code>InvalidArgumentError: Graph Execution Error which points to model.fit(x_train, y_train). 
The size of x_train is (150000, 14, 8, 8) while y_train is (150000, ) 
</code></pre>
<p>Code:</p>
<pre><code> def random_board(max_depth=200):
        board = chess.Board()
        depth = random.randrange(0, max_depth)
        
        for _ in range(depth):
            all_moves = list(board.legal_moves)
            random_move = random.choice(all_moves)
            board.push(random_move)
            if board.is_game_over():
                break
        return board          
    
    
      squares_index = {
          'a': 0,
          'b': 1,
          'c': 2,
          'd': 3,
          'e': 4,
          'f': 5,
          'g': 6,
          'h': 7
        }
        
        
        # example: h3 -&gt; 17
        def square_to_index(square):
            letter = chess.square_name(square)
            return 8 - int(letter[1]), squares_index[letter[0]]
        
        
        def split_dims(board):
          # create empty 3d matrix for board 
            board3d = numpy.zeros((14, 8, 8), dtype=numpy.int8)
          # here we add the pieces's view on the matrix
            for piece in chess.PIECE_TYPES:
                for square in board.pieces(piece, chess.WHITE):
                    idx = numpy.unravel_index(square, (8, 8))
                    board3d[piece - 1][7 - idx[0]][idx[1]] = 1
            for square in board.pieces(piece, chess.BLACK):
                    idx = numpy.unravel_index(square, (8, 8))
                    board3d[piece + 5][7 - idx[0]][idx[1]] = 1
        
          # add attacks and valid moves too
          # so the network knows what is being attacked
            aux = board.turn
            board.turn = chess.WHITE
            for move in board.legal_moves:
                i, j = square_to_index(move.to_square)
                board3d[12][i][j] = 1
                board.turn = chess.BLACK
            for move in board.legal_moves:
                i, j = square_to_index(move.to_square)
                board3d[13][i][j] = 1
            board.turn = aux
        
            return board3d
    
    import tensorflow.keras.models as models
    import tensorflow.keras.layers as layers
    import tensorflow.keras.utils as utils
    import tensorflow.keras.optimizers as optimizers 
    
    def build_model(conv_size, conv_depth):
        board3d = layers.Input(shape=(14, 8, 8))
        
        #convolutional layers
        x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(board3d)
        for _ in range(conv_depth):
            previous = x
            x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)
            x = layers.BatchNormalization()(x)
            x = layers.Activation('relu')(x)
            x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)
            x = layers.BatchNormalization()(x)
            x = layers.Add()([x, previous])
            x = layers.Activation('relu')(x)
        x = layers.Flatten()(x)
        x = layers.Dense(1, 'sigmoid')(x)
    
        return models.Model(inputs=board3d, outputs=x)
    
    model = build_model(32, 4)
    utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=False)
    
    import tensorflow.keras.callbacks as callbacks
    
    def get_dataset():
        container = numpy.load('dataset\\dataset.npz')
        b, v = container['b'], container['v']
        v = numpy.asarray(v / abs(v).max()/2 + 0.5, dtype=numpy.float32) #normalize 
        return b, v
    
    x_train, y_train = get_dataset()
    
    model.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')
    model.summary()
    model.fit(x_train, y_train,
              batch_size=2048,
              epochs=1000,
              verbose=1,
              validation_split=0.1,
              callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),
                         callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])
    model.save('model.h5')
</code></pre>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_15172/3395566405.py in &lt;module&gt;
      1 model.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')
      2 model.summary()
----&gt; 3 model.fit(x_train, y_train,
      4           batch_size=2048,
      5           epochs=1000,

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---&gt; 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:
</code></pre>
<p>Edit: Output log for model.summary()</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 14, 8, 8)]   0           []                               
                                                                                                  
 conv2d (Conv2D)                (None, 32, 8, 8)     4064        ['input_1[0][0]']                
                                                                                                  
 conv2d_1 (Conv2D)              (None, 32, 8, 8)     9248        ['conv2d[0][0]']                 
                                                                                                  
 batch_normalization (BatchNorm  (None, 32, 8, 8)    32          ['conv2d_1[0][0]']               
 alization)                                                                                       
                                                                                                  
 activation (Activation)        (None, 32, 8, 8)     0           ['batch_normalization[0][0]']    
                                                                                                  
 conv2d_2 (Conv2D)              (None, 32, 8, 8)     9248        ['activation[0][0]']             
                                                                                                  
 batch_normalization_1 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_2[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 add (Add)                      (None, 32, 8, 8)     0           ['batch_normalization_1[0][0]',  
                                                                  'conv2d[0][0]']                 
                                                                                                  
 activation_1 (Activation)      (None, 32, 8, 8)     0           ['add[0][0]']                    
                                                                                                  
 conv2d_3 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_1[0][0]']           
                                                                                                  
 batch_normalization_2 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_3[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 activation_2 (Activation)      (None, 32, 8, 8)     0           ['batch_normalization_2[0][0]']  
                                                                                                  
 conv2d_4 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_2[0][0]']           
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_4[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 add_1 (Add)                    (None, 32, 8, 8)     0           ['batch_normalization_3[0][0]',  
                                                                  'activation_1[0][0]']           
                                                                                                  
 activation_3 (Activation)      (None, 32, 8, 8)     0           ['add_1[0][0]']                  
                                                                                                  
 conv2d_5 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_3[0][0]']           
                                                                                                  
 batch_normalization_4 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_5[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 activation_4 (Activation)      (None, 32, 8, 8)     0           ['batch_normalization_4[0][0]']  
                                                                                                  
 conv2d_6 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_4[0][0]']           
                                                                                                  
 batch_normalization_5 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_6[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 add_2 (Add)                    (None, 32, 8, 8)     0           ['batch_normalization_5[0][0]',  
                                                                  'activation_3[0][0]']           
                                                                                                  
 activation_5 (Activation)      (None, 32, 8, 8)     0           ['add_2[0][0]']                  
                                                                                                  
 conv2d_7 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_5[0][0]']           
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_7[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 activation_6 (Activation)      (None, 32, 8, 8)     0           ['batch_normalization_6[0][0]']  
                                                                                                  
 conv2d_8 (Conv2D)              (None, 32, 8, 8)     9248        ['activation_6[0][0]']           
                                                                                                  
 batch_normalization_7 (BatchNo  (None, 32, 8, 8)    32          ['conv2d_8[0][0]']               
 rmalization)                                                                                     
                                                                                                  
 add_3 (Add)                    (None, 32, 8, 8)     0           ['batch_normalization_7[0][0]',  
                                                                  'activation_5[0][0]']           
                                                                                                  
 activation_7 (Activation)      (None, 32, 8, 8)     0           ['add_3[0][0]']                  
                                                                                                  
 flatten (Flatten)              (None, 2048)         0           ['activation_7[0][0]']           
                                                                                                  
 dense (Dense)                  (None, 1)            2049        ['flatten[0][0]']                
                                                                                                  
==================================================================================================
Total params: 80,353
Trainable params: 80,225
Non-trainable params: 128
__________________________________________________________________________________________________
</code></pre>
",84,1,1,3,python;tensorflow;machine-learning,2022-05-09 02:53:59,2022-05-09 02:53:59,2022-05-11 20:56:55,i m working on this chess algorithm for my machine learning class  but i m not sure what s going wrong  i m following a video   but it seems everything goes wrong when i try to fit my model  i ve attached the code below  which sets up a chess board and then a convolutional network  i keep getting the error  code  edit  output log for model summary  
657,657,18768905,72203109,JAWS screen reader is announcing both the name of the two buttons when navigating on second button,"<p>While navigating the &quot;unsubscribe&quot; button screen reader announcing both elements as view results and unsubscribe</p>
<pre><code>           &lt;div  role=&quot;cell&quot; &gt;
                        &lt;div &gt;
                            &lt;span &gt;
                                &lt;a  aria-label=&quot;Data Science &amp;amp; Machine Learning View Results&quot; &gt;
                                  View Results 
                                &lt;/a&gt;
                            &lt;/span&gt;
                            &lt;span&gt;
                                &lt;button role=&quot;button&quot; 
                                  aria-label=&quot;Data Science &amp;amp; Machine Learning Unsubscribe&quot;&gt;  
                                  Unsubscribe 
                                &lt;/button&gt;
                            &lt;/span&gt;
                        &lt;/div&gt;
                    &lt;/div&gt;
</code></pre>
",21,0,0,2,accessibility;jaws-screen-reader,2022-05-11 20:17:02,2022-05-11 20:17:02,2022-05-11 20:17:02,while navigating the  unsubscribe  button screen reader announcing both elements as view results and unsubscribe
658,658,7160346,72193802,Can&#39;t edit the cluster created by mlflow model serving,"<p>I'm trying to deploy  Machine learning model into databricks production using mlflow. while in that process, I have registered the model to mlflow models. After that it created the cluster but then it was in pending state forever. when I checked the model events, I see a problem with https proxy, we have global init scripts which contain proxy information.</p>
<p><a href=""https://i.stack.imgur.com/qF9MT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qF9MT.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://docs.databricks.com/applications/mlflow/model-serving.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/applications/mlflow/model-serving.html</a></p>
<p>so the only way for us to edit the cluster and add them but in that process we are getting an error &quot;error: Cannot edit cluster created by ModelServing&quot;.</p>
<pre><code>[Errno 101] Network is unreachable',)': /simple/mlflow/ WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f258247f710&gt;: Failed to establish a new connection:
</code></pre>
<p>In the &quot;Model Events page&quot;, I see the above logs,</p>
",66,1,0,3,databricks;azure-databricks;mlflow,2022-05-11 04:37:09,2022-05-11 04:37:09,2022-05-11 17:17:48,i m trying to deploy  machine learning model into databricks production using mlflow  while in that process  i have registered the model to mlflow models  after that it created the cluster but then it was in pending state forever  when i checked the model events  i see a problem with https proxy  we have global init scripts which contain proxy information   ref   so the only way for us to edit the cluster and add them but in that process we are getting an error  error  cannot edit cluster created by modelserving   in the  model events page   i see the above logs 
659,659,15109488,72200225,Is there a way I run this script?,"<p>I have a large dataset that I intend generating a sample of 10% from it to run my machine learning model 20 times. To test how it will work, I decided to use <code>iris</code> dataset to try it. First, I split the dataset into training and testing dataset and then used a <code>While loop</code> to try a simple loop but it doesn't seem to work as I got an error message. Please is there something I missed out?</p>
<pre><code>      ### partitioning dataset

      part &lt;- sample(1:150, size = 100, replace = F)
      training &lt;- iris[part,]
      testing &lt;- iris[-part,]

      ## using a loop 
      n &lt;-1
      while (n&lt;6) {
            Train(n)&lt;-training[sample(1:100,0.3*nrow(training), replace = F),]
            fit &lt;- randomForest(Species~., data = Train(n))
            pred &lt;- predict(fit, testing)
            confusionMatrix(pred, testing$Species))
            n &lt;-n+1
      }
</code></pre>
<p>The error message I got is</p>
<pre><code>      Error: unexpected '}' in &quot;}&quot;
</code></pre>
",32,1,1,2,r;machine-learning,2022-05-11 16:56:37,2022-05-11 16:56:37,2022-05-11 17:11:34,i have a large dataset that i intend generating a sample of   from it to run my machine learning model  times  to test how it will work  i decided to use iris dataset to try it  first  i split the dataset into training and testing dataset and then used a while loop to try a simple loop but it doesn t seem to work as i got an error message  please is there something i missed out  the error message i got is
660,660,11189133,72196760,Meaning of diff_from_typical in ElasticSearch Machine Learning custom rules,"<p>When configuring Machine Learning jobs in ES, you can customise your detectors by using <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/master/ml-put-job.html#put-customrules"" rel=""nofollow noreferrer"">custom_rules</a>.</p>
<p>I'm wondering about the actual meaning of the <code>diff_from_typical</code> (one of the values that <code>applies_to</code> can take). My main question is if <code>diff_from_typical</code> considers absolute difference or not. I know that you can use <code>lt</code> or <code>gt</code> operators later (among others) but let's image the following situation:</p>
<p>I have a custom rule for two jobs. The rule is the same but the cases scenarios are different. Let's say that the custom rule is:</p>
<pre><code>&quot;custom_rules&quot;: [{
        &quot;actions&quot;: [&quot;skip_model_update&quot;],
        &quot;conditions&quot;: [
          {
            &quot;applies_to&quot;: &quot;diff_from_typical&quot;,
            &quot;operator&quot;: &quot;gt&quot;,
            &quot;value&quot;: 2000
          }
        ]
      }]
</code></pre>
<p>Case scenario A:</p>
<ul>
<li>Typical value: 5000</li>
<li>Actual value: 2000</li>
<li>diff_from_typical: 5000 - 2000 = 3000</li>
</ul>
<p>Case scenario B:</p>
<ul>
<li>Typical value: 5000</li>
<li>Actual value: 8000</li>
<li>diff_from_typical: 5000 - 8000 = -3000</li>
</ul>
<p>Will the aforementioned custom rule apply in both cases? I mean, using the absolute difference from typical? Or will it only work in the first case (case A)?</p>
<p>I assume that if it only works for the first case, I should write the &quot;inverse&quot; custom rule to manage both cases.</p>
<p>Thanks in advance!</p>
",7,1,0,1,elasticsearch,2022-05-11 12:36:44,2022-05-11 12:36:44,2022-05-11 17:01:52,when configuring machine learning jobs in es  you can customise your detectors by using   i m wondering about the actual meaning of the diff_from_typical  one of the values that applies_to can take   my main question is if diff_from_typical considers absolute difference or not  i know that you can use lt or gt operators later  among others  but let s image the following situation  i have a custom rule for two jobs  the rule is the same but the cases scenarios are different  let s say that the custom rule is  case scenario a  case scenario b  will the aforementioned custom rule apply in both cases  i mean  using the absolute difference from typical  or will it only work in the first case  case a   i assume that if it only works for the first case  i should write the  inverse  custom rule to manage both cases  thanks in advance 
661,661,3582598,71846305,How to deploy a ML model as a local iot edge module,"<p>I have a machine learning model registered in the model registry of my Azure Machine Learning workspace.</p>
<p>Now I want to containerize such model inside a <strong>Linux docker image</strong> exposing a rest api; then, I have to deploy it as an IoT Edge module to an edge PC, where other modules will invoke it locally and receive predictions.</p>
<p>I have searched in Microsoft documentation, but I haven't found a solution to my problem, since the suggested examples and tutorials talk about deploying entire iot edge solutions or using azure ml cli to deploy models, instead I have to add this module to an existing deployment manifest where other modules are already present.</p>
",113,1,0,2,azure-iot-edge;azure-machine-learning-service,2022-04-12 22:07:35,2022-04-12 22:07:35,2022-05-11 16:14:20,i have a machine learning model registered in the model registry of my azure machine learning workspace  now i want to containerize such model inside a linux docker image exposing a rest api  then  i have to deploy it as an iot edge module to an edge pc  where other modules will invoke it locally and receive predictions  i have searched in microsoft documentation  but i haven t found a solution to my problem  since the suggested examples and tutorials talk about deploying entire iot edge solutions or using azure ml cli to deploy models  instead i have to add this module to an existing deployment manifest where other modules are already present 
662,662,13272706,72199091,Azure SQL query inside an Azure ML Pipeline,"<p>So I'm completely new to Azure, so this may not make any sense, but what I want to do is to make a query to an Azure SQL and register this as a step in an Azure ML Pipeline using Python code. I am sorry that the code examples below are not complete or reproducible.</p>
<p>In short: I want to replicate the import data <strong>designer</strong> in azure ML using a python azure pipeline and the PythonScriptStep function (if you know what I mean then just ignore the code below).</p>
<p>That is, I want to register the following function so that I can use it in a Pipeline:</p>
<pre><code>def import_data(datastore_name, query, workspace):
    datastore = Datastore.get(workspace, datastore_name)
    query = DataPath(datastore, query)
    df = Dataset.Tabular.from_sql_query(query, query_timeout=10)
    df = df.to_pandas_dataframe()
    return df
</code></pre>
<p>I want the function above to be part of an &quot;import_data_step&quot; in the ML pipeline which would include a PythonScriptStep, e.g. as shown below:</p>
<pre><code>my_data = import_data(datastore_name, query, workspace)
import_step = PythonScriptStep(
    name=&quot;import_data&quot;,
    script_name=&quot;import_data.py&quot;,
    compute_target=cluster,
    source_directory='./import_src',
    inputs=[mydata.as_named_input('mydata').as_mount()]
)
</code></pre>
<p>I know about the link below, but I still cannot figure out how to do it (not enough code examples in the docs). <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines</a></p>
",36,0,0,2,python;azure,2022-05-11 15:32:18,2022-05-11 15:32:18,2022-05-11 15:32:18,so i m completely new to azure  so this may not make any sense  but what i want to do is to make a query to an azure sql and register this as a step in an azure ml pipeline using python code  i am sorry that the code examples below are not complete or reproducible  in short  i want to replicate the import data designer in azure ml using a python azure pipeline and the pythonscriptstep function  if you know what i mean then just ignore the code below   that is  i want to register the following function so that i can use it in a pipeline  i want the function above to be part of an  import_data_step  in the ml pipeline which would include a pythonscriptstep  e g  as shown below  i know about the link below  but i still cannot figure out how to do it  not enough code examples in the docs   
663,663,19090141,72194943,How to find location of the Microsoft Machine Learning Server installation files in SQL 2019,"<p><a href=""https://i.stack.imgur.com/NRWUA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am stuck here and can't find location of the Microsoft Machine Learning Server installation files. If any one know please answer on this issue.</p>
",82,0,0,2,sql-server;machine-learning,2022-05-11 08:36:51,2022-05-11 08:36:51,2022-05-11 13:41:37, i am stuck here and can t find location of the microsoft machine learning server installation files  if any one know please answer on this issue 
664,664,13080984,72194640,Unknown number of columns to create PredictionEngine in ML.NET,"<p>I have been writing a program to create custom Models for Machine Learning.
The user will read a .csv or .txt and pass that data to a DatagridView.</p>
<p>The DataGridView contains the columns &quot;A&quot;, &quot;B&quot; and &quot;C&quot; (apart from &quot;Label&quot; column, which is the result), I created a model inserting all that columns in a <code>List (of string)</code> and concatenate then like this:(lista2 -&gt; &quot;A&quot;,&quot;B&quot;,&quot;C&quot; / Lista value -&gt; &quot;Label&quot;)</p>
<pre><code> Dim estimator = mlContext.Transforms.Concatenate(&quot;Features&quot;, lista2.ToArray) _
        .Append(mlContext.BinaryClassification.Trainers.FastTree(labelColumnName:=Lista(Lista.Count - 1), featureColumnName:=&quot;Features&quot;))
</code></pre>
<p>After that, I have 2 classes named NewData and NewDataPrediction with the following properties:</p>
<pre><code>Public Class NewData

&lt;LoadColumn(0)&gt;
Public A As Single
&lt;LoadColumn(1)&gt;
Public B As Single
&lt;LoadColumn(2)&gt;
Public C As Single
&lt;LoadColumn(3)&gt;
Public Label As Boolean


Public Class NewDataPrediction
Inherits NewData

&lt;ColumnName(&quot;PredictedLabel&quot;)&gt;
Public Property Prediction As Boolean
Public Property Probability As Single
Public Property Score As Single
</code></pre>
<p>After this, I create the PredictionEngine</p>
<pre><code> Dim predictionFunction As PredictionEngine(Of NewData, NewDataPrediction) = mlContext.Model.CreatePredictionEngine(Of NewData, NewDataPrediction)(model)
</code></pre>
<p>The model is created with no problem and if I assign some values, It predictes everything right.</p>
<p>The problem is the following, Since the program reads the Text File, it can contain a different number of Columns with different names. How can I create the PredictionEngineModel with NewData(where I would like to insert all new columns, in an array,list or something and read the columns inside that list as properties in the class), since I cant create a class with properties at runtime. How can I approach this problem? Thanks</p>
",25,0,0,3,vb.net;machine-learning;ml.net,2022-05-11 07:35:53,2022-05-11 07:35:53,2022-05-11 12:10:22,the datagridview contains the columns  a    b  and  c   apart from  label  column  which is the result   i created a model inserting all that columns in a list  of string  and concatenate then like this  lista   gt   a   b   c    lista value   gt   label   after that  i have  classes named newdata and newdataprediction with the following properties  after this  i create the predictionengine the model is created with no problem and if i assign some values  it predictes everything right  the problem is the following  since the program reads the text file  it can contain a different number of columns with different names  how can i create the predictionenginemodel with newdata where i would like to insert all new columns  in an array list or something and read the columns inside that list as properties in the class   since i cant create a class with properties at runtime  how can i approach this problem  thanks
665,665,19091064,72196217,Matrix Interpolation using ML technique,"<p>I am currently having a matrix of input size -&gt; (512 X 7)with values in it. I would like to interpolate to twice its size i.e. the output size is -&gt; 1024 X 14 using any Machine Learning technique.</p>
<p>I am currently stuck with it as there are no features or labels associated with it. I have read some papers which employ Random Forest for this, but they all have features in their case.
Can anyone guide me as to how I may approach this ?</p>
",12,0,0,4,machine-learning;matrix;interpolation;sparse-matrix,2022-05-11 11:47:05,2022-05-11 11:47:05,2022-05-11 11:47:05,i am currently having a matrix of input size   gt    x  with values in it  i would like to interpolate to twice its size i e  the output size is   gt   x  using any machine learning technique 
666,666,10345435,52269187,Facing ValueError: Target is multiclass but average=&#39;binary&#39;,"<p>I'm a newbie to python as well as machine learning. As per my requirement, I'm trying to use Naive Bayes algorithm for my dataset.</p>
<p>I'm able to find out the accuracy but trying to find out precision and recall for the same. But, it is throwing the following error:</p>
<pre><code>ValueError: Target is multiclass but average='binary'. Please choose another average setting.
</code></pre>
<p>Can anyone please suggest me how to proceed with it. I have tried using average ='micro' in the precision and the recall scores.It worked without any errors but it is giving the same score for accuracy, precision, recall.</p>
<h1>My dataset:</h1>
<h1>train_data.csv:</h1>
<pre><code>review,label
Colors &amp; clarity is superb,positive
Sadly the picture is not nearly as clear or bright as my 40 inch Samsung,negative
</code></pre>
<h1>test_data.csv:</h1>
<pre><code>review,label
The picture is clear and beautiful,positive
Picture is not clear,negative
</code></pre>
<h1>My Code:</h1>
<pre><code>from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score


def load_data(filename):
    reviews = list()
    labels = list()
    with open(filename) as file:
        file.readline()
        for line in file:
            line = line.strip().split(',')
            labels.append(line[1])
            reviews.append(line[0])

    return reviews, labels

X_train, y_train = load_data('/Users/abc/Sep_10/train_data.csv')
X_test, y_test = load_data('/Users/abc/Sep_10/test_data.csv')

vec = CountVectorizer() 

X_train_transformed =  vec.fit_transform(X_train) 

X_test_transformed = vec.transform(X_test)

clf= MultinomialNB()
clf.fit(X_train_transformed, y_train)

score = clf.score(X_test_transformed, y_test)
print(&quot;score of Naive Bayes algo is :&quot; , score)

y_pred = clf.predict(X_test_transformed)
print(confusion_matrix(y_test,y_pred))

print(&quot;Precision Score : &quot;,precision_score(y_test,y_pred,pos_label='positive'))
print(&quot;Recall Score :&quot; , recall_score(y_test, y_pred, pos_label='positive') )
</code></pre>
",80230,1,27,2,python;scikit-learn,2018-09-11 10:58:04,2018-09-11 10:58:04,2022-05-11 06:55:19,i m a newbie to python as well as machine learning  as per my requirement  i m trying to use naive bayes algorithm for my dataset  i m able to find out the accuracy but trying to find out precision and recall for the same  but  it is throwing the following error  can anyone please suggest me how to proceed with it  i have tried using average   micro  in the precision and the recall scores it worked without any errors but it is giving the same score for accuracy  precision  recall 
667,667,5623016,72186067,How do I interpret a quickly converging Q loss/value function loss in actor-critic?,"<p>I am researching an application of actor-critic RL in a nonstationary environment and the loss of the Q-network (or, if I also implement a value function network, that loss too) quickly converges to zero, well before the network finds the optimal policy.</p>
<p>The architecture is kind of successful in finding a good policy, even though it is not very robust to perturbations, and I suspect that the Q-loss converging this fast is revealing of its inability to estimate the state-value or value function correctly. The environment being nonstationary makes it even more suspect, since there should always be some degree of error in the estimation. Any ideas as to what might be causing this?</p>
<p>Specifically, I am using soft actor-critic, and my implementation is based on OpenAI's Spinning Up repo. The optimization targets are as described in the paper [0], but I honestly find their code much more understandable - math in RL is usually not rigorous enough to really make sense of it. Anyway these are the expressions for the value function target:</p>
<p><a href=""https://i.stack.imgur.com/Nc8FZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nc8FZ.png"" alt=""v_targ"" /></a></p>
<p>and for the Q-function target:
<a href=""https://i.stack.imgur.com/1OG4g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1OG4g.png"" alt=""q_target"" /></a></p>
<p>Where <code>\theta\</code>, <code>\psi</code> and <code>\bar{\psi}</code> are neural networks (Q-function, main value and target value network respectively). <em>I slightly modify these equations</em> to optimize for average reward rate since my task is continuous, see [3], and include entropy regularization when taking the log probability of the action given by policy.</p>
<p>My Q- and value functions are simple MLPs:</p>
<pre class=""lang-py prettyprint-override""><code># Soft Actor-Critic from OpenAI https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac
def mlp(sizes, activation, output_activation=nn.Identity):
    layers = []
    for j in range(len(sizes) - 1):
        act = activation if j &lt; len(sizes) - 2 else output_activation
        layers += [nn.Linear(sizes[j], sizes[j + 1]), act()]
    return nn.Sequential(*layers)

class MLPQFunction(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):
        super().__init__()
        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)

    def forward(self, obs, act):
        q = self.q(torch.cat([obs, act], dim=-1))
        return torch.squeeze(q, -1)  # Critical to ensure q has right shape.


class MLPValueFunction(nn.Module):
    def __init__(self, obs_dim, hidden_sizes, activation):
        super().__init__()
        self.v = mlp([obs_dim] + list(hidden_sizes) + [1], activation)

    def forward(self, obs):
        v = self.v(obs)
        return v.squeeze()
</code></pre>
<p>and I compute the losses this way, after sampling a tuple of batches <code>(o, a, r, o2)</code> from a replay buffer. Each variable is <code>[batch x dim(S)]</code> if it's an observation, where <code>dim(S)</code> is the dimension of the state space, 2 in my case, or <code>[batch x 1]</code> if it's an action or reward.</p>
<pre class=""lang-py prettyprint-override""><code>q1 = net.q1(o, a)
q2 = net.q2(o, a)

# Bellman backup for Q functions
with torch.no_grad():
# Target actions come from *current* policy
    a2, logp_a2 = net.pi(o2)
# Target Q-values
    q1_pi_targ = target_net.q1(o2, a2)
    q2_pi_targ = target_net.q2(o2, a2)
    q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
    backup = r - avg_reward + (q_pi_targ - temp * logp_a2)
    
# MSE loss against Bellman backup
loss_q1 = F.smooth_l1_loss(q1, backup)
loss_q2 = F.smooth_l1_loss(q2, backup)
loss_q = loss_q1 + loss_q2

q_optimizer.zero_grad(set_to_none=True)
loss_q.backward()
q_optimizer.step()

# Compute value function loss
v_optimizer.zero_grad(set_to_none=True)
vf = net.v(o)
with torch.no_grad():
    vf_target = q_pi - temp * logp_pi
loss_v = F.smooth_l1_loss(vf, vf_target)
loss_v.backward()
v_optimizer.step()
</code></pre>
<p>Where <code>avg_reward</code> is estimated as a running average:</p>
<pre><code>avg_reward += AVG_REW_LR * (R - avg_reward + target_net.v(next_state.squeeze()) - target_net.v(state.squeeze()))
</code></pre>
<p>[0] Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Proceedings of the 35th International Conference on Machine Learning, 1861–1870. <a href=""https://proceedings.mlr.press/v80/haarnoja18b.html"" rel=""nofollow noreferrer"">https://proceedings.mlr.press/v80/haarnoja18b.html</a></p>
<p>[3] Naik, A., Shariff, R., Yasui, N., Yao, H., &amp; Sutton, R. S. (2019). Discounted Reinforcement Learning Is Not an Optimization Problem. ArXiv:1910.02140 [Cs]. <a href=""http://arxiv.org/abs/1910.02140"" rel=""nofollow noreferrer"">http://arxiv.org/abs/1910.02140</a></p>
",72,0,1,2,optimization;reinforcement-learning,2022-05-10 17:39:27,2022-05-10 17:39:27,2022-05-11 04:22:58,i am researching an application of actor critic rl in a nonstationary environment and the loss of the q network  or  if i also implement a value function network  that loss too  quickly converges to zero  well before the network finds the optimal policy  the architecture is kind of successful in finding a good policy  even though it is not very robust to perturbations  and i suspect that the q loss converging this fast is revealing of its inability to estimate the state value or value function correctly  the environment being nonstationary makes it even more suspect  since there should always be some degree of error in the estimation  any ideas as to what might be causing this  specifically  i am using soft actor critic  and my implementation is based on openai s spinning up repo  the optimization targets are as described in the paper     but i honestly find their code much more understandable   math in rl is usually not rigorous enough to really make sense of it  anyway these are the expressions for the value function target   where  theta    psi and  bar  psi  are neural networks  q function  main value and target value network respectively   i slightly modify these equations to optimize for average reward rate since my task is continuous  see     and include entropy regularization when taking the log probability of the action given by policy  my q  and value functions are simple mlps  and i compute the losses this way  after sampling a tuple of batches  o  a  r  o  from a replay buffer  each variable is  batch x dim s   if it s an observation  where dim s  is the dimension of the state space   in my case  or  batch x   if it s an action or reward  where avg_reward is estimated as a running average     haarnoja  t   zhou  a   abbeel  p    amp  levine  s      soft actor critic  off policy maximum entropy deep reinforcement learning with a stochastic actor  proceedings of the th international conference on machine learning         naik  a   shariff  r   yasui  n   yao  h    amp  sutton  r  s      discounted reinforcement learning is not an optimization problem  arxiv    cs   
668,668,13046404,72193380,Getting same indices even after concatenate,"<p>I am following a book about machine learning <em><strong>Hands-on machine learning with scikit-learn keras and tensorflow</strong></em> and author suggests this code:</p>
<pre><code>def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) &lt; test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
</code></pre>
<p>Essentially what it does is, it returns indices for some starting data and after adding new data it returns same indices plus new ones. Then it separates them to test and train sets. What I'm confused about is this line:</p>
<pre><code>return crc32(np.int64(identifier)) &lt; test_ratio * 2**32
</code></pre>
<p>why is it changed to np.int64 and not np.int32 or np.int16 and why is it compared with <em><strong>test_ratio * 2**32</strong></em>? And most of all, why does it even work? No matter how big my data set is, it always returns correct ratio.</p>
",12,0,0,2,python;machine-learning,2022-05-11 03:32:36,2022-05-11 03:32:36,2022-05-11 03:32:36,i am following a book about machine learning hands on machine learning with scikit learn keras and tensorflow and author suggests this code  essentially what it does is  it returns indices for some starting data and after adding new data it returns same indices plus new ones  then it separates them to test and train sets  what i m confused about is this line  why is it changed to np int and not np int or np int and why is it compared with test_ratio       and most of all  why does it even work  no matter how big my data set is  it always returns correct ratio 
669,669,4427381,72191377,How to learn the order of content by machine learning?,"<p>Data at hand: 1000 questionnaires with a finite database of questions, say 100 questions about name, gender, income etc. Each questionnaire contains 10 to 30 questions from this question database. The wording of a certain question remains identical across different questionnaires. The 100 questions have their unique label (Q1 to Q100) in the database.</p>
<p>Task: creating a new questionnaire. Assuming I know which questions (say 20 questions including Q1, Q5, Q10, Q22 etc) I need to ask on the new questionnaire, I need to know what order should I place these questions.</p>
<p>Machine learning question: how do I learn the patterns from the existing data to help myself order the 20 questions on my new questionnaire?</p>
",10,1,0,3,machine-learning;nlp;data-science,2022-05-10 23:58:31,2022-05-10 23:58:31,2022-05-11 00:07:06,data at hand   questionnaires with a finite database of questions  say  questions about name  gender  income etc  each questionnaire contains  to  questions from this question database  the wording of a certain question remains identical across different questionnaires  the  questions have their unique label  q to q  in the database  task  creating a new questionnaire  assuming i know which questions  say  questions including q  q  q  q etc  i need to ask on the new questionnaire  i need to know what order should i place these questions  machine learning question  how do i learn the patterns from the existing data to help myself order the  questions on my new questionnaire 
670,670,797473,72190231,Airflow - send slack notification with attachments and result as custom message,"<p>I am using airflow to execute my machine learning model which generates an image file and a score as output. Also I am using slack to notify the DAG success or failure.
I want to send the generated image file as attachment and the score as message with the slack notification.</p>
<p>My code:
<strong>send_slack_notification.py</strong>:</p>
<pre><code>from __future__ import absolute_import, division

import os

from   airflow.contrib.operators.slack_webhook_operator \
                            import SlackWebhookOperator


class SlackAlert(object):
&quot;&quot;&quot;
Inherit this class and specify _webhook_token to send alert to a different channel
&quot;&quot;&quot;
_webhook_token = &quot;xxxxxxxxx&quot;


def send_slack_alert(self, context):
    &quot;&quot;&quot;
    Args:
        slack_con_id (Enum): SlackAlertEnum.AdTech or SlackAlertEnum.Others
    &quot;&quot;&quot;
    slack_webhook_token = self._webhook_token

    slack_msg = &quot;&quot;&quot;
            :white_check_mark: Task Success!! :partying_face: 
            *Task*: {task}  
            *Dag*: {dag} 
            *Execution Time*: {exec_date}  
            *Log Url*: {log_url} 
    &quot;&quot;&quot;.format(
        task=context.get('task_instance').task_id,
        dag=context.get('task_instance').dag_id,
        exec_date=context.get('execution_date'),
        log_url=context.get('task_instance').log_url.replace('localhost:8080', os.environ.get('PUBLIC_IP', '')),
    )
    success_alert = SlackWebhookOperator(
        task_id='slack_test',
        http_conn_id='slack',
        webhook_token=slack_webhook_token,
        message=slack_msg,
        username='airflow')
    return success_alert.execute(context=context)


 # TODO: Remove this line once all jobs are moved to new airflow instances managed by AWS
 #send_failure_alert = SlackAlert().send_failure_alert
 send_slack_alert = SlackAlert().send_slack_alert
</code></pre>
<p>Also my <strong>DAG code</strong>:</p>
<pre><code>import logging
from datetime import datetime
from airflow import DAG
from   airflow.operators.python_operator import PythonOperator
#from   data_science.anomaly_detection.scripts.anomaly_detection_live_run import flag_anomaly
from   data_science.anomaly_detection.pipeline.ad_flagging_anomaly_live import flag_anomaly
from   data_science.send_slack_alert import SlackAlertDataScience

def __flag_anomaly(ds, **kwargs):
    result = flag_anomaly()
    logging.info(result)    

dag = DAG('anomaly-infra-test',
      description='Flagging Anomaly detection data hourly',
      schedule_interval='0 * * * *',
      on_failure_callback=SlackAlertDataScience().send_slack_alert,
      on_success_callback=SlackAlertDataScience().send_success_alert,
      start_date=datetime(2022,1,1),
      catchup=False,
      max_active_runs=1,
      tags=[&quot;DataScience&quot;]
      )


anomaly_detection_data_operator = PythonOperator(task_id='hourly_anomaly_detection_data_task',
                                             python_callable=__flag_anomaly,
                                             dag=dag,
                                             provide_context=True)
anomaly_detection_data_operator
</code></pre>
<p>For now, I am getting success and failure notifications on my slack channel.</p>
<p>I have tried to read some other related questions but I was not able to figure out how to pass the file as attachment and generated score as slack notification message in the line <strong>on_success_callback=SlackAlertDataScience().send_success_alert,</strong> of DAG code.</p>
<p>I have started using airflow recently, so any help is appreciated.</p>
",137,0,0,4,python;notifications;airflow;slack,2022-05-10 22:22:04,2022-05-10 22:22:04,2022-05-10 22:22:04,also my dag code  for now  i am getting success and failure notifications on my slack channel  i have tried to read some other related questions but i was not able to figure out how to pass the file as attachment and generated score as slack notification message in the line on_success_callback slackalertdatascience   send_success_alert  of dag code  i have started using airflow recently  so any help is appreciated 
671,671,18784307,72170990,"Handling specific personalised exceptions / conditions ( FastAPI , Pydantic Models , Prediction Model deployment )","<p>I'm using <strong>Pydantic models</strong> for data validation with <strong>FastAPI</strong> to deploy a <strong>Machine Learning model for predictions</strong>, so I want to handle the following exceptions / conditions :</p>
<ul>
<li>Giving many inputs if one of them doesn't match the features requirements (types, length...) throw an exception for that specific invalid input but show outputs of the other valid inputs</li>
</ul>
<p><strong>What I want to achieve</strong></p>
<p>Inputs :</p>
<pre class=""lang-json prettyprint-override""><code>
    [
      {
        &quot;name&quot;:&quot;John&quot;,
        &quot;age&quot;: 20,
        &quot;salary&quot;: 15000
      },
      {
        &quot;name&quot;:&quot;Emma&quot;,
        &quot;age&quot;: 25,
        &quot;salary&quot;: 28000
      },
      {
        &quot;name&quot;:&quot;David&quot;,
        &quot;age&quot;: &quot;test&quot;,
        &quot;salary&quot;: 50000
      },
      {
        &quot;name&quot;:&quot;Liza&quot;,
        &quot;age&quot;: 5000,
        &quot;salary&quot;: 30000
      }
    ]
   
</code></pre>
<p>Outputs :</p>
<pre class=""lang-json prettyprint-override""><code>
    [
      {
        &quot;prediction&quot;:&quot;Class1&quot;,
        &quot;probability&quot;: 0.88
      },
      {
        &quot;prediction&quot;:&quot;Class0&quot;,
        &quot;probability&quot;: 0.79
      },
      {
      &quot;ËRROR: Expected type int but got str instead&quot;
      },
      {
      &quot;ËRROR: invalid age number&quot;
      }
    ]

</code></pre>
<p><strong>What I have with my base model classes :</strong></p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel, validator
from typing import List

n_inputs = 3
n_outputs = 2


class Inputs(BaseModel):
    name: str
    age: int
    salary: float


class InputsList(BaseModel):
    inputs: List[Inputs]

    @validator(&quot;inputs&quot;, pre=True)
    def check_dimension(cls, v):
        for point in v:
            if len(point) != n_inputs:
                raise ValueError(f&quot;Input data must have a length of {n_inputs} features&quot;)

        return v


class Outputs(BaseModel):
    prediction: str
    probability: float

class OutputsList(BaseModel):
    output: List[Outputs]

    @validator(&quot;output&quot;, pre=True)
    def check_dimension(cls, v):
        for point in v:
            if len(point) != n_outputs:
                raise ValueError(f&quot;Output data must a length of {n_outputs}&quot;)

        return v

</code></pre>
<p><strong>My question is :</strong>
-&gt; How can I achieve this kind of exception or condition handling with my code above ?</p>
",93,1,0,5,python;validation;exception;fastapi;pydantic,2022-05-09 16:38:05,2022-05-09 16:38:05,2022-05-10 20:05:36,i m using pydantic models for data validation with fastapi to deploy a machine learning model for predictions  so i want to handle the following exceptions   conditions   what i want to achieve inputs   outputs   what i have with my base model classes  
672,672,3482266,72151843,Branching out a function,"<p>I've built a function that tries to extract some information from a string.</p>
<p>Before: <code>function (string)</code></p>
<p>Now, I want to refactor that function by receiving two extra params, call them param_1, param_2.</p>
<p>Now: <code>function(string, param_1:str, param_2:str)</code></p>
<p>The function is imported to the namespace where the string and parameters reside, and I can only know the exact values of param_1, param_2 at runtime, even though they belong to a long list, which is known in advance.</p>
<p>However, I was thinking that instead of just doing <code>function (string, param_1, param_2)</code>, and then branching out with a lot of <code>elif</code> statements</p>
<pre><code>if param_1 == val_11 and param_2 == val_12:
   &lt;some_code&gt;
elif param_1 == val_21 and param_2 == val_22:
   &lt;some_different_code&gt;
elif (...)
</code></pre>
<p>I could just do something like:</p>
<pre><code>def function(string, param_1:str, param_2:str):

   new_function = eval(param_1_param_2_&lt;old_function_name&gt;)

   return new_function(string)
</code></pre>
<p>And then define separately each <code>param_1_param_2_&lt;old_function_name&gt;</code>.</p>
<ul>
<li>Is there a more pythonic way of solving my original problem?</li>
<li>From a software engineering perspective / clean code, should I  do something else instead?</li>
</ul>
<p>Edit: The objective is to extract information from the string. Let's assume the info is dates in a document. Depending on the document type (param_2), and on the author (param_1), the way a date is formatted will differ. The focus of the question is not on better machine learning models, or functions like dateparser (but if you do have a suggestion, leave a comment :) ), but how to 'branch out' the original function.</p>
<p>extract_dates(string, author, doc_type)</p>
",65,2,0,1,python,2022-05-07 16:26:52,2022-05-07 16:26:52,2022-05-10 19:25:58,i ve built a function that tries to extract some information from a string  before  function  string  now  i want to refactor that function by receiving two extra params  call them param_  param_  now  function string  param_ str  param_ str  the function is imported to the namespace where the string and parameters reside  and i can only know the exact values of param_  param_ at runtime  even though they belong to a long list  which is known in advance  however  i was thinking that instead of just doing function  string  param_  param_   and then branching out with a lot of elif statements i could just do something like  and then define separately each param__param__ lt old_function_name gt   edit  the objective is to extract information from the string  let s assume the info is dates in a document  depending on the document type  param_   and on the author  param_   the way a date is formatted will differ  the focus of the question is not on better machine learning models  or functions like dateparser  but if you do have a suggestion  leave a comment       but how to  branch out  the original function  extract_dates string  author  doc_type 
673,673,9447938,56679132,"Python is in a Conda environment, but it has not been activated in a Windows virtual environment","<p>I created a Windows (10) Python virtual environment (env3.7.3).  When I open a cmd window activated in the virtual environment, I get the following warning message when starting Python in the virtual environment:</p>
<pre class=""lang-none prettyprint-override""><code>(env3.7.3) C:\Users\redex\OneDrive\Documents\Education\Machine Learning-Ng Python\Exercise7&gt;python
Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
Warning:
This Python interpreter is in a conda environment, but the environment has
not been activated.  Libraries may fail to load.  To activate this environment
please see https://conda.io/activation
</code></pre>
<p>This warning has been posted before in a different context, but it did not address my question.  This warning appears only in the Python virtual environment, not the base <a href=""https://en.wikipedia.org/wiki/Conda_(package_manager)"" rel=""nofollow noreferrer"">Conda</a> environment.  This seems like a Windows or Anaconda environment variable issue, but I don't know enough to know!  Anaconda was recently upgraded and it seemed fine before, so there may be a bug or setting issue.</p>
",53230,6,15,5,python;windows-10;anaconda;activation;virtual-environment,2019-06-20 10:16:01,2019-06-20 10:16:01,2022-05-10 16:57:19,i created a windows    python virtual environment  env      when i open a cmd window activated in the virtual environment  i get the following warning message when starting python in the virtual environment  this warning has been posted before in a different context  but it did not address my question   this warning appears only in the python virtual environment  not the base  environment   this seems like a windows or anaconda environment variable issue  but i don t know enough to know   anaconda was recently upgraded and it seemed fine before  so there may be a bug or setting issue 
674,674,6514486,38075714,How to calculate distance between 2D matrices,"<p>
Hello Community, </p>

<blockquote>
  <p>I'm new (as a member) to the site, so if you think it might be better to post it on <a href=""http://datascience.stackexchange.com"">http://datascience.stackexchange.com</a>, let me know. </p>
</blockquote>

<p>I am tackling a Machine Learning problem which requires to calculate the distance between NxM-dimensional elements, in order to implement certain Classification algorithms.</p>

<p>The element's attribute is a 2D matrix (<strong>Matr</strong>), thus I'm searching for the best algorithm to calculate the distance between 2D matrices. 
As you will see bellow the ""easy"" solution is to convert the 2D into a 1D (vector) and then implement any distance algorithm, but I'm searching for something more convenient (if exists).</p>

<p>So far I have used the following approaches:</p>

<ol>
<li><p>Euclidean distance between each element.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
def dist_euclidean(elem1, elem2):
    t_sum=0
    for i in range(len(elem1.Matr)):
        for j in range(len(elem1.Matr[0])):
            t_sum+= np.square(elem1.Matr[i][j]-elem2.Matr[i][j])
    return np.sqrt(t_sum)
</code></pre></li>
<li><p>Cosine Similarity, in which I had to convert the (NxM) 2D matrix into (1xNM) Vector.</p>

<pre class=""lang-py prettyprint-override""><code>from scipy.spatial import distance
def dist_cosine(elem1, elem2):
    temp1=[]
    temp2=[]
    for i in range(len(elem1.Matr)):
        temp1.extend(elem1.Matr[i])
        temp2.extend(elem2.Matr[i])
    return distance.cosine(temp1, temp2)
</code></pre></li>
<li><p>KL divergence (<a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">wiki</a>), also found implementation only for 1D matrix (Vector), thus did the following conversions: </p>

<ul>
<li><p>Found the entropy between each corresponding row and then average them.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import entropy
def dist_KL_row_avg(elem1, elem2):
    Y=[]
    for i in range(len(elem1.Matr)):
        Y.append(entropy(elem1.Matr[i], elem2.Matr[i]))
    return np.average(Y)
</code></pre></li>
<li><p>Convert the (NxM) 2D matrix into (1xNM) Vector by appending the rows and then calculating the total entropy. </p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import entropy
def dist_KL_1d_total(elem1, elem2):
    temp1=[]
    temp2=[]
    for i in range(len(elem1.Matr)):
        temp1.extend(elem1.Matr[i])
        temp2.extend(elem2.Matr[i])
    return entropy(temp1, temp2)
</code></pre></li>
</ul></li>
<li><p>KS test (<a href=""https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"" rel=""nofollow noreferrer"">wiki</a>), also found implementation only for 1D matrix (Vector), thus did the same conversions as in the KL implementation: </p>

<ul>
<li><p>Found the entropy between each corresponding row and then average them.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import ks_2samp
def dist_KS_row_avg(elem1, elem2):
    Y=[]
    Z=[]
    for i in range(len(elem1.Matr)):
        Y.append(ks_2samp(elem1.Matr[i], elem2.Matr[i]))
    Z=[x[0]/x[1] for x in Y]
    return np.average(Z)
</code></pre></li>
<li><p>Convert the (NxM) 2D matrix into (1xNM) Vector by appending the rows and then calculating the total entropy. </p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import ks_2samp
def dist_KS_1d_total(elem1, elem2):
    temp1=[]
    temp2=[]
    for i in range(len(elem1.Matr)):
        temp1.extend(elem1.Matr[i])
        temp2.extend(elem2.Matr[i])
    Y = ks_2samp(temp1, temp2)
    return Y[0]/Y[1]
</code></pre></li>
</ul></li>
</ol>

<p>All of the above work in my problem but I got curious since I couldn't find anything more specific that satisfied me.</p>

<hr>

<p><strong>Edit 1.</strong>
As <a href=""https://stackoverflow.com/users/5903959/pltrdy"">pltrdy</a> suggested, here are some more info regarding the problem.</p>

<p>The initial data of each element is a series of codes ex(C->B->D->B->A) which then is converted to a transition matrix which is also normalized for each row. Thus each cell in our matrix represents the probability of transition from code [i] to code [j]. For example:</p>

<pre class=""lang-py prettyprint-override""><code>IN: A-&gt;C-&gt;B-&gt;B-&gt;A-&gt;C-&gt;C-&gt;A
OUT: 
    A     B     C
 A  0     0     1
 B  0.5   0.5   0
 C  0.33  0.33  0.33
</code></pre>

<p>Having that in mind, the final goal is to classify the different code series. The series do not have the same length but are made from the same codes. Thus the transition probability matrix has the same dimensions in every case. 
I had the initial question in order to find the most suitable distance algorithm, which is going to produce the best classification results.</p>
",1817,2,7,5,python;algorithm;matrix;machine-learning;distance,2016-06-28 17:32:44,2016-06-28 17:32:44,2022-05-10 13:46:22,i m new  as a member  to the site  so if you think it might be better to post it on   let me know   i am tackling a machine learning problem which requires to calculate the distance between nxm dimensional elements  in order to implement certain classification algorithms  so far i have used the following approaches  euclidean distance between each element  cosine similarity  in which i had to convert the  nxm  d matrix into  xnm  vector  kl divergence     also found implementation only for d matrix  vector   thus did the following conversions   found the entropy between each corresponding row and then average them  convert the  nxm  d matrix into  xnm  vector by appending the rows and then calculating the total entropy   ks test     also found implementation only for d matrix  vector   thus did the same conversions as in the kl implementation   found the entropy between each corresponding row and then average them  convert the  nxm  d matrix into  xnm  vector by appending the rows and then calculating the total entropy   all of the above work in my problem but i got curious since i couldn t find anything more specific that satisfied me  the initial data of each element is a series of codes ex c  b  d  b  a  which then is converted to a transition matrix which is also normalized for each row  thus each cell in our matrix represents the probability of transition from code  i  to code  j   for example 
675,675,13241995,61068630,nearly 0% GPU-Util but high GPU Memory,"<p>A newbie for machine learning here. I'm now training a fairly easy model from tutorial using the dataset fashion_mnist on Win10. However, the training process took extremely long and I didn't even finish it. But I used the same code on my friend's Linux system it took less than 1 min. </p>

<p>I tried to examine the problem but the setup and environment of my computer seemed fine.</p>

<pre><code>import tensorflow as tf 
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
print(tf.test.is_built_with_cuda())
</code></pre>

<p>With the outcome:</p>

<pre><code>device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 13701120911614314629
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 3061212774
locality {
  bus_id: 1
  links {
  }
}
incarnation: 7589776483736281928
physical_device_desc: ""device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5""
]
True
</code></pre>

<p>But the problem is almost 0% GPU-Util but high GPU Memory usage.</p>

<pre><code>
C:\Users\Herr LU&gt;nvidia-smi
Mon Apr 06 16:36:53 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 442.19       Driver Version: 442.19       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   64C    P0    18W /  N/A |   3256MiB /  4096MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     22728      C   ...al\Programs\Python\Python37\pythonw.exe N/A      |
+-----------------------------------------------------------------------------+

C:\Users\Herr LU&gt;
</code></pre>

<p>Here is the code:</p>

<pre><code>#shoes recognition
import tensorflow as tf
from tensorflow import keras

import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

#import dataset of clothes, return a path
mnist = keras.datasets.fashion_mnist

#seperate training data and testing data, which is already accomplished
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()

import matplotlib.pyplot as plt

#show the array in pictures,cmap=colormap
#plt.imshow(training_images[0])
#print(training_labels[0])
#print(training_images[0])

with tf.device('/device:gpu:0'):
    #normalizing the color value to 0~1
    training_images = training_images/255.0
    test_images = test_images/255.0

    #Build a model
    model=keras.Sequential([keras.layers.Flatten(),
                            keras.layers.Dense(128,activation=tf.nn.relu),
                            keras.layers.Dense(10,activation=tf.nn.softmax)])

    #Compile the model with an optimzer and a loss function
    model.compile(optimizer = keras.optimizers.Adam(),
                  loss = 'sparse_categorical_crossentropy',
                  metrics = ['accuracy'])

    #train the model with data
    model.fit(training_images, training_labels, epochs=5)

    #evaluate the model
    model.evaluate(test_images, test_labels)

</code></pre>

<p>What should I do to solve this problem?</p>
",2054,1,0,3,python;tensorflow;keras,2020-04-07 02:12:10,2020-04-07 02:12:10,2022-05-10 13:05:26,a newbie for machine learning here  i m now training a fairly easy model from tutorial using the dataset fashion_mnist on win  however  the training process took extremely long and i didn t even finish it  but i used the same code on my friend s linux system it took less than  min   i tried to examine the problem but the setup and environment of my computer seemed fine  with the outcome  but the problem is almost   gpu util but high gpu memory usage  here is the code  what should i do to solve this problem 
676,676,9698518,64207678,How to avoid error &quot;conda --version: conda not found&quot; in az ml run --submit-script command?,"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:</p>
<pre class=""lang-sh prettyprint-override""><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg
</code></pre>
<p>I get a Service Error with the following error message:</p>
<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\nenvironments from a dependency specification. To manage the python environment\nmanually instead, set userManagedDependencies to True in the python environment\nconfiguration. To use system managed python environments, install conda from:\nhttps://conda.io/miniconda.html
</code></pre>
<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version</code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.</p>
<p>Further information on the azure versions:</p>
<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,
  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,
  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,
  &quot;extensions&quot;: {
    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;
  }
</code></pre>
<p>The image I use is:</p>
<pre><code>mcr.microsoft.com/azure-cli:latest
</code></pre>
<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!</p>
<p>EDIT: I tried to update the environment in which the <code>az ml run</code>-command is run.
Essentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=""https://stackoverflow.com/questions/47177538/installing-miniconda-on-alpine-linux-fails"">Installing miniconda on alpine linux fails</a>). I replaced some names with ... and cut out some irrelevant pieces of code.</p>
<pre class=""lang-yaml prettyprint-override""><code>test:
  image: 'mcr.microsoft.com/azure-cli:latest'
  script:
    - echo &quot;Download conda&quot;
    - apk --update add bash curl wget ca-certificates libstdc++ glib
    - wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-node-bower/master/sgerrand.rsa.pub
    - curl -L &quot;https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-2.23-r3.apk&quot; -o glibc.apk
    - apk del libc6-compat
    - apk add glibc.apk
    - curl -L &quot;https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk 
    - apk add glibc-bin.apk 
    - curl -L &quot;https://github.com/andyshinn/alpine-pkg-glibc/releases/download/2.25-r0/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk
    - apk add --allow-untrusted glibc-i18n.apk 
    - /usr/glibc-compat/bin/localedef -i en_US -f UTF-8 en_US.UTF-8 
    - /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc/usr/lib
    - rm -rf glibc*apk /var/cache/apk/*
    - echo &quot;yes&quot; | curl -sSL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh
    - echo &quot;Install conda&quot;
    - (echo -e &quot;\n&quot;; echo &quot;yes&quot;; echo -e &quot;\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh
    - echo &quot;Installing Azure Machine Learning Extension&quot;
    - az extension add -n azure-cli-ml
    - echo &quot;Azure Login&quot;
    - az login
    - az account set --subscription ...
    - az configure --defaults group=...
    - az ml folder attach -w ... 
    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...
</code></pre>
",975,3,1,5,python;azure;conda;azure-cli;azure-machine-learning-service,2020-10-05 16:54:34,2020-10-05 16:54:34,2022-05-10 12:55:51,i would like to run a test script on an existing compute instance of azure using the azure machine learning extension to the azure cli  i get a service error with the following error message  but when i connect to the compute instance through the azure portal and select the default python kernel  conda   version prints     so conda is effectively already installed on the compute instance  this is why i do not understand the error message  further information on the azure versions  the image i use is  can somebody please explain as to why i am getting this error and help me resolve the error  thank you 
677,677,19081199,72179086,"Pytorch, receiving a Nan as a value","<p>I am new to machine learning and pytorch so I am starting with linear regression.</p>
<p>This is my code:</p>
<pre><code>class LinearRegressionModel(torch.nn.Module):

def __init__(self):
    super(LinearRegressionModel, self).__init__()
    self.linear = torch.nn.Linear(4, 1)  # four input and one output

def forward(self, x):
    y_pred = self.linear(x)
    return y_pred
our_model = LinearRegressionModel()

#changed size_average = False to reduction = ‘sum’ because of depracated value
criterion = torch.nn.MSELoss(reduction = ‘sum’)
optimizer = torch.optim.SGD(our_model.parameters(), lr = 0.01)

#running into issue with data being either double or float
#converted all data into float
for epoch in range(500):

pred_y = our_model(X)

loss = criterion(pred_y, ynew)

optimizer.zero_grad()
loss.backward()
optimizer.step()
print('epoch {}, loss {}'.format(epoch, loss.item()))
</code></pre>
<p>I keep getting returned Nan even when the LR is reduced to increased or when the number of epochs is increased.</p>
",129,0,0,3,python;pytorch;linear-regression,2022-05-10 04:38:56,2022-05-10 04:38:56,2022-05-10 04:38:56,i am new to machine learning and pytorch so i am starting with linear regression  this is my code  i keep getting returned nan even when the lr is reduced to increased or when the number of epochs is increased 
678,678,19078458,72178803,List index out of range for a machine learning method,"<pre><code>---&gt; 30     cvtrain = train.subset([ a for a in cvtrains[i]])
     31     cvtest = train.subset([ a for a in cvtests[i]])
     32     cvX_train = scale_orig.fit_transform(cvtrain.features)

IndexError: list index out of range
</code></pre>
<p><code>train.subset([index])</code> takes a subset of train at every list in the index. It is saying that the list index is out of range, however, <code>cvtrains[0]</code> -&gt; <code>cvtrains[4]</code> all exist.
and they are lists, in each of the lists, the max possible value is within the train indexes...</p>
<p>I'm unsure how to debug this issue and would appreciate the assistance.</p>
<p>Thank you.</p>
",35,0,0,3,python;python-3.x;debugging,2022-05-10 03:54:20,2022-05-10 03:54:20,2022-05-10 03:54:20,i m unsure how to debug this issue and would appreciate the assistance  thank you 
679,679,16795309,68997398,How to convert list of list into a dataframe with differential list structure,"<p>I applied machine learning algorithm with caret package (caretlist) to predict death in a cohort of patients according to multiple variables (e.g. age, gender, smoker, etc.):</p>
<pre><code>algorithmList &lt;- c('rf', 'pls','parRF','nnet', 'xgbTree','avNNet',
                    'gbm','monmlp','nb','glm','pcaNNet','lda','C5.0',
                    'svmLinear2','knn')
 
 set.seed(100)
 list_models &lt;- caretList(Death_event~., data=na.exclude(dataset), methodList = algorithmList, metric=&quot;ROC&quot;, trControl=control)
</code></pre>
<p>Then,  I used the varImp command to extract variable importance from that list of algorithm, which yields a list of list</p>
<pre><code>importance &lt;- lapply(list_models, varImp)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/gNcFv.png"" rel=""nofollow noreferrer"">Importance structure</a></p>
<pre><code>&gt; str(importance)
List of 15
 $ rf        :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 53.8 4.1 100 7.44 0 ...
  ..$ model     : chr &quot;rf&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ pls       :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 15.91 4.88 100 18.95 0 ...
  ..$ model     : chr &quot;pls&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ parRF     :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 51.26 3.74 100 7.66 0 ...
  ..$ model     : chr &quot;parRF&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ nnet      :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 14 41.9 56.4 62.1 31.2 ...
  ..$ model     : chr &quot;nnet&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ xgbTree   :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 100 48.1 40.2 21.5 21.1 ...
  ..$ model     : chr &quot;xgbTree&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ avNNet    :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ gbm       :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 13.543 0.749 100 6.743 0 ...
  ..$ model     : chr &quot;gbm&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ monmlp    :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ nb        :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ glm       :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 13 27.3 100 50.5 11.6 ...
  ..$ model     : chr &quot;glm&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ pcaNNet   :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ lda       :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ C5.0      :List of 3
  ..$ importance:'data.frame':  11 obs. of  1 variable:
  .. ..$ Overall: num [1:11] 100 100 100 100 100 ...
  ..$ model     : chr &quot;C5.0&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ svmLinear2:List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
 $ knn       :List of 3
  ..$ importance:'data.frame':  11 obs. of  2 variables:
  .. ..$ No_death: num [1:11] 14.37 14.36 100 45.4 9.04 ...
  .. ..$ Death   : num [1:11] 14.37 14.36 100 45.4 9.04 ...
  ..$ model     : chr &quot;ROC curve&quot;
  ..$ calledFrom: chr &quot;varImp&quot;
  ..- attr(*, &quot;class&quot;)= chr &quot;varImp.train&quot;
</code></pre>
<p>Then, I am facing the first problem</p>
<p>In half of the algorithm, the importance is extracted with a different method (ROC method). That does not change anything to the interpretation, but in some algorithm the title is &quot;Importance&quot; whereas in others the title is &quot;Overall&quot;, but it is exactly the same information :</p>
<pre><code>$gbm
gbm variable importance

                                        Overall
Age_at_CT                              100.0000
Muscle_HU                               48.6376
history_of_CV_yes_noat_leasT_1CV_event  38.1153
VAT_Area_cm2                            19.3376
Liver_HU_Median                         17.7983
SAT_Area_cm2                            17.3343
L3_SMI_cm2m2                            15.5910
BMI                                     13.5431
Tobacco_yes_noSmoker                     6.7431
SexMale                                  0.7494
T2D_at_CTDiabetes                        0.0000

$monmlp
ROC curve variable importance

                     Importance
Age_at_CT               100.000
Muscle_HU                87.085
history_of_CV_yes_no     61.254
VAT_Area_cm2             49.174
Liver_HU_Median          47.712
Tobacco_yes_no           45.404
BMI                      14.372
Sex                      14.363
T2D_at_CT                 9.035
L3_SMI_cm2m2              7.453
SAT_Area_cm2              0.000
</code></pre>
<p>You'll have probably noticed in the structure that for those algorithms in which importance was extracted using ROC method, there is two subcolumns (death and no_death), but the number is exactly the same in both.</p>
<p>What I am trying to create is a simple tibble/data frame, wherein :</p>
<p>1st Column = the name of the algorithm (here the name of the list, e.g. gbm or monmlp), 2nd Column = the name of the variable (e.g. Age_at_CT, muscle_HU, etc.) and 3rd Column = the importance number (which = &quot;Importance&quot; in some algorithm, and &quot;Overall&quot; in others)</p>
<p>The only workaround I found was to print the list and c/c into an excel sheet algorithm per algorithm (yeah...that sucks).</p>
",107,2,0,3,r;list;r-caret,2021-08-31 16:23:41,2021-08-31 16:23:41,2022-05-09 22:52:36,i applied machine learning algorithm with caret package  caretlist  to predict death in a cohort of patients according to multiple variables  e g  age  gender  smoker  etc    then   i used the varimp command to extract variable importance from that list of algorithm  which yields a list of list output   then  i am facing the first problem in half of the algorithm  the importance is extracted with a different method  roc method   that does not change anything to the interpretation  but in some algorithm the title is  importance  whereas in others the title is  overall   but it is exactly the same information   you ll have probably noticed in the structure that for those algorithms in which importance was extracted using roc method  there is two subcolumns  death and no_death   but the number is exactly the same in both  what i am trying to create is a simple tibble data frame  wherein   st column   the name of the algorithm  here the name of the list  e g  gbm or monmlp   nd column   the name of the variable  e g  age_at_ct  muscle_hu  etc   and rd column   the importance number  which    importance  in some algorithm  and  overall  in others  the only workaround i found was to print the list and c c into an excel sheet algorithm per algorithm  yeah   that sucks  
680,680,5281811,72173750,MemoryError in tensorflow.keras.utils.utils.to_categorical / _ArrayMemoryError,"<p>i'm a newbie in deep learning so i was trying to develop a neural machine translation system from scratch using this tutorial <br>(<a href=""https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/"" rel=""nofollow noreferrer"">
How to Develop a Neural Machine Translation System from Scratch</a>) and:
<br>python 3.10.4 <br>tensorflow-cpu 2.8.0 <br> keras 2.8.0 <br> numpy 1.22.3  <br>matplotlib 3.5.2 <br>conda 4.12.0 on Windows10 with 80gb free storage space <br> Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz   2.50 GHz<br>16,0 Gb RAM.
<br>
<br>
When i execute my code i get this error:<br></p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\User\Downloads\TranslationDeepLearning\EnglishToSpanish\main.py&quot;, line 96, in &lt;module&gt;
    trainY = encode_output(trainY, cyrillic_vocab_size)
  File &quot;C:\Users\User\Downloads\TranslationDeepLearning\EnglishToSpanish\main.py&quot;, line 49, in encode_output
    encoded = to_categorical(sequence, num_classes=vocab_size)
  File &quot;C:\Users\User\anaconda3\envs\TranslationDeepLearning\lib\site-packages\keras\utils\np_utils.py&quot;, line 70, in to_categorical
    categorical = np.zeros((n, num_classes), dtype=dtype)
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.22 MiB for an array with shape (17, 49624) and data type float32
</code></pre>
<p>Here's the code for main.py:
<br><br></p>
<pre><code>import re
from pickle import dump
from numpy.random import rand
from numpy.random import shuffle
from pickle import load
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Embedding
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.callbacks import ModelCheckpoint

ListSent = []

with open(&quot;lyrics.txt&quot;,&quot;r&quot;,encoding=&quot;utf-8&quot;) as f:
    for el in f:
        el = el.strip()
        ListSent.append(el)
def max_length(lines):
    return max(len(line.split()) for line in lines)

def save_clean_data(sentences, filename):
    dump(sentences, open(filename, 'wb'))
    print('Saved: %s' % filename)

def load_clean_sentences(filename):
    return load(open(filename, 'rb'))
    
def create_tokenizer(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer
def encode_sequences(tokenizer, length, lines):
    # integer encode sequences
    X = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    X = pad_sequences(X, maxlen=length, padding='post')
    return X

def encode_output(sequences, vocab_size):
    ylist = list()
    for sequence in sequences:
        encoded = to_categorical(sequence, num_classes=vocab_size)
        ylist.append(encoded)
    y = array(ylist)
    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)
    return y
save_clean_data(ListSent, 'english-spanish.pkl')

def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):
    model = Sequential()
    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))
    model.add(LSTM(n_units))
    model.add(RepeatVector(tar_timesteps))
    model.add(LSTM(n_units, return_sequences=True))
    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))
    return model
 

for i in range(100):
    print('[%s] =&gt; [%s]' % (ListSent[i].split(&quot;\t&quot;)[1], ListSent[i].split(&quot;\t&quot;)[0]))

dataset = load_clean_sentences('english-spanish.pkl')
shuffle(dataset)
train, test = dataset[:71164], dataset[71164:]

save_clean_data(dataset, 'english-spanish-both.pkl')
save_clean_data(train, 'english-spanish-train.pkl')
save_clean_data(test, 'english-spanish-test.pkl')

dataset = load_clean_sentences('english-spanish-both.pkl')
train = load_clean_sentences('english-spanish-train.pkl')
test = load_clean_sentences('english-spanish-test.pkl')

english_tokenizer = create_tokenizer([el.split(&quot;\t&quot;)[0] for el in dataset])
english_vocab_size = len(english_tokenizer.word_index) + 1
english_length = max_length([el.split(&quot;\t&quot;)[0] for el in dataset])
print('English Vocabulary Size: %d' % english_vocab_size)
print('English Max Length: %d' % (english_length))
# prepare german tokenizer
spanish_tokenizer = create_tokenizer([el.split(&quot;\t&quot;)[1] for el in dataset])
spanish_vocab_size = len(spanish_tokenizer.word_index) + 1
spanish_length = max_length([el.split(&quot;\t&quot;)[1] for el in dataset])
print('Spanish Vocabulary Size: %d' % spanish_vocab_size)
print('Spanish Max Length: %d' % (spanish_length))

# prepare training data
trainX = encode_sequences(spanish_tokenizer, spanish_length, [el.split(&quot;\t&quot;)[1] for el in train])
trainY = encode_sequences(english_tokenizer, english_length,[el.split(&quot;\t&quot;)[0] for el in train])
trainY = encode_output(trainY, english_vocab_size)
# prepare validation data
testX = encode_sequences(spanish_tokenizer, spanish_length, [el.split(&quot;\t&quot;)[0] for el in test])
testY = encode_sequences(english_tokenizer, english_length,[el.split(&quot;\t&quot;)[0] for el in test])
testY = encode_output(testY, english_vocab_size)

# define model
model = define_model(spanish_vocab_size, english_vocab_size, spanish_length, english_length, 256)
model.compile(optimizer='adam', loss='categorical_crossentropy')
# summarize defined model
print(model.summary())
plot_model(model, to_file='model.png', show_shapes=True)

filename = 'model.h5'
checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)
</code></pre>
<p><br> What should i do to make this work? Thank you very much!</p>
",43,0,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-05-09 20:05:57,2022-05-09 20:05:57,2022-05-09 20:05:57, what should i do to make this work  thank you very much 
681,681,11346784,72173195,How to incorporate the validation set in machine learning?,"<p>I am trying to learn about machine learning, and I am having trouble understanding when and how to use the validation set. I have understood that it is used to evaluate the candidate models, before checking with the test set, but I don't understand how to properly write it in code. Take for example this code I am working on:</p>
<pre><code># Split the set into train, validation, and test set (70:15:15 for train:valid:test)
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)          # Split the data in training and remaining set
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) # Split the remaining data 50/50 into validation and test set

print(&quot;Properties (shapes):\nTraining set: {}\nValidation set: {}\nTest set: {}&quot;.format(X_train.shape, X_valid.shape, X_test.shape))

import warnings # supress warnings
warnings.filterwarnings('ignore')

# SCALING
std = StandardScaler()
minmax = MinMaxScaler()
rob = RobustScaler()

# Transforming the TRAINING set
X_train_Standard = std.fit_transform(X_train)   # Standardization: each value has mean = 0 and std = 1
X_train_MinMax = minmax.fit_transform(X_train)  # Normalization: each value is between 0 and 1
X_train_Robust = rob.fit_transform(X_train)     # Robust scales each values variance and quartiles (ignores outliers)

# Transforming the TEST set
X_test_Standard = std.fit_transform(X_test)
X_test_MinMax = minmax.fit_transform(X_test)
X_test_Robust = rob.fit_transform(X_test)

# Test scalers for decision tree classifier
treeStd = DecisionTreeRegressor(max_depth=3, random_state=0).fit(X_train_Standard, y_train)
treeMinMax = DecisionTreeRegressor(max_depth=3, random_state=0).fit(X_train_MinMax, y_train)
treeRobust = DecisionTreeRegressor(max_depth=3, random_state=0).fit(X_train_Robust, y_train)
print(&quot;Decision tree with standard scaler:\nTraining set score: {:.4f}\nTest set score: {:.4f}\n&quot;.format(treeStd.score(X_train_Standard, y_train), treeStd.score(X_test_Standard, y_test)))
print(&quot;Decision tree with min/max scaler:\nTraining set score: {:.4f}\nTest set score: {:.4f}\n&quot;.format(treeMinMax.score(X_train_MinMax, y_train), treeMinMax.score(X_test_MinMax, y_test)))
print(&quot;Decision tree with robust scaler:\nTraining set score: {:.4f}\nTest set score: {:.4f}\n&quot;.format(treeRobust.score(X_train_Robust, y_train), treeRobust.score(X_test_Robust, y_test)))

# Now we train our model for different values of `max_depth`, ranging from 1 to 20.

max_depths = range(1, 30)
training_error = []

for max_depth in max_depths:
    model_1 = DecisionTreeRegressor(max_depth=max_depth)
    model_1.fit(X,y)
    training_error.append(mean_squared_error(y, model_1.predict(X)))


testing_error = []
for max_depth in max_depths:
    model_2 = DecisionTreeRegressor(max_depth=max_depth)
    model_2.fit(X, y)
    testing_error.append(mean_squared_error(y_test, model_2.predict(X_test)))

plt.plot(max_depths, training_error, color='blue', label='Training error')
plt.plot(max_depths, testing_error, color='green', label='Testing error')
plt.xlabel('Tree depth')
plt.axvline(x=25, color='orange', linestyle='--')
plt.annotate('optimum = 25', xy=(20, 20), color='red')
plt.ylabel('Mean squared error')
plt.title('Hyperparameters tuning', pad=20, size=30)
plt.legend()
</code></pre>
<p>Where would I run the tests on the validation set? How do I incorporate it into the code?</p>
",47,1,-3,4,python;validation;machine-learning;training-data,2022-05-09 19:27:31,2022-05-09 19:27:31,2022-05-09 19:57:48,i am trying to learn about machine learning  and i am having trouble understanding when and how to use the validation set  i have understood that it is used to evaluate the candidate models  before checking with the test set  but i don t understand how to properly write it in code  take for example this code i am working on  where would i run the tests on the validation set  how do i incorporate it into the code 
682,682,19076912,72172944,How can i adjust the size of different chromosomes in a machine learning model?,"<p>Iam trying to build a machine learning model. I want to include the variable &quot;chromosome&quot; in my model. But there is a problem. Every category of chromosome have their own length .
How can i take the length of different chromosomes into account and build my model ?</p>
<pre><code>chr size_Mb
   1   246.1
   2   243.6
   3   199.3
   4   191.7
   5   181.0
   6   170.9

</code></pre>
",17,0,0,1,r,2022-05-09 19:10:36,2022-05-09 19:10:36,2022-05-09 19:10:36,
683,683,17321543,72171634,Using Keras Multi Layer Perceptron with Cross Validation prediction,"<p>my model is as follows:</p>
<pre><code>model = Sequential()
model.add(Dense(units = 56, input_dim = 16, activation = &quot;relu&quot;, kernel_initializer= 
initializer))
model.add(Dropout(0.2))
model.add(Dense(units = 28, activation = &quot;relu&quot;))
model.add(Dropout(0.2))
model.add(Dense(units = 1, activation = &quot;sigmoid&quot;))
</code></pre>
<p>I need to use cross validation to obtain accuracy from my X_train, Y_train.</p>
<p>How do I go ahead doing this?</p>
<p>I know I can use cross val score for other machine learning models but this one isn't working for my keras implementation.</p>
<p>Additionally, I need to record the output for each cross validation i.e the cross val predict for each validation, how would I go ahead implementing this for my keras MLP?</p>
",12,0,0,3,python;machine-learning;keras,2022-05-09 17:28:24,2022-05-09 17:28:24,2022-05-09 18:26:21,my model is as follows  i need to use cross validation to obtain accuracy from my x_train  y_train  how do i go ahead doing this  i know i can use cross val score for other machine learning models but this one isn t working for my keras implementation  additionally  i need to record the output for each cross validation i e the cross val predict for each validation  how would i go ahead implementing this for my keras mlp 
684,684,18323270,72170972,Using Python&#39;s Find() in a while loop to return the index of words in a string,"<p>A problem I'm working on involves using a while loop to iterate through a string, using the Find() function to return the index of a specific word in that string whenever it appears. For this example it's finding the name 'Weena' in a string variable containing the entirety of The Time Machine. I am being asked to specifically use while loops and the Find() function, though I've already solved the problem using an enumerator.</p>
<pre><code>for count, word in enumerate(time_machine_text.split()):
    if 'Weena' in word:
        print(count)
</code></pre>
<p>I'm used to foreach loops because I relied on them a lot when I was learning C#, so every looping problem I see I just default to foreach loops. Any advice to meet the task's requirements of using a while loop and the Find() function would be much appreciated.</p>
",56,1,0,5,python;string;loops;while-loop;find,2022-05-09 16:36:35,2022-05-09 16:36:35,2022-05-09 17:23:01,a problem i m working on involves using a while loop to iterate through a string  using the find   function to return the index of a specific word in that string whenever it appears  for this example it s finding the name  weena  in a string variable containing the entirety of the time machine  i am being asked to specifically use while loops and the find   function  though i ve already solved the problem using an enumerator  i m used to foreach loops because i relied on them a lot when i was learning c   so every looping problem i see i just default to foreach loops  any advice to meet the task s requirements of using a while loop and the find   function would be much appreciated 
685,685,13419675,72117454,Azure POST request redirect using Azure services,"<p>Hello I am trying to deploy my Azure Machine Learning pipeline with a REST endpoint. My problem is that I was able to generate an endpoint but has some sensitive information in it (ex: subscription id, resource group, etc). How can I generate a URL that forwards the request body to my Azure ML REST endpoint?</p>
<p>also, here is an approach I've done:</p>
<ul>
<li>Used Application Gateway Redirect (this approach didn't forward the request body. It instead turned my POST request into a GET request when it redirected to the correct URL.)</li>
</ul>
",37,1,0,4,azure;rest;web-deployment;azure-machine-learning-service,2022-05-04 23:24:49,2022-05-04 23:24:49,2022-05-09 17:06:57,hello i am trying to deploy my azure machine learning pipeline with a rest endpoint  my problem is that i was able to generate an endpoint but has some sensitive information in it  ex  subscription id  resource group  etc   how can i generate a url that forwards the request body to my azure ml rest endpoint  also  here is an approach i ve done 
686,686,8183811,72170022,How to replace data in a certain range with a variable?,"<p>I'm new to machine learning,</p>
<p>I have a dataset :</p>
<p><a href=""https://i.stack.imgur.com/BcVPf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BcVPf.png"" alt=""enter image description here"" /></a></p>
<p>I want to create a &quot;bucket&quot; :</p>
<blockquote>
<p>[0-25] = A</p>
<p>[26-50] = B</p>
<p>[51-75] = C</p>
<p>[76-100] = D</p>
</blockquote>
<p>I tried <code>panda.cut()</code> :</p>
<pre><code>bins = [-1, 26, 51, 76, 100]    
labels = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]    
dataset['UAS'] = pd.cut(dataset['UAS'], bins=bins, labels=labels)
</code></pre>
<p>Result :</p>
<p><a href=""https://i.stack.imgur.com/3F5Wu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3F5Wu.png"" alt=""enter image description here"" /></a></p>
<p>It only works on a 1-dimensional array.
Any tips/lib to &quot;cut&quot; all columns simultaneously without repeating the code?</p>
<p>Thanks a lot.</p>
<p>** tried <code>apply()</code> :</p>
<p><a href=""https://i.stack.imgur.com/rFMcb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rFMcb.png"" alt=""enter image description here"" /></a></p>
",37,1,1,2,python;pandas,2022-05-09 15:18:10,2022-05-09 15:18:10,2022-05-09 16:02:57,i m new to machine learning  i have a dataset    i want to create a  bucket          a       b       c       d i tried panda cut     result    thanks a lot     tried apply     
687,687,18968829,72166808,OpenCV issue while using DNN implementation with any version in Machine Learning Services,"<p>I am using Machine Learning Services and when I am trying to implement Deep Neural Network, I am getting CV2 issue. The CV2 library is being bothering the code block. The following is the error I am getting when I am trying to use CV2 for DNN_BACKEND_CUDA.</p>
<p><a href=""https://i.stack.imgur.com/hTSXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hTSXM.png"" alt=""enter image description here"" /></a></p>
<p>Any help is appreciable.</p>
",54,1,1,4,python;opencv;computer-vision;azure-machine-learning-service,2022-05-09 09:20:34,2022-05-09 09:20:34,2022-05-09 16:01:18,i am using machine learning services and when i am trying to implement deep neural network  i am getting cv issue  the cv library is being bothering the code block  the following is the error i am getting when i am trying to use cv for dnn_backend_cuda   any help is appreciable 
688,688,7618172,72161989,How to use colors as features in machine learning?,"<p>I have colors in RGB form. There are 4 columns</p>
<ol>
<li>'<strong>accent_color</strong>'-&gt; (0.6901960784313725, 0.14901960784313725, 0.10588235294117647)</li>
<li>'<strong>dominant_colors</strong>' -&gt;
[(0.6470588235294118, 0.16470588235294117, 0.16470588235294117), (0.0, 0.0, 0.0), (1.0, 1.0, 1.0)]</li>
<li>'<strong>bg_color</strong>' &gt; (0.6470588235294118, 0.16470588235294117, 0.16470588235294117)</li>
<li>'<strong>fore_color</strong>' -&gt; (0.0, 0.0, 0.0)</li>
</ol>
<p>I want to use these values as features in the textual model. These colors will be used to determine whether the post has more probability of getting likes wrt colors. Any tips will be appreciated.</p>
",23,1,-2,4,machine-learning;nlp;feature-extraction;feature-selection,2022-05-08 19:59:40,2022-05-08 19:59:40,2022-05-08 20:30:57,i have colors in rgb form  there are  columns i want to use these values as features in the textual model  these colors will be used to determine whether the post has more probability of getting likes wrt colors  any tips will be appreciated 
689,689,11927949,67552090,Delete and recreate the registry for an azure machine learning workspace,"<p>Our azure machine learning workspace container registry has grown extremely large (4Tb) and has many obsolete entries. I would like to delete the registry and simply create a new one. We do not need any entries from the old one.</p>
<p>If I delete the current registry, create a new one, how do I attach it to the workspace?  I dont want to create a new workspace.</p>
",123,1,2,1,azure-machine-learning-service,2021-05-16 05:13:11,2021-05-16 05:13:11,2022-05-08 01:27:26,our azure machine learning workspace container registry has grown extremely large  tb  and has many obsolete entries  i would like to delete the registry and simply create a new one  we do not need any entries from the old one  if i delete the current registry  create a new one  how do i attach it to the workspace   i dont want to create a new workspace 
690,690,3531907,68647922,Azure Machine Learning pipeline: How to retry upon failure?,"<p>So I've got an Azure Machine Learning pipeline here that consists of a number of <code>PythonScriptStep</code> tasks - pretty basic really.</p>
<p>Some of these script steps fail intermittently due to network issues or somesuch - really nothing unexpected. The solution here is always to simply trigger a rerun of the failed experiment in the browser interface of Azure Machine Learning studio.</p>
<p>Despite my best efforts I haven't been able to figure out how to set a retry parameter either on the script step objects, the pipeline object, or any other AZ ML-related object.
This is a common pattern in pipelines of any sort: Task fails once - retry a couple of times before deciding it actually fails.</p>
<p>Does anyone have pointers for me please?</p>
<p>Edit: One helpful user suggested an external solution to this which requires an Azure Logic App that listens to ML pipeline events and re-triggers failed pipelines via an HTTP request. While this solution may work for some it just takes you down another rabbit hole of setting up, debugging, and maintaining another external component. I'm looking for a simple &quot;retry upon task failure&quot; option that (IMO) must be baked into the Azure ML pipeline framework and is hopefully just poorly documented.</p>
",359,2,4,4,azure;azure-machine-learning-studio;azure-machine-learning-service;azure-machine-learning-workbench,2021-08-04 13:58:31,2021-08-04 13:58:31,2022-05-08 01:04:29,so i ve got an azure machine learning pipeline here that consists of a number of pythonscriptstep tasks   pretty basic really  some of these script steps fail intermittently due to network issues or somesuch   really nothing unexpected  the solution here is always to simply trigger a rerun of the failed experiment in the browser interface of azure machine learning studio  does anyone have pointers for me please  edit  one helpful user suggested an external solution to this which requires an azure logic app that listens to ml pipeline events and re triggers failed pipelines via an http request  while this solution may work for some it just takes you down another rabbit hole of setting up  debugging  and maintaining another external component  i m looking for a simple  retry upon task failure  option that  imo  must be baked into the azure ml pipeline framework and is hopefully just poorly documented 
691,691,13080984,71933609,Get Labels from loaded ML.NET model,"<p>I have followed the ML.NET tutorial for image classification and already created my first Model.
(<a href=""https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/image-classification"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/image-classification</a>)</p>
<p>When I load the Saved model I write the following:</p>
<pre><code>trainedModel = mlContext.Model.Load(modelRelativePath &amp; ModelName, modelSchema)
</code></pre>
<p>Once I run the model with a picture, it returns if the picture is a cat or dog.
(The labels used are &quot;CAT&quot; and &quot;DOG&quot;)</p>
<p>Is there a way to read the whole list of labels from the loaded model? I would like to display it once the model is loaded.</p>
<p>I have searched the <code>trainedmodel</code> tree but couldn't find them. Thanks</p>
",73,1,0,3,vb.net;ml.net;image-classification,2022-04-20 08:30:23,2022-04-20 08:30:23,2022-05-07 20:44:16,when i load the saved model i write the following  is there a way to read the whole list of labels from the loaded model  i would like to display it once the model is loaded  i have searched the trainedmodel tree but couldn t find them  thanks
692,692,6382643,72153781,AzureML data Transformation tasks,"<p>I have a machine learning task in Azure whereby I need to carry out some simplistic data transformations. Given the nature of the transformations, it is easier for me to implement them as an ML pipeline step than resort to something like DataBricks.</p>
<p>My question is regarding the best way to use such transformations.</p>
<p>In my case, I am chaining data transformations in a scikit-learn pipeline and registering this as an AzureML model. In deployment, I need to load two models; the data transformation and ML model and use them together.</p>
<p>Is this an acceptable way to handle these situations? Is there a way whereby this can be simplified?</p>
<p>Thanks!</p>
",16,0,0,2,azureml;azureml-python-sdk,2022-05-07 20:41:52,2022-05-07 20:41:52,2022-05-07 20:41:52,i have a machine learning task in azure whereby i need to carry out some simplistic data transformations  given the nature of the transformations  it is easier for me to implement them as an ml pipeline step than resort to something like databricks  my question is regarding the best way to use such transformations  in my case  i am chaining data transformations in a scikit learn pipeline and registering this as an azureml model  in deployment  i need to load two models  the data transformation and ml model and use them together  is this an acceptable way to handle these situations  is there a way whereby this can be simplified  thanks 
693,693,9134018,72151905,Training data in sentiment analysis,"<p>I'm doing sentiment analysis of tweets related to recent acquisition of Twitter by Elon Musk. I have a corpus of 10 000 tweets and I'd like to use machine learning methods using models like SVM and Linear Regression. My question is, when I want to train the models, do I have to manually tag big portion of those 10 000 collected tweets with either positive or negative class to train the model correctly or can I use some other dataset of tweets not relating to this topic that's already tagged to train the model for sentiment analysis? Thank you for your answers!</p>
",28,0,0,5,data-science;linear-regression;svm;sentiment-analysis;training-data,2022-05-07 16:35:50,2022-05-07 16:35:50,2022-05-07 16:35:50,i m doing sentiment analysis of tweets related to recent acquisition of twitter by elon musk  i have a corpus of   tweets and i d like to use machine learning methods using models like svm and linear regression  my question is  when i want to train the models  do i have to manually tag big portion of those   collected tweets with either positive or negative class to train the model correctly or can i use some other dataset of tweets not relating to this topic that s already tagged to train the model for sentiment analysis  thank you for your answers 
694,694,18973581,72057609,Sending data from LMS Moodle (php) to python CGI script and get it back,"<p>I installed Learning Managment System (LMS) 'Moodle' to XAMPP local server. And there is instrument for file downloading, called &quot;filepicker&quot;.</p>
<p>I want to pass text, received from 'filepicker' instrument (uploading .txt files only) from LMS Moodle to python-CGI script, process it, get the processed data back, and echo it on a page.</p>
<p>Using:</p>
<p>OS - Windows 10.</p>
<p>Local server: Apache/2.4.53 (Win64) OpenSSL/1.1.1n PHP/7.4.28</p>
<p>python 3.10.4</p>
<p>I made var_dump($content); of uploaded file, So it is definitely a string:</p>
<p>string(828) &quot;..........my text here............&quot;</p>
<p>Also I clearly know, that my CGI script work if i manually input data in it,like:</p>
<p>http://localhost/speciallocation/local/tokenize/morgot.py?someamountoftext=Enter your text here</p>
<p>Output: ['Enter', 'your', 'text', 'here']</p>
<p>But when I press submit button, I only get the name of my file, since I don't transmit it to CGI, but simply echo it.</p>
<p>If just echo content of file, It also works. It brings me to think, that there is something wrong with send&amp;get data part.......</p>
<p>Any thoughts people? Maybe I am doing somethig wrong at the very basics? Like POST vs GET method? Or something else? Please, help!</p>
<p>My php code:</p>
<pre class=""lang-php prettyprint-override""><code>&lt;?php

require_once(DIR . '/../../config.php');
require_once($CFG-&gt;dirroot . '/local/tokenize/classes/forms/tokenization.php');
$PAGE-&gt;set_url(new moodle_url('/local/tokenize/tokenization.php'));
$PAGE-&gt;set_context(\context_system::instance());
$PAGE-&gt;set_title(get_string('TOKENIZATOR', 'local_tokenize'));

$mform= new tokenization();
echo $OUTPUT-&gt;header();
  
if ($mform-&gt;is_cancelled()) {
    //Handle form cancel operation, if cancel button is present on form
} else if ($fromform = $mform-&gt;get_data()) {
  //In this case you process validated data. $mform-&gt;get_data() returns data posted in form.
  
  $name = $mform-&gt;get_new_filename('userfile');
  echo $name. '&lt;br&gt;';
 $content = $mform-&gt;get_file_content('userfile');
 //echo $content;
 var_dump($content);
 
  $morgot_link = &quot;http://localhost/diplom/local/tokenize/morgot.py?someamountoftext=&quot; . $content;
  $morgot_data = file_get_contents($morgot_link);
  echo $morgot_data;
  
} else {
  // this branch is executed if the form is submitted but the data doesn't validate and the form should be redisplayed
  // or on the first display of the form.

  //displays the form
  $mform-&gt;display();
}

echo $OUTPUT-&gt;footer();
</code></pre>
<pre class=""lang-py prettyprint-override""><code>
My CGI python code:

#!C:\Users\HP\AppData\Local\Programs\Python\Python310-32\python.exe


import os
import urllib.parse
import nltk

query_dict = urllib.parse.parse_qs(os.environ['QUERY_STRING'])
input_something = str(query_dict['someamountoftext'])[2: -2]
def tknz_wrd(someamountoftext):
    return(nltk.word_tokenize(someamountoftext))


print(&quot;Content-Type: text/html\n&quot;)

print (tknz_wrd(input_something))
</code></pre>
<p>morgot.py - the name of my CGI python file.</p>
<p>Addition: I checked if the $content of file is being placed into  $morgot_link:</p>
<pre class=""lang-php prettyprint-override""><code>$morgot_link = &quot;http://localhost/diplom/local/tokenize/morgot.py?someamountoftext=&quot; . $content;
  echo $morgot_link;
</code></pre>
<p>Yes, the output for this is correct:</p>
<p>http://localhost/diplom/local/tokenize/morgot.py?someamountoftext=...........many text here.............</p>
<p>It pushes me to think, that the problem is with receiving part (But I don't completely deny probability with sending part issue). Also I think there could be some kinds of restrictions\permissions in Moodle to recieve data like that.</p>
<p>Useful links:</p>
<p>I followed these instructions for CGI-python script:</p>
<p>step 1) Run Python Programs as Web Applications locally on your Machine with Xampp - <a href=""https://www.youtube.com/watch?v=cFAcFP3Di6s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=cFAcFP3Di6s</a></p>
<p>step 2) Build a Web API with Python - <a href=""https://www.youtube.com/watch?v=Bdeclymkt-A"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Bdeclymkt-A</a></p>
<p>step 3) Call Python (Web API) from PHP - <a href=""https://www.youtube.com/watch?v=Bx_BEA8VPq0"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Bx_BEA8VPq0</a></p>
<p>Moodle installation:</p>
<p>How to install Moodle eLearning in localhost (XAMPP) on Windows - <a href=""https://www.youtube.com/watch?v=My5DzB874_o"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=My5DzB874_o</a></p>
<p>P.S. I also tried curl, replacing $morgot_data = file_get_contents($morgot_link);    like this:</p>
<pre class=""lang-php prettyprint-override""><code>$postfields = $content;
$url=&quot;http://localhost/diplom/local/tokenize/morgot.py?someamountoftext=&quot;;
$ch = curl_init();
curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, 5);
curl_setopt($ch, CURLOPT_TIMEOUT, 5);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);
curl_setopt($ch, CURLOPT_VERBOSE, false);
curl_setopt($ch, CURLOPT_URL, $url);
curl_setopt($ch, CURLOPT_POST, true);
curl_setopt($ch, CURLOPT_POSTFIELDS, $postfields);
$result = curl_exec($ch);
echo $result;
</code></pre>
<p>Did not help.
Sorry for bad English.</p>
",88,0,1,4,python;php;cgi;moodle,2022-04-29 17:26:42,2022-04-29 17:26:42,2022-05-07 16:16:12,i installed learning managment system  lms   moodle  to xampp local server  and there is instrument for file downloading  called  filepicker   i want to pass text  received from  filepicker  instrument  uploading  txt files only  from lms moodle to python cgi script  process it  get the processed data back  and echo it on a page  using  os   windows   local server  apache     win  openssl   n php    python    i made var_dump  content   of uploaded file  so it is definitely a string  string              my text here              also i clearly know  that my cgi script work if i manually input data in it like  http   localhost speciallocation local tokenize morgot py someamountoftext enter your text here output    enter    your    text    here   but when i press submit button  i only get the name of my file  since i don t transmit it to cgi  but simply echo it  if just echo content of file  it also works  it brings me to think  that there is something wrong with send amp get data part        any thoughts people  maybe i am doing somethig wrong at the very basics  like post vs get method  or something else  please  help  my php code  morgot py   the name of my cgi python file  addition  i checked if the  content of file is being placed into   morgot_link  yes  the output for this is correct  http   localhost diplom local tokenize morgot py someamountoftext            many text here              it pushes me to think  that the problem is with receiving part  but i don t completely deny probability with sending part issue   also i think there could be some kinds of restrictions permissions in moodle to recieve data like that  useful links  i followed these instructions for cgi python script  step   run python programs as web applications locally on your machine with xampp    step   build a web api with python    step   call python  web api  from php    moodle installation  how to install moodle elearning in localhost  xampp  on windows    p s  i also tried curl  replacing  morgot_data   file_get_contents  morgot_link      like this 
695,695,19059595,72150387,How to address &#39;Input Mapped Classifier&#39; message in Weka,"<p>I am trying to test a machine learning model produced from a training dataset that is triple the size of my test dataset. When I upload my test dataset into weka for training I get a message asking if I would like to &quot;Input Mapped Classifier&quot;. I clicked yes and my results showed multiple question marks and a confusion matrix with very few results in it. I was wondering if there is a way to get around this and improve the output?
Thank you for your help!</p>
",27,0,0,1,weka,2022-05-07 12:57:28,2022-05-07 12:57:28,2022-05-07 12:57:28,
696,696,896802,35074209,How to copy/paste a dataframe from iPython into Google Sheets or Excel?,"<p>I've been using iPython (aka Jupyter) quite a bit lately for data analysis and some machine learning. But one big headache is copying results from the notebook app (browser) into either Excel or Google Sheets so I can manipulate results or share them with people who don't use iPython.</p>

<p>I know how to convert results to csv and save. But then I have to dig through my computer, open the results and paste them into Excel or Google Sheets. That takes too much time.</p>

<p>And just highlighting a resulting dataframe and copy/pasting usually completely messes up the formatting, with columns overflowing. (Not to mention the issue of long resulting dataframes being truncated when printed in iPython.)</p>

<p>How can I easily copy/paste an iPython result into a spreadsheet? </p>
",17669,6,19,5,python;excel;google-sheets;ipython;ipython-notebook,2016-01-29 05:03:19,2016-01-29 05:03:19,2022-05-07 09:26:36,i ve been using ipython  aka jupyter  quite a bit lately for data analysis and some machine learning  but one big headache is copying results from the notebook app  browser  into either excel or google sheets so i can manipulate results or share them with people who don t use ipython  i know how to convert results to csv and save  but then i have to dig through my computer  open the results and paste them into excel or google sheets  that takes too much time  and just highlighting a resulting dataframe and copy pasting usually completely messes up the formatting  with columns overflowing   not to mention the issue of long resulting dataframes being truncated when printed in ipython   how can i easily copy paste an ipython result into a spreadsheet  
697,697,9053474,72148304,How can I make a setup command only run the first time a Jupyter notebook is ran?,"<p>Im doing a machine learning project in google colab. Each time an instance is started, I want to run these commands:</p>
<pre><code>  ! mkdir ~/.kaggle # make directory &quot;.kaggle&quot;
  ! cp kaggle.json ~/.kaggle/ # copy the json file into the directory
  ! chmod 600 ~/.kaggle/kaggle.json # allocate required permission for the file
  ! kaggle datasets download -d alessiocorrado99/animals10 # download animal set
  ! unzip animals10.zip
</code></pre>
<p>These commands download and extract a dataset I need. However, it only needs to be ran the first run through only. When clicking &quot;Run All&quot; after the initial download of the dataset, it requires user input to decide whether to replace the files or not. I also don't want to keep downloading from kaggle and use resources unnecesarily.</p>
<p>My current approach is to run the script once then comment out the initialization script, but this takes time and effort.</p>
<p>How can I automate this process so a certain cell only runs on the first run of the runtime?</p>
",19,2,0,3,python;jupyter-notebook;google-colaboratory,2022-05-07 05:00:39,2022-05-07 05:00:39,2022-05-07 05:16:47,im doing a machine learning project in google colab  each time an instance is started  i want to run these commands  these commands download and extract a dataset i need  however  it only needs to be ran the first run through only  when clicking  run all  after the initial download of the dataset  it requires user input to decide whether to replace the files or not  i also don t want to keep downloading from kaggle and use resources unnecesarily  my current approach is to run the script once then comment out the initialization script  but this takes time and effort  how can i automate this process so a certain cell only runs on the first run of the runtime 
698,698,14458173,64413164,Extracting Structure Failed when importing an sbt project,"<p>I'm trying to setup Scala on IntelliJ IDE and when I create a new project it seems fine. When I import another project it errors: <code>Extracting Structure Failed</code>.</p>
<p>The sbt shell seems to be working. What can be the issue?</p>
<p>This is my <code>build.sbt</code>:</p>
<pre><code>course := &quot;progfun1&quot;
assignment := &quot;example&quot;
scalaVersion := &quot;2.12.12&quot;
scalacOptions ++= Seq(&quot;-language:implicitConversions&quot;, &quot;-deprecation&quot;)
libraryDependencies += &quot;com.novocode&quot; % &quot;junit-interface&quot; % &quot;0.11&quot; % Test 
testOptions in Test += Tests.Argument(TestFrameworks.JUnit, &quot;-a&quot;, &quot;-v&quot;, &quot;-s&quot;)
</code></pre>
<p>Here is the log:</p>
<pre><code>2020-10-17 16:45:29,222 [      0]   INFO -        #com.intellij.idea.Main - ------------------------------------------------------ IDE STARTED ------------------------------------------------------ 
2020-10-17 16:45:29,291 [     69]   INFO -        #com.intellij.idea.Main - IDE: IntelliJ IDEA (build #IC-202.7660.26, 06 Oct 2020 11:32) 
2020-10-17 16:45:29,292 [     70]   INFO -        #com.intellij.idea.Main - OS: Windows 10 (10.0, amd64) 
2020-10-17 16:45:29,292 [     70]   INFO -        #com.intellij.idea.Main - JRE: 11.0.8+10-b944.34 (JetBrains s.r.o.) 
2020-10-17 16:45:29,293 [     71]   INFO -        #com.intellij.idea.Main - JVM: 11.0.8+10-b944.34 (OpenJDK 64-Bit Server VM) 
2020-10-17 16:45:29,294 [     72]   INFO -        #com.intellij.idea.Main - JVM Args: exit -Xms128m -Xmx1776m -XX:ReservedCodeCacheSize=240m -XX:+UseConcMarkSweepGC -XX:SoftRefLRUPolicyMSPerMB=50 -ea -XX:CICompilerCount=2 -Dsun.io.useCanonPrefixCache=false -Djdk.http.auth.tunneling.disabledSchemes=&quot;&quot; -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Djdk.attach.allowAttachSelf=true -Dkotlinx.coroutines.debug=off -Djdk.module.illegalAccess.silent=true -Djb.vmOptionsFile=C:\Users\Melih\AppData\Roaming\JetBrains\IdeaIC2020.2\idea64.exe.vmoptions -Djava.library.path=D:\IntelliJ IDEA Community Edition 2020.2.3\jbr\\bin;D:\IntelliJ IDEA Community Edition 2020.2.3\jbr\\bin\server -Didea.platform.prefix=Idea -Didea.jre.check=true -Dide.native.launcher=true -Didea.vendor.name=JetBrains -Didea.paths.selector=IdeaIC2020.2 -XX:ErrorFile=C:\Users\Melih\java_error_in_idea_%p.log -XX:HeapDumpPath=C:\Users\Melih\java_error_in_idea.hprof 
2020-10-17 16:45:29,294 [     72]   INFO -        #com.intellij.idea.Main - library path: D:\IntelliJ IDEA Community Edition 2020.2.3\jbr\\bin;D:\IntelliJ IDEA Community Edition 2020.2.3\jbr\\bin\server 
2020-10-17 16:45:29,294 [     72]   INFO -        #com.intellij.idea.Main - boot library path: D:\IntelliJ IDEA Community Edition 2020.2.3\jbr\bin 
2020-10-17 16:45:29,296 [     74]   INFO -        #com.intellij.idea.Main - locale=en_US JNU=Cp1254 file.encoding=Cp1254
  idea.config.path=C:\Users\Melih\AppData\Roaming\JetBrains\IdeaIC2020.2
  idea.system.path=C:\Users\Melih\AppData\Local\JetBrains\IdeaIC2020.2
  idea.plugins.path=C:\Users\Melih\AppData\Roaming\JetBrains\IdeaIC2020.2\plugins
  idea.log.path=C:\Users\Melih\AppData\Local\JetBrains\IdeaIC2020.2\log 
2020-10-17 16:45:29,552 [    330]   INFO -        #com.intellij.idea.Main - JNA library (64-bit) loaded in 255 ms 
2020-10-17 16:45:29,865 [    643]   INFO - ntellij.idea.ApplicationLoader - CPU cores: 8; ForkJoinPool.commonPool: java.util.concurrent.ForkJoinPool@10bac343[Running, parallelism = 7, size = 0, active = 0, running = 0, steals = 0, tasks = 0, submissions = 0]; factory: com.intellij.concurrency.IdeaForkJoinWorkerThreadFactory@6f4f8382 
2020-10-17 16:45:30,067 [    845]   INFO - penapi.util.io.win32.IdeaWin32 - Native filesystem for Windows is operational 
2020-10-17 16:45:30,371 [   1149]   INFO - llij.ide.plugins.PluginManager - Plugin PluginDescriptor(name=Groovy, id=org.intellij.groovy, path=D:\IntelliJ IDEA Community Edition 2020.2.3\plugins\Groovy, version=202.7660.26) misses optional descriptor duplicates-groovy.xml 
java.nio.file.NoSuchFileException: /META-INF/duplicates-groovy.xml
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystem.newInputStream(ZipFileSystem.java:591)
    at jdk.zipfs/jdk.nio.zipfs.ZipPath.newInputStream(ZipPath.java:721)
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystemProvider.newInputStream(ZipFileSystemProvider.java:275)
    at java.base/java.nio.file.Files.newInputStream(Files.java:155)
    at com.intellij.openapi.util.JDOMUtil.load(JDOMUtil.java:351)
    at com.intellij.ide.plugins.BasePathResolver.resolvePath(BasePathResolver.java:54)
    at com.intellij.ide.plugins.PluginXmlPathResolver.resolvePath(PluginXmlPathResolver.java:58)
    at com.intellij.ide.plugins.XmlReader.readDependencies(XmlReader.java:215)
    at com.intellij.ide.plugins.IdeaPluginDescriptorImpl.readExternal(IdeaPluginDescriptorImpl.java:190)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromJar(PluginDescriptorLoader.java:94)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromDirAndNormalize(PluginDescriptorLoader.java:145)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromFileOrDir(PluginDescriptorLoader.java:120)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptor(PluginDescriptorLoader.java:44)
    at com.intellij.ide.plugins.PluginDescriptorLoader.lambda$loadDescriptorsFromDir$1(PluginDescriptorLoader.java:246)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at com.intellij.util.concurrency.BoundedTaskExecutor.doRun(BoundedTaskExecutor.java:215)
    at com.intellij.util.concurrency.BoundedTaskExecutor.access$200(BoundedTaskExecutor.java:26)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.execute(BoundedTaskExecutor.java:194)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.run(BoundedTaskExecutor.java:186)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:668)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:665)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1.run(Executors.java:665)
    at java.base/java.lang.Thread.run(Thread.java:834)
2020-10-17 16:45:30,373 [   1151]   INFO - llij.ide.plugins.PluginManager - Plugin PluginDescriptor(name=Groovy, id=org.intellij.groovy, path=D:\IntelliJ IDEA Community Edition 2020.2.3\plugins\Groovy, version=202.7660.26) misses optional descriptor duplicates-detection-groovy.xml 
java.nio.file.NoSuchFileException: /META-INF/duplicates-detection-groovy.xml
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystem.newInputStream(ZipFileSystem.java:591)
    at jdk.zipfs/jdk.nio.zipfs.ZipPath.newInputStream(ZipPath.java:721)
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystemProvider.newInputStream(ZipFileSystemProvider.java:275)
    at java.base/java.nio.file.Files.newInputStream(Files.java:155)
    at com.intellij.openapi.util.JDOMUtil.load(JDOMUtil.java:351)
    at com.intellij.ide.plugins.BasePathResolver.resolvePath(BasePathResolver.java:54)
    at com.intellij.ide.plugins.PluginXmlPathResolver.resolvePath(PluginXmlPathResolver.java:58)
    at com.intellij.ide.plugins.XmlReader.readDependencies(XmlReader.java:215)
    at com.intellij.ide.plugins.IdeaPluginDescriptorImpl.readExternal(IdeaPluginDescriptorImpl.java:190)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromJar(PluginDescriptorLoader.java:94)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromDirAndNormalize(PluginDescriptorLoader.java:145)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromFileOrDir(PluginDescriptorLoader.java:120)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptor(PluginDescriptorLoader.java:44)
    at com.intellij.ide.plugins.PluginDescriptorLoader.lambda$loadDescriptorsFromDir$1(PluginDescriptorLoader.java:246)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at com.intellij.util.concurrency.BoundedTaskExecutor.doRun(BoundedTaskExecutor.java:215)
    at com.intellij.util.concurrency.BoundedTaskExecutor.access$200(BoundedTaskExecutor.java:26)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.execute(BoundedTaskExecutor.java:194)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.run(BoundedTaskExecutor.java:186)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:668)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:665)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1.run(Executors.java:665)
    at java.base/java.lang.Thread.run(Thread.java:834)
2020-10-17 16:45:30,523 [   1301]   INFO - llij.ide.plugins.PluginManager - Plugin PluginDescriptor(name=Java, id=com.intellij.java, path=D:\IntelliJ IDEA Community Edition 2020.2.3\plugins\java, version=202.7660.26) misses optional descriptor profiler-java.xml 
java.nio.file.NoSuchFileException: /META-INF/profiler-java.xml
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystem.newInputStream(ZipFileSystem.java:591)
    at jdk.zipfs/jdk.nio.zipfs.ZipPath.newInputStream(ZipPath.java:721)
    at jdk.zipfs/jdk.nio.zipfs.ZipFileSystemProvider.newInputStream(ZipFileSystemProvider.java:275)
    at java.base/java.nio.file.Files.newInputStream(Files.java:155)
    at com.intellij.openapi.util.JDOMUtil.load(JDOMUtil.java:351)
    at com.intellij.ide.plugins.BasePathResolver.resolvePath(BasePathResolver.java:54)
    at com.intellij.ide.plugins.PluginXmlPathResolver.resolvePath(PluginXmlPathResolver.java:58)
    at com.intellij.ide.plugins.XmlReader.readDependencies(XmlReader.java:215)
    at com.intellij.ide.plugins.IdeaPluginDescriptorImpl.readExternal(IdeaPluginDescriptorImpl.java:190)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromJar(PluginDescriptorLoader.java:94)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromDirAndNormalize(PluginDescriptorLoader.java:145)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptorFromFileOrDir(PluginDescriptorLoader.java:120)
    at com.intellij.ide.plugins.PluginDescriptorLoader.loadDescriptor(PluginDescriptorLoader.java:44)
    at com.intellij.ide.plugins.PluginDescriptorLoader.lambda$loadDescriptorsFromDir$1(PluginDescriptorLoader.java:246)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at com.intellij.util.concurrency.BoundedTaskExecutor.doRun(BoundedTaskExecutor.java:215)
    at com.intellij.util.concurrency.BoundedTaskExecutor.access$200(BoundedTaskExecutor.java:26)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.execute(BoundedTaskExecutor.java:194)
    at com.intellij.util.concurrency.BoundedTaskExecutor$1.run(BoundedTaskExecutor.java:186)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:668)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1$1.run(Executors.java:665)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/java.util.concurrent.Executors$PrivilegedThreadFactory$1.run(Executors.java:665)
    at java.base/java.lang.Thread.run(Thread.java:834)
2020-10-17 16:45:30,603 [   1381]   INFO - llij.ide.plugins.PluginManager - Loaded bundled plugins: IDEA CORE (202.7660.26), com.intellij.platform.images (202.7660.26), JetBrains maven model api classes (202.7660.26), JetBrains Repository Search (202.7660.26), Subversion (202.7660.26), XPathView + XSLT (202.7660.26), XSLT Debugger (202.7660.26), Smali Support (202.7660.26), Configuration Script (202.7660.26), Copyright (202.7660.26), Gradle (202.7660.26), Java (202.7660.26), Bytecode Viewer (202.7660.26), JUnit (202.7660.26), Java IDE Customization (202.7660.26), Java Stream Debugger (202.7660.26), Eclipse Interoperability (202.7660.26), Java Bytecode Decompiler (202.7660.26), Windows 10 Light Theme (202.7660.26), Properties (202.7660.26), Ant (202.7660.26), Java Internationalization (202.7660.26), UI Designer (202.7660.26), JavaFX (202.7660.26), Resource Bundle Editor (202.7660.26), Machine Learning Code Completion (202.7660.26), Task Management (202.7660.26), Mercurial (202.7660.26), Webp Support (202.7660.26), EditorConfig (202.7660.26), Terminal (202.7660.26), Git (202.7660.26), ChangeReminder (202.7660.26), Next File Prediction (202.7660.26), GitHub (202.7660.26), Shell Script (202.7660.26), TextMate Bundles (202.7660.26), YAML (202.7660.26), Settings Repository (202.7660.26), IntelliLang (202.7660.26), TestNG (202.7660.26), Code Coverage for Java (202.7660.26), Groovy (202.7660.26), Maven (202.7660.26), Gradle-Java (202.7660.26), Plugin DevKit (202.7660.26), Gradle-Maven (202.7660.26), Kotlin (1.3.72-release-IJ2020.1-6), Android (10.4.0.202.7660.26), Markdown (202.7660.26), Grazie (202.7660.26) 
2020-10-17 16:45:30,603 [   1381]   INFO - llij.ide.plugins.PluginManager - Loaded custom plugins: Scala (2020.2.27) 
2020-10-17 16:45:31,089 [   1867]   INFO - m.intellij.util.io.StorageLock - lower=100; upper=500; buffer=10; max=1696 
2020-10-17 16:45:31,108 [   1886]   INFO - tellij.util.io.FileChannelUtil - un-interruptible FileChannel-s will be used for indexes 
2020-10-17 16:45:31,307 [   2085]   INFO - rains.ide.BuiltInServerManager - built-in server started, port 63343 
2020-10-17 16:45:31,319 [   2097]   INFO - com.intellij.ide.ui.UISettings - Loaded: fontSize=13, fontScale=1.0; restored: fontSize=13, fontScale=1.0 
2020-10-17 16:45:31,332 [   2110]   INFO - til.net.ssl.CertificateManager - Default SSL context initialized 
2020-10-17 16:45:31,455 [   2233]   INFO - gs.impl.UpdateCheckerComponent - channel: release 
2020-10-17 16:45:31,741 [   2519]   INFO - BridgeProjectLifecycleListener - Using legacy project model to open project 
2020-10-17 16:45:31,849 [   2627]   INFO - pl.local.NativeFileWatcherImpl - Starting file watcher: D:\IntelliJ IDEA Community Edition 2020.2.3\bin\fsnotifier64.exe 
2020-10-17 16:45:31,946 [   2724]   INFO - pl.local.NativeFileWatcherImpl - Native file watcher is operational. 
2020-10-17 16:45:32,654 [   3432]   INFO - rojectCodeStyleSettingsManager - Loading Project code style 
2020-10-17 16:45:32,681 [   3459]   INFO - rojectCodeStyleSettingsManager - Project code style loaded 
2020-10-17 16:45:33,004 [   3782]   WARN - Container.ComponentManagerImpl - Do not use constructor injection (requestorClass=org.jetbrains.android.compose.AndroidComposeAutoDocumentation) 
2020-10-17 16:45:33,716 [   4494]   INFO - leBasedIndexDataInitialization - Initialization done: 2408 
2020-10-17 16:45:33,993 [   4771]   INFO - exImpl$StubIndexInitialization - Initialization done: 276 
2020-10-17 16:45:34,205 [   4983]   WARN - tartup.impl.StartupManagerImpl - Activities registered via registerPostStartupActivity must be dumb-aware: org.jetbrains.plugins.scala.lang.psi.impl.toplevel.synthetic.SyntheticClassesListener$$Lambda$1296/0x0000000100f64440@1eb6449d 
2020-10-17 16:45:34,248 [   5026]   INFO - j.ide.script.IdeStartupScripts - 0 startup script(s) found 
2020-10-17 16:45:34,308 [   5086]   INFO - .CodeStyleSettingsInferService - settings inference skipped: already done 
2020-10-17 16:45:34,320 [   5098]   INFO - pi.util.registry.RegistryValue - Registry value 'compiler.process.jdk' has changed to '' 
2020-10-17 16:45:34,349 [   5127]   WARN - tartup.impl.StartupManagerImpl - Activities registered via registerPostStartupActivity must be dumb-aware: org.jetbrains.kotlin.idea.configuration.ui.KotlinConfigurationCheckerComponent$projectOpened$1@2764562c 
2020-10-17 16:45:34,843 [   5621]   INFO - penapi.application.Experiments - Experimental features enabled for user: wsl.p9.support, wsl.p9.show.roots.in.file.chooser, inline.browse.button, linux.native.menu, recent.and.edited.files.together, show.create.new.element.in.popup, copy.reference.popup, new.large.text.file.viewer, property.value.inplace.editing, terminal.shell.command.handling, scala.uast.enabled 
2020-10-17 16:45:36,312 [   7090]   INFO - ge.ExternalProjectsDataStorage - Load external projects data in 105 millis (read time: 88) 
2020-10-17 16:45:36,638 [   7416]   INFO - CompilerWorkspaceConfiguration - Available processors: 8 
2020-10-17 16:45:36,720 [   7498]   INFO - ation.SbtCompilationSupervisor - Listening to incoming sbt compilation events on port 0. 
2020-10-17 16:45:36,733 [   7511]   INFO - ProjectRootPostStartUpActivity - C:/Users/Melih/Desktop/example/.idea case-sensitivity: expected=false actual=false 
2020-10-17 16:45:36,795 [   7573]   INFO - .diagnostic.PerformanceWatcher - Post-startup activities under progress took 840ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:37,553 [   8331]   INFO - tor.impl.FileEditorManagerImpl - Project opening took 5917 ms 
2020-10-17 16:45:37,722 [   8500]   INFO - .diagnostic.PerformanceWatcher - Pushing properties took 123ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:37,864 [   8642]   INFO - System.util.ExternalSystemUtil - External project [C:/Users/Melih/Desktop/example] resolution task started 
2020-10-17 16:45:39,187 [   9965]   WARN - com.intellij.util.xmlb.Binding - no accessors for org.jetbrains.kotlin.idea.highlighter.KotlinDefaultHighlightingSettingsProvider 
2020-10-17 16:45:46,515 [  17293]   INFO - gnostic.WindowsDefenderChecker - Windows Defender status: Failed to get excluded patterns 
2020-10-17 16:45:55,228 [  26006]   INFO - System.util.ExternalSystemUtil - External project [C:/Users/Melih/Desktop/example] resolution task executed in 17364 ms. 
2020-10-17 16:45:55,796 [  26574]   INFO - ge.ExternalProjectsDataStorage - Save external projects data in 58 ms 
2020-10-17 16:45:55,950 [  26728]   WARN - com.intellij.util.xmlb.Binding - no accessors for org.jetbrains.kotlin.idea.core.script.configuration.utils.ScriptClassRootsStorage 
2020-10-17 16:45:55,978 [  26756]   INFO - rationStore.ComponentStoreImpl - Saving Project(name=example, containerState=ACTIVE, componentStore=C:\Users\Melih\Desktop\example)CommitMessageInspectionProfile took 29 ms 
2020-10-17 16:45:58,027 [  28805]   INFO - .diagnostic.PerformanceWatcher - Indexable file iteration took 20303ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:58,036 [  28814]   INFO - indexing.UnindexedFilesUpdater - Unindexed files update started: 0 files to index 
2020-10-17 16:45:58,052 [  28830]   INFO - j.ide.actions.RevealFileAction - Exit code 1 
2020-10-17 16:45:58,660 [  29438]   INFO - .ScalaCompilerReferenceService - Initialized ScalaCompilerReferenceService in example, current compiler mode = JPS 
2020-10-17 16:45:59,967 [  30745]   INFO - .diagnostic.PerformanceWatcher - Searching for external libraries with Android resources. Found 0 libraries. took 13ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:59,967 [  30745]   INFO - .diagnostic.PerformanceWatcher - Searching for external libraries with Android resources. Found 0 libraries. took 3ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:59,967 [  30745]   INFO - .diagnostic.PerformanceWatcher - Searching for external libraries with Android resources. Found 0 libraries. took 13ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:45:59,967 [  30745]   INFO - .diagnostic.PerformanceWatcher - Searching for external libraries with Android resources. Found 0 libraries. took 13ms; general responsiveness: ok; EDT responsiveness: ok 
2020-10-17 16:49:40,869 [ 251647]   INFO - System.util.ExternalSystemUtil - External project [C:/Users/Melih/Desktop/example] resolution task started 
2020-10-17 16:49:47,002 [ 257780]   INFO - rationStore.ComponentStoreImpl - Saving Project(name=example, containerState=ACTIVE, componentStore=C:\Users\Melih\Desktop\example)KotlinCommonCompilerArguments took 17 ms, ScalaProjectSettings took 13 ms 
2020-10-17 16:49:54,667 [ 265445]   INFO - System.util.ExternalSystemUtil - External project [C:/Users/Melih/Desktop/example] resolution task executed in 13798 ms. 
2020-10-17 16:49:55,129 [ 265907]   INFO - ge.ExternalProjectsDataStorage - Save external projects data in 72 ms 
2020-10-17 16:49:55,205 [ 265983]   INFO - rationStore.ComponentStoreImpl - Saving Project(name=example, containerState=ACTIVE, componentStore=C:\Users\Melih\Desktop\example)JsonSchemaMappingsProjectConfiguration took 49 ms 
2020-10-17 16:50:07,409 [ 278187]   INFO - j.ide.actions.RevealFileAction - Exit code 1 


</code></pre>
",7686,3,6,3,scala;intellij-idea;sbt,2020-10-18 17:45:12,2020-10-18 17:45:12,2022-05-07 04:34:40,i m trying to setup scala on intellij ide and when i create a new project it seems fine  when i import another project it errors  extracting structure failed  the sbt shell seems to be working  what can be the issue  this is my build sbt  here is the log 
699,699,9994946,51948314,Spark not recognizing question mark (?) as nullValue parameter when reading from a csv,"<p>I am using PySpark 2.3.1 with Python 3.6.6 at the moment.</p>
<p>I need to work with a .csv file where <code>?</code> are used as <code>NA</code>. I want to make PySpark recognize <code>?</code> as NA directly, so I can treat them consequently.</p>
<p>I have tried <code>nullValue=</code> argument in <code>spark.read.csv</code> for that without success, and I am not sure if it has to do with the argument being improperly used or the <code>?</code> character being a problem in those cases (I have tried both <code>nullValue='?'</code> and <code>nullValue='\?'</code>).</p>
<p>Having read PySpark API documentation, and tried Pandas <code>pd.read_csv</code> with <code>na_values=</code> with the same outcome, I would say it there is something with <code>?</code> that makes it not to work, but feel free to tell me if I am wrong at that.</p>
<p>What should I do?</p>
<p>The file is the adult dataset from UCI: <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"" rel=""nofollow noreferrer"">http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data</a></p>
",866,1,1,2,apache-spark;pyspark,2018-08-21 17:37:00,2018-08-21 17:37:00,2022-05-07 03:10:06,i am using pyspark    with python    at the moment  i need to work with a  csv file where   are used as na  i want to make pyspark recognize   as na directly  so i can treat them consequently  i have tried nullvalue  argument in spark read csv for that without success  and i am not sure if it has to do with the argument being improperly used or the   character being a problem in those cases  i have tried both nullvalue     and nullvalue        having read pyspark api documentation  and tried pandas pd read_csv with na_values  with the same outcome  i would say it there is something with   that makes it not to work  but feel free to tell me if i am wrong at that  what should i do  the file is the adult dataset from uci  
700,700,18143520,72142910,Gtk-WARNING **: cannot open display: localhost:10.0,"<p>I am on the new end of learning remote connections and I ran into a rather strange issue when connecting remotely to a machine.</p>
<p>Host: Jetson Nano - Ubuntu
Client: Asus desktop - Linux Mint</p>
<p>I am using SSH to connect to the host machine. Once I'm in, I run my program which should open the camera that the host machine has connected via mipi connection... but it does not show a display window. Rather it displays:</p>
<pre><code>Gtk-WARNING **: cannot open display: lcocalhost:10.0
CONSUMER: Done Success 
(Argus)Error InvalidState: Argus client is exiting with 2 outstanding client threads
</code></pre>
<p>If run the program in the machine without SSH connection, it works and the display shows what the camera is capturing. I tried changing the <code>X11forwarding</code> and <code>agent</code> to YES, and I tried <code>export DISPLAY=localhost:10.0</code>. That did not work as well.</p>
<p>Any help would be appreciated.
Thanks, GM</p>
",1243,1,0,4,ssh;display;nvidia-jetson;nvidia-jetson-nano,2022-05-06 19:42:59,2022-05-06 19:42:59,2022-05-07 02:11:35,i am on the new end of learning remote connections and i ran into a rather strange issue when connecting remotely to a machine  i am using ssh to connect to the host machine  once i m in  i run my program which should open the camera that the host machine has connected via mipi connection    but it does not show a display window  rather it displays  if run the program in the machine without ssh connection  it works and the display shows what the camera is capturing  i tried changing the xforwarding and agent to yes  and i tried export display localhost    that did not work as well 
701,701,18812970,72029394,What is the best approach for storing multiple vectors per person for face recognition,"<p>I want to make a face recognition for employees as work. I already have system that gets image from cameras and outputs face embeddings (128-dimensional vectors). So my next step, as far as I understand, is to compare these embeddings with the one stored somewhere in database and find one with nearest distance.</p>
<p>The problem is that I want to enable machine learning for this. Initially, on like every tutorial, only one photo of employee is used to create a reference embedding. But what if a want to store multiple embeddings for one person? For example, maybe this person came with glasses, or slightly changed appearance so that my system no longer recognises it. I want to be able to associate multiple embeddings with one person or another, creating a collection of embeddings for each employee, I think this would improve recognition system. And if in future my system will show me that there's unknown person, I could tell it that this embedding corresponds to specific person.</p>
<p>Is there any database that can store (maybe as array) or associate multiple vectors per person? I've looked into Milvus, FAISS, but didn't find anything about that.</p>
<p>I use Python 3.9 with OpenCV3, Tensorflow and Keras for creating embeddings.</p>
",112,3,1,5,python;tensorflow;keras;face-recognition;deepface,2022-04-27 18:42:30,2022-04-27 18:42:30,2022-05-06 20:21:46,i want to make a face recognition for employees as work  i already have system that gets image from cameras and outputs face embeddings   dimensional vectors   so my next step  as far as i understand  is to compare these embeddings with the one stored somewhere in database and find one with nearest distance  the problem is that i want to enable machine learning for this  initially  on like every tutorial  only one photo of employee is used to create a reference embedding  but what if a want to store multiple embeddings for one person  for example  maybe this person came with glasses  or slightly changed appearance so that my system no longer recognises it  i want to be able to associate multiple embeddings with one person or another  creating a collection of embeddings for each employee  i think this would improve recognition system  and if in future my system will show me that there s unknown person  i could tell it that this embedding corresponds to specific person  is there any database that can store  maybe as array  or associate multiple vectors per person  i ve looked into milvus  faiss  but didn t find anything about that  i use python   with opencv  tensorflow and keras for creating embeddings 
702,702,13379406,72142024,How do I replace missing values with NaN,"<p>I am using the IMDB dataset for machine learning, and it contains a lot of missing values which are entered as '\N'. Specifically in the StartYear column which contains the movie year release I want to convert the values to integers. Which im not able to do right now, I could drop these values but I wanted to see why they're missing first. I tried several things but no success.</p>
<p>This is my latest attempt:</p>
<p><a href=""https://i.stack.imgur.com/hDKp8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hDKp8.png"" alt=""My attempt"" /></a></p>
",36,1,0,3,python;pandas;missing-data,2022-05-06 18:40:53,2022-05-06 18:40:53,2022-05-06 20:12:07,i am using the imdb dataset for machine learning  and it contains a lot of missing values which are entered as   n   specifically in the startyear column which contains the movie year release i want to convert the values to integers  which im not able to do right now  i could drop these values but i wanted to see why they re missing first  i tried several things but no success  this is my latest attempt  
703,703,16709449,72140192,NumPy array value error from training in Auto-Keras with StratifiedKFold,"<h2>Background</h2>
<p>My sentiment analysis research comes across a variety of datasets. Recently I've encountered one dataset that somehow I just cannot train successfully. I mostly work with open data in <code>.CSV</code> file format, hence <code>Pandas</code> and <code>NumPy</code> are heavily used.</p>
<p>During my research, one of the approaches is trying to integrate automated machine learning (<code>AutoML</code>), and the library I chose to use was <code>Auto-Keras</code>, mainly using its <code>TextClassifier()</code> wrapper function to achieve <code>AutoML</code>.</p>
<h2>Main Problem</h2>
<p>I've verified with official documentation, that the <code>TextClassifier()</code> takes data in the format of the NumPy array. However, when I load the data into <code>Pandas DataFrame</code> and used <code>.to_numpy()</code> on the columns that I need to train, the following error kept showing:</p>
<pre class=""lang-py prettyprint-override""><code>
ValueError                                Traceback (most recent call last)
&lt;ipython-input-13-1444bf2a605c&gt; in &lt;module&gt;()
     16     clf = ak.TextClassifier(overwrite=True, max_trials=2)
     17 
---&gt; 18     clf.fit(x_train, y_train, epochs=3, callbacks=cbs)
     19 
     20 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).

</code></pre>
<h2>Error-related code sectors</h2>
<p>The sector where I drop the unneeded <code>Pandas DataFrame</code> columns using <code>.drop()</code>, and convert the needed columns to <code>NumPy</code> Array using the <code>to_numpy()</code> function that <code>Pandas</code> has provided.</p>
<pre class=""lang-py prettyprint-override""><code>
df_src = pd.read_csv(get_data)

df_src = df_src.drop(columns=[&quot;Name&quot;, &quot;Cast&quot;, &quot;Plot&quot;, &quot;Direction&quot;,
                &quot;Soundtrack&quot;, &quot;Acting&quot;, &quot;Cinematography&quot;])

df_src = df_src.reset_index(drop=True)

X = df_src[&quot;Review&quot;].to_numpy()

Y = df_src[&quot;Overall Sentiment&quot;].to_numpy()

print(X, &quot;\n&quot;)
print(&quot;\n&quot;, Y)

</code></pre>
<p>The <strong>main</strong> error code part, where I perform <code>StratifedKFold()</code> and at the same time, use <code>TextClassifier()</code> to train and test the model.</p>
<pre class=""lang-py prettyprint-override""><code>
fold = 0
for train, test in skf.split(X, Y):
    fold += 1
    print(f&quot;Fold #{fold}\n&quot;)
    
    x_train = X[train]
    y_train = Y[train]
    
    x_test = X[test]
    y_test = Y[test]
    
    
    cbs = [tf.keras.callbacks.EarlyStopping(patience=3)]
    
    clf = ak.TextClassifier(overwrite=True, max_trials=2)
    
    
    # The line where it indicated the error.
    clf.fit(x_train, y_train, epochs=3, callbacks=cbs)
    
    
    pred = clf.predict(x_test) # result data type is in lists of `string`
    
    ceval = clf.evaluate(x_test, y_test)
    
    metrics_test = metrics.classification_report(y_test, np.array(list(pred), dtype=int))
    
    print(metrics_test, &quot;\n&quot;)
    
    print(f&quot;Fold #{fold} finished\n&quot;)

</code></pre>
<h2>Supplementary</h2>
<p>I am sharing the full code related to the error through <code>Google Colab</code>, which you can help me <a href=""https://colab.research.google.com/drive/1w11j4ep2yImMJvWqs7plFfcrX600f8r8?usp=sharing"" rel=""nofollow noreferrer"">diagnose here</a>.</p>
<h3>Edit notes</h3>
<p>I have tried the potential solution, such as:</p>
<pre class=""lang-py prettyprint-override""><code>x_train = np.asarray(x_train).astype(np.float32)
y_train = np.asarray(y_train).astype(np.float32)
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>x_train = tf.data.Dataset.from_tensor_slices((x_train,))
y_train = tf.data.Dataset.from_tensor_slices((y_train,))
</code></pre>
<p>However, the problem remains.</p>
",36,1,0,5,pandas;numpy;tensorflow;keras;auto-keras,2022-05-06 16:12:18,2022-05-06 16:12:18,2022-05-06 17:20:18,my sentiment analysis research comes across a variety of datasets  recently i ve encountered one dataset that somehow i just cannot train successfully  i mostly work with open data in  csv file format  hence pandas and numpy are heavily used  during my research  one of the approaches is trying to integrate automated machine learning  automl   and the library i chose to use was auto keras  mainly using its textclassifier   wrapper function to achieve automl  i ve verified with official documentation  that the textclassifier   takes data in the format of the numpy array  however  when i load the data into pandas dataframe and used  to_numpy   on the columns that i need to train  the following error kept showing  the sector where i drop the unneeded pandas dataframe columns using  drop    and convert the needed columns to numpy array using the to_numpy   function that pandas has provided  the main error code part  where i perform stratifedkfold   and at the same time  use textclassifier   to train and test the model  i am sharing the full code related to the error through google colab  which you can help me   i have tried the potential solution  such as  or however  the problem remains 
704,704,11967549,72140540,Is this a valid approach to scale your target in machine learning without leaking information?,"<p>Consider a housing price dataset, where the goal is to predict the sale price.</p>
<p>I would like to do this by predicting the &quot;Sale price per Squaremeter&quot; instead,
since it yields better results.</p>
<p>The question is if I implement it like this - does it introduce an information leak in the test set or not?</p>
<p>When I split my dataset in scikit learn:</p>
<pre><code>df= read(Data)
target = df[&quot;SalePrice&quot;]
df.drop(columns=[&quot;SalePrice&quot;], inplace=True)

X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.20)
</code></pre>
<p>And then scale y_train:</p>
<pre><code># Scale target by LivingSpace and call fit()
y_train = target/X_train[&quot;LivingSpace&quot;]
estimator.fit(X_train, y_train)
</code></pre>
<p>And use predict and scale the target in y_test to get SalePrice per Squaremeter:</p>
<pre><code>y_pred, y_true = estimator.predict(X_test), y_test/X_test[&quot;LivingSpace&quot;]
</code></pre>
<p>I think this is valid, since I only scale the target by a known value. It should not make a difference if I predict the <code>SalePrice</code> directly or <code>SalePrice / LivingSpace</code>, since LivingSpace is given to me anyway when I predict the price.</p>
<p>If this holds true, we could also directly apply this target transformation to the train and test set and just transform the predicted values back in the end, right?</p>
<p>This should of course then also hold true for any feature given in X. As long as Information about the target itself is NOT present in X I see no problem here. Remember the true target is the SalePrice only, so my intention is to scale it back from sale price per squaremeter. The transformation is just used for better training results.</p>
<p>What are your thoughts about this code?</p>
",26,1,-1,5,python;machine-learning;scikit-learn;training-data;data-transform,2022-05-06 16:40:17,2022-05-06 16:40:17,2022-05-06 17:06:45,consider a housing price dataset  where the goal is to predict the sale price  the question is if i implement it like this   does it introduce an information leak in the test set or not  when i split my dataset in scikit learn  and then scale y_train  and use predict and scale the target in y_test to get saleprice per squaremeter  i think this is valid  since i only scale the target by a known value  it should not make a difference if i predict the saleprice directly or saleprice   livingspace  since livingspace is given to me anyway when i predict the price  if this holds true  we could also directly apply this target transformation to the train and test set and just transform the predicted values back in the end  right  this should of course then also hold true for any feature given in x  as long as information about the target itself is not present in x i see no problem here  remember the true target is the saleprice only  so my intention is to scale it back from sale price per squaremeter  the transformation is just used for better training results  what are your thoughts about this code 
705,705,9578256,51833310,What is Killed:9 and how to fix in macOS Terminal?,"<p>I have a simple Python code for a machine learning project. I have a relatively big database of spontaneous speech. I started to train my speech model. Since it's a huge database I let it work overnight. In the morning I woke up and saw a mysterious</p>
<p><strong>Killed: 9</strong></p>
<p>line in my Terminal. Nothing else. There is no other error message or something to work with. The code run well for about 6 hours which is 75% of the whole process so I really don't understand whats went wrong.</p>
<p>What is Killed:9 and how to fix it? It's very frustrating to lose hours of computing time...</p>
<p>I'm on macOS Mojave beta if it's matter. Thank you in advance!</p>
",17086,2,12,4,python;macos;machine-learning;terminal,2018-08-14 08:58:58,2018-08-14 08:58:58,2022-05-06 16:35:13,i have a simple python code for a machine learning project  i have a relatively big database of spontaneous speech  i started to train my speech model  since it s a huge database i let it work overnight  in the morning i woke up and saw a mysterious killed   line in my terminal  nothing else  there is no other error message or something to work with  the code run well for about  hours which is   of the whole process so i really don t understand whats went wrong  what is killed  and how to fix it  it s very frustrating to lose hours of computing time    i m on macos mojave beta if it s matter  thank you in advance 
706,706,10617144,72138278,How to resolve &quot;Bucket max cardinality estimate required for influencers host.name but not supplied&quot; for machine learning job in Kibana,"<p>I'm currently setting up a machine learning job to detect rare events for host names. However I get the following error on the validation page of the job:</p>
<pre><code>Job Validation Error

[Bucket max] cardinality estimate required for [influencers] [host.name] but not supplied

See the full error
</code></pre>
<p>This is the full error:</p>
<pre><code>{
  &quot;statusCode&quot;: 400,
  &quot;error&quot;: &quot;Bad Request&quot;,
  &quot;message&quot;: &quot;[illegal_argument_exception: [illegal_argument_exception] Reason: [Bucket max] cardinality estimate required for [influencers] [host.name] but not supplied]: [Bucket max] cardinality estimate required for [influencers] [host.name] but not supplied&quot;,
  &quot;attributes&quot;: {
    &quot;body&quot;: {
      &quot;error&quot;: {
        &quot;root_cause&quot;: [
          {
            &quot;type&quot;: &quot;illegal_argument_exception&quot;,
            &quot;reason&quot;: &quot;[Bucket max] cardinality estimate required for [influencers] [host.name] but not supplied&quot;
          }
        ],
        &quot;type&quot;: &quot;illegal_argument_exception&quot;,
        &quot;reason&quot;: &quot;[Bucket max] cardinality estimate required for [influencers] [host.name] but not supplied&quot;
      },
      &quot;status&quot;: 400
    }
  }
}
</code></pre>
<p>What causes this error and how can i mitigate it?
Thanks for the help in advance!</p>
",15,0,0,4,elasticsearch;logging;kibana;elastic-stack,2022-05-06 13:46:50,2022-05-06 13:46:50,2022-05-06 16:07:11,i m currently setting up a machine learning job to detect rare events for host names  however i get the following error on the validation page of the job  this is the full error 
707,707,5917999,72138049,"Transform String data into numbers (where string should be always the same number), with the ability to transform them back","<p>I would like to build a Machine Learning solution, predicting upcoming sales per product.</p>
<p>The dataset is containing thousand products (which are represented as a string. E.g., ‘Product_1_12345’).</p>
<p>Since the product information is essential for the modelling (would like to forecast, on product level), I tried different approaches (among others creating dummies).</p>
<p>However, since this was causing too many columns, I am exploring an alternative. What I would like to have:</p>
<pre><code>Original_Product_ID      New_Product_ID
Product1_ABC                1
Product4_ABC                2
Product1_ABC                1
Another_Product             3
Product4_ABC                2
</code></pre>
<p><strong>The goal is to assign each unique string, to a number</strong>. But if we have that product later again, I would like to have the same number.</p>
<p>Later on, I would like to <strong>convert the numbers back to Original</strong> Product ID.</p>
<p>Does anyone know how to do this? A dictionary doesn’t look like a solution, since I need to fill it in automatically (and I have thousands of products).</p>
",24,1,1,2,python;pandas,2022-05-06 13:29:11,2022-05-06 13:29:11,2022-05-06 13:35:51,i would like to build a machine learning solution  predicting upcoming sales per product  the dataset is containing thousand products  which are represented as a string  e g    product__    since the product information is essential for the modelling  would like to forecast  on product level   i tried different approaches  among others creating dummies   however  since this was causing too many columns  i am exploring an alternative  what i would like to have  the goal is to assign each unique string  to a number  but if we have that product later again  i would like to have the same number  later on  i would like to convert the numbers back to original product id  does anyone know how to do this  a dictionary doesn t look like a solution  since i need to fill it in automatically  and i have thousands of products  
708,708,17281547,69978051,"Trying to make game detect mouse clicks using python, all the libraries failed","<p>About 3-4 months ago, I decided to use my basic python knowledge to write a simple farming bot for an old RPG. After some struggling, I found a way to use pyautogui and run the script as an administrator to send key presses and mouse clicks to the game, even managed to bypass their tricky antibot system (used pyautogui locate on screen function and pytesseract OCR, source code below). Needless to say that adding new features and overcoming various problems associated with them has taught me a lot, much more than any book or video, and was a lot of fun.</p>
<p>But the server released a new update, which uses &quot;smart guard&quot; to block any commands not created by actual keyboard and mouse. Not giving up so easily, I've tried multiple workarounds, but unsuccessfully (I'm running Windows 10 x64 and the game uses DirectX):</p>
<ul>
<li><p>Tried using other python modules, such as pydirectinput, pywinauto,
keyboard etc.</p>
</li>
<li><p>Tried using AutoHotKey and its multiple ways of sending commands:
send, sendevent, sendeventraw etc.</p>
</li>
<li><p>Compiled python and AHK scripts to prevent game detecting them.</p>
</li>
<li><p>Ran the game in VirtualBox and the script in the host machine, but
the VM didn't receive input from the host, and the game was very
laggy.</p>
</li>
<li><p>Even tested inputs with Windows On-Screen keyboard, which also got
ignored.</p>
</li>
<li><p>Experimented with win32api, win32con to send mouse clicks/ key
presses as &quot;scancodes&quot;. But my knowledge is very limited in this area, so it could be just wrong.</p>
</li>
</ul>
<p>Anyway, even though I failed to send a single click to the game from the script, this experience has taught me a lot. And I'd like to keep learning and messing around with the code, so I'd be grateful if anyone has any ideas or workarounds for this problem, but please keep my low amount of knowledge in mind when suggesting anything, thanks in advance :)</p>
",192,2,2,4,python;bots;ctypes;pyautogui,2021-11-15 22:23:16,2021-11-15 22:23:16,2022-05-06 13:30:03,about   months ago  i decided to use my basic python knowledge to write a simple farming bot for an old rpg  after some struggling  i found a way to use pyautogui and run the script as an administrator to send key presses and mouse clicks to the game  even managed to bypass their tricky antibot system  used pyautogui locate on screen function and pytesseract ocr  source code below   needless to say that adding new features and overcoming various problems associated with them has taught me a lot  much more than any book or video  and was a lot of fun  but the server released a new update  which uses  smart guard  to block any commands not created by actual keyboard and mouse  not giving up so easily  i ve tried multiple workarounds  but unsuccessfully  i m running windows  x and the game uses directx   compiled python and ahk scripts to prevent game detecting them  anyway  even though i failed to send a single click to the game from the script  this experience has taught me a lot  and i d like to keep learning and messing around with the code  so i d be grateful if anyone has any ideas or workarounds for this problem  but please keep my low amount of knowledge in mind when suggesting anything  thanks in advance   
709,709,6071007,72134294,Can a neo4j graph projected in Python be then transformed to pandas dataframe/tensor/numpy/etc to be used with pytorch/etc.?,"<p>I'm trying to run algorithms on Neo4j's Aura DS databases.</p>
<p>It seems like I've by and large understood how to connect to an Aura DS database, project a particular graph, then apply one of the algorithms from the graphdatascience (GDS) library in order to do node classification or solve some other machine learning problem.</p>
<p>However, can I somehow connect to an Aura DS database and retrieve the data in a format like pandas dataframe/tensor/numpy array/etc. and use other libraries besides GDS to train?</p>
<p>Apologies if this is trivial. I've tried searching for this, but got no satisfactory answer.</p>
",37,1,1,3,python;neo4j;graph-data-science,2022-05-06 04:05:53,2022-05-06 04:05:53,2022-05-06 12:28:13,i m trying to run algorithms on neoj s aura ds databases  it seems like i ve by and large understood how to connect to an aura ds database  project a particular graph  then apply one of the algorithms from the graphdatascience  gds  library in order to do node classification or solve some other machine learning problem  however  can i somehow connect to an aura ds database and retrieve the data in a format like pandas dataframe tensor numpy array etc  and use other libraries besides gds to train  apologies if this is trivial  i ve tried searching for this  but got no satisfactory answer 
710,710,18838702,72123313,Tensorflow accuracy score,"<p>I am creating a machine learning model in the form of regression.
I started with XGBoost to get my first estimates. However, these were not convincing enough (using XGB Regressor I got only 0.60 R2 Score).</p>
<p>So I started looking for solutions with neural networks and ended up using tensorflow. However, I am relatively new to this module and would like to know if there is an equivalent to <code>xgboost.score</code>?</p>
<p>The first code was using xgboost and I am working now on the second one, with tensorflow.</p>
<pre><code>xgb = XGBRegressor(learning_rate = 0.30012, max_depth = 5, n_estimators = 180, subsample = 0.7, colsample_bylevel = 0.7, colsample_bytree = 0.7, min_child_weight = 4, reg_alpha = 10, reg_lambda = 10)
xgb.fit(X_train, y_train)

print(&quot;Score on train data : &quot; + str(xgb.score(X_train, y_train)))
print(&quot;Score on validation data : &quot; + str(xgb.score(X_val, y_val)))
</code></pre>
<p>The second one using TensorFlow:</p>
<pre><code>tf.random.set_seed(123)  #first we set random seed

model = tf.keras.Sequential([
  tf.keras.layers.Dense(100, activation = tf.keras.activations.relu),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Dense(1)
])

model.compile( loss = tf.keras.losses.mae, #mae stands for mean absolute error
              optimizer = tf.keras.optimizers.SGD(), #stochastic GD
              metrics = ['mae'])
model.fit( X_train, y_train, epochs = 100)
</code></pre>
<p>How do I evaluate my tensorflow model using R2 Score?</p>
",72,2,-2,3,python;tensorflow;machine-learning,2022-05-05 12:48:28,2022-05-05 12:48:28,2022-05-06 06:29:39,so i started looking for solutions with neural networks and ended up using tensorflow  however  i am relatively new to this module and would like to know if there is an equivalent to xgboost score  the first code was using xgboost and i am working now on the second one  with tensorflow  the second one using tensorflow  how do i evaluate my tensorflow model using r score 
711,711,10410121,72133526,"ReactJS, Wordpress and plesk - npm command doesn&#39;t exist?","<p>I am an amateur programmer. I've played with varoius languages over the years but php is the one I know best and often reach for even when I'm sure a real programmer would say I'm using the wrong tool.</p>
<p>I had a developer build a wordpress site for me with a custom theme. I have subsequently written a couple of plugins in pure php that work just fine.</p>
<p>I am now wishing to add another plugin for my site that is going to require a little more frontend trickery. The last time I did anything like this was long before reactjs existed and I cannot remember much javascript - however I was able to master some ajax with php and have old code I can look at to remind myself.</p>
<p>I have installed node and then react on my windows laptop and have built a few hello-world apps. But I now want to write a plugin that will work on my wordpress install.</p>
<p>I am trying to follow this tutorial  <a href=""https://www.green-box.co.uk/create-a-wordpress-plugin-that-uses-a-react-app/"" rel=""nofollow noreferrer"">https://www.green-box.co.uk/create-a-wordpress-plugin-that-uses-a-react-app/</a>. It says that a prerequisite is to have node installed - which I have done on my plesk server by following the instructions for adding it through the &quot;add/remove components&quot; of my plesk web interface, rather than over command line.</p>
<p>Now when I SSH to my server, cd to the /wp-content/plugins/helloworld/ and type npm create-react-app helloworld I get the error:</p>
<pre><code>nodenv: npm: command not found

The `npm' command exists in these Node versions:
  12
  14
  16
  18
</code></pre>
<p>I think I'm fundamentally misunderstanding how reactjs works but can't seem to get my head round what to do. Have I got the 'wrong' node installed? Or am I supposed to build the code on my local machine and somehow compile it and upload to my web server? Last time I used javascript it was just dumped in plain text files and referened in the html header like css files. I'm sure it's all quite simple but I'm wondering if I shouldn't just ditch the idea of learning any react and doing it all with vanilla javascript.</p>
<p>Could any of the wise heads out there point me in the right direction with a clue? I'm too old for all this new-fangled techology I'm sure this is how my dad felt when they invented cellphones :(</p>
",23,0,0,4,php;reactjs;wordpress;plesk-onyx,2022-05-06 02:23:40,2022-05-06 02:23:40,2022-05-06 02:23:40,i am an amateur programmer  i ve played with varoius languages over the years but php is the one i know best and often reach for even when i m sure a real programmer would say i m using the wrong tool  i had a developer build a wordpress site for me with a custom theme  i have subsequently written a couple of plugins in pure php that work just fine  i am now wishing to add another plugin for my site that is going to require a little more frontend trickery  the last time i did anything like this was long before reactjs existed and i cannot remember much javascript   however i was able to master some ajax with php and have old code i can look at to remind myself  i have installed node and then react on my windows laptop and have built a few hello world apps  but i now want to write a plugin that will work on my wordpress install  i am trying to follow this tutorial    it says that a prerequisite is to have node installed   which i have done on my plesk server by following the instructions for adding it through the  add remove components  of my plesk web interface  rather than over command line  now when i ssh to my server  cd to the  wp content plugins helloworld  and type npm create react app helloworld i get the error  i think i m fundamentally misunderstanding how reactjs works but can t seem to get my head round what to do  have i got the  wrong  node installed  or am i supposed to build the code on my local machine and somehow compile it and upload to my web server  last time i used javascript it was just dumped in plain text files and referened in the html header like css files  i m sure it s all quite simple but i m wondering if i shouldn t just ditch the idea of learning any react and doing it all with vanilla javascript  could any of the wise heads out there point me in the right direction with a clue  i m too old for all this new fangled techology i m sure this is how my dad felt when they invented cellphones   
712,712,6071007,72132644,"Error using neo4j&#39;s gds.beta.pipeline.nodeClassification.train: Tensor of dimensions greater than 2 are not supported, got 2 dimensions","<p>I'm trying to do node classification with neo4j's graphdatascience (GDS).
There's some example here, but it is clear it is out of date:
<a href=""https://neo4j.com/developer/graph-data-science/node-classification/"" rel=""nofollow noreferrer"">https://neo4j.com/developer/graph-data-science/node-classification/</a>
Here's some newer documentation that I'm also following:
<a href=""https://neo4j.com/docs/graph-data-science/current/machine-learning/nodeclassification-pipelines/"" rel=""nofollow noreferrer"">https://neo4j.com/docs/graph-data-science/current/machine-learning/nodeclassification-pipelines/</a></p>
<p>So I have the Cora dataset of nodes which are &quot;Papers&quot; and edges which are between these Paper nodes. Each &quot;Paper&quot; node has feature x, a list of 0/1 entries that indicate whether specific words are present in the document. Also, a possible feature is &quot;pr&quot;, the pagerank as computed by the algorithm. The label is denoted by &quot;y&quot; and represents the class of the Paper.
Goal is to train model on set of Paper nodes to classify (i.e. find the label y) then check this model on a testing data and generally be able to do predictions on y.</p>
<p>This is my code so far:</p>
<pre><code>    # Client import
from graphdatascience import GraphDataScience

# Replace with the actual URI, username and password
AURA_CONNECTION_URI = &quot;neo4j+s://xxx.databases.neo4j.io&quot;
AURA_USERNAME = &quot;neo4j&quot;
AURA_PASSWORD = &quot;password_here&quot;

# Client instantiation
gds = GraphDataScience(
    AURA_CONNECTION_URI,
    auth=(AURA_USERNAME, AURA_PASSWORD),
    aura_ds=True
)

# Project a graph using the extended syntax
extended_form_graph_final, result = gds.graph.project(
    &quot;Long_form_example_graph&quot;,
    {&quot;Paper&quot;: {&quot;label&quot;: &quot;Paper&quot;, &quot;properties&quot;: {&quot;y&quot;:{&quot;property&quot;:&quot;y&quot;}, &quot;pr&quot;:{&quot;property&quot;:&quot;pr&quot;}, &quot;intX&quot;:{&quot;property&quot;:&quot;intX&quot;}, &quot;train&quot;:{&quot;property&quot;:&quot;train&quot;}, &quot;valid&quot;:{&quot;property&quot;:&quot;valid&quot;}, &quot;test&quot;:{&quot;property&quot;:&quot;test&quot;}}}},
    &quot;Citation&quot;
)

gds.run_cypher(
    &quot;&quot;&quot;
    CALL gds.beta.pipeline.nodeClassification.create('pipe')
    &quot;”&quot;)

gds.run_cypher(
&quot;&quot;&quot;
CALL gds.beta.pipeline.nodeClassification.addLogisticRegression('pipe', {penalty: 0.0625})
&quot;”&quot;)

gds.run_cypher(
&quot;&quot;&quot;
CALL gds.alpha.pipeline.nodeClassification.addRandomForest('pipe', {numberOfDecisionTrees: 5})
&quot;”&quot;)

gds.run_cypher(
&quot;&quot;&quot;
CALL gds.beta.pipeline.nodeClassification.train('Long_form_example_graph', {
  pipeline: 'pipe',
  nodeLabels: ['Paper'],
  modelName: 'hello',
  targetProperty: 'y',
  randomSeed: 1,
  metrics: ['F1_WEIGHTED']
})
&quot;&quot;&quot;)
</code></pre>
<p>I'm using the jupyter lab so all of the code works, except for the last run_cypher where I'm calling gds.beta.pipeline.nodeClassification.train:</p>
<p>ClientError: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure <code>gds.beta.pipeline.nodeClassification.train</code>: Caused by: java.lang.IllegalArgumentException: Tensor of dimensions greater than 2 are not supported, got 2 dimensions}</p>
<p>I've tried using only &quot;pr&quot; as a feature to guess the label y, thinking that maybe the problem was that intX is a list of integers as a feature. But that still gave me the same error :-(</p>
<p>Also, note that initially the feature list x was a &quot;String&quot;, but then I used Neo4j Browser to transform the String into a list on int's which I called intX (code not shown, but I've verified this)</p>
",32,0,0,3,machine-learning;neo4j;graph-data-science,2022-05-06 00:56:31,2022-05-06 00:56:31,2022-05-06 00:56:31,this is my code so far  i m using the jupyter lab so all of the code works  except for the last run_cypher where i m calling gds beta pipeline nodeclassification train  clienterror   code  neo clienterror procedure procedurecallfailed   message  failed to invoke procedure gds beta pipeline nodeclassification train  caused by  java lang illegalargumentexception  tensor of dimensions greater than  are not supported  got  dimensions  i ve tried using only  pr  as a feature to guess the label y  thinking that maybe the problem was that intx is a list of integers as a feature  but that still gave me the same error     also  note that initially the feature list x was a  string   but then i used neoj browser to transform the string into a list on int s which i called intx  code not shown  but i ve verified this 
713,713,8281276,64401511,Expo: Get audio data realtime and send via Socket.IO,"<h2>App I want to make</h2>
<p>I would like to make audio recognition mobile app like <a href=""https://www.shazam.com/"" rel=""nofollow noreferrer"">Shazam</a> with</p>
<ul>
<li>Expo</li>
<li>Expo AV(<a href=""https://docs.expo.io/versions/latest/sdk/audio"" rel=""nofollow noreferrer"">https://docs.expo.io/versions/latest/sdk/audio</a>)</li>
<li>Tensorflow serving</li>
<li>Socket.IO</li>
</ul>
<p>I want to send recording data to machine learning based recognition server via Socket.IO every second or every sample (Maybe it is too much to send data sample-rate times per second), and then mobile app receives and shows predicted result.</p>
<h2>Problem</h2>
<p>How to get data while recording from <code>recordingInstance</code> ? I read Expo audio document, but I couldn't figure out how to do it.</p>
<h2>So far</h2>
<p>I ran two example:</p>
<ul>
<li><a href=""https://github.com/expo/audio-recording-example"" rel=""nofollow noreferrer"">https://github.com/expo/audio-recording-example</a></li>
<li><a href=""https://github.com/expo/socket-io-example"" rel=""nofollow noreferrer"">https://github.com/expo/socket-io-example</a></li>
</ul>
<p>Now I want to mix two examples. Thank you for reading. If I could <code>console.log</code> recording data, it would help much.</p>
<h2>Related questions</h2>
<ul>
<li><p><a href=""https://forums.expo.io/t/measure-loudness-of-the-audio-in-realtime/18259"" rel=""nofollow noreferrer"">https://forums.expo.io/t/measure-loudness-of-the-audio-in-realtime/18259</a><br />
This might be impossible (to play animation? to get data realtime?)</p>
</li>
<li><p><a href=""https://forums.expo.io/t/how-to-get-the-volume-while-recording-an-audio/44100"" rel=""nofollow noreferrer"">https://forums.expo.io/t/how-to-get-the-volume-while-recording-an-audio/44100</a><br />
No answer</p>
</li>
<li><p><a href=""https://forums.expo.io/t/stream-microphone-recording/4314"" rel=""nofollow noreferrer"">https://forums.expo.io/t/stream-microphone-recording/4314</a><br />
According to this question,<br />
<a href=""https://www.npmjs.com/package/react-native-recording"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/react-native-recording</a><br />
seems to be a solution, but it requires eject.</p>
</li>
</ul>
",630,1,4,5,react-native;socket.io;expo;sound-recognition;expo-av,2020-10-17 16:08:24,2020-10-17 16:08:24,2022-05-06 00:26:53,i would like to make audio recognition mobile app like  with i want to send recording data to machine learning based recognition server via socket io every second or every sample  maybe it is too much to send data sample rate times per second   and then mobile app receives and shows predicted result  how to get data while recording from recordinginstance   i read expo audio document  but i couldn t figure out how to do it  i ran two example  now i want to mix two examples  thank you for reading  if i could console log recording data  it would help much 
714,714,4838477,52594370,Parse Grobid .tei.xml output with Beautiful Soup,"<p>I am trying to use Beautiful Soup to extract elements from a .tei.xml file that was generated using Grobid.</p>
<p>I can get title(s) using:</p>
<pre><code>titles = soup.findAll('title')
</code></pre>
<p>What is the correct syntax to access the 'lower level' elements? (Author / Affiliation etc)</p>
<p>This is a portion of the tei.xml file that is the Grobid output:</p>
<pre><code> &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
 &lt;TEI xmlns=&quot;http://www.tei-c.org/ns/1.0&quot; 
 xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; 
 xsi:schemaLocation=&quot;http://www.tei-c.org/ns/1.0 /data/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd&quot;
  xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;
     &lt;teiHeader xml:lang=&quot;en&quot;&gt;
         &lt;encodingDesc&gt;
             &lt;appInfo&gt;
                 &lt;application version=&quot;0.5.1-SNAPSHOT&quot; ident=&quot;GROBID&quot; when=&quot;2018-08-15T14:51+0000&quot;&gt;
                     &lt;ref target=&quot;https://github.com/kermitt2/grobid&quot;&gt;GROBID - A machine learning software for extracting information from scholarly documents&lt;/ref&gt;
                 &lt;/application&gt;
             &lt;/appInfo&gt;
         &lt;/encodingDesc&gt;
         &lt;fileDesc&gt;
             &lt;titleStmt&gt;
                 &lt;title level=&quot;a&quot; type=&quot;main&quot;&gt;The Role of Artificial Intelligence in Software Engineering&lt;/title&gt;
             &lt;/titleStmt&gt;
             &lt;publicationStmt&gt;
                 &lt;publisher/&gt;
                 &lt;availability status=&quot;unknown&quot;&gt;&lt;licence/&gt;&lt;/availability&gt;
             &lt;/publicationStmt&gt;
             &lt;sourceDesc&gt;
                 &lt;biblStruct&gt;
                     &lt;analytic&gt;
                         &lt;author&gt;
                             &lt;persName xmlns=&quot;http://www.tei-c.org/ns/1.0&quot;&gt;&lt;forename type=&quot;first&quot;&gt;Mark&lt;/forename&gt;&lt;surname&gt;Harman&lt;/surname&gt;&lt;/persName&gt;
                             &lt;affiliation key=&quot;aff0&quot;&gt;
                                 &lt;orgName type=&quot;department&quot;&gt;CREST Centre&lt;/orgName&gt;
                                 &lt;orgName type=&quot;institution&quot;&gt;University College London&lt;/orgName&gt;
                                 &lt;address&gt;
                                     &lt;addrLine&gt;Malet Place&lt;/addrLine&gt;
                                     &lt;postCode&gt;WC1E 6BT&lt;/postCode&gt;
                                     &lt;settlement&gt;London&lt;/settlement&gt;
                                     &lt;country key=&quot;GB&quot;&gt;UK&lt;/country&gt;
                                 &lt;/address&gt;
                             &lt;/affiliation&gt;
                         &lt;/author&gt;
                         &lt;title level=&quot;a&quot; type=&quot;main&quot;&gt;The Role of Artificial Intelligence in Software Engineering&lt;/title&gt;
                     &lt;/analytic&gt;
                     &lt;monogr&gt;
                         &lt;imprint&gt;
                             &lt;date/&gt;
                         &lt;/imprint&gt;
                     &lt;/monogr&gt;
                 &lt;/biblStruct&gt;
             &lt;/sourceDesc&gt;
         &lt;/fileDesc&gt;
</code></pre>
<p>Thanks.</p>
",547,1,3,3,python;beautifulsoup;grobid,2018-10-01 21:01:14,2018-10-01 21:01:14,2022-05-05 22:50:39,i am trying to use beautiful soup to extract elements from a  tei xml file that was generated using grobid  i can get title s  using  what is the correct syntax to access the  lower level  elements   author   affiliation etc  this is a portion of the tei xml file that is the grobid output  thanks 
715,715,18302356,71395641,Is there an implemented way to use a kubeflow pipeline&#39;s output outside the pipeline?,"<p>I'm using local kubeflow pipelines for building a continuous machine learning test project. I have one pipeline that preprocess the data using TFX, and it saves the outputs automatically to minio. Outside of this pipeline, I want to train the model using tfx's Trainer, but I need the artifacts generated in the preprocessing pipeline. Is there an implemented way to import this outputs? I've looked through the documentation and some issues, but can't find an answer. And because I'm trying to do it continuous, I can't rely on doing it manually.</p>
<p>Example of my preprocessing pipeline:</p>
<pre><code>
    @kfp.dsl.pipeline(
      name='TFX',
      description='TFX pipeline'
    )
    def tfx_pipeline():
    
        # DL with wget, can use gcs instead as well
        fetch = kfp.dsl.ContainerOp(
          name='download',
          image='busybox',
          command=['sh', '-c'],
          arguments=[
              'sleep 1;'
              'mkdir -p /tmp/data;'
              'wget &lt;gcp link&gt; -O /tmp/data/results.csv'],
          file_outputs={'downloaded': '/tmp/data'})
        records_example = tfx_csv_gen(input_base=fetch.output)
        stats = tfx_statistic_gen(input_data=records_example.output)
        schema_op = tfx_schema_gen(stats.output)
        tfx_example_validator(stats=stats.outputs['output'], schema=schema_op.outputs['output'])
        #tag::tft[]
        transformed_output = tfx_transform(
            input_data=records_example.output,
            schema=schema_op.outputs['output'],
            module_file=module_file) # Path to your TFT code on GCS/S3
        #end::tft[]

</code></pre>
<p>and then executing with</p>
<pre><code>
    kfp.compiler.Compiler().compile(tfx_pipeline, 'tfx_pipeline.zip')

</code></pre>
<pre><code>
    client = kfp.Client()
    client.list_experiments()
    #exp = client.create_experiment(name='mdupdate')

</code></pre>
<pre><code>
    my_experiment = client.create_experiment(name='tfx_pipeline')
    my_run = client.run_pipeline(my_experiment.id, 'tfx', 
      'tfx_pipeline.zip')

</code></pre>
<p>I'm working on a .ipynb in visual studio code</p>
",254,1,0,5,python;minio;kubeflow;kubeflow-pipelines;tfx,2022-03-08 18:41:15,2022-03-08 18:41:15,2022-05-05 22:20:06,i m using local kubeflow pipelines for building a continuous machine learning test project  i have one pipeline that preprocess the data using tfx  and it saves the outputs automatically to minio  outside of this pipeline  i want to train the model using tfx s trainer  but i need the artifacts generated in the preprocessing pipeline  is there an implemented way to import this outputs  i ve looked through the documentation and some issues  but can t find an answer  and because i m trying to do it continuous  i can t rely on doing it manually  example of my preprocessing pipeline  and then executing with i m working on a  ipynb in visual studio code
716,716,13426975,72130632,Load joblib file with custom metrics,"<p>I am training multiple models (on Google Colab) and saving them using joblib. I am then downloading these files and loading them using joblib, but I am getting the following error;</p>
<p>Unable to restore custom object of type _tf_keras_metric</p>
<p>I am compiling the models using metrics=['accuracy', specificity, tf.keras.metrics.Precision()], where specificity is a custom metric function from <a href=""https://www.sabinasz.net/unbalanced-classes-machine-learning/"" rel=""nofollow noreferrer"">https://www.sabinasz.net/unbalanced-classes-machine-learning/</a>.</p>
<pre><code>mobilenet_model = MobileNetV3Small(include_top=True, weights=None, classes=2, input_shape=(100, 100, 3))
mobilenet_model.compile(loss='binary_crossentropy', metrics=['accuracy', specificity, tf.keras.metrics.Precision()])
trained_model  = mobilenet_model.fit(X_train, y_train)

joblib.dump(trained_model, 'mobilenet_model.joblib')
</code></pre>
<p>I tried to load them using keras.models.load_model but I get a Error opening file (File signature not found) which according to <a href=""https://stackoverflow.com/questions/38089950/error-opening-file-in-h5py-file-signature-not-found"">Error opening file in H5PY (File signature not found)</a> means the file is corrupt (I imagine that the way joblib saves models is not compatible with keras's save_model function).</p>
<p>I tried using keras.models.save_model to save a file and passing in custom_objects={'specificity': specificity, 'precision': tf.keras.metrics.Precision()}, but the model seems to be saved without the pretrained weights because when I evaluate it, it only has 0.5 accuracy, when it was getting 0.9 when I trained it.</p>
<p>So what can I do?</p>
",17,0,0,3,python;keras;joblib,2022-05-05 21:59:55,2022-05-05 21:59:55,2022-05-05 21:59:55,i am training multiple models  on google colab  and saving them using joblib  i am then downloading these files and loading them using joblib  but i am getting the following error  unable to restore custom object of type _tf_keras_metric i am compiling the models using metrics   accuracy   specificity  tf keras metrics precision     where specificity is a custom metric function from   i tried to load them using keras models load_model but i get a error opening file  file signature not found  which according to  means the file is corrupt  i imagine that the way joblib saves models is not compatible with keras s save_model function   i tried using keras models save_model to save a file and passing in custom_objects   specificity   specificity   precision   tf keras metrics precision     but the model seems to be saved without the pretrained weights because when i evaluate it  it only has   accuracy  when it was getting   when i trained it  so what can i do 
717,717,9264308,72129065,What is the simplest way to apply a moving average filter to time series sensor data?,"<p>I have this attached time series signal (its actually from an electrostatic sensor, every time someone walks or moves, I can see that in the signal).</p>
<p>Before the machine learning part, I would like to apply a moving average to the sensor data then store only the signal when someone is walking, so when the signal passes a certain threshold for example.</p>
<p>How do I get to do that in C language?
<a href=""https://i.stack.imgur.com/z2j9S.png"" rel=""nofollow noreferrer"">1</a></p>
",20,0,0,3,time-series;sensors;digital-filter,2022-05-05 20:09:16,2022-05-05 20:09:16,2022-05-05 20:09:16,i have this attached time series signal  its actually from an electrostatic sensor  every time someone walks or moves  i can see that in the signal   before the machine learning part  i would like to apply a moving average to the sensor data then store only the signal when someone is walking  so when the signal passes a certain threshold for example 
718,718,19041380,72124626,Calculating training and testing accuracy of LSTM,"<p>I am building an LSTM model with the following code and I wish to calculate the training and testing accuracies of the model. I am a novice in machine learning and the only method I know for calculating the accuracy is using sklearn's &quot;accuracy score&quot;.</p>
<pre><code>y_train = pd.Series(y_train)
lstm_model = Sequential()
lstm_model.add(Embedding(top_words, 32, input_length=req_length))
lstm_model.add(Flatten())
input = (req_length, 32)
lstm_model.add(Reshape(input))
lstm_model.add(LSTM(units = 50, return_sequences = True))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(256, activation='relu'))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', 
metrics=['accuracy'])
lstm = lstm_model.fit(X_train, y_train, epochs = 30, batch_size = 10)
</code></pre>
<p>To calculate y_pred, I wrote it as <code>y_pred = lstm_model.predict(y_test)</code>. However, the accuracy score function on y_pred as its shape is <code>(600, 401, 1)</code>.</p>
<p>What can I do regarding this or share some code?</p>
",73,1,0,5,python;tensorflow;keras;deep-learning;lstm,2022-05-05 14:35:52,2022-05-05 14:35:52,2022-05-05 19:54:20,i am building an lstm model with the following code and i wish to calculate the training and testing accuracies of the model  i am a novice in machine learning and the only method i know for calculating the accuracy is using sklearn s  accuracy score   to calculate y_pred  i wrote it as y_pred   lstm_model predict y_test   however  the accuracy score function on y_pred as its shape is         what can i do regarding this or share some code 
719,719,3838981,37489369,"Selenium Hub, Launches 2 browser only runs test in 1 of them (selenium python)","<p>TL/DR: Right now it launches 2 browsers but only runs the test in 1. What am I missing?</p>

<p>So I'm trying to get selenium hub working on a mac (OS X 10.11.5). I installed with <a href=""https://www.npmjs.com/package/selenium-standalone"" rel=""nofollow noreferrer"">this</a>, then launch hub in a terminal tab with:</p>

<pre><code>selenium-standalone start -- -role hub
</code></pre>

<p>Then in another tab of terminal on same machine register a node.</p>

<pre><code>selenium-standalone start -- -role node -hub http://localhost:4444/grid/register -port 5556
</code></pre>

<p>It shows up in <a href=""http://localhost:4444/grid/console"" rel=""nofollow noreferrer"">console</a> with 5 available firefox and chrome browsers.
<a href=""https://i.stack.imgur.com/kBpKh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kBpKh.png"" alt=""enter image description here""></a></p>

<p>So here's my code. In a file named globes.py I have this.</p>

<pre><code>class globes:
    def __init__(self, number):
        self.number = number

    base_url = ""https://fake-example.com""

    desired_cap = []
    desired_cap.append ({'browserName':'chrome', 'javascriptEnabled':'true', 'version':'', 'platform':'ANY'})
    desired_cap.append ({'browserName':'firefox', 'javascriptEnabled':'true', 'version':'', 'platform':'ANY'})
    selenium_server_url = 'http://127.0.0.1:4444/wd/hub'
</code></pre>

<p>Right now I'm just trying to run a single test that looks like this.</p>

<pre><code>import unittest
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
from selenium.common.exceptions import NoSuchElementException
from globes import *

class HeroCarousel(unittest.TestCase):

    def setUp(self):
        for driver_instance in globes.desired_cap:
            self.driver = webdriver.Remote(
                command_executor=globes.selenium_server_url,
                desired_capabilities=driver_instance)

        self.verificationErrors = []

    def test_hero_carousel(self):
        driver = self.driver
        driver.get(globes.base_url)

        hero_carousel = driver.find_element(By.CSS_SELECTOR, 'div.carousel-featured')
        try: self.assertTrue(hero_carousel.is_displayed())
        except AssertionError, e: self.verificationErrors.append(""home_test: Hero Carousel was not visible"")

    def tearDown(self):
        self.driver.close()
        self.assertEqual([], self.verificationErrors)


if __name__ == ""__main__"":
    unittest.main()
</code></pre>

<p>Right now it launches both Firefox and Chrome, but only runs the test in Firefox. Chrome opens and just sits on a blank page and doesn't close. So I figure there's something wrong with how I wrote the test. So what am I missing? I apologize if this is obvious but I'm just learning how to setup hub and just learned enough python to write selenium tests a couple weeks ago.</p>

<p>I think Hubs working as it launches both, but I did try adding a second node on the same machine on a different port and got the same thing. Just in case here's what hub prints out.</p>

<pre><code>INFO - Got a request to create a new session: Capabilities [{browserName=chrome, javascriptEnabled=true, version=, platform=ANY}]
INFO - Available nodes: [http://192.168.2.1:5557]
INFO - Trying to create a new session on node http://192.168.2.1:5557
INFO - Trying to create a new session on test slot {seleniumProtocol=WebDriver, browserName=chrome, maxInstances=5, platform=MAC}
INFO - Got a request to create a new session: Capabilities [{browserName=firefox, javascriptEnabled=true, version=, platform=ANY}]
INFO - Available nodes: [http://192.168.2.1:5557]
INFO - Trying to create a new session on node http://192.168.2.1:5557
INFO - Trying to create a new session on test slot {seleniumProtocol=WebDriver, browserName=firefox, maxInstances=5, platform=MAC}
</code></pre>
",575,2,0,4,python;selenium;automated-tests;selenium-grid,2016-05-27 23:06:03,2016-05-27 23:06:03,2022-05-05 18:23:58,tl dr  right now it launches  browsers but only runs the test in   what am i missing  so i m trying to get selenium hub working on a mac  os x      i installed with   then launch hub in a terminal tab with  then in another tab of terminal on same machine register a node  so here s my code  in a file named globes py i have this  right now i m just trying to run a single test that looks like this  right now it launches both firefox and chrome  but only runs the test in firefox  chrome opens and just sits on a blank page and doesn t close  so i figure there s something wrong with how i wrote the test  so what am i missing  i apologize if this is obvious but i m just learning how to setup hub and just learned enough python to write selenium tests a couple weeks ago  i think hubs working as it launches both  but i did try adding a second node on the same machine on a different port and got the same thing  just in case here s what hub prints out 
720,720,11696788,72126496,How to use lapply on calculating MCC for two datasets in R?,"<p>I have two lists of data frames and I want to sort the data frames and obtain their relationship based on MCC. How can I go through using apply?</p>
<p>The formula for MCC or sometimes is called the phi coefficient (or mean square contingency) coefficient). . In machine learning, it is known as the Matthews correlation coefficient (MCC) and is used as a measure of the quality of binary (two-class) classifications. I want to measure the relationship between the first list of data frames corresponding the second list of data frames.</p>
<p>Its implementation through R is easy but how can go through based on the lists I have</p>
<p>The first data frame is</p>
<pre><code>[[1]]
         1
china    1
disput   1
island   2
maritim  1
paracel  1
seafocus 1
south    1
sprat    1
studi    1

[[2]]
           1
age        1
asian      1
industri   1
inscript   1
languag    1
methodolog 1
northeast  1
region     1
scope      1
sea        1
studi      1

[[3]]
          1
conflict  1
focus     1
japanes   1
korea     1
liber     1
north     1
pyongyang 1
realiti   1
repatri   1
resid     1
studi     1
</code></pre>
<p>The other data frame is</p>
<pre><code>[[1]]
         1
china    1
disput   1
island   2
maritim  1
paracel  1
seafocus 1
south    1
sprat    1
studi    1

[[2]]
           1
age        1
asian      1
industri   1
inscript   1
languag    1
methodolog 1
northeast  1
region     1
scope      1
sea        1
studi      1

[[3]]
          1
conflict  1
focus     1
japanes   1
korea     1
liber     1
north     1
pyongyang 1
realiti   1
repatri   1
resid     1
studi     1
</code></pre>
",13,0,0,4,r;apply;lapply;sapply,2022-05-05 17:04:56,2022-05-05 17:04:56,2022-05-05 17:04:56,i have two lists of data frames and i want to sort the data frames and obtain their relationship based on mcc  how can i go through using apply  the formula for mcc or sometimes is called the phi coefficient  or mean square contingency  coefficient     in machine learning  it is known as the matthews correlation coefficient  mcc  and is used as a measure of the quality of binary  two class  classifications  i want to measure the relationship between the first list of data frames corresponding the second list of data frames  its implementation through r is easy but how can go through based on the lists i have the first data frame is the other data frame is
721,721,5173325,72125187,Can I customise git autocomplete?,"<p>I want to write <code>git chec</code>, hit tab and have git autocomplete actually autocomplete it to <code>git checkout</code>. Now when I do this I get a list of options, and I have to hit tab four times to get to <code>git checkout</code>, which basically takes more time than to just write the whole thing manually.</p>
<p>Is it possible to tell git autocomplete to just always autocomplete certain input to certain commands?</p>
<p>Note: I am 99% sure it autocompleted to <code>git checkout</code> when I did this on my old mac. Does git autocomplete use machine learning to evolve? Or is my muscle memory just wrong?</p>
",23,0,0,2,git;autocomplete,2022-05-05 15:21:16,2022-05-05 15:21:16,2022-05-05 15:21:16,i want to write git chec  hit tab and have git autocomplete actually autocomplete it to git checkout  now when i do this i get a list of options  and i have to hit tab four times to get to git checkout  which basically takes more time than to just write the whole thing manually  is it possible to tell git autocomplete to just always autocomplete certain input to certain commands  note  i am   sure it autocompleted to git checkout when i did this on my old mac  does git autocomplete use machine learning to evolve  or is my muscle memory just wrong 
722,722,8263951,71578997,loading tensorflow dataset gives NonMatchingChecksumError,"<p>My goal is to use the following dataset from tensorflow-datasets for Machine Learning</p>
<p><a href=""https://www.tensorflow.org/datasets/catalog/wider_face"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/wider_face</a></p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_datasets as tfds

data, info = tfds.load(name='wider_face',as_supervised=True, with_info =True)
</code></pre>
<p>leading to the following error:</p>
<pre><code>NonMatchingChecksumError: Artifact https://drive.google.com/uc?export=download&amp;id=1HIfDbVEWKmsYKJZm4lchTBDLW5N7dY5T, downloaded to C:\Users\user\tensorflow_datasets\downloads\ucexport_download_id_1HIfDbVEWKmsYKJZm4lchTBDLfr62-cNGXcnoWarcPOgb67igMZT4ssm73xhXi-__9lo.tmp.830989d7c2724803b592ac9c747a8300\uc, has wrong checksum:
* Expected: UrlInfo(size=1.72 GiB, checksum='3b0313e11ea292ec58894b47ac4c0503b230e12540330845d70a7798241f88d3', filename='WIDER_test.zip')
* Got: UrlInfo(size=2.15 KiB, checksum='3fe166f3882f9f3b1fb287ca88ec2d39655b74381d54c78b993377663c0f5bb3', filename='uc')
</code></pre>
<p>Does anyone have an idea how to solve this problem? <code>tensorflow_datasets</code> is on the latest version <code>4.52</code>.</p>
",183,1,0,4,python;tensorflow;tensorflow-datasets;checksum,2022-03-23 02:41:22,2022-03-23 02:41:22,2022-05-05 12:51:31,my goal is to use the following dataset from tensorflow datasets for machine learning  leading to the following error  does anyone have an idea how to solve this problem  tensorflow_datasets is on the latest version   
723,723,19033923,72114227,to one-hot encode parts of my string data,"<blockquote>
<p>ACN58946.1|FEATURES|NCBI|macrolide-lincosamide-streptogramin|macB|antibiotic target protection|0\sMAEPVLSVKDLDIRFTTPDGNVHAVKKVSFDIAPGECLGVVGESGSGKSQLFMACIGLLAGNGKATGSVTYRGQELLGQPAAKLNAIRGAKITMIFQDPLTSLTPHMRIGDQIVESLRTHSKLSKGEAEKRAIQALELVRIPEAKRRMRQYPHELSGGMRQRVMIAMATACGPDLLIADEPTTALDVTVQAQILDIMRDLRKELGTSIALISHDMGVIASICDRVQVMRYGEFVETGPADDIFYHPQHPYTRMLLEAMPRIDQPVREGRAALKPLAPQEARTTLLEVDNVKVHFPIQMGGVFFGKYKPLRAVDGVSFTLHQGETIGIVGESGCGKSTLARAVLELLPKTTGGVVWMGRDLGALPPAELRRARKDFQIVFQDPLASLDPRMTIGQSIAEPLQSLEPELSKHEVQSRVRAIMEKVGLDPDWINRYPHEFSGGQNQRVGIARAMILKPKLIVCDEAVSALDVSIQAQIVDLILSLQAEFGMSIIFISHDLSVVRQVSHRVMVLYLGRVVELASRDAIYEDARHPYTKALISAVTVPDPRAERLKKRRELPGELPSPLDTRSALMFLKSKRIDDPDAEQYVPKLIEVAPGHFVAEHDPFEVVEMTG\e&gt;ACN58871.1|FEATURES|NCBI|macrolide-lincosamide-streptogramin|macB|antibiotic efflux|0\sMADYLLEMKNIVKEFGGVRALNGIDIKLKAGECAGLCGENGAGKSTLMKVLSAVYPHGTWDGEILWDGKPLRAQSIRETEAAGIVIIHQELMLVPELSVAENIFLGNEIKLPGGRMDYAAMNRRAEELLAELDIRDVNVVLPVKQYGGGYQQLIEIAKALNKNARLLILDEPSSSLTASEIKVLLRIIHSLKAKGVTCVYISHKLDEVADICDTIVVIRDGQHIATTPMADMNIERIIAQMVGREMNQLYPERSHVPGEVIFEARNVSCYDADNPQRKRVDNISFKLRKGEILGIAGLVGAGRTELVSALFGAYPGPSEAEVWLNGVKLDTRTPLKAIRAGLAMVPEDRKQHGIVPDLGVGHNMTLAVLNDFVRATRIDQQAELATIHKEIKSVKLKTATPFLPITSLSGGNQQKAVLSKMLLTKPKILILDEPTRGVDVGAKFEIYQLMFDLAAQGMSIIMVSSELAEVLGISDRVLVVGEGKLRGDFVNDNLSQETVLAAALDHTQPALH\e&gt;ACN58991.1|FEATURES|NCBI|multidrug|cmeB|antibiotic efflux|0\sMKNDRGEMVPFSAFMTIKKKQGANEINRYNMYNTAAIRGGPATGYSSGEAIKAVQEVAAKNLPNGFDIDWAALSYDETRRGNEAVYIFLIVLAFVYLVLAAQYESFIIPLAVVFSLPAGVFGSFLLIKGMGLANDIYAQVGLVMLVGLLGKNAVLIVEFAVQKQQQGATVFEAAIEGARVRFRPILMTSFAFIAGLIPLVFAHGAGAIGNKTIGSSALGGMFFGTVFGVIVVPGLYYVFGSWAEGRKLIRGEDHDPLTENLVHQMDNFPQSDDK\e&gt;ACN58776.1|FEATURES|NCBI|macrolide-lincosamide-streptogramin|macA|antibiotic efflux|0\sMGNLPRPTLSPSLSGIRPTMNRETTTRVDSSTPAARLGMRVPSTSRAALVGVAALVVILGGWYGIKRWRAHVASEGQYIFAAIQKGDIEDLVTATGSLQPRDYVDVGAQVSGQLDKILVEVGSDVKEGDLLAEIDADVAAARVDASRAQLRSQQAQLVQQQANLTKAERDLTRQQNLMKEDATTAEQVQNAETTLDTTKAQINALKAQMEQLRASMRVDESNLNYTKILAPMSGTVVSISAKQGQTLNTNQQAPTILRIADLSTMTVQTQVSEADVSKLRSGMQAYFTTLGSAGKRWYGQLKKIEPTPTVTNNVVLYNALFEVPNDNKQLLPQMTAQVFFVAAAAHDVLVVPMSAVSLQRTPPGGIPNAAAAQAAGARGAGAQGAGAQGAQGASAQGAGAQSGQGGQGAAALTPEQIARREARRQQRMQSNGGSATGGAIEGGPPRGGFGASMAARGPRHATVRVQAADGKIEERQITIGVTNRVHAEVLSGLKEGERVVAGTKEPEKAPATAGGQQGAGGQRNNIGGFPGGGLGGGFGR\e</p>
</blockquote>
<p>I am working with protein sequences right now. I have these string data which parts I want to convert to one-hot encode. The parts that I want to convert starts after '\s' and end before '\e', then I want to do it for the whole string data. Since the I have thousands of datasets, using pure Python code seems impossible, since it will take a long time to finish the process. Is there any machine learning library for this problem?</p>
<p>Thank you for your help in advance!</p>
",26,2,0,3,string;bioinformatics;one-hot-encoding,2022-05-04 19:26:41,2022-05-04 19:26:41,2022-05-05 07:14:16,acn  features ncbi macrolide lincosamide streptogramin macb antibiotic target protection  smaepvlsvkdldirfttpdgnvhavkkvsfdiapgeclgvvgesgsgksqlfmacigllagngkatgsvtyrgqellgqpaaklnairgakitmifqdpltsltphmrigdqiveslrthsklskgeaekraiqalelvripeakrrmrqyphelsggmrqrvmiamatacgpdlliadepttaldvtvqaqildimrdlrkelgtsialishdmgviasicdrvqvmrygefvetgpaddifyhpqhpytrmlleampridqpvregraalkplapqearttllevdnvkvhfpiqmggvffgkykplravdgvsftlhqgetigivgesgcgkstlaravlellpkttggvvwmgrdlgalppaelrrarkdfqivfqdplasldprmtigqsiaeplqslepelskhevqsrvraimekvgldpdwinryphefsggqnqrvgiaramilkpklivcdeavsaldvsiqaqivdlilslqaefgmsiifishdlsvvrqvshrvmvlylgrvvelasrdaiyedarhpytkalisavtvpdpraerlkkrrelpgelpspldtrsalmflkskriddpdaeqyvpklievapghfvaehdpfevvemtg e gt acn  features ncbi macrolide lincosamide streptogramin macb antibiotic efflux  smadyllemknivkefggvralngidiklkagecaglcgengagkstlmkvlsavyphgtwdgeilwdgkplraqsireteaagiviihqelmlvpelsvaeniflgneiklpggrmdyaamnrraeellaeldirdvnvvlpvkqygggyqqlieiakalnknarllildepsssltaseikvllriihslkakgvtcvyishkldevadicdtivvirdgqhiattpmadmnieriiaqmvgremnqlypershvpgevifearnvscydadnpqrkrvdnisfklrkgeilgiaglvgagrtelvsalfgaypgpseaevwlngvkldtrtplkairaglamvpedrkqhgivpdlgvghnmtlavlndfvratridqqaelatihkeiksvklktatpflpitslsggnqqkavlskmlltkpkilildeptrgvdvgakfeiyqlmfdlaaqgmsiimvsselaevlgisdrvlvvgegklrgdfvndnlsqetvlaaaldhtqpalh e gt acn  features ncbi multidrug cmeb antibiotic efflux  smkndrgemvpfsafmtikkkqganeinrynmyntaairggpatgyssgeaikavqevaaknlpngfdidwaalsydetrrgneavyiflivlafvylvlaaqyesfiiplavvfslpagvfgsfllikgmglandiyaqvglvmlvgllgknavlivefavqkqqqgatvfeaaiegarvrfrpilmtsfafiagliplvfahgagaignktigssalggmffgtvfgvivvpglyyvfgswaegrklirgedhdpltenlvhqmdnfpqsddk e gt acn  features ncbi macrolide lincosamide streptogramin maca antibiotic efflux  smgnlprptlspslsgirptmnretttrvdsstpaarlgmrvpstsraalvgvaalvvilggwygikrwrahvasegqyifaaiqkgdiedlvtatgslqprdyvdvgaqvsgqldkilvevgsdvkegdllaeidadvaaarvdasraqlrsqqaqlvqqqanltkaerdltrqqnlmkedattaeqvqnaettldttkaqinalkaqmeqlrasmrvdesnlnytkilapmsgtvvsisakqgqtlntnqqaptilriadlstmtvqtqvseadvsklrsgmqayfttlgsagkrwygqlkkieptptvtnnvvlynalfevpndnkqllpqmtaqvffvaaaahdvlvvpmsavslqrtppggipnaaaaqaagargagaqgagaqgaqgasaqgagaqsgqggqgaaaltpeqiarrearrqqrmqsnggsatggaieggpprggfgasmaargprhatvrvqaadgkieerqitigvtnrvhaevlsglkegervvagtkepekapataggqqgaggqrnniggfpggglgggfgr e i am working with protein sequences right now  i have these string data which parts i want to convert to one hot encode  the parts that i want to convert starts after   s  and end before   e   then i want to do it for the whole string data  since the i have thousands of datasets  using pure python code seems impossible  since it will take a long time to finish the process  is there any machine learning library for this problem  thank you for your help in advance 
724,724,14252319,72109507,"TypeError at / sequence item 0: expected str instance, _SpecialGenericAlias found in Django templates?","<p>I am working on an application that can parse resume content and summarize it. This application is taking input in pdf format and when I upload the pdf file of resume it is showing this error on rendering the template. I am adding a code snippet for my template folder in which it is showing this error.</p>
<blockquote>
<p>Error during template rendering In template D:\all projects and
programs\Machine Learning
Projects\resume1\resume_Shortlisting\Application\ResumeParser\resume_parser\parser_app\templates\messages.html,
error at line 4</p>
</blockquote>
<pre><code> {% if messages %}
 &lt;div class=&quot;messages&quot; style=&quot;margin-top: 2%;&quot;&gt;
    {% for message in messages %}
    &lt;div {% if message.tags == 'error' %} class=&quot;alert alert-danger&quot; {% else %} class=&quot;alert alert-{{ message.tags }} &quot;{% endif %}&gt;
        {% if message.level == DEFAULT_MESSAGE_LEVELS.ERROR %}Important: {% endif %}
        {{ message }}
    &lt;/div&gt;
    {% endfor %}
&lt;/div&gt;
{% endif %}
</code></pre>
",23,0,0,5,python;html;django;machine-learning;nlp,2022-05-04 13:18:19,2022-05-04 13:18:19,2022-05-04 13:27:17,i am working on an application that can parse resume content and summarize it  this application is taking input in pdf format and when i upload the pdf file of resume it is showing this error on rendering the template  i am adding a code snippet for my template folder in which it is showing this error 
725,725,13413248,72106880,How to determine the cause of not achieving a target in using machine learning model,"<p>Please I want to know if it is possible to know the specific variables' influence in testing a sample data to a model. The model below clarifies the question;</p>
<p>Given a dataset to predict the score of students.</p>
<pre><code>ID  Studies hours   Games hours lectures hours  social Activities   Score
0   1   20  5   15  2   78
1   2   15  6   13  3   69
2   3   31  2   16  1   95
3   4   22  2   15  2   80
4   5   19  7   15  4   71
5   6   10  8   10  8   52
6   7   13  7   11  6   59
7   8   34  1   16  1   96
8   9   25  6   15  1   83
9   10  22  3   16  2   76
10  11  17  7   15  1   66
11  12  28  2   14  2   87
12  13  21  3   16  3   77
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from numpy import absolute
from xgboost import XGBModel
import pickle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
%matplotlib inline
from xgboost import plot_importance 

data = pd.read_csv(&quot;student_score.csv&quot;)

def perfomance(data):
    X = data.iloc[:,:-1]
    y = data.iloc[:,-1:]
    model = XGBModel(booster='gbtree')
    #model = XGBModel(booster='gblinear')
    model.fit(X, y)
    cv = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)
    # evaluate model
    scores = cross_val_score(model, X,y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)
    # force scores to be positive
    scores = np.absolute(scores)
    metrics = ('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )
    # save the model to disk
    filename = 'score.sav'
    pickle.dump(model, open(filename, 'wb'))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
    # load the model from disk
    loaded_model = pickle.load(open('score.sav', 'rb'))
    result = loaded_model.predict(X_test)
    print(result)
    plt.rcParams[&quot;figure.figsize&quot;] = (20,15)
    plot_importance(model)
    plt.show()

</code></pre>
<p>Feature Importances :</p>
<pre><code>[5.6058721e-04 6.7560148e-01 3.1960118e-01 4.2312010e-03 5.4962843e-06] 
</code></pre>
<p>The feature importance is the general importance ranked by the model.</p>
<p>What I need now is:</p>
<p>when I pick A sample test say <code>test = pd.DataFrame([{&quot;Studies hours&quot;:15, &quot;Games hours&quot;:6, &quot;lectures hours&quot;:13,&quot;social Activities&quot;:3}])</code>
and predict; <code>loaded_model.predict(test)</code> and I get a score like 68, Which of the variables specifically (not the general importance) didn't make this specific sample test not score 100 but rather 68?
For Example, the model should tell me studies hours were bad or were less than expected.</p>
<p>Can Machine Learning Model do that?</p>
",51,2,-1,4,python;pandas;machine-learning;scikit-learn,2022-05-04 06:23:56,2022-05-04 06:23:56,2022-05-04 13:18:32,please i want to know if it is possible to know the specific variables  influence in testing a sample data to a model  the model below clarifies the question  given a dataset to predict the score of students  feature importances   the feature importance is the general importance ranked by the model  what i need now is  can machine learning model do that 
726,726,7648650,58203472,Difference between Shapley values and SHAP for interpretable machine learning,"<p><a href=""https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf"" rel=""nofollow noreferrer"">The Paper</a> regarding die <code>shap</code> package gives a formula for the Shapley Values in (4) and for SHAP values apparently in (8)</p>
<p>Still I don't really understand the difference between Shapley and SHAP values. As far as I understand for Shapley I need to retrain my Model on each possible subset of parameters and for SHAP I am just using the basic model trained on all parameters. Is that it? So SHAP is computationally easier?</p>
",2793,2,7,2,machine-learning;shap,2019-10-02 19:51:41,2019-10-02 19:51:41,2022-05-04 12:30:47, regarding die shap package gives a formula for the shapley values in    and for shap values apparently in    still i don t really understand the difference between shapley and shap values  as far as i understand for shapley i need to retrain my model on each possible subset of parameters and for shap i am just using the basic model trained on all parameters  is that it  so shap is computationally easier 
727,727,2551431,72056363,Trying to understand the Hutchinson diagonal Hessian approximation,"<p>I am reading about his paper [1] and I have an implementation taken from <a href=""https://github.com/davda54/ada-hessian/blob/master/ada_hessian.py"" rel=""nofollow noreferrer"">here</a>. At some point of the code the diagonal of the Hessian matrix is approximated by a function <code>set_hessian</code> you can find below. In the end of <code>set_hessian()</code>, it is mentioned that <code># approximate the expected values of z*(H@z)</code>. However, when I print <code>p.hess</code> I get</p>
<pre><code>tensor([[[[ 2.3836e+01,  1.4929e+01,  4.1799e+00],
          [-1.6726e+01,  6.3954e+00, -5.1418e+00],
          [ 2.2580e+01, -1.1916e+01, -2.5049e+00]],
         [[-1.8261e+01,  8.7626e+00,  1.8244e+00],
          [-1.0819e+01, -2.9184e-01,  1.1601e+01],
          [-1.6267e+01,  5.6232e+00,  3.4282e+00]],
         ....
         [[-3.1088e+01,  4.3013e+01, -4.2021e+01],
          [ 1.5338e+01, -2.9806e+01, -3.0049e+01],
          [-9.8979e+00, -2.2835e+00, -6.0549e+00]]]], device='cuda:0')
</code></pre>
<p>How <code>p.hess</code> is considered a diagonal approximation of the Hessian? The reason I am trying to understand this structure is because I want get the smallest eigenvalue, the inverse of the diagonal matrix, and the product between the Hessian and the gradient which is a vector. We know that the smallest eigenvalue of a diagonal matrix is the smallest element of the diagonal, while the inverse of a diagonal matrix can be computed by inverting the elements of the diagonal. Could you please someone cast some light on the structure of <code>p.hess</code>?</p>
<pre><code>    @torch.no_grad()
    def set_hessian(self):
        &quot;&quot;&quot;
        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.
        &quot;&quot;&quot;

        params = []
        for p in filter(lambda p: p.grad is not None, self.get_params()):
            if self.state[p][&quot;hessian step&quot;] % self.update_each == 0:  # compute the trace only each `update_each` step
                params.append(p)
            self.state[p][&quot;hessian step&quot;] += 1

        if len(params) == 0:
            return

        if self.generator.device != params[0].device:  # hackish way of casting the generator to the right device
            self.generator = torch.Generator(params[0].device).manual_seed(2147483647)

        grads = [p.grad for p in params]

        for i in range(self.n_samples):
            zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device,
                                dtype=torch.float32) * 2.0 - 1.0 for p in params]  # Rademacher distribution {-1.0, 1.0}
            h_zs = torch.autograd.grad(grads, params, grad_outputs=zs, only_inputs=True,
                                       retain_graph=i &lt; self.n_samples - 1)
            for h_z, z, p in zip(h_zs, zs, params):
                p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)
</code></pre>
<p>[1] ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning</p>
",61,1,0,3,pytorch;tensor;hessian-matrix,2022-04-29 15:44:45,2022-04-29 15:44:45,2022-05-04 10:01:26,i am reading about his paper    and i have an implementation taken from   at some point of the code the diagonal of the hessian matrix is approximated by a function set_hessian you can find below  in the end of set_hessian    it is mentioned that   approximate the expected values of z  h z   however  when i print p hess i get how p hess is considered a diagonal approximation of the hessian  the reason i am trying to understand this structure is because i want get the smallest eigenvalue  the inverse of the diagonal matrix  and the product between the hessian and the gradient which is a vector  we know that the smallest eigenvalue of a diagonal matrix is the smallest element of the diagonal  while the inverse of a diagonal matrix can be computed by inverting the elements of the diagonal  could you please someone cast some light on the structure of p hess     adahessian  an adaptive second order optimizer for machine learning
728,728,6442390,72021549,"Keras Dense Model ValueError: logits and labels must have the same shape ((None, 200, 1) vs (None, 1, 1))","<p>I'm new in machine learning and I'm trying to train a model.
I'm using this Keras oficial example as a guide to set my dataset and feed it into the model: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence</a></p>
<p>From the training data I have an sliding windows created for a single column and for the labels I have a binary classification (1 or 0).</p>
<p>This is the model creation code:</p>
<pre><code>n = 200
hidden_units = n
dense_model = Sequential()
dense_model.add(Dense(hidden_units, input_shape=([200,1])))
dense_model.add(Activation('relu'))
dense_model.add(Dropout(dropout))
print(hidden_units)

while hidden_units &gt; 2:
    hidden_units = math.ceil(hidden_units/2)
    dense_model.add(Dense(hidden_units))
    dense_model.add(Activation('relu'))
    dense_model.add(Dropout(dropout))
    print(hidden_units)
dense_model.add(Dense(units = 1, activation='sigmoid'))
</code></pre>
<p>This is the functions I'm using to compile the model:</p>
<pre><code>def compile_and_fit(model, window, epochs, patience=2):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                      patience=patience,
                                                      mode='min')
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    model.fit(window.train , epochs=epochs)
    return history
</code></pre>
<p>This is the model training:</p>
<pre><code>break_batchs = find_gaps(df_train, 'date_diff', diff_int_value)
for keys, values in break_batchs.items():
    dense_window = WindowGenerator(data=df_train['price_var'],
                                   data_validation=df_validation['price_var'],
                                   data_test=df_test['price_var'],
                                   input_width=n,
                                   shift=m,
                                   start_index=values[0],
                                   end_index=values[1], 
                                   class_labels=y_buy,
                                   class_labels_train=y_buy_train,
                                   class_labels_test=y_buy_test,
                                   label_width=1,
                                   label_columns=None,
                                   classification=True,
                                   batch_size=batch_size,
                                   seed=None)
    history = compile_and_fit(dense_model, dense_window)

</code></pre>
<p>and those are the shapes of the batches:</p>
<pre><code>(TensorSpec(shape=(None, 200, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 1), dtype=tf.float64, name=None))
</code></pre>
<p>The problem is (I guess) that, from the model summary the model is training from the last dimension when it should be working in the second one:</p>
<pre><code>dense_model.summary()

Model: &quot;sequential_21&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
                                          |Model is being applied here
                                          |
                                          v
dense_232 (Dense)            (None, 200, 200)          400       
_________________________________________________________________
                                     |When it should be applied here
                                     |
                                     v
activation_225 (Activation)  (None, 200, 200)          0         
_________________________________________________________________
dropout_211 (Dropout)        (None, 200, 200)          0         
_________________________________________________________________
dense_233 (Dense)            (None, 200, 100)          20100     
_________________________________________________________________
activation_226 (Activation)  (None, 200, 100)          0         
_________________________________________________________________
dropout_212 (Dropout)        (None, 200, 100)          0         
_________________________________________________________________
dense_234 (Dense)            (None, 200, 50)           5050      
_________________________________________________________________
activation_227 (Activation)  (None, 200, 50)           0         
_________________________________________________________________
dropout_213 (Dropout)        (None, 200, 50)           0         
_________________________________________________________________
dense_235 (Dense)            (None, 200, 25)           1275      
_________________________________________________________________
activation_228 (Activation)  (None, 200, 25)           0         
_________________________________________________________________
dropout_214 (Dropout)        (None, 200, 25)           0         
_________________________________________________________________
dense_236 (Dense)            (None, 200, 13)           338       
_________________________________________________________________
activation_229 (Activation)  (None, 200, 13)           0         
_________________________________________________________________
dropout_215 (Dropout)        (None, 200, 13)           0         
_________________________________________________________________
dense_237 (Dense)            (None, 200, 7)            98        
_________________________________________________________________
activation_230 (Activation)  (None, 200, 7)            0         
_________________________________________________________________
dropout_216 (Dropout)        (None, 200, 7)            0         
_________________________________________________________________
dense_238 (Dense)            (None, 200, 4)            32        
_________________________________________________________________
activation_231 (Activation)  (None, 200, 4)            0         
_________________________________________________________________
dropout_217 (Dropout)        (None, 200, 4)            0         
_________________________________________________________________
dense_239 (Dense)            (None, 200, 2)            10        
_________________________________________________________________
activation_232 (Activation)  (None, 200, 2)            0         
_________________________________________________________________
dropout_218 (Dropout)        (None, 200, 2)            0         
_________________________________________________________________
dense_240 (Dense)            (None, 200, 1)            3         
=================================================================
Total params: 27,306
Trainable params: 27,306
Non-trainable params: 0
_________________________________________________________________


</code></pre>
<p>And because of that Im getting <code>ValueError: logits and labels must have the same shape ((None, 200, 1) vs (None, 1, 1))</code></p>
<p>How can I tell Keras to apply the training in the second dimension and not the last one?</p>
<h1>EDIT</h1>
<p><a href=""https://i.stack.imgur.com/66wlN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/66wlN.png"" alt=""Diagram explanation"" /></a></p>
<p>This is what I understand is happening, is this right? How I fixed it?</p>
<h1>Edit 2</h1>
<p>I tried to modify as suggested, using:</p>
<p><code>dense_model.add(Dense(hidden_units, input_shape=(None,200,1)))</code></p>
<p>but I'm getting the following warning:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, None, 200, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 200, 1), dtype=tf.float32, name='dense_315_input'), name='dense_315_input', description=&quot;created by layer 'dense_315_input'&quot;), but it was called on an input with incompatible shape (None, 200, 1, 1).
</code></pre>
",55,1,0,5,machine-learning;deep-learning;data-science;tf.keras;densenet,2022-04-27 04:44:39,2022-04-27 04:44:39,2022-05-04 07:49:18,from the training data i have an sliding windows created for a single column and for the labels i have a binary classification   or    this is the model creation code  this is the functions i m using to compile the model  this is the model training  and those are the shapes of the batches  the problem is  i guess  that  from the model summary the model is training from the last dimension when it should be working in the second one  and because of that im getting valueerror  logits and labels must have the same shape   none      vs  none       how can i tell keras to apply the training in the second dimension and not the last one   this is what i understand is happening  is this right  how i fixed it  i tried to modify as suggested  using  dense_model add dense hidden_units  input_shape  none      but i m getting the following warning 
729,729,15751629,72103287,Using selected keywords from a string column to form one hot encoding type columns of pandas DataFrame,"<p>To demonstrate my question, consider the following example. Lets assume I have a following dataframe</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>index</th>
<th>ignore_x</th>
<th>ignore_y</th>
<th>phrases</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>43</td>
<td>23</td>
<td><strong>cat</strong> eats mice</td>
</tr>
<tr>
<td>1</td>
<td>1.3</td>
<td>33</td>
<td>water <strong>is</strong> pure</td>
</tr>
<tr>
<td>2</td>
<td>13</td>
<td>63</td>
<td>machine learning</td>
</tr>
<tr>
<td>3</td>
<td>15</td>
<td>35</td>
<td>where there <strong>is</strong> a will, there <strong>is</strong> a way</td>
</tr>
</tbody>
</table>
</div>
<p>Now consider that I have certain words I want to form dummy variables <strong>only</strong> for those.</p>
<p><code>keywords = [cat, is]</code></p>
<p>To do that, separate columns are populated for each of the keyword</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>index</th>
<th>x_ignore</th>
<th>y_ignore</th>
<th>phrases</th>
<th>kw_cat</th>
<th>kw_is</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>43</td>
<td>23</td>
<td><strong>cat</strong> eats mice</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1.3</td>
<td>33</td>
<td>water <strong>is</strong> pure</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>13</td>
<td>63</td>
<td>machine learning</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>15</td>
<td>35</td>
<td>where there <strong>is</strong> a will, there <strong>is</strong> a way</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Each phrase is scanned for the words and if there is a presence, the column returns True or get 1. (An alternative could be to count the occurrence as well, but lets keep it simple for now)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>index</th>
<th>x_ignore</th>
<th>y_ignore</th>
<th>phrases</th>
<th>kw_cat</th>
<th>kw_is</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>43</td>
<td>23</td>
<td><strong>cat</strong> eats mice</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1.3</td>
<td>33</td>
<td>water <strong>is</strong> pure</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>13</td>
<td>63</td>
<td>machine learning</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>15</td>
<td>35</td>
<td>where there <strong>is</strong> a will, there <strong>is</strong> a way</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div><hr />
<p>What I've been trying?
Loosely, I've been trying to do something like this</p>
<pre><code>for row, element in enumerate(df):
    for item in keywords:
        if item in df['phrases'].str.split(' '):
            df.loc[row, element] = 1
</code></pre>
<p>But this is not helping me out. It rather throws me a diagonal of 1s on those dummy variables.</p>
<p>Thanks :)</p>
<p>Edit: Just boldened the keywords to help you guys go through quickly :)</p>
",72,3,0,3,python;pandas;dummy-variable,2022-05-03 23:01:52,2022-05-03 23:01:52,2022-05-04 01:40:17,to demonstrate my question  consider the following example  lets assume i have a following dataframe now consider that i have certain words i want to form dummy variables only for those  keywords    cat  is  to do that  separate columns are populated for each of the keyword each phrase is scanned for the words and if there is a presence  the column returns true or get    an alternative could be to count the occurrence as well  but lets keep it simple for now  but this is not helping me out  it rather throws me a diagonal of s on those dummy variables  thanks    edit  just boldened the keywords to help you guys go through quickly   
730,730,13631591,72102863,Is it possible to reinitialize or reset an arbitrary tf.keras Callback?,"<p>I have a custom function that accepts and tests a machine learning model in a pretty wide range of conditions via a complicated cross-validation procedure that takes a list of Callbacks as an argument. Unfortunately I've encountered some unexpected behaviour, and in debugging I discovered that I neglected to account for the fact that <code>tf.keras.Callback</code>s are not reinitialized when the models are reinitialized, or when <code>tf.keras.backend.clear_session()</code> is called.</p>
<p>Is it possible to reset the state of an arbitrary <code>Callback</code>, or do they need to be constructed with the model to prevent information leakage?</p>
",16,0,0,3,tensorflow;machine-learning;keras,2022-05-03 22:25:49,2022-05-03 22:25:49,2022-05-03 22:25:49,i have a custom function that accepts and tests a machine learning model in a pretty wide range of conditions via a complicated cross validation procedure that takes a list of callbacks as an argument  unfortunately i ve encountered some unexpected behaviour  and in debugging i discovered that i neglected to account for the fact that tf keras callbacks are not reinitialized when the models are reinitialized  or when tf keras backend clear_session   is called  is it possible to reset the state of an arbitrary callback  or do they need to be constructed with the model to prevent information leakage 
731,731,13593338,72101589,How to run a script automaticaly after Azure ML instance VM starts,"<p>My sitiation is as follow: I have a machine learning pipeline and have to execute it automaticaly once the VM starts (I programed the VM to start and stop on specific dates in the months)</p>
<p>The problem is I usualy do it manualy by executing some Docker comands in the terminal. Now I want to save these comands in an sh file, and execute it once the VM starts.</p>
<p>Is this the wright way to do so and how? is there another alternative solution?</p>
<p>Thank you in advance</p>
",25,0,0,4,linux;azure;docker;virtual-machine,2022-05-03 20:43:46,2022-05-03 20:43:46,2022-05-03 21:03:16,my sitiation is as follow  i have a machine learning pipeline and have to execute it automaticaly once the vm starts  i programed the vm to start and stop on specific dates in the months  the problem is i usualy do it manualy by executing some docker comands in the terminal  now i want to save these comands in an sh file  and execute it once the vm starts  is this the wright way to do so and how  is there another alternative solution  thank you in advance
732,732,6457243,48875783,How to Upload Many Files to Google Colab?,"<p>I am working on a <a href=""https://www.kaggle.com/c/data-science-bowl-2018"" rel=""nofollow noreferrer"">image segmentation machine learning project</a> and I would like to test it out on Google Colab.</p>
<p>For the training dataset, I have 700 images, mostly <code>256x256</code>, that I need to upload into a python numpy array for my project. I also have thousands of corresponding mask files to upload. They currently exist in a variety of subfolders on Google drive, but I have been unable to upload them to Google Colab for use in my project.</p>
<p>So far I have attempted using Google Fuse which seems to have very slow upload speeds and PyDrive which has given me a variety of authentication errors. I have been using the Google Colab I/O example code for the most part.</p>
<p>How should I go about this? Would PyDrive be the way to go? Is there code somewhere for uploading a folder structure or many files at a time?</p>
",24788,6,10,4,python;machine-learning;jupyter;google-colaboratory,2018-02-20 05:06:17,2018-02-20 05:06:17,2022-05-03 18:56:50,i am working on a  and i would like to test it out on google colab  for the training dataset  i have  images  mostly x  that i need to upload into a python numpy array for my project  i also have thousands of corresponding mask files to upload  they currently exist in a variety of subfolders on google drive  but i have been unable to upload them to google colab for use in my project  so far i have attempted using google fuse which seems to have very slow upload speeds and pydrive which has given me a variety of authentication errors  i have been using the google colab i o example code for the most part  how should i go about this  would pydrive be the way to go  is there code somewhere for uploading a folder structure or many files at a time 
733,733,1786393,72096125,RASA Dialog Management: Ho to migrate state-based approach to a story-based approach?,"<p><a href=""https://rasa.com/docs/rasa/generating-nlu-data/"" rel=""nofollow noreferrer"">RASA NLU</a> (intents and entities classification) is clear, simple and well implemented.</p>
<p>But one of the things makes RASA different from many competitors is <a href=""https://rasa.com/docs/rasa/writing-stories"" rel=""nofollow noreferrer"">RASA dialog machine-learning based dialog management</a> (using <strong>stories</strong> and <strong>rules</strong>).</p>
<p>Myself (and I guess many of people that use non RASA framework)  I used to manage task-oriented (conversational) workflows using the concept of state-machine. BTW I developed a small open source project <a href=""https://github.com/solyarisoftware/naifjs"" rel=""nofollow noreferrer"">NaifJs</a>, that conceptualize a conversation as a sequence (a graph) of states, where each state is a set of valid intents involved in a conversation context.</p>
<p><a href=""https://github.com/solyarisoftware/naifjs/blob/master/doc/img/state-machine-1-new.png"" rel=""nofollow noreferrer"">https://github.com/solyarisoftware/naifjs/blob/master/doc/img/state-machine-1-new.png</a></p>
<p>So, what is not clear to me is ho to migrate a state-based (or context-based) approach to a story-based approach. Just to simplify/exemplify, I could immagine that given a &quot; dialog flow&quot; realized by conditional statements, I have to &quot;serialize/duplicate&quot; each &quot;if&quot; in multiple &quot;then&quot;stories.</p>
<p>Do you have already explained this problem in some articles?
Maybe a gift for a new RASA blog article/youtube workshop?</p>
<hr />
<p>Thanks</p>
<p>Giorgio</p>
",31,0,0,5,nlp;rasa;rasa-nlu;rasa-core;nlu,2022-05-03 13:04:57,2022-05-03 13:04:57,2022-05-03 13:04:57,  intents and entities classification  is clear  simple and well implemented  but one of the things makes rasa different from many competitors is   using stories and rules   myself  and i guess many of people that use non rasa framework   i used to manage task oriented  conversational  workflows using the concept of state machine  btw i developed a small open source project   that conceptualize a conversation as a sequence  a graph  of states  where each state is a set of valid intents involved in a conversation context   so  what is not clear to me is ho to migrate a state based  or context based  approach to a story based approach  just to simplify exemplify  i could immagine that given a   dialog flow  realized by conditional statements  i have to  serialize duplicate  each  if  in multiple  then stories  thanks giorgio
734,734,1264339,67285735,"When predicting, shall we scale unseen inputs, and un-scale outputs of a model?","<p>I am new to Machine Learning, and I followed this tutorial to implement LSTM model in Keras/Tensorflow:
<a href=""https://www.tensorflow.org/tutorials/structured_data/time_series"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/structured_data/time_series</a></p>
<p>In the tutorial the training/validaton/testing dataset are normalized like that:</p>
<pre class=""lang-py prettyprint-override""><code>train_df = (train_df - train_mean) / train_std
val_df = (val_df - train_mean) / train_std
test_df = (test_df - train_mean) / train_std
</code></pre>
<p>I have exported my model in HDF5 format and use keras to load it and run prediction.
The input timeseries I get are in a Pandas Dataframe <code>df</code>.</p>
<pre class=""lang-py prettyprint-override""><code>model = load_model(model_filename)
prediction_values = model.predict(df.to_numpy()[np.newaxis][0 : 24])
</code></pre>
<p>Reading articles on the internet, it is unclear to me if I should scale my input data the same way that they were scaled in the training and/or after prediction. Some article mentions that a scaling should be done before prediction, and other that it should be done after.</p>
<p>I tried to create those 2 functions:</p>
<pre class=""lang-py prettyprint-override""><code>def scale_data(df):
    df = (df - pd.Series(historical_data_mean)) / pd.Series(historical_data_standard_deviation)
    return df
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def unscale_date(df):
    df = df * pd.Series(historical_data_standard_deviation) + pd.Series(historical_data_mean)
    return df
</code></pre>
<p>And run them like that:</p>
<pre class=""lang-py prettyprint-override""><code>unseen_inputs_df   # A new timeserie that will be used as input of the model

scaled_input = scale_data(unseen_inputs_df)  # Should I scale my 'unseen' inputs here?
  
prediction_values = model.predict(scaled_input.to_numpy()[np.newaxis][0 : 24])
prediction_df = pd.DataFrame(data=prediction_values[0])

unscaled_output = unscale_date()    # Should I unscale the models output here?

</code></pre>
<p>However it returned totally wrong outputs.</p>
<p>Do you have any clue on what is the correct way to proceed?</p>
",131,1,2,5,python;tensorflow;keras;lstm;data-preprocessing,2021-04-27 20:38:18,2021-04-27 20:38:18,2022-05-03 13:00:34,in the tutorial the training validaton testing dataset are normalized like that  reading articles on the internet  it is unclear to me if i should scale my input data the same way that they were scaled in the training and or after prediction  some article mentions that a scaling should be done before prediction  and other that it should be done after  i tried to create those  functions  and run them like that  however it returned totally wrong outputs  do you have any clue on what is the correct way to proceed 
735,735,1462656,59958274,Unable to connect to the server: net/http: TLS handshake timeout,"<p>On minikube for windows I created a deployment on the kubernetes cluster, then I tried to scale it by changing replicas from 1 to 2, and after that kubectl hangs and my disk usage is 100%.
I only have one container in my deployment</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: first-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      run: app
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
      - name: demo
        image: ner_app
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
</code></pre>
<p>all I did was run this after the pods were successfully deployed and running</p>
<pre><code>kubectl scale --replicas=2 deployment first-deployment
</code></pre>
<p>In another terminal I was watching the pods using</p>
<pre><code>kubectl get pods --watch
</code></pre>
<p>But everything is unresponsive and I'm not sure how to recover from this.</p>
<p>When I run <code>kubectl get pods</code> again it gives the following message</p>
<pre><code>PS D:\docker\ner&gt; kubectl get pods
Unable to connect to the server: net/http: TLS handshake timeout
</code></pre>
<p>Is there a way to recover, or cancel whatever process is running?</p>
<p>Also my VM's are on Hyper-V for Windows 10 Pro (minikube and Docker Desktop) both have the default RAM allocated - 2048MB</p>
<p>The container in my pod is a machine learning process and the model it loads could be large, in the order of 200MB to 300MB</p>
",75709,10,16,2,kubernetes;minikube,2020-01-29 04:20:01,2020-01-29 04:20:01,2022-05-03 04:28:43,all i did was run this after the pods were successfully deployed and running in another terminal i was watching the pods using but everything is unresponsive and i m not sure how to recover from this  when i run kubectl get pods again it gives the following message is there a way to recover  or cancel whatever process is running  also my vm s are on hyper v for windows  pro  minikube and docker desktop  both have the default ram allocated   mb the container in my pod is a machine learning process and the model it loads could be large  in the order of mb to mb
736,736,14138404,72090611,Average precision score too high looking at the confusion matrix,"<p>I am developing a machine learning scikit-learn model on an imbalanced dataset (binary classification). Looking at the confusion matrix and the F1 score, I expect a lower average precision score but I almost get a perfect score and I can't figure out why. This is the output I am getting:</p>
<p>Confusion matrix on the test set:</p>
<pre><code>[[6792  199]
[   0  173]]
</code></pre>
<p>F1 score:
0.63</p>
<p>Test AVG precision score:
0.99</p>
<p>I am giving the avg precision score function of scikit-learn probabilities which is what the package says to use. I was wondering where the problem could be.</p>
",42,1,-1,4,machine-learning;scikit-learn;precision-recall;imbalanced-data,2022-05-02 23:34:15,2022-05-02 23:34:15,2022-05-03 03:13:36,i am developing a machine learning scikit learn model on an imbalanced dataset  binary classification   looking at the confusion matrix and the f score  i expect a lower average precision score but i almost get a perfect score and i can t figure out why  this is the output i am getting  confusion matrix on the test set  i am giving the avg precision score function of scikit learn probabilities which is what the package says to use  i was wondering where the problem could be 
737,737,19003011,72091809,#GraphexecutionerrorinImageClassificationUsingML,"<p>Here is my code of image classification project in machine learning, i am not able to understand this error. I have imported all the necessary libraries.</p>
<pre><code>from keras.callbacks import ModelCheckpoint,EarlyStopping
mc=ModelCheckpoint(filepath=&quot;./best_model.h5&quot;,
                     monitor=&quot;val_accuracy&quot;,
                     verbose=1,
                     save_best_only= True)
                                                 
es=EarlyStopping(monitor=&quot;val_accuracy&quot;,
                   min_delta= 0.01,
                   patience=5,
                   verbose=1)
cb=[mc,es]
his = model.fit_generator(train_data,
                          steps_per_epoch= 7,
                          epochs= 26,
                          callbacks= cb)
</code></pre>
<p>I get the following error when I try to run it:</p>
<pre><code>Epoch 1/26
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-30-d3e718f31696&gt; in &lt;module&gt;()
      2                           steps_per_epoch= 7,
      3                           epochs= 26,
----&gt; 4                           callbacks= cb)

2 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'gradient_tape/binary_crossentropy/logistic_loss/mul/BroadcastGradientArgs' defined at (most recent call last):
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
      &quot;__main__&quot;, mod_spec)
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
</code></pre>
<p>and so on.. it is a very long error</p>
",35,0,0,5,python;tensorflow;machine-learning;keras;image-classification,2022-05-03 01:33:04,2022-05-03 01:33:04,2022-05-03 03:10:39,here is my code of image classification project in machine learning  i am not able to understand this error  i have imported all the necessary libraries  i get the following error when i try to run it  and so on   it is a very long error
738,738,19017538,72090829,"Using XState, how can I access name of current state in an action?","<p>I'm playing around learning XState and wanted to include an action in a machine that would just log the current state to console.</p>
<p>Defining a simple example machine like so, how would I go about this? Also note the questions in the comments in the code.</p>
<pre><code>import { createMachine, interpret } from &quot;xstate&quot;

const sm = createMachine({
    initial: 'foo',
    states: {
        foo: {
            entry: 'logState', // Can I only reference an action by string?
                               // Or can I add arguments here somehow?
            on: {
                TOGGLE: {target: 'bar'}
            }
        },
        bar: {
            entry: 'logState',
            on: {
                TOGGLE: {target: 'foo'}
            }
        }
    }
},
{
    actions: {
        logState(/* What arguments can go here? */) =&gt; {
            // What do I do here?
        }
    }
});

</code></pre>
<p>I know that actions are called with <code>context</code> and <code>event</code> as arguments but I don't see a way to get the current state from either of those. Am I missing something here?</p>
",79,0,1,2,javascript;xstate,2022-05-02 23:53:44,2022-05-02 23:53:44,2022-05-02 23:53:44,i m playing around learning xstate and wanted to include an action in a machine that would just log the current state to console  defining a simple example machine like so  how would i go about this  also note the questions in the comments in the code  i know that actions are called with context and event as arguments but i don t see a way to get the current state from either of those  am i missing something here 
739,739,11967549,72089444,Can I write a general wrapper class which adds a function to any class?,"<p>So there is an encoder library with many different encoder classes in Machine Learning:</p>
<pre><code>from category_encoders import BinaryEncoder as BE, OneHotEncoder as OE
</code></pre>
<p>These classes miss some functionality, I want to add a function called <code>get_feature_names_out()</code> - what it does is not important.</p>
<p>My approach is to create a class like this:</p>
<pre><code>class BinaryEncoder(BE):
    def get_feature_names_out():
        # do stuff and return value

encoder = BinaryEncoder()
</code></pre>
<p>This works fine, but I do not want to write a wrapper class for every single encoder. Is it possible to replace <code>BE</code> in the class definition with a general object? So that I can create an instance like this:</p>
<pre><code>encoder = GeneralEncoderClass(BinaryEncoder())
</code></pre>
<p>I can not figure out how I can let the <code>GeneralEncoderClass</code> inherit all the functionality of its &quot;parent&quot; encoder class, without defining it in the the header.</p>
",29,0,0,3,python;class;wrapper,2022-05-02 21:42:56,2022-05-02 21:42:56,2022-05-02 22:52:07,so there is an encoder library with many different encoder classes in machine learning  these classes miss some functionality  i want to add a function called get_feature_names_out     what it does is not important  my approach is to create a class like this  this works fine  but i do not want to write a wrapper class for every single encoder  is it possible to replace be in the class definition with a general object  so that i can create an instance like this  i can not figure out how i can let the generalencoderclass inherit all the functionality of its  parent  encoder class  without defining it in the the header 
740,740,5304366,72089924,Order columns in MLflow ui,"<p>I am using the MLflow UI to track machine learning experiments.
I want to be able to sort the position of the columns (that seems to be in alphabetical order by default).</p>
<h5>My use case.</h5>
<p>I log different metrics with <code>mlflow.log_metric()</code>.<br />
Let's say I track <code>AUC</code> and <code>logloss</code> (and possibly many others) but I am mainly interested in the logloss. I want it to be in the first position in the UI.</p>
<p>How can I achieve that?</p>
",51,0,0,3,python;machine-learning;mlflow,2022-05-02 22:28:47,2022-05-02 22:28:47,2022-05-02 22:51:34,how can i achieve that 
741,741,19014864,72085797,&#39;Text&#39; object has no attribute &#39;lower&#39;,"<p>I am trying to show my machine learning project through web-based and using voila</p>
<pre><code>text = widgets.Text(placeholder='Input text here')

button_send = widgets.Button(
                description='Identify gender',
                tooltip='Send',
                style={'description_width': 'initial'}
            )

output = widgets.Output()

def on_button_clicked(event):
    with output:
        clear_output()
        features = find_features(top_words, text)
        print(&quot;Naive Bayes = &quot; + NB_classifier.classify(features))
        
button_send.on_click(on_button_clicked)

vbox_result = widgets.VBox([button_send, output])

text_0 = widgets.HTML(value=&quot;&lt;h1&gt;Hello!&lt;/h1&gt;&quot;)
text_1 = widgets.HTML(value=&quot;&lt;h2&gt;Type anything&lt;/h2&gt;&quot;)

vbox_text = widgets.VBox([text_0, text_1, text, vbox_result])

page = widgets.HBox([vbox_headline, vbox_text])
display(page)
</code></pre>
<p>and i got this following callback after input the text:
<a href=""https://i.stack.imgur.com/q3PpQ.png"" rel=""nofollow noreferrer"">my error</a></p>
",18,1,0,1,jupyter-notebook,2022-05-02 16:37:13,2022-05-02 16:37:13,2022-05-02 22:42:21,i am trying to show my machine learning project through web based and using voila
742,742,19015301,72086431,"I keep getting this error: Class referenced in the layout file, com.camerakit.CameraKitView, was not found in the project or the libraries","<p>How do I get rid of this error warning in Android Studio?</p>
<p>I am following this guide: <a href=""https://proandroiddev.com/machine-learning-in-android-using-firebase-ml-kit-6e71a14e11f8"" rel=""nofollow noreferrer"">https://proandroiddev.com/machine-learning-in-android-using-firebase-ml-kit-6e71a14e11f8</a></p>
<p><a href=""https://i.stack.imgur.com/CrjtA.png"" rel=""nofollow noreferrer"">build.gradle (app)</a></p>
<pre><code>implementation 'com.camerakit:camerakit:1.0.0-beta3.11'
implementation 'com.camerakit:jpegkit:0.1.0'
implementation 'org.jetbrains.kotlin:kotlin-stdlib-jdk7:1.3.31'
implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-android:1.0.0'
</code></pre>
<p><a href=""https://i.stack.imgur.com/cP2O7.png"" rel=""nofollow noreferrer"">activity_main.xml</a></p>
<pre><code>&lt;com.camerakit.CameraKitView
    android:layout_above=&quot;@id/btn_detect&quot;
    android:layout_width=&quot;match_parent&quot;
    android:id=&quot;@+id/camera_view&quot;
    android:layout_height=&quot;match_parent&quot;
    android:adjustViewBounds=&quot;true&quot;
    android:keepScreenOn=&quot;true&quot;
    app:camera_flash=&quot;auto&quot;
    app:camera_facing=&quot;back&quot;
    app:camera_focus=&quot;continuous&quot;
    app:camera_permissions=&quot;camera&quot;&gt;
&lt;/com.camerakit.CameraKitView&gt;
</code></pre>
",56,0,0,1,android,2022-05-02 17:35:24,2022-05-02 17:35:24,2022-05-02 19:37:35,how do i get rid of this error warning in android studio  i am following this guide    
743,743,9820165,70323521,Get Confidence probability Scores for each Predicted Result in Catboost Classifier,"<p>I have built a machine learning model using Catboost classifier to predict the categoryname of my result as per below screenshot1. However, if I get an unknown as input or any input with which the model has not been trained with, then I need to return it as null.</p>
<p>My idea to approach this is was based on the Probability of confidence score as per below scrrenshot2 (Expected Output). For known input the model would have high probability score and for any unknown unseen input the model would have low confidence score.</p>
<p>How can I achieve this and add probability column to my predicted results as per below screenshot2 (Expected Output)?</p>
<p>Code I am working with</p>
<pre><code>pred = pipe_model_.predict(df_unseen)
predict_proba = pipe_model_.predict_proba(df_unseen)
# Get predicted RawFormulaVal
preds_raw = pipe_model_.predict(df_unseen, 
                          prediction_type='RawFormulaVal')
</code></pre>
<p>Output of above code on Predict_proba is below
<a href=""https://i.stack.imgur.com/N5sMo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N5sMo.png"" alt=""enter image description here"" /></a></p>
<p>Sample Input Trained Dataframe (Screenshot 1)</p>
<p><a href=""https://i.stack.imgur.com/yaYLM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yaYLM.png"" alt=""enter image description here"" /></a></p>
<p>Expected Predicted Output is as below (Screenshot 2) and yellow highlighted is the one which the model has never seen before or trained with so the probability is low and I can write a if condition to omit that as per my requirement</p>
<p><a href=""https://i.stack.imgur.com/lO0AA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lO0AA.png"" alt=""enter image description here"" /></a></p>
",224,1,2,4,python;machine-learning;probability;catboost,2021-12-12 17:57:19,2021-12-12 17:57:19,2022-05-02 14:01:48,i have built a machine learning model using catboost classifier to predict the categoryname of my result as per below screenshot  however  if i get an unknown as input or any input with which the model has not been trained with  then i need to return it as null  my idea to approach this is was based on the probability of confidence score as per below scrrenshot  expected output   for known input the model would have high probability score and for any unknown unseen input the model would have low confidence score  how can i achieve this and add probability column to my predicted results as per below screenshot  expected output   code i am working with sample input trained dataframe  screenshot    expected predicted output is as below  screenshot   and yellow highlighted is the one which the model has never seen before or trained with so the probability is low and i can write a if condition to omit that as per my requirement 
744,744,19012454,72082384,How to remove all strings from a given DataFrame column?,"<p>I need to preprocess a column for machine learning in python. The column contains a series of 1s and 0s (which is the desired output), but there are some strings in there that needs to be removed ['PX7','D1', etc..]</p>
<p>I thought about using df.replace to replace the strings with np.nan and then using df.dropna() to remove it. I was wondering what is the standard way of doing this given that this is probably a very common preprocessing task.</p>
",65,2,0,3,python;pandas;data-preprocessing,2022-05-02 10:10:51,2022-05-02 10:10:51,2022-05-02 13:05:22,i need to preprocess a column for machine learning in python  the column contains a series of s and s  which is the desired output   but there are some strings in there that needs to be removed   px   d   etc    i thought about using df replace to replace the strings with np nan and then using df dropna   to remove it  i was wondering what is the standard way of doing this given that this is probably a very common preprocessing task 
745,745,10011584,51526577,How to get the remaining sample after using random.sample() in Python?,"<p>I have a large list of elements (in this example I'll assume it's filled with numbers). For example: <code>l = [1,2,3,4,5,6,7,8,9,10]
</code> Now I want to take 2 samples from that list, one with the 80% of the elements (randomly chosen of course), and the other one with the remaining elements (the 20%), so I can use the bigger one to train a machine-learning tool, and the rest to test that training. The function I used is from <code>random</code> and I used it this way:</p>

<pre><code>sz = len(l) #Size of the original list
per = int((80 * sz) / 100) #This will be the length of the sample list with the 80% of the elements (I guess)
random.seed(1) # As I want to obtain the same results every time I run it.
l2 = random.sample(l, per)
</code></pre>

<p>I'm not totally sure, but I believe that with that code I'm getting a random sample with the 80% of the numbers. </p>

<pre><code>l2 = [3,4,7,2,9,5,1,8]
</code></pre>

<p>Nonetheless, I can't seem to find the way to get the other sample list with the remaining elements <code>l3 = [6,10]</code> (the <code>sample()</code> function does not remove the elements it takes from the original list). Can you please help me? Thank you in advance.</p>
",2101,2,1,4,python;python-3.x;random;sample,2018-07-26 01:14:39,2018-07-26 01:14:39,2022-05-02 12:22:10,i m not totally sure  but i believe that with that code i m getting a random sample with the   of the numbers   nonetheless  i can t seem to find the way to get the other sample list with the remaining elements l        the sample   function does not remove the elements it takes from the original list   can you please help me  thank you in advance 
746,746,4240413,59760739,How do I return a dict + an image from a FastAPI endpoint?,"<p>I am trying to use FastAPI to allow my (dockerized) server to respond to API calls returning</p>
<ul>
<li>an image <code>image</code>, and</li>
<li>a dictionary <code>additional_dict</code></li>
</ul>
<p>(for a machine learning example, this could be classification labels from a classifier and a saliency map).</p>
<p>In my case, I think it makes sense to make use the same endpoint to get the two objects, because these get generated with the same computations.</p>
<p>I can successfully return an image as binary using something along the lines of <a href=""https://stackoverflow.com/a/55905051/4240413"">https://stackoverflow.com/a/55905051/4240413</a>:</p>
<pre><code>from PIL import Image
from fastapi import FastAPI, File
import tempfile
from starlette.responses import FileResponse

@app.post(&quot;/getstuff&quot;)
async def get_image_and_data(file: bytes = File(...)):
    
    image, additional_dict = process_image(image)

    with tempfile.NamedTemporaryFile(mode=&quot;w+b&quot;, suffix=&quot;.png&quot;, delete=False) as outfile:
        image.save(outfile)
        return FileResponse(outfile.name, media_type=&quot;image/png&quot;)
</code></pre>
<p>This allows me even to see the image response when I invoke the service through the swagger UI on <code>localhost:8000/docs</code>.</p>
<p>However, I don't know how to get the image binary <em>and the dictionary</em> together.</p>
<p>I tried to replace my return statement with:</p>
<pre><code>return FileResponse(outfile.name, media_type=&quot;image/png&quot;), additional_dict
</code></pre>
<p>but this doesn't really work (when trying on swagger at localhost:8000/docs, I just get the json reply below, with a path to the temp file created)</p>
<pre><code>[{
&quot;path&quot;: &quot;/tmp/tmpldwe_79d.png&quot;,
&quot;status_code&quot;: 200,
&quot;filename&quot;: null,
&quot;send_header_only&quot;: false,
&quot;media_type&quot;: &quot;image/png&quot;,
&quot;background&quot;: null,
&quot;raw_headers&quot;: [
    [
    &quot;content-type&quot;,
    &quot;image/png&quot;
    ]
],
&quot;stat_result&quot;: null
},
{
&quot;foo&quot;: 1,
&quot;bar&quot;: 2
}]
</code></pre>
<p>It is possible to get in the response the binary image and the additional dict, from the same endpoint? If so, what's the best way to do it?<br />
Is it possible to have the image rendered in the Swagger UI <code>/docs</code> and read the dict values there?</p>
",4350,1,4,2,python;fastapi,2020-01-16 04:25:37,2020-01-16 04:25:37,2022-05-02 02:46:00,i am trying to use fastapi to allow my  dockerized  server to respond to api calls returning  for a machine learning example  this could be classification labels from a classifier and a saliency map   in my case  i think it makes sense to make use the same endpoint to get the two objects  because these get generated with the same computations  i can successfully return an image as binary using something along the lines of   this allows me even to see the image response when i invoke the service through the swagger ui on localhost  docs  however  i don t know how to get the image binary and the dictionary together  i tried to replace my return statement with  but this doesn t really work  when trying on swagger at localhost  docs  i just get the json reply below  with a path to the temp file created 
747,747,18729632,72079996,Convert video data from .npz to video,"<p>I'm using a <strong>video</strong> data set compressed in several sets of .npz files.
When it is explored, you can see two files in each .npz: 'name1', 'name2'.</p>
<p>name1 - shape: (8, 8, 140, 210, 3) (I asume: batches, frames, height, width, channels)
name2 - shape: (8, 2)  (I asume this might be tags)</p>
<p>How can I read this info for using it in a machine learning model?</p>
",23,0,0,2,video-processing;npz-file,2022-05-02 01:25:40,2022-05-02 01:25:40,2022-05-02 01:25:40,how can i read this info for using it in a machine learning model 
748,748,936332,24666312,Why limit the weight size can prevent overfitting in machine learning,"<p>The most popular way to prevent over-fitting is weight decay(L2, L1) in machine learning(Like <code>logistic regression</code>, <code>Neural network</code>, <code>linear regression</code> etc). The purpose of weight decay is preventing the weight get big.<br>
<strong>My question is why small weight can prevent over-fitting</strong>.<br>
<strong>what if I do weight normalization after each iteration</strong>.</p>
",3054,4,2,3,algorithm;machine-learning;neural-network,2014-07-10 07:24:58,2014-07-10 07:24:58,2022-05-01 20:34:02,
749,749,978769,23450534,How to call a Python function from Node.js,"<p>I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?</p>
",262124,11,305,3,python;node.js;express,2014-05-04 04:05:13,2014-05-04 04:05:13,2022-05-01 11:07:00,i have an express node js application  but i also have a machine learning algorithm to use in python  is there a way i can call python functions from my node js application to make use of the power of machine learning libraries 
750,750,13871346,72066483,Similarity between multiple vectors having same length,"<p><strong>Objective</strong> : Compute a similarity between two users on the basis of their skills</p>
<p><strong>Approach</strong> : Trained a <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">word2vec</a> model using <code>gensim</code> library on the set of skills obtained from Job Descriptions. Model seems to be working pretty fine when used <code>model.wv.most_similar</code>
e.g.
<a href=""https://i.stack.imgur.com/7O4uG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7O4uG.png"" alt=""most_similar skill example"" /></a></p>
<p><strong>Problem</strong> : Vocabulary of the skills on which model was trained doesn't match with the skills which I currently have so I went ahead and found a replacement of the current skills from the model's vocabulary by finding a similarity w.r.t spelling using <code>SequenceMatcher</code> from module <code>difflib</code>. e.g. &quot;PyTorch&quot; was there in my current skills but the model's vocabulary had &quot;torch&quot; present as a skill. So using <code>SequenceMatcher</code> I found that &quot;torch&quot; has the highest similarity from model's vocabulary so I replaced &quot;Pytorch&quot; with &quot;torch&quot; and computed the vector representation of the same by passing &quot;torch&quot; into the model, <code>model.wv[&quot;torch&quot;]</code>
and stored it in a dictionary so that I won't have to compute it again and again.</p>
<p>Function to compute the same :</p>
<pre class=""lang-py prettyprint-override""><code>def new_to_old_embedding(skill_embeddings, new_skill, model)
    &quot;&quot;&quot; Computing embeddings for new skills from app by mapping new skills with old skills from model's vocabulary
    
    Returns:
        dict: Embeddings of new skills after mapping with old skills
    &quot;&quot;&quot;
    if new_skill not in old_skills:
        thresh = 0.6
        replaced_skill = ''
        for old_skill in old_skills :
            spell_sim = SequenceMatcher(None, old_skill, new_skill).ratio()
            if spell_sim &gt; thresh :
                thresh = spell_sim
                replaced_skill = old_skill
        skill_embeddings[new_skill] = model.wv[replaced_skill]
    else :
        skill_embeddings[new_skill] = model.wv[new_skill]
    return skill_embeddings
</code></pre>
<p>Similarly for all of my current skills, I found a nearest skill w.r.t spelling and computed its vector representation and stored it in a python dictionary.</p>
<p>Now if user1 has skills = [&quot;OpenCV&quot;, &quot;Python&quot;] and user2 has skills = [&quot;Machine Learning&quot;, &quot;Deep Learning&quot;, &quot;Python&quot;] and I already have vector representations of each skill stored in a dictionary then how can I compute the similarity between these two sets of skills ?</p>
<p>OR</p>
<p>In other words, I have to find a similarity between two matrices of dimensions (m, L) and (n, L)
where,</p>
<ul>
<li>m is number of skills for user1</li>
<li>n is the number of skills for user2</li>
<li>L is the length of the vector representing skill which is fixed (300 in my case)</li>
</ul>
<p>I did found <a href=""https://math.stackexchange.com/questions/507742/distance-similarity-between-two-matrices"">this</a> question but since my problem is a NLP problem I was not sure whether or not this will work.</p>
",53,1,0,3,python;nlp;word2vec,2022-04-30 12:50:03,2022-04-30 12:50:03,2022-05-01 06:43:18,objective   compute a similarity between two users on the basis of their skills function to compute the same   similarly for all of my current skills  i found a nearest skill w r t spelling and computed its vector representation and stored it in a python dictionary  now if user has skills     opencv    python   and user has skills     machine learning    deep learning    python   and i already have vector representations of each skill stored in a dictionary then how can i compute the similarity between these two sets of skills   or i did found  question but since my problem is a nlp problem i was not sure whether or not this will work 
751,751,16014407,72070421,click command line within conda environment- mac m1 chip,"<p>I am currently working with tensorflow on a new mac with M1 chip.</p>
<p>As you might know, getting tensorflow to work on the M1 chip is a bit messy but I managed to do it following this tutorial: <a href=""https://github.com/mrdbourke/m1-machine-learning-test"" rel=""nofollow noreferrer"">https://github.com/mrdbourke/m1-machine-learning-test</a></p>
<p>Now, I have an environment named <code>tensorflow-test</code> with python 3.8 and tensorflow correctly installed (I can import it without errors).</p>
<p>Within this environment, I want to build some command lines using click package. I used to have everything integrated beautifully until, for some weird reason, I messed up with the environment and now I can't figure out what is going on.</p>
<p>A simple code to reproduce what I have is the following. Within the tensorflow-test conda environment I have a file with click code <code>hello.py</code>:</p>
<pre><code>import click
import tensorflow


@click.command()
@click.option('--count', default=1, help='Number of greetings.')
@click.option('--name', prompt='Your name',
              help='The person to greet.')
def cli(count, name):
    &quot;&quot;&quot;Simple program that greets NAME for a total of COUNT times.&quot;&quot;&quot;
    for x in range(count):
        click.echo(&quot;Hello {name}!&quot;)


if __name__ == '__main__':
    cli()
</code></pre>
<p>and another with the set up, <code>setup.py</code> :</p>
<pre><code>from setuptools import setup

setup(
    name=&quot;hello&quot;,
    version='0.1',
    py_modules=['hello'],
    author='xxx',
    install_requires=[
        'Click',
        'pandas',
        'tensorflow',
    ],
    entry_points='''
        [console_scripts]
         hello=hello:cli
    ''',
   
)
</code></pre>
<p>When I run it using <code>python hello.py</code> everything seems to be working fine. However, when I try to integrate this to be run directly as a command line, I do</p>
<p><code>pip install --editable .</code></p>
<p>and the following error appears:</p>
<pre><code>Requirement already satisfied: Click in ./env/lib/python3.8/site-packages (from hello==0.1) (8.0.4)
Requirement already satisfied: pandas in ./env/lib/python3.8/site-packages (from hello==0.1) (1.4.2)
ERROR: Could not find a version that satisfies the requirement tensorflow (from hello) (from versions: none)
ERROR: No matching distribution found for tensorflow
</code></pre>
<p>I checked and there is a folder named tensorflow in ./env/lib/python3.8/site-packages.</p>
<p>What is going on? Why can't I use hello directly as a command when everything seems to be working fine otherwise?</p>
",39,1,1,3,python;tensorflow;apple-m1,2022-04-30 22:22:25,2022-04-30 22:22:25,2022-05-01 04:09:04,i am currently working with tensorflow on a new mac with m chip  as you might know  getting tensorflow to work on the m chip is a bit messy but i managed to do it following this tutorial   now  i have an environment named tensorflow test with python   and tensorflow correctly installed  i can import it without errors   within this environment  i want to build some command lines using click package  i used to have everything integrated beautifully until  for some weird reason  i messed up with the environment and now i can t figure out what is going on  a simple code to reproduce what i have is the following  within the tensorflow test conda environment i have a file with click code hello py  and another with the set up  setup py   when i run it using python hello py everything seems to be working fine  however  when i try to integrate this to be run directly as a command line  i do pip install   editable   and the following error appears  i checked and there is a folder named tensorflow in   env lib python  site packages  what is going on  why can t i use hello directly as a command when everything seems to be working fine otherwise 
752,752,11126012,70638759,How to activate python virtual environment on remote machine with ansible?,"<p>I'm learning Vagrant and Ansible, I'm trying to setup a local development environment for a basic flask app in ubuntu20.04 with Nginx.</p>
<p>my vagrantfile looks like this:</p>
<pre><code>Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.define :ubuntuserver do | ubuntuserver |
    ubuntuserver.vm.box = &quot;bento/ubuntu-20.04&quot;
    ubuntuserver.vm.hostname = &quot;ubuntuserver&quot;
    ubuntuserver.vm.provision :ansible do | ansible |
      ansible.playbook = &quot;development.yml&quot;
    end
    ubuntuserver.vm.network &quot;private_network&quot;, ip:&quot;10.11.1.105&quot;
    ubuntuserver.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080
    ubuntuserver.vm.network &quot;public_network&quot;, bridge: &quot;en1: Wi-Fi (AirPort)&quot;
    ubuntuserver.vm.provider :virtualbox do |vb|
      vb.memory = &quot;1024&quot;
    end
    ubuntuserver.vm.synced_folder &quot;./shared&quot;, &quot;/var/www&quot;
  end
end
</code></pre>
<p>my ansible-playbook like so:</p>
<pre><code>-
  name: local env
  hosts: ubuntuserver
  tasks:
    - name: update and upgrade apt packages
      become: yes
      apt: 
        upgrade: yes
        update_cache: yes

    - name: install software properties common
      apt:
        name: software-properties-common
        state: present

    - name: install nginx
      become: yes
      apt:
        name: nginx
        state: present
        update_cache: yes

    - name: ufw allow http
      become: yes
      community.general.ufw:
        rule: allow
        name: &quot;Nginx HTTP&quot;
    
    - name: installing packages for python env
      become: yes
      apt:
        name: 
          - python3-pip
          - python3-dev
          - build-essential
          - libssl-dev
          - libffi-dev
          - python3-setuptools
          - python3-venv
        update_cache: yes
    
    - name: Create app directory if it does not exist
      ansible.builtin.file:
        path: /var/www/app
        state: directory
        mode: '0774'

    - name: Install virtualenv via pip
      become: yes
      pip:
        name: virtualenv
        executable: pip3

    - name: Set python virual env
      command:
        cmd: virtualenv /var/www/app/ -p python3
        creates: &quot;/var/www/app/&quot;

    - name: Install requirements
      pip:
        requirements: /var/www/requirements.txt
        virtualenv: /var/www/app/appenv
        virtualenv_python: python3

</code></pre>
<p>My playbook fails at the next task with error:</p>
<pre><code>- name: Activate /var/www/app/appenv
      become: yes
      command: source /var/www/app/appenv/bin/activate
</code></pre>
<pre><code>fatal: [ubuntuserver]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;cmd&quot;: &quot;source /var/www/app/appenv/bin/activate&quot;, &quot;msg&quot;: &quot;[Errno 2] No such file or directory: b'source'&quot;, &quot;rc&quot;: 2}
</code></pre>
<p>Rest of the playbook</p>
<pre><code>   
    - name: ufw allow 5000
      become: yes
      community.general.ufw:
        rule: allow
        to_port: 5000
    
    - name: Run app
      command: python3 /var/www/app/appenv/app.py
</code></pre>
<p>From what I understand from <a href=""https://stackoverflow.com/questions/22256884/not-possible-to-source-bashrc-with-ansible/27541856#27541856"">this</a> thread, The &quot;source&quot; command must be used from inside the vagrant machine. (I tried solutions from the thread but couldn't get it to work)
If I ssh into the vagrant machine and execute the three last commands of my playbook manually:</p>
<pre><code>source /var/www/app/appenv/bin/activate
sudo ufw allow 5000
python3 /var/www/app/appenv/app.py
</code></pre>
<p>my basic flask app is running on port 5000 at the IP set in the vagrantfile 10.11.1.105</p>
<p>My questions are:</p>
<p>How can I get the playbook to work and not have to ssh into the machine to accomplish the same?</p>
<p>Is my approach even correct, knowing that my end goal is to replicate in the vagrant machine a similar environment to what would be the production environment and develop the flask app from my local machine in the <a href=""https://www.vagrantup.com/docs/synced-folders"" rel=""nofollow noreferrer"">synced folder</a>?</p>
<p>to give a maximum of information, if one wants to reproduce this.
I also have a shared/app/appenv/app.py file containing the basic flask app</p>
<pre><code>from flask import Flask
app = Flask(__name__)

@app.route(&quot;/&quot;)
def hello():
    return &quot;&lt;h1 style='color:blue'&gt;Hello There!&lt;/h1&gt;&quot;

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0')
</code></pre>
<p>and shared/requirements.txt file</p>
<pre><code>wheel
uwsgi
flask
</code></pre>
",491,2,1,3,ansible;vagrant;virtualenv,2022-01-09 11:16:52,2022-01-09 11:16:52,2022-05-01 02:50:28,i m learning vagrant and ansible  i m trying to setup a local development environment for a basic flask app in ubuntu  with nginx  my vagrantfile looks like this  my ansible playbook like so  my playbook fails at the next task with error  rest of the playbook my basic flask app is running on port  at the ip set in the vagrantfile     my questions are  how can i get the playbook to work and not have to ssh into the machine to accomplish the same  is my approach even correct  knowing that my end goal is to replicate in the vagrant machine a similar environment to what would be the production environment and develop the flask app from my local machine in the   and shared requirements txt file
753,753,8870965,72052762,boot microservice not accessible due to laptop security,"<p>I crafted a simple Java8/Eclipse/Maven/spring boot microservice on my own personal laptop ie have admin privileges.
I start it with <strong>java -jar myService.jar</strong> from within a CommandPrompt pane.
I can then access it from my browser at http://localhost:808/myService and receive a ping message as expected.</p>
<p>So far so good.</p>
<p>I've entered this service onto a laptop where I'm just a user - no admin privileges.
It compiles w/o errors. I start it with <strong>java -jar myService.jar</strong> from within a CommandPrompt pane. But from the browser on this machine I get the default whitelabel error page.</p>
<p>I'm thinking this must be some security issue on the 2nd laptop. How can I prove/disprove this assertion? If proven, must I execute temporarily assume Admin rights and launch this jar-file as a service?</p>
<p>-update-</p>
<p>upon closer examination of the startup screens on personal laptop I see mappings occuring...</p>
<pre>
2022-04-29 00:07:51 INFO [o.a.c.c.C.[.[.[/]-log] - Initializing Spring embedded WebApplicationContext
2022-04-29 00:07:51 INFO [o.s.w.c.ContextLoader-prepareWebApplicationContext] - Root WebApplicationContext: initialization completed in 2242 ms
2022-04-29 00:07:51 INFO [o.s.b.w.s.ServletRegistrationBean-addRegistration] - Servlet dispatcherServlet mapped to [/]
2022-04-29 00:07:51 INFO [o.s.b.w.s.FilterRegistrationBean-configure] - Mapping filter: 'characterEncodingFilter' to: [/*]
2022-04-29 00:07:51 INFO [o.s.b.w.s.FilterRegistrationBean-configure] - Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2022-04-29 00:07:51 INFO [o.s.b.w.s.FilterRegistrationBean-configure] - Mapping filter: 'httpPutFormContentFilter' to: [/*]
2022-04-29 00:07:51 INFO [o.s.b.w.s.FilterRegistrationBean-configure] - Mapping filter: 'requestContextFilter' to: [/*]
2022-04-29 00:07:54 INFO [o.s.w.s.m.m.a.RequestMappingHandlerMapping-register] - Mapped ""{[/SimpleSpringBootExample/TOD],methods=[GET],produces=[application/json]}"" onto public org.springframework.http.ResponseEntity com.mybiz.application.controller.BaseController.indexHandlerF(javax.servlet.http.HttpServletRequest)
2022-04-29 00:07:54 INFO [o.s.w.s.m.m.a.RequestMappingHandlerMapping-register] - Mapped ""{[/SimpleSpringBootExample/hello],methods=[GET]}"" onto public org.springframework.http.ResponseEntity com.mybiz.application.controller.BaseController.indexHandlerB(javax.servlet.http.HttpServletRequest)
</pre>
<p>whereas on the other laptop no mapping messages are seen. That'd explain the whitelabel error page - no mapping is occuring. But why?</p>
<p>TIA,</p>
<p>Still-learning Steve</p>
",20,1,0,1,spring-boot,2022-04-29 09:48:41,2022-04-29 09:48:41,2022-04-29 23:23:04,so far so good  i m thinking this must be some security issue on the nd laptop  how can i prove disprove this assertion  if proven  must i execute temporarily assume admin rights and launch this jar file as a service   update  upon closer examination of the startup screens on personal laptop i see mappings occuring    whereas on the other laptop no mapping messages are seen  that d explain the whitelabel error page   no mapping is occuring  but why  tia  still learning steve
754,754,18980627,72050161,GeoPy SSL Certificate Verify Failed: certificate has expired (_ssl.c:1056),"<p>UPDATE:</p>
<p>Okay, so I was finally able to figure it out. Apparently I accidentally installed something through <code>pip</code> before leaving the other day, which had some sort of conflict with GeoPy. The package in question was called <code>device</code>, and uninstalling it fixed everything.</p>
<hr />
<p>OP:</p>
<p>my first post here.
I'm still learning python, so please excuse any newbie mistakes.</p>
<p>My OS is Windows 10, and I am using Python 3.7.3.
I am attempting to get an address using the geopy API, using the geolocator.reverse() function, to which I supply a latitude and longitude which I get using os.popen:</p>
<pre><code>lat, lon = os.popen('curl ipinfo.io/loc').read().split(',')
print(lat, lon)
</code></pre>
<p>and I am using the following code to get the address:</p>
<pre><code>geoLoc = Nominatim(user_agent=&quot;GetLoc&quot;, scheme='http')
locname = geoLoc.reverse(str(lat)+&quot;,&quot;+str(lon))
print(location.address)
</code></pre>
<p>This was working fine as of 3:30 PM yesterday afternoon, and I saved it before leaving the office.</p>
<p>However, today when I came back, the same code no longer works (I can only assume there was an update, or someone changed something on my machine while I was gone), and generates a very long error message, which I will post below.</p>
<p>I have looked around for people that have had the same problem, and while I <em>did</em> find some who experienced a similar issue, none of the solutions worked. I tried updating ssl, Geopy, GeoCoders, and certifi, as well as re-installing all of these, but that didn't make any difference.</p>
<p>Here is all the code I am using for this simple script:</p>
<pre><code>import geopy.geocoders
from geopy.geocoders import Nominatim
import os 
import certifi
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
# Get ip information
lat, lon = os.popen('curl ipinfo.io/loc').read().split(',')
print(lat, lon)

# calling the Nominatim tool
geoLoc = Nominatim(user_agent=&quot;GetLoc&quot;, scheme='http') #Program crashes here

# entering the location name
location = geoLoc.reverse(str(lat)+&quot;,&quot;+str(lon))

# print address
print(location.address)
</code></pre>
<p>And below is the full output + error messages it's giving me. Is there something wrong with the GeoPy API? I spent perhaps 4 hours looking for a solution, but never found anything that worked.</p>
<pre><code>C:\Python Scripts&gt;python geo.py
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    17  100    17    0     0    144      0 --:--:-- --:--:-- --:--:--   145
34.7304 -86.5859

Traceback (most recent call last):
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\packages\urllib3\connectionpool.py&quot;, line 541, in urlopen
    body=body, headers=headers)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\packages\urllib3\connectionpool.py&quot;, line 366, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\http\client.py&quot;, line 1229, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\http\client.py&quot;, line 1275, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\http\client.py&quot;, line 1224, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\http\client.py&quot;, line 1016, in _send_output
    self.send(msg)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\http\client.py&quot;, line 956, in send
    self.connect()
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\packages\urllib3\connectionpool.py&quot;, line 131, in connect
    ssl_version=resolved_ssl_version)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\packages\urllib3\util.py&quot;, line 617, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\ssl.py&quot;, line 412, in wrap_socket
    session=session
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\ssl.py&quot;, line 853, in _create
    self.do_handshake()
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\ssl.py&quot;, line 1117, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1056)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\adapters.py&quot;, line 319, in send
    timeout=timeout
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\packages\urllib3\connectionpool.py&quot;, line 574, in urlopen
    raise SSLError(e)
requests.packages.urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1056)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\geopy\adapters.py&quot;, line 448, in _request
    resp = self.session.get(url, timeout=timeout, headers=headers)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 369, in get
    return self.request('GET', url, **kwargs)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 357, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 480, in send
    history = [resp for resp in gen] if allow_redirects else []
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 480, in &lt;listcomp&gt;
    history = [resp for resp in gen] if allow_redirects else []
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 144, in resolve_redirects
    allow_redirects=False,
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\sessions.py&quot;, line 460, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\adapters.py&quot;, line 358, in send
    raise SSLError(e)
requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1056)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;geo.py&quot;, line 15, in &lt;module&gt;
    location = geoLoc.reverse(str(lat)+&quot;,&quot;+str(lon))
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\geopy\geocoders\nominatim.py&quot;, line 362, in reverse
    return self._call_geocoder(url, callback, timeout=timeout)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\geopy\geocoders\base.py&quot;, line 368, in _call_geocoder
    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\geopy\adapters.py&quot;, line 438, in get_json
    resp = self._request(url, timeout=timeout, headers=headers)
  File &quot;C:\Users\CalebC\AppData\Local\Programs\Python\Python37\lib\site-packages\geopy\adapters.py&quot;, line 460, in _request
    raise GeocoderUnavailable(message)
geopy.exc.GeocoderUnavailable: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1056)
</code></pre>
",61,0,0,4,python;ssl;geopy;certifi,2022-04-29 02:29:02,2022-04-29 02:29:02,2022-04-29 18:54:38,update  okay  so i was finally able to figure it out  apparently i accidentally installed something through pip before leaving the other day  which had some sort of conflict with geopy  the package in question was called device  and uninstalling it fixed everything  op  and i am using the following code to get the address  this was working fine as of   pm yesterday afternoon  and i saved it before leaving the office  however  today when i came back  the same code no longer works  i can only assume there was an update  or someone changed something on my machine while i was gone   and generates a very long error message  which i will post below  i have looked around for people that have had the same problem  and while i did find some who experienced a similar issue  none of the solutions worked  i tried updating ssl  geopy  geocoders  and certifi  as well as re installing all of these  but that didn t make any difference  here is all the code i am using for this simple script  and below is the full output   error messages it s giving me  is there something wrong with the geopy api  i spent perhaps  hours looking for a solution  but never found anything that worked 
755,755,12002570,72055546,Is this pointer always a runtime construct,"<p>I am learning about the <code>this</code> pointer in C++. And i came across the following <a href=""https://timsong-cpp.github.io/cppwp/n4659/expr.const#2"" rel=""nofollow noreferrer"">statement</a> from the standard:</p>
<blockquote>
<p>An expression <code>e</code> is a core constant expression unless the evaluation of <code>e</code>, following the rules of the abstract machine, would evaluate one of the following expressions:</p>
<ul>
<li><code>this</code>, except in a constexpr function or a constexpr constructor that is being evaluated as part of <code>e</code>;</li>
</ul>
</blockquote>
<hr />
<p>I do not understand the meaning of the above quoted statement. I thought that the <code>this</code> pointer is a <strong>runtime construct</strong> and so it cannot be used in compile-time contexts. But the above statement seems to suggest otherwise in the mentioned contexts. <strong>My first question is</strong> what does it mean. Can someone give some example so that i can understand the meaning of that statement.</p>
<hr />
<p>I tried to create an <a href=""https://godbolt.org/z/9cE7f6h99"" rel=""nofollow noreferrer"">example</a> to better understand the meaning of the statement by myself but the program did not work as expected(meaning that it gave error while i thought that it should work according to the quoted statement):</p>
<pre><code>struct Person 
{
    constexpr int size()
    {
      return 4;
    }
    void func()
    {
            int arr[this-&gt;size()]; //here size is a constexpr member function and so &quot;this&quot; should be a core constant expression and arr should not be VLA
    }
    
};
</code></pre>
<p><a href=""https://godbolt.org/z/9cE7f6h99"" rel=""nofollow noreferrer"">Demo</a></p>
<p>In the above program, i thought that we're allowed to use the expression <code>this-&gt;size()</code> as the size of an array(which must be a compile time constant) because <code>size</code> is <em>constexpr</em> and so the quoted statement applies and so the program should compile. Also, since <code>size</code> is <em>constexpr</em> <code>arr</code> should not be VLA. But to my surprise, the <code>arr</code> seems to be VLA and the program also doesn't compile in msvc(because msvc don't have vla i think). So <strong>my second question is</strong> why doesn't the quoted statement applicable in the above example and why is <code>arr</code> VLA? I mean <code>size</code> is constexpr so <code>arr</code> shouldn't be VLA.</p>
",101,1,0,5,c++;arrays;language-lawyer;this;constexpr,2022-04-29 14:41:52,2022-04-29 14:41:52,2022-04-29 15:52:03,i am learning about the this pointer in c    and i came across the following  from the standard  an expression e is a core constant expression unless the evaluation of e  following the rules of the abstract machine  would evaluate one of the following expressions  i do not understand the meaning of the above quoted statement  i thought that the this pointer is a runtime construct and so it cannot be used in compile time contexts  but the above statement seems to suggest otherwise in the mentioned contexts  my first question is what does it mean  can someone give some example so that i can understand the meaning of that statement  i tried to create an  to better understand the meaning of the statement by myself but the program did not work as expected meaning that it gave error while i thought that it should work according to the quoted statement    in the above program  i thought that we re allowed to use the expression this  gt size   as the size of an array which must be a compile time constant  because size is constexpr and so the quoted statement applies and so the program should compile  also  since size is constexpr arr should not be vla  but to my surprise  the arr seems to be vla and the program also doesn t compile in msvc because msvc don t have vla i think   so my second question is why doesn t the quoted statement applicable in the above example and why is arr vla  i mean size is constexpr so arr shouldn t be vla 
756,756,6537007,71689560,Error running model trained on cloud on local machine,"<p>I trained a reinforcement learning model using a GPU instance from Lambda Labs cloud service.  The library I used for training is stable_baselines3.
When I try to run the model on my local machine I get the following error which I can't figure out why.</p>
<p>error:</p>
<pre><code>AttributeError: Can't get attribute 'RandomNumberGenerator._generator_ctor' on &lt;module 'gym.utils.seeding' 
</code></pre>
<p>I ensure that the environment in which the model was trained is the same as the one on my local machine.
Any ideas of why this is happening?  When I train the model on my local machine it works just fine.</p>
",47,1,0,3,python;openai-gym;stable-baselines,2022-03-31 14:21:11,2022-03-31 14:21:11,2022-04-29 14:30:04,error 
757,757,8128190,52721662,Clustering images using unsupervised Machine Learning,"<p>I have a database of images that contains identity cards, bills and passports.<br>
I want to classify these images into different groups (i.e identity cards, bills and passports).<br>
As I read about that, one of the ways to do this task is clustering (since it is going to be unsupervised).<br>
The idea for me is like this: the clustering will be based on the similarity between images (i.e images that have similar features will be grouped together).<br>
I know also that this process can be done by using k-means.<br>
So the problem for me is about features and using images with K-means.<br>
If anyone has done this before, or has a clue about it, please would you recommend some links to start with or suggest any features that can be helpful.</p>
",4167,3,7,5,python;computer-vision;cluster-analysis;k-means;unsupervised-learning,2018-10-09 18:28:17,2018-10-09 18:28:17,2022-04-29 12:58:40,
758,758,1361401,23974317,Weka machine learning - Interpeting naive bayes,"<p>I got a training dataset of ill horses, the data it contains is about surgeries and diseases. Some of the fields of the registers are like: temperature of the horse, age, pulse, respiratory rate etc .... </p>

<p>What I want to do a clasificator on the live/dead/euthanized column of every row. What I am asked to check is:</p>

<ul>
<li>Think about hypothesis of independence of variables</li>
<li>Check if I got enought number of elements to obtain reliable probabilities</li>
</ul>

<p>The dataset had like 25% of missing values and them where imputated using MIMMI imputation.</p>

<p>Thinking about the possibility of getting reliable probabilities, I can see that the training dataset is a little unbalanced: 179 horses live and 121 die (dead + euthanized). But im not really sure of that. Any help with that two questions would be so much helpful for me.</p>

<pre><code>=== Run information ===

Scheme:weka.classifiers.bayes.NaiveBayes 
Relation:     horseColic-weka.filters.unsupervised.attribute.Remove-R25-27
Instances:    300
Attributes:   24
              surgery
              age
              id
              temp
              pulse
              respRate
              tempExtrem
              periPulse
              mucMemb
              capRefT
              pain
              peri
              abdDist
              ngTube
              ngReflux
              ngRPH
              feces
              abd
              pCellVol
              totProt
              abdCentApp
              abdCentTotProt
              outc
              surgLes
Test mode:10-fold cross-validation

=== Classifier model (full training set) ===

Naive Bayes Classifier

                                  Class
Attribute                         lived         died   euthanized
                                 (0.59)       (0.26)       (0.15)
==================================================================
surgery
  yes                               97.0         59.0         28.0
  no                                84.0         20.0         18.0
  [total]                          181.0         79.0         46.0

age
  adult                            168.0         67.0         44.0
  young                             13.0         12.0          2.0
  [total]                          181.0         79.0         46.0

id
  mean                      1009274.0202 1452556.3598  751596.8611
  std. dev.                 1431022.1677 1887025.7703  989556.6807
  weight sum                         179           77           44
  precision                    16915.735    16915.735    16915.735

temp
  mean                           34.8733      35.0055       33.054
  std. dev.                      10.2335      13.0545      14.9588
  weight sum                         179           77           44
  precision                       0.9275       0.9275       0.9275

pulse
  mean                           29.2039      33.2115      29.0187
  std. dev.                      10.8578      14.6404      16.7248
  weight sum                         179           77           44
  precision                       0.9107       0.9107       0.9107

respRate
  mean                           15.0771      16.9169      15.9348
  std. dev.                       8.9803       7.0278       8.1221
  weight sum                         179           77           44
  precision                       0.8667       0.8667       0.8667

tempExtrem
  normal                            82.0         16.0         12.0
  warm                              36.0          7.0          3.0
  cool                              53.0         48.0         25.0
  cold                              12.0         10.0          8.0
  [total]                          183.0         81.0         48.0

periPulse
  normal                           133.0         22.0         11.0
  increased                          5.0          8.0          7.0
  reduced                           43.0         47.0         25.0
  absent                             2.0          4.0          5.0
  [total]                          183.0         81.0         48.0

mucMemb
  normal-pink                       95.0          9.0          7.0
  bright-pink                       23.0         13.0          6.0
  pale-pink                         37.0         19.0         12.0
  pale-cyanotic                     16.0         17.0         12.0
  bright-red                         7.0         14.0          8.0
  dark-cyanotic                      7.0         11.0          5.0
  [total]                          185.0         83.0         50.0

capRefT
  short                            153.0         46.0         23.0
  long                              28.0         33.0         23.0
  long2                              1.0          1.0          1.0
  [total]                          182.0         80.0         47.0

pain
  no-pain                           53.0          6.0          8.0
  depressed                         42.0         21.0         14.0
  inte-mild-pain                    64.0         10.0          8.0
  inte-severe-pain                  12.0         18.0         12.0
  cont-severe-pain                  13.0         27.0          7.0
  [total]                          184.0         82.0         49.0

peri
  hypermotile                       42.0          7.0          7.0
  normal                            22.0          8.0          5.0
  hypomotile                        90.0         37.0         17.0
  absent                            29.0         29.0         19.0
  [total]                          183.0         81.0         48.0

abdDist
  none                              88.0         17.0         13.0
  slight                            53.0         18.0          8.0
  moderate                          28.0         30.0         14.0
  severe                            14.0         16.0         13.0
  [total]                          183.0         81.0         48.0

ngTube
  none                              79.0         40.0         27.0
  slight                            90.0         32.0         15.0
  significant                       13.0          8.0          5.0
  [total]                          182.0         80.0         47.0

ngReflux
  none                             149.0         50.0         30.0
  much                              17.0         15.0          6.0
  less                              16.0         15.0         11.0
  [total]                          182.0         80.0         47.0

ngRPH
  mean                           11.3797      13.0882       8.0606
  std. dev.                       2.3535       3.2916       5.1673
  weight sum                         179           77           44
  precision                       0.7917       0.7917       0.7917

feces
  normal                            77.0         14.0         10.0
  increased                         16.0         14.0          8.0
  decreased                         44.0         15.0         11.0
  absent                            46.0         38.0         19.0
  [total]                          183.0         81.0         48.0

abd
  normal                            48.0         13.0          4.0
  other                             39.0          5.0          7.0
  firm-large-intestine              18.0          8.0          6.0
  dist-small-intest                 32.0         24.0          8.0
  distended-large-intest            47.0         32.0         24.0
  [total]                          184.0         82.0         49.0

pCellVol
  mean                           31.0162      47.0465      46.0112
  std. dev.                      14.1207      18.5468       17.672
  weight sum                         179           77           44
  precision                       0.9518       0.9518       0.9518

totProt
  mean                           42.6539       41.451      43.7936
  std. dev.                      16.9138      18.6362      19.3247
  weight sum                         179           77           44
  precision                       0.9432       0.9432       0.9432

abdCentApp
  clear                            112.0         25.0         10.0
  cloudy                            54.0         22.0         20.0
  serosanguinous                    16.0         33.0         17.0
  [total]                          182.0         80.0         47.0

abdCentTotProt
  mean                           16.1341      21.1634      14.3203
  std. dev.                       6.8038       4.9109       8.6619
  weight sum                         179           77           44
  precision                       0.8837       0.8837       0.8837

surgLes
  yes                               94.0         70.0         30.0
  no                                87.0          9.0         16.0
  [total]                          181.0         79.0         46.0



Time taken to build model: 0.01 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances         216               72      %
Incorrectly Classified Instances        84               28      %
Kappa statistic                          0.5134
Mean absolute error                      0.1965
Root mean squared error                  0.3803
Relative absolute error                 52.8451 %
Root relative squared error             88.2672 %
Total Number of Instances              300     

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.777     0.198      0.853     0.777     0.813      0.873    lived
                 0.675     0.175      0.571     0.675     0.619      0.871    died
                 0.568     0.082      0.543     0.568     0.556      0.824    euthanized
Weighted Avg.    0.72      0.175      0.735     0.72      0.725      0.865

=== Confusion Matrix ===

   a   b   c   &lt;-- classified as
 139  28  12 |   a = lived
  16  52   9 |   b = died
   8  11  25 |   c = euthanized
</code></pre>
",116,1,0,2,weka;bayesian,2014-06-01 02:01:04,2014-06-01 02:01:04,2022-04-29 12:54:30,i got a training dataset of ill horses  the data it contains is about surgeries and diseases  some of the fields of the registers are like  temperature of the horse  age  pulse  respiratory rate etc       what i want to do a clasificator on the live dead euthanized column of every row  what i am asked to check is  the dataset had like   of missing values and them where imputated using mimmi imputation  thinking about the possibility of getting reliable probabilities  i can see that the training dataset is a little unbalanced   horses live and  die  dead   euthanized   but im not really sure of that  any help with that two questions would be so much helpful for me 
759,759,4128453,72051527,packet drop Netfilter module crash the system,"<p>I'm learning Linux Netfilter module development with the following demo module code:</p>
<pre><code>#include &lt;linux/kernel.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/netfilter.h&gt;
#include &lt;linux/netfilter_ipv4.h&gt;

static struct nf_hook_ops nfho;

unsigned int hook_func(void *priv, struct sk_buff *skb, const struct nf_hook_state *state)
{
  printk(KERN_INFO &quot;packet dropped\n&quot;);
  return NF_DROP;
}

static int drop_init(void)
{
  nfho.hook = hook_func;
  nfho.hooknum = NF_INET_PRE_ROUTING;
  nfho.pf = PF_INET;
  nfho.priority = NF_IP_PRI_FIRST;
  nf_register_net_hook(&amp;init_net, &amp;nfho);
  return 0;
}

static void drop_exit(void)
{
  nf_unregister_net_hook(&amp;init_net,&amp;nfho);
}

module_init(drop_init);

module_exit(drop_exit);

MODULE_LICENSE(&quot;GPL&quot;);
</code></pre>
<p>and this Makefile:</p>
<pre><code>obj-m += drop-packet.o
all:
        make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
clean:
        make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
</code></pre>
<p>My current Kernel version is <code>4.15.0-176-generic</code>. So in the code level, I did some investigations as follows:</p>
<ul>
<li>change <code>nf_register_hook</code> to <code>nf_register_net_hook</code> and <code>nf_unregister_hook</code> to <code>nf_unregister_net_hook</code></li>
<li>hook function uses the new signature: <code>unsigned int hook_func(void *priv, struct sk_buff *skb,  const struct nf_hook_state *state)</code></li>
</ul>
<p>So far, I can build the module by running <code>make</code> command. I didn't see any potential error message  from the console output:</p>
<pre><code>make -C /lib/modules/4.15.0-176-generic/build M=/home/DIR/jbao6/develop/kernel/drop-packet modules
make[1]: Entering directory '/usr/src/linux-headers-4.15.0-176-generic'
  Building modules, stage 2.
  MODPOST 1 modules
make[1]: Leaving directory '/usr/src/linux-headers-4.15.0-176-generic'
</code></pre>
<p>But when I tried to install the module by running the <code>insmod drop-packet.ko</code> command. The system crashes. In fact the <code>insmod</code> command can return successfully without any message. So at the beginning, I thought it works. But the shell becomes blocked and unresponsive.</p>
<p>I'm running this demo module on a remote virtual machine and each time I need to reboot the VM.</p>
<p>And I did some experiments based on this document <a href=""https://sysprog21.github.io/lkmpg/"" rel=""nofollow noreferrer"">https://sysprog21.github.io/lkmpg/</a>. I can build and install the simplest <strong>hello-world</strong> module successfully like this one:</p>
<pre><code>#include &lt;linux/kernel.h&gt; /* Needed for pr_info() */ 
#include &lt;linux/module.h&gt; /* Needed by all modules */ 
 
int init_module(void) 
{ 
    pr_info(&quot;Hello world 1.\n&quot;); 
 
    /* A non 0 return means init_module failed; module can't be loaded. */ 
    return 0; 
} 
 
void cleanup_module(void) 
{ 
    pr_info(&quot;Goodbye world 1.\n&quot;); 
} 
 
MODULE_LICENSE(&quot;GPL&quot;);
</code></pre>
<p>Is there any solution or ideas to troubleshoot my problem.</p>
",22,0,0,2,linux;netfilter,2022-04-29 05:48:36,2022-04-29 05:48:36,2022-04-29 05:48:36,i m learning linux netfilter module development with the following demo module code  and this makefile  my current kernel version is     generic  so in the code level  i did some investigations as follows  so far  i can build the module by running make command  i didn t see any potential error message  from the console output  but when i tried to install the module by running the insmod drop packet ko command  the system crashes  in fact the insmod command can return successfully without any message  so at the beginning  i thought it works  but the shell becomes blocked and unresponsive  i m running this demo module on a remote virtual machine and each time i need to reboot the vm  and i did some experiments based on this document   i can build and install the simplest hello world module successfully like this one  is there any solution or ideas to troubleshoot my problem 
760,760,6365720,72045740,How to read relational R:BASE data into a data frame(s) in R,"<p>My employer historically used R:BASE to store important information, but we no longer have access to the program and the data are sitting unused. It's four files (survey.rx1, survey.rx2, survey.rx3, and survey.rx4) that together form a relational database. I believe these are binary data from searching for information on the program and &quot;.rx&quot; extension.</p>
<p>I would like to convert these data into a format that is usable in R or Excel (.csv, .xls, .xlsx, .RDS, etc.), but the free trial version of R:BASE does not allow exports and trying to import the data into MS Excel isn't working (using the get data, from file, from folder feature. It recognizes it as binary and shows some metadata but doesn't display the actual content).</p>
<p>Trying to read these data in R I explored using readBin, but am struggling to determine an appropriate value for &quot;what&quot; as the columns represent different modes and I don't have a connection object to provide that information. I explored using RevoScaleR which looked like it could be relevant but the <a href=""https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler"" rel=""nofollow noreferrer"">Documentation</a> says it was built on R 3.5.2 and the <a href=""https://docs.microsoft.com/en-us/previous-versions/machine-learning-server/install/machine-learning-server-install"" rel=""nofollow noreferrer"">Machine Learning Server</a> you need to run it is no longer being supported as of this year.</p>
<p>So, if anyone knows how to convert these data or knows of a free alternative to R:BASE that works for these data types I would be very appreciative. Thank you!</p>
",31,0,1,4,r;excel;relational-database;archive,2022-04-28 20:13:12,2022-04-28 20:13:12,2022-04-28 20:56:36,my employer historically used r base to store important information  but we no longer have access to the program and the data are sitting unused  it s four files  survey rx  survey rx  survey rx  and survey rx  that together form a relational database  i believe these are binary data from searching for information on the program and   rx  extension  i would like to convert these data into a format that is usable in r or excel   csv   xls   xlsx   rds  etc    but the free trial version of r base does not allow exports and trying to import the data into ms excel isn t working  using the get data  from file  from folder feature  it recognizes it as binary and shows some metadata but doesn t display the actual content   trying to read these data in r i explored using readbin  but am struggling to determine an appropriate value for  what  as the columns represent different modes and i don t have a connection object to provide that information  i explored using revoscaler which looked like it could be relevant but the  says it was built on r    and the  you need to run it is no longer being supported as of this year  so  if anyone knows how to convert these data or knows of a free alternative to r base that works for these data types i would be very appreciative  thank you 
761,761,18531462,72046363,Decreasing Accuracy while switching to the CIFAR Database,"<p>I used the code from <a href=""https://builtin.com/data-science/guide-logistic-regression-tensorflow-20"" rel=""nofollow noreferrer"">https://builtin.com/data-science/guide-logistic-regression-tensorflow-20</a> as a starting point for a logistic regression machine learning project.
When I applied the code on the Mnist Database, I got a good accuracy. However when I wanted to apply the same code on the CIFAR database (<a href=""https://www.cs.toronto.edu/%7Ekriz/cifar.html"" rel=""nofollow noreferrer"">https://www.cs.toronto.edu/~kriz/cifar.html</a>) the accuracy could not exceed 20% and it keeps on increasing and decreasing through the training steps.
Here is the code:</p>
<pre><code>(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)
y_train=np.squeeze(y_train)
y_test=np.squeeze(y_test)
x_train, x_test = x_train.reshape([-1, 3072]), x_test.reshape([-1, 3072])
x_train, x_test = x_train / 255., x_test / 255.
num_classes = 10 # 0 to 9
num_features = 3072 # 32*32*3
# Training parameters.
learning_rate = 0.01
training_steps = 1000
batch_size = 1024 

train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))
train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)
W = tf.Variable(tf.ones([num_features, num_classes]), name=&quot;weight&quot;)
b = tf.Variable(tf.zeros([num_classes]), name=&quot;bias&quot;)

def logistic_regression(x):

    # Apply softmax to normalize the logits to a probability distribution.

    return tf.nn.softmax(tf.matmul(x, W) + b)

def cross_entropy(y_pred, y_true):

    # Encode label to a one hot vector.

    y_true = tf.one_hot(y_true, depth=num_classes)

    # Clip prediction values to avoid log(0) error.

    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)

    # Compute cross-entropy.

    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))

def accuracy(y_pred, y_true):

    # Predicted class is the index of the highest score in prediction vector (i.e. argmax).

    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))

    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

optimizer = tf.optimizers.SGD(learning_rate)

# Optimization process. 

def run_optimization(x, y):

# Wrap computation inside a GradientTape for automatic differentiation.

    with tf.GradientTape() as g:

        pred = logistic_regression(x)

        loss = cross_entropy(pred, y)

    # Compute gradients.

    gradients = g.gradient(loss, [W, b])

  

    # Update W and b following gradients.

    optimizer.apply_gradients(zip(gradients, [W, b]))

# Run training for the given number of steps.

for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):

    # Run the optimization to update W and b values.

    run_optimization(batch_x, batch_y)

    

    if step % 50 == 0:

        pred = logistic_regression(batch_x)

        loss = cross_entropy(pred, batch_y)

        acc = accuracy(pred, batch_y)

        print(&quot;step: %i, loss: %f, accuracy: %f&quot; % (step, loss, acc))

pred = logistic_regression(x_test)

print(&quot;Test Accuracy: %f&quot; % accuracy(pred, y_test))
print(&quot;Loss: %f&quot; % cross_entropy(pred, y_test))
</code></pre>
<p>How could I get better accuracy and why is it so low when I switch the database ?</p>
",16,0,0,5,python;tensorflow;machine-learning;logistic-regression;mnist,2022-04-28 20:51:41,2022-04-28 20:51:41,2022-04-28 20:51:41,how could i get better accuracy and why is it so low when i switch the database  
762,762,11984904,72043397,How to find feature importance for each class in multiclass classification,"<p>I have written code to find the importance of each feature in the entire dataset for multiclass classification. Now I want to find feature importance for each class in multiclass classification, i.e. I want to find the list of features (for each class) that are more important to classify that individual classes.</p>
<pre><code>from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
model = DecisionTreeClassifier()

model.fit(x3, y3)

importance = model.feature_importances_

for i,v in enumerate(importance):
    print('Feature[%0d]:%s, Score: %.6f' % (i,df.columns[i],v))
    
plt.subplots(figsize=(15,7))
plt.bar([x for x in range(len(importance))], importance)
plt.xlabel('Feature index')
plt.ylabel('Feature importance score')
plt.xticks(rotation=90)
plt.xticks(np.arange(0,len(df.columns)-2, 2.0))
plt.show()
</code></pre>
<p>EDIT (28-04-2022):</p>
<p>I read a paper titled <a href=""https://www.scitepress.org/papers/2018/66398/66398.pdf"" rel=""nofollow noreferrer"">Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization</a>; quoting:</p>
<blockquote>
<p>On the evaluate section, we fist extract the 80 traffic features from the dataset and clarify the best short feature set to detect each attack family using RandomForestRegressor algorithm. Afterwards, we examine the performance and accuracy of the selected features
with seven common machine learning algorithms.</p>
</blockquote>
<p>Can anyone explain how this is done?<a href=""https://i.stack.imgur.com/ha1G0.jpg"" rel=""nofollow noreferrer"">click for picture from that paper</a></p>
",307,1,-1,5,python;machine-learning;classification;decision-tree;multiclass-classification,2022-04-28 17:36:22,2022-04-28 17:36:22,2022-04-28 19:35:42,i have written code to find the importance of each feature in the entire dataset for multiclass classification  now i want to find feature importance for each class in multiclass classification  i e  i want to find the list of features  for each class  that are more important to classify that individual classes  edit       i read a paper titled   quoting  can anyone explain how this is done 
763,763,5848588,61490893,I Need Assistance to Interpret the Score as a means to decide on the best regressor for my ML model,"<p>I am working on a Model for Machine Learning and was able to generate the scores of the processes.  I am not sure how to use them to make a decision on which is the best kernel to implement for my final prediction or, more specifically, how to reach the best value percentage of the ensemble from a stack.  </p>

<p>For example, I am using these functions to generate the scores for each one of my regressors.</p>

<pre><code>n_folds = 5
def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=""neg_mean_squared_error"", cv = kf))
    return(rmse)

def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))
</code></pre>

<p>Then, I need to build the ensemble to generate my final prediction, utilizing the following two components.  My Score Results are posted last.  Given the Score results, how should I build the ensemble?  To clarify, the stacked_pred is derived from:</p>

<pre><code>stacked_averaged_models = Stacking_Averaged_Models(base_models = (ENet, GBoost, KRR), meta_model = lasso)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
</code></pre>

<p>Which is then completed with the following ensemble to generate the final prediction list:</p>

<pre><code>ensemble = stacked_pred*0.4 + xgb_pred*0.2 + lgb_pred*0.2 + XGBReg_pred*0.2
</code></pre>

<p>The multiple score's report is posted below and provides the list that I can use to build a better ensemble (I assume).  Please provide some guidance that explain the meaning of each score so I can use that understanding to build the ultimate ensemble for my prediction.   </p>

<pre><code>[model]  Lasso score(cv): 0.1117 (0.0073)
[model]  ElasticNet score(cv): 0.1116 (0.0074)
[model]  Kernel Ridge score(cv): 0.1158 (0.0075)
[model]  Gradient Boosting score(cv): 0.1162 (0.0086)
[model]  Xgboost score(cv): 0.1164 (0.0060)
[model]  LGBM score(cv): 0.1173 (0.0061)
[model]  Averaged base models score(cv): 0.1088 (0.0075)
[model]  Stacked averaged-models score(cv): 0.1084 (0.0070)
[model]  stacked regressor: 0.0779745304893225
[model]  xgboost: 0.07845754741653387
[model]  LightGBMs: 0.07201089217729068
[model]  XGBReg: 0.05490294827735702
[model]  ensemble: 0.0685856065104715
</code></pre>
",31,0,1,3,python;machine-learning;prediction,2020-04-29 03:45:07,2020-04-29 03:45:07,2022-04-28 17:36:06,i am working on a model for machine learning and was able to generate the scores of the processes   i am not sure how to use them to make a decision on which is the best kernel to implement for my final prediction or  more specifically  how to reach the best value percentage of the ensemble from a stack    for example  i am using these functions to generate the scores for each one of my regressors  then  i need to build the ensemble to generate my final prediction  utilizing the following two components   my score results are posted last   given the score results  how should i build the ensemble   to clarify  the stacked_pred is derived from  which is then completed with the following ensemble to generate the final prediction list  the multiple score s report is posted below and provides the list that i can use to build a better ensemble  i assume    please provide some guidance that explain the meaning of each score so i can use that understanding to build the ultimate ensemble for my prediction    
764,764,16945062,72038914,Which python version should I use for machine learning?,"<p>I am totally new to machine learning, after going through many tutorials I am bit confused over which python version is most stable for libraries like tensorflow and keras ?</p>
<p>Some are suggesting python 3.7 while some are telling to use latest one. Which one should I use, any suggestions? Please help!</p>
",63,1,0,3,python;tensorflow;keras,2022-04-28 11:53:14,2022-04-28 11:53:14,2022-04-28 11:57:36,i am totally new to machine learning  after going through many tutorials i am bit confused over which python version is most stable for libraries like tensorflow and keras   some are suggesting python   while some are telling to use latest one  which one should i use  any suggestions  please help 
765,765,17383009,71943613,how do i port my machine learning model from python to java web app?,"<p>so I've been developing some machine learning models using sklearn and tensorflow in python .
and I want to integrate it into a java web app.
so far I've been saving my models as .joblib
any idea how i can do it ?
I know this is a general question, but if someone can tell me if its possible , or do i have to retrain the models in java using java machine learning libraries a</p>
<p>some python code i used:</p>
<pre><code>clf = MLPClassifier(solver='adam', alpha=Alpha,hidden_layer_sizes=(hid1,hid2), random_state=1,max_iter=10000)
clf.fit(x_train, y_train)

y_test_pred = clf.predict(x_test)
y_train_pred = clf.predict(x_train)
</code></pre>
",78,1,0,5,python;java;tensorflow;scikit-learn;web-applications,2022-04-20 22:37:11,2022-04-20 22:37:11,2022-04-28 10:18:05,some python code i used 
766,766,18968737,72035391,AzureML schema &quot;list index out of range&quot; error,"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script"" rel=""nofollow noreferrer"">document</a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below</p>
<pre><code>{
    &quot;Inputs&quot;: {
         &quot;input_1&quot; : &quot;content&quot;
         &quot;input_2: : &quot;content&quot;
         ......
    },
    &quot;GlobalParameters&quot;: 0
}
</code></pre>
",59,1,1,3,python;azure;azure-machine-learning-service,2022-04-28 02:47:05,2022-04-28 02:47:05,2022-04-28 07:28:04,i developed a machine learning model using azure ml s clustering  few of the requests made from the cluster are triggering  http error  i followed the  to do modifications in my swagger json file  finally ended up with  list index out of range  error  it seems to be having issue with the global parameter but i am no sure about it  i am using the api from postman with some default headers like mentioned in the body below
767,767,16709885,69675460,getting Fatal error in launcher: Unable to create process using &#39;&quot;c:\python37\python.exe&quot; &quot;C:\Python37\Scripts\pyrcc5.exe&quot;,"<p>So i'm a beginner at machine learning and i want to try out this project i found it youtube (<a href=""https://www.youtube.com/watch?v=IOI0o3Cxv9Q&amp;t=350s"" rel=""nofollow noreferrer"">link here</a>) but the problem is when i get onto the step where i do <code>pyrcc5 -o resources.py resources.qrc</code> in the command prompt it gives the error:</p>
<blockquote>
<p>Fatal error in launcher:<br />
Unable to create process using '&quot;c:\python37\python.exe&quot;  &quot;C:\Python37\Scripts\pyrcc5.exe&quot; -o resources.py resources.qrc'</p>
</blockquote>
<p>which i don't understand why because i did everything the guy did in the video</p>
",135,1,0,1,python,2021-10-22 16:11:32,2021-10-22 16:11:32,2022-04-28 07:10:45,so i m a beginner at machine learning and i want to try out this project i found it youtube    but the problem is when i get onto the step where i do pyrcc  o resources py resources qrc in the command prompt it gives the error  which i don t understand why because i did everything the guy did in the video
768,768,11283020,72034035,How do I use numpy vectors as input for Trax?,"<p>I want to be able to use random numpy vectors as input for the Trax machine learning library. I find it helpful to be able to define my own simple inputs and outputs to see how the models are working. Am I doing this the right way, or am I missing something obvious and using the library totally wrong?</p>
<p><a href=""https://colab.research.google.com/drive/1fb7VMh5R53x5riTnDz3GnOySguiC-aUy?usp=sharing"" rel=""nofollow noreferrer"">Colab notebook with working example</a></p>
<p>Make X matrix (Nx3) for inputs.<br>
Make Y vector (Nx1) equal to the dot product between X and (1,0,0).<br>
So, this makes y_i simply indicate which side of the yz plane the vector is on. The model would be learning this boundary.</p>
<pre><code>def generate_xy(n):
  X = np.random.normal(loc=0, scale=1, size=(n,3))
  Y = np.dot(X, [1,0,0]) &gt; 0
  Y = Y.astype(int).reshape(-1,1)
  return X, Y

X, Y = generate_xy(10000)
</code></pre>
<p>I reviewed the <a href=""https://github.com/google/trax/blob/975725b6e5cf20c58896d62879755c673f4abd0d/trax/supervised/training.py"" rel=""nofollow noreferrer"">codebase at github</a> and it looks like the input is supposed to be:</p>
<blockquote>
<p>labeled_data: Iterator of batches of labeled data tuples. Each tuple
has
1+ data (input value) tensors followed by 1 label (target value)
tensor.  All tensors are NumPy ndarrays or their JAX counterparts.</p>
</blockquote>
<p>Create a very simple model:</p>
<pre><code>model = tl.Serial(
    tl.Dense(1),
    tl.Sigmoid()
)
</code></pre>
<p>Define a train task: <strong>this is the part where I'm wondering if I'm doing something very wrong :)</strong></p>
<pre><code>train_task = ts.TrainTask(
    labeled_data= zip(X,Y),
    loss_layer=tl.CategoryCrossEntropy(),
    optimizer=trax.optimizers.Adam(0.01),
    n_steps_per_checkpoint=None
)
</code></pre>
<p>Define training loop and train model</p>
<pre><code>training_loop = trax.supervised.training.Loop(model, train_task)
training_loop.run(9999) #one epoch
</code></pre>
<p>I'm not sure if this whole example is just contrived and way outside of what the library is intended to be used for, or if I'm just struggling to figure out the best way to handle inputs. Just looking for some guidance on best practices here. Thanks so much!</p>
",11,0,0,2,machine-learning;trax,2022-04-28 00:33:22,2022-04-28 00:33:22,2022-04-28 00:33:22,i want to be able to use random numpy vectors as input for the trax machine learning library  i find it helpful to be able to define my own simple inputs and outputs to see how the models are working  am i doing this the right way  or am i missing something obvious and using the library totally wrong   i reviewed the  and it looks like the input is supposed to be  create a very simple model  define a train task  this is the part where i m wondering if i m doing something very wrong    define training loop and train model i m not sure if this whole example is just contrived and way outside of what the library is intended to be used for  or if i m just struggling to figure out the best way to handle inputs  just looking for some guidance on best practices here  thanks so much 
769,769,18967131,72033082,TensorFlow Keras issues,"<p>I'm trying to learn the very basics of Keras and how to use it and I'm running into trouble getting my model to work. I am following a tutorial online (<a href=""https://youtu.be/qFJeN9V1ZsI?t=1696"" rel=""nofollow noreferrer"">https://youtu.be/qFJeN9V1ZsI?t=1696</a>) word for word and getting errors when using the model.fit() function this is the second tutorial I have tried and gotten the same result.</p>
<p>I have looked at Keras's website and read all of the variable descriptions for the functions/followed instructions on site, and cannot work around the issue.</p>
<p>I am running this in a virtual environment with TensorFlow 2.0.0 and Keras 2.3.1</p>
<p>It may be worth noting that I am on an M1 MacBook Pro (not sure if that's the issue).</p>
<p>This is my first experience with Python and machine learning.</p>
<p>Code:<a href=""https://i.stack.imgur.com/JKmH6.png"" rel=""nofollow noreferrer"">CodeImage</a></p>
<pre><code>model.fit(
x=scaled_train_samples,
y=train_labels,
batch_size=10,
epochs=30,
verbose=2,
callbacks=None,
validation_split=0.0,
validation_data=None,
shuffle=True,
class_weight=None,
sample_weight=None,
initial_epoch=0,
steps_per_epoch=None,
validation_steps=None,
validation_freq=1,
max_queue_size=10,
workers=1,
use_multiprocessing=False)
</code></pre>
<p>Issue:<a href=""https://i.stack.imgur.com/HuKgn.png"" rel=""nofollow noreferrer"">ErrorMessage</a></p>
",28,0,0,3,python-3.x;keras;tensorflow2.0,2022-04-27 23:05:36,2022-04-27 23:05:36,2022-04-27 23:22:28,i m trying to learn the very basics of keras and how to use it and i m running into trouble getting my model to work  i am following a tutorial online    word for word and getting errors when using the model fit   function this is the second tutorial i have tried and gotten the same result  i have looked at keras s website and read all of the variable descriptions for the functions followed instructions on site  and cannot work around the issue  i am running this in a virtual environment with tensorflow    and keras    it may be worth noting that i am on an m macbook pro  not sure if that s the issue   this is my first experience with python and machine learning  code  issue 
770,770,18345575,71313576,AttributeError: &#39;LinearRegression&#39; object has no attribute &#39;coef_&#39;,"<p>I am self-studying machine learning and python. I am using sklearn and I want to plot the regression line, but I get the attributeError: 'LinearRegression' object has no attribute 'coef_. Could somebody help me to fix it, thank you in advance.</p>
<pre><code>x=data['size']
y=data['price']
x_matrix=x.values.reshape(-1,1)
reg=LinearRegression()
reg.fit(x_matrix,y)
plt.scatter(x,y)
yhat= reg.coef_ * x_matrix + reg.intercept_
fig=plt.plot(x, yhat, lw=4, c=&quot;orange&quot;, label=&quot;regression line&quot;)
plt.xlabel(&quot;size&quot;, fontsize=20)
plt.ylabel(&quot;price&quot;, fontsize=20)
plt.show() 
 
AttributeError: 'LinearRegression' object has no attribute 'coef_
</code></pre>
",288,1,1,5,python;python-3.x;machine-learning;linear-regression;sklearn-pandas,2022-03-02 00:48:07,2022-03-02 00:48:07,2022-04-27 22:22:09,i am self studying machine learning and python  i am using sklearn and i want to plot the regression line  but i get the attributeerror   linearregression  object has no attribute  coef_  could somebody help me to fix it  thank you in advance 
771,771,16085646,67774072,"Im new in the development of AR, Can you integrate Augmented Reality into Machine Learning/Image Processing into a single mobile application","<p>Im new in the development of AR and I want to develop an Augmented Reality Application integrated with Image Processing/ Machine Learning Features</p>
<p>I already developed a program for the Image Processing and now i want to integrate it with Augmented Reality</p>
<p>anyone have ideas?</p>
",42,1,0,2,augmented-reality;arcore,2021-05-31 18:19:22,2021-05-31 18:19:22,2022-04-27 16:31:56,im new in the development of ar and i want to develop an augmented reality application integrated with image processing  machine learning features i already developed a program for the image processing and now i want to integrate it with augmented reality anyone have ideas 
772,772,13749833,72011565,Predictions into the future Azure Machine Learning Studio Designer,"<p>I am currently developing an automated mechanism where I use the Azure Machine Learning Designer (AMLD). During development i used an 80/20 Split to test the efficency of my predictions.
Now i want to go live but I've missed the point where i can actually predict into the future.</p>
<p>I currently get a prediction for the last 20% of my data so i can compare them to the actual data. How do i change it so that the prediction actually starts at the end of my data?</p>
<p>A part of my prediction process is attached:</p>
<p><a href=""https://i.stack.imgur.com/eh7Rv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eh7Rv.png"" alt=""enter image description here"" /></a></p>
",23,1,0,3,prediction;forecasting;azure-machine-learning-studio,2022-04-26 15:00:32,2022-04-26 15:00:32,2022-04-27 15:24:15,i currently get a prediction for the last   of my data so i can compare them to the actual data  how do i change it so that the prediction actually starts at the end of my data  a part of my prediction process is attached  
773,773,387263,3208927,Large scale Machine Learning,"<p>I need to run various machine learning techniques on a big dataset (10-100 billions records)
The problems are mostly around text mining/information extraction and include various kernel techniques but are not restricted to them (we use some bayesian methods, bootstrapping, gradient boosting, regression trees -- many different problems and ways to solve them)</p>

<p>What would be the best implementation? I'm experienced in ML but do not have much experience how to do it for huge datasets
Is there any extendable and customizable Machine Learning libraries utilizing MapReduce infrastructure 
Strong preference to c++, but Java and python are ok
Amazon Azure or own datacenter (we can afford it)?</p>
",3061,8,27,5,java;c++;machine-learning;mapreduce;text-mining,2010-07-09 05:28:02,2010-07-09 05:28:02,2022-04-27 13:43:38,
774,774,11448916,71888405,Import Error: cannot import name &#39;BatchNormalization&#39; from &#39;keras.layers.normalization&#39;,"<p>I am getting the following error message when trying to run this AlexNET python code.</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\PycharmProjects\Local-Binary-Patterns\pyimagesearch\AlexCM.py&quot;, line 6, in &lt;module&gt;
    from keras.layers.normalization import BatchNormalization
ImportError: cannot import name 'BatchNormalization' from 'keras.layers.normalization' (C:\Users\PycharmProjects\Local-Binary-Patterns\venv\lib\site-packages\keras\layers\normalization\__init__.py)

I then saw a post to change it to:
from tensorflow.keras.layers import BatchNormalization
but then get the following error message:

C:\Users\PycharmProjects\Local-Binary-Patterns\venv\Scripts\python.exe C:/Users//PycharmProjects/Local-Binary-Patterns/pyimagesearch/AlexCM.py
2022-04-15 15:57:39.219873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-04-15 15:57:39.220029: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File &quot;C:\Users\PycharmProjects\Local-Binary-Patterns\pyimagesearch\AlexCM.py&quot;, line 11, in &lt;module&gt;
    from image_dataset_loader import load
ModuleNotFoundError: No module named 'image_dataset_loader'


Process finished with exit code 1
</code></pre>
<p>Below is the python code for better reference to the error message that I am getting:</p>
<pre><code> #Importing library
import os
import tensorflow as tf
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
#from keras.layers.normalization import BatchNormalization
from tensorflow.keras.layers import BatchNormalization
import numpy as np
from keras.utils.np_utils import to_categorical
from PIL import Image
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

def fix_gpu():
    config = ConfigProto()
    config.gpu_options.allow_growth = True
    session = InteractiveSession(config=config)

fix_gpu()

np.random.seed(1000)

#Instantiation
AlexNet = Sequential()

#1st Convolutional Layer
AlexNet.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#2nd Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#3rd Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

#4th Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

#5th Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#Passing it to a Fully Connected layer
AlexNet.add(Flatten())
# 1st Fully Connected Layer
AlexNet.add(Dense(4096, input_shape=(32,32,3,)))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
# Add Dropout to prevent overfitting
AlexNet.add(Dropout(0.4))

#2nd Fully Connected Layer
AlexNet.add(Dense(4096))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
#Add Dropout
AlexNet.add(Dropout(0.4))

#3rd Fully Connected Layer
AlexNet.add(Dense(1000))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
#Add Dropout
AlexNet.add(Dropout(0.4))

#Output Layer
AlexNet.add(Dense(24))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('softmax'))

#Model Summary
AlexNet.summary()

# Compiling the model
AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])

from image_dataset_loader import load
#Keras library for CIFAR dataset
from keras.datasets import cifar10
(x_train, y_train),(x_test, y_test)=load(path, [imgPath, testPath])
#(x_train, y_train),(x_test, y_test)=cifar10.load_data()

temp = []
for label in y_train:
    temp.append([label])
y_train = np.array(temp)
print('-------------------------4')
print(y_train)
temp = []
for label in y_test:
    temp.append([label])
y_test = np.array(temp)
print('-------------------------5')
print(y_test)



# print('-----train images-----1')
# print(x_train)
# print('-----train labels-----2')
# print(y_train)
# print('-----test images-----3')
# print(x_test)
# print('-----test labels-----4')
# print(y_test)

#Train-validation-test split
from sklearn.model_selection import train_test_split
# SPLIT IS CRITICAL
# 22 IMAGES, 0.045; 11 IMAGES, 0.09; 8 IMAGES, 0.12
x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.12)

#Dimension of the CIFAR10 dataset
print((x_train.shape,y_train.shape))
print((x_val.shape,y_val.shape))
print((x_test.shape,y_test.shape))

#Onehot Encoding the labels.
from sklearn.utils.multiclass import unique_labels


#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10
y_train=to_categorical(y_train)
y_val=to_categorical(y_val)
y_test=to_categorical(y_test)

#Verifying the dimension after one hot encoding
print((x_train.shape,y_train.shape))
print((x_val.shape,y_val.shape))
print((x_test.shape,y_test.shape))

#Image Data Augmentation
from keras.preprocessing.image import ImageDataGenerator

train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )

val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)

test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)

#Fitting the augmentation defined above to the data
train_generator.fit(x_train)
val_generator.fit(x_val)
test_generator.fit(x_test)

#Learning Rate Annealer
from keras.callbacks import ReduceLROnPlateau
lrr= ReduceLROnPlateau(monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5)

#Defining the parameters
batch_size= 10
#CHANGE THE EPOCH NUMBERS
epochs=5
learn_rate=.001

#Training the model
AlexNet.fit(train_generator.flow(x_train, y_train, batch_size=batch_size),
                      epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size,
                      validation_data = val_generator.flow(x_val, y_val, batch_size=batch_size),
                      validation_steps = 2, callbacks = [lrr], verbose=1)

#After successful training, we will visualize its performance.


import matplotlib.pyplot as plt
#Plotting the training and validation loss

f,ax=plt.subplots(1,1) #Creates 2 subplots under 1 column

#Assigning the first subplot to graph training loss and validation loss
ax.plot(AlexNet.history.history['loss'],color='b',label='Training Loss')
ax.plot(AlexNet.history.history['val_loss'],color='r',label='Validation Loss')
plt.legend()
plt.show()
f,ax=plt.subplots(1,1) #Creates 2 subplots under 1 column
#Plotting the training accuracy and validation accuracy
ax.plot(AlexNet.history.history['accuracy'],color='b',label='Training Accuracy')
ax.plot(AlexNet.history.history['val_accuracy'],color='r',label='Validation Accuracy')
plt.legend()
plt.show()


#Defining function for confusion matrix plot
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print(&quot;Normalized confusion matrix&quot;)
    else:
        print('Confusion matrix, without normalization')

#Print Confusion matrix
    fig, ax = plt.subplots(figsize=(4,4))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;,
             rotation_mode=&quot;anchor&quot;)
    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha=&quot;center&quot;, color=&quot;white&quot;
                if cm[i, j] &gt; thresh else &quot;black&quot;)
    plt.tight_layout()
    return ax

np.set_printoptions(precision=2)

#Making prediction
y_pred=(AlexNet.predict(x_test) &gt; 0.5).astype(&quot;int32&quot;)
y_true=np.argmax(y_test,axis=1)

#Plotting the confusion matrix
from sklearn.metrics import confusion_matrix
confusion_mtx=confusion_matrix(y_true,y_pred)

#class_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
CLASS_NAMES = [f.name for f in os.scandir(imgPath) if f.is_dir()]
class_names=CLASS_NAMES
print(class_names)


print(&quot;ypred\n&quot;, y_pred)
print(&quot;ytrue&quot;, y_true)

# Plotting non-normalized confusion matrix
plot_confusion_matrix(y_true, y_pred, classes = class_names,title = 'Confusion matrix, without normalization')
plt.show()
# Plotting normalized confusion matrix
# plot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True, title='Normalized confusion matrix')
# plt.show()
#Classification Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, RocCurveDisplay
acc_score = accuracy_score(y_true, y_pred)
print('\n\n\t\t  Accuracy Score: ', str(round((100*acc_score), 2)), '%')
prec_score = precision_score(y_true, y_pred, average = 'macro')
print('   Precision Score Macro: ', str(round((100*prec_score), 2)), '%')
prec_score = precision_score(y_true, y_pred, average = 'micro')
print('   Precision Score Micro: ', str(round((100*prec_score), 2)), '%')
prec_score = precision_score(y_true, y_pred, average = 'weighted')
print('Precision Score Weighted: ', str(round((100*prec_score), 2)), '%')
rec_score = recall_score(y_true, y_pred, average = 'macro')
print('\t\t\tRecall Macro: ', str(round((100*rec_score), 2)), '%')
rec_score = recall_score(y_true, y_pred, average = 'micro')
print('\t\t\tRecall Micro: ', str(round((100*rec_score), 2)), '%')
rec_score = recall_score(y_true, y_pred, average = 'weighted')
print('\t\t Recall Weighted: ', str(round((100*rec_score), 2)), '%')
f_score = f1_score(y_true, y_pred, average = 'macro')
print('\t\t  F1 Score Macro: ', str(round((100*f_score), 2)), '%')
f_score = f1_score(y_true, y_pred, average = 'micro')
print('\t\t  F1 Score Micro: ', str(round((100*f_score), 2)), '%')
f_score = f1_score(y_true, y_pred, average = 'weighted')
print('\t   F1 Score Weighted: ', str(round((100*f_score), 2)), '%')
print(&quot;Evaluation&quot;)
AlexNet.evaluate(x_test, y_test, batch_size=batch_size,verbose=1)
</code></pre>
<p>Any other information that is needed, let me know. I am also unable to print the output that is listed at the bottom of the code, so please let me know if I am missing anything with the print functions. Thank you again for the help!</p>
",151,0,0,3,python;tensorflow;keras,2022-04-16 01:28:26,2022-04-16 01:28:26,2022-04-27 10:01:41,i am getting the following error message when trying to run this alexnet python code  below is the python code for better reference to the error message that i am getting  any other information that is needed  let me know  i am also unable to print the output that is listed at the bottom of the code  so please let me know if i am missing anything with the print functions  thank you again for the help 
775,775,18955417,72018025,"Convert tick data to tick bars/candlesticks using python-MT5, and pandas","<p>I am trying to convert data obtained using a metatrader module for python found on the official mql5 website. I am trying to use tick data rather than importing candlestick data. Tick bars or candlesticks sample a set amount of ticks rather than a set amount of time in order to calculate ohlc. For example, 100 ticks creates a candle instead of 1 minute. Using the functions to copy ticks from metatrader5</p>
<pre><code>copy_ticks_from
</code></pre>
<p>or</p>
<pre><code>copy_ticks_range
</code></pre>
<p>results in a dataframe called copy_ ticks_from or copy_ticks_range but the data output is the same format.</p>
<p>time      bid      ask  last  volume       time_msc  flags  volume_real</p>
<p>Ive watched videos and searched and searched, and will continue to but any help is greatly appreciated.</p>
<p>an example of code input and out can be found at <a href=""https://www.mql5.com/en/docs/integration/python_metatrader5/mt5copyticksfrom_py"" rel=""nofollow noreferrer"">https://www.mql5.com/en/docs/integration/python_metatrader5/mt5copyticksfrom_py</a></p>
<p>edit426221500</p>
<p>I was inspired by this article <a href=""https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5"" rel=""nofollow noreferrer"">https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5</a></p>
<p>I think I am understanding a but more after this read through. I believe i need to use similar code to get desired output. Im working on converting my dataframe to a numpy array. After I will modify the code found in the reference above to be</p>
<p>Something like</p>
<pre><code>def generate_tickbars(ticks, frequency=1000):
times = ticks[:,0]
time = ticks[:,1]
prices = ticks[:,2,3]
</code></pre>
<p>not sure about volume or the preceding lines but I think im on the right track or this may at least be one way of doing it.</p>
<p>researching from <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html</a> and then going to try and get the conversion working.</p>
<p>Edit426221640</p>
<p>using</p>
<pre><code>ticks_frame.to_numpy(dtype=None, copy=True,)
</code></pre>
<p>I get a numpy array as an output.</p>
<pre><code>array([[Timestamp('2020-01-10 01:05:00'), 1552.91, 1553.16, ...,
    1578618300331, 134, 0.0],
   [Timestamp('2020-01-10 01:05:00'), 1552.83, 1553.32, ...,
    1578618300634, 134, 0.0],
   [Timestamp('2020-01-10 01:05:01'), 1552.87, 1553.32, ...,
    1578618301834, 130, 0.0],
</code></pre>
<p>...,etc
I am now stuck at the code referenced in the link above from the previous edit.</p>
<pre><code># expects a numpy array with trades
# each trade is composed of: [time, price, quantity]
def generate_tickbars(ticks, frequency=1000):
times = ticks[:,0]
prices = ticks[:,1]
volumes = ticks[:,2]
res = np.zeros(shape=(len(range(frequency, len(prices), frequency)), 6))
it = 0
for i in range(frequency, len(prices), frequency):
    res[it][0] = times[i-1]                        # time
    res[it][1] = prices[i-frequency]               # open
    res[it][2] = np.max(prices[i-frequency:i])     # high
    res[it][3] = np.min(prices[i-frequency:i])     # low
    res[it][4] = prices[i-1]                       # close
    res[it][5] = np.sum(volumes[i-frequency:i])    # volume
    it += 1
return res
</code></pre>
<p>How do make this work for my data? Is there a simpler way to accomplish this?</p>
<p>Edit426221745</p>
<p>I believe i have resampled the data correctly using different approach.</p>
<pre><code>def bar(xs, y): return np.int64(xs / y) * y
ticks_frame.groupby(bar(np.arange(len(ticks_frame)), 
1000)).agg({'bid': 'ohlc', 'volume': 'sum'})
</code></pre>
<p>Now onto plotting bars or candlesticks.</p>
<p>Edit426222250</p>
<p>Still stuck at the point of last edit. Although i can use bid for ohlc and group  ticks and view that way it seems my issue is i need to reshape the dataframe or create a new dataframe from ticks_frame that uses bid to calculate ohlc values. Any and all help is greatly appreciated.</p>
",169,0,1,3,python;pandas;plotly,2022-04-26 22:47:24,2022-04-26 22:47:24,2022-04-27 08:21:51,i am trying to convert data obtained using a metatrader module for python found on the official mql website  i am trying to use tick data rather than importing candlestick data  tick bars or candlesticks sample a set amount of ticks rather than a set amount of time in order to calculate ohlc  for example   ticks creates a candle instead of  minute  using the functions to copy ticks from metatrader or results in a dataframe called copy_ ticks_from or copy_ticks_range but the data output is the same format  time      bid      ask  last  volume       time_msc  flags  volume_real ive watched videos and searched and searched  and will continue to but any help is greatly appreciated  an example of code input and out can be found at  edit i was inspired by this article  i think i am understanding a but more after this read through  i believe i need to use similar code to get desired output  im working on converting my dataframe to a numpy array  after i will modify the code found in the reference above to be something like not sure about volume or the preceding lines but i think im on the right track or this may at least be one way of doing it  researching from  and then going to try and get the conversion working  edit using i get a numpy array as an output  how do make this work for my data  is there a simpler way to accomplish this  edit i believe i have resampled the data correctly using different approach  now onto plotting bars or candlesticks  edit still stuck at the point of last edit  although i can use bid for ohlc and group  ticks and view that way it seems my issue is i need to reshape the dataframe or create a new dataframe from ticks_frame that uses bid to calculate ohlc values  any and all help is greatly appreciated 
776,776,9975922,72021406,Handling features with multiple values per instance in Python for Machine Learning model,"<p>I am trying to handle my data set which contain some features that has some multiple values per instances as shown on the image<br />
<a href=""https://i.stack.imgur.com/D78el.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/D78el.png</a><br />
I am trying to separate each value by '|' symbol to apply One-Hot encoding technique  but I can't find any suitable solution to my problem<br />
My idea is to keep every multiple values in one row or by another word convert each cell to list of integers</p>
",47,1,1,3,pandas;machine-learning;series,2022-04-27 04:23:09,2022-04-27 04:23:09,2022-04-27 07:14:25,
777,777,18955151,72017539,Get all authors per article with for loop,"<p>I am trying to create a data frame with a for loop in python to get data from the arXiv API. It all works except for getting the authors: I am only able to get the first author per paper, as the loop only gets the first result for the  tag. I want to be able to get all authors per paper. Some help would be highly appreciated!</p>
<p>This is the loop I am using:</p>
<pre><code>for e in entries:
   article_id=e.find(&quot;id&quot;).text
   article_title=e.find(&quot;title&quot;).text
   article_published=e.find(&quot;published&quot;).text[:-10]
   article_author=e.find(&quot;name&quot;).text
   df = df.append(pd.Series([article_id, article_title, article_published, article_author, article_summary], index=dfcols), ignore_index=True)
</code></pre>
<p>This is what the entries look like:</p>
<pre><code>&lt;entry&gt;
&lt;id&gt;http://arxiv.org/abs/2004.00993v2&lt;/id&gt;
&lt;updated&gt;2020-04-05T17:16:23Z&lt;/updated&gt;
&lt;published&gt;2020-03-31T18:08:23Z&lt;/published&gt;
&lt;title&gt;Augmented Q Imitation Learning (AQIL)&lt;/title&gt;
&lt;summary&gt;  The study of unsupervised learning can be generally divided into two
categories: imitation learning and reinforcement learning. In imitation
learning the machine learns by mimicking the behavior of an expert system
whereas in reinforcement learning the machine learns via direct environment
feedback. Traditional deep reinforcement learning takes a significant time
before the machine starts to converge to an optimal policy. This paper proposes
Augmented Q-Imitation-Learning, a method by which deep reinforcement learning
convergence can be accelerated by applying Q-imitation-learning as the initial
training process in traditional Deep Q-learning.
&lt;/summary&gt;
&lt;author&gt;
&lt;name&gt;Xiao Lei Zhang&lt;/name&gt;
&lt;/author&gt;
&lt;author&gt;
&lt;name&gt;Anish Agarwal&lt;/name&gt;
&lt;/author&gt;
&lt;arxiv:comment xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot;&gt;5 pages&lt;/arxiv:comment&gt;
&lt;link href=&quot;http://arxiv.org/abs/2004.00993v2&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot;/&gt;
&lt;link href=&quot;http://arxiv.org/pdf/2004.00993v2&quot; rel=&quot;related&quot; title=&quot;pdf&quot; type=&quot;application/pdf&quot;/&gt;
&lt;arxiv:primary_category scheme=&quot;http://arxiv.org/schemas/atom&quot; term=&quot;cs.LG&quot; xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot;&gt;&lt;/arxiv:primary_category&gt;
&lt;category scheme=&quot;http://arxiv.org/schemas/atom&quot; term=&quot;cs.LG&quot;&gt;&lt;/category&gt;
&lt;category scheme=&quot;http://arxiv.org/schemas/atom&quot; term=&quot;cs.AI&quot;&gt;&lt;/category&gt;
&lt;/entry&gt;
</code></pre>
",27,0,0,5,python;xml;dataframe;api;loops,2022-04-26 22:06:08,2022-04-26 22:06:08,2022-04-27 00:16:15,i am trying to create a data frame with a for loop in python to get data from the arxiv api  it all works except for getting the authors  i am only able to get the first author per paper  as the loop only gets the first result for the  tag  i want to be able to get all authors per paper  some help would be highly appreciated  this is the loop i am using  this is what the entries look like 
778,778,3977061,60383128,Express Gateway errors on https self-signed certificate even with certificate verification disabled in Node,"<p>I am playing around and learning <a href=""https://www.express-gateway.io/"" rel=""nofollow noreferrer"">Express Gateway</a>.  The getting started guide requires that I make calls to an https endpoint.  My organization runs all requests through a middlebox that sniffs https content and has its root certs installed on our machines.  So basically all https payloads are signed with the organization's own certs.</p>

<p>Basically when Express Gateway makes the call and sees this cert it doesn't trust, it throws an error.  I have tried to set <code>NODE_TLS_REJECT_UNAUTHORIZED=0</code> to temporarily disable certificate verification, but still I see the error and the connection is rejected:</p>

<p><a href=""https://i.stack.imgur.com/CF8Uq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CF8Uq.png"" alt=""enter image description here""></a></p>

<p>How can I temporarily disable certificate verification in Express Gateway?</p>

<p>As a longer term solution, I've seen a post about how to get <a href=""https://nodejs.org/api/cli.html#cli_node_extra_ca_certs_file"" rel=""nofollow noreferrer"">Node to trust my OS certificates</a> by setting the <code>NODE_EXTRA_CA_CERTS</code> environment variable, but I don't know which file to point to on my system as the certificate file</p>

<h3>Update 1 (2020.26.02)</h3>

<p>I followed Vicenzo's advice and tried to call the script directly rather than via an <code>npm script</code> but it doesn't work.  I've added a console log at the top of the <code>server.js</code> file to see the value of <code>NODE_TLS_REJECT_UNAUTHORIZED</code>.  When I don't set the variable, and I try to proxy to an https service this is what I see:</p>

<p><a href=""https://i.stack.imgur.com/81pqF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/81pqF.png"" alt=""enter image description here""></a></p>

<p>When I set the variable, again executing the script directly, this is what I see -- the gateway still rejects the self-signed certificate:</p>

<p><a href=""https://i.stack.imgur.com/E33VV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E33VV.png"" alt=""enter image description here""></a></p>
",429,0,2,4,node.js;ssl-certificate;self-signed-certificate;express-gateway,2020-02-25 01:34:40,2020-02-25 01:34:40,2022-04-27 00:13:18,i am playing around and learning    the getting started guide requires that i make calls to an https endpoint   my organization runs all requests through a middlebox that sniffs https content and has its root certs installed on our machines   so basically all https payloads are signed with the organization s own certs  basically when express gateway makes the call and sees this cert it doesn t trust  it throws an error   i have tried to set node_tls_reject_unauthorized  to temporarily disable certificate verification  but still i see the error and the connection is rejected   how can i temporarily disable certificate verification in express gateway  as a longer term solution  i ve seen a post about how to get  by setting the node_extra_ca_certs environment variable  but i don t know which file to point to on my system as the certificate file i followed vicenzo s advice and tried to call the script directly rather than via an npm script but it doesn t work   i ve added a console log at the top of the server js file to see the value of node_tls_reject_unauthorized   when i don t set the variable  and i try to proxy to an https service this is what i see   when i set the variable  again executing the script directly  this is what i see    the gateway still rejects the self signed certificate  
779,779,18931737,72017489,Is there a way to calculate cosine similarity between documents sets in Python?,"<p>I'm trying to calculate cosine similarity between documents sets. I'm using this code and it works very well, but the problem is that it sorts the results in descending order. Is there a way to get the results according to the comparison order of the inserted documents? Or is there another way to do it?
Thanks in advance to everyone.</p>
<p>This is the code that I'm using:</p>
<pre><code>import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances


documents = ['Machine learning is the study of computer algorithms that improve automatically through experience.\
Machine learning algorithms build a mathematical model based on sample data, known as training data.\
The discipline of machine learning employs various approaches to teach computers to accomplish tasks \
where no fully satisfactory algorithm is available.',
'A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concerned\
about the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.\
Developing a machine learning application is more iterative and explorative process than software engineering.',
             'Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. \
It involves computers learning from data provided so that they carry out certain tasks.',
             'Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the &quot;signal&quot;\
or &quot;feedback&quot; available to the learning system: Supervised, Unsupervised and Reinforcement',
             'Software engineering is the systematic application of engineering approaches to the development of software.\
Software engineering is a computing discipline.',
'Machine learning is closely related to computational statistics, which focuses on making predictions using computers.\
The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.']
documents_df = pd.DataFrame(documents, columns=['documents'])

# removing special characters and stop words from the text
stop_words_l = stopwords.words('english')
documents_df['documents_cleaned'] = documents_df.documents.apply(lambda x: &quot; &quot;.join(
    re.sub(r'[^a-zA-Z]', ' ', w).lower() for w in x.split() if
    re.sub(r'[^a-zA-Z]', ' ', w).lower() not in stop_words_l))

tfidfvectoriser = TfidfVectorizer()
tfidfvectoriser.fit(documents_df.documents_cleaned)
tfidf_vectors = tfidfvectoriser.transform(documents_df.documents_cleaned)

pairwise_similarities = np.dot(tfidf_vectors, tfidf_vectors.T).toarray()
pairwise_differences = euclidean_distances(tfidf_vectors)

def most_similar(doc_id, similarity_matrix, matrix):
    print(similarity_matrix)
    print(f'Document: {documents_df.iloc[doc_id][&quot;documents&quot;]}')
    print('\n')
    print('Similar Documents:')
    if matrix == 'Cosine Similarity':
        similar_ix = np.argsort(similarity_matrix[doc_id])[::-1]
    elif matrix == 'Euclidean Distance':
        similar_ix = np.argsort(similarity_matrix[doc_id])
    for ix in similar_ix:
        if ix == doc_id:
            continue
        print('\n')
        print(f'Document: {documents_df.iloc[ix][&quot;documents&quot;]}')
        print(f'{matrix} : {similarity_matrix[doc_id][ix]}')

most_similar(0, pairwise_similarities, 'Cosine Similarity')
most_similar(0, pairwise_differences, 'Euclidean Distance')

</code></pre>
<p>This is the output:</p>
<p>Document: Machine learning is the study of computer algorithms that improve automatically through experience.Machine learning algorithms build a mathematical model based on sample data, known as training data.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available.</p>
<p>Similar Documents:</p>
<p>Document: Machine learning is closely related to computational statistics, which focuses on making predictions using computers.The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.
Cosine Similarity : 0.22860560787391593</p>
<p>Document: Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks.
Cosine Similarity : 0.22581304743529423</p>
<p>Document: Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the &quot;signal&quot;or &quot;feedback&quot; available to the learning system: Supervised, Unsupervised and Reinforcement
Cosine Similarity : 0.15314340308039842</p>
<p>Document: A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concernedabout the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.Developing a machine learning application is more iterative and explorative process than software engineering.
Cosine Similarity : 0.12407396777398046</p>
<p>Document: Software engineering is the systematic application of engineering approaches to the development of software.Software engineering is a computing discipline.
Cosine Similarity : 0.04978528121489196</p>
",42,1,1,1,python,2022-04-26 22:01:45,2022-04-26 22:01:45,2022-04-26 22:38:35,this is the code that i m using  this is the output  document  machine learning is the study of computer algorithms that improve automatically through experience machine learning algorithms build a mathematical model based on sample data  known as training data the discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available  similar documents 
780,780,18953017,72014500,Python project to exe with some files,"<p>I have converted some py to exe but my problem is that now I have external files that I want to add into the exe so that the exe can run on its own
To be more specific I want to load a machine learning model and run it in the exe.</p>
",21,0,0,5,python;machine-learning;pycharm;pyinstaller;executable,2022-04-26 18:35:36,2022-04-26 18:35:36,2022-04-26 18:35:36,
781,781,18941473,72002847,"ValueError: Input 0 of layer &quot;sequential&quot; is incompatible with the layer: expected shape=(None, 160, 160, 3), found shape=(160, 160, 3)","<p>I need help with some Machine Learning code of mine.
I am using the base of a pretrained model by google and adding the last flattening and output layers of my own.</p>
<p>After training the model and saving it, I commented out the training part of the code and loaded and used the saved model.</p>
<p>my code:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
tfds.disable_progress_bar()

class_names = ['cat','dog']

#load the images
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

get_label_name = metadata.features['label'].int2str

#function to resize image
IMG_SIZE=160
def format_example(image, label):
    image = tf.cast(image, tf.float32) #cast to convert integer values in pixels to float32
    image = (image/127.5) - 1
    image = tf.image.resize(image,  (IMG_SIZE, IMG_SIZE))
    return image,label

#.map is to apply a function to all raw images
train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)

BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000

train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)

IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)
#Create base model from pretrained model MobileNetV2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                                include_top=False,
                                                weights='imagenet')

#freezing the base (preventing any more training)
base_model.trainable = False

#Flattening it down
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
#Adding dense layer of only 1 neuron as only 2 possibilities(cat/dog)
prediction_layer = keras.layers.Dense(1)

model = tf.keras.Sequential([
    base_model,
    global_average_layer,
    prediction_layer
])

#slow learning rate since it is a pretrained model
base_learning_rate = 0.0001
#Compiling the model
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

initial_epochs = 3
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)

print('training...')
history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data = validation_batches)

acc = history.history['accuracy']
print(acc)

model.save(&quot;dogs_vs_cats.h5&quot;)

model = tf.keras.models.load_model('dogs_vs_cats.h5', compile=True)
print('predicting... ')
prediction = model.predict(test, verbose=1)
print(prediction)
</code></pre>
<p>I am getting the following error:</p>
<pre><code>ValueError: Input 0 of layer &quot;sequential&quot; is incompatible with the layer: expected shape=(None, 160, 160, 3), found shape=(160, 160, 3)
</code></pre>
",124,1,0,5,python;machine-learning;keras;conv-neural-network;tensorflow2.0,2022-04-25 22:02:52,2022-04-25 22:02:52,2022-04-26 16:32:51,after training the model and saving it  i commented out the training part of the code and loaded and used the saved model  my code  i am getting the following error 
782,782,9442390,71984526,Change priority rule and reorder queued agents in runtime using Reinforcement Learning,"<p>I am developing a model comprised of m consecutive machines in which n agents must be processed in random sequences of machines. I want to have an intelligent agent (Reinforcement Learning) to, in each action, set the priority rule to rank queued agents in each machine.
The problem I have is that I am not sure if I am correctly changing the queueing order of agents in each queue, whenever the ranking rule is changed.
After some googling, I found this post, which seems to be what I want.:</p>
<p><a href=""https://stackoverflow.com/questions/68141855/change-priority-rule-of-a-queue-block-at-runtime-in-anylogic"">Change priority rule of a Queue block at runtime in Anylogic</a></p>
<p>In this post, user Stuart Rossiter posted an interesting solution, (case 2 - using service block), which consists of sorting the agents queued on the embedded service's queue, using <code>self.queue.sortAgents()</code>.</p>
<p>However, AnyLogic does not recognize this expression, as when I try to use it, I get the error <em>&quot;queue cannot be resolved or is not a field&quot;</em>. After some more googling, I was able to find that the embedded queue of services can be accessed through <code>service.seize.queue</code>; however, even through this way, the method <code>sortAgents()</code> cannot be used, as I get an error saying that the method is undefined.</p>
<p>So, I am asking how can I reorder the agents in the embedded queue of a service after changing the ranking rule in runtime?</p>
<p>Obviously, I am assuming that playing with the task priority of the service would not be enough, as that would only be used to rank the order of agents that arrive to the queue after the ranking rule is set, i.e., it does not update the order of jobs queued before the ranking rule is changed (this is also clearly explained by the same user Stuart Rossiter).</p>
<p>Thank you.</p>
",40,0,0,4,service;priority-queue;reinforcement-learning;anylogic,2022-04-24 05:36:24,2022-04-24 05:36:24,2022-04-26 15:51:59, in this post  user stuart rossiter posted an interesting solution   case    using service block   which consists of sorting the agents queued on the embedded service s queue  using self queue sortagents    however  anylogic does not recognize this expression  as when i try to use it  i get the error  queue cannot be resolved or is not a field   after some more googling  i was able to find that the embedded queue of services can be accessed through service seize queue  however  even through this way  the method sortagents   cannot be used  as i get an error saying that the method is undefined  so  i am asking how can i reorder the agents in the embedded queue of a service after changing the ranking rule in runtime  obviously  i am assuming that playing with the task priority of the service would not be enough  as that would only be used to rank the order of agents that arrive to the queue after the ranking rule is set  i e   it does not update the order of jobs queued before the ranking rule is changed  this is also clearly explained by the same user stuart rossiter   thank you 
783,783,13749833,71937697,Forecast data with multiple Features seperately in Azure Designer,"<p>I am currently predicting values of &quot;type A&quot; in Azure Machine Learning Studio Designer. I am importing a file from azure blob storage and use that file as my past data. The current structure of the file is the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>12345</td>
</tr>
<tr>
<td>2022-03-01</td>
<td>12345</td>
</tr>
</tbody>
</table>
</div>
<p>I now want to predict multiple different types of that value, while still using one pipeline and one input file. The file structure would look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>type</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type C</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type C</td>
<td>12345</td>
</tr>
</tbody>
</table>
</div>
<p>I can currently predict those values and extract them properly, but the quality of those results is way worse then when predicting them one by one. This is probably because the linear regression is trying to find connections between type A and type C for example. I've edited the metadata and changed the &quot;type&quot; into a categorical feature but it is still not treating each type one by one.</p>
<p>Is there any possible option that the types will be predicted one by one, so first type A for all dates and after that type B, etc.?
Is there any way to increase the forecasting quality so that it will reach the same quality as predicting them one by one?
Using multiple pipelines or multiple files is not an option due to the high amount of different types (300+). Already using hyperparamter tuning so that is not an option.</p>
<p>Thanks in advance!</p>
",48,1,1,4,linear-regression;prediction;forecasting;azure-machine-learning-studio,2022-04-20 15:30:50,2022-04-20 15:30:50,2022-04-26 15:49:59,i am currently predicting values of  type a  in azure machine learning studio designer  i am importing a file from azure blob storage and use that file as my past data  the current structure of the file is the following  i now want to predict multiple different types of that value  while still using one pipeline and one input file  the file structure would look something like this  i can currently predict those values and extract them properly  but the quality of those results is way worse then when predicting them one by one  this is probably because the linear regression is trying to find connections between type a and type c for example  i ve edited the metadata and changed the  type  into a categorical feature but it is still not treating each type one by one  thanks in advance 
784,784,18477534,72011084,Multi variate LSTM model to predict future price data with features -keras,"<p>I am relatively new to machine learning, my current project has me taking data in 6 hour intervals for the past 50 days (300 columns of data), making it indexed by datetime and trying to predict prices. I have 4 columns of data for the average high price, average low price, high price volume and low price volume. I want to use these columns as features to predict the avg low price every 6 hours for a certain number of days. Currently I have been following guides which have led me to this multivariate LSTM model which has some errors, this is my code:</p>
<pre><code>df=pd.read_csv('clawdata.csv')
df['datetime']=pd.to_datetime(df['datetime'],format='%d-%m-%y %H:%M:%S')
df=df.set_index('datetime')
print(df)
df.sort_index(inplace=True)
cols = list(df)[:5]
#removing commas
df = df[cols].astype(str)
for i in cols:
    for j in range(0,len(df)):
        df[i][j] = df[i][j].replace(',','')
#convert back to float
df = df.astype(float)
#using multiple predictors/making data into matrix form
training_set = df.values
print(&quot;Shape of training set == {}.&quot;.format(df.shape))
#different feature have different measurements, scale them all to 1 scale
sc = StandardScaler()
training_set_scaled = sc.fit_transform(training_set)#scaling for all features that should be used as predictors

sc_predict = StandardScaler()
print(sc_predict.fit_transform(training_set[:, 1:2]))#scaling for avgLowPrice
print(training_set)
#creating data structure
x_train = []
y_train = []

n_future_time_intervals = 30#number of 6 hour time intervals we want to predict into the future.
n_past_time_intervals = 100#number of 6 hour time intervals we want to use to predict the future.
print(len(training_set_scaled))
for i in range(n_past_time_intervals, len(training_set_scaled)-n_future_time_intervals + 1):
    x_train.append(training_set_scaled[i - n_past_time_intervals:i, 0:df.shape[1]-1])##********
    y_train.append(training_set_scaled[i + n_future_time_intervals:i + n_future_time_intervals, 0])

x_train,y_train = np.array(x_train), np.array(y_train)

print(&quot;x_train shape == {}&quot;.format(x_train.shape))
print(&quot;y_train shape == {}&quot;.format(y_train.shape))

model = Sequential()
model.add(LSTM(64, return_sequences=True,input_shape=(n_past_time_intervals,df.shape[1]-1)))
model.add(LSTM(32,return_sequences=False))
model.add(Dropout(0.25))
model.add(Dense(13, activation='linear'))#receives input from all neurons of its previous layer
model.compile(optimizer= 'adam',loss='mean_squared_error')
print(model.summary())
es = EarlyStopping(monitor='val_loss',min_delta=1e-10,patience=10,verbose=1)
rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=10,verbose=1)
mcp = ModelCheckpoint(filepath='weights.h5',monitor='val_loss',verbose=1,save_best_only=True,save_weights_only=True)

tb = TensorBoard('logs')

history = model.fit(x_train,y_train, shuffle=True, epochs=15, callbacks=[es,rlr,mcp,tb],validation_split=0.2,verbose=1,batch_size=256)
</code></pre>
<ol>
<li>I am struggling to understand the section of code in which i append values to my x_train and y_train.</li>
<li>I am getting an issue when I run the code just to test the output and if it works that &quot;Dimensions must be equal but are 13 and 0 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential/dense/BiasAdd, IteratorGetNext:1)' with input shapes: [?,13], [?,0].orGetNext:1)' with input shapes: [?,13], [?,0].&quot;. I am puzzled as to what is going on here as i followed a guide accomplishing a similar goal and try applying it to my dataset and it's not what i was expecting.</li>
<li>Is this code just taking a certain % of past data to predict the rest of the past data, or can
I use it for future days like I wanted?</li>
</ol>
<p>Here is the csv for the dataframe created at the start of the code: <a href=""https://www.filemail.com/d/vxpranwaldbpchu"" rel=""nofollow noreferrer"">https://www.filemail.com/d/vxpranwaldbpchu</a>.</p>
<p>This is a project to help me learn supervised learning and the best way I could go about this was making this personal project for prices of items, trying to understand how someone else implemented something similar and seeing how I can apply something like that to my situation, so if you cant answer certain questions if you could point me in the right direction to achieve what I want here that would be much appreciated :) .
fyi the data is for prices of items in a game I play :D</p>
",31,0,0,5,python;pandas;tensorflow;keras;recurrent-neural-network,2022-04-26 14:24:26,2022-04-26 14:24:26,2022-04-26 14:24:26,i am relatively new to machine learning  my current project has me taking data in  hour intervals for the past  days   columns of data   making it indexed by datetime and trying to predict prices  i have  columns of data for the average high price  average low price  high price volume and low price volume  i want to use these columns as features to predict the avg low price every  hours for a certain number of days  currently i have been following guides which have led me to this multivariate lstm model which has some errors  this is my code  here is the csv for the dataframe created at the start of the code   
785,785,18949465,72009942,ImportError: cannot import name &#39;save_model for keras2onnx,"<p>I am new to Machine Learning and I'm getting an import error. My task is to convert my .h5 model into onnx. But I'm getting the error &quot;ImportError: cannot import name 'save_model&quot; eventhough I have imported all the necessary libraries.<br />
<a href=""https://i.stack.imgur.com/Rp4H4.png"" rel=""nofollow noreferrer"">Import libraries</a>
<a href=""https://i.stack.imgur.com/CCWw6.png"" rel=""nofollow noreferrer"">Error displayed</a></p>
<p>Tensorflow version: 1.3.0
Keras version:2.3.1</p>
<p>Please help me in resolving this issue, thanks in advance!</p>
",16,0,0,2,python;keras,2022-04-26 12:57:52,2022-04-26 12:57:52,2022-04-26 12:57:52,please help me in resolving this issue  thanks in advance 
786,786,18938151,72007717,Sawtooth network and CoopEdge - installation problem with dockerfile,"<p>I'm learning about blockchain and now I'm starting with Sawtooth network as I have heard that it's quite popular. I came across and research paper about CoopEdge and it's very interesting (github link: <a href=""https://github.com/coopedge/prototype"" rel=""nofollow noreferrer"">https://github.com/coopedge/prototype</a>).</p>
<p>However, I don't know clearly how to make this work. I personally send an email and still don't have the response so I have to try it with basic knowledge (blindly, somehow). There are two folder sawtooth-core and sawtooth-poer. I went with poer because the publication was talking about it. There are two type of dockerfile - docker-compose.yml and docker-compose-installed.yaml. I installed the first one with docker-compose and there was no problem. However, when I tried to install the latter one I keep getting the error:</p>
<pre><code>Step 9/13 : RUN export VERSION=$(./bin/get_version)  &amp;&amp; sed -i -e &quot;0,/version.*$/ s/version.*$/version\ =\ \&quot;${VERSION}\&quot;/&quot; Cargo.toml  &amp;&amp; /root/.cargo/bin/cargo deb --deb-version $VERSION
 ---&gt; Running in 7d244fc29e30
/bin/sh: 1: ./bin/get_version: Permission denied
91mcargo-deb: Argument to option 'deb-version' missing
</code></pre>
<p>I tried several methods by searching the internet but no luck so far. I also tried to install with root (sudo -i) but still it doesn't work at all.</p>
<p>Another thing is I don't know the second docker-file is mandatory for installation as there is no document or guidance provided by the author.</p>
<p>I appreciate any help that could solve this permission problem. Thank so much.</p>
<p>P.s: I'm using virtual machine with Ubuntu 18.04.</p>
",30,0,0,3,docker;blockchain;hyperledger-sawtooth,2022-04-26 07:56:56,2022-04-26 07:56:56,2022-04-26 07:56:56,i m learning about blockchain and now i m starting with sawtooth network as i have heard that it s quite popular  i came across and research paper about coopedge and it s very interesting  github link     however  i don t know clearly how to make this work  i personally send an email and still don t have the response so i have to try it with basic knowledge  blindly  somehow   there are two folder sawtooth core and sawtooth poer  i went with poer because the publication was talking about it  there are two type of dockerfile   docker compose yml and docker compose installed yaml  i installed the first one with docker compose and there was no problem  however  when i tried to install the latter one i keep getting the error  i tried several methods by searching the internet but no luck so far  i also tried to install with root  sudo  i  but still it doesn t work at all  another thing is i don t know the second docker file is mandatory for installation as there is no document or guidance provided by the author  i appreciate any help that could solve this permission problem  thank so much  p s  i m using virtual machine with ubuntu   
787,787,17187749,72006544,Join two SQL SELECT statements and have output in different columns,"<p>I have a Database with salary values of specific job types and am trying to get an output that separates the job, the amount of jobs with salary info given, and then the amount of jobs above a certain salary limit.</p>
<pre><code>SELECT * FROM
(SELECT job_list_name, COUNT(job_list_name) FROM Jobs_Salaries 
WHERE salary_range_low = &quot;100K+&quot; OR salary_range_low = &quot;125K+&quot; 
OR salary_range_low = &quot;150K&quot; OR salary_range_low = &quot;200K+&quot;) AS t1
INNER JOIN
(SELECT job_list_name, COUNT(job_list_name) FROM Jobs_Salaries 
WHERE indeed_salary != &quot;None&quot;) AS t2
ON t1.job_list_name = t2.job_list_name
GROUP BY t1.job_list_name
ORDER BY COUNT(t2.job_list_name) DESC;
</code></pre>
<p>This is what I currently have yet my output is this:</p>
<pre><code>MACHINE LEARNING ENGINEER|178
DATA ENGINEER|148
DATA SCIENTIST|122
DATA ANALYST|15
MACHINE LEARNING ENGINEER|241
DATA ANALYST|224
DATA ENGINEER|219
DATA SCIENTIST|187
DATA ANALYST|463|DATA ANALYST|871
</code></pre>
<p>the values above the last data analyst value being the separate outputs of each statement but I'm not sure why that last value is there. I'm trying to achieve an output like this:</p>
<pre><code>Job Type                  # of Jobs with Salary Values   # of Jobs above $100k
DATA ENGINEER                        400                          150
DATA ANALYST                         300                          100
DATA SCIENTIST                       200                          125
MACHINE LEARNING ENGINEER            100                          100
</code></pre>
<p>With the values separated by the counts of each SELECT statement and being grouped by the Job Title. Any help is appreciated!</p>
",26,1,0,2,sql;sqlite,2022-04-26 04:27:48,2022-04-26 04:27:48,2022-04-26 05:45:09,i have a database with salary values of specific job types and am trying to get an output that separates the job  the amount of jobs with salary info given  and then the amount of jobs above a certain salary limit  this is what i currently have yet my output is this  the values above the last data analyst value being the separate outputs of each statement but i m not sure why that last value is there  i m trying to achieve an output like this  with the values separated by the counts of each select statement and being grouped by the job title  any help is appreciated 
788,788,11171682,71990882,Docker - how to use a saved file created in the container,"<p><strong>Objective</strong>: train a machine learning model in a <code>.py</code> (<code>train_model.py</code>) file, save the model to a <code>.joblib</code> file (<code>Inference_xgb.joblib</code>), load the model into another <code>.py</code> (<code>Inference.py</code>) file, use the model to make predictions and save the output.</p>
<p><strong>Issue</strong>: <code>Inference.py</code> cannot find the <code>Inference_xgb.joblib</code> file.</p>
<p><strong>Relevant code snippets</strong>:</p>
<p>Training (<code>train_model.py</code>):</p>
<pre><code>#!/usr/bin/python3

import pandas as pd
from xgboost import XGBClassifier
from joblib import dump

def train():
    # load in and read training data
    training = './train.csv'
    data_train = pd.read_csv(training)
    label = data_train['2020 Failure'] # what we want to predict
    features = data_train.drop(['2020 Failure', 'FACILITYID'], axis =1, inplace=False) # what we train on the model to learn
    features = features.drop('Unnamed: 0', axis=1)
    x_train = features
    y_train = label

    # XGBoost model training
    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=&quot;logloss&quot;)
    xgb_model.fit(x_train, y_train)
    # save model
    dump(xgb_model, 'Inference_xgb.joblib')

if __name__== '__main__':
    train()
</code></pre>
<p>Testing (<code>Inference.py</code>):</p>
<pre><code>#!/usr/bin/python3

import pandas as pd
from joblib import load
from sklearn.metrics import confusion_matrix
import os

def inference():
    # load and read in test data
    testing = './test.csv'
    data_test = pd.read_csv(testing)

    label = data_test['2020 Failure'] # what we want to predict
    features = data_test.drop(['2020 Failure', 'FACILITYID'], axis =1 ) # what we train on the model to learn
    features = features.drop('Unnamed: 0', axis=1)
    IDS = data_test['FACILITYID']
    x_test = features
    y_test = label

    # run model
    xgb_model = load('Inference_xgb.joblib')
    y_label = xgb_model.predict(x_test)
    cm = confusion_matrix(y_test,y_label)
    print(&quot;Confusion Matrix: &quot;)
    print(cm)

    # write results
    dirpath = os.getcwd()
    print('CURRENT PATH: ', dirpath)
    output_path = os.path.join(dirpath, 'output.csv')
    output_df = pd.DataFrame(y_label, columns=['Prediction'])
    output_df.insert(0, &quot;FACILITYID&quot;, IDS.values)
    output_df.to_csv(output_path)
    print('OUTPUT DF')
    print(output_df)

if __name__ == &quot;__main__&quot;:
    inference()
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM jupyter/scipy-notebook 

RUN pip install joblib
RUN pip install xgboost==1.5.0

USER root

WORKDIR /scaleable-model

COPY train.csv ./train.csv
COPY test.csv ./test.csv

COPY train_model.py ./train_model.py
COPY inference.py ./inference.py

RUN python3 train_model.py
</code></pre>
<p><strong>Comments, observations, and what I've tried</strong>:</p>
<p>I've noticed that removing <code>WORKDIR /scaleable-model</code> fixes the issue, but I want to keep the <code>WORKDIR</code> to <code>/scaleable-model</code> so I can mount the <code>.csv</code> output to my host machine.</p>
<p>I am running <code>docker build</code> in the <code>scaleable-model</code> directory on my host machine. That is, I cd to <code>/home/user/pathto/scaleable-model</code> and run <code>docker build -t scaleable-model -f Dockerfile .</code></p>
<p>I then call <code>docker run</code> and specify I want to call <code>Inference.py</code>, this is how the error is generated.</p>
<p>I've tried hardcoded paths as well but this did not help. I also created a <code>Inference_xgb.joblib</code> on my host machine in the same directory where I am building the container, but this did nothing either.</p>
<p>I suspect that either:</p>
<ul>
<li>the <code>Inference_xgb.joblib</code> file is not being created properly in the container</li>
<li>I am messing up the directory structure somehow inside the container and thus <code>Inference.py</code> cannot find the file.</li>
</ul>
<p>To quote Michael Burry, &quot;I guess when someone's wrong, they never know how&quot;. I'd like to try to understand the how here.</p>
<p>EDIT:
Checking the contents of the container, the file (<code>Inference_xgb.joblib) IS being created in the directory that I want (</code>/scaleable-model<code>). Therefore, it must be an issue with </code>Inference.py` not detecting the file for some reason.</p>
",56,1,1,2,python;docker,2022-04-24 23:15:46,2022-04-24 23:15:46,2022-04-25 19:57:54,objective  train a machine learning model in a  py  train_model py  file  save the model to a  joblib file  inference_xgb joblib   load the model into another  py  inference py  file  use the model to make predictions and save the output  issue  inference py cannot find the inference_xgb joblib file  relevant code snippets  training  train_model py   testing  inference py   dockerfile  comments  observations  and what i ve tried  i ve noticed that removing workdir  scaleable model fixes the issue  but i want to keep the workdir to  scaleable model so i can mount the  csv output to my host machine  i am running docker build in the scaleable model directory on my host machine  that is  i cd to  home user pathto scaleable model and run docker build  t scaleable model  f dockerfile   i then call docker run and specify i want to call inference py  this is how the error is generated  i ve tried hardcoded paths as well but this did not help  i also created a inference_xgb joblib on my host machine in the same directory where i am building the container  but this did nothing either  i suspect that either  to quote michael burry   i guess when someone s wrong  they never know how   i d like to try to understand the how here 
789,789,10399496,72001004,Predictive Distribution of Time series with Uncertain Future Values,"<p>In machine Learning, and especially in Turning Point Detection Problem, it is important to have the best estimate for the probability distribution function (PDF) of the future samples. Lets say that we have ${x_1, \cdots, x_n}$ as a time series, probably a Gaussian one with $f_{X_1, \cdots, X_n}(x_1, \cdots, x_n)$ as its joint distribution function. We want to estimate the predictive distribution of the next $k$ uncertain samples, having the previous $n$ samples with or wouthout the ARMA/ARIMA assumptions, i.e., we are looking to find the $f_{X_{n+1}, \cdots, X_{n+k}}(x_{n+1}, \cdots, x_{n+k} | x_1, \cdots, x_n)$. With the Normal distribution assumption, what would be the mean and variance of the predictive distribution and how could we estimate them using known $n$ samples.</p>
",11,0,0,4,statistics;prediction;multivariate-time-series;turning-point,2022-04-25 19:45:57,2022-04-25 19:45:57,2022-04-25 19:45:57,in machine learning  and especially in turning point detection problem  it is important to have the best estimate for the probability distribution function  pdf  of the future samples  lets say that we have   x_   cdots  x_n   as a time series  probably a gaussian one with  f_ x_   cdots  x_n  x_   cdots  x_n   as its joint distribution function  we want to estimate the predictive distribution of the next  k  uncertain samples  having the previous  n  samples with or wouthout the arma arima assumptions  i e   we are looking to find the  f_ x_ n     cdots  x_ n k   x_ n     cdots  x_ n k    x_   cdots  x_n    with the normal distribution assumption  what would be the mean and variance of the predictive distribution and how could we estimate them using known  n  samples 
790,790,12362769,71999480,Why am I getting the invalid syntax?,"<p>I am new to Machine learning. I am trying to plot the input data.
but it gives me syntax error:</p>
<p>&quot;C:\Users\shafi\AppData\Local\Temp/ipykernel_14876/3654013207.py&quot;, line 1
plt.scatter(year,per capita income (US$),color='blue',marker='+')
^
SyntaxError: invalid syntax</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn import linear_model 
import matplotlib.pyplot as plt


df=pd.read_csv('E:/CV (ALL FOR JOB)/ALL PROJECTS/5. IMAGE DETECTION/canada.csv')
df
df.head(3)

%matplotlib inline 
plt.xlabel('year',fontsize=20)
plt.ylabel('per capita income (US$)',fontsize=20)
plt.scatter(df.year,df.per capita income (US$),color='blue',marker='+')
</code></pre>
",73,1,0,3,pandas;numpy;matplotlib,2022-04-25 17:53:01,2022-04-25 17:53:01,2022-04-25 18:01:08,
791,791,14636941,67581111,Loading TensorFlow model to manipulate an audio stream with C++,"<p>I want to load a machine learning model created with TensorFlow into my C++ Audio Application made with JUCE6. In order to use TensorFlow inside C++, I am using the TensorFlow wrapper CppFlow. I have the problem, that I don't know how to load the model for use in an audio stream.</p>
<p>Tensorflow models are loaded with cppFlow like this:</p>
<pre><code>cppflow::model model(&quot;path_to_model&quot;);
</code></pre>
<p>Then I can use the model inside my audio processing block like this.</p>
<pre><code>auto output = model(input);
</code></pre>
<p>Here is an example: <a href=""https://github.com/serizba/cppflow/blob/master/examples/load_model/main.cpp"" rel=""nofollow noreferrer"">https://github.com/serizba/cppflow/blob/master/examples/load_model/main.cpp</a></p>
<p>If I implement it like this inside my AudioProcessingBlock, the Application calls abort() without an error code and says: Debug Error! Probably due to a CPU overflow, because the model is loaded with every sample -&gt; 44k times per second.
If I implement the model loading inside my prepareToPlay method (called once), where I would place it anyway, the application runs just fine, but I cannot access the model inside my AudioProcessingBlock. Therefore I am not able to call</p>
<pre><code>auto output = model(input);
</code></pre>
<p>.</p>
<p>the cppflow::model::model is an inline function:</p>
<pre><code>inline model::model(const std::string &amp;filename){
this-&gt; graph = {TF_NewGraph(), TF_DeleteGraph()};
...
</code></pre>
<p>The complete implementation can be found here, starting line 46: <a href=""https://github.com/serizba/cppflow/blob/master/include/cppflow/model.h"" rel=""nofollow noreferrer"">https://github.com/serizba/cppflow/blob/master/include/cppflow/model.h</a></p>
<p>How could I save the model inside a private variable inside my class? So I can instantiate the model once, and use it inside the AudioProcessingBlock while my application is running.</p>
",142,1,0,4,c++;tensorflow;juce;tensorflow-c++,2021-05-18 12:07:45,2021-05-18 12:07:45,2022-04-25 16:40:19,i want to load a machine learning model created with tensorflow into my c   audio application made with juce  in order to use tensorflow inside c    i am using the tensorflow wrapper cppflow  i have the problem  that i don t know how to load the model for use in an audio stream  tensorflow models are loaded with cppflow like this  then i can use the model inside my audio processing block like this  here is an example     the cppflow  model  model is an inline function  the complete implementation can be found here  starting line    how could i save the model inside a private variable inside my class  so i can instantiate the model once  and use it inside the audioprocessingblock while my application is running 
792,792,18938018,71997954,Unable to solve multiprocessing.Manager.Lock() error in Python code (VS editor),"<p>I am using machine learning in my Python (version 3.8.5) code. In the preprocessing part, I need to hash encode few features. So earlier I have dumped a hash encoder pickle file using the features in the training phase. Saved the file with the name of 'hash_encoder.pkl'. Now in the testing phase, I need to transform the features using this pickle file. I'm using the following code <a href=""https://i.stack.imgur.com/Gvj3b.png"" rel=""nofollow noreferrer"">given in screenshot</a> to hash encode three string features as given in the first line.</p>
<p>In the encoder.transform line, I'm getting the error of &quot;data_lock=mutiprocessing.Manager().Lock()&quot;.
At the end I'm also getting 'raise EOF error'.
I have tried using same version of pandas (1.1.3) to dump the hash_encoder file and also to load it. I'm not sure why is this coming up.
Can someone help me in understand or debugging this part?
<a href=""https://i.stack.imgur.com/yuYJI.png"" rel=""nofollow noreferrer"">I have added the screenshot of the error.</a></p>
",33,0,0,5,python-3.x;pandas;locking;pickle;encoder,2022-04-25 15:47:54,2022-04-25 15:47:54,2022-04-25 15:47:54,i am using machine learning in my python  version     code  in the preprocessing part  i need to hash encode few features  so earlier i have dumped a hash encoder pickle file using the features in the training phase  saved the file with the name of  hash_encoder pkl   now in the testing phase  i need to transform the features using this pickle file  i m using the following code  to hash encode three string features as given in the first line 
793,793,18937474,71996846,Data Visualization in Python- relationship between 3 variables,"<p>I am currently working on a project where I am using machine learning to create a model that predicts which passengers survived the Titanic shipwreck.</p>
<p>I want to find the relationship between the gender of the person and their survival rate. I have one-hot encoded the gender column for the same.</p>
<p>So now I have 3 columns- Female( values 0 or 1), Male( values 0 or 1) and Survived( values 0 or 1). Can you suggest me a suitable graph to visualize and compare their relationship.</p>
<p>I tried scatter plot, but it wasn't helpful.</p>
",49,1,0,4,python;machine-learning;data-visualization;exploratory-data-analysis,2022-04-25 14:25:40,2022-04-25 14:25:40,2022-04-25 15:01:23,i am currently working on a project where i am using machine learning to create a model that predicts which passengers survived the titanic shipwreck  i want to find the relationship between the gender of the person and their survival rate  i have one hot encoded the gender column for the same  so now i have  columns  female  values  or    male  values  or   and survived  values  or    can you suggest me a suitable graph to visualize and compare their relationship  i tried scatter plot  but it wasn t helpful 
794,794,3477339,71993915,I&#39;m testing machine learning multilayer perceptron using rubix/ml and have a question for scores and losses,"<p>well.. This is my first attempt to ml modelling. Given data is like below..</p>
<pre><code>// normalized stock price data with label
// is just EXAMPLE
// $labels = ['10%', '30%', '-30%', ..];
// $samples= [[1,0,45,29,100], [100,13,0,14,5], [3,2,0,10,100], ..];
</code></pre>
<p>Each sample has 31 value and all label has 7 types. I successfully trained Model itself but score doesn't add up. Every attempt make under 0.2 score.</p>
<pre><code>array:4 [▼
    &quot;environment&quot; =&gt; array:4 [▶]
    &quot;filename&quot; =&gt; &quot;MP-test-220425-010403.rbx&quot;
    &quot;scores&quot; =&gt; array:9 [▼
        1 =&gt; 0.077863577863578
        2 =&gt; 0.077863577863578
        3 =&gt; 0.071805006587615
        4 =&gt; 0.14278357892554
        5 =&gt; 0.077863577863578
        6 =&gt; 0.077863577863578
        7 =&gt; 0.13096842384232
        8 =&gt; 0.078355812459859
        9 =&gt; 0.077863577863578
    ]
    &quot;losses&quot; =&gt; array:9 [▼
        1 =&gt; 0.2336202611478
        2 =&gt; 0.22053903758692
        3 =&gt; 0.22142868877431
        4 =&gt; 0.21962296766134
        5 =&gt; 0.21888143998952
        6 =&gt; 0.21872846102315
        7 =&gt; 0.21900067894143
        8 =&gt; 0.21882642822037
        9 =&gt; 0.21780065553406
    ]
]
</code></pre>
<p>This is what I got .. Scores keep getting same value. What does it mean? I can find examples about underfitting and overfitting but they all just explaining on somewhat high accuracy.. at least above 50% ;;;;</p>
<p>Is.. underfitting or overfitting? Do I need more training data? Data is actually cannot classifiable? ..</p>
<p>What can I do to improving result?</p>
",12,0,0,1,machine-learning,2022-04-25 07:58:22,2022-04-25 07:58:22,2022-04-25 07:58:22,well   this is my first attempt to ml modelling  given data is like below   each sample has  value and all label has  types  i successfully trained model itself but score doesn t add up  every attempt make under   score  this is what i got    scores keep getting same value  what does it mean  i can find examples about underfitting and overfitting but they all just explaining on somewhat high accuracy   at least above        is   underfitting or overfitting  do i need more training data  data is actually cannot classifiable     what can i do to improving result 
795,795,12451380,61130625,Horovod Timeline and MPI Tracing in Azure Machine Learning Workspace(MPI Configuration),"<p>All,<BR>
I am trying to train a distributed model using Horovod on Azure Machine Learning Service as shown below.</p>

<pre><code>estimator = TensorFlow(source_directory=script_folder,
                       entry_script='train_script.py',
                       script_params=script_params,
                       compute_target=compute_target_gpu_4,
                       conda_packages=['scikit-learn'],                       
                       node_count=2,                        
                       distributed_training=MpiConfiguration(),
                       framework_version = '1.13',
                       use_gpu=True
                      )
run = exp.submit(estimator)
</code></pre>

<ul>
<li>How to enable Horovod timeline?</li>
<li>How to enable more detailed MPI tracing to see the communication between the nodes?</li>
</ul>

<p>Thanks.</p>
",144,1,1,4,distributed-computing;azure-machine-learning-studio;azure-machine-learning-service;horovod,2020-04-10 03:07:53,2020-04-10 03:07:53,2022-04-25 06:03:39,thanks 
796,796,13607284,61985554,How to replace make_one_shot_iterator () from google machine learning crash course,"<p>I am following the Google Machine Learning Intensive Course. But it uses version 1.x of TensorFlow, so I was planning to change the exercises to be able to run them in TensorFlow 2.0. But I am stuck in that exercise:</p>

<p><a href=""https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=firststeps-colab&amp;hl=es#scrollTo=7UwqGbbxP53O"" rel=""nofollow noreferrer"">https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=firststeps-colab&amp;hl=es#scrollTo=7UwqGbbxP53O</a></p>

<p>Specifically the code:</p>

<pre><code>    def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):
    """"""Trains a linear regression model of one feature.

    Args:
      features: pandas DataFrame of features
      targets: pandas DataFrame of targets
      batch_size: Size of batches to be passed to the model
      shuffle: True or False. Whether to shuffle the data.
      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely
    Returns:
      Tuple of (features, labels) for next data batch
    """"""

    # Convert pandas data into a dict of np arrays.
    features = {key:np.array(value) for key,value in dict(features).items()}                                           

    # Construct a dataset, and configure batching/repeating.
    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit
    ds = ds.batch(batch_size).repeat(num_epochs)

    # Shuffle the data, if specified.
    if shuffle:
      ds = ds.shuffle(buffer_size=10000)

    # Return the next batch of data.
    features, labels = ds.make_one_shot_iterator().get_next()
    return features, labels
</code></pre>

<p>I have replaced <code>features, labels = ds.make_one_shot_iterator().get_next()</code> with <code>features, labels = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()</code></p>

<p>and it seems to work but make_one_shot_iterator() is depreceated, so, how can i replace it?</p>

<p>Also according to <a href=""https://github.com/tensorflow/tensorflow/issues/29252"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/29252</a> , I have tried</p>

<pre><code> features, labels = ds.__iter__()
    next(ds.__iter__())
    return features, labels
</code></pre>

<p>but it returns the error <code>__iter __ () is only supported inside of tf.function or when eager execution is enabled.</code></p>

<p>I am quite inexperienced in python and follow the course as a hobbyist. Any ideas on how to solve it? Thank you.</p>
",960,2,1,3,tensorflow;iterator;tensorflow-datasets,2020-05-24 17:24:16,2020-05-24 17:24:16,2022-04-24 16:21:48,i am following the google machine learning intensive course  but it uses version  x of tensorflow  so i was planning to change the exercises to be able to run them in tensorflow    but i am stuck in that exercise   specifically the code  i have replaced features  labels   ds make_one_shot_iterator   get_next   with features  labels   tf compat v data make_one_shot_iterator ds  get_next   and it seems to work but make_one_shot_iterator   is depreceated  so  how can i replace it  also according to    i have tried but it returns the error __iter __    is only supported inside of tf function or when eager execution is enabled  i am quite inexperienced in python and follow the course as a hobbyist  any ideas on how to solve it  thank you 
797,797,18786269,71984733,How can I fix tensor dimension matching error (with 1 unit difference),"<p>I'm trying to run my code for Graph Convolution Network (GCN) in PyTorch with several .csv input files, but I get error below:</p>
<blockquote>
<p>RuntimeError: The expanded size of the tensor (732) must match the existing size (731) at non-singleton dimension 0.  Target sizes: [732].  Tensor sizes: [731]</p>
</blockquote>
<p>here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import csv
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
from sklearn.metrics import r2_score
import numpy as np
import datetime
import dgl.function as fn

# Below are the graph convolution functions:
# (where each node collects information about nearby nodes)
def gcn_message(edges):
    return {'msg' : edges.src['h']}

def gcn_reduce(nodes):
    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}

# Below is the pytorch module that defines the operations at each graph convolution layer
class gcnLayer(nn.Module):

    def __init__(self, in_feats, out_feats):
        super(gcnLayer, self).__init__()
        self.linear = nn.Linear(in_feats*2, out_feats)

    def forward(self, g, inputs):
        with g.local_scope():
             g.ndata['h'] = inputs # inputs: POI features
             print(g.ndata['h'])
             g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N'))
             h_N=g.ndata['h_N']
             h_total = torch.cat([inputs, h_N], dim=1)    # Result (Convoluted POIs) of convolution at a layer is extracted
             return self.linear(h_total) # Result is linearly transformed

# Below is the pytorch class (machine learning architectures are initiliazed as classes)
# that defines the the graph convolutional network (GCN) architecture (number of hidden layers, neurons, activation function, etc)
class gcn(torch.nn.Module):

    def __init__(self, input, hidden, output):
        super(gcn, self).__init__()
        # Initially each row in the input has (input) number of elements.
        #In other words, each node in the network has (input number of features, i.e.: number of POI types)
        self.gcnInput = gcnLayer(input,hidden) # Input size is converted into hidden size
        self.gcnHidden = gcnLayer(hidden,hidden) # Hidden size is converted into hidden size
        self.gcnOutput = gcnLayer(hidden,output) # Hidden size is converted into desired output size

    # Forward function: this function is run when we call the class
    def forward(self, g, pois):
        y = F.relu(self.gcnInput(g, pois)) # Result of the input layer is sent through activation function
        y = F.relu(self.gcnHidden(g, y)) # Result of the hidden layer is sent through activation function
        y = F.relu(self.gcnHidden(g, y)) # Result of the hidden layer is sent through activation function (Here, an arbitrary amount of hidden layers can be added)
        y = self.gcnOutput(g, y) # Result of the output layer (not activated)
        return y

# Below is the pytorch class that defines the the multilayer perceptron (MLP) architecture
# (number of hidden layers, neurons, activation function, etc)
class mlp(torch.nn.Module):

    def __init__(self, input, hidden):
        super(mlp, self).__init__() #initialize
        self.classifier = nn.Sequential( # Sequential is used when combining different layers
            nn.Linear(input, hidden), # Input feature matrix is converted into a matrix with shape (hidden) and linearly transformated
            nn.ReLU(), # Activation function is applied
            nn.Linear(hidden, hidden), # Result of previous layer is linearly transformaed
            nn.ReLU(), # Activation function is applied
            nn.Linear(hidden, 1))  # At the final layer, one output is given (Trip amount)

    def forward(self, x):
        x = self.classifier(x) # the input is sent throught the MLP architecture defined above
        return x

# Below is the pytorch class that defines the the the combined deep learning architecture
class od(nn.Module):

    def __init__(self, gcnInput, gcnHidden, gcnOutput, mlpHidden):
        super(od, self).__init__()
        self.gcn = gcn(gcnInput, gcnHidden,gcnOutput) # First: GCN
        self.mlp = mlp((2*gcnoutput+1), mlpHidden) # Afterwards: MLP

    def forward(self, g, pois, costs, indices, q, zoneCount):
        y = self.gcn(g,pois) # First, send the input through GCN
        p = torch.zeros(len(costs),2*q).cuda() # Prepare a matrix that will have the POI output at origin (size: q), POI output at destination (size: q)
        count = 0
        for i in range(zoneCount):
            for j in range(zoneCount):
                p[count][:q] = y[i][:] # POI output at origin (size: q)
                p[count][q:] = y[j][:] # POI output at destination (size: q)
                count +=1

        p = p[indices][:] # Order the input matrix in the order of shuffled zones (or OD pairs)
        costs = costs[indices][:] # Order the cost matrix in the order of shuffled zones (or OD pairs)
        inputs = torch.cat((p, costs), 1).cuda() # Combine POI and cost matrices
        y = self.mlp(inputs) # Last, send through MLP
        return y

def train(optimizer, model, criterion, pois, costs, labels, indices, zoneCount, gcnOutput):
    model.train() # Model is in the training mode (meaning gradients are calculated)
    optimizer.zero_grad() # Gradients are zeroed
    print(optimizer)
    pred = model(g, pois, costs, indices, gcnOutput, zoneCount) # Get model output as predicted output
    loss = criterion(pred, labels) # Calculate loss between prediction and label
    loss.backward() # Backpropagate the gradients
    optimizer.step() # (I dont fully know what happens with this code)
    return loss.item() # Return loss

def test(model, pois, costs, labels, indices, zoneCount, gcnOutput):
    model.eval() # Mode is in evaluation mode: no gradients are calcualted
    with torch.no_grad(): # In tensorflow if tensor has a parameter &quot;autograd:true&quot; then, gradients are calculated. This code sets the autograd to false for all tensors below
        pred = model(g, pois, costs, indices,gcnOutput, zoneCount) # Get prediction
        predictions = pred.detach().cpu() # Move prediction tensor from GPU to CPU
        r2 = r2_score(labels.cpu(), predictions) # Calculate R2
        return r2

def data_collection(key): #Below part gets the data from the files into the program (POIS, nodes, costs, labels). If the file types are different than the ones used in this research, this part should be adjusted.

    if key == &quot;mb&quot;: #mb: manhattan and brooklyn case
        no = 3
    else:
        no = 2

    with open(&quot;/nodes.csv&quot;.format(key)) as f:
        nodeCount = sum(1 for line in f)
        print (nodeCount)

    with open(&quot;/poisInfo.csv&quot;.format(key)) as f:
        poiCount = sum(1 for line in f)
        print(poiCount)

    with open(&quot;/zones.csv&quot;.format(key)) as f:
        zoneCount = sum(1 for line in f)
        print(zoneCount)

    pois = torch.zeros((nodeCount,poiCount)).cuda()
    print(pois)
    i = 0

    with open('/nodes.csv'.format(key), mode='r') as rx:
        r = csv.reader(rx, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        for row in r:
            print(row)
            pois[i][:] = torch.FloatTensor([int(i) for i in row[no:]])
            i += 1

    costs = torch.zeros((zoneCount*zoneCount,1)).cuda()
    labels = torch.zeros((zoneCount*zoneCount,1)).cuda()
    count = 0

    with open('/costsTrips.csv'.format(key), mode='r') as rx:
        r = csv.reader(rx, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        for row in r:
            costs[count][0] = int(row[2])
            labels[count][0] = int(row[3])
            count += 1

    g = dgl.DGLGraph().to(torch.device('cuda:0')) # dgl: deep graph learning library: We move POIs to the graph for graph convolution
    print (nodeCount)
    g.add_nodes(nodeCount) # Add nodes to the graph
    print (nodeCount)
    print (g.number_of_nodes)

    with open('/edges.csv'.format(key), mode='r') as rx:
        r = csv.reader(rx, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        for row in r:
            g.add_edge(int(row[0]), int(row[1])) # If edge exists between 2 nodes, add edge

    print('We have %d nodes.' % g.number_of_nodes())
    print('We have %d edges.' % g.number_of_edges())
    return([g, pois, labels,costs, zoneCount, poiCount])

gcnoutput = 10
keys = [&quot;manhattan&quot;, &quot;brooklyn&quot;, &quot;mb&quot;]
count = 0

with open(&quot;costFinal.csv&quot;, mode='w', newline=&quot;&quot;) as wx:
    w = csv.writer(wx, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
    w.writerow([&quot;place&quot;, &quot;iteration&quot;, &quot;split&quot;, &quot;r2&quot;])
    for key in keys:
        [g, pois, labels, costs, zoneCount, poiCount] = data_collection(key)
        for iteration in range(1,11): # We test each split ratio with 10 times to get the average
            a = np.random.permutation(zoneCount) # randomize the zones
            for i in range(1,10):
                split = i/10 # Below lines split the training and test subsets
                breaker = int(split * zoneCount)
                train_zones = a[:breaker]
                test_zones = a[breaker:]
                train_indices = []
                test_indices = []
                for z in train_zones:
                    train_indices += [j for j in range(z * zoneCount, z * zoneCount + zoneCount)]
                for z in test_zones:
                    test_indices += [j for j in range(z * zoneCount, z * zoneCount + zoneCount)]

                # model parameters: gcninput, gcnhidden, gcnoutput, mlphidden
                model = od(poiCount, 64, gcnoutput, 64).cuda() # construct the model
                print(model)

                optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # optimizer: adam optimizer
                print(optimizer)

                criterion = torch.nn.MSELoss() # loss: mean squared error loss
                print(criterion)

                for epoch in range(1, 11): # Train the algorithm 500 epochs
                    print (epoch)
                    loss = train(optimizer, model, criterion, pois, costs, labels[train_indices], train_indices, zoneCount, gcnoutput)

                    # print(count, datetime.datetime.now() - start, key, iteration, i, epoch, loss)
                    count += 1

                r2 = test(model, pois, costs, labels[test_indices], test_indices, zoneCount, gcnoutput) # At the end of the algorithm, test the model and get r2
                w.writerow([key, iteration, i*10, r2]) # write key[manhattan,brooklyn,manhattan and brooklyn], iteration[0...9], split ratio[10%...90%], r2 to the file
</code></pre>
",25,0,0,4,neural-network;pytorch;graph-neural-network;gnn,2022-04-24 06:33:09,2022-04-24 06:33:09,2022-04-24 14:31:17,i m trying to run my code for graph convolution network  gcn  in pytorch with several  csv input files  but i get error below  runtimeerror  the expanded size of the tensor    must match the existing size    at non singleton dimension    target sizes       tensor sizes     here is my code 
798,798,9917763,71982500,predicting age using machine learning model,"<p>I am new to machine learning and I am trying to learn and grow. I have stumbled upon this problem. I have this dataframe that contains these columns:</p>
<pre><code>ispublicaccount country_AE  country_SA  age
</code></pre>
<p>now the ispublicaccount contains some missing data and the age (which i want to find the missing age) also contains missing data.</p>
<p>how can I create a machine learning model to predict the missing age values ?</p>
<p>I have the original dataframe: df_simplified</p>
<pre><code>ispublicaccount|country_AE|country_SA|age|
1               1          0         |41
2               1          0          NaN
1               0          1          NaN
NaN             1          0          23
0               0          1          31
1               0          1          NaN
1               0          1          19
2               1          0          24
.....
</code></pre>
<p>of course there are a lot more data but this is in a nuthsell</p>
<p>now I know how to create a model and predict if I have full dataset with no missing values then I would create a model and fit the data and predict. but how can I deal with these missing data in here and predict the missing age values? thank you for your helpp</p>
",34,1,0,3,python;pandas;dataframe,2022-04-23 23:51:14,2022-04-23 23:51:14,2022-04-24 09:52:27,i am new to machine learning and i am trying to learn and grow  i have stumbled upon this problem  i have this dataframe that contains these columns  now the ispublicaccount contains some missing data and the age  which i want to find the missing age  also contains missing data  how can i create a machine learning model to predict the missing age values   i have the original dataframe  df_simplified of course there are a lot more data but this is in a nuthsell now i know how to create a model and predict if i have full dataset with no missing values then i would create a model and fit the data and predict  but how can i deal with these missing data in here and predict the missing age values  thank you for your helpp
799,799,18664602,71708147,MLFlow tracking ui not showing experiments on local machine (laptop),"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.
I have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.</p>
<p>'''</p>
<pre><code>import os
import warnings
import sys

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
from urllib.parse import urlparse
import mlflow
import mlflow.sklearn

import logging

logging.basicConfig(level=logging.WARN)
logger = logging.getLogger(__name__)

warnings.filterwarnings(&quot;ignore&quot;)
np.random.seed(40)


mlflow.set_tracking_uri(&quot;file:///Users/Swapnil/Documents/LocalPython/MLFLowDemo/mlrun&quot;)

mlflow.get_tracking_uri()

mlflow.get_experiment

#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)
experiment_id = mlflow.create_experiment(&quot;Demo3&quot;)
experiment = mlflow.get_experiment(experiment_id)
print(&quot;Name: {}&quot;.format(experiment.name))
print(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))
print(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))
print(&quot;Tags: {}&quot;.format(experiment.tags))
print(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))

mlflow.set_experiment(&quot;Demo3&quot;)

def eval_metrics(actual, pred):
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mae = mean_absolute_error(actual, pred)
    r2 = r2_score(actual, pred)
    return rmse, mae, r2

# Read the wine-quality csv file from the URL
csv_url =\
    'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
try:
    data = pd.read_csv(csv_url, sep=';')
except Exception as e:
    logger.exception(
        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)

data.head(2)


def train_model(data, alpha, l1_ratio):
    
    # Split the data into training and test sets. (0.75, 0.25) split.
    train, test = train_test_split(data)

    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]
    train_x = train.drop([&quot;quality&quot;], axis=1)
    test_x = test.drop([&quot;quality&quot;], axis=1)
    train_y = train[[&quot;quality&quot;]]
    test_y = test[[&quot;quality&quot;]]

    # Set default values if no alpha is provided
    alpha = alpha
    l1_ratio = l1_ratio


    # Execute ElasticNet
    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
    lr.fit(train_x, train_y)

    # Evaluate Metrics
    predicted_qualities = lr.predict(test_x)
    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

    # Print out metrics
    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))
    print(&quot;  RMSE: %s&quot; % rmse)
    print(&quot;  MAE: %s&quot; % mae)
    print(&quot;  R2: %s&quot; % r2)
    
    # Log parameter, metrics, and model to MLflow
    with mlflow.start_run(experiment_id = experiment_id):
        mlflow.log_param(&quot;alpha&quot;, alpha)
        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)
        mlflow.log_metric(&quot;rmse&quot;, rmse)
        mlflow.log_metric(&quot;r2&quot;, r2)
        mlflow.log_metric(&quot;mae&quot;, mae)
        mlflow.sklearn.log_model(lr, &quot;model&quot;)
        

train_model(data, 0.5, 0.5)

train_model(data, 0.5, 0.3)

train_model(data, 0.4, 0.3)
</code></pre>
<p>'''</p>
<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:</p>
<p><a href=""https://i.stack.imgur.com/jKqgX.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:</p>
<p><a href=""https://i.stack.imgur.com/6KaQK.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Could you help me in finding where I am going wrong?</p>
",380,1,1,4,python;windows;mlflow;mlops,2022-04-01 19:31:52,2022-04-01 19:31:52,2022-04-24 06:16:51,        using above code  i am successfully able to create  different experiment as i can see the folders created in my local directory as shown below   now  i am trying to run the mlflow ui using the jupyter terminal in my chrome browser and i am able to open the mlflow ui but cannot see and experiments as shown below   could you help me in finding where i am going wrong 
800,800,1569055,16906161,Git push hangs when pushing to Github?,"<p>Git push hangs everytime I try to push to github. I am using Cygwin and Windows 7. Git functions fine locally tracking branches, providing status, setting global user.name and user.email and allowing commits. </p>

<p>I'm still new and learning.</p>

<p>I enter <code>git push</code> , <code>git push origin master</code> or <code>git push -u origin master</code> and I get nothing but a blank line requiring me to ctl-c to get the prompt back. </p>

<p><code>ssh-keygen -t rsa -C ""me@example.com""</code> asks me for a file name and hangs</p>

<p><code>git push heroku master</code> hangs</p>

<p><code>$ git status</code> returns <code>On branch master nothing to commit, working directory clean</code></p>

<p><code>$ git pull</code> returns <code>Already up to date</code></p>

<p><code>$ git remote -v</code> returns:</p>

<pre><code>heroku  git@heroku.com:myherokusite.git (fetch)

heroku  git@heroku.com:myherokusite.git (push) origin  

https://github.com/gitusername/appname.git (fetch) origin  

https://github.com/gitusername/appname.git (push)

or the correct ssh remote settings are returned when trying this with ssh
</code></pre>

<p><strong>Updated:</strong> Using the SSH url <code>git@github.com:gitusername/gitrepo.git</code> also hangs</p>

<p><code>git remote set-url origin https://github.com/gitusername/appname.git</code> is correct</p>

<p><strong>Updated:</strong> I can see the git processes running in Windows Task Manager while it hangs.</p>

<p>I've tried:</p>

<p>Using different internet connection locations</p>

<p>switching between https and ssh and it hangs</p>

<p>Uninstalled git. Reinstalled from: <a href=""https://code.google.com/p/msysgit/downloads/list"" rel=""noreferrer"">https://code.google.com/p/msysgit/downloads/list</a> </p>

<p>Uninstalled git. Installed Cygwin's git</p>

<p>Uninstalled git. Installed Github for Windows GUI app and it I WAS able to push. But this app has limited functionality, forces me out of my Cygwin window into another app which then forces me into a Windows command prompt for complete functionality which I thought I had escaped by using Cygwin. </p>

<p>Spent many, many hours trying to resolve this, it worked faultlessly before, thanks.</p>

<p><strong>UPDATE 4/2014:</strong> I rebuilt my entire machine Win 7, Cygwin etc and all is now working fine</p>
",169955,29,125,5,git;github;cygwin;freeze;git-push,2013-06-04 02:59:48,2013-06-04 02:59:48,2022-04-23 17:20:39,git push hangs everytime i try to push to github  i am using cygwin and windows   git functions fine locally tracking branches  providing status  setting global user name and user email and allowing commits   i m still new and learning  i enter git push   git push origin master or git push  u origin master and i get nothing but a blank line requiring me to ctl c to get the prompt back   ssh keygen  t rsa  c me example com asks me for a file name and hangs git push heroku master hangs   git status returns on branch master nothing to commit  working directory clean   git pull returns already up to date   git remote  v returns  updated  using the ssh url git github com gitusername gitrepo git also hangs git remote set url origin https   github com gitusername appname git is correct updated  i can see the git processes running in windows task manager while it hangs  i ve tried  using different internet connection locations switching between https and ssh and it hangs uninstalled git  reinstalled from    uninstalled git  installed cygwin s git uninstalled git  installed github for windows gui app and it i was able to push  but this app has limited functionality  forces me out of my cygwin window into another app which then forces me into a windows command prompt for complete functionality which i thought i had escaped by using cygwin   spent many  many hours trying to resolve this  it worked faultlessly before  thanks  update    i rebuilt my entire machine win   cygwin etc and all is now working fine
801,801,3626720,71861859,Importing a data frame from CSV file using Pandas with column name having spaces,"<p>I am trying to import a data frame from a <code>.csv</code> file which contains Per Capita Income. Moreover, in the above mentioned file the column name is <strong>Per Capita Income (US$)</strong>, however when I try to import it with the same name it gives me a syntax error, which is quite obvious. However, when I remove spaces from the column name in the subject <code>.csv</code> file it works fine - as per the below code<code>df.percapita</code>; as I have changed it from <strong>Per Capita Income (US$)</strong> to <strong>percapita</strong>. What could I possibly do to use the same column name as mentioned in the given file to import the required data frame.</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

df = pd.read_csv(&quot;/xxxx/xxxx/Desktop/Python/Machine Learning/Linear Regression/PortugalIncome.csv&quot;)

reg = linear_model.LinearRegression()
reg.fit(df[['year']], df.percapita)
reg.predict(np.array([2017]).reshape(1,1))

y = reg.coef_*2016+reg.intercept_

print(y)
</code></pre>
",33,1,0,4,python;pandas;dataframe;csv,2022-04-13 23:21:23,2022-04-13 23:21:23,2022-04-23 15:46:01,i am trying to import a data frame from a  csv file which contains per capita income  moreover  in the above mentioned file the column name is per capita income  us    however when i try to import it with the same name it gives me a syntax error  which is quite obvious  however  when i remove spaces from the column name in the subject  csv file it works fine   as per the below codedf percapita  as i have changed it from per capita income  us   to percapita  what could i possibly do to use the same column name as mentioned in the given file to import the required data frame 
802,802,2369137,71960074,openpyxl ImportError in Airflow docker when using pd.read_excel(),"<p>When using pandas pd.read_excel() in an airflow task inside a container I get the openpyxl error below. I tried installing openpyxl using poetry and even using pip in the dockerfile but no success.</p>
<pre><code>  File &quot;/home/airflow/.local/lib/python3.8/site-packages/pandas/io/excel/_openpyxl.py&quot;, line 521, in __init__
    import_optional_dependency(&quot;openpyxl&quot;)
  File &quot;/home/airflow/.local/lib/python3.8/site-packages/pandas/compat/_optional.py&quot;, line 118, in import_optional_dependency
    raise ImportError(msg) from None
ImportError: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.
</code></pre>
<p>Here the versions in poetry toml</p>
<pre><code>[tool.poetry.dependencies]
python = &quot;&gt;=3.8, &lt;3.11&quot;
pandas = &quot;^1.3.3&quot;
apache-airflow = &quot;2.2.4&quot;
openpyxl = &quot;^3.0.9&quot;
</code></pre>
<p>Similar and recent issue is reported here:
<a href=""https://dockerquestions.com/2022/03/02/docker-compose-airflow-no-error-during-build-missing-python-package-in-worker-container/"" rel=""nofollow noreferrer"">https://dockerquestions.com/2022/03/02/docker-compose-airflow-no-error-during-build-missing-python-package-in-worker-container/</a>
Any suggestion?</p>
<h3>docker-compose.yaml</h3>
<p>(don't forget to create .env, see comment down)</p>
<pre><code>version: '3'
x-airflow-common:
  &amp;airflow-common
  build:
    dockerfile: ./docker/airflow.dockerfile
    context: .
  environment:
    &amp;airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags  # laptop_folder:container_folder sync
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/scheduler:/opt/airflow/scheduler
  user: &quot;${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}&quot; # echo -e &quot;AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0&quot; &gt; .env
  depends_on:
    &amp;airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [&quot;CMD&quot;, &quot;pg_isready&quot;, &quot;-U&quot;, &quot;airflow&quot;]
      interval: 5s
      retries: 5
    restart: always

  redis:
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: [&quot;CMD&quot;, &quot;redis-cli&quot;, &quot;ping&quot;]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  airflow-webserver:
    &lt;&lt;: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;--fail&quot;, &quot;http://localhost:8080/health&quot;]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      &lt;&lt;: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    &lt;&lt;: *airflow-common
    command: scheduler
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, 'airflow jobs check --job-type SchedulerJob --hostname &quot;$${HOSTNAME}&quot;']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      &lt;&lt;: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    &lt;&lt;: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - &quot;CMD-SHELL&quot;
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d &quot;celery@$${HOSTNAME}&quot;'
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      &lt;&lt;: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: &quot;0&quot;
    restart: always
    depends_on:
      &lt;&lt;: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    &lt;&lt;: *airflow-common
    command: triggerer
    healthcheck:
      test: [&quot;CMD-SHELL&quot;, 'airflow jobs check --job-type TriggererJob --hostname &quot;$${HOSTNAME}&quot;']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      &lt;&lt;: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    &lt;&lt;: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        function ver() {
          printf &quot;%04d%04d%04d%04d&quot; $${1//./ }
        }
        airflow_version=$$(gosu airflow airflow version)
        airflow_version_comparable=$$(ver $${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=$$(ver $${min_airflow_version})
        if (( airflow_version_comparable &lt; min_airflow_version_comparable )); then
          echo
          echo -e &quot;\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m&quot;
          echo &quot;The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!&quot;
          echo
          exit 1
        fi
        if [[ -z &quot;${AIRFLOW_UID}&quot; ]]; then
          echo
          echo -e &quot;\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m&quot;
          echo &quot;If you are on Linux, you SHOULD follow the instructions below to set &quot;
          echo &quot;AIRFLOW_UID environment variable, otherwise files will be owned by root.&quot;
          echo &quot;For other operating systems you can get rid of the warning with manually created .env file:&quot;
          echo &quot;    See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user&quot;
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=&quot;false&quot;
        if (( mem_available &lt; 4000 )) ; then
          echo
          echo -e &quot;\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m&quot;
          echo &quot;At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))&quot;
          echo
          warning_resources=&quot;true&quot;
        fi
        if (( cpus_available &lt; 2 )); then
          echo
          echo -e &quot;\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m&quot;
          echo &quot;At least 2 CPUs recommended. You have $${cpus_available}&quot;
          echo
          warning_resources=&quot;true&quot;
        fi
        if (( disk_available &lt; one_meg * 10 )); then
          echo
          echo -e &quot;\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m&quot;
          echo &quot;At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))&quot;
          echo
          warning_resources=&quot;true&quot;
        fi
        if [[ $${warning_resources} == &quot;true&quot; ]]; then
          echo
          echo -e &quot;\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m&quot;
          echo &quot;Please follow the instructions to increase amount of resources available:&quot;
          echo &quot;   https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin&quot;
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R &quot;${AIRFLOW_UID}:0&quot; /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      &lt;&lt;: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: &quot;0:0&quot;
    volumes:
      - ./airflow/:/sources

  airflow-cli:
    &lt;&lt;: *airflow-common
    profiles:
      - debug
    environment:
      &lt;&lt;: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: &quot;0&quot;
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  flower:
    &lt;&lt;: *airflow-common
    command: celery flower
    ports:
      - 5555:5555
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;--fail&quot;, &quot;http://localhost:5555/&quot;]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      &lt;&lt;: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:

</code></pre>
<h3>a docker folder with an airflow.dockerfile</h3>
<pre><code>FROM apache/airflow:2.2.5-python3.8

# use root for settings
USER root

ARG YOUR_ENV=&quot;virtualenv&quot;

ENV YOUR_ENV=${YOUR_ENV} \
    PYTHONPATH=&quot;/opt/&quot; \
    # PYTHONPATH=&quot;/opt/&quot; -&gt; python path to airflow-common volumes
    PYTHONFAULTHANDLER=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONHASHSEED=random \
    PIP_NO_CACHE_DIR=off \
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    LC_ALL=C.UTF-8 \
    LANG=C.UTF-8

# linux libs 
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
    gcc curl libpq-dev \ 
    &amp;&amp; pip3 install openpyxl pandas apache-airflow \
    &amp;&amp; apt-get autoremove -yqq --purge \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/* 

# use airflow for r/w files
USER airflow
</code></pre>
<h3>py file inside dags folder</h3>
<pre><code>import os
import pandas as pd
from datetime import datetime, timedelta

from airflow import DAG

from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator



# pipeline setup
dag = DAG(
    'pipeline_etl',
    start_date=datetime.now(),
    schedule_interval='@daily',
    catchup=False,
)

URL = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx&quot;

def read_xls():
    df = pd.read_excel(URL,nrows=1000)
    return df


run_python_report = PythonOperator(
    task_id='python_report', python_callable=read_xls, dag=dag
)


start_op = DummyOperator(task_id='start_task', dag=dag)
end_op = DummyOperator(task_id='end_task', dag=dag)

start_op &gt;&gt; run_python_report &gt;&gt; end_op
</code></pre>
",155,1,0,5,python;pandas;docker;airflow;openpyxl,2022-04-22 01:11:18,2022-04-22 01:11:18,2022-04-23 01:50:13,when using pandas pd read_excel   in an airflow task inside a container i get the openpyxl error below  i tried installing openpyxl using poetry and even using pip in the dockerfile but no success  here the versions in poetry toml  don t forget to create  env  see comment down 
803,803,18910930,71973069,How to set the same scale range for all the choropleth maps?,"<p>So, I decided to challenge myself to do choropleth maps of new corona cases per Brazilian states, from 2020 to 2022. The issue I'm having is that these maps don't all have the same scale, or the same range on the scale, so it can lead to misleading interpretation.  The legend or color bar that appears on the right side is the legend of the first map, and it is not the same for all the maps. How can I make all the maps have the same scale, ranging from 0 to 4.5?</p>
<p><em>Here is my code:</em></p>
<pre><code>import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

states_info = gpd.read_file(r'C:\Users\lepae\Documents\IGTI\Programação\Udemy - Python for Data Science and Machine Learning Bootcamp\Py_DS_ML_Bootcamp-master\Refactored_Py_DS_ML_Bootcamp-master\09-Geographical-Plotting\bcim_2016_21_11_2018.gpkg', layer = 'lim_unidade_federacao_a')

corona_cases = pd.read_csv('HIST_PAINEL_COVIDBR_2022_Parte1_14abr2022.csv',  sep=';')

merged_df = pd.merge(left=states_info, right=corona_sum, left_on='sigla', right_on='estado')
#(repeats same process for all the semesters)
#...

fig, axs = plt.subplots(2,3, figsize=(16,10), 
                        constrained_layout=True, 
                        sharex=True, sharey=True, 
                        subplot_kw=dict(aspect='equal'))

merged_df_2020_01.plot(column='casosNovos', ax=axs[0,0], figsize = (16,10), cmap = 'Reds', edgecolor= 'black')
axs[0,0].set_title('First semester of 2020')

merged_df_2020_02.plot(column='casosNovos', ax=axs[0,1], figsize = (16,10), cmap = 'Reds', edgecolor= 'black')
axs[0,1].set_title('Second semester of 2020')

merged_df_2021_01.plot(column='casosNovos', ax=axs[0,2], figsize = (16,10), cmap = 'Reds', edgecolor= 'black')
axs[0,2].set_title('First semester of 2021')

merged_df_2021_02.plot(column='casosNovos', ax=axs[1,0], figsize = (16,10), cmap = 'Reds', edgecolor= 'black')
axs[1,0].set_title('Second semester of 2021')

merged_df.plot(column='casosNovos', ax=axs[1,1], figsize = (16,10), cmap = 'Reds', edgecolor= 'black')
axs[1,1].set_title('First semester of 2022')

fig.delaxes(axs[1,2])
fig.suptitle('Number of new cases of COVID-19 per Brazilian States', fontsize=20, y=1.05)
patch_col = axs[0,0].collections[0]
cb = fig.colorbar(patch_col, ax=axs, shrink = 0.95)
</code></pre>
<p><strong>Image of what I have so far:</strong>
[1]: <a href=""https://i.stack.imgur.com/P9bKy.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/P9bKy.png</a></p>
",41,0,0,4,python;maps;data-visualization;choropleth,2022-04-22 23:46:34,2022-04-22 23:46:34,2022-04-22 23:46:34,so  i decided to challenge myself to do choropleth maps of new corona cases per brazilian states  from  to   the issue i m having is that these maps don t all have the same scale  or the same range on the scale  so it can lead to misleading interpretation   the legend or color bar that appears on the right side is the legend of the first map  and it is not the same for all the maps  how can i make all the maps have the same scale  ranging from  to    here is my code 
804,804,11215296,71959708,AWS: How to add tensorflow modules from EFS to Lambda function dependencies?,"<p>I'm trying to deploy my machine learning model to AWS Lambda function. I trained my model locally with Keras and got .h5 file.</p>
<p>I zipped Tensorflow and Keras modules and added it to EFS mounted on EC2 instance. Then, i successfully added the EFS to the file system of Lambda function.</p>
<pre><code>print(&quot;EFS Directory :&quot;,os.listdir(&quot;/mnt/efs&quot;))

EFS Directory : ['utils.py', 'model.py', 'parallel_model.py', 'm_rcnn.py', 'mask_rcnn_object_0008.h5', 'python2.zip']
</code></pre>
<p>(python2.zip is a zip folder of Tensorflow and Keras modules)</p>
<p>The last step is to load my model with <code>keras.models.load_model()</code> to load my .h5 model and predict the result. The issue is that i can't import the dependencies from python2.zip.</p>
<p>Firstly, i tried</p>
<pre><code>sys.path.insert(0, &quot;/mnt/efs/python2.zip&quot;)
from python2.keras.models import load_model
</code></pre>
<p>I got</p>
<pre><code>Response
{
  &quot;errorMessage&quot;: &quot;No module named 'python2'&quot;,
  &quot;errorType&quot;: &quot;ModuleNotFoundError&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/lambda_function.py\&quot;, line 49, in lambda_handler\n    from python2.keras.models import load_model\n&quot;
  ]
}
</code></pre>
<p>Then, I used the tool <code>zipimport.zipimporter.find_module()</code> to find the path to keras.model at <code>'python2/tensorflow-1.13.1.data/purelib/tensorflow/python/keras/models'</code> but it return <code>'None'</code></p>
<p>Does anyone have ideas how to add <code>keras.models</code> from <code>python2.zip</code> in EFS to the Lambda function path? My goal is just to use <code>load_model</code> and <code>predict</code> function from Keras.</p>
<p>Thank you</p>
<p>EDIT</p>
<p>I used <code>pip3 install tensorflow-cpu</code> in EC2 cli which seems to work since i didn't get an error when open python3 editor and <code>import tensorflow</code> in EC2 cli.</p>
<pre><code>pip3 show tensorflow-cpu

Name: tensorflow-cpu
Version: 2.8.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/ec2-user/.local/lib/python3.7/site-packages
Requires: six, google-pasta, tensorflow-io-gcs-filesystem, protobuf, termcolor, libclang, numpy, flatbuffers, tf-estimator-nightly, gast, typing-extensions, keras-preprocessing, h5py, astunparse, opt-einsum, grpcio, keras, absl-py, tensorboard, setuptools, wrapt
Required-by:
</code></pre>
<p>However, i still unable to add that tensorflow dependencies to Lambda function.</p>
",62,0,0,5,python;amazon-web-services;tensorflow;keras;aws-lambda,2022-04-22 00:33:53,2022-04-22 00:33:53,2022-04-22 22:46:19,i m trying to deploy my machine learning model to aws lambda function  i trained my model locally with keras and got  h file  i zipped tensorflow and keras modules and added it to efs mounted on ec instance  then  i successfully added the efs to the file system of lambda function   python zip is a zip folder of tensorflow and keras modules  the last step is to load my model with keras models load_model   to load my  h model and predict the result  the issue is that i can t import the dependencies from python zip  firstly  i tried i got then  i used the tool zipimport zipimporter find_module   to find the path to keras model at  python tensorflow    data purelib tensorflow python keras models  but it return  none  does anyone have ideas how to add keras models from python zip in efs to the lambda function path  my goal is just to use load_model and predict function from keras  thank you edit i used pip install tensorflow cpu in ec cli which seems to work since i didn t get an error when open python editor and import tensorflow in ec cli  however  i still unable to add that tensorflow dependencies to lambda function 
805,805,8230283,71967001,ErrImagePull: Kubectl and Minikube when creating a pod,"<p>I'm new to Kubernetes and learning it these days. I'm trying to create a deployment with the help of <code>kubectl</code> and every time I create a deployment, the container is not running and I get <code>ErrImagePull</code> or <code>ImagePullBackOff</code>.</p>
<p>I have tried on two machines, both have the same problem.</p>
<pre><code>kubectl create deployment nginx-depl --image=nginx
</code></pre>
<p>Following is the <code>description</code> of one of the deployments I was creating</p>
<pre><code>Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  23s   default-scheduler  Successfully assigned default/nginx-85b98978db-z6b2n to minikube
  Normal   Pulling    22s   kubelet            Pulling image &quot;nginx&quot;
  Warning  Failed     7s    kubelet            Failed to pull image &quot;nginx&quot;: rpc error: code = Unknown desc = Error response from daemon: Get &quot;https://registry-1.docker.io/v2/&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Failed     7s    kubelet            Error: ErrImagePull
  Normal   BackOff    7s    kubelet            Back-off pulling image &quot;nginx&quot;
  Warning  Failed     7s    kubelet            Error: ImagePullBackOff

</code></pre>
",118,1,0,4,docker;nginx;kubernetes;kubectl,2022-04-22 15:35:55,2022-04-22 15:35:55,2022-04-22 18:30:21,i m new to kubernetes and learning it these days  i m trying to create a deployment with the help of kubectl and every time i create a deployment  the container is not running and i get errimagepull or imagepullbackoff  i have tried on two machines  both have the same problem  following is the description of one of the deployments i was creating
806,806,10161315,71962637,Deep Learning Image Detection - Help needed deciphering machine learning loss and accuracy graph and finding solutions to fix model,"<p>I have an imbalanced dataset from Google OpenImages of 6 classes</p>
<p><strong>Train</strong>
(starfish=439; Dolphin = 890; Turtle = 1362; Fish = 6216; Jellyfish = 733; Shellfish = 1141 )</p>
<p><strong>Validation</strong>
(starfish=20; Dolphin = 61; Turtle = 6; Fish = 370; Jellyfish = 38; Shellfish = 42 )</p>
<p><strong>Test</strong>
(starfish=72; Dolphin = 139; Turtle = 12; Fish = 1028; Jellyfish = 105; Shellfish = 115 )</p>
<p>I have 4 different PyVision pre-trained models that I am using.  I am using data augmentation and have tried adjusting the weights manually to compensate for the imbalance.  I finally got my models to run on Google Colab 100 epochs, batch=8.</p>
<p>optimizer = torch.optim.SGD(params, lr=0.05, momentum=0.9, weight_decay=0.0005)</p>
<p>lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)</p>
<p>These are my graphs from the training of the pretrained model on my image set.</p>
<p><strong>torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn</strong>
<a href=""https://i.stack.imgur.com/owdYY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/owdYY.png"" alt=""enter image description here"" /></a></p>
<p>My Interpretation:</p>
<ul>
<li>Loss: not training and not learning</li>
<li>Accuracies: Underrepresented</li>
</ul>
<p><strong>torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn</strong>
<a href=""https://i.stack.imgur.com/lEPT3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lEPT3.png"" alt=""enter image description here"" /></a></p>
<p>My Interpretation:</p>
<ul>
<li>Loss: not training and not learning</li>
<li>Accuracies: Underrepresented</li>
</ul>
<p><strong>torchvision.models.detection.fasterrcnn_resnet50_fpn</strong></p>
<p>TBD - assume similar graph - model running</p>
<p><strong>torchvision.models.detection.FasterRCNN</strong></p>
<p>TBD - assume similar graph - model running</p>
<p>QUESTIONS:</p>
<p><strong>Are my interpretations correct?  What more does this say and what other graph would you recommend that would tell me what to do to fix this?</strong></p>
<p><strong>What would you do to fix this?</strong></p>
<p>Would you create extra images to pad the uneven classes? i.e. for the Train dataset, make all the classes equal the same amount as the largest class. Then run it again?</p>
<p>I can't think of anything else.  Using pretrained models is supposed to make things better and overcome imbalances, as well as data augmentation, and <strong>designating the weights</strong> for the imbalanced classes.<br />
I used weight = [9.36, 1.48, 12.87, 13.98, 21.18, 86.83] for classes = ['Jellyfish', 'Fish', 'Dolphin', 'Starfish', 'Shellfish', 'Turtle'].</p>
<p>Did I do that wrong?</p>
",26,0,0,5,deep-learning;transfer-learning;image-classification;imbalanced-data;graph-data-science,2022-04-22 07:25:38,2022-04-22 07:25:38,2022-04-22 17:04:21,i have an imbalanced dataset from google openimages of  classes i have  different pyvision pre trained models that i am using   i am using data augmentation and have tried adjusting the weights manually to compensate for the imbalance   i finally got my models to run on google colab  epochs  batch   optimizer   torch optim sgd params  lr    momentum    weight_decay    lr_scheduler   torch optim lr_scheduler steplr optimizer  step_size   gamma    these are my graphs from the training of the pretrained model on my image set  my interpretation  my interpretation  torchvision models detection fasterrcnn_resnet_fpn tbd   assume similar graph   model running torchvision models detection fasterrcnn tbd   assume similar graph   model running questions  are my interpretations correct   what more does this say and what other graph would you recommend that would tell me what to do to fix this  what would you do to fix this  would you create extra images to pad the uneven classes  i e  for the train dataset  make all the classes equal the same amount as the largest class  then run it again  did i do that wrong 
807,807,18889609,71956939,Error: transform() missing 1 required positional argument: &#39;X&#39; after ML deployment while testing through Postman,"<p><a href=""https://i.stack.imgur.com/G5Ahw.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>We have deployed ML model to Azure Machine Learning workspace &amp; getting endpoints. But while testing, we are getting the above error.</p>
",20,1,-1,2,azure-machine-learning-service;azureml,2022-04-21 20:50:11,2022-04-21 20:50:11,2022-04-22 14:36:00, we have deployed ml model to azure machine learning workspace  amp  getting endpoints  but while testing  we are getting the above error 
808,808,1073207,68120283,Unable to connect IBMi machine with Spring Data &amp; JPA,"<p>Trying connect IBMi machine by using Spring Data &amp; JPA, but it's generating the error:</p>
<pre><code>org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [HibernateJpaConfiguration.class]: Bean instantiation via factory method failed; 
nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean]: Factory method 'entityManagerFactory' threw exception; nested exception is java.lang.AbstractMethodError: Receiver class com.ibm.as400.access.AS400JDBCConnectionImpl does not define or inherit an implementation of the resolved method 'abstract boolean isValid(int)' of interface java.sql.Connection.
    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:658) ~[spring-beans-5.3.8.jar:5.3.8]
    at spring.security.as400.SpringSecurityDemoApplication2.main(SpringSecurityDemoApplication2.java:10) ~[classes/:na]
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean]: Factory method 'entityManagerFactory' threw exception; nested exception is java.lang.AbstractMethodError: Receiver class com.ibm.as400.access.AS400JDBCConnectionImpl does not define or inherit an implementation of the resolved method 'abstract boolean isValid(int)' of interface java.sql.Connection.
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.3.8.jar:5.3.8]
Caused by: java.lang.AbstractMethodError: Receiver class com.ibm.as400.access.AS400JDBCConnectionImpl does not define or inherit an implementation of the resolved method 'abstract boolean isValid(int)' of interface java.sql.Connection.
    at com.zaxxer.hikari.pool.PoolBase.checkValidationSupport(PoolBase.java:458) ~[HikariCP-3.4.5.jar:na]
</code></pre>
<p>The project is a starter project for learning Spring Data using JPA to connect with IBMi machine, but seems Spring Data doesn't provide the easiest way for this.</p>
<p>My Project classes are:</p>
<ol>
<li>pom.xml</li>
</ol>
<pre><code>    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
        xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
        &lt;parent&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
            &lt;version&gt;2.4.7&lt;/version&gt;
            &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
        &lt;/parent&gt;
        &lt;groupId&gt;Spring-Security-JT400&lt;/groupId&gt;
        &lt;artifactId&gt;Security-JT400&lt;/artifactId&gt;
        &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
        &lt;name&gt;Security-JT400&lt;/name&gt;
        &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
        &lt;properties&gt;
            &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;/properties&gt;
    
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
            &lt;/dependency&gt;
    
            &lt;dependency&gt;
                &lt;groupId&gt;mysql&lt;/groupId&gt;
                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
                &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;/dependency&gt;
            
            
            &lt;dependency&gt;
                &lt;groupId&gt;net.sf.jt400&lt;/groupId&gt;
                &lt;artifactId&gt;jt400&lt;/artifactId&gt;
                &lt;version&gt;10.6&lt;/version&gt;
                &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;/dependency&gt;
    
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
                &lt;scope&gt;test&lt;/scope&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
                &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt;
                &lt;scope&gt;test&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;
    
    &lt;/project&gt;
</code></pre>
<ol start=""2"">
<li>Configuration</li>
</ol>
<pre><code>     @Configuration
        @EnableWebSecurity
        public class WebSecurityConfiguration extends WebSecurityConfigurerAdapter
    {
        @Autowired
        CustomUserDetailsService customUserDetailsService;
    
        @Bean
        AuthenticationProvider authenticationProvider()
        {
            DaoAuthenticationProvider provider = new DaoAuthenticationProvider();
            provider.setUserDetailsService(customUserDetailsService);
            provider.setPasswordEncoder(new BCryptPasswordEncoder());
    
            return provider;
        }
    
        @Override
        protected void configure(HttpSecurity http) throws Exception
        {
            http.authorizeRequests()
                .antMatchers(&quot;/&quot;).permitAll()
                .antMatchers(&quot;/home&quot;).hasAuthority(&quot;USER&quot;)
                .antMatchers(&quot;/admin&quot;).hasAuthority(&quot;ADMIN&quot;)
                .anyRequest().authenticated()
                .and()
                .httpBasic();
        }
    }
</code></pre>
<ol start=""3"">
<li>CustomUserDetailsService</li>
</ol>
<pre><code>    @Service
    public class CustomUserDetailsService implements UserDetailsService
    {
        @Autowired
        private UserRepository userRepository;
        
        @Override
        public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException
        {
            User user = userRepository.findByUsername(username);
            if (user == null)
            {
                throw new UsernameNotFoundException(&quot;User not found.&quot;);
            }
    
            return new CustomUserDetails(user);
        }
    }
</code></pre>
<ol start=""4"">
<li></li>
</ol>
<pre><code>    @Repository
    public interface UserRepository extends JpaRepository&lt;User, Integer&gt;
    {
        public User findByUsername(String username);
    }
</code></pre>
<ol start=""5"">
<li></li>
</ol>
<pre><code>    @RestController
    public class HomeController
    {
        @GetMapping(&quot;/home&quot;)
        public String home()
        {
            return &quot;This is Home Page.&quot;;
        }
    
        @GetMapping(&quot;/admin&quot;)
        public String admin()
        {
            return &quot;This is Admin Page.&quot;;
        }
        
    }
</code></pre>
<ol start=""6"">
<li></li>
</ol>
<pre><code>    public class CustomUserDetails implements UserDetails
    {
        private User user;
        
        public CustomUserDetails(User user)
        {
            super();
            this.user = user;
        }
    
        private static final long serialVersionUID = -228521734196009612L;
    
        @Override
        public Collection&lt;? extends GrantedAuthority&gt; getAuthorities()
        {
            return Collections.singleton(new SimpleGrantedAuthority(user.getRole()));
        }
    
        @Override
        public String getPassword()
        {
            return user.getPassword();
        }
    
        @Override
        public String getUsername()
        {
            return user.getUsername();
        }
    
        @Override
        public boolean isAccountNonExpired()
        {
            return true;
        }
    
        @Override
        public boolean isAccountNonLocked()
        {
            return true;
        }
    
        @Override
        public boolean isCredentialsNonExpired()
        {
            return true;
        }
    
        @Override
        public boolean isEnabled()
        {
            return true;
        }
    }
</code></pre>
<ol start=""7"">
<li></li>
</ol>
<pre><code>    @Entity
    public class User
    {
        @Id
        private int id;
        private String username;
        private String password;
        private String role;
    }
</code></pre>
<ol start=""8"">
<li></li>
</ol>
<pre><code>    @SpringBootApplication
    public class SpringSecurityDemoApplication2 {
    
        public static void main(String[] args) {
            SpringApplication.run(SpringSecurityDemoApplication2.class, args);
        }
    }
</code></pre>
<ol start=""9"">
<li>application.properties</li>
</ol>
<pre><code>    spring.datasource.url=jdbc:as400://192.168.xx.xxx/;translate binary=true;prompt=false
    spring.datasource.username=USERNAME
    spring.datasource.password=PASSWORD
    spring.datasource.driver-class-name=com.ibm.as400.access.AS400JDBCDriver
    spring.jpa.database-platform=org.hibernate.dialect.DB2400Dialect

</code></pre>
<p><strong>Note:</strong> Using same classes and making changes in <strong>application.properties</strong>, able to connect with MYSQL database.</p>
",397,2,0,5,java;spring;spring-boot;spring-data-jpa;spring-data,2021-06-24 22:57:15,2021-06-24 22:57:15,2022-04-22 14:01:39,trying connect ibmi machine by using spring data  amp  jpa  but it s generating the error  the project is a starter project for learning spring data using jpa to connect with ibmi machine  but seems spring data doesn t provide the easiest way for this  my project classes are  note  using same classes and making changes in application properties  able to connect with mysql database 
809,809,11763752,65590971,How to fix NPE when transforming RasterFrameLayer into Raster?,"<p>I'm trying to convert a predicted RasterFrameLayer in RasterFrames into a GeoTiff file after training a machine learning model.
When using the demo data Elkton-VA from <a href=""https://github.com/locationtech/rasterframes/tree/develop/core/src/test/resources"" rel=""nofollow noreferrer"">rasterframes</a>，it works fine.<br />
But when using one cropping sentinel 2a tif with ndvi indice (normalized from -1000 to 1000), it failed with NullPointedException in <code>toRaster </code> step.<br />
Feel like it's due to nodata value outside the ROI.
The test data is <a href=""https://dsafsadfafdsa.s3.ap-northeast-2.amazonaws.com/xiangfuqu_202003_mask_ndvi.tif"" rel=""nofollow noreferrer"">here</a>, <a href=""https://dsafsadfafdsa.s3.ap-northeast-2.amazonaws.com/test.geojson"" rel=""nofollow noreferrer"">geojson</a> and <a href=""https://dsafsadfafdsa.s3.ap-northeast-2.amazonaws.com/log.txt"" rel=""nofollow noreferrer"">log</a>.</p>
<p>Geotrellis version:3.3.0<br />
Rasterframes version:0.9.0</p>
<pre class=""lang-scala prettyprint-override""><code>
import geotrellis.proj4.LatLng
import geotrellis.raster._
import geotrellis.raster.io.geotiff.{MultibandGeoTiff, SinglebandGeoTiff}
import geotrellis.raster.io.geotiff.reader.GeoTiffReader
import geotrellis.raster.render.{ColorRamps, Png}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql._
import org.locationtech.rasterframes._
import org.locationtech.rasterframes.ml.{NoDataFilter, TileExploder}

object ClassificiationRaster extends App {

  def readTiff(name: String) =  GeoTiffReader.readSingleband(getClass.getResource(s&quot;/$name&quot;).getPath)

  def readMtbTiff(name: String): MultibandGeoTiff =  GeoTiffReader.readMultiband(getClass.getResource(s&quot;/$name&quot;).getPath)

  implicit val spark = SparkSession.builder()
    .master(&quot;local[*]&quot;)
    .appName(getClass.getName)
    .withKryoSerialization
    .getOrCreate()
    .withRasterFrames

  import spark.implicits._

  val filenamePattern = &quot;xiangfuqu_202003_mask_%s.tif&quot;
  val bandNumbers = &quot;ndvi&quot;.split(&quot;,&quot;).toSeq
  val bandColNames = bandNumbers.map(b ⇒ s&quot;band_$b&quot;).toArray
  val tileSize = 256

  val joinedRF: RasterFrameLayer = bandNumbers
    .map { b ⇒ (b, filenamePattern.format(b)) }
    .map { case (b, f) ⇒ (b, readTiff(f)) }
    .map { case (b, t) ⇒ t.projectedRaster.toLayer(tileSize, tileSize, s&quot;band_$b&quot;) }
    .reduce(_ spatialJoin _)
    .withCRS()
    .withExtent()

  val tlm = joinedRF.tileLayerMetadata.left.get

//  println(tlm.totalDimensions.cols)
//  println(tlm.totalDimensions.rows)

  joinedRF.printSchema()

  val targetCol = &quot;label&quot;

  val geojsonPath = &quot;/Users/ethan/work/data/L2a10m4326/zds/test.geojson&quot;
  spark.sparkContext.addFile(geojsonPath)
  import org.locationtech.rasterframes.datasource.geojson._

  val jsonDF: DataFrame = spark.read.geojson.load(geojsonPath)
  val label_df: DataFrame = jsonDF
    .select($&quot;CLASS_ID&quot;, st_reproject($&quot;geometry&quot;,LatLng,LatLng).alias(&quot;geometry&quot;))
    .hint(&quot;broadcast&quot;)

  val df_joined = joinedRF.join(label_df, st_intersects(st_geometry($&quot;extent&quot;), $&quot;geometry&quot;))
    .withColumn(&quot;dims&quot;,rf_dimensions($&quot;band_ndvi&quot;))

  val df_labeled: DataFrame = df_joined.withColumn(
    &quot;label&quot;,
    rf_rasterize($&quot;geometry&quot;, st_geometry($&quot;extent&quot;), $&quot;CLASS_ID&quot;, $&quot;dims.cols&quot;, $&quot;dims.rows&quot;)
  )

  df_labeled.printSchema()

  val tmp = df_labeled.filter(rf_tile_sum($&quot;label&quot;) &gt; 0).cache()

  val exploder = new TileExploder()

  val noDataFilter = new NoDataFilter().setInputCols(bandColNames :+ targetCol)

  val assembler = new VectorAssembler()
    .setInputCols(bandColNames)
    .setOutputCol(&quot;features&quot;)

  val classifier = new DecisionTreeClassifier()
    .setLabelCol(targetCol)
    .setFeaturesCol(assembler.getOutputCol)

  val pipeline = new Pipeline()
    .setStages(Array(exploder, noDataFilter, assembler, classifier))

  val evaluator = new MulticlassClassificationEvaluator()
    .setLabelCol(targetCol)
    .setPredictionCol(&quot;prediction&quot;)
    .setMetricName(&quot;f1&quot;)

  val paramGrid = new ParamGridBuilder()
    //.addGrid(classifier.maxDepth, Array(1, 2, 3, 4))
    .build()

  val trainer = new CrossValidator()
    .setEstimator(pipeline)
    .setEvaluator(evaluator)
    .setEstimatorParamMaps(paramGrid)
    .setNumFolds(4)

  val model = trainer.fit(tmp)

  val metrics = model.getEstimatorParamMaps
    .map(_.toSeq.map(p ⇒ s&quot;${p.param.name} = ${p.value}&quot;))
    .map(_.mkString(&quot;, &quot;))
    .zip(model.avgMetrics)
  metrics.toSeq.toDF(&quot;params&quot;, &quot;metric&quot;).show(false)

  val scored = model.bestModel.transform(joinedRF)

  scored.groupBy($&quot;prediction&quot; as &quot;class&quot;).count().show

  scored.show(20)


  val retiled: DataFrame = scored.groupBy($&quot;crs&quot;, $&quot;extent&quot;).agg(
    rf_assemble_tile(
      $&quot;column_index&quot;, $&quot;row_index&quot;, $&quot;prediction&quot;,
      tlm.tileCols, tlm.tileRows, IntConstantNoDataCellType
    )
  )

  val rf: RasterFrameLayer = retiled.toLayer(tlm)

  val raster: ProjectedRaster[Tile] = rf.toRaster($&quot;prediction&quot;, 5848, 4189)

  SinglebandGeoTiff(raster.tile,tlm.extent, tlm.crs).write(&quot;/Users/ethan/project/IdeaProjects/learn/spark_ml_learn.git/src/main/resources/easy_b1.tif&quot;)

    val clusterColors = ColorRamp(
      ColorRamps.Viridis.toColorMap((0 until 1).toArray).colors
    )

//  val pngBytes = retiled.select(rf_render_png($&quot;prediction&quot;, clusterColors)).first  //It can output the png.
//  retiled.tile.renderPng(clusterColors).write(&quot;/Users/ethan/project/IdeaProjects/learn/spark_ml_learn.git/src/main/resources/classified2.png&quot;)

//  Png(pngBytes).write(&quot;/Users/ethan/project/IdeaProjects/learn/spark_ml_learn.git/src/main/resources/classified2.png&quot;)

  spark.stop()
}

</code></pre>
",78,1,1,3,machine-learning;geotrellis;rasterframes,2021-01-06 11:37:02,2021-01-06 11:37:02,2022-04-22 12:21:12,
810,810,18903779,71964038,Can&#39;t run expo&#39;s virtual machine on window?,"<p>I faced this problem when I was testing expo's virtual machine on Windows. I just started learning about react-native, but when I use expo's virtual machine, it gives me an error like that. Hope to get some help from everyone. Thanks for all.</p>
<pre><code>C:\Users\Windows 10&gt;npm run FDM
npm ERR! code ENOENT
npm ERR! syscall open
npm ERR! path C:\Users\Windows 10/package.json
npm ERR! errno -4058
npm ERR! enoent ENOENT: no such file or directory, open 'C:\Users\Windows 10\package.json'
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent

npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\Windows 10\AppData\Local\npm-cache\_logs\2022-04-22T05_47_48_796Z-debug-0.log
</code></pre>
",21,0,0,2,react-native;expo,2022-04-22 11:28:29,2022-04-22 11:28:29,2022-04-22 11:32:04,i faced this problem when i was testing expo s virtual machine on windows  i just started learning about react native  but when i use expo s virtual machine  it gives me an error like that  hope to get some help from everyone  thanks for all 
811,811,13080984,71963551,Is it possible to load more than one folder to create a Model in ML.NET?,"<p>I created a model for image classification following the ML.NET tutorial for machine learning and works perfectly. Now I would like to load images from different folders and create a model from there.</p>
<pre><code> Dim preprocessingPipeline = mlContext.Transforms.Conversion.MapValueToKey(inputColumnName:=&quot;Label&quot;, outputColumnName:=&quot;LabelAsKey&quot;) _
    .Append(mlContext.Transforms.LoadRawImageBytes(outputColumnName:=&quot;Image&quot;, Pathfolder, inputColumnName:=&quot;ImagePath&quot;))
</code></pre>
<p>If I add a second append with a different folder path following the first append, the program just crashes and gives no error, and the code does not continue. I have been looking for tutorial or any other function inside ml.Context.Transforms but I found nothing. Any idea?</p>
",21,0,0,4,machine-learning;append;pipeline;ml.net,2022-04-22 10:10:13,2022-04-22 10:10:13,2022-04-22 10:10:13,i created a model for image classification following the ml net tutorial for machine learning and works perfectly  now i would like to load images from different folders and create a model from there  if i add a second append with a different folder path following the first append  the program just crashes and gives no error  and the code does not continue  i have been looking for tutorial or any other function inside ml context transforms but i found nothing  any idea 
812,812,18765048,71820291,InvalidArgumentError: Graph execution error: with cats vs dogs program,"<p>so I am new to machine learning, and I was building a cats and dogs program from a youtube tutorial when I came across this error. I have already looked through the video and confirmed that our code is the same. However, I am using google colab while she is using a jupyter notebook. Is this the issue, or do I need to change my code?</p>
<p>code</p>
<pre><code>model = Sequential([
    Conv2D(filters=32, kernel_size=(3,3,), activation = 'relu', padding = 'same', input_shape=(244, 224, 3)),
    MaxPool2D(pool_size=(2,2), strides = 2),
    Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same'),
    MaxPool2D(pool_size=(2,2), strides = 2),
    Flatten(),
    Dense(units=2, activation='softmax'),
])
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics =['accuracy'])
model.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)
</code></pre>
<p>error</p>
<pre><code>InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-155-2c9b6572eabb&gt; in &lt;module&gt;()
----&gt; 1 model.fit(x=train_batches, validation_data=valid_batches, epochs=1, verbose=2)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'sequential_16/flatten_16/Reshape' defined at (most recent call last):
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
      &quot;__main__&quot;, mod_spec)
    File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
      exec(code, run_globals)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py&quot;, line 16, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py&quot;, line 846, in launch_instance
      app.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py&quot;, line 499, in start
      self.io_loop.start()
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 132, in start
      self.asyncio_loop.run_forever()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 541, in run_forever
      self._run_once()
    File &quot;/usr/lib/python3.7/asyncio/base_events.py&quot;, line 1786, in _run_once
      handle._run()
    File &quot;/usr/lib/python3.7/asyncio/events.py&quot;, line 88, in _run
      self._context.run(self._callback, *self._args)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py&quot;, line 122, in _handle_events
      handler_func(fileobj, events)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 452, in _handle_events
      self._handle_recv()
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 481, in _handle_recv
      self._run_callback(callback, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py&quot;, line 431, in _run_callback
      callback(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py&quot;, line 300, in null_wrapper
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 283, in dispatcher
      return self.dispatch_shell(stream, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 233, in dispatch_shell
      handler(stream, idents, msg)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py&quot;, line 399, in execute_request
      user_expressions, allow_stdin)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py&quot;, line 208, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File &quot;/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py&quot;, line 537, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2718, in run_cell
      interactivity=interactivity, compiler=compiler, result=result)
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2828, in run_ast_nodes
      if self.run_code(code, result):
    File &quot;/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py&quot;, line 2882, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;&lt;ipython-input-155-2c9b6572eabb&gt;&quot;, line 1, in &lt;module&gt;
      model.fit(x=train_batches, validation_data=valid_batches, epochs=1, verbose=2)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1384, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1021, in train_function
      return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1010, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1000, in run_step
      outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 859, in train_step
      y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py&quot;, line 374, in call
      return super(Sequential, self).call(inputs, training=training, mask=mask)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 452, in call
      inputs, training=training, mask=mask)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py&quot;, line 589, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 64, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 92, in error_handler
      return fn(*args, **kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/layers/core/flatten.py&quot;, line 96, in call
      return tf.reshape(inputs, flattened_shape)
Node: 'sequential_16/flatten_16/Reshape'
Input to reshape is a tensor with 2381440 values, but the requested shape requires a multiple of 218624
     [[{{node sequential_16/flatten_16/Reshape}}]] [Op:__inference_train_function_9593]
</code></pre>
",278,1,1,2,python;machine-learning,2022-04-11 01:53:56,2022-04-11 01:53:56,2022-04-22 09:21:08,so i am new to machine learning  and i was building a cats and dogs program from a youtube tutorial when i came across this error  i have already looked through the video and confirmed that our code is the same  however  i am using google colab while she is using a jupyter notebook  is this the issue  or do i need to change my code  code error
813,813,11098908,71962015,Merit of using a small sample of data for hyperparameter tuning,"<p>In the section <strong>Blueprint recap and conclusion</strong> of <a href=""https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch06/Text_Classification.ipynb"" rel=""nofollow noreferrer"">this notebook</a>, the author used only 20% of the data for the hyperparameter tuning with Grid Search as shown in these lines of code</p>
<pre><code># Step 1 - Data Preparation
df['text'] = df['text'].apply(clean)
df = df[df['text'].str.len() &gt; 50]

if (runSVC):
    # Sample the data when running SVC to ensure reasonable run-times
    df = df.groupby('Component', as_index=False).apply(pd.DataFrame.sample,
                                                       random_state=42,
                                                       frac=.2)

# Step 2 - Train-Test Split
X_train, X_test, Y_train, Y_test = train_test_split(df['text'],
                                                    df['Component'],
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=df['Component'])
print('Size of Training Data ', X_train.shape[0])
print('Size of Test Data ', X_test.shape[0])

# Step 3 - Training the Machine Learning model
tfidf = TfidfVectorizer(stop_words=&quot;english&quot;)

if (runSVC):
    model = SVC(random_state=42, probability=True)
    grid_param = [{
        'tfidf__min_df': [5, 10],
        'tfidf__ngram_range': [(1, 3), (1, 6)],
        'model__C': [1, 100],
        'model__kernel': ['linear']
    }]
else:
    model = LinearSVC(random_state=42, tol=1e-5)
    grid_param = {
        'tfidf__min_df': [5, 10],
        'tfidf__ngram_range': [(1, 3), (1, 6)],
        'model__C': [1, 100],
        'model__loss': ['hinge']
    }
</code></pre>
<p>My question is: would the <code>best_params</code> found by the Grid Search using smaller dataset be <em>actually</em> best? Would a <strong>different</strong> <code>best_params</code> be found if we use the full dataset?</p>
",16,0,0,4,python;scikit-learn;svm;grid-search,2022-04-22 05:25:04,2022-04-22 05:25:04,2022-04-22 05:25:04,in the section blueprint recap and conclusion of   the author used only   of the data for the hyperparameter tuning with grid search as shown in these lines of code my question is  would the best_params found by the grid search using smaller dataset be actually best  would a different best_params be found if we use the full dataset 
814,814,18455985,71961386,Is it possible to compile a go binary that runs an external python script?,"<p>I'm planning to write a python script that executes a machine learning algorithm. But, I need a go binary to implement it as a server extension (chirpstack server). Is it possible to compile a go binary using go code that executes an external python script ?</p>
",31,0,0,3,python;go;binaries,2022-04-22 03:35:26,2022-04-22 03:35:26,2022-04-22 03:35:26,i m planning to write a python script that executes a machine learning algorithm  but  i need a go binary to implement it as a server extension  chirpstack server   is it possible to compile a go binary using go code that executes an external python script  
815,815,11390282,71961139,Passing Github Action Workflow Secret to Local Python Environment,"<p>I've seen a few similar questions here but I don't think this specific one has been answered yet. I am on a machine learning team and we do a LOT of discovery/exploratory analysis in a local environment.</p>
<p>I am trying to pass secrets stored in my github enterprise account to my local environment the same way that Azure Keyvault does.</p>
<p>Here is my workflow file:</p>
<pre><code>name: qubole_access
on: [pull_request, push] 

env:
    ## Sets environment variable
  QUBOLE_API_TOKEN: ${{secrets.QUBOLE_API_TOKEN}}    

jobs:
  job1:
    runs-on: self-hosted 
    steps:
      - name: step 1
        run: echo &quot;The API key is:${{env.QUBOLE_API_TOKEN}}&quot;
</code></pre>
<p>I can tell it's working because the job runs successfully in the workflow</p>
<p><a href=""https://i.stack.imgur.com/8gU2D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8gU2D.png"" alt=""image of successful job in workflow"" /></a></p>
<p>The workflow file is referencing an API token to access our Qubole database. This token is stored as a secret in the 'secrets' area of my repo</p>
<p><a href=""https://i.stack.imgur.com/lnlU0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lnlU0.png"" alt=""Secret Storage"" /></a></p>
<p>What I want to do now is reference that environment variable in a LOCAL python environment. It's important that it be in a local environment because it's less expensive and I don't want to risk anyone on my team accidentally forgetting and pushing secrets in their code, even if it's in a local git ignore file.</p>
<p>I have fetched/pulled/pushed/restarted etc etc and I can't get the variable into my environment.</p>
<p><a href=""https://i.stack.imgur.com/Z8z5o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z8z5o.png"" alt=""localenvironment"" /></a></p>
<p>When I check the environment variables by running env in the terminal, no environment variables show up there either.
Is there a way to treat github secrets like secrets in azure keyvault? Or am I missing something obvious?</p>
",70,0,0,4,python;github;workflow;github-actions,2022-04-22 03:07:14,2022-04-22 03:07:14,2022-04-22 03:34:41,i ve seen a few similar questions here but i don t think this specific one has been answered yet  i am on a machine learning team and we do a lot of discovery exploratory analysis in a local environment  i am trying to pass secrets stored in my github enterprise account to my local environment the same way that azure keyvault does  here is my workflow file  i can tell it s working because the job runs successfully in the workflow  the workflow file is referencing an api token to access our qubole database  this token is stored as a secret in the  secrets  area of my repo  what i want to do now is reference that environment variable in a local python environment  it s important that it be in a local environment because it s less expensive and i don t want to risk anyone on my team accidentally forgetting and pushing secrets in their code  even if it s in a local git ignore file  i have fetched pulled pushed restarted etc etc and i can t get the variable into my environment  
816,816,18610866,71958972,How to Send an Audio file (.wav) from My Flutter Application to a Flask Server Instance?,"<p>I have a machine learning model that is saved as .h5 and used in a flask server. The server is supposed to take an audio file as input and return a prediction string.
My Flask server code:</p>
<pre><code>  @app.route(&quot;/predict&quot;, methods=[&quot;POST&quot;])
def predict():
# get file from POST request and save it
audio_file = request.files[&quot;file&quot;]
file_name = str(random.randint(0, 100000)) # generate file name as a dummy random number
#wav_filename = str(random.randint(0, 100000))
audio_file.save(file_name)

# instantiate keyword spotting service singleton and get prediction
kss = Keyword_Spotting_Service() # Where our model is hold

predicted_emotion = kss.predict(file_name)

# we don't need the audio file any more - let's delete it!
os.remove(file_name)

# send back result as a json file (dictionary)
result = {&quot;emotion&quot;: predicted_emotion}
return jsonify(result)
</code></pre>
<p>I tested my server using python client and it worked.</p>
<p>in my flutter app I created a predict method:</p>
<pre><code> final uri = Uri.parse('http://192.168.1.14:5000/predict');
  final request = new http.MultipartRequest(&quot;POST&quot;, uri);
  request.fields['audio'] = &quot;audio&quot;;
  //myStreamController.stream.asBroadcastStream().listen(request);
  final multipartFile = new http.MultipartFile.fromBytes('file', (await rootBundle.load(&quot;assets/audioFile.wav&quot;)).buffer.asUint8List( ), filename: 'audioFile.wav');
  request.files.add(multipartFile);
  request.headers[&quot;Content-Type&quot;] = 'multipart/form-data';
  final streamedResponse = await request.send();
  // final x =  await streamedResponse.stream.toBytes();
  Response response = await http.Response.fromStream(streamedResponse);

  Map&lt;String, dynamic&gt; result = jsonDecode(response.body);
  var resultx = jsonDecode(json.encode(response.body));
  predic = &quot;${resultx['emotion']}&quot;;
  // resultx.clear();
  return predic;
</code></pre>
<p>It keeps giving me this error: File contains data in an unknown format (Runtime Error).</p>
<p>What am I missing?
Any help will be highly appreciated.</p>
",160,0,0,3,flutter;flask;audio,2022-04-21 23:32:41,2022-04-21 23:32:41,2022-04-21 23:32:41,i tested my server using python client and it worked  in my flutter app i created a predict method  it keeps giving me this error  file contains data in an unknown format  runtime error  
817,817,18895899,71957522,How to block host port in RYU controller when anormal traffic is detected?,"<p>I am using ryu controller and mininet:
I am creating an traffic network classifier using Machine Learning to predict &quot;normal traffic&quot; or &quot;anormal traffic&quot;. But now, I have a issue in &quot;how to block the port of the victim host when the traffic is anormal?&quot;</p>
<p>This is my partial code:</p>
<p>*if normal_traffic:
self.logger.info(&quot;Normal Traffic...&quot;)</p>
<p>elif anormal_traffic:
self.logger.info(&quot;Anormal Traffic Detected...&quot;)
#How to add mitigation rules?*</p>
",49,0,0,4,firewall;mininet;ddos;ryu,2022-04-21 21:31:31,2022-04-21 21:31:31,2022-04-21 23:05:40,this is my partial code 
818,818,389976,71958142,"horovod timeline not saving in databricks, copying timeline.json from /tmp to /dbfs fails?","<p>I am successfully training in horovod.. a simple MNIST model.. however, when I went to add Horovod Timeline.. chaos ensued.. based on the only example code I could find (A TF Keras example)..</p>
<p><a href=""https://docs.databricks.com/applications/machine-learning/train-model/distributed-training/horovod-runner.html#examples"" rel=""nofollow noreferrer"">https://docs.databricks.com/applications/machine-learning/train-model/distributed-training/horovod-runner.html#examples</a></p>
<p>I must add some code...</p>
<pre><code>def train_hvd(timeline=None):
  
  # --- WORKAROUND FOR DYNAMIC TIMELINE ISSUE --- #
  worker_timeline = None
  dbfs_timeline = None

  if timeline and timeline.startswith(&quot;/dbfs&quot;):
    dbfs_timeline = timeline
    worker_timeline = &quot;/tmp&quot;+timeline[5:]
    timeline = worker_timeline
    os.makedirs(os.path.dirname(worker_timeline), exist_ok=True)

  if os.environ.get(&quot;HOROVOD_TIMELINE&quot;, &quot;&quot;).startswith(&quot;/dbfs&quot;):
    dbfs_timeline = os.environ[&quot;HOROVOD_TIMELINE&quot;]
    worker_timeline = &quot;/tmp&quot;+os.environ[&quot;HOROVOD_TIMELINE&quot;][5:]
    os.environ[&quot;HOROVOD_TIMELINE&quot;] = worker_timeline
    os.makedirs(os.path.dirname(worker_timeline), exist_ok=True)
    print(f&quot;dbfs_timeline:{dbfs_timeline} worker_timeline:{worker_timeline}&quot;)
    # --- WORKAROUND FOR DYNAMIC TIMELINE ISSUE --- #
    
  # Initialize Horovod
  hvd.init()
  if timeline:
    hvd.start_timeline(timeline)
  
  try:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # IMAGINE horovod enabled training code HERE....
    
    if timeline:
      hvd.stop_timeline()
      
  finally:
    # --- WORKAROUND FOR DYNAMIC TIMELINE ISSUE --- #
    if dbfs_timeline and worker_timeline:
      if os.path.exists(worker_timeline):
        import shutil
        print(f&quot;Copying: from: {worker_timeline} to {dbfs_timeline} \n&quot;)
        shutil.copy(worker_timeline, dbfs_timeline)
    # --- WORKAROUND FOR DYNAMIC TIMELINE ISSUE --- #
</code></pre>
<p>it is in this <code>finally</code> clause where it crashes.. is this a permissions issue on my databricks workspace ?</p>
<pre><code>[1,1]&lt;stdout&gt;:Copying: from: /tmp/ml/horovod-timeline/43f0d66c-9e17-465d-b3a6-634309e21570/horovod_timeline.json to /dbfs/ml/horovod-timeline/43f0d66c-9e17-465d-b3a6-634309e21570/horovod_timeline.json
[1,1]&lt;stdout&gt;:
[1,0]&lt;stderr&gt;:Traceback (most recent call last):
[1,0]&lt;stderr&gt;:  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
[1,0]&lt;stderr&gt;:  File &quot;/databricks/.python_edge_libs/sparkdl/horovod/runner.py&quot;, line 222, in wrapped_main
[1,0]&lt;stderr&gt;:    return_value = main(**kwargs)
[1,0]&lt;stderr&gt;:  File &quot;&lt;command-2605470994584287&gt;&quot;, line 68, in train_hvd
[1,0]&lt;stderr&gt;:  File &quot;/usr/lib/python3.8/shutil.py&quot;, line 418, in copy
[1,0]&lt;stderr&gt;:    copyfile(src, dst, follow_symlinks=follow_symlinks)
[1,0]&lt;stderr&gt;:  File &quot;/usr/lib/python3.8/shutil.py&quot;, line 276, in copyfile
[1,0]&lt;stderr&gt;:    return dst
[1,0]&lt;stderr&gt;:OSError: [Errno 22] Invalid argument
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
</code></pre>
",21,0,0,3,databricks;azure-databricks;horovod,2022-04-21 22:23:02,2022-04-21 22:23:02,2022-04-21 22:23:02,i am successfully training in horovod   a simple mnist model   however  when i went to add horovod timeline   chaos ensued   based on the only example code i could find  a tf keras example     i must add some code    it is in this finally clause where it crashes   is this a permissions issue on my databricks workspace  
819,819,18884127,71946099,Why am I getting a value error when I try to load the csv file?,"<p>I have been doing a course on Python and Machine Learning. I typed in the code below and I got a value error:</p>
<pre><code>import numpy as np
from sklearn import preprocessing 
raw_csv_data = np.loadtxt(&quot;Audiobooks_data.csv&quot;, delimiter = &quot;,&quot;)
unscaled_inputs_all = raw_csv_data[:,1:-1] 
targets_all = raw_csv_data[:,-1] 
</code></pre>
<p>The error message said:</p>
<pre><code>ValueError                                Traceback (most recent call last)
Input In [2], in &lt;cell line: 3&gt;()
      1 import numpy as np
      2 from sklearn import preprocessing ## should standardize inputs using sklearn, accuracy decreases by 10% otherwise 
----&gt; 3 raw_csv_data = np.loadtxt(&quot;Audiobooks_data.csv&quot;, delimiter = &quot;,&quot;)
      4 unscaled_inputs_all = raw_csv_data[:,1:-1] ## takes all data except first and last columns (Id and target columns)
      5 targets_all = raw_csv_data[:,-1]

File ~\Anaconda3\envs\py3-TF2.0\lib\site-packages\numpy\lib\npyio.py:1148, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)
   1143 # read data in chunks and fill it into an array via resize
   1144 # over-allocating and shrinking the array later may be faster but is
   1145 # probably not relevant compared to the cost of actually reading and
   1146 # converting the data
   1147 X = None
-&gt; 1148 for x in read_data(_loadtxt_chunksize):
   1149     if X is None:
   1150         X = np.array(x, dtype)

File ~\Anaconda3\envs\py3-TF2.0\lib\site-packages\numpy\lib\npyio.py:999, in loadtxt.&lt;locals&gt;.read_data(chunk_size)
    995     raise ValueError(&quot;Wrong number of columns at line %d&quot;
    996                      % line_num)
    998 # Convert each value according to its column and store
--&gt; 999 items = [conv(val) for (conv, val) in zip(converters, vals)]
   1001 # Then pack it according to the dtype's nesting
   1002 items = pack_items(items, packing)

File ~\Anaconda3\envs\py3-TF2.0\lib\site-packages\numpy\lib\npyio.py:999, in &lt;listcomp&gt;(.0)
    995     raise ValueError(&quot;Wrong number of columns at line %d&quot;
    996                      % line_num)
    998 # Convert each value according to its column and store
--&gt; 999 items = [conv(val) for (conv, val) in zip(converters, vals)]
   1001 # Then pack it according to the dtype's nesting
   1002 items = pack_items(items, packing)

File ~\Anaconda3\envs\py3-TF2.0\lib\site-packages\numpy\lib\npyio.py:736, in _getconv.&lt;locals&gt;.floatconv(x)
    734 if '0x' in x:
    735     return float.fromhex(x)
--&gt; 736 return float(x)

ValueError: could not convert string to float: ''
</code></pre>
<p>Do you have any idea why I am getting it? I get a similar error if I remove the delimiter, but it quotes the first row of the file instead. I tried to remove the blank rows of the file (which form every other row of the 28,000 row CSV file), but it was going to take a while, and I thought I should see what the best course of action is.</p>
<p>Any help would be greatly appreciated.</p>
",94,2,0,2,python;numpy,2022-04-21 02:25:24,2022-04-21 02:25:24,2022-04-21 18:06:36,i have been doing a course on python and machine learning  i typed in the code below and i got a value error  the error message said  do you have any idea why i am getting it  i get a similar error if i remove the delimiter  but it quotes the first row of the file instead  i tried to remove the blank rows of the file  which form every other row of the   row csv file   but it was going to take a while  and i thought i should see what the best course of action is  any help would be greatly appreciated 
820,820,18784307,71849683,"Can we use Pydantic models (Basemodel) directly inside model.predict() using FastAPI, and if not ,why?","<p>I'm using Pydantic model (<code>Basemodel</code>) with FastAPI and converting the input into a <code>dictionary</code>, and then converting it into a Pandas <code>DataFrame</code> to assign it into <code>model.predict()</code> function for Machine Learning prediction, as shown below :</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI
import uvicorn
from pydantic import BaseModel
import pandas as pd
from typing import List

class Inputs(BaseModel):
    f1: float,
    f2: float,
    f3: str

@app.post('/predict')
def predict(features: List[Inputs]):
    output = []

    # loop the list of input features
    for data in features:
         result = {}

         # Convert data into dict() and then into a DataFrame
            data = data.dict()
            df = pd.DataFrame([data])

         # get predictions
            prediction = classifier.predict(df)[0]

         # get probability
            probability = classifier.predict_proba(df).max()

         # assign to dictionary 
            result[&quot;prediction&quot;] = prediction
            result[&quot;probability&quot;] = probability

         # append dictionary to list (many outputs)
            output.append(result)

    return output
</code></pre>
<p>It works fine, I'm just not quite sure if it's <strong>optimized</strong> or the right way to do it, since I convert the input two times to get the predictions. Also, I'm not sure if it is going to work <strong>fast</strong> in the case of having a <strong>huge number</strong> of inputs. Any improvements for this? If there's a way (even other than using Pydantic models, where I can work directly and avoid going through conversions and the loop.</p>
",235,1,2,5,python;machine-learning;fastapi;prediction;pydantic,2022-04-13 03:41:19,2022-04-13 03:41:19,2022-04-21 17:05:28,i m using pydantic model  basemodel  with fastapi and converting the input into a dictionary  and then converting it into a pandas dataframe to assign it into model predict   function for machine learning prediction  as shown below   it works fine  i m just not quite sure if it s optimized or the right way to do it  since i convert the input two times to get the predictions  also  i m not sure if it is going to work fast in the case of having a huge number of inputs  any improvements for this  if there s a way  even other than using pydantic models  where i can work directly and avoid going through conversions and the loop 
821,821,3448011,71789149,tensorflow 2 use keras.sequence as data generator for training machine learning model with multiprocessing error,"<p>I would like to do a test about training a machine learning model on EC2 instance with only CPUs from jupyter notebook.</p>
<p>The code is tensorflow 2.8.
Based on the tf doc, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a>,</p>
<pre><code>use_multiprocessing Boolean. 
Used for generator or keras.utils.Sequence input only. 
If True, use process-based threading. If unspecified, use_multiprocessing will 
default to False. Note that because this implementation relies on multiprocessing, 
you should not pass non-picklable arguments to the generator as they can't be passed 
easily to children processes.
</code></pre>
<p>I am trying to enable &quot;use_multiprocessing&quot; for model.fit().</p>
<pre><code>import tensorflow.keras as keras
import tensorflow as tf
from tensorflow.keras.utils import Sequence

feature_format = {
                         &quot;a&quot;: tf.io.FixedLenFeature((1,), dtype=tf.int8),
                         &quot;b&quot;: tf.io.FixedLenFeature((1,), dtype=tf.int8),
                 }

  label_format = {
                 &quot;label&quot;: tf.io.FixedLenFeature((1,), dtype=tf.int64, default_value=(0,))
                 }

class DataGenerator(Sequence):
    def __init__(self):
        features = { 'a': tf.constant([1,2,3]),
                     'b': tf.constant([-1,-2,-3]),
                     'labels': tf.constant([1,1,0])
                }
        feature_dataset = tf.data.Dataset.from_tensors(features)
     
        tf.print(f&quot;type{feature_dataset}&quot;)
        for x in feature_dataset:
            print(x['a'])

    def _process(input_dataset):
        features = tf.io.parse_single_example(input_dataset, feature_format)
        labels = tf.io.parse_single_example(input_dataset, label_format)
        return (features, labels)
    
    feature_dataset = feature_dataset.shuffle(1)
    feature_dataset = feature_dataset.map(_process)
    feature_dataset = feature_dataset.cache()
    feature_dataset = feature_dataset.repeat(2)
    feature_dataset = feature_dataset.batch(1)
    self.feature_dataset = feature_dataset.prefetch(1)
    self.data_size = 3
    self.batch_size = 1
    
def __len__(self):
    return np.math.ceil(self.data_size/self.batch_size)

def __getitem__(self):
    return self.feature_dataset


class MyModel(keras.Model):

    def __init__(self):
        super().__init__()
        self.embedding_layer1 = keras.layers.embedding(10, 32)
        self.embedding_layer2 = keras.layers.embedding(10, 32)

    # the input data needs to be accessed by column name
    # so, the dataset is created with dictionary.
    def call(self, input_dataset, training=True):
        f1_data, f2_data = {}, {}
        for col in ['a']:
            f1_data[col] = input_dataset(col)
        for col in ['b']:
            f2_data[col] = input_dataset(col)
    
        f1_out = self.embedding_layer1(f1_data)
        f2_out = self.embedding_layer2(f2_data)
        result = tf.reduce_sum(tf.multiply(f1_out, f2_out))
        return result

model = MyModel()
dg = DataGenerator()
model.fit_generator(dg,
      epochs=2,
      workers=psutil.cpu_count(),
      use_multiprocessing=True
    )

TypeError: in user code:

features = tf.io.parse_single_example(input_dataset, feature_format)

TypeError: Expected any non-tensor type, but got a tensor instead.
</code></pre>
<p>How can I make a dataset such that it can be accepted by the parse_single_example()?
Currently, the dataset is a TensorDataset and each tensor can be accessed by a key.
thanks</p>
<p><strong>UPDATE</strong></p>
<p>I have tried the following answer.  But, got an error.</p>
<pre><code>import psutil
import numpy as np
class MyModel(keras.Model):

   def __init__(self):
        super().__init__()
        self.embedding_layer1 = keras.layers.Embedding(10, 32)
        self.embedding_layer2 = keras.layers.Embedding(10, 32)

    # the input data needs to be accessed by feature name
    def call(self, input_dataset, training=True):
        f1_data, f2_data = {}, {}
        for col in ['a']:
            f1_data[col] = input_dataset[col] # error !
        for col in ['b']:
            f2_data[col] = input_dataset[col]

        f1_out = self.embedding_layer1(f1_data)
        f2_out = self.embedding_layer2(f2_data)
        result = tf.reduce_sum(tf.multiply(f1_out, f2_out))
        return result

model = MyModel()
dg = DataGenerator()

model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001),
          loss = tf.keras.losses.BinaryCrossentropy(),
          run_eagerly=False,
          metrics=[keras.metrics.BinaryAccuracy()]
         )

 model.fit_generator(dg,
                    epochs=2,
                    workers=psutil.cpu_count(),
                    use_multiprocessing=True
                    )

 TypeError: Exception encountered when calling layer &quot;my_model_7&quot; (type MyModel).

'PrefetchDataset' object is not subscriptable

Call arguments received:
  • input_dataset=&lt;PrefetchDataset element_spec=({'a': 
  TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'b': 
  TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)}, 
  {'label': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)})&gt;
  • training=False
</code></pre>
",285,1,0,5,python;tensorflow;machine-learning;keras;tensorflow-datasets,2022-04-08 03:01:36,2022-04-08 03:01:36,2022-04-21 13:11:14,i would like to do a test about training a machine learning model on ec instance with only cpus from jupyter notebook  i am trying to enable  use_multiprocessing  for model fit    update i have tried the following answer   but  got an error 
822,822,196697,71950286,git download only some files matching pattern,"<p>What is the easiest way to clone only files matching a certain pattern, e.g. Java files, from a repository? I can do a <code>git clone</code> followed by a <code>git checkout</code> with some pattern, but this will result in downloading unnecessary files, potentially a large number of them, which I want to avoid.</p>
<p><strong>Context</strong>: I am building a tool to download code from a large number of repositories for a Machine Learning model training. I plan to download thousands or tens of thousands of repositories, so I want to make sure I only download the files I need, first, to speed up the process and, second, to reduce the likelihood of throttling from GitHub.</p>
<p>Thanks,
Rafid</p>
",14,0,1,3,git;github;github-cli,2022-04-21 12:40:18,2022-04-21 12:40:18,2022-04-21 12:40:18,what is the easiest way to clone only files matching a certain pattern  e g  java files  from a repository  i can do a git clone followed by a git checkout with some pattern  but this will result in downloading unnecessary files  potentially a large number of them  which i want to avoid  context  i am building a tool to download code from a large number of repositories for a machine learning model training  i plan to download thousands or tens of thousands of repositories  so i want to make sure i only download the files i need  first  to speed up the process and  second  to reduce the likelihood of throttling from github 
823,823,12682136,71948857,One hot encoding with ambiguitiy (nucleotide sequences),"<p>Nucleotide sequences (or DNA sequences) generally are comprised of 4 bases: ATGC which makes for a very nice, easy and efficient way of encoding this for machine learning purposes.</p>
<pre><code>sequence = AAATGCC
ohe_sequence = [[1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 1]]
</code></pre>
<p>But when you take into account RNA sequences and mistakes that can sometimes occur in sequencing machines, the letters UYRWSKMDVHBXN are added... When one-hot encoding this you end up with a matrix of 17 rows of which the last 13 rows are generally all 0's.</p>
<p>This is very inefficient and does not confer the biological meaning that these extra (ambiguous) letters have.</p>
<p>For example:</p>
<ul>
<li>T and U are interchangeable</li>
<li>Y represents there to be a C or T</li>
<li>N and X represent there to be any of the 4 bases (ATGC)</li>
</ul>
<p>And so I have made a dictionary that represents this biological meaning</p>
<pre><code>nucleotide_dict = {'A': 'A', 'T':'T', 'U':'T', 'G':'G', 'C':'C', 'Y':['C', 'T'], 'R':['A', 'G'], 'W':['A', 'T'], 'S':['G', 'C'], 'K':['T', 'G'], 'M':['C', 'A'], 'D':['A', 'T', 'G'], 'V':['A', 'G', 'C'], 'H':['A', 'T', 'C'], 'B':['T', 'G', 'C'], 'X':['A', 'T', 'G', 'C'], 'N':['A', 'T', 'G', 'C']}
</code></pre>
<p>But I can't seem to figure out how to make an efficient one-hot-encoding script (or wether there is a way to do this with the scikit learn module) that utilizes this dictionary in order to get a result like this:</p>
<pre><code>sequence = ANTUYCC
ohe_sequence = [[1, 0, 0, 0], [1, 1, 1, 1], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]]

# or even better:
ohe_sequence = [[1, 0, 0, 0], [0.25, 0.25, 0.25, 0.25], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0.5, 0, 0.5], [0, 0, 0, 1], [0, 0, 0, 1]]
</code></pre>
<p><a href=""https://i.stack.imgur.com/xz4t1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xz4t1.png"" alt=""This figure illustrates the nucleotide ambiguity"" /></a></p>
",82,3,0,4,python;machine-learning;deep-learning;one-hot-encoding,2022-04-21 09:41:10,2022-04-21 09:41:10,2022-04-21 12:01:17,nucleotide sequences  or dna sequences  generally are comprised of  bases  atgc which makes for a very nice  easy and efficient way of encoding this for machine learning purposes  but when you take into account rna sequences and mistakes that can sometimes occur in sequencing machines  the letters uyrwskmdvhbxn are added    when one hot encoding this you end up with a matrix of  rows of which the last  rows are generally all  s  this is very inefficient and does not confer the biological meaning that these extra  ambiguous  letters have  for example  and so i have made a dictionary that represents this biological meaning but i can t seem to figure out how to make an efficient one hot encoding script  or wether there is a way to do this with the scikit learn module  that utilizes this dictionary in order to get a result like this  
824,824,18226649,71941240,How to perform a search in a custom website and read results?,"<p>I am developing a function to download protein .pdb files online as part of a body of code I am creating to dock protein and ligands generated by our AIBind machine learning model. For around 60% of these proteins I am able to use gene libraries to convert their HGNC IDs to pdb IDs, which I then query through the website uniprot and RCSB to download pdb files. However, for the other 40% there only exist computationally generated alphafold PDB models for the proteins, and the gene libraries I have been using do not recognize these proteins as having valid PDB IDs. Thankfully, there is a search function on the alphafold website, where by searching with the HGNC ID, I recieve a list of entries (where the top one is 99% the protein I am looking for), as shown below;</p>
<p><a href=""https://i.stack.imgur.com/lRqWI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lRqWI.png"" alt=""enter image description here"" /></a></p>
<p>Once I have the uniprot ID (which is shown in this example as Q7K0E6), I can then navigate to the alphafold entry page and access the file server to download the PDB file for that protein, which I have already been able to successfully perform for proteins that have a registered uniprot ID in the databanks that I have been utilizing.</p>
<p>I've been using the following code to scrape the search webpage with the HGNC symbol inputted as a search entry, putting all of the HTML page data into a text file.</p>
<pre><code>  import urllib
  import urllib.request
  import requests

  url = 'https://alphafold.ebi.ac.uk/search/text/'
  fname = 'alphaname.txt'
  HGNC = 'vr1'
  url = url + 'vr1'

  get = urllib.request.urlopen(url)
  html = get.read()
  r = requests.get(url)
  with open(fname, &quot;wb&quot;) as f:
       f.write(html) 
</code></pre>
<p>When I perform a search in the file itself (manual as well as through python), I don't see any data from any of the entries queried as search results.</p>
<p><a href=""https://i.stack.imgur.com/iulQy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iulQy.png"" alt=""enter image description here"" /></a></p>
<p>How would I use python to retrieve data from searches performed within the search function of a website?</p>
",29,1,-1,3,python;web-scraping;beautifulsoup,2022-04-20 19:42:54,2022-04-20 19:42:54,2022-04-20 21:34:37,i am developing a function to download protein  pdb files online as part of a body of code i am creating to dock protein and ligands generated by our aibind machine learning model  for around   of these proteins i am able to use gene libraries to convert their hgnc ids to pdb ids  which i then query through the website uniprot and rcsb to download pdb files  however  for the other   there only exist computationally generated alphafold pdb models for the proteins  and the gene libraries i have been using do not recognize these proteins as having valid pdb ids  thankfully  there is a search function on the alphafold website  where by searching with the hgnc id  i recieve a list of entries  where the top one is   the protein i am looking for   as shown below   once i have the uniprot id  which is shown in this example as qke   i can then navigate to the alphafold entry page and access the file server to download the pdb file for that protein  which i have already been able to successfully perform for proteins that have a registered uniprot id in the databanks that i have been utilizing  i ve been using the following code to scrape the search webpage with the hgnc symbol inputted as a search entry  putting all of the html page data into a text file  when i perform a search in the file itself  manual as well as through python   i don t see any data from any of the entries queried as search results   how would i use python to retrieve data from searches performed within the search function of a website 
825,825,18878780,71939977,Azure SQL Managed Instance PREDICT with an ONNX model,"<p>I repeat the example from <a href=""https://docs.microsoft.com/en-us/azure/azure-sql-edge/deploy-onnx"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/azure-sql-edge/deploy-onnx</a>
&quot;Deploy and make predictions with an ONNX model and SQL machine learning&quot;
In this quickstart, you'll learn how to train a model, convert it to ONNX, deploy it
to Azure SQL Edge, and then run native PREDICT on data using the uploaded ONNX model.</p>
<p>Successfully create a model using Python, convert to onnx format,
I test the model using Python, save the model to the database, load
the necessary data and try to execute the SQL query</p>
<pre><code>USE onnx
DECLARE @model VARBINARY(max) = (
    SELECT DATA
    FROM dbo.models
    WHERE id = 1
    );
WITH predict_input
AS (
    SELECT TOP (1000) [id]
    , CRIM
    , ZN
    , INDUS
    , CHAS
    , NOX
    , RM
    , AGE
    , DIS
    , RAD
    , TAX
    , PTRATIO
    , B
    , LSTAT
FROM [dbo].[features]
)
SELECT predict_input.id
, p.variable1 AS MEDV
FROM PREDICT(MODEL = @model, DATA = predict_input, RUNTIME=ONNX) WITH (variable1 FLOAT) AS p;
</code></pre>
<p>As a result I get an error
Msg 102, Level 16, State 5, Line 27
Incorrect syntax near 'RUNTIME'.</p>
<p>I can't figure out what's wrong. The documentation clearly says
&quot;The RUNTIME = ONNX argument is only available in Azure SQL Edge,
Azure Synapse Analytics, and is in Preview in Azure SQL Managed Instance.&quot;</p>
",49,0,0,5,sql;azure;azure-sql-database;predict;onnx,2022-04-20 18:20:37,2022-04-20 18:20:37,2022-04-20 18:22:03,
826,826,8078693,44811228,remove unwanted space between body and footer,"<p>Hello i am new for css and html.. I need to remove the space between body and footer. Any help ?
I want to add an image at the same lines with paragraph </p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
&lt;head&gt;
&lt;title&gt;Neotic&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt; 
&lt;div&gt;
&lt;center&gt;&lt;img src=""{{STATIC_URL}}Neoticlogo.png"" height=""200"" width=""400""  alt=""Logo""  &gt;&lt;/center&gt;

&lt;center&gt; &lt;font face=""Arial"" size=""14.18"" color="" #587974"" &gt;Beat the markets with AI &lt;/font&gt;&lt;/center&gt;

&lt;/div&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;div class='row' style=""background: #3CB371"" &gt;
&lt;div class=""col-lg-4""&gt;

&lt;p style="" color:#EDEDED; font-family: Arial; font-size:+9; padding-left: 90px; width:400px"" align=""justify""&gt; Neotic is a trading support platform, that allows traders to test trading strategies and provides related trading recommendations leveraging artificial intelligence, without writing a single line of code.&lt;/p&gt; 


 &lt;/br&gt;
  &lt;p style="" color:#EDEDED; font-family: Arial; font-size:+9; padding-left: 90px; width:400px"" align=""justify""&gt; The artificial intelligence is based on a machine learning algorithm that incorporates corporate fundamentals, historical prices and financial news&lt;/p&gt; 
</code></pre>

<p></p>

<pre><code> &lt;b&gt; &lt;p style="" color:#EDEDED; font-family: Arial; font-size:+10; padding-left: 90px; width:400px"" align=""justify""&gt; We are upgrading our services and revamping our brand&lt;/p&gt; &lt;/b&gt;


&lt;/div&gt;
</code></pre>

<p></p>

<p></p>

<pre><code>  &lt;div class=""well""&gt;
   &lt;center&gt; &lt;p class=""text-muted"" style=""color:#EDEDED; font-family: Arial; font-size:+2"" &gt;©2017 Neotic. All rights reserved &lt;/p&gt; &lt;/center&gt;
&lt;/div&gt;
&lt;/footer&gt;


&lt;/body&gt;
</code></pre>

<p><a href=""https://i.stack.imgur.com/QieMF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QieMF.png"" alt=""Image""></a> </p>
",138,1,0,2,html;css,2017-06-29 01:10:11,2017-06-29 01:10:11,2022-04-20 15:26:36,    
827,827,4696214,40824043,Using matplotlib to create a spectrogram of a wavfile,"<pre><code>import scipy.io.wavfile as wav
import matplotlib.pyplot as plt
import scipy
sample_rate, X = wav.read(""/Users/sinaastani/Downloads/partynextdoor.wav"")
X = scipy.mean(X, axis=1)
plt.specgram(X, Fs=sample_rate, xextent=(0,30))
</code></pre>

<p>I get an error whenever I run the code above: </p>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/matplotlib/axes/_axes.py:7017: RuntimeWarning: divide by zero encountered in log10
  Z = 10. * np.log10(spec)
</code></pre>

<p>This occurs with several wav files that I have tried. I'm simply trying to replicate an example from ""Building Machine Learning Systems with Python - Second Edition"".</p>

<p>The wavfile.read function returns a numpy array. It looks like at the beginning and end of this array, there are a bunch of 0 values so when it tries to calculate log(0) it is undefined. What is the appropriate to deal with this? Should I simply get rid of 0 values from the array?</p>
",827,1,4,4,python;numpy;matplotlib;scipy,2016-11-27 04:32:57,2016-11-27 04:32:57,2022-04-20 12:24:28,i get an error whenever i run the code above   this occurs with several wav files that i have tried  i m simply trying to replicate an example from building machine learning systems with python   second edition  the wavfile read function returns a numpy array  it looks like at the beginning and end of this array  there are a bunch of  values so when it tries to calculate log   it is undefined  what is the appropriate to deal with this  should i simply get rid of  values from the array 
828,828,18872919,71933813,Enhance the accuracy of Teachable Machine,"<p>I'm working on creating Machine Learning models (image projects) by using Teachable Machine.  Does anyone have experience to improve the accuracy of the model creation by using Teachable Machine?</p>
<p>I currently work for the image project to identify the similar face from a different set of archived images and would like to improve the accuracy.
(what I already tried is: increased the sample pictures, added a variety of different facial expressions in each dataset)</p>
<p>Any tips/ advice you think would be effective will be apprecaited!</p>
",37,0,0,1,machine-learning,2022-04-20 09:07:20,2022-04-20 09:07:20,2022-04-20 09:07:20,i m working on creating machine learning models  image projects  by using teachable machine   does anyone have experience to improve the accuracy of the model creation by using teachable machine  any tips  advice you think would be effective will be apprecaited 
829,829,18867305,71930404,After Split-apply-combing in PySpark the code does not show the final result (from the book &quot;Data Analysis with Python and PySpark&quot; by JONATHAN RIOUX),"<p>After Split-apply-combing in PySpark the code does not show the final result (from the book &quot;Data Analysis with Python and PySpark&quot; by JONATHAN RIOUX), the code seems to be working, but fails to &quot;show&quot; results.
NOTE This is not a machine learning exercise: I am just using scikit-learn’s plumbing to create a feature.
We are Creating a grouped aggregate UDF.
Then Creating a grouped aggregate UDF.
Then a group map UDF to scale temperature values.
Split-apply-combing in PySpark.
Finally, Moving one station, one month’s worth of data into a local pandas DataFrame.</p>
<pre><code>&gt; The code seems to be working, but fails to &quot;show&quot; results:

    import pyspark
    import os
    import sys
    import pandas as pd
    from pyspark.sql import SparkSession
    # from pyspark.sql.functions import col, explode
    from functools import reduce
    import pyspark.sql.types as T
    import pyspark.sql.functions as F
    from sklearn.linear_model import LinearRegression

    spark = pyspark.sql.SparkSession.builder.appName(&quot;MyApp&quot;).getOrCreate()
    spark.sparkContext.setLogLevel(&quot;WARN&quot;)
    os.environ['PYSPARK_PYTHON'] = 'C:/bigdatasetup/anaconda3/envs/pyspark-env/python.exe'
    os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/bigdatasetup/anaconda3/envs/pyspark-env/python.exe'
    #os.environ['PYTHONPATH'] = &quot;:&quot;.join(sys.path)
</code></pre>
<pre><code>    gsod = (
        reduce(
            lambda x, y: x.unionByName(y, allowMissingColumns=True),
            [                
                spark.read.parquet(
                    f&quot;C:/bigdatasetup/spark/data/gsod_noaa/gsod{year}
                    .parquet&quot;)
                for year in range(2010, 2020)
            ],
        )    
        .dropna(subset=[&quot;year&quot;, &quot;mo&quot;, &quot;da&quot;, &quot;temp&quot;])    
        .where(F.col(&quot;temp&quot;) != 9999.9)
        .drop(&quot;date&quot;)
    )


&gt; Listing 9.8 Creating a grouped aggregate UDF

    @F.pandas_udf(T.DoubleType())
    def rate_of_change_temperature(day: pd.Series, temp: pd.Series) -   &gt; float:
        return (
            LinearRegression()
            .fit(X=day.astype(int).values.reshape(-1, 1), y=temp)
            .coef_[0]
        )


&gt; #Listing 9.10 A group map UDF to scale temperature values

    def scale_temperature(temp_by_day: pd.DataFrame) -&gt; pd.DataFrame:
        temp = temp_by_day.temp
        answer = temp_by_day[[&quot;stn&quot;, &quot;year&quot;, &quot;mo&quot;, &quot;da&quot;, &quot;temp&quot;]]
        if temp.min() == temp.max():
            return answer.assign(temp_norm = 0.5)
        return answer.assign(
            temp_norm = (temp-temp.min()) / (temp.max()-temp.min())
        )


&gt; # Listing 9.11 Split-apply-combing in PySpark

    gsod_map = gsod.groupby(&quot;stn&quot;, &quot;year&quot;, &quot;mo&quot;).applyInPandas(
        scale_temperature, 
        schema=(
            &quot;stn string, year string, mo string, &quot;
            &quot;da string, temp double, temp_norm double&quot;),
    )

    gsod_map.show(5, False)
&gt; # Here the showing does not work


&gt; # Listing 9.12 Moving one station, one month’s worth of data into a local pandas DataFrame

    gsod_local = gsod.where(
        &quot;year = '2018' and mo = '08' and stn = '710920'&quot;
    ).toPandas()

    print(
        rate_of_change_temperature.func(
            gsod_local[&quot;da&quot;], gsod_local[&quot;temp_norm&quot;]
            )   
    )
**&gt; # It gives this error:
&gt; # File &quot;C:\bigdatasetup\anaconda3\envs\pyspark-env\lib\site-packages
&gt; # \pandas\core\indexes\base.py&quot;, line 3623, in get_loc
&gt; # raise KeyError(key) from err
&gt; # KeyError: 'temp_norm'**
</code></pre>
",28,0,0,5,python;pandas;dataframe;apache-spark;pyspark,2022-04-20 01:06:31,2022-04-20 01:06:31,2022-04-20 05:53:59,
830,830,6579854,71932183,canonical use of _ as a variable in python,"<p>I'm currently reading a textbook studying machine learning and I came across a strange variable in the example code.</p>
<pre><code>class Regression(object):
# __init__
# set_data

def regress(self, query_point):
&quot;&quot;&quot;
Calculates predicted value for house with particular parameters
:param query_point: pandas series with house parameters
:return: house value
&quot;&quot;&quot;
_, indexes = self.kdtree.query(query_point, self.k)
value = self.metric(self.values.iloc[indexes])
if np.isnan(value):
    raise Exception('Unexpected result')
else:
    return value
</code></pre>
<p>I notice the line <code>_, indexes = self.kdtree.query(query_point, self.k)</code>, uses multiple assignment. Its first variable in an underscore. This variable is never used again in the method. This makes me think that the book just uses ' _ ' as a throw away variable. This is reinforced by further example code using ' _ ' as a for loop variable.</p>
<p>Does this follow any kind of known convention? Perhaps this is a convention handmedown from another language? I've never seen a bare underscore used a variable before and it really threw me off.</p>
",23,0,0,2,python;conventions,2022-04-20 04:22:04,2022-04-20 04:22:04,2022-04-20 04:23:58,i m currently reading a textbook studying machine learning and i came across a strange variable in the example code  i notice the line _  indexes   self kdtree query query_point  self k   uses multiple assignment  its first variable in an underscore  this variable is never used again in the method  this makes me think that the book just uses   _   as a throw away variable  this is reinforced by further example code using   _   as a for loop variable  does this follow any kind of known convention  perhaps this is a convention handmedown from another language  i ve never seen a bare underscore used a variable before and it really threw me off 
831,831,7775099,71928062,Identifying misclassified raw data in after machine learning in Python,"<p>I have a pretty basic question. My X data is <code>df['input']</code>, Y data is <code>df['label']</code>.  This is my code:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
Xfeatures = df['input']
y = df['label']

tfidf_vec = TfidfVectorizer(max_features= MF,
                           max_df = MAXDF)
X = tfidf_vec.fit_transform(Xfeatures)
featurenames = tfidf_vec.get_feature_names()
X.todense()
df_vec = pd.DataFrame(X.todense(),columns=tfidf_vec.get_feature_names())
df_vec.T
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(X, y,test_size = 0.33,random_state = 28)
</code></pre>
<p>This is the model that I run for text classification:</p>
<pre><code>from sklearn.svm import SVC
lr_model = SVC()
lr_model.fit(x_train,y_train)
y_pred = lr_model.predict(x_test)
# Acccuracy
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
</code></pre>
<p>I would like to identify the those that are misclassified (i.e. <code>df['input']</code>). I can write down predicted and actual the categories into csv, but not the text that is misclassified (or training data in general):</p>
<pre><code>import csv
rows = zip(y_test, y_pred)
with open(r&quot;C:\Users\erdem\Desktop\data.csv&quot;, &quot;w&quot;, newline=&quot;&quot;) as f:
    writer = csv.writer(f)
    for row in rows:
        writer.writerow(row) 
</code></pre>
",24,1,1,2,python;machine-learning,2022-04-19 21:41:23,2022-04-19 21:41:23,2022-04-20 03:02:48,i have a pretty basic question  my x data is df  input    y data is df  label     this is my code  this is the model that i run for text classification  i would like to identify the those that are misclassified  i e  df  input     i can write down predicted and actual the categories into csv  but not the text that is misclassified  or training data in general  
832,832,18021400,71930838,Protect python source code that runs as API,"<p>The company has built a python API with Machine Learning modules and we want to install this API on our customers' local server.</p>
<p>Of course, we don't want them to read the code, but when I try to use pyinstaller or pyarmor the fastAPI server can't understand the code anymore.</p>
<p>Is there a way to obfuscate or compile the python code and make it work with a server like fastapi with the uvicorn command?</p>
",60,0,0,5,python;api;compilation;obfuscation;fastapi,2022-04-20 01:46:42,2022-04-20 01:46:42,2022-04-20 01:46:42,the company has built a python api with machine learning modules and we want to install this api on our customers  local server  of course  we don t want them to read the code  but when i try to use pyinstaller or pyarmor the fastapi server can t understand the code anymore  is there a way to obfuscate or compile the python code and make it work with a server like fastapi with the uvicorn command 
833,833,18018480,71928808,"Stacking scikit, Ensemble Machine Learning With Python","<p>I am trying to create a stack with various algorithms to compare their performances, taking into consideration the feature scales I created which are in dictionary format.</p>
<p>I ave the stack code here, which gives me this error: <code>TypeError: 'numpy.float64' object is not callable</code></p>
<p>My X and y are respectively:</p>
<pre><code>array([[  2.47475454,   0.40165523,   1.68081787, ...,  -6.59044146,
         -2.21290585,  -3.139579  ],
       [  0.84802507,   2.81841945,  -2.76008732, ...,   3.00844461,
          0.78661954,  -1.27681551],
       [ -1.90041246,  -0.56901823,  -1.76220236, ...,   3.37336417,
         -2.28613707,   1.90344983],
</code></pre>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0
</code></pre>
<p>This is the stacking code:</p>
<pre><code>
from sklearn.datasets import make_classification 
from sklearn.model_selection import RepeatedStratifiedKFold

# get the dataset
def get_dataset():
    return X, y

def get_models():
    models = dict()
    models['lr'] = LogisticRegression()
    models['knn'] = KNeighborsClassifier()
    models['cart'] = DecisionTreeClassifier()
    models['svm'] = SVC()
    models['bayes'] = GaussianNB()
    return (models)
    
def evaluate_model(model, X, y):
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
    return scores

# define dataset
X, y = get_dataset()

# get the models to evaluate
models = get_models()

# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
    scores = evaluate_model(model, X, y)
    results.append(scores)
    names.append(name)
    print('&gt;%s %.3f (%.3f)' % (names, mean(scores), std(scores)))
    
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

</code></pre>
<p>I would like to fix this error, which I dont understand why its being called... then I would like to somehow include the iteration on the feature scales for normalisation etc</p>
",37,0,0,3,python;machine-learning;scikit-learn,2022-04-19 22:44:43,2022-04-19 22:44:43,2022-04-20 00:20:01,i am trying to create a stack with various algorithms to compare their performances  taking into consideration the feature scales i created which are in dictionary format  i ave the stack code here  which gives me this error  typeerror   numpy float  object is not callable my x and y are respectively  this is the stacking code  i would like to fix this error  which i dont understand why its being called    then i would like to somehow include the iteration on the feature scales for normalisation etc
834,834,18018480,71927295,Average of the values in a dictionary,"<p>I have a dictionary called <code>model_scores_for_datasets</code> that looks like this:</p>
<pre><code>{'Unprocessed': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.933', 'LinearDiscriminant': '1.000', 'K-Nearest Neighbour': '1.000', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Standardisation': {'Logistic Regression': '0.933', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.967', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Normalisation': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.967', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Rescale': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.933', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}}
{'Unprocessed': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.933', 'LinearDiscriminant': '1.000', 'K-Nearest Neighbour': '1.000', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Standardisation': {'Logistic Regression': '0.933', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.967', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Normalisation': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.967', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}, 'Rescale': {'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.933', 'LinearDiscriminant': '0.967', 'K-Nearest Neighbour': '0.967', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}}
</code></pre>
<p>I want to get the average for each, dictionary in the list of dictionaries. There are 4 total &quot;Unprocessed
Standardisation
Normalisation
Rescale&quot;
and 8 total metrics for each that look like this:</p>
<pre><code>{'Logistic Regression': '0.967', 'Support Vector Machine': '0.967', 'Decision Tree': '0.933', 'Random Forest': '0.933', 'LinearDiscriminant': '1.000', 'K-Nearest Neighbour': '1.000', 'Naive Bayes': '0.967', 'XGBoost': '0.933'}
</code></pre>
<p>So each of the 4 scales have 8 different ML altos and I want to get an average to say that for example on average &quot;standardisation&quot; scored the highest so it will be used during the machine learning process.</p>
<p>This is the code, but it giving me an error: <code>TypeError: can't convert type 'str' to numerator/denominator</code></p>
<pre><code>
avgDict = model_scores_for_datasets
for st,vals in avgDict.items():
    print(st,(vals))
    #print (st)
    for st,vals in avgDict.items():
        print(&quot;Average for {} is {}&quot;.format(st,mean(vals)))
</code></pre>
",33,3,0,2,python;dictionary,2022-04-19 20:44:22,2022-04-19 20:44:22,2022-04-19 21:06:41,i have a dictionary called model_scores_for_datasets that looks like this  so each of the  scales have  different ml altos and i want to get an average to say that for example on average  standardisation  scored the highest so it will be used during the machine learning process  this is the code  but it giving me an error  typeerror  can t convert type  str  to numerator denominator
835,835,16763713,71927015,How do OpenGL buffers relate to the VAO?,"<p>I'm currently learning OpenGL, but I'm having some problems understanding how the different buffers relate to the VAO. In my following code, I'm creating one VAO and two buffers (VBO for the vertex positions and EBO for the vertex order). At this point, if I understood it correctly, there is no connection between the VAO, VBO and EBO. I basically just created one VAO and two buffers. Now with <code>glBindVertexArray</code> and <code>glBindBuffer</code> I tell the OpenGL state machine my currently used VAO and assign my created buffers to a specific buffer type. With <code>glBufferData</code> I then load my data into the buffers. As I understand it, the VAO is still empty at this point and only gets data loaded into with the <code>glVertexAttribPointer</code> function. Now, OpenGL interprets the Data from <code>GL_ELEMENT_ARRAY_BUFFER</code> and loads them into the VAO at index 0. With <code>glEnableVertexAttribArray(0)</code> I then specify that the data at index 0 from my VAO (the vertex positions) should be used in the rendering process.</p>
<pre><code>unsigned int VAO, VBO, EBO;
glGenVertexArrays(1, &amp;VAO);
glGenBuffers(1, &amp;VBO);
glGenBuffers(1, &amp;EBO);

glBindVertexArray(VAO);

glBindBuffer(GL_ARRAY_BUFFER, VBO);
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);
    
glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);
glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);

glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), 0);
glEnableVertexAttribArray(0);
</code></pre>
<p>In my main loop, I specify a shader for my triangles, bind a VAO for rendering and then draw the elements followed by swapping the front and back buffers.</p>
<pre><code>glUseProgram(shader);
glBindVertexArray(VAO);
glDrawElements(GL_TRIANGLES, 9, GL_UNSIGNED_INT, 0);
glfwSwapBuffers(window);
</code></pre>
<p>But how exactly does OpenGL know the order from the vertices? I have only uploaded the vertex position data into my VAO, but not the order(element) data. Does <code>glVertexAttribPointer</code> perhaps take into account the currently bound data from the <code>GL_ELEMENT_ARRAY_BUFFER</code> when loading data into the VAO? What am I missing here?</p>
",41,1,0,4,opengl;buffer;opengl-3;vao,2022-04-19 20:23:03,2022-04-19 20:23:03,2022-04-19 20:36:02,i m currently learning opengl  but i m having some problems understanding how the different buffers relate to the vao  in my following code  i m creating one vao and two buffers  vbo for the vertex positions and ebo for the vertex order   at this point  if i understood it correctly  there is no connection between the vao  vbo and ebo  i basically just created one vao and two buffers  now with glbindvertexarray and glbindbuffer i tell the opengl state machine my currently used vao and assign my created buffers to a specific buffer type  with glbufferdata i then load my data into the buffers  as i understand it  the vao is still empty at this point and only gets data loaded into with the glvertexattribpointer function  now  opengl interprets the data from gl_element_array_buffer and loads them into the vao at index   with glenablevertexattribarray   i then specify that the data at index  from my vao  the vertex positions  should be used in the rendering process  in my main loop  i specify a shader for my triangles  bind a vao for rendering and then draw the elements followed by swapping the front and back buffers  but how exactly does opengl know the order from the vertices  i have only uploaded the vertex position data into my vao  but not the order element  data  does glvertexattribpointer perhaps take into account the currently bound data from the gl_element_array_buffer when loading data into the vao  what am i missing here 
836,836,1720743,62661128,How to remove 2d array from 3d array if it contains NA values,"<p>I am working on a seq2seq machine learning problem with Conv1D and LSTM, to do this I must produce a tensor input of the shape <code>samples, timesteps, features</code>. Aside from the problems that I was having with the LSTM layer (different topic). I find myself struggling to delete a 2d slice of my 3d input tensor if it contains NA value(s). I want to delete the entire sample if any feature, in anytimestep is NA.</p>
<p>Up until now to keep it simple i was working with univariate data and my solution was to simply transform my array into a pandas dataframe and use their <code>df.dropna(axis=0)</code> function to drop the entire sample. However that function only works with 2d dataframes. I've tried looping over my samples to produce 2d arrays that i can then convert into pandas dataframes, but got stuck trying to add the 2d arrays together again. And i figured, there has GOT to be a cleaner way to go about this. So i found this example:</p>
<pre><code>x = np.array([[[1,2,3], [4,5,np.nan]], [[7,8,9], [10,11,12]]])
print(&quot;Original array:&quot;)
print(x)
print(&quot;Remove all non-numeric elements of the said array&quot;)
print(x[~np.isnan(x).any(axis=2)])
</code></pre>
<p>which works for 2d arrays, but i figured it would work with any number of dimensions, I was wrong... I don't understand what I'm doing wrong here. For completeness sake, here is my function that successfully deletes input and its corresponding output from X_train and y_train if either X_train OR y_train contains NA value(s) (but this only works for univariate data as the 3rd dimension in the X_train tensor is of shape 1 and can therefore be dropped):</p>
<pre><code>def drop_days_with_na(df, df1):
    df_shape = df.shape
    df = df.reshape(df.shape[0], df.shape[1])
    df = np.concatenate((df, df1), axis=1)
    df = pd.DataFrame(df)
    na_index = df.isna()
    df = df.dropna(axis=0)
    df = np.array(df)
    df = df.reshape(df.shape[0], df.shape[1], 1)
    df1 = df[:,df_shape[1]:,:]
    df1 = df1.reshape(df1.shape[0], df1.shape[1])
    df = df[:,:df_shape[1],:]
    return df, df1, na_index
</code></pre>
",143,1,0,2,python;numpy,2020-06-30 21:07:26,2020-06-30 21:07:26,2022-04-19 19:28:11,i am working on a seqseq machine learning problem with convd and lstm  to do this i must produce a tensor input of the shape samples  timesteps  features  aside from the problems that i was having with the lstm layer  different topic   i find myself struggling to delete a d slice of my d input tensor if it contains na value s   i want to delete the entire sample if any feature  in anytimestep is na  up until now to keep it simple i was working with univariate data and my solution was to simply transform my array into a pandas dataframe and use their df dropna axis   function to drop the entire sample  however that function only works with d dataframes  i ve tried looping over my samples to produce d arrays that i can then convert into pandas dataframes  but got stuck trying to add the d arrays together again  and i figured  there has got to be a cleaner way to go about this  so i found this example  which works for d arrays  but i figured it would work with any number of dimensions  i was wrong    i don t understand what i m doing wrong here  for completeness sake  here is my function that successfully deletes input and its corresponding output from x_train and y_train if either x_train or y_train contains na value s   but this only works for univariate data as the rd dimension in the x_train tensor is of shape  and can therefore be dropped  
837,837,6225661,44908807,Jenkins deployment to Tomcat failed,"<p>I'm currently learning and implementing Jenkin's Pipeline for automated integration for java web application.</p>

<p>My plan is that once developers successfully commit and push their codes, Jenkins on admin server check out the code, build the project, stop the currently running Tomcat, deploy the project to the Tomcat, and restart.</p>

<p>For testing purpose, I'm currently trying everything, including running Tomcat and Jenkins, on local machine. But Jenkins is running its own (by executing command ""Java -jar jenkins.war"") but not using tomcat</p>

<p>Everything worked fine with pipeline except for starting Tomcat. 
below is the pipeline code snippet for tomcat restart.</p>

<pre><code>stage('Restart') {
        steps {
            echo 'Restarting Tomcat.....'
            sh ''' 
                cd ~/PATH-TO-Tomcat/apache-tomcat-8.0.44/bin
                ./startup.sh
            '''
        }
}    
</code></pre>

<p>As you can see, it simply goes to tomcat directory and execute the startup.sh script. However, when executing the restart stage, only the message ""Tomcat started."" is recored in console log file, but when checking with local host, Tomcat was not started at all. </p>

<p>I tried to start tomcat manually by executing command using terminal, it works okay.</p>

<p>Please help</p>

<p>Thank you</p>
",1691,2,2,5,tomcat;jenkins;groovy;continuous-integration;tomcat8,2017-07-04 20:11:16,2017-07-04 20:11:16,2022-04-19 18:55:52,i m currently learning and implementing jenkin s pipeline for automated integration for java web application  my plan is that once developers successfully commit and push their codes  jenkins on admin server check out the code  build the project  stop the currently running tomcat  deploy the project to the tomcat  and restart  for testing purpose  i m currently trying everything  including running tomcat and jenkins  on local machine  but jenkins is running its own  by executing command java  jar jenkins war  but not using tomcat as you can see  it simply goes to tomcat directory and execute the startup sh script  however  when executing the restart stage  only the message tomcat started  is recored in console log file  but when checking with local host  tomcat was not started at all   i tried to start tomcat manually by executing command using terminal  it works okay  please help thank you
838,838,17045296,71925433,Naive Bayes model cannot predict with python,"<p>Here this the code</p>
<pre><code>bayes = joblib.load(&quot;D:/model/bayess.pkl&quot;)
datas = [profile_pict,  num_username, _fullname_words, num_fullname, cocok, _desc, url, private, __post, __followers, __following ,_rasio]
identifikasi = bayes.predict([datas])
print(identifikasi)
</code></pre>
<p>And the error:</p>
<pre><code>C:\Users\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\base.py:329: UserWarning: Trying to unpickle estimator GaussianNB from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations
  warnings.warn(
Traceback (most recent call last):
  File &quot;bayes.py&quot;, line 132, in &lt;module&gt;
    identifikasi = bayes.predict([datas])
  File &quot;C:\Users\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\naive_bayes.py&quot;, line 83, in predict
    jll = self._joint_log_likelihood(X)
  File &quot;C:\Users\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\naive_bayes.py&quot;, line 489, in _joint_log_likelihood
    n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))
AttributeError: 'GaussianNB' object has no attribute 'var_'**

</code></pre>
<p>I did load the model using other machine learning models such as decision tree, svm and it worked, but, when using the naive bayes model, there was an error</p>
",52,0,1,2,python;scikit-learn,2022-04-19 18:33:20,2022-04-19 18:33:20,2022-04-19 18:49:41,here this the code and the error  i did load the model using other machine learning models such as decision tree  svm and it worked  but  when using the naive bayes model  there was an error
839,839,18864064,71925419,Resolving error 524 og Google Cloud Platform,"<p>I’m very new to cloud and wanted to try using it for machine learning. I set up the notebook to 8vCPUs 30GB RAM, with a T4 GPU. Somehow it runs slower compared to when I would work on colab, even when just loading the libraries. I haven’t reached to training the actual dataset (~200k images) because the site would eventually just error 524. Is there a way to resolve this?</p>
",24,0,0,1,google-cloud-platform,2022-04-19 18:32:09,2022-04-19 18:32:09,2022-04-19 18:32:09,i m very new to cloud and wanted to try using it for machine learning  i set up the notebook to vcpus gb ram  with a t gpu  somehow it runs slower compared to when i would work on colab  even when just loading the libraries  i haven t reached to training the actual dataset   k images  because the site would eventually just error   is there a way to resolve this 
840,840,13993985,71916174,Is there a way we can calculate estimated model training time when applying machine learning with scikitlearn or otherwise?,"<p>Many a times specially when the dataset is large or has multiple features, it takes ages (long hours) for sci-kit learn to train the model. Since it is using the computational resources, working on other things during this time on the same machine becomes exceptionally slow, thus, reducing overall productivity.</p>
<p>Is there  a way to estimate the time required for training a model? It doesnt have to be actually beforehand but it can be estimated once the training has started.</p>
<p>I have tried scitime but thats a very invasive method. Would prefer a method that is more tightly coupled with sklearn functionality.</p>
",49,0,1,3,python;machine-learning;scikit-learn,2022-04-19 00:54:15,2022-04-19 00:54:15,2022-04-19 15:07:42,many a times specially when the dataset is large or has multiple features  it takes ages  long hours  for sci kit learn to train the model  since it is using the computational resources  working on other things during this time on the same machine becomes exceptionally slow  thus  reducing overall productivity  is there  a way to estimate the time required for training a model  it doesnt have to be actually beforehand but it can be estimated once the training has started  i have tried scitime but thats a very invasive method  would prefer a method that is more tightly coupled with sklearn functionality 
841,841,7009990,71897851,Azure Machine Learning Computes - Template properties - Required properties for attach operation,"<p>As described in <a href=""https://docs.microsoft.com/en-us/azure/templates/microsoft.machinelearningservices/workspaces/computes?tabs=bicep"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/templates/microsoft.machinelearningservices/workspaces/computes?tabs=bicep</a> there are the properties <code>location</code>, <code>sku</code>, <code>tags</code> and <code>identity</code>.</p>
<p>For me it is not clear whether these properties relate to the parent workspace or to the compute resource (e.g. as the there is also <code>computeLocation</code> or <code>sku</code> as far as I can see should have the same value as the workspace)...</p>
<p>It would be great when someone can clarify to which resource these properties and related values belong to (workspace vs. compute resource).</p>
<p><strong>EDIT:</strong>
Also: which properties are actually required for attach versus create? E.g. do I need <code>identity</code> or <code>computeLocation</code> for attach, and if yes what is the purpose of it as the compute resource is being created in another context?</p>
<p>I also figured out that <code>location</code> as well as <code>disableLocalAuth</code> are required for the attach operation - why when the resource is being deployed in another context and only attached?</p>
<p>And why do I get <code>unsupported compute type</code> when checking for the compute resources via Azure CLI for the attached AKS?</p>
<pre><code>{
    &quot;description&quot;: &quot;Default AKS Cluster&quot;,
    &quot;id&quot;: &quot;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.MachineLearningServices/workspaces/xxx/computes/DefaultAKS&quot;,
    &quot;location&quot;: &quot;westeurope&quot;,
    &quot;name&quot;: &quot;DefaultAKS&quot;,
    &quot;provisioning_state&quot;: &quot;Succeeded&quot;,
    &quot;resourceGroup&quot;: &quot;xxx&quot;,
    &quot;resource_id&quot;: &quot;/subscriptions/xxx/resourcegroups/xxx/providers/Microsoft.ContainerService/managedClusters/xxx&quot;,
    &quot;type&quot;: &quot;*** unsupported compute type ***&quot;
}
</code></pre>
<p><strong>EDIT-2:</strong></p>
<p>So based on the response from @SairamTadepalli-MT all the properties actually relate to the compute resource - what makes sense. Still, I don't understand the purpose of a few of these properties. For instance why is there a &quot;location&quot; and a &quot;computeLocation&quot; or what is the meaning of &quot;sku&quot; (e.g. I tried &quot;AmlCompute&quot; and provided the value &quot;Basic&quot; - but &quot;Basic&quot; is the &quot;sku&quot; of the workspace and for &quot;AmlCompute&quot; the size is actually defined by the &quot;vmSize&quot; or?...).</p>
<p>What brings me to the next point: the current documentation currently lacks a detailed description in which scenarios which properties can have which values respectively need to be provided (beside &quot;properties&quot;).</p>
<p>This is also true for attach (i.e. providing a &quot;resourceId&quot;) vs. create (i.e. providing &quot;properties&quot;): which properties are actually required for attach? For what I figured out it requires &quot;location&quot; and &quot;disableLocalAuth&quot; - why do I need these properties as I would assume &quot;name&quot; and &quot;resourceId&quot; (and maybe &quot;computeType&quot;) should be sufficient to attach a compute resource? What is the purpose of properties like &quot;sku&quot;, &quot;tags&quot; or &quot;identity&quot; when I attach an existing compute resource?</p>
<p>Finally regarding &quot;unsupported compute type&quot;: not sure if your response really helps me. The AKS is successfully attached, so I don't understand why I get &quot;unsupported compute type&quot;. This should be fixed.</p>
",65,1,0,2,azure-machine-learning-service;azure-bicep,2022-04-17 03:44:33,2022-04-17 03:44:33,2022-04-19 12:52:58,as described in  there are the properties location  sku  tags and identity  for me it is not clear whether these properties relate to the parent workspace or to the compute resource  e g  as the there is also computelocation or sku as far as i can see should have the same value as the workspace     it would be great when someone can clarify to which resource these properties and related values belong to  workspace vs  compute resource   i also figured out that location as well as disablelocalauth are required for the attach operation   why when the resource is being deployed in another context and only attached  and why do i get unsupported compute type when checking for the compute resources via azure cli for the attached aks  edit   so based on the response from  sairamtadepalli mt all the properties actually relate to the compute resource   what makes sense  still  i don t understand the purpose of a few of these properties  for instance why is there a  location  and a  computelocation  or what is the meaning of  sku   e g  i tried  amlcompute  and provided the value  basic    but  basic  is the  sku  of the workspace and for  amlcompute  the size is actually defined by the  vmsize  or       what brings me to the next point  the current documentation currently lacks a detailed description in which scenarios which properties can have which values respectively need to be provided  beside  properties    this is also true for attach  i e  providing a  resourceid   vs  create  i e  providing  properties    which properties are actually required for attach  for what i figured out it requires  location  and  disablelocalauth    why do i need these properties as i would assume  name  and  resourceid   and maybe  computetype   should be sufficient to attach a compute resource  what is the purpose of properties like  sku    tags  or  identity  when i attach an existing compute resource  finally regarding  unsupported compute type   not sure if your response really helps me  the aks is successfully attached  so i don t understand why i get  unsupported compute type   this should be fixed 
842,842,16317165,71919670,Can the day of the month be encoded similarily to other cyclical variables?,"<p>I am working with time-series data in Python to see if variables like the time of day and the day of the month and the month of the year affect attendance at a gym. I have read up on encoding the time series data cyclicly using sine and cosine. I was wondering if you can do the same thing for the day of the month. The reason I ask is that, unlike the number of months in a year or the number of days in a week, the number of days in a month is variable (for example, February has 28, whereas March has 31). Is there any way to deal with that?</p>
<p>Here is a link describing what I mean by cyclic encoding: <a href=""https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/"" rel=""nofollow noreferrer"">https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/</a></p>
<p>Essentially, what this is saying is that you can't just convert the hour into a series of values like 1, 2, 3, ..., 24 when you are doing machine learning because that implies that the 24th hour is further away (from a euclidean geometric perspective) from the 1st hour than the 1st hour is from the 2nd hour, which is not true. Cyclical encoding (assigning sine and cosine values to each hour) allows you to represent the fact that the 24th hour and the 2nd hour are equidistant from the 1st hour.</p>
<p>My question is that I do not know if this cyclical conversion will work for days in a month, seeing as different months can have different numbers of days.</p>
",122,1,0,5,python;datetime;encoding;time-series;data-preprocessing,2022-04-19 09:55:58,2022-04-19 09:55:58,2022-04-19 10:33:29,i am working with time series data in python to see if variables like the time of day and the day of the month and the month of the year affect attendance at a gym  i have read up on encoding the time series data cyclicly using sine and cosine  i was wondering if you can do the same thing for the day of the month  the reason i ask is that  unlike the number of months in a year or the number of days in a week  the number of days in a month is variable  for example  february has   whereas march has    is there any way to deal with that  here is a link describing what i mean by cyclic encoding   essentially  what this is saying is that you can t just convert the hour into a series of values like             when you are doing machine learning because that implies that the th hour is further away  from a euclidean geometric perspective  from the st hour than the st hour is from the nd hour  which is not true  cyclical encoding  assigning sine and cosine values to each hour  allows you to represent the fact that the th hour and the nd hour are equidistant from the st hour  my question is that i do not know if this cyclical conversion will work for days in a month  seeing as different months can have different numbers of days 
843,843,7572426,47518978,What is my SQL missing?,"<p>I'm learning about PHP and MySQL. I have a lab that I'm working on where I have created a database (using the terminal in Ubuntu through a Virtual Machine) and I am trying to access this database using a PHP file.</p>

<p>I have created 1 database (demo) which holds 2 tables (demo_table, lab4) using this script</p>

<pre><code>CREATE TABLE `lab4` (
 id int(11) NOT NULL AUTO_INCREMENT,
 first_name varchar(100) DEFAULT NULL,
 last_name varchar(100) DEFAULT NULL,
 email varchar(128) DEFAULT NULL,
 phone varchar(25) DEFAULT NULL,
 PRIMARY KEY (id)
);
</code></pre>

<p>I can see through the terminal that they have been created the way I want, etc.</p>

<p>I have a php file (provided to me) in the appropriate directory (/var/www/html) which runs correctly, producing the output it is supposed to. Here is that file:</p>

<p>note: i have redacted my passwords!</p>

<pre><code>&lt;?php

function listRecords($dbc, $dbtable){
    $qry = ""SELECT * FROM "".$dbtable."";"";
    if($result = $dbc-&gt;query($qry)){
            if ($result-&gt;num_rows &gt; 0){
                while ($row = $result-&gt;fetch_assoc()){
                        foreach($row as $k=&gt;$v){
                Print $k.""  "".$v.""\t"";
            }
            print ""\n"";
                }
        } else {
                echo ""Nothing to output\n"";
        }
        $result-&gt;free();
} else {
        die(""Error running database query for categories\n  $qry \n"");
}
}

$db_conn = new mysqli('localhost', 'l*******r', '*******', 'demo');
if ($db_conn-&gt;connect_errno) {
die (""Could not connect to database server"".$db_host.""\n Error: "".$db_conn-
&gt;connect_errno .""\n Report: "".$db_conn-&gt;connect_error.""\n"");
}

$qry = ""INSERT INTO demo_table values('1', 'First User', 
'first@somewhare.com');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO demo_table values('2', 'Second User', 
'second@somewhare.com');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO demo_table values('3', 'Third User', 
'third@somewhare.com');"";
$db_conn-&gt;query($qry);

listRecords($db_conn, 'demo_table');


print ""\nDeleteing record 1\n"";
$db_conn-&gt;query(""DELETE FROM demo_table WHERE id='1';"");

listRecords($db_conn, 'demo_table');

print ""\nModifyingrecord 3\n"";

$db_conn-&gt;query(""UPDATE demo_table SET email='THIRD@newdomain.com' WHERE 
id='3';"");

listRecords($db_conn, 'demo_table');

$db_conn-&gt;close();

?&gt;
</code></pre>

<p>This was provided to me, and I have not changed it in any way from the way my instructor gave it to me. This confirms that i at least created the <code>demo_table</code> correctly.</p>

<p>I was tasked with creating another php file which will perform some other similar tasks (basic sql stuff). So, I have copied my instructor's <code>listRecords</code> function and then written a bunch of sql queries myself. Here is my code:</p>

<pre><code>&lt;?php
//A.


//function to be used in D. below
function listRecords($dbc, $dbtable)
{
$qry = ""SELECT * FROM "".$dbtable."";"";

if($result = $dbc-&gt;query($qry))
{
        if ($result-&gt;num_rows &gt; 0)
        {
                while ($row = $result-&gt;fetch_assoc())
                {
                        foreach($row as $k=&gt;$v)
                        {
                           Print $k.""  "".$v.""\t"";
                        }
                        print ""\n"";
                }
        }  else 
        {
            echo ""Nothing to output\n"";
        }
        $result-&gt;free();
} else 
{
    die(""Error running database query for categories\n  $qry \n"");
}
}

//B.
//create a connection object
$db_conn = new mysqli('localhost', '*********', '*******', 'demo');

//IF CONNECTION FAILED: display errors
if ($db_conn-&gt;connect_errno)
{
die (""Could not connect to database server"".$db_host.""\n Error: "".$db_conn-
&gt;connect_errno.""\n Report: "".$db_conn-&gt;connect_error.""\n"");
}

//C.
//IF CONNECTION SUCCEEDED: perform SQL
//single variable name can be reused
$qry = ""INSERT INTO lab4 values('First', 'User', 'first@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Second', 'User', 'second@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Third', 'User', 'third@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Fourth', 'User', 'fourth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Fifth', 'User', 'fifth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Sixth', 'User', 'sixth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Seventh', 'User', 'seventh@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Eighth', 'User', 'eighth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values('Ninth', 'User', 'ninth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);
$qry = ""INSERT INTO lab4 values(Tenth', 'User', 'tenth@somewhare.com', '1111111');"";
$db_conn-&gt;query($qry);

//D.
//display records
listRecords($db_conn, 'lab4');

//E.
//delete rows with IDs 2 and 4
$db_conn-&gt;query(""DELETE from lab4 WHERE id=2"");
$db_conn-&gt;query(""DELETE from lab4 WHERE id=4"");


//F.
//display all records after deletion in step E.
listRecords($db_conn, 'lab4');

//G.
//change email of 5th record
$db_conn-&gt;query(""UPDATE lab4 SET email='newemail@somewhare.com' WHERE         id=5"");

//H.
//change first name of 8th record
$db_conn-&gt;query(""UPDATE lab4 SET first_name='Newname' WHERE id=8"");

//I.
//change last name of 10th record
$db_conn-&gt;query(""UPDATE lab4 SET last_name='Lastname' WHERE id=10"");

//K.
//close the database connection
$db_conn-&gt;close();
?&gt;
</code></pre>

<p>So I have changed some of the formatting, and changed every instance of ""demo_table"" to ""lab4"".</p>

<p>When I run my php (lab4.php, located in same directory as the previous code) I get the message <code>Nothing to output</code> twice. My interpretation of this is that my table is working correctly and my listRecords is working correctly (as it is not giving me the error message associated with a failed connection).</p>

<p>My best guess is that it is a problem with my SQL queries. </p>
",94,2,2,3,php;mysql;ubuntu,2017-11-28 01:37:24,2017-11-28 01:37:24,2022-04-19 06:40:20,i m learning about php and mysql  i have a lab that i m working on where i have created a database  using the terminal in ubuntu through a virtual machine  and i am trying to access this database using a php file  i have created  database  demo  which holds  tables  demo_table  lab  using this script i can see through the terminal that they have been created the way i want  etc  i have a php file  provided to me  in the appropriate directory   var www html  which runs correctly  producing the output it is supposed to  here is that file  note  i have redacted my passwords  this was provided to me  and i have not changed it in any way from the way my instructor gave it to me  this confirms that i at least created the demo_table correctly  i was tasked with creating another php file which will perform some other similar tasks  basic sql stuff   so  i have copied my instructor s listrecords function and then written a bunch of sql queries myself  here is my code  so i have changed some of the formatting  and changed every instance of demo_table to lab  when i run my php  lab php  located in same directory as the previous code  i get the message nothing to output twice  my interpretation of this is that my table is working correctly and my listrecords is working correctly  as it is not giving me the error message associated with a failed connection   my best guess is that it is a problem with my sql queries  
844,844,13828837,71915063,GPU batch-processing of a large stack of matrices in PyTorch,"<p>I am quite new to pytorch so I am trying to write some examples. One of these example is the matrix-matrix product of M NxN matrices, i.e. a certain tensor contraction of two tensors of shape (M, N, N). The simplest way of doing that (on the CPU) is to use <a href=""https://pytorch.org/docs/stable/generated/torch.bmm.html#torch-bmm"" rel=""nofollow noreferrer""><code>torch.bmm</code></a> which directly operates on stacks of matrices. There is also an example in the link which I want to extend to processing on the GPU. If M is sufficiently small, it is possible to &quot;dump&quot; everything directly to the GPU by calling <code>torch.Tensor.to</code>:</p>
<pre><code>N = 5
M = 10
input = torch.randn(M, N, N).to(&quot;cuda&quot;)
mat2 = torch.randn(M, N, N).to(&quot;cuda&quot;)
res = torch.bmm(input, mat2)
res
# Prints the entire tensor (and I assume performs the
# actual calculation)
res.size() == torch.Size([M, N, N])
# prints True
res.is_cuda
# prints True
</code></pre>
<p>However, if for example M and N are in the order of 1000, this becomes unfeasible because <code>input</code> and <code>mat2</code> do not fit into the GPU memory (simultaneously) and the following exception is printed:</p>
<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 3.73 GiB (GPU 0; 2.00 GiB total capacity; 0 bytes already allocated; 1.56 GiB bytes free; 0 bytes reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>I have read parts of the referenced document, but it does not seem to directly mention a solution for this problem. My idea would here to process these stacks of matrices along M in chunks that fit into the GPU memory while using as much available space as possible and copying back the processed data into the CPU memory after the <code>torch.bmm</code> call.</p>
<p>The most straightforward way of doing so would be to simply use slicing to split <code>input</code> and <code>mat2</code>, copy the resulting tensors into device memory, call <code>torch.bmm</code> with <code>out</code> set to some device tensor and copy the data back from there. This piece of code is then put inside a loop and one can choose an appropriate chunk size.</p>
<p>However, to me it feels like something like this should already be implemented because this seems like a quite common use case, even if this is only quite &quot;low-level&quot; compared to what <code>pytorch</code> is developed for, which is machine learning. However, I am quite new to this and do not have any hands-on experience with that.</p>
<p>Secondly, with this splitting it is also possible to use GPU(s) and CPU(s) in tandem to process the matrix stack for maybe even better performance. What would be a first step at implementing something like this or is something like this maybe already implemented in <code>pytorch</code>?</p>
",62,0,0,3,python;pytorch;gpu,2022-04-18 22:59:51,2022-04-18 22:59:51,2022-04-19 06:12:20,i am quite new to pytorch so i am trying to write some examples  one of these example is the matrix matrix product of m nxn matrices  i e  a certain tensor contraction of two tensors of shape  m  n  n   the simplest way of doing that  on the cpu  is to use  which directly operates on stacks of matrices  there is also an example in the link which i want to extend to processing on the gpu  if m is sufficiently small  it is possible to  dump  everything directly to the gpu by calling torch tensor to  however  if for example m and n are in the order of   this becomes unfeasible because input and mat do not fit into the gpu memory  simultaneously  and the following exception is printed  runtimeerror  cuda out of memory  tried to allocate   gib  gpu     gib total capacity   bytes already allocated    gib bytes free   bytes reserved in total by pytorch  if reserved memory is  gt  gt  allocated memory try setting max_split_size_mb to avoid fragmentation   see documentation for memory management and pytorch_cuda_alloc_conf i have read parts of the referenced document  but it does not seem to directly mention a solution for this problem  my idea would here to process these stacks of matrices along m in chunks that fit into the gpu memory while using as much available space as possible and copying back the processed data into the cpu memory after the torch bmm call  the most straightforward way of doing so would be to simply use slicing to split input and mat  copy the resulting tensors into device memory  call torch bmm with out set to some device tensor and copy the data back from there  this piece of code is then put inside a loop and one can choose an appropriate chunk size  however  to me it feels like something like this should already be implemented because this seems like a quite common use case  even if this is only quite  low level  compared to what pytorch is developed for  which is machine learning  however  i am quite new to this and do not have any hands on experience with that  secondly  with this splitting it is also possible to use gpu s  and cpu s  in tandem to process the matrix stack for maybe even better performance  what would be a first step at implementing something like this or is something like this maybe already implemented in pytorch 
845,845,18854296,71918356,how to completely remove Visual studio code from MacBook Pro,"<p>I downloaded and installed the latest version and used it for a few weeks. I the process of learning how it worked, I loaded several extensions. Explored they and then uninstalled them. At the end of this nothing seamed to work as it should, the natural thing for me was to uninstall and then reinstall. directions I read said to goto the applications folder, locate visual studio code app and move it to trash. Which I did. Shut down and restarted the machine. Went back to the website and downloaded and installed another copy for MAC. When installed it was the exact version as before with all the sites, code and extensions as before.</p>
<p>I have tried every thing I know to get a totally fresh copy installed. Any help would be greatly appreciated.</p>
",20,1,0,1,visual-studio-code,2022-04-19 05:52:56,2022-04-19 05:52:56,2022-04-19 05:55:37,i downloaded and installed the latest version and used it for a few weeks  i the process of learning how it worked  i loaded several extensions  explored they and then uninstalled them  at the end of this nothing seamed to work as it should  the natural thing for me was to uninstall and then reinstall  directions i read said to goto the applications folder  locate visual studio code app and move it to trash  which i did  shut down and restarted the machine  went back to the website and downloaded and installed another copy for mac  when installed it was the exact version as before with all the sites  code and extensions as before  i have tried every thing i know to get a totally fresh copy installed  any help would be greatly appreciated 
846,846,17934590,71910500,how to forecast next time step using machine learning algorithm such as random forest,"<p>I want to know how to forecast the next time step using a machine learning algorithm such as random forest, xgboost etc
lets say you have sales data up to 10th April, and you are doing daily forecasting, how do you obtain the sales for 11th April using these algorithms</p>
",52,1,-1,4,python;time-series;random-forest;forecast,2022-04-18 15:46:40,2022-04-18 15:46:40,2022-04-18 18:55:27,
847,847,18843710,71908318,error : OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function &#39;resize&#39;,"<p>(im new to both machine learning and python so i wanted to see if i can do a project from github to learn while doing it)
there were a lot of errors i got that i was able to fix by myself with the help of stackoverflow but im unable to fix this one so i hope someone can help me with this</p>
<pre><code>global cap1     
cap1 = WebcamVideoStream(src=0).start() 
frame1 = cap1.read()
frame1 = cv2.resize(frame1,(600,500)) 
</code></pre>
<p>it says &quot;OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize' &quot;</p>
<p>i tried doing this</p>
<pre><code>frame1 = cv2.resize(frame1,(600,500),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)
</code></pre>
<p>but it still shows the same error</p>
<p>this is the code for WebcamVideoStream class :</p>
<pre><code>    class WebcamVideoStream:
    
    def __init__(self, src=0):
        self.stream = cv2.VideoCapture(src,cv2.CAP_DSHOW)
        (self.grabbed, self.frame) = self.stream.read()
        self.stopped = False

    def start(self):
            # start the thread to read frames from the video stream
        Thread(target=self.update, args=()).start()
        return self
        
    def update(self):
        # keep looping infinitely until the thread is stopped
        while True:
            # if the thread indicator variable is set, stop the thread
            if self.stopped:
                return
            # otherwise, read the next frame from the stream
            (self.grabbed, self.frame) = self.stream.read()

    def read(self):
        # return the frame most recently read
        return self.frame
    def stop(self):
        # indicate that the thread should be stopped
        self.stopped = True
</code></pre>
",270,0,0,3,python;opencv;imutils,2022-04-18 12:02:11,2022-04-18 12:02:11,2022-04-18 18:14:06,it says  opencv      io opencv modules imgproc src resize cpp   error     assertion failed   ssize empty   in function  resize    i tried doing this but it still shows the same error this is the code for webcamvideostream class  
848,848,18448792,71911266,Remote interpreter deployment for python script,"<p>I'm planning to run a machine learning script consistently on one of my Google Cloud VMs.</p>
<p>When I configured the remote interpreter, unfortunately all the imported libraries where not recognized anymore (as they might not be installed in the virtual envoirement in the cloud). I tried to install the missing modules (for example yfinance) through the Pycharm terminal extension within my remote host connection over SSH and SFTP. So I basically chose the 188.283.xxx.xxx @username in the Pycharm terminal, and used pip3 install to install the missing modules. Unfortnately my server (due to limited ressources) collapses during the build process.
Is there a way to automatically install the needed libraries when connecting the script to the remote interpreter?
Shouldn't that be the standard procedure? And if not: does my approach make sense?</p>
<p>Thank you all in advance</p>
<p>Peter</p>
",50,1,-1,5,python;pycharm;ftp;libraries;remote-server,2022-04-18 17:05:48,2022-04-18 17:05:48,2022-04-18 17:26:12,i m planning to run a machine learning script consistently on one of my google cloud vms  thank you all in advance peter
849,849,179081,25568249,How to query count of all tags on a Stack Exchange site in a single request,"<p>I'm experimenting with some machine learning techniques.
In this case <a href=""https://ieeexplore.ieee.org/document/4651388"" rel=""nofollow noreferrer"">PSO-KMean</a> for clustering.</p>
<p>I thought I might test it out by hitting the Stack Exchange API up
to grab a list of tags and a count of each tag,
then convert that into a array of floats representing each sites position in &quot;tag-space&quot;</p>
<p>I am using <a href=""https://github.com/lucjon/Py-StackExchange"" rel=""nofollow noreferrer"">Py-Stack-Exchange</a></p>
<pre><code>from stackauth import StackAuth
import stackexchange 

site_data = {}
n_sites= 20
for site_auth in StackAuth().sites()[3:n_sites+3]: #Skip big 3
    site=site_auth.get_site()
    site_tags = {}
    for tag in site.all_tags():
        site_tags[(tag.name)]=tag.count
    site_data[site.domain] = site_tags
</code></pre>
<p>Now this must have <a href=""https://api.stackexchange.com/docs/throttle"">gone over the 10,000 requests limit</a> after I messed around with it a few times  because
I got  <code>StackExchangeError: 502 [throttle_violation]: too many requests from this IP, more requests available in 81719 seconds</code></p>
<p>So I guess it is making a request for each and every tag on the site to get its count.
This is no good for anyone,
it is slower for me, and more work on the Stack Exchange Infrastructure.
I feel like there must be a way to get the information in 1 hit per site,
but am not familiar enough with the API to work it out.</p>
",404,1,1,2,python;stackexchange-api,2014-08-29 17:55:02,2014-08-29 17:55:02,2022-04-18 16:36:12,i am using 
850,850,18652980,71891070,How to avoid busy wait in a single threaded event loop,"<p>I'm coding a simple piano roll app that uses the SDL2 bindings for Rust to do its event handling and rendering. I have something very similar to the following code:</p>
<pre><code>let fps = 60;
let accumulator = 0; // Time in seconds

'running: loop {
    let t0 = std::time::Instant::now();
    poll_events();

    if accumulator &gt; 1.0 / fps {
        update();
        render();
        counter -= 1.0 / fps;
    }
    let t1 = std::time::Instant::now();
    let delta = (t1 - t0).as_secs_f64();
    accumulator += delta;

    // Busy wait?
}
</code></pre>
<p>Generally, the application is running fine and it doesn't have any noticeable artifacts, at least to my untrained eye. However, the CPU usage is through the roof, using nearly 25% in average (plus some GPU usage for the rendering).</p>
<p>I've checked the CPU usage of a very similar program which has many more features and also has better graphics, and when given the same MIDI notes to display, it averages at 2% CPU usage, plus 10% on the GPU side.</p>
<p>I also benchmarked my code, and found out the following approximated timings:</p>
<ul>
<li>poll_events() : ~0.002 ms</li>
<li>update()      : ~0.1 ms</li>
<li>render()      : ~1 ms</li>
</ul>
<p>Given that I'm aiming for 60 fps at both the logic level and the rendering level, I have roughly 16 milliseconds to do a full poll/update/render cycle. At the moment I'm using about 2 milliseconds (being generous) of the full range, so my conclusion is that the main loop has some busy wait going on, which I want to get rid of.</p>
<p>I've tried mainly sleep-based solutions which are quite unreliable, since the sleeping time depends on the operating system and at least in my machine (Windows 11) it's around 10-20 milliseconds, causing noticeable delays in the animation.</p>
<p>From what I read there are some thread-related solutions to avoid this kind of situation, but I feel like it's a totally unnecessary area to get into since I'm way below the point of needing any concurrency to squeeze more performance out of the machine.</p>
<p>I've been learning Rust for a couple of weeks, and although I've used SDL2 before in a smaller project using C++, I had the same problem and I couldn't find a suitable solution.</p>
<p>I'm not sure if this is a problem specifically related to SDL2 or if it also happens using other libraries, but any help would be very appreciated.</p>
",86,1,2,5,multithreading;rust;sdl-2;game-loop;busy-waiting,2022-04-16 09:52:19,2022-04-16 09:52:19,2022-04-18 09:43:54,i m coding a simple piano roll app that uses the sdl bindings for rust to do its event handling and rendering  i have something very similar to the following code  generally  the application is running fine and it doesn t have any noticeable artifacts  at least to my untrained eye  however  the cpu usage is through the roof  using nearly   in average  plus some gpu usage for the rendering   i ve checked the cpu usage of a very similar program which has many more features and also has better graphics  and when given the same midi notes to display  it averages at   cpu usage  plus   on the gpu side  i also benchmarked my code  and found out the following approximated timings  given that i m aiming for  fps at both the logic level and the rendering level  i have roughly  milliseconds to do a full poll update render cycle  at the moment i m using about  milliseconds  being generous  of the full range  so my conclusion is that the main loop has some busy wait going on  which i want to get rid of  i ve tried mainly sleep based solutions which are quite unreliable  since the sleeping time depends on the operating system and at least in my machine  windows   it s around   milliseconds  causing noticeable delays in the animation  from what i read there are some thread related solutions to avoid this kind of situation  but i feel like it s a totally unnecessary area to get into since i m way below the point of needing any concurrency to squeeze more performance out of the machine  i ve been learning rust for a couple of weeks  and although i ve used sdl before in a smaller project using c    i had the same problem and i couldn t find a suitable solution  i m not sure if this is a problem specifically related to sdl or if it also happens using other libraries  but any help would be very appreciated 
851,851,14602471,71902532,Machine Learning Model Choice: Use Features to Predict the Order/Schedule of Labels,"<p>The question is, now we have a list of items, and for each item, we have its features (multi-dimensional) and an order/schedule of labels (of another category of item, and we only know the labels instead of detailed information). Then can we learn from this to predict the order of labels if given an item and its features?</p>
<p>For example:
We have <em>item_1</em> and its features: {feature_11, feature_12, feature_13 ...} and an order {l1, l2, l3 ...}. Then given <em>item2</em> and its features {feature_21, feature_22, feature_23 ...}, could we predict the order of labels for <em>item2</em>?</p>
",20,0,0,3,machine-learning;data-mining;machine-learning-model,2022-04-17 19:33:52,2022-04-17 19:33:52,2022-04-17 21:55:01,the question is  now we have a list of items  and for each item  we have its features  multi dimensional  and an order schedule of labels  of another category of item  and we only know the labels instead of detailed information   then can we learn from this to predict the order of labels if given an item and its features 
852,852,17267446,71891627,"Model Evaluations (Precision,Recall, F1 Score) using Stratified K-Fold Cross Validation Machine Learning","<p>I have a data Set on which i have applied Stratified K Fold Cross Validation and split the data into 5 folds. Then i have applied Logistic Regression.
For Evaluation i have got precision recall and f1 score for each fold.
Finally i have to report these evaluations in numbers (precision,recall and f1 score)
am i allowed to average precision for all the 5 folds to present just average value
same for recall and f1 score.
as i have list of five values for each evaluation score after K Fold.</p>
",49,1,1,1,k-fold,2022-04-16 12:01:50,2022-04-16 12:01:50,2022-04-17 21:26:53,
853,853,179521,71901812,I changed a python file in docker container but it&#39;s not taking effect (print not working),"<p><strong>Main issue</strong> : print commands in python code files not showing up.</p>
<p>Apart from restarting the docker image, do I need to do something else ?
The application is a python machine learning model.</p>
<p>Here are the detailed steps of what I did:</p>
<ul>
<li>I have a logged into the docker container using the below command - <strong>docker exec -u 0 -it abx /bin/bash</strong></li>
<li>Changed a file. I put some print commands in it, just to make sure
they show up before I make any bigger change.</li>
<li>Restarted the docker container using <strong>docker stop abx</strong> &amp; then <strong>docker start abx</strong></li>
<li>I'm checking the logs by doing <strong>docker logs abx</strong></li>
</ul>
<p>What am I missing.. ?</p>
",20,0,0,3,python;docker;pytorch,2022-04-17 17:46:02,2022-04-17 17:46:02,2022-04-17 19:03:57,main issue   print commands in python code files not showing up  here are the detailed steps of what i did  what am i missing    
854,854,15335166,71902246,TypeError: only size-1 arrays can be converted to Python scalars ValueError: setting an array element with a sequence. sklearn,"<p>I'm new to machine learning, and I faced this error. I tried a lot but couldn't resolve it</p>
<pre><code>    # Function to compute histogram from LBP features
    def compute_lbp(image, eps=1e-7,numPoints=24,radius=8):
        # compute the Local Binary Pattern representation
        # of the image, and then use the LBP representation
        # to build the histogram of patterns
        lbp = feature.local_binary_pattern(image, numPoints,
            radius, method=&quot;uniform&quot;)
        (hist, _) = np.histogram(lbp.ravel(),
            bins=np.arange(0, numPoints + 3),
            range=(0, numPoints + 2))
        # normalize the histogram
        hist = hist.astype(&quot;float&quot;)
        hist /= (hist.sum() + eps)
        # return the histogram of Local Binary Patterns
        return hist
print(&quot;[INFO] extracting different features:  Raw Pixel values from image/Color Histogram/HOG/LBP&quot;)
train_data_hog=[]
#TODO : Add a vector to stock labels
train_labels=[]
train_row=[]
train_color=[]
train_lbp=[]
for imagePath in paths.list_images('caltech_dataset/train_set'):
  print (imagePath)
  #TODO: Retrieve the name of the class from each image path 
  #....

  label=imagePath.split('/')
  
  

  image = cv2.imread(imagePath)
  #TODO: Display the image
  #.... 
  cv2_imshow(image)
  
  
  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  (H, hogImage) = feature.hog(gray_image, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2),transform_sqrt=True, block_norm=&quot;L1&quot;,visualize=True)
  hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))
  hogImage = hogImage.astype(&quot;uint8&quot;)
  #TODO: Display HOG image
  #....
  cv2_imshow(hogImage)
  
  train_data_hog.append(H)
  #Raw Pixel values from image
  image1=image.flatten()
  
  train_row.append(image1)
  # histogramme de couleur
  img_hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV) 
  hist_hsv= cv2.calcHist([img_hsv], [0, 1],None, [180, 256], [0, 180, 0, 256])
  plt.plot(hist_hsv)
  plt.show()
  train_color.append(img_hsv)
  #Histogramm LBP
   
  
  hist_lbp= compute_lbp(gray_image)
  train_lbp.append(hist_lbp)
  plt.plot(hist_lbp)
  plt.show()
  #TODO: update labels vector
  #...  
  train_labels.append(label[2])

model1 = KNeighborsClassifier(n_neighbors=1)
model2 = SVC(kernel='linear')

#TODO: compute the different features on the testing set and stock the  corresponding test labels

test_data_hog=[]
#TODO : Add a vector to stock labels
test_labels=[]
test_row=[]
test_color=[]
test_lbp=[]
for imagePath in paths.list_images('caltech_dataset/test_set'):
  print (imagePath)
  #TODO: Retrieve the name of the class from each image path 
  #....

      label=imagePath.split('/')
      
      
    
      image = cv2.imread(imagePath)
      #TODO: Display the image
      #.... 
      cv2_imshow(image)
      
      
      gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
      (H, hogImage) = feature.hog(gray_image, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2),transform_sqrt=True, block_norm=&quot;L1&quot;,visualize=True)
      hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))
      hogImage = hogImage.astype(&quot;uint8&quot;)
      #TODO: Display HOG image
      #....
      cv2_imshow(hogImage)
      
      test_data_hog.append(H)
      #Raw Pixel values from image
      image1=image.flatten()
      
      test_row.append(image1)
      # histogramme de couleur
      img_hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV) 
      hist_hsv= cv2.calcHist([img_hsv], [0, 1],None, [180, 256], [0, 180, 0, 256])
      plt.plot(hist_hsv)
      plt.show()
      test_color.append(img_hsv)
      #Histogramm LBP
       
      
      hist_lbp= compute_lbp(gray_image)
      test_lbp.append(hist_lbp)
      plt.plot(hist_lbp)
      plt.show()
      #TODO: update labels vector
      #...  
      test_labels.append(label[2])
    # Train and predict based on model 2, evalute your results as well!
    # ...
    
    #HOG
    
    model1.fit(train_data_hog,train_labels)
    
    predicted_classes1 = model2.predict(test_data_hog)
    print(&quot; SVM HOG Accuracy:&quot;,metrics.accuracy_score(test_labels,predicted_classes1))
    #TODO: evalute the obtained results by comparing the predicted classes to the ground truth
</code></pre>
<p>ERROR in this line</p>
<pre><code>model1.fit(train_data_hog,train_labels)
TypeError                                 Traceback (most recent call last)
TypeError: only size-1 arrays can be converted to Python scalars

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-69-5eb73423a722&gt; in &lt;module&gt;()
      4 #HOG
      5 
----&gt; 6 model1.fit(train_data_hog,train_labels)
      7 
      8 predicted_classes1 = model2.predict(test_data_hog)

4 frames
/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    744                     array = array.astype(dtype, casting=&quot;unsafe&quot;, copy=False)
    745                 else:
--&gt; 746                     array = np.asarray(array, order=order, dtype=dtype)
    747             except ComplexWarning as complex_warning:
    748                 raise ValueError(

ValueError: setting an array element with a sequence
</code></pre>
",52,0,0,5,python;machine-learning;scikit-learn;svm;sklearn-pandas,2022-04-17 18:52:28,2022-04-17 18:52:28,2022-04-17 18:56:12,i m new to machine learning  and i faced this error  i tried a lot but couldn t resolve it error in this line
855,855,7330929,65940509,botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data,"<p>I want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in <a href=""https://towardsdatascience.com/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198"" rel=""nofollow noreferrer"">this blog post</a>.</p>
<pre><code>import mlflow.sagemaker as mfs

run_id = run_id # the model you want to deploy - this run_id was saved when we trained our model
region = &quot;us-east-1&quot; # region of your account
aws_id = &quot;XXXXXXXXXXX&quot; # from the aws-cli output
arn = &quot;arn:aws:iam::XXXXXXXXXXX:role/your-role&quot;
app_name = &quot;iris-rf-1&quot;
model_uri = &quot;mlruns/%s/%s/artifacts/random-forest-model&quot; % (experiment_id,run_id) # edit this path based on your working directory
image_url = aws_id + &quot;.dkr.ecr.&quot; + region + &quot;.amazonaws.com/mlflow-pyfunc:1.2.0&quot; # change to your mlflow version

mfs.deploy(app_name=app_name, 
           model_uri=model_uri, 
           region_name=region, 
           mode=&quot;create&quot;,
           execution_role_arn=arn,
           image_url=image_url)
</code></pre>
<p>But I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.</p>
<p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at <a href=""https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz</a>. Please ensure that the role &quot;arn:aws:iam::xxx:role/mlflow-sagemaker-dev&quot; exists and that its trust relationship policy allows the action &quot;sts:AssumeRole&quot; for the service principal &quot;sagemaker.amazonaws.com&quot;. Also ensure that the role has &quot;s3:GetObject&quot; permissions and that the object is located in us-east-1.</p>
",1434,2,0,3,amazon-web-services;amazon-sagemaker;mlflow,2021-01-28 21:17:29,2021-01-28 21:17:29,2022-04-17 14:43:36,i want to deploy an mlflow image to an aws sagemaker endpoint that contains a machine learning model  i executed the following code  which i found in   but i got the following error  i checked all policies and permissions attached to the iam role  they all comply with what the error message complains about  i don t know what to do next  i d appreciate your help  thanks  botocore exceptions clienterror  an error occurred  validationexception  when calling the createmodel operation  could not access model data at   please ensure that the role  arn aws iam  xxx role mlflow sagemaker dev  exists and that its trust relationship policy allows the action  sts assumerole  for the service principal  sagemaker amazonaws com   also ensure that the role has  s getobject  permissions and that the object is located in us east  
856,856,14714474,71900230,Will there be a problem if I don&#39;t add Python to environment variables? I have installed Python and worked on Pycharm and haven&#39;t had any problems,"<p>I am working on Python 3.9 with Pycharm and it has worked fine so far. Anytime I need to check something from the command prompt I write 'py [rest of the command]' and it does the work. I am trying to install jupyter now, and will be learning Machine Learning in future. Will this cause any problems?</p>
",22,1,0,3,python;jupyter-notebook;environment-variables,2022-04-17 13:28:24,2022-04-17 13:28:24,2022-04-17 13:39:56,i am working on python   with pycharm and it has worked fine so far  anytime i need to check something from the command prompt i write  py  rest of the command   and it does the work  i am trying to install jupyter now  and will be learning machine learning in future  will this cause any problems 
857,857,10956327,71890943,ModuleNotFoundError: No module named &#39;wrappers&#39; when trying to load file using Pickle,"<p>I'm trying to load a machine learning model <code>.sav</code> file using <code>Pickle</code></p>
<p>but it always outputs the error  <code>ModuleNotFoundError: No module named 'wrappers'</code></p>
<p>I tried adding the import wrapper after a <code>pip intall wrapper</code> but it didn't solve my problem,
and I didn't seem to find stuff related to this anywhere else</p>
<p>this is my code :</p>
<pre class=""lang-py prettyprint-override""><code>import pandas
import os
import sys
import numpy as np
import pickle
import wrapper

with open('C:\\Users\\SBS\\IMPORTED_DATA\\Trend_Perfect_SVC\\machine_learning_model.sav', 'rb') as file:
    response = pickle.load(file)
</code></pre>
<p>This is the complete error output:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_5184/102863235.py in &lt;module&gt;
      1 with open('C:\\Users\\SBS\\IMPORTED_DATA\\Trend_Perfect_SVC\\machine_learning_model.sav', 'rb') as file:
----&gt; 2     response = pickle.load(file)

ModuleNotFoundError: No module named 'wrappers'
</code></pre>
<p>I'm running this code on my local machine using the <code>JupyterLab</code> environment</p>
<p>I can provide a link to the file I'm using if it helps
Any help would be appreciated</p>
",110,1,0,5,python;tensorflow;machine-learning;keras;data-science,2022-04-16 09:20:52,2022-04-16 09:20:52,2022-04-17 12:11:22,i m trying to load a machine learning model  sav file using pickle but it always outputs the error  modulenotfounderror  no module named  wrappers  this is my code   this is the complete error output  i m running this code on my local machine using the jupyterlab environment
858,858,12371612,71895580,Deploying a Machine Learning Model by developing a web application without sharing the patient data i.e. without sending the data to the server,"<p>I have a machine learning model that has been trained using patient data. Now I want to deploy this model via a web application, so that doctors can use it!
I know I can use Flask and Django for this but there is a problem that the patient data should not be sent anywhere because of data privacy.</p>
<p>The patient data entered in the application should only remain on the end user's device as all data processing should be done locally in the application or web browser environment.</p>
<p>I know it is possible to do this with native app. But I want to avoid a native app as much as possible. I would like to have the model as a web page.</p>
<p>Please does anyone have an idea of how I can deploy a Machine Learning Model, so that the data entered by the user for the model to make the prediction also happens on the user's browser? I.e. no server, no anything like that. Everything (Input of new data, Predictions ) has to happen on the end user's device</p>
<p>Thanks in advance</p>
",19,0,1,5,javascript;python;json;machine-learning;web,2022-04-16 21:51:34,2022-04-16 21:51:34,2022-04-16 21:51:34,the patient data entered in the application should only remain on the end user s device as all data processing should be done locally in the application or web browser environment  i know it is possible to do this with native app  but i want to avoid a native app as much as possible  i would like to have the model as a web page  please does anyone have an idea of how i can deploy a machine learning model  so that the data entered by the user for the model to make the prediction also happens on the user s browser  i e  no server  no anything like that  everything  input of new data  predictions   has to happen on the end user s device thanks in advance
859,859,196697,71891423,Building a Dataset From a Large Number of GitHub Repositories for an NLP Project,"<p>I am working on a Machine Learning project for which I need as many code repositories as possible. What is the easiest to download/clone a large number of GitHub repositories? I assume I could use the GitHub API to search for repositories I want to download, e.g. repositories with &gt; 1k stars, and then I could start doing <code>git clone</code> on those repositories. However, in addition to being incredibly slow, I am pretty sure GitHub has some throttling mechanism, which would result in making this even slower if I am to implement some retry mechanism to overcome throttling. I appreciate some alternative ideas.</p>
<p>Thanks!</p>
",16,0,0,4,github;nlp;dataset;github-api,2022-04-16 11:13:58,2022-04-16 11:13:58,2022-04-16 11:13:58,i am working on a machine learning project for which i need as many code repositories as possible  what is the easiest to download clone a large number of github repositories  i assume i could use the github api to search for repositories i want to download  e g  repositories with  gt  k stars  and then i could start doing git clone on those repositories  however  in addition to being incredibly slow  i am pretty sure github has some throttling mechanism  which would result in making this even slower if i am to implement some retry mechanism to overcome throttling  i appreciate some alternative ideas  thanks 
860,860,14189979,71890647,(Solved) How to read from mongodb collection by pymongo when densely writing into it?,"<p><strong>Update</strong>:</p>
<p><strong>It is solved. Please check myself's answer if you are interested in it. Thanks to everyone all the same!</strong></p>
<p>My original post:</p>
<p><strong>MongoDB server version: 3.6.8 (WSL Ubuntu 20.04)</strong></p>
<p><strong>pymongo                 4.1.0</strong></p>
<p>I am learning <strong>machine learning</strong>. Because I feel TensorBoard is hard to use, I try to implement a simple &quot;<strong>traceable and visible training system</strong>&quot; (&quot;<strong>tvts</strong>&quot;) that has partial features of TensorBoard by MongoDB and pymongo. I choose MongoDB because it is document-based, NoSQL, and more suitable for recording arbitrary properties of model training.</p>
<p>Below is how I use it to record training conditions:</p>
<pre class=""lang-python prettyprint-override""><code>import tvts

# before training the modle
ts = tvts.tvts(NAME, '172.26.41.157', init_params={
    'ver': VER,
    'batch_size': N_BATCH_SIZE,
    'lr': LR,
    'n_epoch': N_EPOCH,
}, save_dir=SAVE_DIR, save_freq=SAVE_FREQ)

# after an epoch is done
ts.save(epoch, {
    'cost': cost_avg,
    'acc': metrics_avg[0][0],
    'cost_val': cost_avg_val,
    'acc_val': metrics_avg_val[0][0],
}, save_path)
</code></pre>
<p>I write all such data into a collection of my MondoDB, and then I can get statistics and charts like below:</p>
<pre class=""lang-text prettyprint-override""><code>Name: mnist_multi_clf_by_numpynet_mlp_softmax_ok from 172.26.41.157:27017 tvts.train_log
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| train_id | parent    | cost(min:last)  | LR(b-e:max-min) | epoch_count | existed save/save | from                       | to                         | duration       |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 1        | None-None | 1.01055:1.01055 | 0.1:0.1         | 100         | 10/10             | 2022-04-14 11:56:17.618000 | 2022-04-14 11:56:21.273000 | 0:00:03.655000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 2        | 1-100     | 0.56357:0.56357 | 0.1:0.1         | 100         | 10/10             | 2022-04-14 12:00:53.170000 | 2022-04-14 12:00:56.705000 | 0:00:03.535000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 3        | 2-100     | 0.15667:0.15667 | 0.1:0.1         | 300         | 15/15             | 2022-04-14 12:01:35.233000 | 2022-04-14 12:01:45.795000 | 0:00:10.562000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 4        | 3-300     | 0.06820:0.06839 | 0.1:0.1         | 300         | 15/15             | 2022-04-14 18:16:08.720000 | 2022-04-14 18:16:19.606000 | 0:00:10.886000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 5        | 2-100     | 0.03418:0.03418 | 0.5:0.5         | 200         | 10/10             | 2022-04-14 18:18:27.665000 | 2022-04-14 18:18:34.644000 | 0:00:06.979000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
| 6        | None-None | 1.68796:1.68858 | 0.001:0.001     | 3000        | 30/30             | 2022-04-16 09:15:56.085000 | 2022-04-16 09:18:01.608000 | 0:02:05.523000 |
+----------+-----------+-----------------+-----------------+-------------+-------------------+----------------------------+----------------------------+----------------+
</code></pre>
<p><a href=""https://i.stack.imgur.com/W6qBU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W6qBU.png"" alt=""enter image description here"" /></a></p>
<p>I found out that <strong>it get stuck if I try to get the list of statistics when I densely writing into the collection at the same time. I.e. I try to get the statistics on-the-fly of training and each epoch of the training is very short (about 0.03 second)</strong>.</p>
<p>But I found out that I can still read out the records by <strong>Stuido 3T (a GUI of MongoDB)</strong> when I densely writing into the collection.</p>
<p>I googled a lot, but I still cannot solve it. Someone said the writing lock is exclusive (such as link: <a href=""https://stackoverflow.com/questions/27309424/mongodb-write-is-occuring-then-a-read-must-wait-or-not-wait"">mongodb write is occuring then a read must wait or not wait?</a>), <strong>but why the Studio 3T can make it</strong>?</p>
<p>Acturally <strong>I am new to MongoDB</strong>, I can use it because I have a littel experience with MySQL and in this &quot;tvts&quot; there is only insertion and query, i.e. it is a rahter simple usage of MongoDB. Is there some equivalent concepts of &quot;concurrent inserts&quot; in MySQL? (such as link: <a href=""https://stackoverflow.com/questions/15637532/concurrent-read-and-write-on-mysql-table"">concurrent read and write in MySQL</a>) I guess it is not a very hard task of MongoDB to read from it when writing into it.</p>
<p>Although it is a simple simulation of partial features of TensorBoard, I have already coded almost 600 lines of code. So, I am sorry that <strong>changing database is not prefered</strong>.</p>
<p>Please help me. Thanks a lot!</p>
",23,1,0,2,mongodb;pymongo,2022-04-16 08:08:00,2022-04-16 08:08:00,2022-04-16 08:44:05,update  it is solved  please check myself s answer if you are interested in it  thanks to everyone all the same  my original post  mongodb server version      wsl ubuntu    pymongo                    i am learning machine learning  because i feel tensorboard is hard to use  i try to implement a simple  traceable and visible training system    tvts   that has partial features of tensorboard by mongodb and pymongo  i choose mongodb because it is document based  nosql  and more suitable for recording arbitrary properties of model training  below is how i use it to record training conditions  i write all such data into a collection of my mondodb  and then i can get statistics and charts like below   i found out that it get stuck if i try to get the list of statistics when i densely writing into the collection at the same time  i e  i try to get the statistics on the fly of training and each epoch of the training is very short  about   second   but i found out that i can still read out the records by stuido t  a gui of mongodb  when i densely writing into the collection  i googled a lot  but i still cannot solve it  someone said the writing lock is exclusive  such as link     but why the studio t can make it  acturally i am new to mongodb  i can use it because i have a littel experience with mysql and in this  tvts  there is only insertion and query  i e  it is a rahter simple usage of mongodb  is there some equivalent concepts of  concurrent inserts  in mysql   such as link    i guess it is not a very hard task of mongodb to read from it when writing into it  although it is a simple simulation of partial features of tensorboard  i have already coded almost  lines of code  so  i am sorry that changing database is not prefered  please help me  thanks a lot 
861,861,8473002,62922604,Python scikit learn pipelines (no transformation on features),"<p>I am running different machine learning models on my data set. I am using sklearn pipelines to try different transforms on the numeric features to evaluate if one transformation gives better results. The basic structure I am using is simple:</p>
<pre><code>from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScalar

pipe = Pipeline(steps=[('stdscaler', StandardScaler()), ('clf', RandomForestClassifier())])

pipe.fit(X_train, y_train)
</code></pre>
<p>I am trying a bunch of transformations but I also want to test the scenario where no transformation are performed on the numeric feature set (i.e. features are used as is). Is there a way to include that within the pipeline? Something like:</p>
<pre><code>pipe = Pipeline(steps=[('do nothing', do_nothing()), ('clf', RandomForestClassifier())])
</code></pre>
",708,1,3,2,python;scikit-learn,2020-07-16 01:08:59,2020-07-16 01:08:59,2022-04-16 04:47:07,i am running different machine learning models on my data set  i am using sklearn pipelines to try different transforms on the numeric features to evaluate if one transformation gives better results  the basic structure i am using is simple  i am trying a bunch of transformations but i also want to test the scenario where no transformation are performed on the numeric feature set  i e  features are used as is   is there a way to include that within the pipeline  something like 
862,862,0,71885279,"Ways to fullfil NaN Values for Intrusion Detection with ML, Unsupervised ML","<p>I created a CSV file. It contains 250800 rows and 75 columns. I am doing an EDA analysis to use the data for ML.
<a href=""https://i.stack.imgur.com/X7Y8S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X7Y8S.png"" alt=""enter image description here"" /></a></p>
<p>It looks like this. All of the columns are float or integer values. (df.info())
When I do :</p>
<pre><code>df.dropna()
</code></pre>
<p>It removes NaN values but the issue is that columns like <strong>protocol</strong> lose all unique values and just have one value, same for <strong>dstport</strong> and this is not something I want, losing data is not welcoming. As suggested here, I did this:</p>
<pre><code>df = df.dropna(subset = [&quot;Protocol&quot;,&quot;DstPort&quot;, &quot;State&quot;])
</code></pre>
<p>But the result is the same, still same NaN values, and cannot apply Kmeans clustering for example.</p>
<p>I would like to ask for your suggestion. What should I do? Can I fill these values somehow, but I don't know in which sense?
Which machine learning model I should choose?</p>
",59,1,0,5,python;pandas;machine-learning;intrusion-detection;network-flow,2022-04-15 19:58:13,2022-04-15 19:58:13,2022-04-16 04:34:25,it removes nan values but the issue is that columns like protocol lose all unique values and just have one value  same for dstport and this is not something i want  losing data is not welcoming  as suggested here  i did this  but the result is the same  still same nan values  and cannot apply kmeans clustering for example 
863,863,999452,71888343,Host cannot connect to docker composed kafka service,"<p>I've been following this guide on how to create my own apache Kafka docker-compose instance for my own learning.</p>
<ul>
<li><a href=""https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/messaging/kafka"" rel=""nofollow noreferrer"">https://github.com/marcel-dempers/docker-development-youtube-series/tree/master/messaging/kafka</a></li>
</ul>
<p>On my end, I only have a few services defined in my <code>docker-compose.yml</code></p>
<pre><code>version: &quot;3.8&quot;
services:
  zookeeper-1:
    container_name: zookeeper-1
    build:
      context: ./zookeeper
    volumes:
      - ./zookeeper/zookeeper.properties:/kafka/config/zookeeper.properties
      - ./data/zookeeper-1:/tmp/zookeeper/
    networks:
      - kafka
  kafka-1:
    container_name: kafka-1
    build:
      context: ./kafka
    volumes:
      - ./kafka/server.properties:/kafka/config/server.properties
      - ./data/kafka-1:/tmp/kafka-logs/
    ports:
      - &quot;9092:9092&quot;
    networks:
      - kafka
networks:
  kafka:
    name: kafka
</code></pre>
<p>When I do a <code>docker compose up</code> then a <code>docker ps</code> get me this</p>
<pre><code>CONTAINER ID   IMAGE                      COMMAND                CREATED         STATUS         PORTS                    NAMES
4aaf6e37e999   apacha-kafka_zookeeper-1   &quot;start-zookeeper.sh&quot;   2 minutes ago   Up 2 minutes                            zookeeper-1
1a8ca86cb2d2   apacha-kafka_kafka-1       &quot;start-kafka.sh&quot;       2 minutes ago   Up 2 minutes   0.0.0.0:9092-&gt;9092/tcp   kafka-1

</code></pre>
<p>The issue I am getting is that I cannot access <code>kafka-1</code> outside of the docker local instance. I've tried to consume any messages from any topic from the host machine via  <code>&lt;kafka-host-directory&gt;/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic example --from-beginning</code> it always retuns me this</p>
<pre><code>[2022-04-16 03:48:01,673] WARN [Consumer clientId=console-consumer, groupId=console-consumer-8373] Error connecting to node 1a8ca86cb2d2:9092 (id: 1 rack: null) (org.apache.kafka.clients.NetworkClient)
java.net.UnknownHostException: 1a8ca86cb2d2
    at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
    at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
    at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
    ...
</code></pre>
<p>Do I need to add <code>EXPOS 9092</code> within <code>./kafka/Dockerfile</code>, but even that approach didn't do anything despite rebuilding the image and the stack.</p>
",11,0,0,3,docker;apache-kafka;docker-compose,2022-04-16 01:22:28,2022-04-16 01:22:28,2022-04-16 01:22:28,i ve been following this guide on how to create my own apache kafka docker compose instance for my own learning  on my end  i only have a few services defined in my docker compose yml when i do a docker compose up then a docker ps get me this the issue i am getting is that i cannot access kafka  outside of the docker local instance  i ve tried to consume any messages from any topic from the host machine via   lt kafka host directory gt  bin kafka console consumer sh   bootstrap server localhost    topic example   from beginning it always retuns me this do i need to add expos  within   kafka dockerfile  but even that approach didn t do anything despite rebuilding the image and the stack 
864,864,16721845,71886446,"preparing two df for prediction; predictor is string, numbers are in second df; dictionary, map in machine learning?","<p>I want to predict &quot;score&quot; from df &quot;A&quot; using &quot;1,2,3,4,5&quot; as predictors. Goal is to mix up 5 new predictors/names and get the dependent variable &quot;score&quot;. The numbers behind each independent variable are in df &quot;B&quot;. There are about 300 names/predictors in different combinations in df &quot;A&quot;, number of rows 2500.</p>
<p>I thought about a dictionary, nested variable. I added the numbers from df &quot;B&quot; already using map and dictionary but in a form not suitable for the prediction (additional columns behind score). What could be the best way to set up/combine these two df for a prediction model?</p>
<p>df &quot;A&quot;</p>
<pre><code>import pandas as pd

data = {'1':['Tom','Peter', 'Frank'],
        '2':['Julius', 'Zach', 'Thomas'],
               '3':['Ervin', 'Larry', 'Devin'],
       '4':['Nigel', 'Frank', 'John' ],
       '5': ['Jalen', 'Jamar', 'Phil'],
       'Score': [2, 3, 1]}
 

df = pd.DataFrame(data)
 
print(df)
   
      1       2      3      4      5      Score
0    Tom  Julius  Ervin  Nigel  Jalen      2
1  Peter    Zach  Larry  Frank  Jamar      3
2  Frank  Thomas  Devin   John   Phil      1
</code></pre>
<p>df &quot;B&quot;</p>
<pre><code>import pandas as pd
data = {'Name':['Tom','Peter', 'Frank', 'Julius', 'Zach', 'Thomas', 'Ervin', 'Larry', 'Devin', 'Nigel', 'Frank', 'John', 'Jalen', 'Jamar', 'Phil'],
        'A':[0.75, 0.78, 0.91, 0.54, 0.92, 0.88, 0.81, 0.23, 0.25, 0, 0.71, 0.64, 0, 0, 0.82 ],
               'B':[0.34, 0.43, 0.55, 0.65, 0.21, 0.98, 0.77, 0, 0.67, 0.78, 0.12, 0.22, 0, 0.76, 0],
       'C':[0.91, 0.64, 0.33, 0.50, 0.75, 0.89, 0.82, 0.72, 0.62, 0.87, 0.21, 0.45, 0.56, 0, 0],
       'D': [0.12, 0.33, 0.65, 0.79, 0.41, 0.78, 0.66, 0.82, 0.74, 0.81, 0, 0.51, 0.44, 0, 0.73],
       'E': [ 0.50, 0.65, 0.98, 0.91, 0.51, 0.42, 0.38, 0.85, 0.21, 0.67, 0, 0.53, 0, 0.48, 0.69]}
 

df2 = pd.DataFrame(data)
 

print(df2)

      Name     A     B     C     D     E
0      Tom  0.75  0.34  0.91  0.12  0.50
1    Peter  0.78  0.43  0.64  0.33  0.65
2    Frank  0.91  0.55  0.33  0.65  0.98
3   Julius  0.54  0.65  0.50  0.79  0.91
4     Zach  0.92  0.21  0.75  0.41  0.51
5   Thomas  0.88  0.98  0.89  0.78  0.42
6    Ervin  0.81  0.77  0.82  0.66  0.38
7    Larry  0.23  0.00  0.72  0.82  0.85
8    Devin  0.25  0.67  0.62  0.74  0.21
9    Nigel  0.00  0.78  0.87  0.81  0.67
10   Frank  0.71  0.12  0.21  0.00  0.00
11    John  0.64  0.22  0.45  0.51  0.53
12   Jalen  0.00  0.00  0.56  0.44  0.00
13   Jamar  0.00  0.76  0.00  0.00  0.48
14    Phil  0.82  0.00  0.00  0.73  0.69
</code></pre>
<p>Possible Outcome?</p>
<pre><code>      1       2      3      4      5      Score   Tom A Tom B ....  Julius A
                                                                 
0  Tom  Julius  Ervin  Nigel  Jalen        2      0.75   0.34       0.54
1  Peter    Zach  Larry  Frank  Jamar      3
2  Frank  Thomas  Devin   John   Phil      1
</code></pre>
",18,0,0,4,pandas;dataframe;machine-learning;prediction,2022-04-15 21:51:28,2022-04-15 21:51:28,2022-04-15 23:14:41,i want to predict  score  from df  a  using        as predictors  goal is to mix up  new predictors names and get the dependent variable  score   the numbers behind each independent variable are in df  b   there are about  names predictors in different combinations in df  a   number of rows   i thought about a dictionary  nested variable  i added the numbers from df  b  already using map and dictionary but in a form not suitable for the prediction  additional columns behind score   what could be the best way to set up combine these two df for a prediction model  df  a  df  b  possible outcome 
865,865,4126415,71886572,Azure ML Studio - Error when deploying real-time inference to endpoint,"<p>I just started working with Machine Learning using Azure ML Studio.  I have been able to successfully train a model and create a real-time inference pipeline as well.  When I try to deploy the real-time inference pipeline to an endpoint, it keeps vascillating between Unhealthy and Failed.</p>
<p>When I look through the logs, there is only three logs that point to a possible problem:</p>
<blockquote>
<p>Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (urllib3 1.26.9 (/azureml-envs/azureml_9a27d0b682f7325ef536eaeb801b2a62/lib/python3.6/site-packages), Requirement.parse('urllib3&lt;=1.26.7,&gt;=1.23'), {'azureml-core'}).</p>
</blockquote>
<blockquote>
<p>Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (urllib3 1.26.9 (/azureml-envs/azureml_9a27d0b682f7325ef536eaeb801b2a62/lib/python3.6/site-packages), Requirement.parse('urllib3&lt;=1.26.7,&gt;=1.23'), {'azureml-core'}).</p>
</blockquote>
<blockquote>
<p>Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (urllib3 1.26.9 (/azureml-envs/azureml_9a27d0b682f7325ef536eaeb801b2a62/lib/python3.6/site-packages), Requirement.parse('urllib3&lt;=1.26.7,&gt;=1.23')).</p>
</blockquote>
<p>While this maybe something easy to fix if this was built in a Notebook, all of my components were built using Azure ML Studio, so I didn't acutally write any code to perform these steps, just using the Azure ML Studio UI.</p>
<p>Any help towards fixing this would be appreciated.</p>
",31,0,0,2,azure;machine-learning,2022-04-15 22:02:35,2022-04-15 22:02:35,2022-04-15 22:02:35,i just started working with machine learning using azure ml studio   i have been able to successfully train a model and create a real time inference pipeline as well   when i try to deploy the real time inference pipeline to an endpoint  it keeps vascillating between unhealthy and failed  when i look through the logs  there is only three logs that point to a possible problem  failure while loading azureml_run_type_providers  failed to load entrypoint azureml reusedsteprun   azureml pipeline core run steprun _from_reused_dto with exception  urllib      azureml envs azureml_adbfefeaebba lib python  site packages   requirement parse  urllib lt      gt         azureml core     failure while loading azureml_run_type_providers  failed to load entrypoint azureml steprun   azureml pipeline core run steprun _from_dto with exception  urllib      azureml envs azureml_adbfefeaebba lib python  site packages   requirement parse  urllib lt      gt         azureml core     failure while loading azureml_run_type_providers  failed to load entrypoint azureml scriptrun   azureml core script_run scriptrun _from_run_dto with exception  urllib      azureml envs azureml_adbfefeaebba lib python  site packages   requirement parse  urllib lt      gt        while this maybe something easy to fix if this was built in a notebook  all of my components were built using azure ml studio  so i didn t acutally write any code to perform these steps  just using the azure ml studio ui  any help towards fixing this would be appreciated 
866,866,12585838,71885179,Create train and test with lags of multiple features,"<p>I have a classification problem for which I want to create a train and test dataframe with 21 lags of multiple features (X-variables). I already have an easy way to do this with only one feature but I don't know how to adjust this code if I want to use more variables (e.g. df['ETHLogReturn']).</p>
<p>The code I have for one variable is:</p>
<pre><code>Ntest = 252
train = df.iloc[:-Ntest]
test = df.iloc[-Ntest:]

# Create data ready for machine learning algoritm
series = df['BTCLogReturn'].to_numpy()[1:] # first change is NaN

# Did the price go up or down?
target = (targets &gt; 0) * 1

T = 21 # 21 Lags
X = []
Y = []
for t in range(len(series)-T):
  x = series[t:t+T]
  X.append(x)
  y = target[t+T]
  Y.append(y)
  
X = np.array(X).reshape(-1,T)
Y = np.array(Y)
N = len(X)
print(&quot;X.shape&quot;, X.shape, &quot;Y.shape&quot;, Y.shape)

#output --&gt; X.shape (8492, 21) Y.shape (8492,)
</code></pre>
<p>Then I create my train and test datasets like this:</p>
<pre><code>Xtrain, Ytrain = X[:-Ntest], Y[:-Ntest]
Xtest, Ytest = X[-Ntest:], Y[-Ntest:]

# example of model:
lr = LogisticRegression()
lr.fit(Xtrain, Ytrain)
print(lr.score(Xtrain, Ytrain))
print(lr.score(Xtest, Ytest))
</code></pre>
<p>Does anyone have a suggestion how to adjust this code for a model with lagging variables of multiple columns? Like:</p>
<pre><code>df[['BTCLogReturn','ETHLogReturn']]
</code></pre>
<p>Many thanks for your help!</p>
",15,0,0,5,python;classification;logistic-regression;series;train-test-split,2022-04-15 19:48:36,2022-04-15 19:48:36,2022-04-15 19:48:36,i have a classification problem for which i want to create a train and test dataframe with  lags of multiple features  x variables   i already have an easy way to do this with only one feature but i don t know how to adjust this code if i want to use more variables  e g  df  ethlogreturn     the code i have for one variable is  then i create my train and test datasets like this  does anyone have a suggestion how to adjust this code for a model with lagging variables of multiple columns  like  many thanks for your help 
867,867,4848960,55252394,CreateML MLModel works on playground UI but not in app,"<p>I'm working on a machine learning app that classifies numbers that are hand drawn. I have made a model using CreateML that supposedly has 100% accuracy (I will admit my sample size was only about 50 images per number). When running it on my app however, it does not work. To see if it was a problem with my app, I downloaded the <a href=""https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml"" rel=""nofollow noreferrer"">Apple Vision+CoreML Example</a> Xcode project and replaced the MobileNet classifier with my own. I loaded in the images saved on my phone from my own app and the classifications were still inaccurate. What makes this interesting is that I tried testing the exact same images in the CreateML UI space on the playground where you can test images and the classification works.</p>

<p>TL/DR: The image classification works on the CreateML Live View on playgrounds but does not on the exact copy of the vision+coreML example project from Apple.</p>

<p><a href=""https://i.stack.imgur.com/xb3cr.jpg"" rel=""nofollow noreferrer"">Here is an example of an image that I tried to classify</a></p>

<p><a href=""https://i.stack.imgur.com/UZ9B5.png"" rel=""nofollow noreferrer"">Here is what shows up on the app for 7</a>, <a href=""https://i.stack.imgur.com/8ATqu.png"" rel=""nofollow noreferrer"">Here is what shows up on the app for 5</a></p>

<p><a href=""https://i.stack.imgur.com/lb1oG.png"" rel=""nofollow noreferrer"">Here is what shows up on the playground for 7</a>, <a href=""https://i.stack.imgur.com/dsvOi.png"" rel=""nofollow noreferrer"">Here is what shows up on the playground for 5</a></p>
",212,1,3,5,ios;swift;coreml;apple-vision;createml,2019-03-20 07:15:16,2019-03-20 07:15:16,2022-04-15 19:25:05,i m working on a machine learning app that classifies numbers that are hand drawn  i have made a model using createml that supposedly has   accuracy  i will admit my sample size was only about  images per number   when running it on my app however  it does not work  to see if it was a problem with my app  i downloaded the  xcode project and replaced the mobilenet classifier with my own  i loaded in the images saved on my phone from my own app and the classifications were still inaccurate  what makes this interesting is that i tried testing the exact same images in the createml ui space on the playground where you can test images and the classification works  tl dr  the image classification works on the createml live view on playgrounds but does not on the exact copy of the vision coreml example project from apple        
868,868,18798648,71865371,Pandas calculating sales for recurring monthly payments,"<p>I have a dataset with millions of records just like below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>CustomerID</th>
<th>StartTime</th>
<th>EndTime</th>
</tr>
</thead>
<tbody>
<tr>
<td>1111</td>
<td>2015-7-10</td>
<td>2016-3-7</td>
</tr>
<tr>
<td>1112</td>
<td>2016-1-5</td>
<td>2016-1-19</td>
</tr>
<tr>
<td>1113</td>
<td>2015-10-18</td>
<td>2020-9-1</td>
</tr>
</tbody>
</table>
</div>
<p>This dataset contains the information for different subscription contracts and it is assumed that:</p>
<ul>
<li>if the contract is active then the customer will need to pay a monthly fee in advance. The first payment will be collected on the start date.</li>
<li>If the contract ends before the next payment date, which is exactly one month after the last payment date, the customer does not need to pay the next subscription. For instance, customer 1112 only needs to pay once.</li>
<li>monthly payment fee is $10</li>
</ul>
<p>In this situation, I need to calculate the monthly/quarterly/annual sales between 2015 and 2020. It is ideal to also show the breakdown of sales by different customer IDs so that subsequent machine learning tasks can be performed.</p>
",171,1,0,5,python;pandas;aggregate;calculated-columns;recurring-billing,2022-04-14 06:41:51,2022-04-14 06:41:51,2022-04-15 18:50:26,i have a dataset with millions of records just like below this dataset contains the information for different subscription contracts and it is assumed that  in this situation  i need to calculate the monthly quarterly annual sales between  and   it is ideal to also show the breakdown of sales by different customer ids so that subsequent machine learning tasks can be performed 
869,869,17628152,70279189,How to implement new methods into pipeline using H2O?,"<p>I'm new to machine learning and H2O tools, and I'd like to know if there is a high-level H2O interface that allows us to implement new methods into a pipeline.</p>
<p>I know we can build models thanks to Flow interface and export them as POJO/MOJO. But how can I, for example, decide to use kNN method as an imputation method for my data, when Flow only allows simple imputation like mean/mode ?</p>
",39,1,1,3,methods;h2o;imputation,2021-12-08 22:52:34,2021-12-08 22:52:34,2022-04-15 17:54:11,i m new to machine learning and ho tools  and i d like to know if there is a high level ho interface that allows us to implement new methods into a pipeline  i know we can build models thanks to flow interface and export them as pojo mojo  but how can i  for example  decide to use knn method as an imputation method for my data  when flow only allows simple imputation like mean mode  
870,870,18812600,71883615,Comparing different neural networks resulting in truth value in my dataframe is too ambiguous,"<p>I am new to coding and trying to use machine learning for my masters research project to help predict price, return and volatility in stock markets but when I try and compare the 4 different neural networks I keep getting the error that my dataframe truth value is too ambiguous, but I am not sure where this issue is coming from. I have tried a few different suggests from other peoples questions but none of them have resolved the issue I have been having.</p>
<p>I am trying to compare 4 different Neural Networks to determine which provides the best prediction for my dataset. But I keep getting the above error, the error is:</p>
<pre><code>ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_30680/3175689273.py in &lt;module&gt;
     11 parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]
     12 
---&gt; 13 RMSE_info = compare_ANN_methods(price_data, test_price_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)

~\AppData\Local\Temp/ipykernel_30680/2096097431.py in compare_ANN_methods(price_data, test_price_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)
      2 
      3     information_FNN_df = get_accuracies_FNN(price_data, test_price_data, parameters_FNN, scaler)
----&gt; 4     optimized_params_FNN = analyze_results(information_FNN_df, test_price_data, 'FNN').all()
      5 
      6     information_TLNN_df = get_accuracies_TLNN(price_data, test_price_data, parameters_TLNN, scaler)

~\AppData\Local\Temp/ipykernel_30680/2506428711.py in analyze_results(data_frame, test_rainfall_data, name, flag)
      1 def analyze_results(data_frame, test_rainfall_data, name, flag=False):
----&gt; 2     optimized_params = data_frame.iloc[data_frame.RMSE.argmin]
      3     future_steps = optimized_params.future_steps
      4     forecast_values = optimized_params[-1*int(future_steps):]
      5     y_true = test_rainfall_data.iloc[:int(future_steps)]

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
    928             axis = self.axis or 0
    929 
--&gt; 930             maybe_callable = com.apply_if_callable(key, self.obj)
    931             return self._getitem_axis(maybe_callable, axis=axis)
    932 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\common.py in apply_if_callable(maybe_callable, obj, **kwargs)
    356     &quot;&quot;&quot;
    357     if callable(maybe_callable):
--&gt; 358         return maybe_callable(obj, **kwargs)
    359 
    360     return maybe_callable

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\base.py in argmin(self, axis, skipna, *args, **kwargs)
    696     def argmin(self, axis=None, skipna=True, *args, **kwargs) -&gt; int:
    697         delegate = self._values
--&gt; 698         nv.validate_minmax_axis(axis)
    699         skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)
    700 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\compat\numpy\function.py in validate_minmax_axis(axis, ndim)
    406     if axis is None:
    407         return
--&gt; 408     if axis &gt;= ndim or (axis &lt; 0 and ndim + axis &lt; 0):
    409         raise ValueError(f&quot;`axis` must be fewer than the number of dimensions ({ndim})&quot;)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in __nonzero__(self)
   1535     @final
   1536     def __nonzero__(self):
-&gt; 1537         raise ValueError(
   1538             f&quot;The truth value of a {type(self).__name__} is ambiguous. &quot;
   1539             &quot;Use a.empty, a.bool(), a.item(), a.any() or a.all().&quot;

ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
<p>The code is:</p>
<pre><code># look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps
parameters_FNN = [[1,2,3,6,8,10,12], [3,4,5,6], [1], [500], [20], [future_steps]]

# time_lagged_points, hidden_nodes, output_nodes, epochs, batch_size, future_steps
parameters_TLNN = [[[1,2,3,11,12], [1,2,3,4,11,12], [1,2,3,11,12,13], [1,2,3,4,5,6,10,11,12]], [3,4,5,6], [1], [300], [20], [future_steps]]

# seasonal_period, hidden_nodes, epochs, batch_size, future_steps
parameters_SANN = [[12], [3,4,5,6,7,8,9,10], [500], [20], [future_steps]]

# look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps
parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]

RMSE_info = compare_ANN_methods(price_data, test_price_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps) 
</code></pre>
<p>Here is where the compare_ANN_methods is defined:</p>
<pre><code>def compare_ANN_methods(price_data, test_price_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps):
    
    information_FNN_df = get_accuracies_FNN(price_data, test_price_data, parameters_FNN, scaler)
    optimized_params_FNN = analyze_results(information_FNN_df, test_price_data, 'FNN')
    
    information_TLNN_df = get_accuracies_TLNN(price_data, test_price_data, parameters_TLNN, scaler)
    optimized_params_TLNN = analyze_results(information_TLNN_df, test_price_data, 'TLNN')
    
    information_SANN_df = get_accuracies_SANN(price_data, test_price_data, parameters_SANN, scaler)
    optimized_params_SANN = analyze_results(information_SANN_df, test_price_data, 'SANN')
    
    information_LSTM_df = get_accuracies_LSTM(price_data, test_price_data, parameters_LSTM, scaler)
    optimized_params_LSTM = analyze_results(information_LSTM_df, test_price_data, 'LSTM')
    
    list_of_methods = [optimized_params_FNN, optimized_params_TLNN, optimized_params_SANN, optimized_params_LSTM]
    information = [information_FNN_df, information_TLNN_df, information_SANN_df, information_LSTM_df]
    index, name, RMSE_info = best_of_all(list_of_methods)
    best_optimized_params = analyze_results(information[index], test_price_data, name, True)
    return RMSE_info
</code></pre>
<p>The dataframe looks like this:
<a href=""https://i.stack.imgur.com/avxJU.png"" rel=""nofollow noreferrer"">Dataframe</a></p>
<p>Thanks for any help</p>
",7,0,0,2,python;pandas,2022-04-15 17:15:52,2022-04-15 17:15:52,2022-04-15 17:15:52,i am new to coding and trying to use machine learning for my masters research project to help predict price  return and volatility in stock markets but when i try and compare the  different neural networks i keep getting the error that my dataframe truth value is too ambiguous  but i am not sure where this issue is coming from  i have tried a few different suggests from other peoples questions but none of them have resolved the issue i have been having  i am trying to compare  different neural networks to determine which provides the best prediction for my dataset  but i keep getting the above error  the error is  the code is  here is where the compare_ann_methods is defined  thanks for any help
871,871,16684517,71737023,Unable to send the logs to Splunk Enterprise local using log4j2,"<p>I'm using log4j2 and splunk within java to send logs into my Splunk Enterprise HEC (HTTP Event Collector) Splunk Enterprise is running in my local machine.</p>
<p>I'm doing all log4j2 configuration programmatically. (I know this is not the correct way to do this but I'm still doing this for learning purpose).</p>
<p>I tried to send the logs into Splunk Enterprise directly from postman with the same URL and token and it works fine, but when I tried to send the logs from java using log4j2 I don't get anything in splunk.</p>
<p>My code is =&gt;</p>
<pre><code>import org.apache.logging.log4j.Level;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.logging.log4j.core.config.Configurator;
import org.apache.logging.log4j.core.config.builder.api.ConfigurationBuilder;
import org.apache.logging.log4j.core.config.builder.api.ConfigurationBuilderFactory;
import org.apache.logging.log4j.core.config.builder.impl.BuiltConfiguration;
import org.apache.logging.log4j.core.layout.PatternLayout;
import com.splunk.logging.*;

public class Main {
private static final Logger log;

static {
  configureLog4J();
  log = LogManager.getLogger(Main.class);
}
public static void configureLog4J() {
      ConfigurationBuilder&lt;BuiltConfiguration&gt; builder =
              ConfigurationBuilderFactory.newConfigurationBuilder();

      // configure a splunk appender
      builder.add(
          builder.newAppender(&quot;splunkH&quot;, &quot;SplunkHttp&quot;)
              .add(
                  builder.newLayout(PatternLayout.class.getSimpleName())
                      .addAttribute(
                          &quot;pattern&quot;,
                          &quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;
                      )
              )
              .addAttribute(&quot;sourcetype&quot;, &quot;log4j2&quot;)
              .addAttribute(&quot;index&quot;, &quot;main&quot;)
              .addAttribute(&quot;url&quot;, &quot;http://localhost:8088/services/collector&quot;) //I tried this url in postman and its working fine there
              .addAttribute(&quot;token&quot;, &quot;xxx&quot;)
              .addAttribute(&quot;disableCertificateValidation&quot;, &quot;true&quot;)
              
              
      );

      // configure the root logger
      builder.add(
          builder.newRootLogger(Level.INFO)
              .add(builder.newAppenderRef(&quot;splunkH&quot;))
      );

      // apply the configuration
      Configurator.initialize(builder.build());

    }//end of configureLog4J

public static void main(String ar[]) {
    log.log(Level.INFO, &quot;Hello from log4j2&quot;);
    
    log.log(Level.ERROR, &quot;Error from log4j2&quot;);

}//end of main method
}//end of class
</code></pre>
<p>my POM file</p>
<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.splunk.logging&lt;/groupId&gt;
        &lt;artifactId&gt;splunk-library-javalogging&lt;/artifactId&gt;
        &lt;version&gt;1.11.4&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
        &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
        &lt;version&gt;2.11.2&lt;/version&gt;
    &lt;/dependency&gt;


    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
        &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;
        &lt;version&gt;2.11.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.splunk&lt;/groupId&gt;
        &lt;artifactId&gt;splunk&lt;/artifactId&gt;
        &lt;version&gt;1.6.5.0&lt;/version&gt;
    &lt;/dependency&gt;

&lt;/dependencies&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;splunk-artifactory&lt;/id&gt;
        &lt;name&gt;Splunk Releases&lt;/name&gt;
        &lt;url&gt;https://splunk.jfrog.io/splunk/ext-releases-local&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>
<p>I cannot see any logs in splunk. Did I miss something ?</p>
",163,1,0,4,java;log4j;log4j2;splunk,2022-04-04 17:39:21,2022-04-04 17:39:21,2022-04-15 15:34:48,i m using logj and splunk within java to send logs into my splunk enterprise hec  http event collector  splunk enterprise is running in my local machine  i m doing all logj configuration programmatically   i know this is not the correct way to do this but i m still doing this for learning purpose   i tried to send the logs into splunk enterprise directly from postman with the same url and token and it works fine  but when i tried to send the logs from java using logj i don t get anything in splunk  my code is   gt  my pom file i cannot see any logs in splunk  did i miss something  
872,872,18810526,71881515,Adding weights to K-means clustering algorithm,"<p>I found an implementation of a K-means clustering algorithm, built from scratch. It is easy to understand and each step is well documented.</p>
<p>You have to specify the number of clusters k and the maximum iteration as input to run the algorithm.</p>
<pre><code>def kmeans(X, k, maxiter, seed = None):

n_row, n_col = X.shape

# randomly choose k data points as initial centroids
if seed is not None:
    np.random.seed(seed)

rand_indices = np.random.choice(n_row, size = k)
centroids = X[rand_indices]

for itr in range(maxiter):
    # compute distances between each data point and the set of centroids
    # and assign each data point to the closest centroid
    distances_to_centroids = pairwise_distances(X, centroids, metric = 'euclidean')
    cluster_assignment = np.argmin(distances_to_centroids, axis = 1)

    # select all data points that belong to cluster i and compute
    # the mean of these data points (each feature individually)
    # this will be our new cluster centroids
    new_centroids = np.array([X[cluster_assignment == i].mean(axis = 0) for i in range(k)])        
    # if the updated centroid is still the same,
    # then the algorithm converged
    if np.all(centroids == new_centroids):
        break
    
    centroids = new_centroids

return centroids, cluster_assignment
</code></pre>
<p>I run the algorithm by running:</p>
<p><code> k = 4 centers, label = kmeans(X, k, maxiter = 100)</code></p>
<p>I now want to add weights to finding the new cluster centers. Therefore, I would like to add weights to this step:</p>
<p><code>new_centroids = np.array([X[cluster_assignment == i].mean(axis = 0) for i in range(k)])</code></p>
<p>I have a dataset with three columns, 2 that would function for the x and y coordinates for the clustering and a third that would act as weight for each datapoint.</p>
<p>However, adding weights to .mean(axis = 0), as .mean(axis = 0, weights = X[:,2]) is not possible. Error:</p>
<p><em>_mean() got an unexpected keyword argument 'weights'</em></p>
<p>This is due to the fact that this argument is not known in the functions library (if I'm correct).</p>
<p>It is possible to add the argument weights to np.average: (<a href=""https://numpy.org/doc/stable/reference/generated/numpy.average.html"" rel=""nofollow noreferrer"">https://numpy.org/doc/stable/reference/generated/numpy.average.html</a>)</p>
<p>However, simply changing the code to:</p>
<blockquote>
<p>new_centroids = np.array([X[cluster_assignment == i].average(axis =
0,weights=X[:,1]) for i in range(k)])</p>
</blockquote>
<p>Does not work. This gives the error:
&quot;'numpy.ndarray' object has no attribute 'average'&quot;</p>
<p>This is due to the fact that numpy.ndarray has a mean, max, std method, but does not have a average method (<a href=""https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html"" rel=""nofollow noreferrer"">https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html</a>)</p>
<p>I do not know how to rewrite the code now. Would be really great if anyone could help me out with this!</p>
<p>Happy easters everyone.</p>
<p>If anyone wants the full code as the example provides:
<a href=""http://ethen8181.github.io/machine-learning/clustering/kmeans.html"" rel=""nofollow noreferrer"">http://ethen8181.github.io/machine-learning/clustering/kmeans.html</a></p>
",71,0,1,2,python;k-means,2022-04-15 13:32:24,2022-04-15 13:32:24,2022-04-15 15:12:58,i found an implementation of a k means clustering algorithm  built from scratch  it is easy to understand and each step is well documented  you have to specify the number of clusters k and the maximum iteration as input to run the algorithm  i run the algorithm by running   k    centers  label   kmeans x  k  maxiter     i now want to add weights to finding the new cluster centers  therefore  i would like to add weights to this step  new_centroids   np array  x cluster_assignment    i  mean axis     for i in range k    i have a dataset with three columns   that would function for the x and y coordinates for the clustering and a third that would act as weight for each datapoint  however  adding weights to  mean axis      as  mean axis     weights   x      is not possible  error  _mean   got an unexpected keyword argument  weights  this is due to the fact that this argument is not known in the functions library  if i m correct   it is possible to add the argument weights to np average     however  simply changing the code to  this is due to the fact that numpy ndarray has a mean  max  std method  but does not have a average method    i do not know how to rewrite the code now  would be really great if anyone could help me out with this  happy easters everyone 
873,873,18793341,71879305,what is traditional additive models and the differences between these models and machine learning models?,"<p>I read a SCI(PMID: 32437368) in the paper it said &quot;Current stroke risk assessment tools presume the impact of risk factors is linear and cumulative. However, both novel risk factors and their interplay influencing stroke incidence are difficult to reveal using traditional additive models.&quot;I want to know what is traditional additive models?The difference between these models and machine learning models？Thank you very much for your answer!</p>
",14,1,0,2,machine-learning;model,2022-04-15 07:32:44,2022-04-15 07:32:44,2022-04-15 07:45:44,i read a sci pmid    in the paper it said  current stroke risk assessment tools presume the impact of risk factors is linear and cumulative  however  both novel risk factors and their interplay influencing stroke incidence are difficult to reveal using traditional additive models  i want to know what is traditional additive models the difference between these models and machine learning models thank you very much for your answer 
874,874,15012258,69053789,[Python][SQL Machine Learning Services] Cannot install XGBoost on my SQL instance,"<p>I am trying to find a solution to my problem. We are trying to install XGBoost package on our MS SQL 2018 and we are facing this problem:</p>
<p><a href=""https://i.stack.imgur.com/OiBCP.png"" rel=""nofollow noreferrer"">Error Description</a></p>
<p>The error (Błąd ogólny sieci. Zajrzyj do dokumentacji sieci.) is EN is 'General network error. See your network documentation.'. We managed to install other packages, there's only problem with this one. Any ideas what we can try to do different?</p>
<p>Installation is by default code:</p>
<pre><code>import sqlmlutils
connection = sqlmlutils.ConnectionInfo(server=&quot;name,port&quot;, database=&quot;dbname&quot;, uid=&quot;accname&quot;, pwd=&quot;accpwd&quot;)
sqlmlutils.SQLPackageManager(connection).install(&quot;XGBoost&quot;)
</code></pre>
",91,1,0,5,python;sql;machine-learning;xgboost;microsoft-machine-learning-server,2021-09-04 14:21:43,2021-09-04 14:21:43,2022-04-14 23:23:56,i am trying to find a solution to my problem  we are trying to install xgboost package on our ms sql  and we are facing this problem   the error  błąd ogólny sieci  zajrzyj do dokumentacji sieci   is en is  general network error  see your network documentation    we managed to install other packages  there s only problem with this one  any ideas what we can try to do different  installation is by default code 
875,875,18804738,71874103,Problems with Python tensorflow,"<p>I am a noob in programming who tried to study machine learning. I used tensorflow for Python. Here's the code, written (but <strong>not 100% copied</strong>) with official tensorflow guide (here's it <a href=""https://www.tensorflow.org/guide/basics"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/basics</a>). I can't see the final graph with the results after training. I've tried two methods of training and both share the same problem. Could anyone help me?</p>
<pre><code>import matplotlib as mp
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as pl

mp.rcParams[&quot;figure.figsize&quot;] = [20, 10]
precision = 500
x = tf.linspace(-10.0, 10.0, precision)


def y(x): return 4 * np.sin(x - 1) + 3

newY = y(x) + tf.random.normal(shape=[precision])

class Model(tf.keras.Model):
    def __init__(self, units):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(units = units, activation = tf.nn.relu, kernel_initializer=tf.random.normal, bias_initializer=tf.random.normal)
        self.dense2 = tf.keras.layers.Dense(1)
    
    def __call__(self, x, training = True):
        x = x[:, tf.newaxis]
        x = self.dense1(x)
        x = self.dense2(x)
        return tf.squeeze(x, axis=1)

model = Model(164)

pl.plot(x, y(x), label = &quot;origin&quot;)
pl.plot(x, newY, &quot;.&quot;, label = &quot;corrupted&quot;)
pl.plot(x, model(x), label = &quot;before training&quot;)

&quot;&quot;&quot;                                                     The first method
vars = model.variables
optimizer = tf.optimizers.SGD(learning_rate = 0.01)

for i in range(1000):
    with tf.GradientTape() as tape:
        prediction = model(x)
        error = (newY-prediction)**2
        mean_error = tf.reduce_mean(error)
    gradient = tape.gradient(mean_error, vars)
    optimizer.apply_gradients(zip(gradient, vars))
&quot;&quot;&quot;

model.compile(loss = tf.keras.losses.MSE, optimizer = tf.optimizers.SGD(learning_rate = 0.01))
model.fit(x, newY, epochs=100,batch_size=32,verbose=0)

pl.plot(x, model(x), label = &quot;after training&quot;)
pl.legend()
pl.show()
</code></pre>
",55,2,0,5,python;numpy;tensorflow;matplotlib;machine-learning,2022-04-14 21:00:26,2022-04-14 21:00:26,2022-04-14 22:31:28,i am a noob in programming who tried to study machine learning  i used tensorflow for python  here s the code  written  but not   copied  with official tensorflow guide  here s it    i can t see the final graph with the results after training  i ve tried two methods of training and both share the same problem  could anyone help me 
876,876,7111876,56225510,"Codesandbox gives &quot;Target container is not a DOM element&quot; error for React app, but webpack-dev-server does not","<p>I am attempting to take my git repo with a practice React app and put it into codesandbox.io so that I can show it to others I am working with more easily. I followed the instructions at got my sandbox up here: <a href=""https://codesandbox.io/s/github/cdpautsch/react-learning-area/tree/master/test-app2"" rel=""nofollow noreferrer"">https://codesandbox.io/s/github/cdpautsch/react-learning-area/tree/master/test-app2</a></p>

<p>However, I get the error: <code>Target container is not a DOM element</code></p>

<pre><code>evaluate
/src/index.js:52:9
  49 |     }
  50 | }
  51 | 
&gt; 52 | ReactDOM.render((
     |         ^
  53 |     &lt;Provider store={store}&gt;
  54 |         &lt;BrowserRouter&gt;
  55 |             &lt;div&gt;
</code></pre>

<p>This error does NOT come up on my machine when I am running with webpack-dev-server.</p>

<p>My code from <code>index.js</code>:</p>

<pre class=""lang-js prettyprint-override""><code>ReactDOM.render((
    &lt;Provider store={store}&gt;
        &lt;BrowserRouter&gt;
            &lt;div&gt;
                &lt;Navbar /&gt;

                &lt;Route exact path = ""/"" component = {Home} /&gt;
                &lt;Route path = ""/cards"" component = {CardsGame} /&gt;
            &lt;/div&gt;
        &lt;/BrowserRouter&gt;
    &lt;/Provider&gt;
), document.getElementById('app'));
</code></pre>

<p>My code from <code>index.html</code>:</p>

<pre class=""lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
    &lt;head&gt;
        &lt;meta charset=""UTF-8"" /&gt;
        &lt;title&gt;TestApp2&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div id=""app""&gt;&lt;/div&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>It says it's not a valid element, but it's definitely there and definitely has the right name. What am I missing?</p>

<p>Updating with additional information:
* The app was not created with <code>create-react-app</code>, but initialized and loaded with packages manually
* Sandbox seems to default to CRA, and this may affect how it runs?</p>
",1040,3,2,2,reactjs;codesandbox,2019-05-20 22:48:54,2019-05-20 22:48:54,2022-04-14 19:37:42,i am attempting to take my git repo with a practice react app and put it into codesandbox io so that i can show it to others i am working with more easily  i followed the instructions at got my sandbox up here   however  i get the error  target container is not a dom element this error does not come up on my machine when i am running with webpack dev server  my code from index js  my code from index html  it says it s not a valid element  but it s definitely there and definitely has the right name  what am i missing 
877,877,4195846,71863292,aws sagemaker cannot import TensorFlowModel,"<p><code>from sagemaker.tensorflow import TensorFlowModel</code></p>
<p>throws</p>
<p><code>ImportError: cannot import name 'is_pipeline_variable' from 'sagemaker.workflow'</code></p>
<p>full error stack is:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-54-e8eac6a9c02f&gt; in &lt;module&gt;
----&gt; 1 from sagemaker.tensorflow import TensorFlowModel

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/__init__.py in &lt;module&gt;
     14 from __future__ import absolute_import
     15 
---&gt; 16 from sagemaker.tensorflow.estimator import TensorFlow  # noqa: F401 (imported but unused)
     17 from sagemaker.tensorflow.model import TensorFlowModel, TensorFlowPredictor  # noqa: F401
     18 from sagemaker.tensorflow.processing import TensorFlowProcessor  # noqa: F401

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/estimator.py in &lt;module&gt;
     23 import sagemaker.fw_utils as fw
     24 from sagemaker.tensorflow import defaults
---&gt; 25 from sagemaker.tensorflow.model import TensorFlowModel
     26 from sagemaker.transformer import Transformer
     27 from sagemaker.vpc_utils import VPC_CONFIG_DEFAULT

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/model.py in &lt;module&gt;
     22 from sagemaker.predictor import Predictor
     23 from sagemaker.serializers import JSONSerializer
---&gt; 24 from sagemaker.workflow import is_pipeline_variable
     25 
     26 

ImportError: cannot import name 'is_pipeline_variable' from 'sagemaker.workflow' (/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/__init__.py)
</code></pre>
<p>following amazon's own documentation here:
<a href=""https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/</a></p>
<p>trying to accommodate the v2 api change documented here:
<a href=""https://sagemaker.readthedocs.io/en/stable/v2.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/v2.html</a></p>
<pre><code>TensorFlow Serving Model

sagemaker.tensorflow.serving.Model has been renamed to sagemaker.tensorflow.model.TensorFlowModel. (For the previous implementation of that class, see Remove Legacy TensorFlow).
</code></pre>
",123,0,0,3,amazon-web-services;tensorflow;amazon-sagemaker,2022-04-14 01:35:53,2022-04-14 01:35:53,2022-04-14 19:05:21,from sagemaker tensorflow import tensorflowmodel throws importerror  cannot import name  is_pipeline_variable  from  sagemaker workflow  full error stack is 
878,878,8947036,48360972,ValueError: special directives must be the first entry,"<p>why this error appears and what does it mean exactly?</p>
<p>It appears on this code (I put only the part of machine learning, because the code is so long):</p>
<pre><code>import numpy as np
from sklearn import neighbors
n_neighbors = 3

if (automatic == 'true'):
    # import some data to play with
    home = Homes.query.filter_by(device_id = request.args.get('?device_id')).first()

    htng_orders = Heating_orders.query.filter_by(home_id = home.id).all()

    X_h = [[ho.timeInMinutes, ho.season, ho.ext_temp] for ho in htng_orders]
    y_h = [ho.instruction for ho in htng_orders] 

    clf_h = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
    clf_h.fit(X_h, y_h)

    new_time = datetime.datetime.now().time()
    new_timeInMinutes = (new_time.hour*60 + new_time.minute)
    new_season = get_season(date.today())
    new_ext_temp = getExtTemperature(home.city)
    new_data_h = np.c_[new_timeInMinutes, new_season, new_ext_temp]
    preddiction_h = clf_h.predict(new_data_h)
</code></pre>
<p>The error is the following:</p>
<pre><code>[...]

File &quot;C:\[...]\FlaskREST\app.py&quot;, line 525, in get
    new_data_h = np.c_[new_timeInMinutes, new_season, new_ext_temp]
File &quot;C:\Python\Python36-32\lib\site-packages\numpy\lib\index_tricks.py&quot;, line 289, in __getitem__
    raise ValueError(&quot;special directives must be the &quot;
ValueError: special directives must be the first entry.
</code></pre>
<p>Thank you in advance!</p>
",1591,2,0,4,python;numpy;machine-learning;scikit-learn,2018-01-21 02:20:01,2018-01-21 02:20:01,2022-04-14 03:49:30,why this error appears and what does it mean exactly  it appears on this code  i put only the part of machine learning  because the code is so long   the error is the following  thank you in advance 
879,879,6105459,71859083,Run script inside Docker container using Azure Machine Learning,"<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning"" rel=""nofollow noreferrer"">Azure Machine Learning</a> provides provides encapsulation of the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments"" rel=""nofollow noreferrer"">environment</a> for your code to run. As far as I know you can specify custom Docker images and Dockerfiles to create an environment.</p>
<p>But in my specific use case, I want to run the script inside a specific Docker container. Does Azure ML provide some ways to run a script inside a custom Docker container?</p>
",50,1,0,5,azure;docker;azure-machine-learning-service;azureml;azureml-python-sdk,2022-04-13 19:48:31,2022-04-13 19:48:31,2022-04-14 00:20:53, provides provides encapsulation of the  for your code to run  as far as i know you can specify custom docker images and dockerfiles to create an environment  but in my specific use case  i want to run the script inside a specific docker container  does azure ml provide some ways to run a script inside a custom docker container 
880,880,18608433,71652903,TorchText Vocab TypeError: Vocab.__init__() got an unexpected keyword argument &#39;min_freq&#39;,"<p>I am working on a CNN Sentiment analysis machine learning model which uses the IMDb dataset provided by the Torchtext library.
On one of my lines of code</p>
<p><code>vocab = Vocab(counter, min_freq = 1, specials=('\&lt;unk\&gt;', '\&lt;BOS\&gt;', '\&lt;EOS\&gt;', '\&lt;PAD\&gt;'))</code></p>
<p>I am getting a TypeError for the min_freq argument even though I am certain that it is one of the accepted arguments for the function. I am also getting UserWarning Lambda function is not supported for pickle, please use regular python function or functools partial instead. Full code</p>
<pre><code>from torchtext.datasets import IMDB
from collections import Counter
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import Vocab
tokenizer = get_tokenizer('basic_english')  
train_iter = IMDB(split='train')
test_iter = IMDB(split='test')
counter = Counter()
for (label, line) in train_iter:
    counter.update(tokenizer(line))
vocab = Vocab(counter, min_freq = 1, specials=('\&lt;unk\&gt;', '\&lt;BOS\&gt;', '\&lt;EOS\&gt;', '\&lt;PAD\&gt;'))
</code></pre>
<p>Source Links
<a href=""https://towardsdatascience.com/cnn-sentiment-analysis-9b1771e7cdd6"" rel=""nofollow noreferrer"">towardsdatascience</a>
<a href=""https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb"" rel=""nofollow noreferrer"">github Legacy to new</a></p>
<p>I have tried removing the min_freq argument and use the functions default as follows</p>
<p><code>vocab = Vocab(counter, specials=('\&lt;unk\&gt;', '\&lt;BOS\&gt;', '\&lt;EOS\&gt;', '\&lt;PAD\&gt;'))</code></p>
<p>however I end up getting the same type error but for the specials argument rather than min_freq.</p>
<p>Any help will be much appreciated</p>
<p>Thank you.</p>
",675,2,3,5,python;conv-neural-network;tokenize;imdb;torchtext,2022-03-29 01:11:14,2022-03-29 01:11:14,2022-04-13 18:30:30,vocab   vocab counter  min_freq     specials     lt unk  gt       lt bos  gt       lt eos  gt       lt pad  gt     i am getting a typeerror for the min_freq argument even though i am certain that it is one of the accepted arguments for the function  i am also getting userwarning lambda function is not supported for pickle  please use regular python function or functools partial instead  full code i have tried removing the min_freq argument and use the functions default as follows vocab   vocab counter  specials     lt unk  gt       lt bos  gt       lt eos  gt       lt pad  gt     however i end up getting the same type error but for the specials argument rather than min_freq  any help will be much appreciated thank you 
881,881,18518405,71845651,Is there a way to classify `/` divide operator image using machine learning,"<p>I am working on a project for solving basic maths handwritten questions with only machine learning models such as SVM, KNN or RFC.</p>
<p>SVM classifier gives the best accuracy on my dataset so I used it for the project. But I am unable to make it work for the '/' operator as the model confuses it with '7', '1', or 'X'.
The other 2 are even performing even worse.</p>
<p>What can I do to overcome this?</p>
<p>Is this doable with any machine learning model or is deep learning/Neural networks the only way for me?</p>
",26,0,1,4,python;machine-learning;computer-vision;ocr,2022-04-12 21:17:19,2022-04-12 21:17:19,2022-04-13 17:44:55,i am working on a project for solving basic maths handwritten questions with only machine learning models such as svm  knn or rfc  what can i do to overcome this  is this doable with any machine learning model or is deep learning neural networks the only way for me 
882,882,10724050,55496289,How to fix &quot;AttributeError: module &#39;tensorflow&#39; has no attribute &#39;get_default_graph&#39;&quot;?,"<p>I am trying to run some code to create an LSTM model but i get an error:</p>

<p><code>AttributeError: module 'tensorflow' has no attribute 'get_default_graph'</code></p>

<p>My code is as follows:</p>

<pre><code>from keras.models import Sequential

model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))
model.add(LSTM(17))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>I have found someone else with a similar problem and they updated tensorflow and it works; but mine is up to date and still does not work. I am new to using keras and machine learning so I apologise if this is something silly!</p>
",105818,19,54,5,python;tensorflow;keras;keras-layer;tf.keras,2019-04-03 18:53:46,2019-04-03 18:53:46,2022-04-13 15:51:54,i am trying to run some code to create an lstm model but i get an error  attributeerror  module  tensorflow  has no attribute  get_default_graph  my code is as follows  i have found someone else with a similar problem and they updated tensorflow and it works  but mine is up to date and still does not work  i am new to using keras and machine learning so i apologise if this is something silly 
883,883,18719507,71854171,"How can I use large hexadecimal values as training data? In machine learning,","<p>I am thinking of doing machine learning using <code>sklearn</code>. But the training data I have is a large hexadecimal value. How do I process this into training data? The code below is an example of a hexadecimal value</p>
<p><code>import sklearn</code> <code>hex_train='0x504F1728378126389BACDDDDDFF12873788912893788265722F75706C6F61642F7068702F75706C6F61642E7068703F547970653D4D6564696120485454502FAABBD10D0A436F6E74656E742D547970653A206D756C7469706172742F666F726D2D646174613B20436861727365743D5554462D383B20626F756E646172793D5430504861636B5465616D5F5745424675636B0AD0557365722D4167656E743A205765624675636B205430504861636B5465616D207777772E7430702E78797A200D0A526566657265723A206874747012334BDBFABFBDBF123FBDFBE74656E742D4C656E6774683A203234370D0A4163636570743A202A2F2A0D0A486F73743A206F6365616E2E6B697374691636B5465616D5F5745424675636B0D0A436F6E74656E742D446973706F736974696F6E3A20666F726D2D646174613B206E616D653D224E657746696C65223B2066696C656E616D653D224C75616E2E747874220D0A436F6E74656E742D547970653A20696D6165672F6A7065670D0A0D0A3C3F7068700D0A40707265675F7265706C61636528222F5B706167656572726F725D2F65222C245F504F53545B27446176F756E6427293B0D0A3F3E0D0A2D2D5430504861636B5465616D5F5745424675636B2D2D0D0A'</code></p>
<p>All the training data I have are these values. I don't know how to preprocess these values and use them as training data. What I know is that should I convert these values into <code>float</code> types?</p>
",38,1,0,3,python;machine-learning;sklearn-pandas,2022-04-13 13:50:01,2022-04-13 13:50:01,2022-04-13 14:01:44,i am thinking of doing machine learning using sklearn  but the training data i have is a large hexadecimal value  how do i process this into training data  the code below is an example of a hexadecimal value import sklearn hex_train  xfbacdddddfffcfffcfefddfaabbddafeedadcffddbddbfedbdfbaddeabbdeeadaabdbfabfbdbffbdfbeedceadaaafadafafeebbdfbdafeedffeafddbeddecbceddceedafeedadfadadacfdafcfbfdfcffbfebdafedaddbdfbddda  all the training data i have are these values  i don t know how to preprocess these values and use them as training data  what i know is that should i convert these values into float types 
884,884,13440007,71031598,Random Search: Constructing set of unit directions,"<p>I am currently looking into an example implementation of <a href=""https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/2_Zero_order_methods/2_5_Random.ipynb"" rel=""nofollow noreferrer"">Random Search</a> from <em>Machine Learning Refined</em> and have trouble understanding the following code snippet where <em>w</em> is an input vector and <em>num_samples</em> is number of random directions to sample:</p>
<pre><code># construct set of random unit directions
directions = np.random.randn(num_samples,np.size(w))
norms = np.sqrt(np.sum(directions*directions,axis = 1))[:,np.newaxis]
directions = directions/norms   
</code></pre>
<p>What I do understand is that we sample random vectors in the size of <em>w</em>, which we then need to normalize in order to get unit directions. So we calculate <code>|V| = sqrt(x*x + y*y + z*z)</code> for all directions or <code>V/|V| = (x/|V|, y/|V|, z/|V|)</code> respectively.</p>
<p>Could someone explain to me in simple terms what happens in line 3, specifically why we sum over axis=1 and what the <code>[:,np.newaxis]</code> does? I have a vague grasp on what this is supposed to do, but some intuition would help a lot.</p>
",29,1,0,4,python;numpy;machine-learning;optimization,2022-02-08 15:09:32,2022-02-08 15:09:32,2022-04-13 13:00:47,i am currently looking into an example implementation of  from machine learning refined and have trouble understanding the following code snippet where w is an input vector and num_samples is number of random directions to sample  what i do understand is that we sample random vectors in the size of w  which we then need to normalize in order to get unit directions  so we calculate  v    sqrt x x   y y   z z  for all directions or v  v     x  v   y  v   z  v   respectively  could someone explain to me in simple terms what happens in line   specifically why we sum over axis  and what the    np newaxis  does  i have a vague grasp on what this is supposed to do  but some intuition would help a lot 
885,885,18784307,71845425,FastAPI array of JSON in Request Body for Machine Learning prediction,"<p>I’m working with FastAPI for Model inference in Machine Learning, so I need to have as inputs an array of <code>JSON</code> like this:</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;feature1&quot;:&quot;value&quot;,
    &quot;feature2&quot;:&quot;value&quot;,
    &quot;feature3&quot;:&quot;value&quot;
  },
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;feature1&quot;:&quot;value&quot;,
    &quot;feature2&quot;:&quot;value&quot;,
    &quot;feature3&quot;:&quot;value&quot;
  },
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;feature1&quot;:&quot;value&quot;,
    &quot;feature2&quot;:&quot;value&quot;,
    &quot;feature3&quot;:&quot;value&quot;
  }
]
</code></pre>
<p>The output (result of prediction) should look like this :</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;prediction&quot;:&quot;value&quot;
  },
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;prediction&quot;:&quot;value&quot;
  },
  {
    &quot;Id&quot;:&quot;value&quot;,
    &quot;prediction&quot;:&quot;value&quot;
  }
]
</code></pre>
<p>How to implement this with FastAPI in Python?</p>
",170,1,0,5,python;api;machine-learning;fastapi;inference,2022-04-12 21:02:11,2022-04-12 21:02:11,2022-04-13 12:10:21,i m working with fastapi for model inference in machine learning  so i need to have as inputs an array of json like this  the output  result of prediction  should look like this   how to implement this with fastapi in python 
886,886,11842835,71822482,Connect to on prem SQL server through Azure Databricks notebook using Python PYODBC,"<p>Currently I connect to my on premises SQL servers using Windows authentication. I would like to do the same using Databricks so that I can load the data extracted using the query into a dataframe  and do some machine learning experiments. Is it possible to connect to  sql server (on-prem) using pyodbc or any other driver in  notebooks?</p>
<p>Currently I am trying this code to connect to server database using databricks notebooks and I run into a connection timeout error</p>
<pre><code>cxn=pyodbc.connect(&quot;Driver ={SQL Server Native Client 11.0};&quot;&quot;Server=server name;&quot;
&quot;Port=1433;&quot;Database=database_name;&quot;Trusted_Connection=yes;&quot;)

</code></pre>
<p>If its a login issue is the solution to connect my databricks cluster to my network? If not it would be great if somebody  can point me in right direction.</p>
",226,0,1,2,sql-server;azure-databricks,2022-04-11 09:07:01,2022-04-11 09:07:01,2022-04-13 11:42:50,currently i connect to my on premises sql servers using windows authentication  i would like to do the same using databricks so that i can load the data extracted using the query into a dataframe  and do some machine learning experiments  is it possible to connect to  sql server  on prem  using pyodbc or any other driver in  notebooks  currently i am trying this code to connect to server database using databricks notebooks and i run into a connection timeout error if its a login issue is the solution to connect my databricks cluster to my network  if not it would be great if somebody  can point me in right direction 
887,887,16197519,71846855,What is the difference between these two layers : CONV and MBConv?,"<p>I am working on a machine learning project to learn more about this field. The project is about image classification. I want to use the EffnetB0 architecure and they mention in this architecure they use in the fisrt stage the following layer: &quot;Conv3X3&quot; and the following layers they use &quot;MBConv1&quot;.
I tried to understand the difference between these two layers but I can't seem to find the answer. These two layers are both convolutional layers right ?</p>
<p>But what exactly is the difference between &quot;Conv&quot; and &quot;MBConv&quot;?</p>
<p>Thank you for helping me!</p>
",78,1,0,4,machine-learning;deep-learning;conv-neural-network;image-classification,2022-04-12 22:56:12,2022-04-12 22:56:12,2022-04-13 07:09:42,but what exactly is the difference between  conv  and  mbconv   thank you for helping me 
888,888,11083136,71849177,Serving images in real time with request API in local environment,"<p>I have a machine learning pipeline I want to serve images captured from local client's webcam through some sort of a request-response API. Note that both server and client will be running on the same machine.</p>
<p>First thing that came to my mind was REST API, so I tried to encode and decode frames in base64 but the latency was not very good.</p>
<pre class=""lang-py prettyprint-override""><code>def encode_image(image):
    string = cv2.imencode('.jpg', image)[1]
    string_encoded = base64.b64encode(string).decode()
    return string_encoded

def decode_image(encoded):
    string_decoded = base64.b64decode(encoded)
    numpy_jpg = np.frombuffer(string_decoded, dtype=np.uint8)
    image = cv2.imdecode(numpy_jpg, flags=1)
    return image
</code></pre>
<p>What I'd like to do is to take advantage of the fact that the data does not have to be copied, since it is already present in memory. I'd imagine I could send the frame's memory address to the server and read it on the server side.</p>
<p>Is this possible in python? I'm open to any cython magic if necessary. For now I'm trying to make it work in pure python but later on I'd like to make requests from C++, meaning C++ program reads frames from webcam and serves them to python driven server.</p>
",20,0,0,3,python;rest;request,2022-04-13 02:42:19,2022-04-13 02:42:19,2022-04-13 02:42:19,i have a machine learning pipeline i want to serve images captured from local client s webcam through some sort of a request response api  note that both server and client will be running on the same machine  first thing that came to my mind was rest api  so i tried to encode and decode frames in base but the latency was not very good  what i d like to do is to take advantage of the fact that the data does not have to be copied  since it is already present in memory  i d imagine i could send the frame s memory address to the server and read it on the server side  is this possible in python  i m open to any cython magic if necessary  for now i m trying to make it work in pure python but later on i d like to make requests from c    meaning c   program reads frames from webcam and serves them to python driven server 
889,889,4652515,54614960,Optimal way to store/index/manage large amounts of image training data for machine learning?,"<p>I understand that you generally do not want to store images in a database. What you instead do is store metadata about the images (owner, creation date, size, file format, etc) and a link to the image (S3 location or path to the image on the local filesystem). If you need to recover the image you can then look up the path in the database and read it in from object storage or the local filesystem.</p>

<p>This use case seems to be designed for cases where the system needs to just read a few images per request, for example, to get a few images that belong to the user's web page. </p>

<p>My situation is a little bit different. I'm aggregating a large number of labeled images for training data for various machine learning algorithms. For each image, there will be a row in a table that contains information on where the image came from, its size, and the labels associated with that image (i.e. one image might have the labels: [""car"", ""vehicle"", ""sedan"", ""honda"", ""civic"", ""blue"", ""2002""], another might only have [""vehicle"", ""truck""], another might have [""human"", ""pedestrian"", ""woman""]). </p>

<p>My goal is to have the data structured in such a way that I can make training sets of data from this table arbitrarily as I see fit according to different label groupings. So I could say 'gather all images with the label ""animal"", and group based on the label ""dog"", ""cat"", ""horse""' (should one of those labels exist). Now, from my flat list of training data, I'll have images grouped into three categories that I can train a CNN classifier from.</p>

<p>The trouble comes from the fact that I can have millions of images, so if I run the above query to get all images with the label ""animal"", I will need to run the SQL query to find all the images with that label, then I will need to do millions of RPC calls to S3 or the local filesystem to actually get the image data I need. If I actually keep the images stored in the database, the images will come right out of the query itself.</p>

<p>So, as a general question, what is the best way to store and index a large number of images and their metadata for machine learning? On the one hand, we can simply group a large number of images into ZIP files and store the zip files in some object store. This is convenient because I only need a single handle and RPC call to get all of the training data onto whatever server I'm performing the ML training sequence on, but this causes me to lose any granular visibility into my training data. On the other hand, I can store all of my data indexed in some large SQL table, image data included. This gives me maximum visibility into my data, but is cost prohibitive and makes it inconvenient to actually get the images onto a server that needs the image data to perform a training sequence.</p>
",436,1,1,4,database;image;machine-learning;training-data,2019-02-10 14:43:00,2019-02-10 14:43:00,2022-04-13 02:19:34,i understand that you generally do not want to store images in a database  what you instead do is store metadata about the images  owner  creation date  size  file format  etc  and a link to the image  s location or path to the image on the local filesystem   if you need to recover the image you can then look up the path in the database and read it in from object storage or the local filesystem  this use case seems to be designed for cases where the system needs to just read a few images per request  for example  to get a few images that belong to the user s web page   my situation is a little bit different  i m aggregating a large number of labeled images for training data for various machine learning algorithms  for each image  there will be a row in a table that contains information on where the image came from  its size  and the labels associated with that image  i e  one image might have the labels   car  vehicle  sedan  honda  civic  blue     another might only have  vehicle  truck   another might have  human  pedestrian  woman     my goal is to have the data structured in such a way that i can make training sets of data from this table arbitrarily as i see fit according to different label groupings  so i could say  gather all images with the label animal  and group based on the label dog  cat  horse   should one of those labels exist   now  from my flat list of training data  i ll have images grouped into three categories that i can train a cnn classifier from  the trouble comes from the fact that i can have millions of images  so if i run the above query to get all images with the label animal  i will need to run the sql query to find all the images with that label  then i will need to do millions of rpc calls to s or the local filesystem to actually get the image data i need  if i actually keep the images stored in the database  the images will come right out of the query itself  so  as a general question  what is the best way to store and index a large number of images and their metadata for machine learning  on the one hand  we can simply group a large number of images into zip files and store the zip files in some object store  this is convenient because i only need a single handle and rpc call to get all of the training data onto whatever server i m performing the ml training sequence on  but this causes me to lose any granular visibility into my training data  on the other hand  i can store all of my data indexed in some large sql table  image data included  this gives me maximum visibility into my data  but is cost prohibitive and makes it inconvenient to actually get the images onto a server that needs the image data to perform a training sequence 
890,890,18784792,71846154,Opposite coefficient &quot;sign&quot; for two logistic regressions,"<p>I am trying to build an xG-model using Distance (from goal) as feature and the target variable is a dummy-variable indicating whether the shot resulted in a goal or not. So I am trying to make a simple logistic regression. I tried to replicate a model where the fitting was done with the statsmodels-package, which resulted in a positive coefficient of 0.16 and an intercept of -0.5.</p>
<p>When I fitted the line using scikit-learn the coefficient was -0.16. The same happened with the intercept, which was around 0.5. So somehow the coefficients have &quot;flipped&quot;.</p>
<p>Dataset example:</p>
<pre><code>
Goal    X   Y   C       Distance    Angle
1       12  41  9.0     13.891814   0.474451
0       15  52  2.0     15.803560   0.453823
0       19  33  17.0    22.805811   0.280597
0       25  30  20.0    29.292704   0.223680
0       10  39  11.0    12.703248   0.479051
</code></pre>
<p>scikit-learn code:</p>
<pre><code>feature_cols = ['Distance']

X = shots_model[feature_cols] # Features
y = shots_model['Goal'] # Target
y = y.astype('category')
m1 = LogisticRegression()
m1.fit(X_train, y_train)
</code></pre>
<p>statsmodels code:</p>
<pre><code>test_model = smf.glm(formula=&quot;Goal ~ &quot; + model, data=shots_model, 
                           family=sm.families.Binomial()).fit()
print(test_model.summary())        
b=test_model.params
</code></pre>
<p>I am probably missing something simple, as I am pretty new to Machine Learning, and this has been puzzling me for some time now. Please help.</p>
",50,2,0,4,python;scikit-learn;regression;statsmodels,2022-04-12 21:54:57,2022-04-12 21:54:57,2022-04-13 00:01:33,i am trying to build an xg model using distance  from goal  as feature and the target variable is a dummy variable indicating whether the shot resulted in a goal or not  so i am trying to make a simple logistic regression  i tried to replicate a model where the fitting was done with the statsmodels package  which resulted in a positive coefficient of   and an intercept of     when i fitted the line using scikit learn the coefficient was     the same happened with the intercept  which was around    so somehow the coefficients have  flipped   dataset example  scikit learn code  statsmodels code  i am probably missing something simple  as i am pretty new to machine learning  and this has been puzzling me for some time now  please help 
891,891,13243719,71845302,How to extract a fixed set of frames from a live video stream for machine learning prediction in PyTorch?,"<p>I recently created a <a href=""https://github.com/SwinTransformer/Video-Swin-Transformer"" rel=""nofollow noreferrer"">Video Swin Transformer</a> model that takes in a ([batch_size], 3, 32, 224, 224) [batch_size, channel, temporal_dim, height, width] tensor for video and outputs logits. The goal is to have the model predict on a live stream from a camera. Is there any way to capture the fixed sequence of 32 frames repetitively and have the model predict on a live stream. If prediction time is longer than 32 frames, can I stretch out the frames over a longer time period like a minute? Thanks.</p>
",57,1,0,5,machine-learning;video;ffmpeg;pytorch;computer-vision,2022-04-12 20:53:45,2022-04-12 20:53:45,2022-04-12 22:53:05,i recently created a  model that takes in a   batch_size            batch_size  channel  temporal_dim  height  width  tensor for video and outputs logits  the goal is to have the model predict on a live stream from a camera  is there any way to capture the fixed sequence of  frames repetitively and have the model predict on a live stream  if prediction time is longer than  frames  can i stretch out the frames over a longer time period like a minute  thanks 
892,892,18337640,71846371,How can I split train and test data based on some conditions?,"<p>How can I split train and test data based on some conditions for the machine learning models? The test data should include the same spatial areas (x-y) for each year. Namely, I don't want the same spatial area to be in the training and test set. For example:</p>
<pre><code>import pandas as pd
data = {'x': [ 80.1, 90.1, 0, 300.1, 80.1, 90.1, 0, 300.1, 80.1, 90.1, 0, 300.1], 'y': [ 140.1, 150.1, 160.1, 400.1, 140.1, 150.1, 160.1, 400.1, 140.1, 150.1, 160.1, 400.1], 'a': [1, 2, 3, 4, 5, 10, 11, 12, 13, 14, 15, 16], 'c': [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0], 'year': [2000, 2000, 2000, 2000, 2001, 2001, 2001, 2001, 2002, 2002, 2002, 2002]}   
df = pd.DataFrame(data)
df
            
             x        y     a    c      year
        
        0   80.1    140.1   1   0.0     2000
        1   90.1    150.1   2   0.0     2000
        2   0.0     160.1   3   0.0     2000
        3   300.1   400.1   4   0.0     2000
        4   80.1    140.1   5   0.0     2001
        5   90.1    150.1   10  0.0     2001
        6   0.0     160.1   11  1.0     2001
        7   300.1   400.1   12  0.0     2001
        8   80.1    140.1   13  1.0     2002
        9   90.1    150.1   14  1.0     2002
        10  0.0     160.1   15  0.0     2002
        11  300.1   400.1   16  0.0     2002

    Expected train dataset:          
                  x       y     a      c     year   
            
            0   80.1    140.1   1     0.0    2000  
            1   90.1    150.1   2     0.0    2000   
             
            3   300.1   400.1   4     0.0    2000  
            4   80.1    140.1   5     0.0    2001  
            5   90.1    150.1   10    0.0    2001  
             
            7   300.1   400.1   12    0.0    2001  
            8   80.1    140.1   13    1.0    2002  
            9   90.1    150.1   14    1.0    2002   
            
            11  300.1   400.1   16    0.0    2002   
    
    Expected test dataset:           
                  x       y     a      c     year   
                           
            2   0.0     160.1   3     0.0    2000 
            
            6   0.0     160.1   11    1.0    2001  
             
            10  0.0     160.1   15    0.0    2002  
              
</code></pre>
",116,2,1,4,python;pandas;machine-learning;training-data,2022-04-12 22:12:33,2022-04-12 22:12:33,2022-04-12 22:45:40,how can i split train and test data based on some conditions for the machine learning models  the test data should include the same spatial areas  x y  for each year  namely  i don t want the same spatial area to be in the training and test set  for example 
893,893,17536470,71540319,DataSpell Inverse matrix,"<p>What would be the code to write to make a simple inverse matrix in DataSpell? I am very new to coding and am trying to figure out the basics to then get started with more advanced material. I am currently looking through Andrew Ng's course on machine learning.</p>
",11,0,0,2,matrix;dataspell,2022-03-19 22:43:29,2022-03-19 22:43:29,2022-04-12 22:20:03,what would be the code to write to make a simple inverse matrix in dataspell  i am very new to coding and am trying to figure out the basics to then get started with more advanced material  i am currently looking through andrew ng s course on machine learning 
894,894,12379947,71843641,Using tensorflow Conv1D: how can I solve error &quot;Input 0 of layer &quot;conv1d_9&quot; is incompatible with the layer: &quot;?,"<p>I am using TensorFlow to conduct binary classification of ultrasonic signals that I have simulated and I want to use CNN. I am new to programming and machine learning so I don't know if I am using the correct terms, please bear with me.
The data is organised into an array called 'sig_data' where the columns are the timesteps and the rows are different samples of signals. The values are the amplitudes of the signals. The labels are in another 1D array called 'sig_id' containing values of 1 and 0. The data is shaped as follows:</p>
<pre><code>data shape: (1000, 1000)
label shape: 1000
</code></pre>
<p>I've put the data into a TF dataset and separated into train, validation and test sets:</p>
<pre><code>data_ds = tf.data.Dataset.from_tensors((sig_data, sig_id))

train_ds = data_ds.take(700)
val_ds = data_ds.skip(700).take(200)
test_ds = data_ds.skip(900).take(100)

train_ds = train_ds.shuffle(shuffle_buffer_size).batch(batch)
val_ds = val_ds.shuffle(shuffle_buffer_size).batch(batch)
test_ds = test_ds.batch(batch)
</code></pre>
<p>The model I created is:</p>
<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(1000,1)),
    tf.keras.layers.Conv1D(50, 3, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1, activation='sigmoid')
    ])

model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
  metrics=['accuracy'])

history = model.fit(
  train_ds,
  validation_data=val_ds,
  batch_size=batch,
  epochs=25)
</code></pre>
<p>And I get the following error:</p>
<pre><code>ValueError: Exception encountered when calling layer &quot;sequential_3&quot; (type Sequential).
    
    Input 0 of layer &quot;conv1d_3&quot; is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 1000, 1000)
</code></pre>
<p>I have looked this up to try and solve it. I think that the problem is with the input shape so I have tried to reshape my arrays as follows:</p>
<pre><code>sig_data_reshaped = np.expand_dims(sig_data, axis=-1)
sig_id_reshaped = np.expand_dims(sig_id, axis=-1)

reshaped data shape: (1000, 1000, 1)
reshaped label shape: (1000, 1)
</code></pre>
<p>But when I run my code I still get an error,</p>
<pre><code>Input 0 of layer &quot;conv1d_8&quot; is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 1000, 1000)
</code></pre>
<p>Is my error due to how I organised my dataset? Why is it that when I reshape the arrays into 3D, it still gives me an error?</p>
",57,1,0,5,python;tensorflow;machine-learning;keras;conv-neural-network,2022-04-12 19:02:00,2022-04-12 19:02:00,2022-04-12 20:43:40,i ve put the data into a tf dataset and separated into train  validation and test sets  the model i created is  and i get the following error  i have looked this up to try and solve it  i think that the problem is with the input shape so i have tried to reshape my arrays as follows  but when i run my code i still get an error  is my error due to how i organised my dataset  why is it that when i reshape the arrays into d  it still gives me an error 
895,895,14372590,71842115,Alternatives to NaN or null in pandas,"<p>I am working on a data set where there are marks of students and I want to predict the next term mark of the student. This is a sample view of how my data looks like</p>
<p><a href=""https://i.stack.imgur.com/YgVjR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YgVjR.png"" alt=""enter image description here"" /></a></p>
<p>There are some elective subjects students can choose from in addition to the compulsory subjects. Then the students who do not register for that elective subject, do not get a mark for that subject. Is there any way I can represent that.</p>
<p>The student can obtain a mark from 0 to 100. The students have not received negative marks as per the data set.</p>
<p>I tried using  0 marks for such students but I realized those attempts are wrong.I thought of putting 0 and a column adjacent to the considered subject, which records a boolean value stating has the student registered for that subject or not. Yet there are many subjects so putting such adjacent columns would be time consuming and again putting 0 to the marks of a student not even registered to that particular subject is wrong cause model can understand that 0 as the student wrote the exam and obtained 0 marks.</p>
<p>When I tried putting NaN and then train the machine learning models, I got the error:</p>
<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float64')
</code></pre>
<p>This was the code I tried applying random forest to the data set</p>
<pre><code>rf = RandomForestClassifier()
scores = cross_val_score(rf, X, y, cv=10, scoring='accuracy')
print('Model accuracy score : {0:0.4f}'.format(scores.mean()))
</code></pre>
",51,0,0,4,python;pandas;null;nan,2022-04-12 17:17:46,2022-04-12 17:17:46,2022-04-12 20:17:28,i am working on a data set where there are marks of students and i want to predict the next term mark of the student  this is a sample view of how my data looks like  there are some elective subjects students can choose from in addition to the compulsory subjects  then the students who do not register for that elective subject  do not get a mark for that subject  is there any way i can represent that  the student can obtain a mark from  to   the students have not received negative marks as per the data set  i tried using   marks for such students but i realized those attempts are wrong i thought of putting  and a column adjacent to the considered subject  which records a boolean value stating has the student registered for that subject or not  yet there are many subjects so putting such adjacent columns would be time consuming and again putting  to the marks of a student not even registered to that particular subject is wrong cause model can understand that  as the student wrote the exam and obtained  marks  when i tried putting nan and then train the machine learning models  i got the error  this was the code i tried applying random forest to the data set
896,896,18776657,71835115,Why is my Android app crashing every time I run a python script with chaquopy?,"<p>I am building an Android app that will allow the user to get a picture either by taking it in real time or uploading it from their saved images. Then, it will go through a machine learning script in python to determine their location. Before I completely connect to the algorithm, I am trying a test program that just returns a double.</p>
<pre><code>from os.path import dirname, join
import csv
import random
filename = join(dirname(__file__), &quot;new.csv&quot;)

def testlat():
    return 30.0

def testlong():
    return 30.0
</code></pre>
<p>These returned values are used in a Kotlin file that will then send those values to the Google Maps activity on the app for the location to be plotted.</p>
<pre><code>class MainActivity : AppCompatActivity() {
    var lat = 0.0
    var long = 0.0
    var dynamic = false
    private val cameraRequest = 1888
    lateinit var imageView: ImageView
    lateinit var button: Button
    private val pickImage = 100
    private var imageUri: Uri? = null
    var active = false

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        // Accesses the info image button
        val clickMe = findViewById&lt;ImageButton&gt;(R.id.imageButton)

        // Runs this function when the info icon is pressed by the user
        // It will display the text in the variable infoText
        clickMe.setOnClickListener {
            Toast.makeText(this, infoText, Toast.LENGTH_LONG).show()
        }

        if (ContextCompat.checkSelfPermission(applicationContext, Manifest.permission.CAMERA)
            == PackageManager.PERMISSION_DENIED) {
            ActivityCompat.requestPermissions(
                this,
                arrayOf(Manifest.permission.CAMERA),
                cameraRequest
            )
        }
        imageView = findViewById(R.id.imageView)
        val photoButton: Button = findViewById(R.id.button2)
        photoButton.setOnClickListener {
            val cameraIntent = Intent(MediaStore.ACTION_IMAGE_CAPTURE)
            startActivityForResult(cameraIntent, cameraRequest)
            dynamic = true
        }

        /*
        The below will move to external photo storage once button2 is clicked
         */
        button = findViewById(R.id.button)
        button.setOnClickListener {
            val gallery = Intent(Intent.ACTION_PICK, MediaStore.Images.Media.INTERNAL_CONTENT_URI)
            startActivityForResult(gallery, pickImage)
        }

        // PYTHON HERE
        if (! Python.isStarted()) {
            Python.start(AndroidPlatform(this))
        }
    }

    override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?) {
        super.onActivityResult(requestCode, resultCode, data)
        if (resultCode == RESULT_OK &amp;&amp; requestCode == pickImage) {
            imageUri = data?.data
            imageView.setImageURI(imageUri)

            // PYTHON HERE
            val py = Python.getInstance()
            val pyobj = py.getModule(&quot;main&quot;)
            this.lat = pyobj.callAttr(&quot;testlat&quot;).toDouble()
            this.long = pyobj.callAttr(&quot;testlong&quot;).toDouble()

            /* Open the map after image has been received from user
             This will be changed later to instead call the external object recognition/pathfinding
             scripts and then pull up the map after those finish running
             */

            val mapsIntent = Intent(this, MapsActivity::class.java)
            startActivity(mapsIntent)
        }
    }
}
</code></pre>
<p>I set up chaquopy and the gradle is building successfully, but everytime I get to the python part of emulating the app, it crashes. I'm not quite sure why that is; I thought maybe the program was too much for the phone to handle but it is a very basic python script so I doubt that's the issue.</p>
",74,1,0,5,python;android;android-studio;chaquopy;google-maps-android-api-3,2022-04-12 04:14:46,2022-04-12 04:14:46,2022-04-12 19:28:39,i am building an android app that will allow the user to get a picture either by taking it in real time or uploading it from their saved images  then  it will go through a machine learning script in python to determine their location  before i completely connect to the algorithm  i am trying a test program that just returns a double  these returned values are used in a kotlin file that will then send those values to the google maps activity on the app for the location to be plotted  i set up chaquopy and the gradle is building successfully  but everytime i get to the python part of emulating the app  it crashes  i m not quite sure why that is  i thought maybe the program was too much for the phone to handle but it is a very basic python script so i doubt that s the issue 
897,897,11148674,71843635,How to predict price for a future day given historical data,"<p>so currently this is the code I have.  Not attached are various graphs that I have made that show the actual stock price from the CSV and then my projections.  I'm wanting to make it where I simply predict tomorrow's stock price given all of this historical data but I'm having a difficult time. The &quot;df.loc[len(df.index)] = ['2022-04-05',0,0,0,0,0,0]&quot; was where I was trying to put the predictions for future days although I am open to other ways.</p>
<pre><code># Machine learning
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# For data manipulation
import pandas as pd
import numpy as np

# To plot
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')

# To ignore warnings
import warnings
warnings.filterwarnings(&quot;ignore&quot;)





# method of pandas


df = pd.read_csv('data_files/MSFT.csv')


#add extra row of blank data for future prediction

df.loc[len(df.index)] = ['2022-04-05',0,0,0,0,0,0]
df.loc[len(df.index)] = ['2022-04-06',0,0,0,0,0,0]
df.loc[len(df.index)] = ['2022-04-07',0,0,0,0,0,0]
df.loc[len(df.index)] = ['2022-04-08',0,0,0,0,0,0]

#  Changes The Date column as index columns
df.index = pd.to_datetime(df['Date'])


# drop The original date column
df = df.drop(['Date'], axis='columns')
print(df)

# Create predictor variables
df['Open-Close'] = df.Open - df.Close
df['High-Low'] = df.High - df.Low

# Store all predictor variables in a variable X
X = df[['Open-Close', 'High-Low']]
X.head()

# Target variables
y = np.where(df['Close'].shift(-1) &gt; df['Close'], 1, 0)
print(y)

split_percentage = 0.8
split = int(split_percentage*len(df))

# Train data set
X_train = X[:split]
y_train = y[:split]

# Test data set
X_test = X[split:]
y_test = y[split:]

# Support vector classifier
cls = SVC().fit(X_train, y_train)


df['Predicted_Signal'] = cls.predict(X)


# Calculate daily returns
df['Return'] = df.Close.pct_change()


# Calculate strategy returns
df['Strategy_Return'] = df.Return * df.Predicted_Signal.shift(1)


# Calculate Cumulutive returns
df['Cum_Ret'] = df['Return'].cumsum()


# Plot Strategy Cumulative returns
df['Cum_Strategy'] = df['Strategy_Return'].cumsum()
</code></pre>
",68,0,0,4,python;pandas;dataframe;numpy,2022-04-12 19:01:40,2022-04-12 19:01:40,2022-04-12 19:01:40,so currently this is the code i have   not attached are various graphs that i have made that show the actual stock price from the csv and then my projections   i m wanting to make it where i simply predict tomorrow s stock price given all of this historical data but i m having a difficult time  the  df loc len df index                   was where i was trying to put the predictions for future days although i am open to other ways 
898,898,17385106,71843275,How to send audio segments to the API until user presses STOP,"<p>I am developing in React Native. I need to classify sound segments in a Machine Learning API. The Model in this API accepts fragments of 5 seconds.</p>
<p>I would like to know what is the best way of implementing this in react native.</p>
<p>So far, I am recording the sound, using setTimeOut to define the 5 seconds of recording (defined 5 seconds). And the part of recording and sending until the user presses stop, I am handling with setInterval (defined 7 seconds).</p>
<p>The problem is, setInterval requires a time to repeat again, I put 7 seconds, and sometimes, it starts a new cycle when I am still recording the sound.So I am thowrn an error saying that I cant prepare two audio records at the same time.</p>
<p>How to control this?</p>
<p>Is there another way? I tryied using WHILE cycles, but without success.</p>
<p>So, the dynamics are :</p>
<ol>
<li><p>user starts recording.</p>
</li>
<li><p>the 5 seconds sound is recorded and sent to the API.</p>
</li>
<li><p>again the point 2) until user presses STOP.</p>
</li>
<li><p>user presses STOP.</p>
</li>
</ol>
",17,0,0,5,javascript;react-native;audio;settimeout;setinterval,2022-04-12 18:34:22,2022-04-12 18:34:22,2022-04-12 18:34:22,i am developing in react native  i need to classify sound segments in a machine learning api  the model in this api accepts fragments of  seconds  i would like to know what is the best way of implementing this in react native  so far  i am recording the sound  using settimeout to define the  seconds of recording  defined  seconds   and the part of recording and sending until the user presses stop  i am handling with setinterval  defined  seconds   the problem is  setinterval requires a time to repeat again  i put  seconds  and sometimes  it starts a new cycle when i am still recording the sound so i am thowrn an error saying that i cant prepare two audio records at the same time  how to control this  is there another way  i tryied using while cycles  but without success  so  the dynamics are   user starts recording  the  seconds sound is recorded and sent to the api  again the point   until user presses stop  user presses stop 
899,899,18531462,71842322,Applying Multithreading to Neural Network Training,"<p>I'm working on a logistic regression machine learning project and utilized the code from <a href=""https://builtin.com/data-science/guide-logistic-regression-tensorflow-20"" rel=""nofollow noreferrer"">https://builtin.com/data-science/guide-logistic-regression-tensorflow-20</a> as a starting point.</p>
<p>My goal is to divide the data into batches for training and train them all at the same time.</p>
<p>This is the solution I came up with.
Is it doing its job properly?</p>
<pre><code>from __future__ import absolute_import, division, print_function
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
import threading
import time
from concurrent.futures import ThreadPoolExecutor

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)
x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])
x_train, x_test = x_train / 255., x_test / 255.
num_classes = 10 # 0 to 9 digits
num_features = 784 # 28*28
# Training parameters.
learning_rate = 0.01
training_steps = 1000
batch_size = 256 
#The batch size defines the number of samples that will be propagated through the network.
train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))
train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)
W = tf.Variable(tf.ones([num_features, num_classes]), name=&quot;weight&quot;)
b = tf.Variable(tf.zeros([num_classes]), name=&quot;bias&quot;)
def logistic_regression(x):

    # Apply softmax to normalize the logits to a probability distribution.

    return tf.nn.softmax(tf.matmul(x, W) + b)

def cross_entropy(y_pred, y_true):

    # Encode label to a one hot vector.

    y_true = tf.one_hot(y_true, depth=num_classes)

    # Clip prediction values to avoid log(0) error.

    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)

    # Compute cross-entropy.

    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))

def accuracy(y_pred, y_true):

    # Predicted class is the index of the highest score in prediction vector (i.e. argmax).

    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))

    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
optimizer = tf.optimizers.SGD(learning_rate)

def run_optimization1(x, y):
# Wrap computation inside a GradientTape for automatic differentiation.
    with tf.GradientTape() as g:
        pred = logistic_regression(x)
        loss = cross_entropy(pred, y)
        acc = accuracy(pred,y)
    # Compute gradients.
    gradients = g.gradient(loss, [W, b])
    return gradients

for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):
    x1, x2 = tf.split(batch_x, num_or_size_splits=2)
    y1, y2 = tf.split(batch_y, num_or_size_splits=2)
    with ThreadPoolExecutor(max_workers=2) as executor:
        f1=executor.submit(run_optimization1, x1,y1)
        f2=executor.submit(run_optimization1,x2,y2)
    gradients = f1.result()+f2.result()
    optimizer.apply_gradients(zip(gradients, [W, b]))
    if step % 50 == 0:
        

        pred3 = logistic_regression(batch_x)

        loss3 = cross_entropy(pred3, batch_y)

        acc3 = accuracy(pred3, batch_y)
       
        print(&quot;step: %i, loss: %f, accuracy: %f&quot; % (step, loss3, acc3))
</code></pre>
",35,0,1,5,python;multithreading;numpy;tensorflow;logistic-regression,2022-04-12 17:31:17,2022-04-12 17:31:17,2022-04-12 17:31:17,i m working on a logistic regression machine learning project and utilized the code from  as a starting point  my goal is to divide the data into batches for training and train them all at the same time 
