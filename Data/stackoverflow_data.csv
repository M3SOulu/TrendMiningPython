,Unnamed: 0,Unnamed: 0.1,AuthorId,Q_id,Title,Abstract,Views,Answers,Cites,Tags_n,Tags,Date,CR_Date,LA_Date,Abstract_clean,Title_clean,Abstrat_without_stopwords,Title_without_stopwords,Merged_title_and_abs,Tokenized_data,Stem_data
0,0,0,3879858,43049545,Python: Check if dataframe column contain string type,"<p>I want check if columns in a dataframe consists of strings so I can label them with numbers for machine learning purposes. Some columns consists of numbers, I dont want to change them. Columns example can be seen below:</p>

<pre><code>TRAIN FEATURES
  Age              Level  
  32.0              Silver      
  61.0              Silver  
  66.0              Silver      
  36.0              Gold      
  20.0              Silver     
  29.0              Silver     
  46.0              Silver  
  27.0              Silver      
</code></pre>

<p>Thank you=)</p>
",53186,8,25,2,python;dataframe,2017-03-27 20:14:56,2017-03-27 20:14:56,2022-07-15 10:37:24,i want check if columns in a dataframe consists of strings so i can label them with numbers for machine learning purposes  some columns consists of numbers  i dont want to change them  columns example can be seen below  thank you  ,python  check if dataframe column contain string type,want check columns dataframe consists strings label numbers machine learning purposes columns consists numbers dont want change columns example seen thank,python check dataframe column contain string type,python check dataframe column contain string typewant check columns dataframe consists strings label numbers machine learning purposes columns consists numbers dont want change columns example seen thank,"['python', 'check', 'dataframe', 'column', 'contain', 'string', 'typewant', 'check', 'columns', 'dataframe', 'consists', 'strings', 'label', 'numbers', 'machine', 'learning', 'purposes', 'columns', 'consists', 'numbers', 'dont', 'want', 'change', 'columns', 'example', 'seen', 'thank']","['python', 'check', 'datafram', 'column', 'contain', 'string', 'typew', 'check', 'column', 'datafram', 'consist', 'string', 'label', 'number', 'machin', 'learn', 'purpos', 'column', 'consist', 'number', 'dont', 'want', 'chang', 'column', 'exampl', 'seen', 'thank']"
1,1,1,19523154,72932792,what is difference between computer vision and NLP? in detail,"<p><strong>what is difference between computer vision and NLP? in detail</strong></p>
<p>Deep learning neural networks have recently have shown very powerful improvements in tasks in computer vision and NLP compared to some other machine learning methods that have been popular for longer.</p>
<p>Deep learning neural networks have recently have shown very powerful improvements in tasks in computer vision and NLP compared to some other machine learning methods that have been popular for longer.</p>
",27,0,-3,4,python;nlp;computer-vision;artificial-intelligence,2022-07-11 06:11:46,2022-07-11 06:11:46,2022-07-15 10:21:17,what is difference between computer vision and nlp  in detail deep learning neural networks have recently have shown very powerful improvements in tasks in computer vision and nlp compared to some other machine learning methods that have been popular for longer  deep learning neural networks have recently have shown very powerful improvements in tasks in computer vision and nlp compared to some other machine learning methods that have been popular for longer ,what is difference between computer vision and nlp  in detail,difference computer vision nlp detail deep learning neural networks recently shown powerful improvements tasks computer vision nlp compared machine learning methods popular longer deep learning neural networks recently shown powerful improvements tasks computer vision nlp compared machine learning methods popular longer,difference computer vision nlp detail,difference computer vision nlp detaildifference computer vision nlp detail deep learning neural networks recently shown powerful improvements tasks computer vision nlp compared machine learning methods popular longer deep learning neural networks recently shown powerful improvements tasks computer vision nlp compared machine learning methods popular longer,"['difference', 'computer', 'vision', 'nlp', 'detaildifference', 'computer', 'vision', 'nlp', 'detail', 'deep', 'learning', 'neural', 'networks', 'recently', 'shown', 'powerful', 'improvements', 'tasks', 'computer', 'vision', 'nlp', 'compared', 'machine', 'learning', 'methods', 'popular', 'longer', 'deep', 'learning', 'neural', 'networks', 'recently', 'shown', 'powerful', 'improvements', 'tasks', 'computer', 'vision', 'nlp', 'compared', 'machine', 'learning', 'methods', 'popular', 'longer']","['differ', 'comput', 'vision', 'nlp', 'detaildiffer', 'comput', 'vision', 'nlp', 'detail', 'deep', 'learn', 'neural', 'network', 'recent', 'shown', 'power', 'improv', 'task', 'comput', 'vision', 'nlp', 'compar', 'machin', 'learn', 'method', 'popular', 'longer', 'deep', 'learn', 'neural', 'network', 'recent', 'shown', 'power', 'improv', 'task', 'comput', 'vision', 'nlp', 'compar', 'machin', 'learn', 'method', 'popular', 'longer']"
2,2,2,3098629,72982696,How to train data of different lengths in machine learning?,"<p>I am analyzing the text of some literary works and I want to look at the distance between certain words in the text. Specifically, I am looking for parallelism.</p>
<p>Since I can’t know the specific number of tokens in a text I can’t simply put all words in the text in the training data because it would not be uniform across all training data.</p>
<p>For example, the text:</p>
<p>“I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.&quot;</p>
<p>Is not the same text length as</p>
<p>&quot;My fellow Americans, ask not what your country can do for you, ask what you can do for your country.&quot;</p>
<p>So therefore I could not columns out of each word and then assign the distance in a row because the lengths would be different.</p>
<p>How could I go about representing this in training data?  I was under the assumption that training data had to be the same type and length.</p>
",19,1,0,2,data-science;training-data,2022-07-14 20:33:22,2022-07-14 20:33:22,2022-07-15 06:05:54,i am analyzing the text of some literary works and i want to look at the distance between certain words in the text  specifically  i am looking for parallelism  since i can t know the specific number of tokens in a text i can t simply put all words in the text in the training data because it would not be uniform across all training data  for example  the text   i have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character  i have a dream today   is not the same text length as  my fellow americans  ask not what your country can do for you  ask what you can do for your country   so therefore i could not columns out of each word and then assign the distance in a row because the lengths would be different  how could i go about representing this in training data   i was under the assumption that training data had to be the same type and length ,how to train data of different lengths in machine learning ,analyzing text literary works want look distance certain text specifically looking parallelism since know specific number tokens text simply put text training data would uniform across training data example text dream four little children one day live nation judged color skin content character dream today text length fellow americans ask country ask country therefore could columns word assign distance row lengths would different could go representing training data assumption training data type length,train data different lengths machine learning,train data different lengths machine learninganalyzing text literary works want look distance certain text specifically looking parallelism since know specific number tokens text simply put text training data would uniform across training data example text dream four little children one day live nation judged color skin content character dream today text length fellow americans ask country ask country therefore could columns word assign distance row lengths would different could go representing training data assumption training data type length,"['train', 'data', 'different', 'lengths', 'machine', 'learninganalyzing', 'text', 'literary', 'works', 'want', 'look', 'distance', 'certain', 'text', 'specifically', 'looking', 'parallelism', 'since', 'know', 'specific', 'number', 'tokens', 'text', 'simply', 'put', 'text', 'training', 'data', 'would', 'uniform', 'across', 'training', 'data', 'example', 'text', 'dream', 'four', 'little', 'children', 'one', 'day', 'live', 'nation', 'judged', 'color', 'skin', 'content', 'character', 'dream', 'today', 'text', 'length', 'fellow', 'americans', 'ask', 'country', 'ask', 'country', 'therefore', 'could', 'columns', 'word', 'assign', 'distance', 'row', 'lengths', 'would', 'different', 'could', 'go', 'representing', 'training', 'data', 'assumption', 'training', 'data', 'type', 'length']","['train', 'data', 'differ', 'length', 'machin', 'learninganalyz', 'text', 'literari', 'work', 'want', 'look', 'distanc', 'certain', 'text', 'specif', 'look', 'parallel', 'sinc', 'know', 'specif', 'number', 'token', 'text', 'simpli', 'put', 'text', 'train', 'data', 'would', 'uniform', 'across', 'train', 'data', 'exampl', 'text', 'dream', 'four', 'littl', 'children', 'one', 'day', 'live', 'nation', 'judg', 'color', 'skin', 'content', 'charact', 'dream', 'today', 'text', 'length', 'fellow', 'american', 'ask', 'countri', 'ask', 'countri', 'therefor', 'could', 'column', 'word', 'assign', 'distanc', 'row', 'length', 'would', 'differ', 'could', 'go', 'repres', 'train', 'data', 'assumpt', 'train', 'data', 'type', 'length']"
3,3,3,15637303,72987647,Warning: File_get_contents(Http://127.0.0.1:5000/): Failed To Open Stream: HTTP Request Failed! HTTP/1.0 405 METHOD NOT ALLOWED,"<p>I m trying to call a python flask api in a php website so I used the following code and got the warning mentioned in the title as a result</p>
<pre><code>function bot_msg($user_msg){
    $url = 'http://127.0.0.1:5000/';

    $data = array('msg' =&gt; $user_msg);
    $options = array(
        'http' =&gt; array(
            'header'  =&gt; &quot;Content-type: application/x-www-form-urlencoded\r\n&quot;,
            'method'  =&gt; 'POST',
            'content' =&gt; http_build_query($data)
        )
    );
      $context  = stream_context_create($options);
      $result = file_get_contents($url, false, $context);  $result = json_decode($result, true);
    return $result; }
</code></pre>
<p>python code</p>
<pre><code>@app.route(&quot;/&quot;)
def get_bot_response():
    userText = request.args.get('msg')
    #chatbot_resp=chatbot_response(userText)
    return chatbot_response(userText)
</code></pre>
<p>in chatbot_response I used a machine learning to predict a response</p>
",18,0,-3,2,python;flask,2022-07-15 05:02:25,2022-07-15 05:02:25,2022-07-15 05:15:00,i m trying to call a python flask api in a php website so i used the following code and got the warning mentioned in the title as a result python code in chatbot_response i used a machine learning to predict a response,warning  file_get_contents http           failed to open stream  http request failed  http    method not allowed,trying call python flask api php website used following code got warning mentioned title result python code chatbot_response used machine learning predict response,warning file_get_contents http failed open stream http request failed http method allowed,warning file_get_contents http failed open stream http request failed http method allowedtrying call python flask api php website used following code got warning mentioned title result python code chatbot_response used machine learning predict response,"['warning', 'file_get_contents', 'http', 'failed', 'open', 'stream', 'http', 'request', 'failed', 'http', 'method', 'allowedtrying', 'call', 'python', 'flask', 'api', 'php', 'website', 'used', 'following', 'code', 'got', 'warning', 'mentioned', 'title', 'result', 'python', 'code', 'chatbot_response', 'used', 'machine', 'learning', 'predict', 'response']","['warn', 'file_get_cont', 'http', 'fail', 'open', 'stream', 'http', 'request', 'fail', 'http', 'method', 'allowedtri', 'call', 'python', 'flask', 'api', 'php', 'websit', 'use', 'follow', 'code', 'got', 'warn', 'mention', 'titl', 'result', 'python', 'code', 'chatbot_respons', 'use', 'machin', 'learn', 'predict', 'respons']"
4,4,4,19549708,72981396,I am getting many errors including tcl error in my GUI code in python,"<p>I am working on creating a GUI for digit recognizer. I had completed all the machine learning part successfully however i am getting many errors in my GUI code for it. Here is my code for GUI of digit recognizer:</p>
<pre><code>from keras.models import load_model
from tkinter import *
import tkinter  as tk
import win32gui
from PIL import ImageGrab, Image
import numpy as np
model = load_model('mnist.h5')
def predict_digit(img):
    #resize image to 28x28 pixels
    img = img.resize((28,28))
    #convert rgb to grayscale
    img = img.convert('L')
    img = np.array(img)
    #reshaping for model normalization
    img = img.reshape(1,28,28,1)
    img = img/255.0
    #predicting the class
    res = model.predict([img])[0]
    return np.argmax(res), max(res)
class App(tk.Tk):
    def __init__(self):
        tk.Tk.__init__(self)
        self.x = self.y = 0
        # Creating elements
        self.canvas = tk.Canvas(self, width=200, height=200, bg = &quot;black&quot;, cursor=&quot;cross&quot;)
        self.label = tk.Label(self, text=&quot;Analyzing..&quot;, font=(&quot;Helvetica&quot;, 48))
        self.classify_btn = tk.Button(self, text = &quot;Searched&quot;, command = self.classify_handwriting) 
        self.button_clear = tk.Button(self, text = &quot;Dlt&quot;, command = self.clear_all)
        # Grid structure
        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )
        self.label.grid(row=0, column=1,pady=2, padx=2)
        self.classify_btn.grid(row=1, column=1, pady=2, padx=2)
        self.button_clear.grid(row=1, column=0, pady=2)
        #self.canvas.bind(&quot;&quot;, self.start_pos)
        self.canvas.bind(&quot;&quot;, self.draw_lines)
    def clear_all(self):
        self.canvas.delete(&quot;all&quot;)
    def classify_handwriting(self):
        Hd = self.canvas.winfo_id() # to fetch the handle of the canvas
        rect = win32gui.GetWindowRect(Hd) # to fetch the edges of the canvas
        im = ImageGrab.grab(rect)
        digit, acc = predict_digit(im)
        self.label.configure(text= str(digit)+', '+ str(int(acc*100))+'%')
    def draw_lines(slf, event):
        slf.x = event.x
        slf.y = event.y
        r=8
        slf.canvas.create_oval(slf.x-r, slf.y-r, slf.x + r, slf.y + r, fill='black')
app = App()
mainloop()
</code></pre>
<p>list of errors in my code:</p>
<p><a href=""https://i.stack.imgur.com/o8FuM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8FuM.png"" alt=""enter image description here"" /></a></p>
<p>Please provide me the right code.</p>
",24,0,-2,3,python;machine-learning;tkinter,2022-07-14 19:02:14,2022-07-14 19:02:14,2022-07-15 04:44:04,i am working on creating a gui for digit recognizer  i had completed all the machine learning part successfully however i am getting many errors in my gui code for it  here is my code for gui of digit recognizer  list of errors in my code   please provide me the right code ,i am getting many errors including tcl error in my gui code in python,working creating gui digit recognizer completed machine learning part successfully however getting many errors gui code code gui digit recognizer errors code please provide right code,getting many errors including tcl error gui code python,getting many errors including tcl error gui code pythonworking creating gui digit recognizer completed machine learning part successfully however getting many errors gui code code gui digit recognizer errors code please provide right code,"['getting', 'many', 'errors', 'including', 'tcl', 'error', 'gui', 'code', 'pythonworking', 'creating', 'gui', 'digit', 'recognizer', 'completed', 'machine', 'learning', 'part', 'successfully', 'however', 'getting', 'many', 'errors', 'gui', 'code', 'code', 'gui', 'digit', 'recognizer', 'errors', 'code', 'please', 'provide', 'right', 'code']","['get', 'mani', 'error', 'includ', 'tcl', 'error', 'gui', 'code', 'pythonwork', 'creat', 'gui', 'digit', 'recogn', 'complet', 'machin', 'learn', 'part', 'success', 'howev', 'get', 'mani', 'error', 'gui', 'code', 'code', 'gui', 'digit', 'recogn', 'error', 'code', 'pleas', 'provid', 'right', 'code']"
5,5,5,7798822,72987174,How to loop through a dictionary values and get all numbers between two numbers?,"<p>I have a dictionary storing machine learning prediction probabilities. The keys of the dictionary are the indices of predicted instances and the values are lists containing class probabilities. I want to create a new dictionary to store all class probabilities that are between two numbers (0.48 and 0.55). With the code below I did not get the expected result as it seems my for loop does not iterate through all the values of my dictionary.</p>
<pre><code>d={348: [0,0,0,0.5,0,0,0.49], 349: [0,0,0.3,0.48,0.49,0.55,0.9], 350: 
[0,0,0.3,0.45,0.0,0.52,0.8]} 

dt={}
for i in d:
  for index, value in enumerate(d[i]):
    if value &gt;= 0.45 and value &lt;= 0.55:
      dt={i: {(str(index + 1)): [value]} }
print(dt) 
</code></pre>
<p>My output is now is only:</p>
<pre><code>{350: {'6': 0.52}}. 
</code></pre>
<p>However, I want to get all the numbers from all the values list between 0.48 and 0.55 and get the keys associated with the numbers selected as well as the indices of the numbers in the values list. My desired output is below:</p>
<pre><code>{348: {'4': 0.5, '7': 0,49}, 349: {'4': 0.48, '5': 0.49, '6', 0.55}, 350: {'4': 0.45, 
'6': 0.52}
</code></pre>
",38,3,1,3,python;list;dictionary,2022-07-15 03:46:00,2022-07-15 03:46:00,2022-07-15 04:23:40,i have a dictionary storing machine learning prediction probabilities  the keys of the dictionary are the indices of predicted instances and the values are lists containing class probabilities  i want to create a new dictionary to store all class probabilities that are between two numbers    and     with the code below i did not get the expected result as it seems my for loop does not iterate through all the values of my dictionary  my output is now is only  however  i want to get all the numbers from all the values list between   and   and get the keys associated with the numbers selected as well as the indices of the numbers in the values list  my desired output is below ,how to loop through a dictionary values and get all numbers between two numbers ,dictionary storing machine learning prediction probabilities keys dictionary indices predicted instances values lists containing class probabilities want create dictionary store class probabilities two numbers code get expected result seems loop iterate values dictionary output however want get numbers values get keys associated numbers selected well indices numbers values desired output,loop dictionary values get numbers two numbers,loop dictionary values get numbers two numbersdictionary storing machine learning prediction probabilities keys dictionary indices predicted instances values lists containing class probabilities want create dictionary store class probabilities two numbers code get expected result seems loop iterate values dictionary output however want get numbers values get keys associated numbers selected well indices numbers values desired output,"['loop', 'dictionary', 'values', 'get', 'numbers', 'two', 'numbersdictionary', 'storing', 'machine', 'learning', 'prediction', 'probabilities', 'keys', 'dictionary', 'indices', 'predicted', 'instances', 'values', 'lists', 'containing', 'class', 'probabilities', 'want', 'create', 'dictionary', 'store', 'class', 'probabilities', 'two', 'numbers', 'code', 'get', 'expected', 'result', 'seems', 'loop', 'iterate', 'values', 'dictionary', 'output', 'however', 'want', 'get', 'numbers', 'values', 'get', 'keys', 'associated', 'numbers', 'selected', 'well', 'indices', 'numbers', 'values', 'desired', 'output']","['loop', 'dictionari', 'valu', 'get', 'number', 'two', 'numbersdictionari', 'store', 'machin', 'learn', 'predict', 'probabl', 'key', 'dictionari', 'indic', 'predict', 'instanc', 'valu', 'list', 'contain', 'class', 'probabl', 'want', 'creat', 'dictionari', 'store', 'class', 'probabl', 'two', 'number', 'code', 'get', 'expect', 'result', 'seem', 'loop', 'iter', 'valu', 'dictionari', 'output', 'howev', 'want', 'get', 'number', 'valu', 'get', 'key', 'associ', 'number', 'select', 'well', 'indic', 'number', 'valu', 'desir', 'output']"
6,6,6,19528421,72982603,Problem with conditional rendering with React,"<p>I don't really have an error but I'm not getting the desired outcome. I have a Flask API as my backend. It returns JSON data, this data is then passed to the React frontend to be displayed after certain actions such as a button click.</p>
<p>The Flask backend is working fine, the data is being sent to the React frontend and I can use <code>console.log(data)</code> to see it in the console.</p>
<p>The issue is I tried doing something to make it so that while the data is being fetched from the API, I want a message such as &quot;Loading...&quot; to be displayed.</p>
<p>Here's what I did in that regard.</p>
<pre class=""lang-jsx prettyprint-override""><code>const [data, setData] = useState([{}])

useEffect(() =&gt;{
  fetch(&quot;/details&quot;).then(
    res =&gt; res.json()
  ).then(
    data =&gt; {
      setData(data)
      console.log(data)
    }
  )
}, [])
</code></pre>
<p>That <code>console.log(data)</code> does show my the JSON response in the console.</p>
<p>Then I do  this for the display.</p>
<pre class=""lang-jsx prettyprint-override""><code>return (
  &lt;div&gt;
    ...
    &lt;div&gt;
      {(typeof data.members === 'undefined') ? (
        &lt;p&gt;Loading...&lt;/p&gt;
      ) : (
        data.members.map((member, i) =&gt; (
          &lt;p key={i}&gt;{member}&lt;/p&gt;
        ))
      )}
    &lt;/div&gt;
  &lt;/div&gt;
);
</code></pre>
<p>This is supposed to display &quot;Loading...&quot; while the data is being fetched and then display the data. But it keeps displaying &quot;Loading...&quot; even though my data was fetched and it's never displayed.</p>
<p>I tried what emrich said and got this error <code>TypeError: Cannot read properties of undefined (reading 'map')</code></p>
<pre><code>App
D:/React/Candetect/src/App.jsx:32
  29 | &lt;FileList files={files} removeFile={removeFile} /&gt;
  30 | &lt;div&gt;
  31 | {(data.length === 0) ? (
&gt; 32 |   &lt;p&gt;Loading...&lt;/p&gt;
     | ^  33 | ) : (
  34 |   data.members.map((member, i) =&gt; (
  35 |     &lt;p key={i}&gt;{member}&lt;/p&gt;

D:/React/Candetect/src/App.jsx:15
  12 |   res =&gt; res.json()
  13 | ).then(
  14 |   data =&gt; {
&gt; 15 |     setData(data)
     | ^  16 |     console.log(data)
  17 |   }
  18 | )
</code></pre>
<p>This is the object from the backend</p>
<pre><code>{
    &quot;Information&quot;: [
        {
            &quot;college_name&quot;: null,
            &quot;company_names&quot;: [
                &quot;Marathwada Mitra Mandal’s College of Engineering&quot;
            ],
            &quot;degree&quot;: [
                &quot;B.E. IN COMPUTER ENGINEERING&quot;
            ],
            &quot;designation&quot;: [
                &quot;Machine Learning&quot;,
                &quot;TECHNICAL CONTENT WRITER&quot;,
                &quot;Schlumberger\nDATA ENGINEER&quot;
            ],
            &quot;email&quot;: &quot;omkarpathak27@gmail.com&quot;,
            &quot;experience&quot;: [
                &quot;Schlumberger&quot;,
                &quot;DATA ENGINEER&quot;,
                &quot;July 2018 - Present&quot;,
                &quot;• Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;brought migration time to 1.5 hours from 9 hours without any manual intervention&quot;,
                &quot;• Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business&quot;,
                &quot;units&quot;,
                &quot;• Working with L3 developer team to ensure the discussed Scrum PBI’s are delivered on time for data ingestions&quot;,
                &quot;• Planning and Executing QA and Production Release Cycle activities&quot;,
                &quot;Truso&quot;,
                &quot;FULL STACK DEVELOPER INTERN&quot;,
                &quot;• Created RESTful apis&quot;,
                &quot;• Tried my hands on Angular 5/6&quot;,
                &quot;• Was responsible for Django backend development&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;June 2018 - July 2018&quot;,
                &quot;Propeluss&quot;,
                &quot;DATA ENGINEERING INTERN&quot;,
                &quot;• Wrote various automation scripts to scrape data from various websites.&quot;,
                &quot;• Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;October 2017 - January 2018&quot;,
                &quot;extraction algorithms and applying Machine Learning to classify these articles.&quot;,
                &quot;• Also applied KNN with LSA for extracting relevant tags for various startups based on their works.&quot;,
                &quot;GeeksForGeeks&quot;,
                &quot;TECHNICAL CONTENT WRITER&quot;,
                &quot;• Published 4 articles for the topics such as Data Structures and Algorithms and Python&quot;,
                &quot;Pune, Maharashtra, India&quot;,
                &quot;July 2017 - September 2017&quot;,
                &quot;Softtestlab Technologies&quot;,
                &quot;WEB DEVELOPER INTERN&quot;,
                &quot;• Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes&quot;,
                &quot;• Worked on a live project for creating closure reports using PHP and Excel&quot;
            ],
            &quot;mobile_number&quot;: &quot;8087996634&quot;,
            &quot;name&quot;: &quot;Omkar Pathak&quot;,
            &quot;no_of_pages&quot;: 3,
            &quot;skills&quot;: [
                &quot;Python&quot;,
                &quot;Cloud&quot;,
                &quot;Github&quot;,
                &quot;Django&quot;,
                &quot;Writing&quot;,
                &quot;Unix&quot;,
                &quot;Algorithms&quot;,
                &quot;C&quot;,
                &quot;Windows&quot;,
                &quot;Training&quot;,
                &quot;C++&quot;,
                &quot;Flask&quot;,
                &quot;Scrum&quot;,
                &quot;Testing&quot;,
                &quot;Reports&quot;,
                &quot;Programming&quot;,
                &quot;Operating systems&quot;,
                &quot;Automation&quot;,
                &quot;Engineering&quot;,
                &quot;Html&quot;,
                &quot;Css&quot;,
                &quot;Analytics&quot;,
                &quot;Opencv&quot;,
                &quot;Content&quot;,
                &quot;Excel&quot;,
                &quot;Mysql&quot;,
                &quot;Migration&quot;,
                &quot;Api&quot;,
                &quot;Parser&quot;,
                &quot;Machine learning&quot;,
                &quot;System&quot;,
                &quot;Php&quot;,
                &quot;Apis&quot;,
                &quot;Auditing&quot;,
                &quot;Technical&quot;,
                &quot;Photography&quot;,
                &quot;Shell&quot;,
                &quot;Linux&quot;,
                &quot;Security&quot;,
                &quot;Website&quot;,
                &quot;Javascript&quot;
            ],
            &quot;total_experience&quot;: 4.5
        }
    ]
} 
</code></pre>
",45,2,0,2,javascript;reactjs,2022-07-14 20:26:50,2022-07-14 20:26:50,2022-07-15 04:06:56,i don t really have an error but i m not getting the desired outcome  i have a flask api as my backend  it returns json data  this data is then passed to the react frontend to be displayed after certain actions such as a button click  the flask backend is working fine  the data is being sent to the react frontend and i can use console log data  to see it in the console  the issue is i tried doing something to make it so that while the data is being fetched from the api  i want a message such as  loading     to be displayed  here s what i did in that regard  that console log data  does show my the json response in the console  then i do  this for the display  this is supposed to display  loading     while the data is being fetched and then display the data  but it keeps displaying  loading     even though my data was fetched and it s never displayed  i tried what emrich said and got this error typeerror  cannot read properties of undefined  reading  map   this is the object from the backend,problem with conditional rendering with react,really error getting desired outcome flask api backend returns json data data passed react frontend displayed certain actions button click flask backend working fine data sent react frontend use console log data see console issue tried something make data fetched api want message loading displayed regard console log data show json response console display supposed display loading data fetched display data keeps displaying loading even though data fetched never displayed tried emrich said got error typeerror cannot read properties undefined reading map object backend,problem conditional rendering react,problem conditional rendering reactreally error getting desired outcome flask api backend returns json data data passed react frontend displayed certain actions button click flask backend working fine data sent react frontend use console log data see console issue tried something make data fetched api want message loading displayed regard console log data show json response console display supposed display loading data fetched display data keeps displaying loading even though data fetched never displayed tried emrich said got error typeerror cannot read properties undefined reading map object backend,"['problem', 'conditional', 'rendering', 'reactreally', 'error', 'getting', 'desired', 'outcome', 'flask', 'api', 'backend', 'returns', 'json', 'data', 'data', 'passed', 'react', 'frontend', 'displayed', 'certain', 'actions', 'button', 'click', 'flask', 'backend', 'working', 'fine', 'data', 'sent', 'react', 'frontend', 'use', 'console', 'log', 'data', 'see', 'console', 'issue', 'tried', 'something', 'make', 'data', 'fetched', 'api', 'want', 'message', 'loading', 'displayed', 'regard', 'console', 'log', 'data', 'show', 'json', 'response', 'console', 'display', 'supposed', 'display', 'loading', 'data', 'fetched', 'display', 'data', 'keeps', 'displaying', 'loading', 'even', 'though', 'data', 'fetched', 'never', 'displayed', 'tried', 'emrich', 'said', 'got', 'error', 'typeerror', 'can', 'not', 'read', 'properties', 'undefined', 'reading', 'map', 'object', 'backend']","['problem', 'condit', 'render', 'reactreal', 'error', 'get', 'desir', 'outcom', 'flask', 'api', 'backend', 'return', 'json', 'data', 'data', 'pass', 'react', 'frontend', 'display', 'certain', 'action', 'button', 'click', 'flask', 'backend', 'work', 'fine', 'data', 'sent', 'react', 'frontend', 'use', 'consol', 'log', 'data', 'see', 'consol', 'issu', 'tri', 'someth', 'make', 'data', 'fetch', 'api', 'want', 'messag', 'load', 'display', 'regard', 'consol', 'log', 'data', 'show', 'json', 'respons', 'consol', 'display', 'suppos', 'display', 'load', 'data', 'fetch', 'display', 'data', 'keep', 'display', 'load', 'even', 'though', 'data', 'fetch', 'never', 'display', 'tri', 'emrich', 'said', 'got', 'error', 'typeerror', 'can', 'not', 'read', 'properti', 'undefin', 'read', 'map', 'object', 'backend']"
7,7,7,19552165,72986505,How to display the fit method list of parameters?,"<p>Im beginner to Machine learning and I'm currently taking a course (introduction to machine learning with python), and im using &quot;PyCharm&quot; for coding. One of the exercises im working on it is the &quot;Iris example&quot; with the KNeighborsClassifer.</p>
<hr />
<p>`***</p>
<pre><code>    In[26]:
    knn.fit(X_train, y_train) 

    Out[26]:
    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=1, p=2,
           weights='uniform')***
</code></pre>
<p>`</p>
<p>the problem here is when that I cannot get the same output( the list of parameters) for fit_method() when typing the syntax in [26]. I know that the book is using a different IDE than Pycharm but I get nothing when I run it. Any advice?</p>
",18,1,0,2,python;machine-learning,2022-07-15 02:18:39,2022-07-15 02:18:39,2022-07-15 02:57:53,im beginner to machine learning and i m currently taking a course  introduction to machine learning with python   and im using  pycharm  for coding  one of the exercises im working on it is the  iris example  with the kneighborsclassifer         the problem here is when that i cannot get the same output  the list of parameters  for fit_method   when typing the syntax in     i know that the book is using a different ide than pycharm but i get nothing when i run it  any advice ,how to display the fit method list of parameters ,im beginner machine learning currently taking course introduction machine learning python im using pycharm coding one exercises im working iris example kneighborsclassifer problem cannot get output parameters fit_method typing syntax know book using different ide pycharm get nothing run advice,display fit method parameters,display fit method parametersim beginner machine learning currently taking course introduction machine learning python im using pycharm coding one exercises im working iris example kneighborsclassifer problem cannot get output parameters fit_method typing syntax know book using different ide pycharm get nothing run advice,"['display', 'fit', 'method', 'parametersim', 'beginner', 'machine', 'learning', 'currently', 'taking', 'course', 'introduction', 'machine', 'learning', 'python', 'im', 'using', 'pycharm', 'coding', 'one', 'exercises', 'im', 'working', 'iris', 'example', 'kneighborsclassifer', 'problem', 'can', 'not', 'get', 'output', 'parameters', 'fit_method', 'typing', 'syntax', 'know', 'book', 'using', 'different', 'ide', 'pycharm', 'get', 'nothing', 'run', 'advice']","['display', 'fit', 'method', 'parametersim', 'beginn', 'machin', 'learn', 'current', 'take', 'cours', 'introduct', 'machin', 'learn', 'python', 'im', 'use', 'pycharm', 'code', 'one', 'exercis', 'im', 'work', 'iri', 'exampl', 'kneighborsclassif', 'problem', 'can', 'not', 'get', 'output', 'paramet', 'fit_method', 'type', 'syntax', 'know', 'book', 'use', 'differ', 'ide', 'pycharm', 'get', 'noth', 'run', 'advic']"
8,8,8,19550109,72982291,Machine Learning - How does a Single Perceptron learn?,"<p>How does a Perceptron learn?</p>
<p>To be more specific:
In university we had following exercise:
<a href=""https://i.stack.imgur.com/gZKE2.png"" rel=""nofollow noreferrer"">Perceptron exercicse</a></p>
<p>The solution was kind of easy:
After the first Data-Point the weights were (0, -4, -3, 6) after the second Data-Point (1,-2, -5, 3) and so on. The algorithm we used to update the weights was (in Pseudocode):</p>
<p>If Act.Fct(f(x)) != y:</p>
<p>w_new = w_old + y * x</p>
<p>Else: Do nothing.</p>
<p>[Act.Fct = Activation Function]</p>
<p>What I dont understand at this point is: What is with the &quot;Error-Function&quot; (or Loss-Function)? Does a single layer Perceptron dont use this function to update the weights? Because in this algorith we havent calculatet any Error?</p>
<p>I know that in Neuronal-Networks (that are just a bunch of chained Perceptrons?) the Error-Function is used for backpropagation. Im just confused by this exercise, that it is not used for the single layer perceptron as well?</p>
",27,0,0,2,machine-learning;perceptron,2022-07-14 20:05:45,2022-07-14 20:05:45,2022-07-15 01:31:12,how does a perceptron learn  if act fct f x      y  w_new   w_old   y   x else  do nothing   act fct   activation function  what i dont understand at this point is  what is with the  error function   or loss function   does a single layer perceptron dont use this function to update the weights  because in this algorith we havent calculatet any error  i know that in neuronal networks  that are just a bunch of chained perceptrons   the error function is used for backpropagation  im just confused by this exercise  that it is not used for the single layer perceptron as well ,machine learning   how does a single perceptron learn ,perceptron learn act fct f x w_new w_old x else nothing act fct activation function dont understand point error function loss function single layer perceptron dont use function update weights algorith havent calculatet error know neuronal networks bunch chained perceptrons error function used backpropagation im confused exercise used single layer perceptron well,machine learning single perceptron learn,machine learning single perceptron learnperceptron learn act fct f x w_new w_old x else nothing act fct activation function dont understand point error function loss function single layer perceptron dont use function update weights algorith havent calculatet error know neuronal networks bunch chained perceptrons error function used backpropagation im confused exercise used single layer perceptron well,"['machine', 'learning', 'single', 'perceptron', 'learnperceptron', 'learn', 'act', 'fct', 'f', 'x', 'w_new', 'w_old', 'x', 'else', 'nothing', 'act', 'fct', 'activation', 'function', 'dont', 'understand', 'point', 'error', 'function', 'loss', 'function', 'single', 'layer', 'perceptron', 'dont', 'use', 'function', 'update', 'weights', 'algorith', 'havent', 'calculatet', 'error', 'know', 'neuronal', 'networks', 'bunch', 'chained', 'perceptrons', 'error', 'function', 'used', 'backpropagation', 'im', 'confused', 'exercise', 'used', 'single', 'layer', 'perceptron', 'well']","['machin', 'learn', 'singl', 'perceptron', 'learnperceptron', 'learn', 'act', 'fct', 'f', 'x', 'w_new', 'w_old', 'x', 'els', 'noth', 'act', 'fct', 'activ', 'function', 'dont', 'understand', 'point', 'error', 'function', 'loss', 'function', 'singl', 'layer', 'perceptron', 'dont', 'use', 'function', 'updat', 'weight', 'algorith', 'havent', 'calculatet', 'error', 'know', 'neuron', 'network', 'bunch', 'chain', 'perceptron', 'error', 'function', 'use', 'backpropag', 'im', 'confus', 'exercis', 'use', 'singl', 'layer', 'perceptron', 'well']"
9,9,9,19550683,72983497,Where can I find a dataset containing plant nutrient deficiency,"<p>PlantVillage does not contain the data I need for a machine learning project. I need a dataset that contains plant nutrient deficiencies.</p>
",13,0,-2,3,machine-learning;deep-learning;computer-vision,2022-07-14 21:32:46,2022-07-14 21:32:46,2022-07-15 01:27:20,plantvillage does not contain the data i need for a machine learning project  i need a dataset that contains plant nutrient deficiencies ,where can i find a dataset containing plant nutrient deficiency,plantvillage contain data need machine learning project need dataset contains plant nutrient deficiencies,find dataset containing plant nutrient deficiency,find dataset containing plant nutrient deficiencyplantvillage contain data need machine learning project need dataset contains plant nutrient deficiencies,"['find', 'dataset', 'containing', 'plant', 'nutrient', 'deficiencyplantvillage', 'contain', 'data', 'need', 'machine', 'learning', 'project', 'need', 'dataset', 'contains', 'plant', 'nutrient', 'deficiencies']","['find', 'dataset', 'contain', 'plant', 'nutrient', 'deficiencyplantvillag', 'contain', 'data', 'need', 'machin', 'learn', 'project', 'need', 'dataset', 'contain', 'plant', 'nutrient', 'defici']"
10,10,10,19551051,72984124,IBM Watson Machine Learning Training a NLU Classifications Model Error,"<p>I am working on training a NLU classifications model with IBM Watson but I am stuck on the following error code:</p>
<pre><code>ApiException: Error: Have to specify either training_data or model, Code: 400 ,
X-global-transaction-id: d0c7f9d9-5abb-4327-a3cc-5976c540cc68
</code></pre>
<p>Here is my code:</p>
<pre><code>with open(RQ2H1_All_Roles_L_Train_JSON, 'rb') as file:
RQ2H1_All_Roles_L_Model = nlu.create_classifications_model(language='en', training_data=RQ2H1_All_Roles_L_Train_JSON, training_data_content_type='application/json', name='RQ2H1_All_Roles_L_Model', model_version='1.0.1').get_result()

print(&quot;Created a NLU Classifications model, RQ2H1_All_Roles_L_Model:&quot;)
print(json.dumps(RQ2H1_All_Roles_L_Model, indent=4))
</code></pre>
",16,0,-1,3,python;machine-learning;ibm-cloud,2022-07-14 22:24:48,2022-07-14 22:24:48,2022-07-15 01:23:20,i am working on training a nlu classifications model with ibm watson but i am stuck on the following error code  here is my code ,ibm watson machine learning training a nlu classifications model error,working training nlu classifications model ibm watson stuck following error code code,ibm watson machine learning training nlu classifications model error,ibm watson machine learning training nlu classifications model errorworking training nlu classifications model ibm watson stuck following error code code,"['ibm', 'watson', 'machine', 'learning', 'training', 'nlu', 'classifications', 'model', 'errorworking', 'training', 'nlu', 'classifications', 'model', 'ibm', 'watson', 'stuck', 'following', 'error', 'code', 'code']","['ibm', 'watson', 'machin', 'learn', 'train', 'nlu', 'classif', 'model', 'errorwork', 'train', 'nlu', 'classif', 'model', 'ibm', 'watson', 'stuck', 'follow', 'error', 'code', 'code']"
11,11,11,17119410,72985935,How to run Machine Learning algorithms in GPU,"<p>I have used backward elimination algorithm to reduce features. But the point is with large amount of features and samples, it run on CPU and pretty slow.</p>
<p>How could i run it multithreading on GPU like i train Deep Learning. This is my code</p>
<pre><code>import pandas as pd
data = pd.read_csv('data.csv')
X = data.drop(['Path','id','label'], axis=1)
y = data['label']

from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from sklearn.linear_model import LinearRegression
lreg = LinearRegression()
new_sfs = sfs(lreg, k_features=1600, forward=False, verbose=1, scoring='neg_mean_squared_error')
new_sfs = new_sfs.fit(X, y)
feat_names = list(sfs1.k_feature_names_)
</code></pre>
",13,0,-1,5,python;machine-learning;time;process;gpu,2022-07-15 01:19:29,2022-07-15 01:19:29,2022-07-15 01:19:29,i have used backward elimination algorithm to reduce features  but the point is with large amount of features and samples  it run on cpu and pretty slow  how could i run it multithreading on gpu like i train deep learning  this is my code,how to run machine learning algorithms in gpu,used backward elimination algorithm reduce features point large amount features samples run cpu pretty slow could run multithreading gpu like train deep learning code,run machine learning algorithms gpu,run machine learning algorithms gpuused backward elimination algorithm reduce features point large amount features samples run cpu pretty slow could run multithreading gpu like train deep learning code,"['run', 'machine', 'learning', 'algorithms', 'gpuused', 'backward', 'elimination', 'algorithm', 'reduce', 'features', 'point', 'large', 'amount', 'features', 'samples', 'run', 'cpu', 'pretty', 'slow', 'could', 'run', 'multithreading', 'gpu', 'like', 'train', 'deep', 'learning', 'code']","['run', 'machin', 'learn', 'algorithm', 'gpuus', 'backward', 'elimin', 'algorithm', 'reduc', 'featur', 'point', 'larg', 'amount', 'featur', 'sampl', 'run', 'cpu', 'pretti', 'slow', 'could', 'run', 'multithread', 'gpu', 'like', 'train', 'deep', 'learn', 'code']"
12,12,12,18670887,72985526,Loading ONNX model asynchronously runs forever or throws Null Reference,"<p>I am currently building a Xamarin app for UWP. I am integrating the Windows.AI.MachineLearning/Microsoft.ML.Tensorflow packages so that I can use an .onnx format TensorFlow model trained in Python within my C# Xamarin app. I have verified using the ONNX python package that my .onnx file is a valid model, so that is not an issue.</p>
<p>The function is question is called LoadModelAsync(). I get two errors depending on how I try to fix it. Either I get a null reference exception,
<strong>System.NullReferenceException: 'Object reference not set to an instance of an object.'</strong>, or the function runs forever and never returns. I know this because I have a string that should update on my XAML page once the function completes, and it never shows up.</p>
<p>I am following closely this guide: <a href=""https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-started-uwp"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-started-uwp</a>
and this code: <a href=""https://github.com/Azure-Samples/cognitive-services-onnx-customvision-sample/blob/master/MainPage.xaml.cs"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/cognitive-services-onnx-customvision-sample/blob/master/MainPage.xaml.cs</a></p>
<p>Here is the function where the error occurs, LoadModelAsync. I am calling an .onnx model to use it to make a prediction. Here is how the model, called <em>modelGen</em>, is initialize and later called. The function It is used exactly the same as the code in the first link:</p>
<pre><code>private TempModel modelGen = null;

...

private async Task LoadModelAsync() {
        // Load a machine learning model
        StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx:///Assets/model.onnx&quot;));
        modelGen = await TempModel.CreateFromStreamAsync(modelFile as IRandomAccessStreamReference);
    }

...

async Task StartInventoryAsync() {
        ...

        _activitystatus = &quot;Before load model...&quot;; RaisePropertyChanged(() =&gt; activitystatus);

        if (_model == null) { // Load the model here
            // two options here, the Task.Run or just running the function
            // await Task.Run(async () =&gt; await LoadModelAsync()); // tried this, never runs
            LoadModelAsync();  // gives the null reference exception
        }

        // this string will update on my Xamarin UI if the previous conditional finishes
        _activitystatus = &quot;After load model.&quot;; RaisePropertyChanged(() =&gt; activitystatus);

        ...
    }
</code></pre>
<p>I am not sure why the code runs forever. I believe it to be an issue with the asynchronous nature of the code, but as far as I can tell the amount of awaits and asyncs are the same as the MNIST example. I've read the answers to <a href=""https://stackoverflow.com/questions/9895048/async-call-with-await-in-httpclient-never-returns"">this question</a> and <a href=""https://stackoverflow.com/questions/10343632/httpclient-getasync-never-returns-when-using-await-async/10351400#10351400"">this question</a>, but I cannot seem to draw a solution from them regarding which async/await uses I should get rid of.</p>
<p>For reference, here is my TempModel class:</p>
<pre><code>public sealed class TempModel {
        private LearningModel model;
        private LearningModelSession session;
        private LearningModelBinding binding;
        public static async Task&lt;TempModel&gt; CreateFromStreamAsync(IRandomAccessStreamReference stream) {
            TempModel _model = new TempModel();
            _model.model   = await LearningModel.LoadFromStreamAsync(stream);
            _model.session = new LearningModelSession(_model.model);
            _model.binding = new LearningModelBinding(_model.session);
            return _model;
        }
        public async Task&lt;List&lt;float&gt;&gt; EvaluateAsync(TempInput input) {
            binding.Bind(&quot;lstm_12_input&quot;, input.Input3);
            var result = await session.EvaluateAsync(binding, string.Empty);
            var resultTensor = result.Outputs[&quot;dense_13&quot;] as TensorFloat;
            var resultVector = resultTensor.GetAsVectorView();
            List&lt;float&gt; op = new List&lt;float&gt;(resultVector);
            return op;
        }
    }

    private async Task LoadModelAsync() {  // Load a machine learning model
        StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx://newmodel.onnx&quot;));
        _model = await TempModel.CreateFromStreamAsync(modelFile as IRandomAccessStreamReference);
        _activitystatus = &quot;Finished LoadModelAsync&quot;; RaisePropertyChanged(() =&gt; activitystatus);
    }
</code></pre>
",17,0,0,5,c#;.net;tensorflow;uwp;onnx,2022-07-15 00:36:45,2022-07-15 00:36:45,2022-07-15 00:36:45,i am currently building a xamarin app for uwp  i am integrating the windows ai machinelearning microsoft ml tensorflow packages so that i can use an  onnx format tensorflow model trained in python within my c  xamarin app  i have verified using the onnx python package that my  onnx file is a valid model  so that is not an issue  here is the function where the error occurs  loadmodelasync  i am calling an  onnx model to use it to make a prediction  here is how the model  called modelgen  is initialize and later called  the function it is used exactly the same as the code in the first link  i am not sure why the code runs forever  i believe it to be an issue with the asynchronous nature of the code  but as far as i can tell the amount of awaits and asyncs are the same as the mnist example  i ve read the answers to  and   but i cannot seem to draw a solution from them regarding which async await uses i should get rid of  for reference  here is my tempmodel class ,loading onnx model asynchronously runs forever or throws null reference,currently building xamarin app uwp integrating windows ai machinelearning microsoft ml tensorflow packages use onnx format tensorflow model trained python within c xamarin app verified using onnx python package onnx file valid model issue function error occurs loadmodelasync calling onnx model use make prediction model called modelgen initialize later called function used exactly code first link sure code runs forever believe issue asynchronous nature code far tell amount awaits asyncs mnist example read answers cannot seem draw solution regarding async await uses get rid reference tempmodel class,loading onnx model asynchronously runs forever throws null reference,loading onnx model asynchronously runs forever throws null referencecurrently building xamarin app uwp integrating windows ai machinelearning microsoft ml tensorflow packages use onnx format tensorflow model trained python within c xamarin app verified using onnx python package onnx file valid model issue function error occurs loadmodelasync calling onnx model use make prediction model called modelgen initialize later called function used exactly code first link sure code runs forever believe issue asynchronous nature code far tell amount awaits asyncs mnist example read answers cannot seem draw solution regarding async await uses get rid reference tempmodel class,"['loading', 'onnx', 'model', 'asynchronously', 'runs', 'forever', 'throws', 'null', 'referencecurrently', 'building', 'xamarin', 'app', 'uwp', 'integrating', 'windows', 'ai', 'machinelearning', 'microsoft', 'ml', 'tensorflow', 'packages', 'use', 'onnx', 'format', 'tensorflow', 'model', 'trained', 'python', 'within', 'c', 'xamarin', 'app', 'verified', 'using', 'onnx', 'python', 'package', 'onnx', 'file', 'valid', 'model', 'issue', 'function', 'error', 'occurs', 'loadmodelasync', 'calling', 'onnx', 'model', 'use', 'make', 'prediction', 'model', 'called', 'modelgen', 'initialize', 'later', 'called', 'function', 'used', 'exactly', 'code', 'first', 'link', 'sure', 'code', 'runs', 'forever', 'believe', 'issue', 'asynchronous', 'nature', 'code', 'far', 'tell', 'amount', 'awaits', 'asyncs', 'mnist', 'example', 'read', 'answers', 'can', 'not', 'seem', 'draw', 'solution', 'regarding', 'async', 'await', 'uses', 'get', 'rid', 'reference', 'tempmodel', 'class']","['load', 'onnx', 'model', 'asynchron', 'run', 'forev', 'throw', 'null', 'referencecurr', 'build', 'xamarin', 'app', 'uwp', 'integr', 'window', 'ai', 'machinelearn', 'microsoft', 'ml', 'tensorflow', 'packag', 'use', 'onnx', 'format', 'tensorflow', 'model', 'train', 'python', 'within', 'c', 'xamarin', 'app', 'verifi', 'use', 'onnx', 'python', 'packag', 'onnx', 'file', 'valid', 'model', 'issu', 'function', 'error', 'occur', 'loadmodelasync', 'call', 'onnx', 'model', 'use', 'make', 'predict', 'model', 'call', 'modelgen', 'initi', 'later', 'call', 'function', 'use', 'exactli', 'code', 'first', 'link', 'sure', 'code', 'run', 'forev', 'believ', 'issu', 'asynchron', 'natur', 'code', 'far', 'tell', 'amount', 'await', 'async', 'mnist', 'exampl', 'read', 'answer', 'can', 'not', 'seem', 'draw', 'solut', 'regard', 'async', 'await', 'use', 'get', 'rid', 'refer', 'tempmodel', 'class']"
13,13,13,8973620,72978955,Time periods to evenly-spaced time series,"<p>I need to prepare data with time periods for machine learning in the way that I get equal spacing between timestamps. For example, for 3 hours spacing, I would like to have the following timestamps: 00:00, 03:00, 6:00, 9:00, 12:00, 15:00... For example:</p>
<pre><code>df = pd.DataFrame({'Start': ['2022-07-01 11:30', '2022-07-01 22:30'], 'End': ['2022-07-01 18:30', '2022-07-02 3:30'], 'Val': ['a', 'b']})
for col in ['Start', 'End']:
    df[col] = df[col].apply(pd.to_datetime)
print(df)
</code></pre>
<p>Output:</p>
<pre><code>                Start                 End Val
0 2022-07-01 11:30:00 2022-07-01 18:30:00   a
1 2022-07-01 22:30:00 2022-07-02 03:30:00   b
</code></pre>
<p>I try to get timestamps:</p>
<pre><code>df['Datetime'] = df.apply(lambda x: pd.date_range(x['Start'], x['End'], freq='3H'), axis=1)
df = df.explode('Datetime').drop(['Start', 'End'], axis=1)
df['Datetime'] = df['Datetime'].dt.round('H')
print(df[['Datetime', 'Val']])
</code></pre>
<p>Output:</p>
<pre><code>             Datetime Val
0 2022-07-01 12:00:00   a
0 2022-07-01 14:00:00   a
0 2022-07-01 18:00:00   a
1 2022-07-01 22:00:00   b
1 2022-07-02 02:00:00   b
</code></pre>
<p>As you can see, those timestamps are not equally spaced. My expected result:</p>
<pre><code>            Datetime  Val
4 2022-07-01 12:00:00    a
5 2022-07-01 15:00:00    a
6 2022-07-01 18:00:00    a
7 2022-07-01 21:00:00  NaN
8 2022-07-02 00:00:00    b
9 2022-07-02 03:00:00    b
</code></pre>
",58,4,2,3,python;pandas;time-series,2022-07-14 15:51:06,2022-07-14 15:51:06,2022-07-14 22:35:28,i need to prepare data with time periods for machine learning in the way that i get equal spacing between timestamps  for example  for  hours spacing  i would like to have the following timestamps                      for example  output  i try to get timestamps  output  as you can see  those timestamps are not equally spaced  my expected result ,time periods to evenly spaced time series,need prepare data time periods machine learning way get equal spacing timestamps example hours spacing would like following timestamps example output try get timestamps output see timestamps equally spaced expected result,time periods evenly spaced time series,time periods evenly spaced time seriesneed prepare data time periods machine learning way get equal spacing timestamps example hours spacing would like following timestamps example output try get timestamps output see timestamps equally spaced expected result,"['time', 'periods', 'evenly', 'spaced', 'time', 'seriesneed', 'prepare', 'data', 'time', 'periods', 'machine', 'learning', 'way', 'get', 'equal', 'spacing', 'timestamps', 'example', 'hours', 'spacing', 'would', 'like', 'following', 'timestamps', 'example', 'output', 'try', 'get', 'timestamps', 'output', 'see', 'timestamps', 'equally', 'spaced', 'expected', 'result']","['time', 'period', 'evenli', 'space', 'time', 'seriesne', 'prepar', 'data', 'time', 'period', 'machin', 'learn', 'way', 'get', 'equal', 'space', 'timestamp', 'exampl', 'hour', 'space', 'would', 'like', 'follow', 'timestamp', 'exampl', 'output', 'tri', 'get', 'timestamp', 'output', 'see', 'timestamp', 'equal', 'space', 'expect', 'result']"
14,14,14,16469159,72963263,Python read from csv with condition for TimeSeriesGenerator,"<p>I have a .csv file with many entries that looks like this:</p>
<pre><code>observation1, observation2, tag
observation1, observation2, tag
...
b r e a k
observation1, observation2, tag
...
b r e a k
</code></pre>
<p>whereas the observations are some numbers and the tag the ground truth true/false.</p>
<p>the <code>break</code> part comes with the data and symbolizes the end of a file and the end of an observation chain. Datapoints within two <code>break</code> entries belong together. (All those datapoints are merged from multiple files into one huge csv).</p>
<p>With this data I am supposed to do some machine learning using the tensorflow TimeSeriesGenerator.</p>
<p>I found out however, that TSG uses a fixed time series chain length, which means I have to do some cutting/filtering of my data given.</p>
<p>Condition one, is that if a <code>true</code> appears in the chain, it has to be the last value. Condition two, that all chains consist of the same amount of entries.</p>
<p>This means, if say my chain length would be 3, then the following chains are allowed:</p>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, false
observation1, observation2, true
b r e a k
</code></pre>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, false
observation1, observation2, false
b r e a k
</code></pre>
<p>but not</p>
<pre><code>b r e a k
observation1, observation2, false
observation1, observation2, true
observation1, observation2, false
b r e a k
</code></pre>
<p>A chain like this would also be allowed</p>
<pre><code>observation1, observation2, false
observation1, observation2, false
observation1, observation2, false
observation1, observation2, true
</code></pre>
<p>as I could simply throw the first line away to get a length of 3.</p>
<p>But not a chain like this:</p>
<pre><code>observation1, observation2, false
b r e a k
observation1, observation2, false
observation1, observation2, true
b r e a k
</code></pre>
<p>This means I need some way (my guess would be pandas) to filter the .csv file and find all occurences, where between to <code>b r e a k</code> lines there are at least x amount of <code>false</code> datapoints followed by a <code>true</code> or another <code>false</code>.</p>
<p>What would be a good way of achieving this filtering?</p>
",29,1,1,4,python;csv;tensorflow;filter,2022-07-13 14:10:11,2022-07-13 14:10:11,2022-07-14 22:21:17,i have a  csv file with many entries that looks like this  whereas the observations are some numbers and the tag the ground truth true false  the break part comes with the data and symbolizes the end of a file and the end of an observation chain  datapoints within two break entries belong together   all those datapoints are merged from multiple files into one huge csv   with this data i am supposed to do some machine learning using the tensorflow timeseriesgenerator  i found out however  that tsg uses a fixed time series chain length  which means i have to do some cutting filtering of my data given  condition one  is that if a true appears in the chain  it has to be the last value  condition two  that all chains consist of the same amount of entries  this means  if say my chain length would be   then the following chains are allowed  but not a chain like this would also be allowed as i could simply throw the first line away to get a length of   but not a chain like this  this means i need some way  my guess would be pandas  to filter the  csv file and find all occurences  where between to b r e a k lines there are at least x amount of false datapoints followed by a true or another false  what would be a good way of achieving this filtering ,python read from csv with condition for timeseriesgenerator,csv file many entries looks like whereas observations numbers tag ground truth true false break part comes data symbolizes end file end observation chain datapoints within two break entries belong together datapoints merged multiple files one huge csv data supposed machine learning using tensorflow timeseriesgenerator found however tsg uses fixed time series chain length means cutting filtering data given condition one true appears chain last value condition two chains consist amount entries means say chain length would following chains allowed chain like would also allowed could simply throw first line away get length chain like means need way guess would pandas filter csv file find occurences b r e k lines least x amount false datapoints followed true another false would good way achieving filtering,python read csv condition timeseriesgenerator,python read csv condition timeseriesgeneratorcsv file many entries looks like whereas observations numbers tag ground truth true false break part comes data symbolizes end file end observation chain datapoints within two break entries belong together datapoints merged multiple files one huge csv data supposed machine learning using tensorflow timeseriesgenerator found however tsg uses fixed time series chain length means cutting filtering data given condition one true appears chain last value condition two chains consist amount entries means say chain length would following chains allowed chain like would also allowed could simply throw first line away get length chain like means need way guess would pandas filter csv file find occurences b r e k lines least x amount false datapoints followed true another false would good way achieving filtering,"['python', 'read', 'csv', 'condition', 'timeseriesgeneratorcsv', 'file', 'many', 'entries', 'looks', 'like', 'whereas', 'observations', 'numbers', 'tag', 'ground', 'truth', 'true', 'false', 'break', 'part', 'comes', 'data', 'symbolizes', 'end', 'file', 'end', 'observation', 'chain', 'datapoints', 'within', 'two', 'break', 'entries', 'belong', 'together', 'datapoints', 'merged', 'multiple', 'files', 'one', 'huge', 'csv', 'data', 'supposed', 'machine', 'learning', 'using', 'tensorflow', 'timeseriesgenerator', 'found', 'however', 'tsg', 'uses', 'fixed', 'time', 'series', 'chain', 'length', 'means', 'cutting', 'filtering', 'data', 'given', 'condition', 'one', 'true', 'appears', 'chain', 'last', 'value', 'condition', 'two', 'chains', 'consist', 'amount', 'entries', 'means', 'say', 'chain', 'length', 'would', 'following', 'chains', 'allowed', 'chain', 'like', 'would', 'also', 'allowed', 'could', 'simply', 'throw', 'first', 'line', 'away', 'get', 'length', 'chain', 'like', 'means', 'need', 'way', 'guess', 'would', 'pandas', 'filter', 'csv', 'file', 'find', 'occurences', 'b', 'r', 'e', 'k', 'lines', 'least', 'x', 'amount', 'false', 'datapoints', 'followed', 'true', 'another', 'false', 'would', 'good', 'way', 'achieving', 'filtering']","['python', 'read', 'csv', 'condit', 'timeseriesgeneratorcsv', 'file', 'mani', 'entri', 'look', 'like', 'wherea', 'observ', 'number', 'tag', 'ground', 'truth', 'true', 'fals', 'break', 'part', 'come', 'data', 'symbol', 'end', 'file', 'end', 'observ', 'chain', 'datapoint', 'within', 'two', 'break', 'entri', 'belong', 'togeth', 'datapoint', 'merg', 'multipl', 'file', 'one', 'huge', 'csv', 'data', 'suppos', 'machin', 'learn', 'use', 'tensorflow', 'timeseriesgener', 'found', 'howev', 'tsg', 'use', 'fix', 'time', 'seri', 'chain', 'length', 'mean', 'cut', 'filter', 'data', 'given', 'condit', 'one', 'true', 'appear', 'chain', 'last', 'valu', 'condit', 'two', 'chain', 'consist', 'amount', 'entri', 'mean', 'say', 'chain', 'length', 'would', 'follow', 'chain', 'allow', 'chain', 'like', 'would', 'also', 'allow', 'could', 'simpli', 'throw', 'first', 'line', 'away', 'get', 'length', 'chain', 'like', 'mean', 'need', 'way', 'guess', 'would', 'panda', 'filter', 'csv', 'file', 'find', 'occur', 'b', 'r', 'e', 'k', 'line', 'least', 'x', 'amount', 'fals', 'datapoint', 'follow', 'true', 'anoth', 'fals', 'would', 'good', 'way', 'achiev', 'filter']"
15,15,15,18089331,72914328,Compare similarity of two names and identify duplicates with neural network,"<p>I have a dataset which contains pairs of names, it looks like this:</p>
<pre><code>ID; name1; name2
1; Mike Miller; Mike Miler
2; John Doe; Pete McGillen
3; Sara Johnson; Edita Johnson
4; John Lemond-Lee Peter; John LL. Peter
5; Marta Sunz; Martha Sund
6; John Peter; Johanna Petera
7; Joanna Nemzik; Joanna Niemczik
</code></pre>
<p>I have some cases, which are labelled. So I check them manually and decide if these are duplicates or not. The manual judgement in these cases would be:</p>
<pre><code>1: Is a duplicate
2: Is not a duplicate
3: Is not a duplicate
4: Is a duplicate
5: Is not a duplicate
6: Is not a duplicate
7: Is a duplicate
</code></pre>
<p>(The 7th case is a specific case, because here phonetics come into the game too. However, this is not the main problem, I am ok with ignoring phonetics.)</p>
<p>A first approach would be to calculate the Levenshtein-distance for each pair and mark those as a duplicate, where the Levenshtein-distance is for example less or equal than 2. This would lead to the following output:</p>
<pre><code>1: Levenshtein distance: 2 =&gt; duplicate
2: Levenshtein distance: 11 =&gt; not a duplicate
3: Levenshtein distance: 4 =&gt; not a duplicate
4: Levenshtein distance: 8 =&gt; not a duplicate
5: Levenshtein distance: 2 =&gt; duplicate
6: Levenshtein distance: 4 =&gt; not a duplicate
7: Levenshtein distance: 2 =&gt; duplicate
</code></pre>
<p>This would be an approach which uses a &quot;fixed&quot; algorithm based on the Levinshtein distance.</p>
<p>Now, I would like to do this task with using a neural network / machine learning:</p>
<p>I do not need the neural network to detect semantic similarity, like &quot;hospital&quot; and &quot;clininc&quot;. However, I would like to avoid the Levenshtein-distance, as I would like the ML algorithm to be able to detect &quot;John Lemond-Lee Peter&quot; and &quot;John LL. Peter&quot; as a potential duplicate, also not with a 100% certainty. The Levenshtein distance would lead to a relative high number in this case (8), as there are quite some characters to be added. In a case like &quot;John Peter&quot; and &quot;Johanna Petera&quot; the Levenshtein-distance would lead to a smaller number (4), however this is in fact no duplicate and for this case I would hope that the ML algorithm would be able to detect that this is likely not a duplicate. So I need the ML algorithm to &quot;learn the way I need the duplicates to be checked&quot;. With my labelling I would give as an input I would give the ML algorithm the direction, of what I want.</p>
<p>I actually thought that this should be an easy task for a ML algorithm / neural network, but I am not sure.</p>
<p><strong>How can I implement a neural network to compare the pairs of names and identify duplicates without using an explicit distance metric (like the Levenshtein distance, euclidean etc.)?</strong></p>
<p>I thought that it would be possible to convert the strings to numbers and a neural network can work with this and learn to detect duplicates according to my labelling style. So without having to specify a distance metric. <strong>I thought about an human: I would give this task to a person and this person would judge and make a decision. This person has no clue about a Levenshtein-distance or any other mathematical concept. So I just want to train the neural network to learn to do what the human is doing.</strong> Of course, every human is different and it also depends on my labelling.</p>
<p>(<em>Edit</em>: The ML/neural network solutions I have seen so far (like <a href=""https://medium.springboard.com/identifying-duplicate-questions-a-machine-learning-case-study-37117723844"" rel=""nofollow noreferrer"">this</a>) use a metric like levenshtein as a feature input. But as I said I thought it should be possible to teach the neural network the &quot;human judgement&quot; without making use of such a distance measure? Regarding my specific case with having pairs of names: What would the benefit be a of a ML approach using levenshtein distance as a feature? Because it will just detect those pairs of names as a duplicate that have a low levenshtein distance. So I could use a simple algorithm to mark a pair as duplicate if the levenshtein distance between the two names is less than x. Why use a ML instead, what would be the additional benefit?)</p>
",51,1,3,5,python;tensorflow;machine-learning;neural-network;levenshtein-distance,2022-07-08 21:55:04,2022-07-08 21:55:04,2022-07-14 21:05:05,i have a dataset which contains pairs of names  it looks like this  i have some cases  which are labelled  so i check them manually and decide if these are duplicates or not  the manual judgement in these cases would be   the th case is a specific case  because here phonetics come into the game too  however  this is not the main problem  i am ok with ignoring phonetics   a first approach would be to calculate the levenshtein distance for each pair and mark those as a duplicate  where the levenshtein distance is for example less or equal than   this would lead to the following output  this would be an approach which uses a  fixed  algorithm based on the levinshtein distance  now  i would like to do this task with using a neural network   machine learning  i do not need the neural network to detect semantic similarity  like  hospital  and  clininc   however  i would like to avoid the levenshtein distance  as i would like the ml algorithm to be able to detect  john lemond lee peter  and  john ll  peter  as a potential duplicate  also not with a   certainty  the levenshtein distance would lead to a relative high number in this case     as there are quite some characters to be added  in a case like  john peter  and  johanna petera  the levenshtein distance would lead to a smaller number     however this is in fact no duplicate and for this case i would hope that the ml algorithm would be able to detect that this is likely not a duplicate  so i need the ml algorithm to  learn the way i need the duplicates to be checked   with my labelling i would give as an input i would give the ml algorithm the direction  of what i want  i actually thought that this should be an easy task for a ml algorithm   neural network  but i am not sure  how can i implement a neural network to compare the pairs of names and identify duplicates without using an explicit distance metric  like the levenshtein distance  euclidean etc    i thought that it would be possible to convert the strings to numbers and a neural network can work with this and learn to detect duplicates according to my labelling style  so without having to specify a distance metric  i thought about an human  i would give this task to a person and this person would judge and make a decision  this person has no clue about a levenshtein distance or any other mathematical concept  so i just want to train the neural network to learn to do what the human is doing  of course  every human is different and it also depends on my labelling   edit  the ml neural network solutions i have seen so far  like   use a metric like levenshtein as a feature input  but as i said i thought it should be possible to teach the neural network the  human judgement  without making use of such a distance measure  regarding my specific case with having pairs of names  what would the benefit be a of a ml approach using levenshtein distance as a feature  because it will just detect those pairs of names as a duplicate that have a low levenshtein distance  so i could use a simple algorithm to mark a pair as duplicate if the levenshtein distance between the two names is less than x  why use a ml instead  what would be the additional benefit  ,compare similarity of two names and identify duplicates with neural network,dataset contains pairs names looks like cases labelled check manually decide duplicates manual judgement cases would th case specific case phonetics come game however main problem ok ignoring phonetics first approach would calculate levenshtein distance pair mark duplicate levenshtein distance example less equal would lead following output would approach uses fixed algorithm based levinshtein distance would like task using neural network machine learning need neural network detect semantic similarity like hospital clininc however would like avoid levenshtein distance would like ml algorithm able detect john lemond lee peter john peter potential duplicate also certainty levenshtein distance would lead relative high number case quite characters added case like john peter johanna petera levenshtein distance would lead smaller number however fact duplicate case would hope ml algorithm would able detect likely duplicate need ml algorithm learn way need duplicates checked labelling would give input would give ml algorithm direction want actually thought easy task ml algorithm neural network sure implement neural network compare pairs names identify duplicates without using explicit distance metric like levenshtein distance euclidean etc thought would possible convert strings numbers neural network work learn detect duplicates according labelling style without specify distance metric thought human would give task person person would judge make decision person clue levenshtein distance mathematical concept want train neural network learn human course every human different also depends labelling edit ml neural network solutions seen far like use metric like levenshtein feature input said thought possible teach neural network human judgement without making use distance measure regarding specific case pairs names would benefit ml approach using levenshtein distance feature detect pairs names duplicate low levenshtein distance could use simple algorithm mark pair duplicate levenshtein distance two names less x use ml instead would additional benefit,compare similarity two names identify duplicates neural network,compare similarity two names identify duplicates neural networkdataset contains pairs names looks like cases labelled check manually decide duplicates manual judgement cases would th case specific case phonetics come game however main problem ok ignoring phonetics first approach would calculate levenshtein distance pair mark duplicate levenshtein distance example less equal would lead following output would approach uses fixed algorithm based levinshtein distance would like task using neural network machine learning need neural network detect semantic similarity like hospital clininc however would like avoid levenshtein distance would like ml algorithm able detect john lemond lee peter john peter potential duplicate also certainty levenshtein distance would lead relative high number case quite characters added case like john peter johanna petera levenshtein distance would lead smaller number however fact duplicate case would hope ml algorithm would able detect likely duplicate need ml algorithm learn way need duplicates checked labelling would give input would give ml algorithm direction want actually thought easy task ml algorithm neural network sure implement neural network compare pairs names identify duplicates without using explicit distance metric like levenshtein distance euclidean etc thought would possible convert strings numbers neural network work learn detect duplicates according labelling style without specify distance metric thought human would give task person person would judge make decision person clue levenshtein distance mathematical concept want train neural network learn human course every human different also depends labelling edit ml neural network solutions seen far like use metric like levenshtein feature input said thought possible teach neural network human judgement without making use distance measure regarding specific case pairs names would benefit ml approach using levenshtein distance feature detect pairs names duplicate low levenshtein distance could use simple algorithm mark pair duplicate levenshtein distance two names less x use ml instead would additional benefit,"['compare', 'similarity', 'two', 'names', 'identify', 'duplicates', 'neural', 'networkdataset', 'contains', 'pairs', 'names', 'looks', 'like', 'cases', 'labelled', 'check', 'manually', 'decide', 'duplicates', 'manual', 'judgement', 'cases', 'would', 'th', 'case', 'specific', 'case', 'phonetics', 'come', 'game', 'however', 'main', 'problem', 'ok', 'ignoring', 'phonetics', 'first', 'approach', 'would', 'calculate', 'levenshtein', 'distance', 'pair', 'mark', 'duplicate', 'levenshtein', 'distance', 'example', 'less', 'equal', 'would', 'lead', 'following', 'output', 'would', 'approach', 'uses', 'fixed', 'algorithm', 'based', 'levinshtein', 'distance', 'would', 'like', 'task', 'using', 'neural', 'network', 'machine', 'learning', 'need', 'neural', 'network', 'detect', 'semantic', 'similarity', 'like', 'hospital', 'clininc', 'however', 'would', 'like', 'avoid', 'levenshtein', 'distance', 'would', 'like', 'ml', 'algorithm', 'able', 'detect', 'john', 'lemond', 'lee', 'peter', 'john', 'peter', 'potential', 'duplicate', 'also', 'certainty', 'levenshtein', 'distance', 'would', 'lead', 'relative', 'high', 'number', 'case', 'quite', 'characters', 'added', 'case', 'like', 'john', 'peter', 'johanna', 'petera', 'levenshtein', 'distance', 'would', 'lead', 'smaller', 'number', 'however', 'fact', 'duplicate', 'case', 'would', 'hope', 'ml', 'algorithm', 'would', 'able', 'detect', 'likely', 'duplicate', 'need', 'ml', 'algorithm', 'learn', 'way', 'need', 'duplicates', 'checked', 'labelling', 'would', 'give', 'input', 'would', 'give', 'ml', 'algorithm', 'direction', 'want', 'actually', 'thought', 'easy', 'task', 'ml', 'algorithm', 'neural', 'network', 'sure', 'implement', 'neural', 'network', 'compare', 'pairs', 'names', 'identify', 'duplicates', 'without', 'using', 'explicit', 'distance', 'metric', 'like', 'levenshtein', 'distance', 'euclidean', 'etc', 'thought', 'would', 'possible', 'convert', 'strings', 'numbers', 'neural', 'network', 'work', 'learn', 'detect', 'duplicates', 'according', 'labelling', 'style', 'without', 'specify', 'distance', 'metric', 'thought', 'human', 'would', 'give', 'task', 'person', 'person', 'would', 'judge', 'make', 'decision', 'person', 'clue', 'levenshtein', 'distance', 'mathematical', 'concept', 'want', 'train', 'neural', 'network', 'learn', 'human', 'course', 'every', 'human', 'different', 'also', 'depends', 'labelling', 'edit', 'ml', 'neural', 'network', 'solutions', 'seen', 'far', 'like', 'use', 'metric', 'like', 'levenshtein', 'feature', 'input', 'said', 'thought', 'possible', 'teach', 'neural', 'network', 'human', 'judgement', 'without', 'making', 'use', 'distance', 'measure', 'regarding', 'specific', 'case', 'pairs', 'names', 'would', 'benefit', 'ml', 'approach', 'using', 'levenshtein', 'distance', 'feature', 'detect', 'pairs', 'names', 'duplicate', 'low', 'levenshtein', 'distance', 'could', 'use', 'simple', 'algorithm', 'mark', 'pair', 'duplicate', 'levenshtein', 'distance', 'two', 'names', 'less', 'x', 'use', 'ml', 'instead', 'would', 'additional', 'benefit']","['compar', 'similar', 'two', 'name', 'identifi', 'duplic', 'neural', 'networkdataset', 'contain', 'pair', 'name', 'look', 'like', 'case', 'label', 'check', 'manual', 'decid', 'duplic', 'manual', 'judgement', 'case', 'would', 'th', 'case', 'specif', 'case', 'phonet', 'come', 'game', 'howev', 'main', 'problem', 'ok', 'ignor', 'phonet', 'first', 'approach', 'would', 'calcul', 'levenshtein', 'distanc', 'pair', 'mark', 'duplic', 'levenshtein', 'distanc', 'exampl', 'less', 'equal', 'would', 'lead', 'follow', 'output', 'would', 'approach', 'use', 'fix', 'algorithm', 'base', 'levinshtein', 'distanc', 'would', 'like', 'task', 'use', 'neural', 'network', 'machin', 'learn', 'need', 'neural', 'network', 'detect', 'semant', 'similar', 'like', 'hospit', 'clininc', 'howev', 'would', 'like', 'avoid', 'levenshtein', 'distanc', 'would', 'like', 'ml', 'algorithm', 'abl', 'detect', 'john', 'lemond', 'lee', 'peter', 'john', 'peter', 'potenti', 'duplic', 'also', 'certainti', 'levenshtein', 'distanc', 'would', 'lead', 'rel', 'high', 'number', 'case', 'quit', 'charact', 'ad', 'case', 'like', 'john', 'peter', 'johanna', 'petera', 'levenshtein', 'distanc', 'would', 'lead', 'smaller', 'number', 'howev', 'fact', 'duplic', 'case', 'would', 'hope', 'ml', 'algorithm', 'would', 'abl', 'detect', 'like', 'duplic', 'need', 'ml', 'algorithm', 'learn', 'way', 'need', 'duplic', 'check', 'label', 'would', 'give', 'input', 'would', 'give', 'ml', 'algorithm', 'direct', 'want', 'actual', 'thought', 'easi', 'task', 'ml', 'algorithm', 'neural', 'network', 'sure', 'implement', 'neural', 'network', 'compar', 'pair', 'name', 'identifi', 'duplic', 'without', 'use', 'explicit', 'distanc', 'metric', 'like', 'levenshtein', 'distanc', 'euclidean', 'etc', 'thought', 'would', 'possibl', 'convert', 'string', 'number', 'neural', 'network', 'work', 'learn', 'detect', 'duplic', 'accord', 'label', 'style', 'without', 'specifi', 'distanc', 'metric', 'thought', 'human', 'would', 'give', 'task', 'person', 'person', 'would', 'judg', 'make', 'decis', 'person', 'clue', 'levenshtein', 'distanc', 'mathemat', 'concept', 'want', 'train', 'neural', 'network', 'learn', 'human', 'cours', 'everi', 'human', 'differ', 'also', 'depend', 'label', 'edit', 'ml', 'neural', 'network', 'solut', 'seen', 'far', 'like', 'use', 'metric', 'like', 'levenshtein', 'featur', 'input', 'said', 'thought', 'possibl', 'teach', 'neural', 'network', 'human', 'judgement', 'without', 'make', 'use', 'distanc', 'measur', 'regard', 'specif', 'case', 'pair', 'name', 'would', 'benefit', 'ml', 'approach', 'use', 'levenshtein', 'distanc', 'featur', 'detect', 'pair', 'name', 'duplic', 'low', 'levenshtein', 'distanc', 'could', 'use', 'simpl', 'algorithm', 'mark', 'pair', 'duplic', 'levenshtein', 'distanc', 'two', 'name', 'less', 'x', 'use', 'ml', 'instead', 'would', 'addit', 'benefit']"
16,16,16,19407276,72981357,Python Machine Learning dataframe,"<p><code>y_true_test = pd.DataFrame([np.nan] * np.ones((para.n_stock,para.month_test[-1]))) y_pred_test = pd.DataFrame([np.nan] * np.ones((para.n_stock,para.month_test[-1]))) y_score_test = pd.DataFrame([np.nan] * np.ones((para.n_stock,para.month_test[-1]))) </code>[TypeError: 'DataFrame' object cannot be interpreted as an integer][1]</p>
<p><a href=""https://i.stack.imgur.com/bzWFB.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/bzWFB.png</a></p>
<p>how can i fix it thx!</p>
",35,0,-3,3,python;pandas;numpy,2022-07-14 18:59:38,2022-07-14 18:59:38,2022-07-14 19:00:44,y_true_test   pd dataframe  np nan    np ones  para n_stock para month_test       y_pred_test   pd dataframe  np nan    np ones  para n_stock para month_test       y_score_test   pd dataframe  np nan    np ones  para n_stock para month_test        typeerror   dataframe  object cannot be interpreted as an integer     how can i fix it thx ,python machine learning dataframe,y_true_test pd dataframe np nan np ones para n_stock para month_test y_pred_test pd dataframe np nan np ones para n_stock para month_test y_score_test pd dataframe np nan np ones para n_stock para month_test typeerror dataframe object cannot interpreted integer fix thx,python machine learning dataframe,python machine learning dataframey_true_test pd dataframe np nan np ones para n_stock para month_test y_pred_test pd dataframe np nan np ones para n_stock para month_test y_score_test pd dataframe np nan np ones para n_stock para month_test typeerror dataframe object cannot interpreted integer fix thx,"['python', 'machine', 'learning', 'dataframey_true_test', 'pd', 'dataframe', 'np', 'nan', 'np', 'ones', 'para', 'n_stock', 'para', 'month_test', 'y_pred_test', 'pd', 'dataframe', 'np', 'nan', 'np', 'ones', 'para', 'n_stock', 'para', 'month_test', 'y_score_test', 'pd', 'dataframe', 'np', 'nan', 'np', 'ones', 'para', 'n_stock', 'para', 'month_test', 'typeerror', 'dataframe', 'object', 'can', 'not', 'interpreted', 'integer', 'fix', 'thx']","['python', 'machin', 'learn', 'dataframey_true_test', 'pd', 'datafram', 'np', 'nan', 'np', 'one', 'para', 'n_stock', 'para', 'month_test', 'y_pred_test', 'pd', 'datafram', 'np', 'nan', 'np', 'one', 'para', 'n_stock', 'para', 'month_test', 'y_score_test', 'pd', 'datafram', 'np', 'nan', 'np', 'one', 'para', 'n_stock', 'para', 'month_test', 'typeerror', 'datafram', 'object', 'can', 'not', 'interpret', 'integ', 'fix', 'thx']"
17,17,17,17126954,72964814,How to calibrate probabilities in R?,"<p>I am trying to calibrate probabilities that I get with the predict function in the R package.
I have in my case two classes and mutiple predictors. I used the iris dataset as an example for you to try and help me out.</p>
<pre><code>my_data  &lt;- iris %&gt;% #reducing the data to have two classes only
  dplyr::filter((Species ==&quot;virginica&quot; | Species == &quot;versicolor&quot;) ) %&gt;% dplyr::select(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species)

my_data &lt;- droplevels(my_data)

index &lt;- createDataPartition(y=my_data$Species,p=0.6,list=FALSE) 
#creating train and test set for machine learning
Train &lt;- my_data[index,]
Test &lt;-  my_data[-index,]

#machine learning based on Train data partition with glmnet method
classCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10,repeats=5,classProbs =  TRUE,savePredictions = &quot;final&quot;)
set.seed(355)
glmnet_ML &lt;- train(Species~., Train, method= &quot;glmnet&quot;,  trControl=classCtrl)
glmnet_ML

#probabilities to assign each row of data to one class or the other on Test
predTestprob &lt;- predict(glmnet_ML,Test,type=&quot;prob&quot;)
pred 


#trying out calibration following &quot;Applied predictive modeling&quot; book from Max Kuhn p266-273
predTrainprob &lt;- predict(glmnet_ML,Train,type=&quot;prob&quot;)
predTest &lt;- predict(glmnet_ML,Test)
predTestprob &lt;- predict(glmnet_ML,test,type=&quot;prob&quot;)

Test$PredProb &lt;- predTestprob[,&quot;versicolor&quot;]
Test$Pred &lt;- predTest
Train$PredProb &lt;- predTrainprob[,&quot;versicolor&quot;]

#logistic regression to calibrate
sigmoidalCal &lt;- glm(relevel(Species, ref= &quot;virginica&quot;) ~ PredProb,data = Train,family = binomial)
coef(summary(sigmoidalCal))

#predicting calibrated scores
sigmoidProbs &lt;- predict(sigmoidalCal,newdata = Test[,&quot;PredProb&quot;, drop = FALSE],type = &quot;response&quot;)
Test$CalProb &lt;- sigmoidProbs

#plotting to see if it works
calCurve2 &lt;- calibration(Species ~ PredProb +  CalProb, data = Test)
xyplot(calCurve2,auto.key = list(columns = 2))
</code></pre>
<p>According to me, the result given by the plot is not good which indicates a mistake in the calibration, the Calprob curve should follow the diagonal but it doe not.</p>
<p>Has anyone done anything similar ?</p>
",23,0,0,4,r;probability;caret;calibration,2022-07-13 16:03:02,2022-07-13 16:03:02,2022-07-14 17:22:32,according to me  the result given by the plot is not good which indicates a mistake in the calibration  the calprob curve should follow the diagonal but it doe not  has anyone done anything similar  ,how to calibrate probabilities in r ,according result given plot good indicates mistake calibration calprob curve follow diagonal doe anyone done anything similar,calibrate probabilities r,calibrate probabilities raccording result given plot good indicates mistake calibration calprob curve follow diagonal doe anyone done anything similar,"['calibrate', 'probabilities', 'raccording', 'result', 'given', 'plot', 'good', 'indicates', 'mistake', 'calibration', 'calprob', 'curve', 'follow', 'diagonal', 'doe', 'anyone', 'done', 'anything', 'similar']","['calibr', 'probabl', 'raccord', 'result', 'given', 'plot', 'good', 'indic', 'mistak', 'calibr', 'calprob', 'curv', 'follow', 'diagon', 'doe', 'anyon', 'done', 'anyth', 'similar']"
18,19,19,7241279,60628482,How to fix the error &quot;Singleton array cannot be considered a valid collection&quot;,"<p>I'm starting my first machine learning code with python. But, I encountered an error while developing the confusion matrix for my multiclass model.</p>

<pre class=""lang-py prettyprint-override""><code>#Defining the model 

model = Sequential()

model.add(Dense(32,input_shape=(22,),activation='tanh'))
model.add(Dense(16,activation='tanh'))
model.add(Dense(6,activation='tanh'))
model.add(Dense(5,activation='softmax'))

model.compile(Adam(lr=0.004),'sparse_categorical_crossentropy',metrics=['accuracy'])

#fitting the model and predicting 

model.fit(X_train,Y_train,epochs=1)

Y_pred = model.predict(X_test)

Y_pred = Y_pred.astype(int)

Y_test_class = np.argmax(Y_test, axis=0)
Y_pred_class = np.argmax(Y_pred, axis=0)

#Accuracy of the predicted values

print(metrics.classification_report(Y_test_class,Y_pred_class))
print(metrics.confusion_matrix(Y_test_class,Y_pred_class))
</code></pre>

<p>I'm getting this error:</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Singleton array 3045 cannot be considered a valid collection.
</code></pre>

<p>Test data details
X_test[:5]</p>

<pre class=""lang-py prettyprint-override""><code>[['0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '1' '0' '0']
 ['1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '0' '0' '0']
 ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'
  '0' '1' '1' '0']]
</code></pre>

<p>Y_test[:5]</p>

<pre class=""lang-py prettyprint-override""><code>['1' '2' '2' '2' '2']
</code></pre>

<p>The shape of </p>

<p><code>Y_test_class</code> ==> <code>()</code><br>
<code>Y_pred_class</code> ==> <code>(5,)</code></p>
",2956,1,1,4,python;machine-learning;confusion-matrix;argmax,2020-03-11 08:04:41,2020-03-11 08:04:41,2022-07-14 13:36:42,i m starting my first machine learning code with python  but  i encountered an error while developing the confusion matrix for my multiclass model  i m getting this error  y_test    the shape of ,how to fix the error  singleton array cannot be considered a valid collection ,starting first machine learning code python encountered error developing confusion matrix multiclass model getting error y_test shape,fix error singleton array cannot considered valid collection,fix error singleton array cannot considered valid collectionstarting first machine learning code python encountered error developing confusion matrix multiclass model getting error y_test shape,"['fix', 'error', 'singleton', 'array', 'can', 'not', 'considered', 'valid', 'collectionstarting', 'first', 'machine', 'learning', 'code', 'python', 'encountered', 'error', 'developing', 'confusion', 'matrix', 'multiclass', 'model', 'getting', 'error', 'y_test', 'shape']","['fix', 'error', 'singleton', 'array', 'can', 'not', 'consid', 'valid', 'collectionstart', 'first', 'machin', 'learn', 'code', 'python', 'encount', 'error', 'develop', 'confus', 'matrix', 'multiclass', 'model', 'get', 'error', 'y_test', 'shape']"
19,20,20,19500560,72976376,How to train a spacy model by using streaming data?,"<p>I have created a spacy model. But I need to retrain it until it reaches it maximum level. I need to train this model and retrain the model using the streaming data. I have seen that we can train some machine learning model using stream data. Is it possible to do the same to NLP models?</p>
",14,1,-1,5,python;machine-learning;nlp;stream;spacy,2022-07-14 12:23:32,2022-07-14 12:23:32,2022-07-14 12:44:05,i have created a spacy model  but i need to retrain it until it reaches it maximum level  i need to train this model and retrain the model using the streaming data  i have seen that we can train some machine learning model using stream data  is it possible to do the same to nlp models ,how to train a spacy model by using streaming data ,created spacy model need retrain reaches maximum level need train model retrain model using streaming data seen train machine learning model using stream data possible nlp models,train spacy model using streaming data,train spacy model using streaming datacreated spacy model need retrain reaches maximum level need train model retrain model using streaming data seen train machine learning model using stream data possible nlp models,"['train', 'spacy', 'model', 'using', 'streaming', 'datacreated', 'spacy', 'model', 'need', 'retrain', 'reaches', 'maximum', 'level', 'need', 'train', 'model', 'retrain', 'model', 'using', 'streaming', 'data', 'seen', 'train', 'machine', 'learning', 'model', 'using', 'stream', 'data', 'possible', 'nlp', 'models']","['train', 'spaci', 'model', 'use', 'stream', 'datacr', 'spaci', 'model', 'need', 'retrain', 'reach', 'maximum', 'level', 'need', 'train', 'model', 'retrain', 'model', 'use', 'stream', 'data', 'seen', 'train', 'machin', 'learn', 'model', 'use', 'stream', 'data', 'possibl', 'nlp', 'model']"
20,22,22,8389618,60319321,Enhancing the resolution of an image,"<p>I have images that are having very low quality and these images I have to use for person identification but with this quality it's difficult to detect. I want to enhance the quality of the images using deep learning/machine learning techniques. I have studied about SRCNN, perceptual Loss, SRResNet, SRGAN but most of the super image resolution techniques require original images for improving the quality of the images. So my question is there any deep learning techniques that can be used for the improving the quality of the images without using the original images.</p>

<p><a href=""https://i.stack.imgur.com/Lwv4T.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lwv4T.jpg"" alt=""image with low quality""></a></p>
",148,2,2,5,image;machine-learning;image-processing;deep-learning;computer-vision,2020-02-20 17:22:30,2020-02-20 17:22:30,2022-07-14 11:16:37,i have images that are having very low quality and these images i have to use for person identification but with this quality it s difficult to detect  i want to enhance the quality of the images using deep learning machine learning techniques  i have studied about srcnn  perceptual loss  srresnet  srgan but most of the super image resolution techniques require original images for improving the quality of the images  so my question is there any deep learning techniques that can be used for the improving the quality of the images without using the original images  ,enhancing the resolution of an image,images low quality images use person identification quality difficult detect want enhance quality images using deep learning machine learning techniques studied srcnn perceptual loss srresnet srgan super image resolution techniques require original images improving quality images question deep learning techniques used improving quality images without using original images,enhancing resolution image,enhancing resolution imageimages low quality images use person identification quality difficult detect want enhance quality images using deep learning machine learning techniques studied srcnn perceptual loss srresnet srgan super image resolution techniques require original images improving quality images question deep learning techniques used improving quality images without using original images,"['enhancing', 'resolution', 'imageimages', 'low', 'quality', 'images', 'use', 'person', 'identification', 'quality', 'difficult', 'detect', 'want', 'enhance', 'quality', 'images', 'using', 'deep', 'learning', 'machine', 'learning', 'techniques', 'studied', 'srcnn', 'perceptual', 'loss', 'srresnet', 'srgan', 'super', 'image', 'resolution', 'techniques', 'require', 'original', 'images', 'improving', 'quality', 'images', 'question', 'deep', 'learning', 'techniques', 'used', 'improving', 'quality', 'images', 'without', 'using', 'original', 'images']","['enhanc', 'resolut', 'imageimag', 'low', 'qualiti', 'imag', 'use', 'person', 'identif', 'qualiti', 'difficult', 'detect', 'want', 'enhanc', 'qualiti', 'imag', 'use', 'deep', 'learn', 'machin', 'learn', 'techniqu', 'studi', 'srcnn', 'perceptu', 'loss', 'srresnet', 'srgan', 'super', 'imag', 'resolut', 'techniqu', 'requir', 'origin', 'imag', 'improv', 'qualiti', 'imag', 'question', 'deep', 'learn', 'techniqu', 'use', 'improv', 'qualiti', 'imag', 'without', 'use', 'origin', 'imag']"
21,23,23,19543504,72969730,Getting error while checking hosts IP via Ansible Server machine,"<p>I'm getting an error prompt while checking the node's IP in hosts/inventory from my server machines, all nodes are connected.</p>
<p>Error Prompt:</p>
<pre><code>[WARNING]: Unable to parse /etc/ansible/hosts #poll_interval  = 15 as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'
[WARNING]: Could not match supplied host pattern, ignoring: demo 
</code></pre>
<p>SSH connections are already established with my nodes and can access from the server machine as well</p>
<p>Also, in the <em>hosts</em> file inside the ansible server a group is created with the node's private IPs:</p>
<pre class=""lang-ini prettyprint-override""><code># Ex 1: Ungrouped hosts, specify before any group headers.

[demo]
172.31.31.15
172.31.31.128
</code></pre>
<p>Also, inside the ansible server config file:</p>
<pre><code>inventory = /etc/ansible/hosts
sudo_user = root
</code></pre>
<p>Both these are uncommented as well. I'm learning ansible and new with this configuration tool.</p>
<p>Please help as I'm unable to fetch details of nodes with the above-said command not by group which I created as <code>demo</code>.</p>
",34,0,0,1,ansible,2022-07-13 22:06:53,2022-07-13 22:06:53,2022-07-14 10:48:05,i m getting an error prompt while checking the node s ip in hosts inventory from my server machines  all nodes are connected  error prompt  ssh connections are already established with my nodes and can access from the server machine as well also  in the hosts file inside the ansible server a group is created with the node s private ips  also  inside the ansible server config file  both these are uncommented as well  i m learning ansible and new with this configuration tool  please help as i m unable to fetch details of nodes with the above said command not by group which i created as demo ,getting error while checking hosts ip via ansible server machine,getting error prompt checking node ip hosts inventory server machines nodes connected error prompt ssh connections already established nodes access server machine well also hosts file inside ansible server group created node private ips also inside ansible server config file uncommented well learning ansible configuration tool please help unable fetch details nodes said command group created demo,getting error checking hosts ip via ansible server machine,getting error checking hosts ip via ansible server machinegetting error prompt checking node ip hosts inventory server machines nodes connected error prompt ssh connections already established nodes access server machine well also hosts file inside ansible server group created node private ips also inside ansible server config file uncommented well learning ansible configuration tool please help unable fetch details nodes said command group created demo,"['getting', 'error', 'checking', 'hosts', 'ip', 'via', 'ansible', 'server', 'machinegetting', 'error', 'prompt', 'checking', 'node', 'ip', 'hosts', 'inventory', 'server', 'machines', 'nodes', 'connected', 'error', 'prompt', 'ssh', 'connections', 'already', 'established', 'nodes', 'access', 'server', 'machine', 'well', 'also', 'hosts', 'file', 'inside', 'ansible', 'server', 'group', 'created', 'node', 'private', 'ips', 'also', 'inside', 'ansible', 'server', 'config', 'file', 'uncommented', 'well', 'learning', 'ansible', 'configuration', 'tool', 'please', 'help', 'unable', 'fetch', 'details', 'nodes', 'said', 'command', 'group', 'created', 'demo']","['get', 'error', 'check', 'host', 'ip', 'via', 'ansibl', 'server', 'machineget', 'error', 'prompt', 'check', 'node', 'ip', 'host', 'inventori', 'server', 'machin', 'node', 'connect', 'error', 'prompt', 'ssh', 'connect', 'alreadi', 'establish', 'node', 'access', 'server', 'machin', 'well', 'also', 'host', 'file', 'insid', 'ansibl', 'server', 'group', 'creat', 'node', 'privat', 'ip', 'also', 'insid', 'ansibl', 'server', 'config', 'file', 'uncom', 'well', 'learn', 'ansibl', 'configur', 'tool', 'pleas', 'help', 'unabl', 'fetch', 'detail', 'node', 'said', 'command', 'group', 'creat', 'demo']"
22,25,25,0,72107817,I Cannot Install SQL Server 2019 Express. It gets stuck on Offline Installation of Microsoft Machine Learning Server Components,"<p>I'm trying to install SQL Server 2019 Express on my laptop. I initially click on custom to start and everything seems to go smooth up to the point where it's at the Offline Installation of Microsoft Machine Learning Server Components section.</p>
<p>When I get there I get a screenshot that looks like this:<br>
<p><img src=""https://bobgatto.com/images/sql_install.jpg""></p>
<p>From this point on I cannot figure out what to do next. I tried creating a directory, downloading all of the files listed to that directory, and enter that directory in the Install Path line. But when I do that the Next button still is disabled.</p>
<p>So what is the next step?</p>
<p>Thanks for any help</p> 
",3448,3,2,1,sql-server,2022-05-04 09:37:37,2022-05-04 09:37:37,2022-07-14 08:23:19,i m trying to install sql server  express on my laptop  i initially click on custom to start and everything seems to go smooth up to the point where it s at the offline installation of microsoft machine learning server components section   from this point on i cannot figure out what to do next  i tried creating a directory  downloading all of the files listed to that directory  and enter that directory in the install path line  but when i do that the next button still is disabled  so what is the next step  thanks for any help,i cannot install sql server  express  it gets stuck on offline installation of microsoft machine learning server components,trying install sql server express laptop initially click start everything seems go smooth point offline installation microsoft machine learning server components section point cannot figure next tried creating directory downloading files listed directory enter directory install path line next button still disabled next step thanks help,cannot install sql server express gets stuck offline installation microsoft machine learning server components,cannot install sql server express gets stuck offline installation microsoft machine learning server componentstrying install sql server express laptop initially click start everything seems go smooth point offline installation microsoft machine learning server components section point cannot figure next tried creating directory downloading files listed directory enter directory install path line next button still disabled next step thanks help,"['can', 'not', 'install', 'sql', 'server', 'express', 'gets', 'stuck', 'offline', 'installation', 'microsoft', 'machine', 'learning', 'server', 'componentstrying', 'install', 'sql', 'server', 'express', 'laptop', 'initially', 'click', 'start', 'everything', 'seems', 'go', 'smooth', 'point', 'offline', 'installation', 'microsoft', 'machine', 'learning', 'server', 'components', 'section', 'point', 'can', 'not', 'figure', 'next', 'tried', 'creating', 'directory', 'downloading', 'files', 'listed', 'directory', 'enter', 'directory', 'install', 'path', 'line', 'next', 'button', 'still', 'disabled', 'next', 'step', 'thanks', 'help']","['can', 'not', 'instal', 'sql', 'server', 'express', 'get', 'stuck', 'offlin', 'instal', 'microsoft', 'machin', 'learn', 'server', 'componentstri', 'instal', 'sql', 'server', 'express', 'laptop', 'initi', 'click', 'start', 'everyth', 'seem', 'go', 'smooth', 'point', 'offlin', 'instal', 'microsoft', 'machin', 'learn', 'server', 'compon', 'section', 'point', 'can', 'not', 'figur', 'next', 'tri', 'creat', 'directori', 'download', 'file', 'list', 'directori', 'enter', 'directori', 'instal', 'path', 'line', 'next', 'button', 'still', 'disabl', 'next', 'step', 'thank', 'help']"
23,26,26,6237651,55079999,Adding an image to a toolbar in a Vue + Vuetify Single File Compnonent,"<p>I'm essentially just remixing the code available here for a side project: <a href=""https://github.com/aws-samples/aws-ai-qna-bot.git"" rel=""nofollow noreferrer"">https://github.com/aws-samples/aws-ai-qna-bot.git</a></p>
<p><strong>Problem</strong>: I'm trying to insert a centered logo in the toolbar between the app drawer and the logout button.  Typically I could accomplish this pretty easily with vanilla HTML and CSS, but this project is leveraging Vue.js and Vuetify, which I'm doing my best to get myself up to speed with.</p>
<p>I've referenced the following documents, including the <code>README.md</code> in the git repo:</p>
<p><a href=""https://vuetifyjs.com/en/components/images"" rel=""nofollow noreferrer"">https://vuetifyjs.com/en/components/images</a></p>
<p><a href=""https://v2.vuejs.org/v2/guide/single-file-components.html"" rel=""nofollow noreferrer"">https://v2.vuejs.org/v2/guide/single-file-components.html</a></p>
<p><strong>File path</strong>: <code>qna-bot-template/website/js/admin.vue</code></p>
<pre><code>&lt;template lang=&quot;pug&quot;&gt;
  v-app
    v-navigation-drawer(temporary v-model=&quot;drawer&quot; app)
      v-toolbar(flat)
        v-list
          v-list-tile
            v-list-tile-title.title Tools
      v-divider
      v-list(dense three-line subheader)
        v-list-tile(v-for=&quot;(page,key) in pages&quot; :key=&quot;key&quot;
          @click=&quot;drawer=false&quot; 
          :href=&quot;page.href&quot;
          :id=&quot;'page-link-'+page.id&quot;
          :target=&quot;page.target || '_self'&quot;) 
          v-list-tile-avatar
            v-icon(color=&quot;primary&quot;) {{page.icon}}
          v-list-tile-content
            v-list-tile-title {{page.title}}
            v-list-tile-sub-title {{page.subTitle}}
        v-list-group( prepend-icon=&quot;info&quot; value=&quot;true&quot; color=&quot;primary&quot;)
          v-list-tile(slot=&quot;activator&quot;) 
            v-list-tile-title QnABot Help
          v-list-tile
            v-list-tile-content 
              v-list-tile-title Version: {{Version}}
              v-list-tile-title BuildDate: {{BuildDate}}
          v-list-tile
            v-list-tile-content
              v-list-tile-title 
                a(href=&quot;https://amazon.com/qnabot&quot; target=&quot;_blank&quot;) General Instructions / QnABot Blog Post
              v-list-tile-title
                a(href=&quot;https://aws.amazon.com/blogs/machine-learning/creating-virtual-guided-navigation-using-a-question-and-answer-bot-with-amazon-lex-and-amazon-alexa/&quot; target=&quot;_blank&quot;) Guided Navigation using QnABot
              v-list-tile-title
                a(href=&quot;https://aws.amazon.com/blogs/machine-learning/create-a-questionnaire-bot-with-amazon-lex-and-amazon-alexa/&quot; target=&quot;_blank&quot;) Create a questionnaire using QnABot
    v-toolbar(app fixed)
      v-toolbar-side-icon.primary--text(
        id=&quot;nav-open&quot;
        @click.stop=&quot;drawer = !drawer&quot;
      )
      v-toolbar-title 
        v-breadcrumbs
          v-breadcrumbs-item(href='#/edit') {{$store.state.info.StackName}}:{{$store.state.user.name}}
          v-breadcrumbs-item {{page}}
      v-spacer
      v-toolbar-items
        v-btn.primary--text(flat
          id=&quot;logout-button&quot;
          @click=&quot;logout&quot;
          v-if=&quot;login&quot;) LogOut
    v-container(fluid id=&quot;workspace&quot;)
      v-layout(column)
        v-flex
          router-view
    v-footer
&lt;/template&gt;
</code></pre>
<p>So far I've tried following the following syntax, which I added right after the <code>v-spacer</code> toward the bottom of the wrapping template tags.</p>
<pre><code>v-container
  v-img(:src=&quot;/abc/xyz&quot;)
</code></pre>
<p>and this doesn't seem to be working.</p>
<p>Lastly I'll add that since this environment is deployed to an EC2 instance (don't think you can deploy it locally to prototype via <code>vue serve</code> or at least I haven't been able to), I'm having to do this very roundabout way of prototyping by deploying this S3 bucket where the webpages are built to, then I <code>make</code> this webpack listener which will see whenever I modify a file.  Then I can refresh the index.html that is built in the S3 bucket to see my changes.  Extremely clunky workflow, I know, but I've never worked in an environment like this so I'm not sure if there's a better way, plus the readme provided in the github repo is very light on details for how to modify the default layout.</p>
<p>Any help/pointers would be greatly appreciated.</p>
",5757,1,0,3,amazon-web-services;vue.js;vuetify.js,2019-03-09 22:52:23,2019-03-09 22:52:23,2022-07-14 07:14:12,i m essentially just remixing the code available here for a side project   problem  i m trying to insert a centered logo in the toolbar between the app drawer and the logout button   typically i could accomplish this pretty easily with vanilla html and css  but this project is leveraging vue js and vuetify  which i m doing my best to get myself up to speed with  i ve referenced the following documents  including the readme md in the git repo    file path  qna bot template website js admin vue so far i ve tried following the following syntax  which i added right after the v spacer toward the bottom of the wrapping template tags  and this doesn t seem to be working  lastly i ll add that since this environment is deployed to an ec instance  don t think you can deploy it locally to prototype via vue serve or at least i haven t been able to   i m having to do this very roundabout way of prototyping by deploying this s bucket where the webpages are built to  then i make this webpack listener which will see whenever i modify a file   then i can refresh the index html that is built in the s bucket to see my changes   extremely clunky workflow  i know  but i ve never worked in an environment like this so i m not sure if there s a better way  plus the readme provided in the github repo is very light on details for how to modify the default layout  any help pointers would be greatly appreciated ,adding an image to a toolbar in a vue   vuetify single file compnonent,essentially remixing code available side project problem trying insert centered logo toolbar app drawer logout button typically could accomplish pretty easily vanilla html css project leveraging vue js vuetify best get speed referenced following documents including readme md git repo file path qna bot template website js admin vue far tried following following syntax added right v spacer toward bottom wrapping template tags seem working lastly since environment deployed ec instance think deploy locally prototype via vue serve least able roundabout way prototyping deploying bucket webpages built make webpack listener see whenever modify file refresh index html built bucket see changes extremely clunky workflow know never worked environment like sure better way plus readme provided github repo light details modify default layout help pointers would greatly appreciated,adding image toolbar vue vuetify single file compnonent,adding image toolbar vue vuetify single file compnonentessentially remixing code available side project problem trying insert centered logo toolbar app drawer logout button typically could accomplish pretty easily vanilla html css project leveraging vue js vuetify best get speed referenced following documents including readme md git repo file path qna bot template website js admin vue far tried following following syntax added right v spacer toward bottom wrapping template tags seem working lastly since environment deployed ec instance think deploy locally prototype via vue serve least able roundabout way prototyping deploying bucket webpages built make webpack listener see whenever modify file refresh index html built bucket see changes extremely clunky workflow know never worked environment like sure better way plus readme provided github repo light details modify default layout help pointers would greatly appreciated,"['adding', 'image', 'toolbar', 'vue', 'vuetify', 'single', 'file', 'compnonentessentially', 'remixing', 'code', 'available', 'side', 'project', 'problem', 'trying', 'insert', 'centered', 'logo', 'toolbar', 'app', 'drawer', 'logout', 'button', 'typically', 'could', 'accomplish', 'pretty', 'easily', 'vanilla', 'html', 'css', 'project', 'leveraging', 'vue', 'js', 'vuetify', 'best', 'get', 'speed', 'referenced', 'following', 'documents', 'including', 'readme', 'md', 'git', 'repo', 'file', 'path', 'qna', 'bot', 'template', 'website', 'js', 'admin', 'vue', 'far', 'tried', 'following', 'following', 'syntax', 'added', 'right', 'v', 'spacer', 'toward', 'bottom', 'wrapping', 'template', 'tags', 'seem', 'working', 'lastly', 'since', 'environment', 'deployed', 'ec', 'instance', 'think', 'deploy', 'locally', 'prototype', 'via', 'vue', 'serve', 'least', 'able', 'roundabout', 'way', 'prototyping', 'deploying', 'bucket', 'webpages', 'built', 'make', 'webpack', 'listener', 'see', 'whenever', 'modify', 'file', 'refresh', 'index', 'html', 'built', 'bucket', 'see', 'changes', 'extremely', 'clunky', 'workflow', 'know', 'never', 'worked', 'environment', 'like', 'sure', 'better', 'way', 'plus', 'readme', 'provided', 'github', 'repo', 'light', 'details', 'modify', 'default', 'layout', 'help', 'pointers', 'would', 'greatly', 'appreciated']","['ad', 'imag', 'toolbar', 'vue', 'vuetifi', 'singl', 'file', 'compnonentessenti', 'remix', 'code', 'avail', 'side', 'project', 'problem', 'tri', 'insert', 'center', 'logo', 'toolbar', 'app', 'drawer', 'logout', 'button', 'typic', 'could', 'accomplish', 'pretti', 'easili', 'vanilla', 'html', 'css', 'project', 'leverag', 'vue', 'js', 'vuetifi', 'best', 'get', 'speed', 'referenc', 'follow', 'document', 'includ', 'readm', 'md', 'git', 'repo', 'file', 'path', 'qna', 'bot', 'templat', 'websit', 'js', 'admin', 'vue', 'far', 'tri', 'follow', 'follow', 'syntax', 'ad', 'right', 'v', 'spacer', 'toward', 'bottom', 'wrap', 'templat', 'tag', 'seem', 'work', 'lastli', 'sinc', 'environ', 'deploy', 'ec', 'instanc', 'think', 'deploy', 'local', 'prototyp', 'via', 'vue', 'serv', 'least', 'abl', 'roundabout', 'way', 'prototyp', 'deploy', 'bucket', 'webpag', 'built', 'make', 'webpack', 'listen', 'see', 'whenev', 'modifi', 'file', 'refresh', 'index', 'html', 'built', 'bucket', 'see', 'chang', 'extrem', 'clunki', 'workflow', 'know', 'never', 'work', 'environ', 'like', 'sure', 'better', 'way', 'plu', 'readm', 'provid', 'github', 'repo', 'light', 'detail', 'modifi', 'default', 'layout', 'help', 'pointer', 'would', 'greatli', 'appreci']"
24,27,27,19520507,72973487,Which Random Forest for stock market analysis?,"<p>Wanting to use machine learning for stock market analysis as a summer project (new to compter sci). Want to use random forest but am unsure if i need randomfroestclassifier or randomforestregression and am also unsure about the difference between the two. I want to also find out advantaged of this over LSTM models. THANKS</p>
",12,0,2,1,lstm,2022-07-14 04:24:13,2022-07-14 04:24:13,2022-07-14 04:24:13,wanting to use machine learning for stock market analysis as a summer project  new to compter sci   want to use random forest but am unsure if i need randomfroestclassifier or randomforestregression and am also unsure about the difference between the two  i want to also find out advantaged of this over lstm models  thanks,which random forest for stock market analysis ,wanting use machine learning stock market analysis summer project compter sci want use random forest unsure need randomfroestclassifier randomforestregression also unsure difference two want also find advantaged lstm models thanks,random forest stock market analysis,random forest stock market analysiswanting use machine learning stock market analysis summer project compter sci want use random forest unsure need randomfroestclassifier randomforestregression also unsure difference two want also find advantaged lstm models thanks,"['random', 'forest', 'stock', 'market', 'analysiswanting', 'use', 'machine', 'learning', 'stock', 'market', 'analysis', 'summer', 'project', 'compter', 'sci', 'want', 'use', 'random', 'forest', 'unsure', 'need', 'randomfroestclassifier', 'randomforestregression', 'also', 'unsure', 'difference', 'two', 'want', 'also', 'find', 'advantaged', 'lstm', 'models', 'thanks']","['random', 'forest', 'stock', 'market', 'analysisw', 'use', 'machin', 'learn', 'stock', 'market', 'analysi', 'summer', 'project', 'compter', 'sci', 'want', 'use', 'random', 'forest', 'unsur', 'need', 'randomfroestclassifi', 'randomforestregress', 'also', 'unsur', 'differ', 'two', 'want', 'also', 'find', 'advantag', 'lstm', 'model', 'thank']"
25,28,28,14748027,72971178,How to specify columns in data for Machine Learning?,"<p>I am learning some Machine Learning stuff? and need some help on data preparation.
The data gives details about properties in an area.</p>
<p><a href=""https://i.stack.imgur.com/M4Csr.png"" rel=""nofollow noreferrer"">Property Data</a><br />
<a href=""https://i.stack.imgur.com/WVfWM.png"" rel=""nofollow noreferrer"">Data info</a></p>
<p>So, during loading time I discovered that one column named &quot;BHK&quot; (which specifies for no. of bedrooms, hall, kitchen in the property) is categorical but values are numeric separated by ','.</p>
<p><a href=""https://i.stack.imgur.com/wthVp.png"" rel=""nofollow noreferrer"">BHK column categories</a><br />
For example: for the 1st row, the BHK column specifies (5,1,2) meaning 5 bedrooms, 1 hall, 2 kitchens</p>
<p>So, how should I format it so, that it becomes easy for further modelling and prediction stages?
should I use separated values from BHK column?</p>
",14,0,-1,4,pandas;csv;machine-learning;data-modeling,2022-07-14 00:14:32,2022-07-14 00:14:32,2022-07-14 00:14:32,so  during loading time i discovered that one column named  bhk   which specifies for no  of bedrooms  hall  kitchen in the property  is categorical but values are numeric separated by     ,how to specify columns in data for machine learning ,loading time discovered one column named bhk specifies bedrooms hall kitchen property categorical values numeric separated,specify columns data machine learning,specify columns data machine learningloading time discovered one column named bhk specifies bedrooms hall kitchen property categorical values numeric separated,"['specify', 'columns', 'data', 'machine', 'learningloading', 'time', 'discovered', 'one', 'column', 'named', 'bhk', 'specifies', 'bedrooms', 'hall', 'kitchen', 'property', 'categorical', 'values', 'numeric', 'separated']","['specifi', 'column', 'data', 'machin', 'learningload', 'time', 'discov', 'one', 'column', 'name', 'bhk', 'specifi', 'bedroom', 'hall', 'kitchen', 'properti', 'categor', 'valu', 'numer', 'separ']"
26,29,29,308827,72970228,Ensemble of machine learning models in scikit-learn,"<pre><code>group        feature_1        feature_2       year            dependent_variable
group_a         12               19           2010               0.4
group_a         11               13           2011               0.9
group_a         10               5            2012               1.2
group_a         16               9            2013               3.2
group_b         8               29            2010               0.6
group_b         9               33            2011               0.1 
group_b         111             15            2012               2.1 
group_b         16              19            2013               12.2  
</code></pre>
<p>In the dataframe above, I want to use <code>feature_1</code>, <code>feature_2</code> to predict <code>dependent_variable</code>. To do this, I want to construct two models: In the first model, I want to construct a separate model for each group. In the second model, I want to use all the available data. In both cases, data from the years 2010 to 2012 will be used for training and 2013 will be used for testing.</p>
<p>How can I construct an ensemble model using the two models outlined above? The data is a toy dataset but in the real dataset, there will be a lot more groups, years and features. In particular, I am interested in an approach that will work with scikit-learn compatible models.</p>
",13,0,0,3,python;scikit-learn;ensemble-learning,2022-07-13 22:47:38,2022-07-13 22:47:38,2022-07-13 22:47:38,in the dataframe above  i want to use feature_  feature_ to predict dependent_variable  to do this  i want to construct two models  in the first model  i want to construct a separate model for each group  in the second model  i want to use all the available data  in both cases  data from the years  to  will be used for training and  will be used for testing  how can i construct an ensemble model using the two models outlined above  the data is a toy dataset but in the real dataset  there will be a lot more groups  years and features  in particular  i am interested in an approach that will work with scikit learn compatible models ,ensemble of machine learning models in scikit learn,dataframe want use feature_ feature_ predict dependent_variable want construct two models first model want construct separate model group second model want use available data cases data years used training used testing construct ensemble model using two models outlined data toy dataset real dataset lot groups years features particular interested approach work scikit learn compatible models,ensemble machine learning models scikit learn,ensemble machine learning models scikit learndataframe want use feature_ feature_ predict dependent_variable want construct two models first model want construct separate model group second model want use available data cases data years used training used testing construct ensemble model using two models outlined data toy dataset real dataset lot groups years features particular interested approach work scikit learn compatible models,"['ensemble', 'machine', 'learning', 'models', 'scikit', 'learndataframe', 'want', 'use', 'feature_', 'feature_', 'predict', 'dependent_variable', 'want', 'construct', 'two', 'models', 'first', 'model', 'want', 'construct', 'separate', 'model', 'group', 'second', 'model', 'want', 'use', 'available', 'data', 'cases', 'data', 'years', 'used', 'training', 'used', 'testing', 'construct', 'ensemble', 'model', 'using', 'two', 'models', 'outlined', 'data', 'toy', 'dataset', 'real', 'dataset', 'lot', 'groups', 'years', 'features', 'particular', 'interested', 'approach', 'work', 'scikit', 'learn', 'compatible', 'models']","['ensembl', 'machin', 'learn', 'model', 'scikit', 'learndatafram', 'want', 'use', 'feature_', 'feature_', 'predict', 'dependent_vari', 'want', 'construct', 'two', 'model', 'first', 'model', 'want', 'construct', 'separ', 'model', 'group', 'second', 'model', 'want', 'use', 'avail', 'data', 'case', 'data', 'year', 'use', 'train', 'use', 'test', 'construct', 'ensembl', 'model', 'use', 'two', 'model', 'outlin', 'data', 'toy', 'dataset', 'real', 'dataset', 'lot', 'group', 'year', 'featur', 'particular', 'interest', 'approach', 'work', 'scikit', 'learn', 'compat', 'model']"
27,30,30,19295532,72969484,Non-Linear Machine Learning Algorithms,"<p>I am working on a dataset using python I have 17 variables that need to be used to predict one thing (Thing being a %) is there any non-linear machine learning algorithms anyone knows that could achieve this. I have. already implemented a multiple linear regression to it but the data is not very linear so I would want a better fit. All the other algorithms I've tried such as polynomial regression need the same values X=Y but in this case that is not possible as I have 17 variables all needed to predict one thing.</p>
",26,1,-1,3,python;regression;non-linear,2022-07-13 21:46:37,2022-07-13 21:46:37,2022-07-13 22:05:30,i am working on a dataset using python i have  variables that need to be used to predict one thing  thing being a    is there any non linear machine learning algorithms anyone knows that could achieve this  i have  already implemented a multiple linear regression to it but the data is not very linear so i would want a better fit  all the other algorithms i ve tried such as polynomial regression need the same values x y but in this case that is not possible as i have  variables all needed to predict one thing ,non linear machine learning algorithms,working dataset using python variables need used predict one thing thing non linear machine learning algorithms anyone knows could achieve already implemented multiple linear regression data linear would want better fit algorithms tried polynomial regression need values x case possible variables needed predict one thing,non linear machine learning algorithms,non linear machine learning algorithmsworking dataset using python variables need used predict one thing thing non linear machine learning algorithms anyone knows could achieve already implemented multiple linear regression data linear would want better fit algorithms tried polynomial regression need values x case possible variables needed predict one thing,"['non', 'linear', 'machine', 'learning', 'algorithmsworking', 'dataset', 'using', 'python', 'variables', 'need', 'used', 'predict', 'one', 'thing', 'thing', 'non', 'linear', 'machine', 'learning', 'algorithms', 'anyone', 'knows', 'could', 'achieve', 'already', 'implemented', 'multiple', 'linear', 'regression', 'data', 'linear', 'would', 'want', 'better', 'fit', 'algorithms', 'tried', 'polynomial', 'regression', 'need', 'values', 'x', 'case', 'possible', 'variables', 'needed', 'predict', 'one', 'thing']","['non', 'linear', 'machin', 'learn', 'algorithmswork', 'dataset', 'use', 'python', 'variabl', 'need', 'use', 'predict', 'one', 'thing', 'thing', 'non', 'linear', 'machin', 'learn', 'algorithm', 'anyon', 'know', 'could', 'achiev', 'alreadi', 'implement', 'multipl', 'linear', 'regress', 'data', 'linear', 'would', 'want', 'better', 'fit', 'algorithm', 'tri', 'polynomi', 'regress', 'need', 'valu', 'x', 'case', 'possibl', 'variabl', 'need', 'predict', 'one', 'thing']"
28,31,31,8993562,47445310,Python : (Titanic) debugging error in imputing fare,"<p>I will thank very much for your help.I have a problem with the classic Kaggle Titanic Tutorial for machine learning. My problem is when using a pivotal table to imput means in a dataframe (or tupple):</p>

<pre><code>fare_means = df.pivot_table(""Fare"", index= ""Pclass"", aggfunc=""mean"")

​fare_means.info()    
fare_class 'pandas.core.frame.DataFrame'&gt;    
Int64   
Index: 3 entries, 1 to 3    
Data columns (total 1 columns):  Fare    3 non-null float64  
dtypes: float64(1)   
memory usage: 48.0 bytes

fare_means   
Out[46]:   
Fare ---Pclass     
1     ------- 84.154687  
2      -------20.662183   
3      -------13.675550

df_test['Fare'] = df_test[['Fare', 'Pclass']].apply(lambda x:

fare_means[x['Pclass']] if pd.isnull(x['Fare'])

else x['Fare'], axis=1)
</code></pre>

<h1>KeyError: (3, u'occurred at index 152')</h1>

<pre><code>df_test.iloc[150:155, 0: ]   
  Index-- Pclass    ----    Fare       
  150 ------ 1      --------83.1583   
  151 ------ 3      --------7.8958    
  152 ------ 3      --------NaN    
  153 ------ 3      --------12.1833  
  154 ------ 3      --------31.3875 
</code></pre>
",146,1,0,3,python;python-2.7;debugging,2017-11-23 04:48:24,2017-11-23 04:48:24,2022-07-13 21:10:50,i will thank very much for your help i have a problem with the classic kaggle titanic tutorial for machine learning  my problem is when using a pivotal table to imput means in a dataframe  or tupple  ,python    titanic  debugging error in imputing fare,thank much help problem classic kaggle titanic tutorial machine learning problem using pivotal table imput means dataframe tupple,python titanic debugging error imputing fare,python titanic debugging error imputing farethank much help problem classic kaggle titanic tutorial machine learning problem using pivotal table imput means dataframe tupple,"['python', 'titanic', 'debugging', 'error', 'imputing', 'farethank', 'much', 'help', 'problem', 'classic', 'kaggle', 'titanic', 'tutorial', 'machine', 'learning', 'problem', 'using', 'pivotal', 'table', 'imput', 'means', 'dataframe', 'tupple']","['python', 'titan', 'debug', 'error', 'imput', 'farethank', 'much', 'help', 'problem', 'classic', 'kaggl', 'titan', 'tutori', 'machin', 'learn', 'problem', 'use', 'pivot', 'tabl', 'imput', 'mean', 'datafram', 'tuppl']"
29,32,32,18355714,72955247,GPU question about multithreading and machine learning,"<p>I have a machine learning model that uses nearly 3GB of my GPU. I would like to multithread this. In my head, this is how it should happen:</p>
<p>For each thread, I create I would need to generate a duplicate neural network to put into the GPU. As having all threads using the same model could be dangerous? and I don't want to wrap the model in a critical lock section.</p>
<p>Therefore to have 2 threads running I would need 6GB of GPU space?</p>
",23,0,-1,4,multithreading;machine-learning;computer-vision;gpu,2022-07-12 21:34:15,2022-07-12 21:34:15,2022-07-13 14:07:34,i have a machine learning model that uses nearly gb of my gpu  i would like to multithread this  in my head  this is how it should happen  for each thread  i create i would need to generate a duplicate neural network to put into the gpu  as having all threads using the same model could be dangerous  and i don t want to wrap the model in a critical lock section  therefore to have  threads running i would need gb of gpu space ,gpu question about multithreading and machine learning,machine learning model uses nearly gb gpu would like multithread head happen thread create would need generate duplicate neural network put gpu threads using model could dangerous want wrap model critical lock section therefore threads running would need gb gpu space,gpu question multithreading machine learning,gpu question multithreading machine learningmachine learning model uses nearly gb gpu would like multithread head happen thread create would need generate duplicate neural network put gpu threads using model could dangerous want wrap model critical lock section therefore threads running would need gb gpu space,"['gpu', 'question', 'multithreading', 'machine', 'learningmachine', 'learning', 'model', 'uses', 'nearly', 'gb', 'gpu', 'would', 'like', 'multithread', 'head', 'happen', 'thread', 'create', 'would', 'need', 'generate', 'duplicate', 'neural', 'network', 'put', 'gpu', 'threads', 'using', 'model', 'could', 'dangerous', 'want', 'wrap', 'model', 'critical', 'lock', 'section', 'therefore', 'threads', 'running', 'would', 'need', 'gb', 'gpu', 'space']","['gpu', 'question', 'multithread', 'machin', 'learningmachin', 'learn', 'model', 'use', 'nearli', 'gb', 'gpu', 'would', 'like', 'multithread', 'head', 'happen', 'thread', 'creat', 'would', 'need', 'gener', 'duplic', 'neural', 'network', 'put', 'gpu', 'thread', 'use', 'model', 'could', 'danger', 'want', 'wrap', 'model', 'critic', 'lock', 'section', 'therefor', 'thread', 'run', 'would', 'need', 'gb', 'gpu', 'space']"
30,34,34,19539060,72961125,Best way to build an &quot;engine&quot; or &quot;solver&quot; for a card game,"<p>I am interested in building a card game(cabo) and a solver or engine to try and learn the most optimal way to play it as a project for my resume. I'd say that I am pretty fluent in C++ and am open to learning python/JS if it will make things easier(a little background if it helps: I just finished intro to CS 2 at my college and will be taking DSA in the fall). From what I have read, a machine learning/neural network model is what most chess engines and poker solvers use, which is what gave me inspiration for this project. I just wanted to get some input on what would be the best way to approach this. Thanks.</p>
",15,0,-4,3,machine-learning;neural-network;solver,2022-07-13 10:25:52,2022-07-13 10:25:52,2022-07-13 10:25:52,i am interested in building a card game cabo  and a solver or engine to try and learn the most optimal way to play it as a project for my resume  i d say that i am pretty fluent in c   and am open to learning python js if it will make things easier a little background if it helps  i just finished intro to cs  at my college and will be taking dsa in the fall   from what i have read  a machine learning neural network model is what most chess engines and poker solvers use  which is what gave me inspiration for this project  i just wanted to get some input on what would be the best way to approach this  thanks ,best way to build an  engine  or  solver  for a card game,interested building card game cabo solver engine try learn optimal way play project resume say pretty fluent c open learning python js make things easier little background helps finished intro cs college taking dsa fall read machine learning neural network model chess engines poker solvers use gave inspiration project wanted get input would best way approach thanks,best way build engine solver card game,best way build engine solver card gameinterested building card game cabo solver engine try learn optimal way play project resume say pretty fluent c open learning python js make things easier little background helps finished intro cs college taking dsa fall read machine learning neural network model chess engines poker solvers use gave inspiration project wanted get input would best way approach thanks,"['best', 'way', 'build', 'engine', 'solver', 'card', 'gameinterested', 'building', 'card', 'game', 'cabo', 'solver', 'engine', 'try', 'learn', 'optimal', 'way', 'play', 'project', 'resume', 'say', 'pretty', 'fluent', 'c', 'open', 'learning', 'python', 'js', 'make', 'things', 'easier', 'little', 'background', 'helps', 'finished', 'intro', 'cs', 'college', 'taking', 'dsa', 'fall', 'read', 'machine', 'learning', 'neural', 'network', 'model', 'chess', 'engines', 'poker', 'solvers', 'use', 'gave', 'inspiration', 'project', 'wanted', 'get', 'input', 'would', 'best', 'way', 'approach', 'thanks']","['best', 'way', 'build', 'engin', 'solver', 'card', 'gameinterest', 'build', 'card', 'game', 'cabo', 'solver', 'engin', 'tri', 'learn', 'optim', 'way', 'play', 'project', 'resum', 'say', 'pretti', 'fluent', 'c', 'open', 'learn', 'python', 'js', 'make', 'thing', 'easier', 'littl', 'background', 'help', 'finish', 'intro', 'cs', 'colleg', 'take', 'dsa', 'fall', 'read', 'machin', 'learn', 'neural', 'network', 'model', 'chess', 'engin', 'poker', 'solver', 'use', 'gave', 'inspir', 'project', 'want', 'get', 'input', 'would', 'best', 'way', 'approach', 'thank']"
31,35,35,19538261,72960178,how to insert or update a new data into existing table using mysql in python flask (data from machine learning),"<p><strong>My problem when i using query insert or update the result of value from naive bayes algorithm into my existing table and column</strong></p>
<p>example of problem 1 that occurs when I use the query insert into my table data_uji3 at column prob which is the column that is used as the result value from the naive bayes algorithm, the data stored in the data_uji3 table in the &quot;prob&quot; column does not start from the top or the first row, but it start from the bottom row and makes as many rows as the amount of data available.
i have tried to insert it to another table for the result of algorithm and it works, but i need this value from algorithm must be place at table data_uji3 at column prob, the code for insert describe at below</p>
<pre><code>akurasi=pd.read_sql_query('SELECT*FROM data_uji3',cur )
        x1 = akurasi.iloc[:,[4,5,6,7,8]].values
        y_pred1=NBmodel.predict(x1)
        print(y_pred1)
        pt= y_pred1.reshape(250,1)
        if request.method == &quot;POST&quot;:
            
            cur1 =mysql.connection.cursor()
            for i in pt:
             cur1.execute(&quot;INSERT INTO data_uji3 (prob) VALUES (%s)&quot;,i)
             print(i)       
             mysql.connection.commit()
</code></pre>
<p>and the result at my table at this image
<a href=""https://i.stack.imgur.com/pFJjn.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/pFJjn.png</a></p>
<p>example of next problem is when i used the update query for that case, the data updated from result of algorithm just write by one, the code for update describe at below</p>
<pre><code>akurasi=pd.read_sql_query('SELECT*FROM data_uji3',cur )
        x1 = akurasi.iloc[:,[4,5,6,7,8]].values
        y_pred1=NBmodel.predict(x1)
        print(y_pred1)
        pt= y_pred1.reshape(250,1)
        if request.method == &quot;POST&quot;:
            
            cur1 =mysql.connection.cursor()
            for i in pt:
             cur1.execute(&quot;UPDATE data_uji3 SET prob=%s&quot;,i)
             print(i)       
             mysql.connection.commit()
</code></pre>
<p>and the result at my table at this image
<a href=""https://i.stack.imgur.com/tqa2I.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/tqa2I.png</a></p>
<p>on those my case, is there any unused code in my program or is there any solution for my case, please some one to help me</p>
",20,0,-1,3,python;mysql;pandas,2022-07-13 07:43:33,2022-07-13 07:43:33,2022-07-13 08:42:59,my problem when i using query insert or update the result of value from naive bayes algorithm into my existing table and column example of next problem is when i used the update query for that case  the data updated from result of algorithm just write by one  the code for update describe at below on those my case  is there any unused code in my program or is there any solution for my case  please some one to help me,how to insert or update a new data into existing table using mysql in python flask  data from machine learning ,problem using query insert update result value naive bayes algorithm existing table column example next problem used update query case data updated result algorithm write one code update describe case unused code program solution case please one help,insert update data existing table using mysql python flask data machine learning,insert update data existing table using mysql python flask data machine learningproblem using query insert update result value naive bayes algorithm existing table column example next problem used update query case data updated result algorithm write one code update describe case unused code program solution case please one help,"['insert', 'update', 'data', 'existing', 'table', 'using', 'mysql', 'python', 'flask', 'data', 'machine', 'learningproblem', 'using', 'query', 'insert', 'update', 'result', 'value', 'naive', 'bayes', 'algorithm', 'existing', 'table', 'column', 'example', 'next', 'problem', 'used', 'update', 'query', 'case', 'data', 'updated', 'result', 'algorithm', 'write', 'one', 'code', 'update', 'describe', 'case', 'unused', 'code', 'program', 'solution', 'case', 'please', 'one', 'help']","['insert', 'updat', 'data', 'exist', 'tabl', 'use', 'mysql', 'python', 'flask', 'data', 'machin', 'learningproblem', 'use', 'queri', 'insert', 'updat', 'result', 'valu', 'naiv', 'bay', 'algorithm', 'exist', 'tabl', 'column', 'exampl', 'next', 'problem', 'use', 'updat', 'queri', 'case', 'data', 'updat', 'result', 'algorithm', 'write', 'one', 'code', 'updat', 'describ', 'case', 'unus', 'code', 'program', 'solut', 'case', 'pleas', 'one', 'help']"
32,36,36,17752727,72960044,How are decision trees in random forest algorithms made?,"<p>I am new to machine learning! While doing an assignment I was suddenly presented with the profound question of how does a computer generate a decision tree.</p>
<p>For example, consider the problem of predicting a specific flower species using the random forest algorithm. There are two specific attributes of a flower (petal width, petal length) that distinguish the species.</p>
<p>In terms of the flower problem, when using sclearn's Python decisiontreemaker(), how does the computer figure out what petal width and petal length to create the tree? Also, does a random forest algorithm create these decision trees by brute forcing and testing every single variation of a tree by weighing entrophy? Thanks!</p>
",22,0,0,3,python;artificial-intelligence;sklearn-pandas,2022-07-13 07:15:45,2022-07-13 07:15:45,2022-07-13 07:15:45,i am new to machine learning  while doing an assignment i was suddenly presented with the profound question of how does a computer generate a decision tree  for example  consider the problem of predicting a specific flower species using the random forest algorithm  there are two specific attributes of a flower  petal width  petal length  that distinguish the species  in terms of the flower problem  when using sclearn s python decisiontreemaker    how does the computer figure out what petal width and petal length to create the tree  also  does a random forest algorithm create these decision trees by brute forcing and testing every single variation of a tree by weighing entrophy  thanks ,how are decision trees in random forest algorithms made ,machine learning assignment suddenly presented profound question computer generate decision tree example consider problem predicting specific flower species using random forest algorithm two specific attributes flower petal width petal length distinguish species terms flower problem using sclearn python decisiontreemaker computer figure petal width petal length create tree also random forest algorithm create decision trees brute forcing testing every single variation tree weighing entrophy thanks,decision trees random forest algorithms made,decision trees random forest algorithms mademachine learning assignment suddenly presented profound question computer generate decision tree example consider problem predicting specific flower species using random forest algorithm two specific attributes flower petal width petal length distinguish species terms flower problem using sclearn python decisiontreemaker computer figure petal width petal length create tree also random forest algorithm create decision trees brute forcing testing every single variation tree weighing entrophy thanks,"['decision', 'trees', 'random', 'forest', 'algorithms', 'mademachine', 'learning', 'assignment', 'suddenly', 'presented', 'profound', 'question', 'computer', 'generate', 'decision', 'tree', 'example', 'consider', 'problem', 'predicting', 'specific', 'flower', 'species', 'using', 'random', 'forest', 'algorithm', 'two', 'specific', 'attributes', 'flower', 'petal', 'width', 'petal', 'length', 'distinguish', 'species', 'terms', 'flower', 'problem', 'using', 'sclearn', 'python', 'decisiontreemaker', 'computer', 'figure', 'petal', 'width', 'petal', 'length', 'create', 'tree', 'also', 'random', 'forest', 'algorithm', 'create', 'decision', 'trees', 'brute', 'forcing', 'testing', 'every', 'single', 'variation', 'tree', 'weighing', 'entrophy', 'thanks']","['decis', 'tree', 'random', 'forest', 'algorithm', 'mademachin', 'learn', 'assign', 'suddenli', 'present', 'profound', 'question', 'comput', 'gener', 'decis', 'tree', 'exampl', 'consid', 'problem', 'predict', 'specif', 'flower', 'speci', 'use', 'random', 'forest', 'algorithm', 'two', 'specif', 'attribut', 'flower', 'petal', 'width', 'petal', 'length', 'distinguish', 'speci', 'term', 'flower', 'problem', 'use', 'sclearn', 'python', 'decisiontreemak', 'comput', 'figur', 'petal', 'width', 'petal', 'length', 'creat', 'tree', 'also', 'random', 'forest', 'algorithm', 'creat', 'decis', 'tree', 'brute', 'forc', 'test', 'everi', 'singl', 'variat', 'tree', 'weigh', 'entrophi', 'thank']"
33,37,37,13555760,72958457,custom neural network model that shares the same fixed weights across a variable number of inputs,"<p>I have a special machine learning problem and I've come up with a weird solution that I'm not entirely sure how to implement in tensorflow for python (and it has to be tensorflow for python). The problem is essentially just picking the best of 5 options, where each option is represented by a vector of length 10. The training data is organized into input vectors of length 50 (5 options * 10 option length) and labeled with one-hot vectors of length 5.</p>
<p>Here's the wacky part: it might not be 5. The model needs to be able to choose from any number of options. So, just having a dense model with 50 inputs and 5 outputs isn't going to work.</p>
<p>The best solution I can think of is to just evaluate individual options rather than a full set. Make a model with 10 inputs and 1 output, where the output is just the model's &quot;confidence&quot; in how good the input vector is. Pass all 5 (or however many) inputs through the model, put each confidence value into an array, and pass that array through a softmax function to get an output.</p>
<p>Now, I can't just train a model with 10 inputs and 1 output and do the combination and softmax stuff afterwards in numpy, because my training data isn't 10 values mapped to a confidence value. It's 30, 40, 50, or 10*n values mapped to a one-hot vector of size 3, 4, 5, or n. It needs to train in sets, so the model itself needs to be able to handle sets. I have no clue how to do this in tensorflow. As far as I can tell, Keras forces you to build models with a fixed input and output size. Even though it has a nice functional API, it won't let me do variable input/output sizes.</p>
<p>If this were libtorch for C++, I would just create a forward() function when defining my model and it would let me do whatever input and output sizes I want, but I can't find how to make the equivalent of this in tensorflow for python. Is this even possible?</p>
<p>Any tips are appreciated, thanks.</p>
",24,0,0,3,python;tensorflow;keras,2022-07-13 02:44:11,2022-07-13 02:44:11,2022-07-13 04:26:19,i have a special machine learning problem and i ve come up with a weird solution that i m not entirely sure how to implement in tensorflow for python  and it has to be tensorflow for python   the problem is essentially just picking the best of  options  where each option is represented by a vector of length   the training data is organized into input vectors of length    options    option length  and labeled with one hot vectors of length   here s the wacky part  it might not be   the model needs to be able to choose from any number of options  so  just having a dense model with  inputs and  outputs isn t going to work  the best solution i can think of is to just evaluate individual options rather than a full set  make a model with  inputs and  output  where the output is just the model s  confidence  in how good the input vector is  pass all   or however many  inputs through the model  put each confidence value into an array  and pass that array through a softmax function to get an output  now  i can t just train a model with  inputs and  output and do the combination and softmax stuff afterwards in numpy  because my training data isn t  values mapped to a confidence value  it s       or  n values mapped to a one hot vector of size       or n  it needs to train in sets  so the model itself needs to be able to handle sets  i have no clue how to do this in tensorflow  as far as i can tell  keras forces you to build models with a fixed input and output size  even though it has a nice functional api  it won t let me do variable input output sizes  if this were libtorch for c    i would just create a forward   function when defining my model and it would let me do whatever input and output sizes i want  but i can t find how to make the equivalent of this in tensorflow for python  is this even possible  any tips are appreciated  thanks ,custom neural network model that shares the same fixed weights across a variable number of inputs,special machine learning problem come weird solution entirely sure implement tensorflow python tensorflow python problem essentially picking best options option represented vector length training data organized input vectors length options option length labeled one hot vectors length wacky part might model needs able choose number options dense model inputs outputs going work best solution think evaluate individual options rather full set make model inputs output output model confidence good input vector pass however many inputs model put confidence value array pass array softmax function get output train model inputs output combination softmax stuff afterwards numpy training data values mapped confidence value n values mapped one hot vector size n needs train sets model needs able handle sets clue tensorflow far tell keras forces build models fixed input output size even though nice functional api let variable input output sizes libtorch c would create forward function defining model would let whatever input output sizes want find make equivalent tensorflow python even possible tips appreciated thanks,neural network model shares fixed weights across variable number inputs,neural network model shares fixed weights across variable number inputsspecial machine learning problem come weird solution entirely sure implement tensorflow python tensorflow python problem essentially picking best options option represented vector length training data organized input vectors length options option length labeled one hot vectors length wacky part might model needs able choose number options dense model inputs outputs going work best solution think evaluate individual options rather full set make model inputs output output model confidence good input vector pass however many inputs model put confidence value array pass array softmax function get output train model inputs output combination softmax stuff afterwards numpy training data values mapped confidence value n values mapped one hot vector size n needs train sets model needs able handle sets clue tensorflow far tell keras forces build models fixed input output size even though nice functional api let variable input output sizes libtorch c would create forward function defining model would let whatever input output sizes want find make equivalent tensorflow python even possible tips appreciated thanks,"['neural', 'network', 'model', 'shares', 'fixed', 'weights', 'across', 'variable', 'number', 'inputsspecial', 'machine', 'learning', 'problem', 'come', 'weird', 'solution', 'entirely', 'sure', 'implement', 'tensorflow', 'python', 'tensorflow', 'python', 'problem', 'essentially', 'picking', 'best', 'options', 'option', 'represented', 'vector', 'length', 'training', 'data', 'organized', 'input', 'vectors', 'length', 'options', 'option', 'length', 'labeled', 'one', 'hot', 'vectors', 'length', 'wacky', 'part', 'might', 'model', 'needs', 'able', 'choose', 'number', 'options', 'dense', 'model', 'inputs', 'outputs', 'going', 'work', 'best', 'solution', 'think', 'evaluate', 'individual', 'options', 'rather', 'full', 'set', 'make', 'model', 'inputs', 'output', 'output', 'model', 'confidence', 'good', 'input', 'vector', 'pass', 'however', 'many', 'inputs', 'model', 'put', 'confidence', 'value', 'array', 'pass', 'array', 'softmax', 'function', 'get', 'output', 'train', 'model', 'inputs', 'output', 'combination', 'softmax', 'stuff', 'afterwards', 'numpy', 'training', 'data', 'values', 'mapped', 'confidence', 'value', 'n', 'values', 'mapped', 'one', 'hot', 'vector', 'size', 'n', 'needs', 'train', 'sets', 'model', 'needs', 'able', 'handle', 'sets', 'clue', 'tensorflow', 'far', 'tell', 'keras', 'forces', 'build', 'models', 'fixed', 'input', 'output', 'size', 'even', 'though', 'nice', 'functional', 'api', 'let', 'variable', 'input', 'output', 'sizes', 'libtorch', 'c', 'would', 'create', 'forward', 'function', 'defining', 'model', 'would', 'let', 'whatever', 'input', 'output', 'sizes', 'want', 'find', 'make', 'equivalent', 'tensorflow', 'python', 'even', 'possible', 'tips', 'appreciated', 'thanks']","['neural', 'network', 'model', 'share', 'fix', 'weight', 'across', 'variabl', 'number', 'inputsspeci', 'machin', 'learn', 'problem', 'come', 'weird', 'solut', 'entir', 'sure', 'implement', 'tensorflow', 'python', 'tensorflow', 'python', 'problem', 'essenti', 'pick', 'best', 'option', 'option', 'repres', 'vector', 'length', 'train', 'data', 'organ', 'input', 'vector', 'length', 'option', 'option', 'length', 'label', 'one', 'hot', 'vector', 'length', 'wacki', 'part', 'might', 'model', 'need', 'abl', 'choos', 'number', 'option', 'dens', 'model', 'input', 'output', 'go', 'work', 'best', 'solut', 'think', 'evalu', 'individu', 'option', 'rather', 'full', 'set', 'make', 'model', 'input', 'output', 'output', 'model', 'confid', 'good', 'input', 'vector', 'pass', 'howev', 'mani', 'input', 'model', 'put', 'confid', 'valu', 'array', 'pass', 'array', 'softmax', 'function', 'get', 'output', 'train', 'model', 'input', 'output', 'combin', 'softmax', 'stuff', 'afterward', 'numpi', 'train', 'data', 'valu', 'map', 'confid', 'valu', 'n', 'valu', 'map', 'one', 'hot', 'vector', 'size', 'n', 'need', 'train', 'set', 'model', 'need', 'abl', 'handl', 'set', 'clue', 'tensorflow', 'far', 'tell', 'kera', 'forc', 'build', 'model', 'fix', 'input', 'output', 'size', 'even', 'though', 'nice', 'function', 'api', 'let', 'variabl', 'input', 'output', 'size', 'libtorch', 'c', 'would', 'creat', 'forward', 'function', 'defin', 'model', 'would', 'let', 'whatev', 'input', 'output', 'size', 'want', 'find', 'make', 'equival', 'tensorflow', 'python', 'even', 'possibl', 'tip', 'appreci', 'thank']"
34,38,38,19480081,72880854,How to balance a dataset using SCUT and the scutr-package in R,"<p>I was given a machine learning project in R by a colleague who can no longer work on it. I am currently trying to balance the used dataset with the SCUT function in the scutr package and I keep running into the following Problem:</p>
<p>The project I am working with contains the base dataset, formatted as a standard dataframe that contains different information on different YouTube channels (URL, name, description, etc.) and also a classification of 4 classes (hkgeschlecht). The classification is numerical, some of the other information as well, but the channel description for example is a text:</p>
<pre><code>'data.frame':   199 obs. of  6 variables:
 $ ctitle       : chr  &quot;Gaming Kati&quot; &quot;EinfallsReich&quot; &quot;Frank / Generation - E&quot; &quot;Gladiator Glubschi&quot; ...
 $ cdescr       : chr  &quot;Dieser Kanal ist einfach ein Kanal von einem Mädel, welches einfach im Animal Crossing hype ist &lt;U+0001F61D&gt;&lt;U+&quot;| __truncated__ &quot;Kurze und EinfallsReiche Fakten Videos mit folgenden Themen:\n\n                                               &quot;| __truncated__ &quot;Ich bin Frank aus Hamburg...\n\n...glücklicher Ehemann und Vater von zwei fantastischen Jungs. \n\nZu meinen gr&quot;| __truncated__ &quot;Gladiator Glubschi\n- Ein Glubschi\n- Zwei krasse Kanäle\n- Drei Unterhaltung!\n\nUnd damit erstmal danke fürs &quot;| __truncated__ ...
 $ cthumbnailurl: chr  &quot;https://yt3.ggpht.com/a/AATXAJwsWCPoVZ6g-uk_9UbMU3NqOU-QuoQyunPoYg=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJxunaT5qD2CbS7AQodCYq-HDOVee87NYBnRnw=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJzaeY6aZJuWpCsa8ul1CXHmQ1bC6reTWk9mTw=s240-c-k-c0xffffffff-no-rj-mo&quot; &quot;https://yt3.ggpht.com/a/AATXAJx0pmglui0v3YZblGuT1yOdNTm33qVP7mLXxQ=s240-c-k-c0xffffffff-no-rj-mo&quot; ...
 $ cviews       : int  1348087 2764 229744 15556 1884 1077314 158044 113570 25495 2364116 ...
 $ csubscriber  : int  13000 0 1140 320 0 7940 623 823 406 34700 ...
 $ hkgeschlecht : num  2 99 1 1 1 2 1 1 1 2 ...
</code></pre>
<p>The project uses a Naive Bayes Classifier and thus the channel description (sdescr) in the dataframe is transformed into a document feature matrix dfm which then is split into a training dataset and test dataset. This all works out fine and the model gives me decent predictions.</p>
<p>However the main dataset is unbalanced as one class is much more dominant than the others. I now want to balance this dataset using the SCUT-method so that the prediction of the minority classes improves. I had planned on using the <a href=""https://mran.microsoft.com/web/packages/scutr/scutr.pdf"" rel=""nofollow noreferrer"">scutr package</a> and the SCUT function in it since it is seems fairly straight forward.</p>
<p>Now my problem is, if I apply the function to main dataset like this:</p>
<pre><code>ret &lt;- SCUT(mldata, &quot;hkgeschlecht&quot;, oversample = oversample_smote, undersample = undersample_hclust,)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error in get.knnx(data, query, k, algorithm) : Data non-numeric</p>
</blockquote>
<p>I assume that is due to the differently formated variables in the dataframe.</p>
<p>But if I try to apply it only to the training dataset like this:</p>
<pre><code>ret &lt;- SCUT(testdfm1, testdfm1@docvars$docvars, oversample = oversample_smote, undersample = undersample_hclust,)
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error in validate_dataset(data, cls_col) :
Column not found in data: 22211299112111312111122333311133211111111</p>
</blockquote>
<p>Which I assume is due to the SCUT function needing a dataframe format and not a document feature matrix.</p>
<p>My question thus is: How I can apply the SCUT method in this case? Is there a way to make the function work with a document feature matrix, say to get it to recognize the column with the classification? Would that even make sense? Or do I have to go about it in a completely different way?</p>
",22,0,0,4,r;dataframe;machine-learning;matrix,2022-07-06 14:46:41,2022-07-06 14:46:41,2022-07-13 01:36:23,i was given a machine learning project in r by a colleague who can no longer work on it  i am currently trying to balance the used dataset with the scut function in the scutr package and i keep running into the following problem  the project i am working with contains the base dataset  formatted as a standard dataframe that contains different information on different youtube channels  url  name  description  etc   and also a classification of  classes  hkgeschlecht   the classification is numerical  some of the other information as well  but the channel description for example is a text  the project uses a naive bayes classifier and thus the channel description  sdescr  in the dataframe is transformed into a document feature matrix dfm which then is split into a training dataset and test dataset  this all works out fine and the model gives me decent predictions  however the main dataset is unbalanced as one class is much more dominant than the others  i now want to balance this dataset using the scut method so that the prediction of the minority classes improves  i had planned on using the  and the scut function in it since it is seems fairly straight forward  now my problem is  if i apply the function to main dataset like this  i get this error  error in get knnx data  query  k  algorithm    data non numeric i assume that is due to the differently formated variables in the dataframe  but if i try to apply it only to the training dataset like this  i get this error  which i assume is due to the scut function needing a dataframe format and not a document feature matrix  my question thus is  how i can apply the scut method in this case  is there a way to make the function work with a document feature matrix  say to get it to recognize the column with the classification  would that even make sense  or do i have to go about it in a completely different way ,how to balance a dataset using scut and the scutr package in r,given machine learning project r colleague longer work currently trying balance used dataset scut function scutr package keep running following problem project working contains base dataset formatted standard dataframe contains different information different youtube channels url name description etc also classification classes hkgeschlecht classification numerical information well channel description example text project uses naive bayes classifier thus channel description sdescr dataframe transformed document feature matrix dfm split training dataset test dataset works fine model gives decent predictions however main dataset unbalanced one class much dominant others want balance dataset using scut method prediction minority classes improves planned using scut function since seems fairly straight forward problem apply function main dataset like get error error get knnx data query k algorithm data non numeric assume due differently formated variables dataframe try apply training dataset like get error assume due scut function needing dataframe format document feature matrix question thus apply scut method case way make function work document feature matrix say get recognize column classification would even make sense go completely different way,balance dataset using scut scutr package r,balance dataset using scut scutr package rgiven machine learning project r colleague longer work currently trying balance used dataset scut function scutr package keep running following problem project working contains base dataset formatted standard dataframe contains different information different youtube channels url name description etc also classification classes hkgeschlecht classification numerical information well channel description example text project uses naive bayes classifier thus channel description sdescr dataframe transformed document feature matrix dfm split training dataset test dataset works fine model gives decent predictions however main dataset unbalanced one class much dominant others want balance dataset using scut method prediction minority classes improves planned using scut function since seems fairly straight forward problem apply function main dataset like get error error get knnx data query k algorithm data non numeric assume due differently formated variables dataframe try apply training dataset like get error assume due scut function needing dataframe format document feature matrix question thus apply scut method case way make function work document feature matrix say get recognize column classification would even make sense go completely different way,"['balance', 'dataset', 'using', 'scut', 'scutr', 'package', 'rgiven', 'machine', 'learning', 'project', 'r', 'colleague', 'longer', 'work', 'currently', 'trying', 'balance', 'used', 'dataset', 'scut', 'function', 'scutr', 'package', 'keep', 'running', 'following', 'problem', 'project', 'working', 'contains', 'base', 'dataset', 'formatted', 'standard', 'dataframe', 'contains', 'different', 'information', 'different', 'youtube', 'channels', 'url', 'name', 'description', 'etc', 'also', 'classification', 'classes', 'hkgeschlecht', 'classification', 'numerical', 'information', 'well', 'channel', 'description', 'example', 'text', 'project', 'uses', 'naive', 'bayes', 'classifier', 'thus', 'channel', 'description', 'sdescr', 'dataframe', 'transformed', 'document', 'feature', 'matrix', 'dfm', 'split', 'training', 'dataset', 'test', 'dataset', 'works', 'fine', 'model', 'gives', 'decent', 'predictions', 'however', 'main', 'dataset', 'unbalanced', 'one', 'class', 'much', 'dominant', 'others', 'want', 'balance', 'dataset', 'using', 'scut', 'method', 'prediction', 'minority', 'classes', 'improves', 'planned', 'using', 'scut', 'function', 'since', 'seems', 'fairly', 'straight', 'forward', 'problem', 'apply', 'function', 'main', 'dataset', 'like', 'get', 'error', 'error', 'get', 'knnx', 'data', 'query', 'k', 'algorithm', 'data', 'non', 'numeric', 'assume', 'due', 'differently', 'formated', 'variables', 'dataframe', 'try', 'apply', 'training', 'dataset', 'like', 'get', 'error', 'assume', 'due', 'scut', 'function', 'needing', 'dataframe', 'format', 'document', 'feature', 'matrix', 'question', 'thus', 'apply', 'scut', 'method', 'case', 'way', 'make', 'function', 'work', 'document', 'feature', 'matrix', 'say', 'get', 'recognize', 'column', 'classification', 'would', 'even', 'make', 'sense', 'go', 'completely', 'different', 'way']","['balanc', 'dataset', 'use', 'scut', 'scutr', 'packag', 'rgiven', 'machin', 'learn', 'project', 'r', 'colleagu', 'longer', 'work', 'current', 'tri', 'balanc', 'use', 'dataset', 'scut', 'function', 'scutr', 'packag', 'keep', 'run', 'follow', 'problem', 'project', 'work', 'contain', 'base', 'dataset', 'format', 'standard', 'datafram', 'contain', 'differ', 'inform', 'differ', 'youtub', 'channel', 'url', 'name', 'descript', 'etc', 'also', 'classif', 'class', 'hkgeschlecht', 'classif', 'numer', 'inform', 'well', 'channel', 'descript', 'exampl', 'text', 'project', 'use', 'naiv', 'bay', 'classifi', 'thu', 'channel', 'descript', 'sdescr', 'datafram', 'transform', 'document', 'featur', 'matrix', 'dfm', 'split', 'train', 'dataset', 'test', 'dataset', 'work', 'fine', 'model', 'give', 'decent', 'predict', 'howev', 'main', 'dataset', 'unbalanc', 'one', 'class', 'much', 'domin', 'other', 'want', 'balanc', 'dataset', 'use', 'scut', 'method', 'predict', 'minor', 'class', 'improv', 'plan', 'use', 'scut', 'function', 'sinc', 'seem', 'fairli', 'straight', 'forward', 'problem', 'appli', 'function', 'main', 'dataset', 'like', 'get', 'error', 'error', 'get', 'knnx', 'data', 'queri', 'k', 'algorithm', 'data', 'non', 'numer', 'assum', 'due', 'differ', 'format', 'variabl', 'datafram', 'tri', 'appli', 'train', 'dataset', 'like', 'get', 'error', 'assum', 'due', 'scut', 'function', 'need', 'datafram', 'format', 'document', 'featur', 'matrix', 'question', 'thu', 'appli', 'scut', 'method', 'case', 'way', 'make', 'function', 'work', 'document', 'featur', 'matrix', 'say', 'get', 'recogn', 'column', 'classif', 'would', 'even', 'make', 'sens', 'go', 'complet', 'differ', 'way']"
35,40,40,13329315,72956556,"column 3&#39;s mode is needed, but i cannot name columns to get data","<p>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
data = np.genfromtxt(url, delimiter=',', dtype='object')</p>
<p>columns = ['A', 'B', 'C', 'D', 'E']
import io
import requests</p>
<p>df = pd.read_csv(io.StringIO(data.decode('utf-8')), header = None, names=columns)</p>
<p>#data.columns = (65+np.arange(data.shape[1])).view('U2')</p>
",14,0,0,1,pandas,2022-07-12 23:28:01,2022-07-12 23:28:01,2022-07-12 23:28:01,df   pd read_csv io stringio data decode  utf      header   none  names columns   data columns     np arange data shape     view  u  ,column    s mode is needed  but i cannot name columns to get data,df pd read_csv io stringio data decode utf header none names columns data columns np arange data shape view u,column mode needed cannot name columns get data,column mode needed cannot name columns get datadf pd read_csv io stringio data decode utf header none names columns data columns np arange data shape view u,"['column', 'mode', 'needed', 'can', 'not', 'name', 'columns', 'get', 'datadf', 'pd', 'read_csv', 'io', 'stringio', 'data', 'decode', 'utf', 'header', 'none', 'names', 'columns', 'data', 'columns', 'np', 'arange', 'data', 'shape', 'view', 'u']","['column', 'mode', 'need', 'can', 'not', 'name', 'column', 'get', 'datadf', 'pd', 'read_csv', 'io', 'stringio', 'data', 'decod', 'utf', 'header', 'none', 'name', 'column', 'data', 'column', 'np', 'arang', 'data', 'shape', 'view', 'u']"
36,41,41,18543927,72937452,ImportError: dlopen(...): Library not loaded: @rpath/_pywrap_tensorflow_internal.so,"<p>I am a beginner at machine learning. I try to use LSTM algorism but when I write</p>
<p><code>from keras.models import Sequential</code></p>
<p>it shows error as below:</p>
<pre><code>ImportError: dlopen(/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so, 2): Library not loaded: @rpath/_pywrap_tensorflow_internal.so
  Referenced from: /Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
  Reason: image not found
</code></pre>
<p>How can I fix this? Thank you so much!</p>
<p>full error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/wangzifan/Desktop/machine/LSTM.py&quot;, line 39, in &lt;module&gt;
    from keras.models import Sequential
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/keras/__init__.py&quot;, line 21, in &lt;module&gt;
    from tensorflow.python import tf2
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.eager import context
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 33, in &lt;module&gt;
    from tensorflow.python import pywrap_tfe
  File &quot;/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/pywrap_tfe.py&quot;, line 25, in &lt;module&gt;
    from tensorflow.python._pywrap_tfe import *
ImportError: dlopen(/Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so, 2): Library not loaded: @rpath/_pywrap_tensorflow_internal.so
  Referenced from: /Users/wangzifan/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
  Reason: image not found

</code></pre>
",22,1,0,5,python;tensorflow;keras;pip;lstm,2022-07-11 16:26:50,2022-07-11 16:26:50,2022-07-12 22:07:14,i am a beginner at machine learning  i try to use lstm algorism but when i write from keras models import sequential it shows error as below  how can i fix this  thank you so much  full error message ,importerror  dlopen       library not loaded   rpath _pywrap_tensorflow_internal so,beginner machine learning try use lstm algorism write keras models import sequential shows error fix thank much full error message,importerror dlopen library loaded rpath _pywrap_tensorflow_internal,importerror dlopen library loaded rpath _pywrap_tensorflow_internalbeginner machine learning try use lstm algorism write keras models import sequential shows error fix thank much full error message,"['importerror', 'dlopen', 'library', 'loaded', 'rpath', '_pywrap_tensorflow_internalbeginner', 'machine', 'learning', 'try', 'use', 'lstm', 'algorism', 'write', 'keras', 'models', 'import', 'sequential', 'shows', 'error', 'fix', 'thank', 'much', 'full', 'error', 'message']","['importerror', 'dlopen', 'librari', 'load', 'rpath', '_pywrap_tensorflow_internalbeginn', 'machin', 'learn', 'tri', 'use', 'lstm', 'algor', 'write', 'kera', 'model', 'import', 'sequenti', 'show', 'error', 'fix', 'thank', 'much', 'full', 'error', 'messag']"
37,42,42,10018602,72850230,"Creating and Merging Multiple Datasets Does Not Fit Into Memory, Use Dask?","<p>I'm not quite sure how to ask this question, but I need some clarification on how to make use of Dask's ability to &quot;handle datasets that don't fit into memory&quot;, because I'm a little confused on how it works from the CREATION of these datasets.</p>
<p>I have made a reproducible code below that closely emulates my problem. Although this example DOES fit into my 16Gb memory, we can assume that it doesn't because it does take up ALMOST all of my RAM.</p>
<p>I'm working with 1min, 5min, 15min and Daily stock market datasets, all of which have their own technical indicators, so each of these separate dataframes are 234 columns in width, with the 1min dataset having the most rows (521,811), and going down from there. Each of these datasets can be created and fit into memory on their own, but here's where it gets tricky.</p>
<p>I'm trying to merge them column-wise into 1 dataframe, each column prepended with their respective timeframes so I can tell them apart, but this creates the memory problem. This is what I'm looking to accomplish visually:</p>
<p><a href=""https://i.stack.imgur.com/DEtpN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DEtpN.png"" alt=""DesiredOutcome"" /></a></p>
<p>I'm not really sure if Dask is what I need here, but I assume so. I'm NOT looking to use any kind of &quot;parallel calculations&quot; here (yet), I just need a way to create this dataframe before feeding it into a machine learning pipeline (yes, I know it's a stock market problem, just overlook that for now). I know Dask has a machine learning pipeline I can use, so maybe I'll make use of that in the future, however I need a way to save this big dataframe to disk, or create it upon importing it on the fly.</p>
<p>What I need help with is how to do this. Seeing as each of these datasets on their own fit into memory nicely, an idea I had (and this may not be correct at all so please let me know), would be to save each of the dataframes to separate parquet files to disk, then create a Dask dataframe object to import each of them into, when I go to start the machine learning pipeline. Something like this:</p>
<p><a href=""https://i.stack.imgur.com/rR8Co.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rR8Co.png"" alt=""Idea1"" /></a></p>
<p>Is this conceptually correct with what I need to do, or am I way off? haha. I've read through the documentation on Dask, and also checked out <a href=""https://docs.dask.org/en/latest/10-minutes-to-dask.html"" rel=""nofollow noreferrer"">this guide</a> specifically, which is good, however as a newbie I need some guidance with how to do this for the first time.</p>
<p>How can I create and save this big merged dataframe to disk, if I can't create it in memory in the first place?</p>
<p>Here is my reproducible dataframe/memory problem code. Be careful when you go to run this as it'll eat up your RAM pretty quickly, I have 16Gb of RAM and it does run on my fairly light machine, but not without some red-lining RAM, just wanted to give the Dask gods out there something specific to work with. Thanks!</p>
<pre><code>from pandas import DataFrame, date_range, merge
from numpy import random

# ------------------------------------------------------------------------------------------------ #
#                                         1 MINUTE DATASET                                         #
# ------------------------------------------------------------------------------------------------ #
ONE_MIN_NUM_OF_ROWS = 521811
ONE_MIN_NUM_OF_COLS = 234
main_df = DataFrame(random.randint(0,100, size=(ONE_MIN_NUM_OF_ROWS, ONE_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;col_&quot; + str(x) for x in range(ONE_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;min&quot;, periods=ONE_MIN_NUM_OF_ROWS))


# ------------------------------------------------------------------------------------------------ #
#                                         5 MINUTE DATASET                                         #
# ------------------------------------------------------------------------------------------------ #
FIVE_MIN_NUM_OF_ROWS = 117732
FIVE_MIN_NUM_OF_COLS = 234
five_min_df = DataFrame(random.randint(0,100, size=(FIVE_MIN_NUM_OF_ROWS, FIVE_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;5_min_col_&quot; + str(x) for x in range(FIVE_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;5min&quot;, periods=FIVE_MIN_NUM_OF_ROWS))
# Merge the 5 minute to the 1 minute df
main_df = merge(main_df, five_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                         15 MINUTE DATASET                                        #
# ------------------------------------------------------------------------------------------------ #
FIFTEEN_MIN_NUM_OF_ROWS = 117732
FIFTEEN_MIN_NUM_OF_COLS = 234
fifteen_min_df = DataFrame(random.randint(0,100, size=(FIFTEEN_MIN_NUM_OF_ROWS, FIFTEEN_MIN_NUM_OF_COLS)), 
                    columns=list(&quot;15_min_col_&quot; + str(x) for x in range(FIFTEEN_MIN_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;15min&quot;, periods=FIFTEEN_MIN_NUM_OF_ROWS))
# Merge the 15 minute to the main df
main_df = merge(main_df, fifteen_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                           DAILY DATASET                                          #
# ------------------------------------------------------------------------------------------------ #
DAILY_NUM_OF_ROWS = 933
DAILY_NUM_OF_COLS = 234
fifteen_min_df = DataFrame(random.randint(0,100, size=(DAILY_NUM_OF_ROWS, DAILY_NUM_OF_COLS)), 
                    columns=list(&quot;daily_col_&quot; + str(x) for x in range(DAILY_NUM_OF_COLS)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;D&quot;, periods=DAILY_NUM_OF_ROWS))
# Merge the daily to the main df (don't worry about &quot;forward peaking&quot; dates)
main_df = merge(main_df, fifteen_min_df, how=&quot;outer&quot;, left_index=True, right_index=True, sort=True)


# ------------------------------------------------------------------------------------------------ #
#                                            FFILL NAN's                                           #
# ------------------------------------------------------------------------------------------------ #
main_df = main_df.fillna(method=&quot;ffill&quot;)

# ------------------------------------------------------------------------------------------------ #
#                                              INSPECT                                             #
# ------------------------------------------------------------------------------------------------ #
print(main_df)
</code></pre>
<p><strong>UPDATE</strong></p>
<p>I took a stab at an attempt for a solution, but I'm still lost.</p>
<p>I see in the tutorials that it's best to merge a pandas DF with a Dask DF. Knowing that one of these dataframes fits into memory on its own, I figured what I could try was:</p>
<ol>
<li>Make the 1 minute dataframe (the biggest one) a Dask dataframe</li>
<li>Generate the 5, 15 and Daily dataframes one at a time in pandas</li>
<li>Merge the pandas dataframes with the Dask dataframe, starting with the 5 min</li>
<li>Save the final, merged Dask dataframe to disk as a parquet (or parquet's as I see it being saved as now)</li>
</ol>
<p>HOWEVER, I still don't understand how the &quot;chunk&quot;/partition sizes fits into what I'm trying to do here because, aren't partitions just chunks of ROWS that it processes? I'm working with COLUMNS here.</p>
<p>My code attempt (thanks to the first answer I received) looks something like this (but again, I'm a newbie here):</p>
<pre><code>from pandas import DataFrame, date_range, merge
from numpy import random
import dask.dataframe as dd, dask.delayed

# ------------------------------------------------------------------------------------------------ #
#                                             Variables                                            #
# ------------------------------------------------------------------------------------------------ #
ONE_MIN_NUM_OF_ROWS = 521811
FIVE_MIN_NUM_OF_ROWS = 117732
FIFTEEN_MIN_NUM_OF_ROWS = 5000
DAILY_NUM_OF_ROWS = 933
ONE_MIN_INDEX = date_range(
    start=&quot;2019-12-09 04:00:00&quot;,
    freq=&quot;1min&quot;,
    periods=ONE_MIN_NUM_OF_ROWS,
)

# ------------------------------------------------------------------------------------------------ #
#                                Function to load data by chuck size                               #
# ------------------------------------------------------------------------------------------------ #
@dask.delayed
def load_data_subset(start_date, freq, periods):
    # presumably, you'd query some API or something here
    dummy_ind = date_range(start_date, freq=freq, periods=periods)
    dummy_response = DataFrame(
        random.randint(0, 100, size=(len(dummy_ind), 234)),
        columns=list(freq + str(x) for x in range(234)),
        index=dummy_ind
    )
    return dummy_response

# ------------------------------------------------------------------------------------------------ #
#                       Create the Dask dataframe from the 1 minute dataframe                      #
# ------------------------------------------------------------------------------------------------ #
ddf = dd.from_delayed([
    load_data_subset(ONE_MIN_INDEX[i], freq=&quot;1min&quot;, periods=10000) for i in range(0, ONE_MIN_NUM_OF_ROWS, 10000)
])

# ------------------------------------------------------------------------------------------------ #
#              Merge the 5, 15, and Daily datasets to the Dask dataframe, COLUMN WISE              #
# ------------------------------------------------------------------------------------------------ #
for current_num_of_rows, current_timeframe in zip([FIVE_MIN_NUM_OF_ROWS, FIFTEEN_MIN_NUM_OF_ROWS, DAILY_NUM_OF_ROWS],
                                                  [&quot;5min&quot;, &quot;15min&quot;, &quot;1D&quot;]):
    five_min_df = DataFrame(random.randint(0,100, size=(FIVE_MIN_NUM_OF_ROWS, 234)), 
                    columns=list(&quot;5_min_col_&quot; + str(x) for x in range(234)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;5min&quot;, periods=FIVE_MIN_NUM_OF_ROWS))
    ddf = ddf.merge(five_min_df, how=&quot;outer&quot;, left_index=True, right_index=True)
    
    fifteen_min_df = DataFrame(random.randint(0,100, size=(FIFTEEN_MIN_NUM_OF_ROWS, 234)), 
                    columns=list(&quot;15_min_col_&quot; + str(x) for x in range(234)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;15min&quot;, periods=FIFTEEN_MIN_NUM_OF_ROWS))
    ddf = ddf.merge(fifteen_min_df, how=&quot;outer&quot;, left_index=True, right_index=True)

    daily_df = DataFrame(random.randint(0,100, size=(DAILY_NUM_OF_ROWS, 234)), 
                    columns=list(&quot;daily_col_&quot; + str(x) for x in range(234)),
                    index=date_range(start=&quot;2019-12-09 04:00:00&quot;, freq=&quot;1D&quot;, periods=DAILY_NUM_OF_ROWS))
    ddf = ddf.merge(daily_df, how=&quot;outer&quot;, left_index=True, right_index=True)

# ------------------------------------------------------------------------------------------------ #
#                                              Export                                              #
# ------------------------------------------------------------------------------------------------ #
ddf.to_parquet(&quot;test&quot;, overwrite=True)
</code></pre>
<p>When I do this, my resources look like this for a while:</p>
<p><a href=""https://i.stack.imgur.com/RZkgR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RZkgR.png"" alt=""Resources"" /></a></p>
<p>...then as it starts writing the parquet files to disk, it's this:</p>
<p><a href=""https://i.stack.imgur.com/P4Ker.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P4Ker.png"" alt=""Resources2"" /></a>
<a href=""https://i.stack.imgur.com/olPS5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/olPS5.png"" alt=""Resources3"" /></a></p>
<p>I'm hoping this provides some direction in what I'm looking to accomplish here, I just have no idea how to make this work.</p>
<p>KEEP IN MIND, that technical indicators are being used in the application I'm trying to emulate, so I can't calculate the technical indicators on &quot;chunks&quot; of rows, because, in the instance of a moving average for example, the first X rows would all be nan's while it's making the moving average. The entire dataframe needs to be made first in order to keep the technical indicators in tact before merging the dataframes together.</p>
<p>Thanks!</p>
",96,1,1,5,python;pandas;dataframe;dask;dask-dataframe,2022-07-04 03:40:09,2022-07-04 03:40:09,2022-07-12 21:23:49,i m not quite sure how to ask this question  but i need some clarification on how to make use of dask s ability to  handle datasets that don t fit into memory   because i m a little confused on how it works from the creation of these datasets  i have made a reproducible code below that closely emulates my problem  although this example does fit into my gb memory  we can assume that it doesn t because it does take up almost all of my ram  i m working with min  min  min and daily stock market datasets  all of which have their own technical indicators  so each of these separate dataframes are  columns in width  with the min dataset having the most rows      and going down from there  each of these datasets can be created and fit into memory on their own  but here s where it gets tricky  i m trying to merge them column wise into  dataframe  each column prepended with their respective timeframes so i can tell them apart  but this creates the memory problem  this is what i m looking to accomplish visually   i m not really sure if dask is what i need here  but i assume so  i m not looking to use any kind of  parallel calculations  here  yet   i just need a way to create this dataframe before feeding it into a machine learning pipeline  yes  i know it s a stock market problem  just overlook that for now   i know dask has a machine learning pipeline i can use  so maybe i ll make use of that in the future  however i need a way to save this big dataframe to disk  or create it upon importing it on the fly  what i need help with is how to do this  seeing as each of these datasets on their own fit into memory nicely  an idea i had  and this may not be correct at all so please let me know   would be to save each of the dataframes to separate parquet files to disk  then create a dask dataframe object to import each of them into  when i go to start the machine learning pipeline  something like this   is this conceptually correct with what i need to do  or am i way off  haha  i ve read through the documentation on dask  and also checked out  specifically  which is good  however as a newbie i need some guidance with how to do this for the first time  how can i create and save this big merged dataframe to disk  if i can t create it in memory in the first place  here is my reproducible dataframe memory problem code  be careful when you go to run this as it ll eat up your ram pretty quickly  i have gb of ram and it does run on my fairly light machine  but not without some red lining ram  just wanted to give the dask gods out there something specific to work with  thanks  update i took a stab at an attempt for a solution  but i m still lost  i see in the tutorials that it s best to merge a pandas df with a dask df  knowing that one of these dataframes fits into memory on its own  i figured what i could try was  however  i still don t understand how the  chunk  partition sizes fits into what i m trying to do here because  aren t partitions just chunks of rows that it processes  i m working with columns here  my code attempt  thanks to the first answer i received  looks something like this  but again  i m a newbie here   when i do this  my resources look like this for a while      then as it starts writing the parquet files to disk  it s this  i m hoping this provides some direction in what i m looking to accomplish here  i just have no idea how to make this work  keep in mind  that technical indicators are being used in the application i m trying to emulate  so i can t calculate the technical indicators on  chunks  of rows  because  in the instance of a moving average for example  the first x rows would all be nan s while it s making the moving average  the entire dataframe needs to be made first in order to keep the technical indicators in tact before merging the dataframes together  thanks ,creating and merging multiple datasets does not fit into memory  use dask ,quite sure ask question need clarification make use dask ability handle datasets fit memory little confused works creation datasets made reproducible code closely emulates problem although example fit gb memory assume take almost ram working min min min daily stock market datasets technical indicators separate dataframes columns width min dataset rows going datasets created fit memory gets tricky trying merge column wise dataframe column prepended respective timeframes tell apart creates memory problem looking accomplish visually really sure dask need assume looking use kind parallel calculations yet need way create dataframe feeding machine learning pipeline yes know stock market problem overlook know dask machine learning pipeline use maybe make use future however need way save big dataframe disk create upon importing fly need help seeing datasets fit memory nicely idea may correct please let know would save dataframes separate parquet files disk create dask dataframe object import go start machine learning pipeline something like conceptually correct need way haha read documentation dask also checked specifically good however newbie need guidance first time create save big merged dataframe disk create memory first place reproducible dataframe memory problem code careful go run eat ram pretty quickly gb ram run fairly light machine without red lining ram wanted give dask gods something specific work thanks update took stab attempt solution still lost see tutorials best merge pandas df dask df knowing one dataframes fits memory figured could try however still understand chunk partition sizes fits trying partitions chunks rows processes working columns code attempt thanks first answer received looks something like newbie resources look like starts writing parquet files disk hoping provides direction looking accomplish idea make work keep mind technical indicators used application trying emulate calculate technical indicators chunks rows instance moving average example first x rows would nan making moving average entire dataframe needs made first order keep technical indicators tact merging dataframes together thanks,creating merging multiple datasets fit memory use dask,creating merging multiple datasets fit memory use daskquite sure ask question need clarification make use dask ability handle datasets fit memory little confused works creation datasets made reproducible code closely emulates problem although example fit gb memory assume take almost ram working min min min daily stock market datasets technical indicators separate dataframes columns width min dataset rows going datasets created fit memory gets tricky trying merge column wise dataframe column prepended respective timeframes tell apart creates memory problem looking accomplish visually really sure dask need assume looking use kind parallel calculations yet need way create dataframe feeding machine learning pipeline yes know stock market problem overlook know dask machine learning pipeline use maybe make use future however need way save big dataframe disk create upon importing fly need help seeing datasets fit memory nicely idea may correct please let know would save dataframes separate parquet files disk create dask dataframe object import go start machine learning pipeline something like conceptually correct need way haha read documentation dask also checked specifically good however newbie need guidance first time create save big merged dataframe disk create memory first place reproducible dataframe memory problem code careful go run eat ram pretty quickly gb ram run fairly light machine without red lining ram wanted give dask gods something specific work thanks update took stab attempt solution still lost see tutorials best merge pandas df dask df knowing one dataframes fits memory figured could try however still understand chunk partition sizes fits trying partitions chunks rows processes working columns code attempt thanks first answer received looks something like newbie resources look like starts writing parquet files disk hoping provides direction looking accomplish idea make work keep mind technical indicators used application trying emulate calculate technical indicators chunks rows instance moving average example first x rows would nan making moving average entire dataframe needs made first order keep technical indicators tact merging dataframes together thanks,"['creating', 'merging', 'multiple', 'datasets', 'fit', 'memory', 'use', 'daskquite', 'sure', 'ask', 'question', 'need', 'clarification', 'make', 'use', 'dask', 'ability', 'handle', 'datasets', 'fit', 'memory', 'little', 'confused', 'works', 'creation', 'datasets', 'made', 'reproducible', 'code', 'closely', 'emulates', 'problem', 'although', 'example', 'fit', 'gb', 'memory', 'assume', 'take', 'almost', 'ram', 'working', 'min', 'min', 'min', 'daily', 'stock', 'market', 'datasets', 'technical', 'indicators', 'separate', 'dataframes', 'columns', 'width', 'min', 'dataset', 'rows', 'going', 'datasets', 'created', 'fit', 'memory', 'gets', 'tricky', 'trying', 'merge', 'column', 'wise', 'dataframe', 'column', 'prepended', 'respective', 'timeframes', 'tell', 'apart', 'creates', 'memory', 'problem', 'looking', 'accomplish', 'visually', 'really', 'sure', 'dask', 'need', 'assume', 'looking', 'use', 'kind', 'parallel', 'calculations', 'yet', 'need', 'way', 'create', 'dataframe', 'feeding', 'machine', 'learning', 'pipeline', 'yes', 'know', 'stock', 'market', 'problem', 'overlook', 'know', 'dask', 'machine', 'learning', 'pipeline', 'use', 'maybe', 'make', 'use', 'future', 'however', 'need', 'way', 'save', 'big', 'dataframe', 'disk', 'create', 'upon', 'importing', 'fly', 'need', 'help', 'seeing', 'datasets', 'fit', 'memory', 'nicely', 'idea', 'may', 'correct', 'please', 'let', 'know', 'would', 'save', 'dataframes', 'separate', 'parquet', 'files', 'disk', 'create', 'dask', 'dataframe', 'object', 'import', 'go', 'start', 'machine', 'learning', 'pipeline', 'something', 'like', 'conceptually', 'correct', 'need', 'way', 'haha', 'read', 'documentation', 'dask', 'also', 'checked', 'specifically', 'good', 'however', 'newbie', 'need', 'guidance', 'first', 'time', 'create', 'save', 'big', 'merged', 'dataframe', 'disk', 'create', 'memory', 'first', 'place', 'reproducible', 'dataframe', 'memory', 'problem', 'code', 'careful', 'go', 'run', 'eat', 'ram', 'pretty', 'quickly', 'gb', 'ram', 'run', 'fairly', 'light', 'machine', 'without', 'red', 'lining', 'ram', 'wanted', 'give', 'dask', 'gods', 'something', 'specific', 'work', 'thanks', 'update', 'took', 'stab', 'attempt', 'solution', 'still', 'lost', 'see', 'tutorials', 'best', 'merge', 'pandas', 'df', 'dask', 'df', 'knowing', 'one', 'dataframes', 'fits', 'memory', 'figured', 'could', 'try', 'however', 'still', 'understand', 'chunk', 'partition', 'sizes', 'fits', 'trying', 'partitions', 'chunks', 'rows', 'processes', 'working', 'columns', 'code', 'attempt', 'thanks', 'first', 'answer', 'received', 'looks', 'something', 'like', 'newbie', 'resources', 'look', 'like', 'starts', 'writing', 'parquet', 'files', 'disk', 'hoping', 'provides', 'direction', 'looking', 'accomplish', 'idea', 'make', 'work', 'keep', 'mind', 'technical', 'indicators', 'used', 'application', 'trying', 'emulate', 'calculate', 'technical', 'indicators', 'chunks', 'rows', 'instance', 'moving', 'average', 'example', 'first', 'x', 'rows', 'would', 'nan', 'making', 'moving', 'average', 'entire', 'dataframe', 'needs', 'made', 'first', 'order', 'keep', 'technical', 'indicators', 'tact', 'merging', 'dataframes', 'together', 'thanks']","['creat', 'merg', 'multipl', 'dataset', 'fit', 'memori', 'use', 'daskquit', 'sure', 'ask', 'question', 'need', 'clarif', 'make', 'use', 'dask', 'abil', 'handl', 'dataset', 'fit', 'memori', 'littl', 'confus', 'work', 'creation', 'dataset', 'made', 'reproduc', 'code', 'close', 'emul', 'problem', 'although', 'exampl', 'fit', 'gb', 'memori', 'assum', 'take', 'almost', 'ram', 'work', 'min', 'min', 'min', 'daili', 'stock', 'market', 'dataset', 'technic', 'indic', 'separ', 'datafram', 'column', 'width', 'min', 'dataset', 'row', 'go', 'dataset', 'creat', 'fit', 'memori', 'get', 'tricki', 'tri', 'merg', 'column', 'wise', 'datafram', 'column', 'prepend', 'respect', 'timefram', 'tell', 'apart', 'creat', 'memori', 'problem', 'look', 'accomplish', 'visual', 'realli', 'sure', 'dask', 'need', 'assum', 'look', 'use', 'kind', 'parallel', 'calcul', 'yet', 'need', 'way', 'creat', 'datafram', 'feed', 'machin', 'learn', 'pipelin', 'ye', 'know', 'stock', 'market', 'problem', 'overlook', 'know', 'dask', 'machin', 'learn', 'pipelin', 'use', 'mayb', 'make', 'use', 'futur', 'howev', 'need', 'way', 'save', 'big', 'datafram', 'disk', 'creat', 'upon', 'import', 'fli', 'need', 'help', 'see', 'dataset', 'fit', 'memori', 'nice', 'idea', 'may', 'correct', 'pleas', 'let', 'know', 'would', 'save', 'datafram', 'separ', 'parquet', 'file', 'disk', 'creat', 'dask', 'datafram', 'object', 'import', 'go', 'start', 'machin', 'learn', 'pipelin', 'someth', 'like', 'conceptu', 'correct', 'need', 'way', 'haha', 'read', 'document', 'dask', 'also', 'check', 'specif', 'good', 'howev', 'newbi', 'need', 'guidanc', 'first', 'time', 'creat', 'save', 'big', 'merg', 'datafram', 'disk', 'creat', 'memori', 'first', 'place', 'reproduc', 'datafram', 'memori', 'problem', 'code', 'care', 'go', 'run', 'eat', 'ram', 'pretti', 'quickli', 'gb', 'ram', 'run', 'fairli', 'light', 'machin', 'without', 'red', 'line', 'ram', 'want', 'give', 'dask', 'god', 'someth', 'specif', 'work', 'thank', 'updat', 'took', 'stab', 'attempt', 'solut', 'still', 'lost', 'see', 'tutori', 'best', 'merg', 'panda', 'df', 'dask', 'df', 'know', 'one', 'datafram', 'fit', 'memori', 'figur', 'could', 'tri', 'howev', 'still', 'understand', 'chunk', 'partit', 'size', 'fit', 'tri', 'partit', 'chunk', 'row', 'process', 'work', 'column', 'code', 'attempt', 'thank', 'first', 'answer', 'receiv', 'look', 'someth', 'like', 'newbi', 'resourc', 'look', 'like', 'start', 'write', 'parquet', 'file', 'disk', 'hope', 'provid', 'direct', 'look', 'accomplish', 'idea', 'make', 'work', 'keep', 'mind', 'technic', 'indic', 'use', 'applic', 'tri', 'emul', 'calcul', 'technic', 'indic', 'chunk', 'row', 'instanc', 'move', 'averag', 'exampl', 'first', 'x', 'row', 'would', 'nan', 'make', 'move', 'averag', 'entir', 'datafram', 'need', 'made', 'first', 'order', 'keep', 'technic', 'indic', 'tact', 'merg', 'datafram', 'togeth', 'thank']"
38,43,43,17126954,72936240,identifying miss classified values in confusion matrix in R,"<p>I am using the caret package along with the confusionMatrix function and I would like to know if it is possible to know which are the exact values that were not clasified properly.</p>
<p>Here is a subset of my train data</p>
<pre><code>train_sub &lt;- structure(
  list(
    corr = c(
      0.629922866893549,
      0.632354159559817,
      0.656112138936032,
      0.4469719807955,
      0.598136079870775,
      0.314461239093862,
      0.379065842199838,
      0.347331370037428,
      0.310270891798492,
      0.361064451331448,
      0.335628455451358
    ),
    rdist = c(
      0.775733824285612,
      0.834148208687529,
      0.884167982488944,
      0.633989717138057,
      0.850225777237626,
      0.626197919283803,
      0.649597055761598,
      0.680382136363523,
      0.627828985862852,
      0.713674404108905,
      0.646094473468118
    ),
    CCF2 = c(
      0.634465565134314,
      0.722096802135009,
      0.792385621105087,
      0.46497582143802,
      0.739612023831014,
      0.470724554509749,
      0.505961260826622,
      0.527876803999064,
      0.461724328071479,
      0.564117580569802,
      0.490084457081904
    ),
    Wcorr = c(
      0.629,
      0.613,
      0.812,
      0.424,
      0.593,
      0.36,
      0.346,
      0.286,
      0.333,
      0.381,
      0.333
    ),
    Wcorr2 = c(
      0.735,
      0.743,
      0.802,
      0.588,
      0.691,
      0.632,
      0.61,
      0.599,
      0.599,
      0.632,
      0.613
    ),
    Wcorr3 = c(
      0.21,
      0.301,
      0.421,
      -0.052,
      0.169,
      -0.032,
      -0.042,-0.048,
      -0.035,
      0.006,
      -0.004
    ),
    Var = c(&quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;,
            &quot;W&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;)
  ),
  row.names = c(1L, 2L,
                3L, 5L, 7L, 214L, 215L, 216L, 217L, 218L, 221L),
  class = &quot;data.frame&quot;
)
</code></pre>
<p>and here is a subset of my test data</p>
<pre><code>test_sub &lt;- structure(
  list(
    corr = c(
      0.636658204667785,
      0.5637857758104,
      0.540558984461647,
      0.392647603023863,
      0.561801911406989,
      0.297187412065481,
      0.278864501603015,
      0.505277007007347,
      0.403811785308709,
      0.510158398354856,
      0.459607853624603
    ),
    rdist = c(
      0.887270722679019,
      0.843656768956754,
      0.815806338767273,
      0.732093571145576,
      0.832944903081762,
      0.485497073465096,
      0.454461718498521,
      0.69094669881886,
      0.627667080657035,
      0.705558894672344,
      0.620838398507191
    ),
    CCF2 = c(
      0.802017782695131,
      0.731763898271157,
      0.689402284804853,
      0.577932997250877,
      0.715111899030751,
      0.324826043263382,
      0.298456267077388,
      0.544808216945995,
      0.458148923874818,
      0.551160266327893,
      0.461228649848996
    ),
    Wcorr = c(
      0.655,
      0.536,
      0.677,
      0.556,
      0.571,
      0.29,
      0.25,
      0.484,
      0.25,
      0.515,
      0.314
    ),
    Wcorr2 = c(
      0.779,
      0.682,
      0.734,
      0.675,
      0.736,
      0.5,
      0.529,
      0.611,
      0.555,
      0.639,
      0.572
    ),
    Wcorr3 = c(
      0.368,
      0.154,
      0.266,
      0.103,
      0.224,
      -0.204,
      -0.16,
      -0.026,
      -0.149,
      0.032,
      -0.097
    ),
    Var = c(&quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;,
            &quot;B&quot;)
  ),
  row.names = c(4L, 6L, 8L, 13L, 15L, 321L, 322L, 329L,
                334L, 341L, 344L),
  class = &quot;data.frame&quot;
)
</code></pre>
<p>When I use this line,</p>
<pre><code>confusionMatrix(reference=as.factor(test$Var),data=fittedTL,mode = &quot;everything&quot;)
</code></pre>
<p>With this I compute some machine learning using glmnet method (it gives the best accuracy ini my case)</p>
<pre><code>classCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10,repeats=5,classProbs =  TRUE,savePredictions = &quot;final&quot;)
set.seed(355)
glmnetTL &lt;- train(Var~., train_sub, method= &quot;glmnet&quot;,   trControl=classCtrl)
glmnetTL
</code></pre>
<p>And finally I compute the confusion matrix on my test set:</p>
<pre><code>predict_glmnet &lt;- predict(glmnetTL,test_sub)
predict_glmnet

CM_glmnet &lt;- confusionMatrix(reference=as.factor(test_sub$Var),data=predict_glmnet,mode = &quot;everything&quot;)
CM_glmnet
</code></pre>
<p>The output of the confusion matrix is a table like so</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: center;"">B</th>
<th style=""text-align: right;"">W</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: left;"">W</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">5</td>
</tr>
</tbody>
</table>
</div>
<p>So here I have two predictions/classifications that are not good.</p>
<p>Is there any way I can traceback to which row of my test set it corresponds ?</p>
",23,0,0,3,r;r-caret;confusion-matrix,2022-07-11 14:50:10,2022-07-11 14:50:10,2022-07-12 19:54:26,i am using the caret package along with the confusionmatrix function and i would like to know if it is possible to know which are the exact values that were not clasified properly  here is a subset of my train data and here is a subset of my test data when i use this line  with this i compute some machine learning using glmnet method  it gives the best accuracy ini my case  and finally i compute the confusion matrix on my test set  the output of the confusion matrix is a table like so so here i have two predictions classifications that are not good  is there any way i can traceback to which row of my test set it corresponds  ,identifying miss classified values in confusion matrix in r,using caret package along confusionmatrix function would like know possible know exact values clasified properly subset train data subset test data use line compute machine learning using glmnet method gives best accuracy ini case finally compute confusion matrix test set output confusion matrix table like two predictions classifications good way traceback row test set corresponds,identifying miss classified values confusion matrix r,identifying miss classified values confusion matrix rusing caret package along confusionmatrix function would like know possible know exact values clasified properly subset train data subset test data use line compute machine learning using glmnet method gives best accuracy ini case finally compute confusion matrix test set output confusion matrix table like two predictions classifications good way traceback row test set corresponds,"['identifying', 'miss', 'classified', 'values', 'confusion', 'matrix', 'rusing', 'caret', 'package', 'along', 'confusionmatrix', 'function', 'would', 'like', 'know', 'possible', 'know', 'exact', 'values', 'clasified', 'properly', 'subset', 'train', 'data', 'subset', 'test', 'data', 'use', 'line', 'compute', 'machine', 'learning', 'using', 'glmnet', 'method', 'gives', 'best', 'accuracy', 'ini', 'case', 'finally', 'compute', 'confusion', 'matrix', 'test', 'set', 'output', 'confusion', 'matrix', 'table', 'like', 'two', 'predictions', 'classifications', 'good', 'way', 'traceback', 'row', 'test', 'set', 'corresponds']","['identifi', 'miss', 'classifi', 'valu', 'confus', 'matrix', 'ruse', 'caret', 'packag', 'along', 'confusionmatrix', 'function', 'would', 'like', 'know', 'possibl', 'know', 'exact', 'valu', 'clasifi', 'properli', 'subset', 'train', 'data', 'subset', 'test', 'data', 'use', 'line', 'comput', 'machin', 'learn', 'use', 'glmnet', 'method', 'give', 'best', 'accuraci', 'ini', 'case', 'final', 'comput', 'confus', 'matrix', 'test', 'set', 'output', 'confus', 'matrix', 'tabl', 'like', 'two', 'predict', 'classif', 'good', 'way', 'traceback', 'row', 'test', 'set', 'correspond']"
39,44,44,19534112,72952963,Neural network doesn&#39;t classify sleep EEG recordings,"<p>We have a problem with the code that I'm running in an academic course and I'd appreciate getting some help from someone on the forum.
We're trying to train a CNN model to classify EEG sleep recordings as male or female. It's something that was done in some papers but we use a different dataset in our course. The problem is that the network doesn't learn no matter what we do - we tried changing the number of the layers, the size of each layer, the learning rate, the number of epochs, the batch sizem the optimizer and also adding data.
We also tried using an RNN with a GRU (Gated Recurrent Unit) instead of a CNN but it didn't help.
Here are some examples of the network not learning:</p>
<p><a href=""https://i.stack.imgur.com/kr8yD.png"" rel=""nofollow noreferrer"">Example 1</a>
<a href=""https://i.stack.imgur.com/WYUuG.png"" rel=""nofollow noreferrer"">Example 2</a></p>
<p>Note that the &quot;test&quot; dataset is actually validation.</p>
<p>We can't find any problem with the machine learning part of our code. We think that maybe the problem is with the data processing part but we're not sure so we wanted someone to check if he/she can find a problem with the machine learning part.
Before I should say that we have 200 8-hour recordings of sleep EEG obtained from 200 patients. These are 100 Hz recording. The shape of each sample is 4X2X1000 (4 batches X 2 EEG channels for each recording X 1000 voltage values representing 10 seconds of recording).</p>
<p>Here's the machine learning part (I post a lot of code in case someone says that more code is needed...):</p>
<pre><code>import _pickle
import contextlib
import io
import torch
import os
import numpy as np
from aux_eegproj_funcs_simplified import *
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import torchviz
import mne
</code></pre>
<pre><code># DATASET
table_name = 'sleep_1_10sec.csv' # name of the table from which we take the beginning and end time of each sample (subrecording)
eeg_ds = EEGDataset(EEGTransform, exp_table=build_experiment_tbl(table_name), filtered=0)  # EEGDataSet and EEGTransform are a class
batch_sz = 4 # batch size

# splitting the data to train, validation and test
dl_train, dl_val, dl_test = tt_split_by_pid_mf(dataset=eeg_ds, batch_size=batch_sz, train_rt=.8,  num_workers=0, verbose=1)
nF = sum(eeg_ds.table['sex (F=1)'][dl_train.sampler.indices] == 1)  # number of females
nM = sum(eeg_ds.table['sex (F=1)'][dl_train.sampler.indices] == 2)  # number of males
wF = nM / (nF + nM)  # females percentage
wM = nF / (nF + nM)  # males percentage
</code></pre>
<p>The model:</p>
<pre><code>class eegSexNet(nn.Module):  # Define a network to classify Sex
    def __init__(self, input_shape):
        &quot;&quot;&quot;
        :param input_shape: input tensor shape - every batch size will be ok as it is used to compute the FCs input size.
        &quot;&quot;&quot;
        super().__init__()
        # Define the CNN layers in a nn.Sequential.
        # Remember to use the number of input channels as the first layer input shape.
        self.CNN = nn.Sequential(
            nn.Conv1d(in_channels=input_shape[1], out_channels=8, kernel_size=5, stride=1, padding=0, dilation=2),
            # TODO try changing the kernel sizes they were 3
            nn.ReLU(),
            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=2),
            nn.ReLU(),
            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0, dilation=2),
            nn.ReLU(),
            Residual(in_channels=32)
        )

        # Compute the CNN output size here to use as the input size for the fully-connected part.
        CNN_forward = self.CNN(torch.zeros(input_shape))

        self.FCs = nn.Sequential(
            nn.Linear(CNN_forward.shape[1] * CNN_forward.shape[2], 10),
            nn.ReLU(),
            nn.Linear(10, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # ------Your code------#
        # Forward through the CNN by passing x, flatten and then forward through the linears.
        features = self.CNN(x)
        features = features.view(features.size(0), -1)  # reshape/flatten
        scores = self.FCs(features)
        # ------^^^^^^^^^------#
        return torch.squeeze(scores)
</code></pre>
<p>Residual block used in the class of the model:</p>
<pre><code>class Residual(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # Define self.direct_path by adding the layers into a nn.Sequential. Use nn.Conv1d and nn.Relu.
        # You can use padding to avoid reducing L size, to allow the skip-connection adding.
        self.direct_path = nn.Sequential(
            nn.Conv1d(in_channels=in_channels, out_channels=16, kernel_size=7, padding=3),
            nn.ReLU(),
            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=7, padding=3)
        )
        # You should use convolution layer with a kernel size of 1 to consider the case where the input and output shapes mismatch.
        skip_layers = []
        if in_channels != 32:  # HOW DOES THIS PART WORK? When are you adding the layers
            skip_layers.append(
                nn.Conv1d(in_channels=in_channels, out_channels=32, kernel_size=1, stride=1, padding=0, dilation=1,
                          bias=False)
            )
        else:
            self.skip_path = nn.Sequential(*skip_layers)

    def forward(self, x):
        # Compute the two paths and add the results to each other, then use ReLU (torch.relu) to activate the output.
        direct_output = self.direct_path(x)
        skip_output = self.skip_path(x)
        activated_output = torch.relu(direct_output + skip_output)
        return activated_output
</code></pre>
<p>Training loop:</p>
<pre><code>def Train_Sex_Net(epochs=n_epochs, fn='None', optimizer=opt_sex, loss_function=bce):  # Training SEXNET
    global sex_net
    gpu_0 = torch.device(1)
    label = 0  # select the sex label
    train_loss_vec = []
    test_loss_vec = []
    train_acc_vec = []
    test_acc_vec = []
    for i_epoch in range(epochs):
        train_loss = 0
        test_loss = 0
        # Train set
        train_loss, y_true_train, y_pred_train = forward_epoch(sex_net, dl_train, loss_function, optimizer,
                                                                wM, train_loss,
                                                               to_train=True, desc='Train', device=gpu_0, label=label)
        # Test set
        test_loss, y_true_test, y_pred_test = forward_epoch(sex_net, dl_test, loss_function, optimizer, wM, test_loss,
                                                            to_train=False, desc='Test', device=gpu_0, label=label)

        # Metrics:
        train_loss = train_loss / len(dl_train)  # we want to get the mean over batches.
        test_loss = test_loss / len(dl_test)
        train_loss_vec.append(train_loss)
        test_loss_vec.append(test_loss)

        train_accuracy = accuracy_score(y_true_train.cpu(),
                                        (y_pred_train.cpu().detach() &gt; 0.5) * 1)
        test_accuracy = accuracy_score(y_true_test.cpu(),
                                       (y_pred_test.cpu().detach() &gt; 0.5) * 1)

        train_acc_vec.append(train_accuracy)
        test_acc_vec.append(test_accuracy)

        print('\n')
        print(f'train_loss={round(train_loss, 3)}; train_accuracy={round(train_accuracy, 3)} \
              test_loss={round(test_loss, 3)}; test_accuracy={round(test_accuracy, 3)}')

    return (train_loss_vec, train_acc_vec), (test_loss_vec, test_acc_vec) # (val_loss_vec, val_acc_vec)

</code></pre>
<p>Called by the training loop:</p>
<pre><code>def forward_epoch(model, dl, loss_function, optimizer, weight, total_loss=0,
                  to_train=False, desc=None, device=torch.device('cpu'), label=0): # Training loop
    # label =0 is for sex
    # label = 1 is for Age
    # total loss is over the entire epoch
    # y_trues is by patient for the entire epoch; can get last batch with [-batch_size]
    # y_preds is by patient for the entire epoch
    #
    with tqdm(total=len(dl), desc=desc, ncols=100) as pbar:
        model = model.double().to(device)  # solving runtime memory issue

        y_trues = torch.empty(0).type(torch.int).to(device)
        y_preds = torch.empty(0).type(torch.int).to(device)
        for i_batch, (X, y) in enumerate(dl):
            X = X.to(device)
            X = X.type(torch.double)
            y = y[label].to(device)  # added index because of get label returning sex, age
            y_pred = model(X)  # Forward
            y_true = y.type(torch.double)  # Loss:
            y_true_copy = torch.clone(y_true)
            loss = loss_function(y_pred, y_true)  # loss of one batch
            total_loss += loss.item()

            y_trues = torch.cat((y_trues, y_true))
            y_preds = torch.cat((y_preds, y_pred))
            if to_train:
                optimizer.zero_grad()  #  Backward:zero the gradients to not accumulate their changes.
                loss.backward()  # get gradients
                optimizer.step()  # Optimization step: use gradients
            pbar.update(1)  # Progress bar

    return total_loss, y_trues, y_preds
</code></pre>
<p>Calling all the functions:</p>
<pre><code>sex_net = eegSexNet(torch.Size([4, 2, 1000]))  # Instantiate the network

learning_rate = 0.0001
opt_sex = torch.optim.Adam(params=sex_net.parameters(), lr=learning_rate)  # Optimizer for eegnet
bce = nn.BCELoss()
n_epochs = 6
f0 = 'sex_k7'  # file format '.pickle' added automatically
train_res, test_res = Train_Sex_Net(epochs=n_epochs, fn=f0)
</code></pre>
<p>Does anyone see anything that might be wrong in the code? Or maybe the problem is in the data processing and choosing part that I didn't show?</p>
",46,1,0,4,python;machine-learning;neural-network;pytorch,2022-07-12 18:47:11,2022-07-12 18:47:11,2022-07-12 19:30:34,note that the  test  dataset is actually validation  here s the machine learning part  i post a lot of code in case someone says that more code is needed      the model  residual block used in the class of the model  training loop  called by the training loop  calling all the functions  does anyone see anything that might be wrong in the code  or maybe the problem is in the data processing and choosing part that i didn t show ,neural network doesn   t classify sleep eeg recordings,note test dataset actually validation machine learning part post lot code case someone says code needed model residual block used class model training loop called training loop calling functions anyone see anything might wrong code maybe problem data processing choosing part show,neural network classify sleep eeg recordings,neural network classify sleep eeg recordingsnote test dataset actually validation machine learning part post lot code case someone says code needed model residual block used class model training loop called training loop calling functions anyone see anything might wrong code maybe problem data processing choosing part show,"['neural', 'network', 'classify', 'sleep', 'eeg', 'recordingsnote', 'test', 'dataset', 'actually', 'validation', 'machine', 'learning', 'part', 'post', 'lot', 'code', 'case', 'someone', 'says', 'code', 'needed', 'model', 'residual', 'block', 'used', 'class', 'model', 'training', 'loop', 'called', 'training', 'loop', 'calling', 'functions', 'anyone', 'see', 'anything', 'might', 'wrong', 'code', 'maybe', 'problem', 'data', 'processing', 'choosing', 'part', 'show']","['neural', 'network', 'classifi', 'sleep', 'eeg', 'recordingsnot', 'test', 'dataset', 'actual', 'valid', 'machin', 'learn', 'part', 'post', 'lot', 'code', 'case', 'someon', 'say', 'code', 'need', 'model', 'residu', 'block', 'use', 'class', 'model', 'train', 'loop', 'call', 'train', 'loop', 'call', 'function', 'anyon', 'see', 'anyth', 'might', 'wrong', 'code', 'mayb', 'problem', 'data', 'process', 'choos', 'part', 'show']"
40,45,45,19520661,72933415,ROOT&#39;s TMVA user guide,"<p>I am using ROOT's TMVA (developed by CERN), the version of the ROOT is 6.24.
the user manual i have is for TMVA version 4.3.0 (for ROOT &gt;= 6.12/00 on May 26, 2020)
but the manual seems to be a little bit different from my current version (for example, the options available for a particular machine learning model).</p>
<p>is there any updated user manual, or portals that provide guides on the options available for a particular machine learning model.</p>
",17,1,0,1,root-framework,2022-07-11 09:00:43,2022-07-11 09:00:43,2022-07-12 19:21:37,is there any updated user manual  or portals that provide guides on the options available for a particular machine learning model ,root   s tmva user guide,updated user manual portals provide guides options available particular machine learning model,root tmva user guide,root tmva user guideupdated user manual portals provide guides options available particular machine learning model,"['root', 'tmva', 'user', 'guideupdated', 'user', 'manual', 'portals', 'provide', 'guides', 'options', 'available', 'particular', 'machine', 'learning', 'model']","['root', 'tmva', 'user', 'guideupd', 'user', 'manual', 'portal', 'provid', 'guid', 'option', 'avail', 'particular', 'machin', 'learn', 'model']"
41,46,46,5654564,38380795,"pandas read_json: &quot;If using all scalar values, you must pass an index&quot;","<p>I have some difficulty in importing a JSON file with pandas.</p>

<pre><code>import pandas as pd
map_index_to_word = pd.read_json('people_wiki_map_index_to_word.json')
</code></pre>

<p>This is the  error that I get: </p>

<pre><code>ValueError: If using all scalar values, you must pass an index
</code></pre>

<p>The file structure is simplified like this:</p>

<pre><code>{""biennials"": 522004, ""lb915"": 116290, ""shatzky"": 127647, ""woode"": 174106, ""damfunk"": 133206, ""nualart"": 153444, ""hatefillot"": 164111, ""missionborn"": 261765, ""yeardescribed"": 161075, ""theoryhe"": 521685}
</code></pre>

<p>It is from the machine learning course of University of Washington on Coursera. You can find the file <a href=""https://www.coursera.org/learn/ml-clustering-and-retrieval/supplement/gJSfc/choosing-features-and-metrics-for-nearest-neighbor-search"" rel=""noreferrer"">here</a>.</p>
",81477,7,56,3,python;json;pandas,2016-07-14 23:11:10,2016-07-14 23:11:10,2022-07-12 14:40:53,i have some difficulty in importing a json file with pandas  this is the  error that i get   the file structure is simplified like this  it is from the machine learning course of university of washington on coursera  you can find the file  ,pandas read_json   if using all scalar values  you must pass an index ,difficulty importing json file pandas error get file structure simplified like machine learning course university washington coursera find file,pandas read_json using scalar values must pass index,pandas read_json using scalar values must pass indexdifficulty importing json file pandas error get file structure simplified like machine learning course university washington coursera find file,"['pandas', 'read_json', 'using', 'scalar', 'values', 'must', 'pass', 'indexdifficulty', 'importing', 'json', 'file', 'pandas', 'error', 'get', 'file', 'structure', 'simplified', 'like', 'machine', 'learning', 'course', 'university', 'washington', 'coursera', 'find', 'file']","['panda', 'read_json', 'use', 'scalar', 'valu', 'must', 'pass', 'indexdifficulti', 'import', 'json', 'file', 'panda', 'error', 'get', 'file', 'structur', 'simplifi', 'like', 'machin', 'learn', 'cours', 'univers', 'washington', 'coursera', 'find', 'file']"
42,47,47,19457689,72948809,terraform conditionals to use different resources in azure,"<p>I want to create resources like azure kubernetes cluster and azure compute cluster to attach it with the Azure machine learning workspace. I need to create a conditional to use either one of Kuberenetes cluster or compute cluster. Is there a way to do that.</p>
",25,0,-1,3,azure;terraform;conditional-statements,2022-07-12 13:23:45,2022-07-12 13:23:45,2022-07-12 13:23:45,i want to create resources like azure kubernetes cluster and azure compute cluster to attach it with the azure machine learning workspace  i need to create a conditional to use either one of kuberenetes cluster or compute cluster  is there a way to do that ,terraform conditionals to use different resources in azure,want create resources like azure kubernetes cluster azure compute cluster attach azure machine learning workspace need create conditional use either one kuberenetes cluster compute cluster way,terraform conditionals use different resources azure,terraform conditionals use different resources azurewant create resources like azure kubernetes cluster azure compute cluster attach azure machine learning workspace need create conditional use either one kuberenetes cluster compute cluster way,"['terraform', 'conditionals', 'use', 'different', 'resources', 'azurewant', 'create', 'resources', 'like', 'azure', 'kubernetes', 'cluster', 'azure', 'compute', 'cluster', 'attach', 'azure', 'machine', 'learning', 'workspace', 'need', 'create', 'conditional', 'use', 'either', 'one', 'kuberenetes', 'cluster', 'compute', 'cluster', 'way']","['terraform', 'condit', 'use', 'differ', 'resourc', 'azurew', 'creat', 'resourc', 'like', 'azur', 'kubernet', 'cluster', 'azur', 'comput', 'cluster', 'attach', 'azur', 'machin', 'learn', 'workspac', 'need', 'creat', 'condit', 'use', 'either', 'one', 'kuberenet', 'cluster', 'comput', 'cluster', 'way']"
43,48,48,11053795,72945697,how to get pycharm to do GraphViz,"<p>The data for this comes from the UCI machine learning repository
Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.</p>
<p>This code runs without any technical error but whether I run it with None for out_file or give it a png file name it doesnt make the decision tree visual, instead it just prints it like this:</p>
<pre><code>digraph Tree {
node [shape=box, style=&quot;filled&quot;, color=&quot;black&quot;, fontname=&quot;helvetica&quot;] ;
edge [fontname=&quot;helvetica&quot;] ;
0 [label=&quot;income_ &gt;50K &lt;= 0.5\ngini = 0.366\nsamples = 32561\nvalue = [24720, 
7841]\nclass = i&quot;, fillcolor=&quot;#eda978&quot;] ;
1 [label=&quot;gini = 0.0\nsamples = 24720\nvalue = [24720, 0]\nclass = i&quot;, 
fillcolor=&quot;#e58139&quot;] ;
0 -&gt; 1 [labeldistance=2.5, labelangle=45, headlabel=&quot;True&quot;] ;
2 [label=&quot;gini = 0.0\nsamples = 7841\nvalue = [0, 7841]\nclass = n&quot;, 
fillcolor=&quot;#399de5&quot;] ;
0 -&gt; 2 [labeldistance=2.5, labelangle=-45, headlabel=&quot;False&quot;] ;
}
</code></pre>
<p><strong>here is my code</strong></p>
<pre><code>csv_file = age, workclass, fnlwgt, education, educationnum, maritalstatus, occupation, 
relationship, race, sex, capitalgain, capitalloss, hoursperweek, nativecountry, income
39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, 
Male, 2174, 0, 40, United-States, &lt;=50K
50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, 
Husband, White, Male, 0, 0, 13, United-States, &lt;=50K
38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, 
Male, 0, 0, 40, United-States, &lt;=50K
53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, 
Male, 0, 0, 40, United-States, &lt;=50K


########## alternative 2 ##########
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
import graphviz

# basic dataframe
dataframe = pd.read_csv(csv_file)

# cleaned up dataframe
dataframe.columns = dataframe.columns.str.strip()

# one hot encoded dataframe
ohe = pd.get_dummies(dataframe, columns=dataframe.columns)

# decision tree classifier
clf = tree.DecisionTreeClassifier(max_leaf_nodes=3)
clf = clf.fit(ohe, ohe['income_ &gt;50K'])

# display for user
dot_data = tree.export_graphviz(clf, out_file=None,
                            feature_names=ohe.columns, class_names='income_ &gt;50K', 
filled=True)

graph = graphviz.Source(dot_data, format='png')

print(graph)
</code></pre>
<p>I am unsure how to proceed...</p>
",25,0,1,4,python-3.x;scikit-learn;pycharm;graphviz,2022-07-12 05:19:55,2022-07-12 05:19:55,2022-07-12 06:37:07,this code runs without any technical error but whether i run it with none for out_file or give it a png file name it doesnt make the decision tree visual  instead it just prints it like this  here is my code i am unsure how to proceed   ,how to get pycharm to do graphviz,code runs without technical error whether run none out_file give png file name doesnt make decision tree visual instead prints like code unsure proceed,get pycharm graphviz,get pycharm graphvizcode runs without technical error whether run none out_file give png file name doesnt make decision tree visual instead prints like code unsure proceed,"['get', 'pycharm', 'graphvizcode', 'runs', 'without', 'technical', 'error', 'whether', 'run', 'none', 'out_file', 'give', 'png', 'file', 'name', 'doesnt', 'make', 'decision', 'tree', 'visual', 'instead', 'prints', 'like', 'code', 'unsure', 'proceed']","['get', 'pycharm', 'graphvizcod', 'run', 'without', 'technic', 'error', 'whether', 'run', 'none', 'out_fil', 'give', 'png', 'file', 'name', 'doesnt', 'make', 'decis', 'tree', 'visual', 'instead', 'print', 'like', 'code', 'unsur', 'proceed']"
44,49,49,78259,648246,At what point does a config file become a programming language?,"<p>I have been mulling over config files and their relationship to code for a while now and depending on the day and direction of the wind my opinions seem to change.  More and more though I keep coming back to the realization I first had while learning Lisp: there is little difference between data and code.  This seems doubly true for config files.  When looked at in the right light a Perl script is little more than a config file for perl.  This tends to have fairly heavy consequences for tasks such as QA and divisions of labor like who should be responsible for changing config files.</p>

<p>The creep from config file to full fledged language is generally slow and seems to be driven by the desire to have a generic system.  Most projects seem to start out small with a few config items like where to write logs, where to look for data, user names and passwords, etc.  But then they start to grow: features start to be able to be turned on or off, the timings and order of operations start to be controlled, and, inevitably, someone wants to start adding logic to it (e.g. use 10 if the machine is X and 15 if the machine is Y).  At a certain point the config file becomes a domain specific language, and a poorly written one at that.</p>

<p>Now that I have rambled on to set the stage, here are my questions:</p>

<ol>
<li>What is the true purpose of a config
file?</li>
<li>Should an attempt be made to keep
config files simple?</li>
<li>Who should be responsible for making
changes to them (developers, users,
admins, etc.)?</li>
<li>Should they be source controlled
(see question 3)?</li>
</ol>

<p>As I said earlier my answers to these questions shift constantly, but right now I am thinking:</p>

<ol>
<li>to allow a non-programmers to change
large chunks of behaviour quickly</li>
<li>yes, anything that is not coarsely
grained should be in code</li>
<li>users should be responsible for
config files and programmers should
be responsible for a configuration
layer between config files and code
that gives more fine grained control
of the application</li>
<li>no, but the finer grained middle layer should be</li>
</ol>
",17125,19,101,4,configuration;programming-languages;configuration-files;config,2009-03-15 23:37:14,2009-03-15 23:37:14,2022-07-12 05:30:09,i have been mulling over config files and their relationship to code for a while now and depending on the day and direction of the wind my opinions seem to change   more and more though i keep coming back to the realization i first had while learning lisp  there is little difference between data and code   this seems doubly true for config files   when looked at in the right light a perl script is little more than a config file for perl   this tends to have fairly heavy consequences for tasks such as qa and divisions of labor like who should be responsible for changing config files  the creep from config file to full fledged language is generally slow and seems to be driven by the desire to have a generic system   most projects seem to start out small with a few config items like where to write logs  where to look for data  user names and passwords  etc   but then they start to grow  features start to be able to be turned on or off  the timings and order of operations start to be controlled  and  inevitably  someone wants to start adding logic to it  e g  use  if the machine is x and  if the machine is y    at a certain point the config file becomes a domain specific language  and a poorly written one at that  now that i have rambled on to set the stage  here are my questions  as i said earlier my answers to these questions shift constantly  but right now i am thinking ,at what point does a config file become a programming language ,mulling config files relationship code depending day direction wind opinions seem change though keep coming back realization first learning lisp little difference data code seems doubly true config files looked right light perl script little config file perl tends fairly heavy consequences tasks qa divisions labor like responsible changing config files creep config file full fledged language generally slow seems driven desire generic system projects seem start small config items like write logs look data user names passwords etc start grow features start able turned timings order operations start controlled inevitably someone wants start adding logic e g use machine x machine certain point config file becomes domain specific language poorly written one rambled set stage questions said earlier answers questions shift constantly right thinking,point config file become programming language,point config file become programming languagemulling config files relationship code depending day direction wind opinions seem change though keep coming back realization first learning lisp little difference data code seems doubly true config files looked right light perl script little config file perl tends fairly heavy consequences tasks qa divisions labor like responsible changing config files creep config file full fledged language generally slow seems driven desire generic system projects seem start small config items like write logs look data user names passwords etc start grow features start able turned timings order operations start controlled inevitably someone wants start adding logic e g use machine x machine certain point config file becomes domain specific language poorly written one rambled set stage questions said earlier answers questions shift constantly right thinking,"['point', 'config', 'file', 'become', 'programming', 'languagemulling', 'config', 'files', 'relationship', 'code', 'depending', 'day', 'direction', 'wind', 'opinions', 'seem', 'change', 'though', 'keep', 'coming', 'back', 'realization', 'first', 'learning', 'lisp', 'little', 'difference', 'data', 'code', 'seems', 'doubly', 'true', 'config', 'files', 'looked', 'right', 'light', 'perl', 'script', 'little', 'config', 'file', 'perl', 'tends', 'fairly', 'heavy', 'consequences', 'tasks', 'qa', 'divisions', 'labor', 'like', 'responsible', 'changing', 'config', 'files', 'creep', 'config', 'file', 'full', 'fledged', 'language', 'generally', 'slow', 'seems', 'driven', 'desire', 'generic', 'system', 'projects', 'seem', 'start', 'small', 'config', 'items', 'like', 'write', 'logs', 'look', 'data', 'user', 'names', 'passwords', 'etc', 'start', 'grow', 'features', 'start', 'able', 'turned', 'timings', 'order', 'operations', 'start', 'controlled', 'inevitably', 'someone', 'wants', 'start', 'adding', 'logic', 'e', 'g', 'use', 'machine', 'x', 'machine', 'certain', 'point', 'config', 'file', 'becomes', 'domain', 'specific', 'language', 'poorly', 'written', 'one', 'rambled', 'set', 'stage', 'questions', 'said', 'earlier', 'answers', 'questions', 'shift', 'constantly', 'right', 'thinking']","['point', 'config', 'file', 'becom', 'program', 'languagemul', 'config', 'file', 'relationship', 'code', 'depend', 'day', 'direct', 'wind', 'opinion', 'seem', 'chang', 'though', 'keep', 'come', 'back', 'realiz', 'first', 'learn', 'lisp', 'littl', 'differ', 'data', 'code', 'seem', 'doubli', 'true', 'config', 'file', 'look', 'right', 'light', 'perl', 'script', 'littl', 'config', 'file', 'perl', 'tend', 'fairli', 'heavi', 'consequ', 'task', 'qa', 'divis', 'labor', 'like', 'respons', 'chang', 'config', 'file', 'creep', 'config', 'file', 'full', 'fledg', 'languag', 'gener', 'slow', 'seem', 'driven', 'desir', 'gener', 'system', 'project', 'seem', 'start', 'small', 'config', 'item', 'like', 'write', 'log', 'look', 'data', 'user', 'name', 'password', 'etc', 'start', 'grow', 'featur', 'start', 'abl', 'turn', 'time', 'order', 'oper', 'start', 'control', 'inevit', 'someon', 'want', 'start', 'ad', 'logic', 'e', 'g', 'use', 'machin', 'x', 'machin', 'certain', 'point', 'config', 'file', 'becom', 'domain', 'specif', 'languag', 'poorli', 'written', 'one', 'rambl', 'set', 'stage', 'question', 'said', 'earlier', 'answer', 'question', 'shift', 'constantli', 'right', 'think']"
45,50,50,2451456,72409120,"Unity&#39;s ml-agents assets throw warnings and errors [PushBlockWithInput, Actuator, Barracuda]","<h2>The Problem</h2>
<p>I'm trying to work with <a href=""https://unity.com/products/machine-learning-agents"" rel=""nofollow noreferrer"">Unity Machine Learning Agents</a> and encountered problems during the setup. When I try to import the assets from <a href=""https://github.com/Unity-Technologies/ml-agents"" rel=""nofollow noreferrer"">Unity's ml-agents git</a> into Unity, I get many warnings and errors inside Unity. For the purpose of context, I'm at the very beginning of learning Unity, so I don't know if the errors are due to the ml-agents package or user error from my side in how to set everything up.</p>
<h2>The errors and warnings</h2>
<p>Instructions to create a first test scene with assets from Unity's ml-agents git suggest making a new 3D project in Unity and drag and drop the folder <code>projects/assets/ml-agents</code> into the project's assets. At this point, Unity is showing many errors and warnings in the Terminal. It still has the examples in the assets but every element in the scene is full of warnings.</p>
<p>according to these tutorials from 2020 by dragging and dropping the assets into Unity <a href=""https://www.youtube.com/watch?v=GhS6-vvhOy8&amp;t=469s"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://www.youtube.com/watch?v=_9aPZH6pyA8&amp;t=86s"" rel=""nofollow noreferrer"">[2]</a>, I subsequently
<a href=""https://i.stack.imgur.com/FIVsQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FIVsQ.jpg"" alt=""enter image description here"" /></a></p>
<p>In my case the majority of the warnings seem to come from <code>PushBlockWithInput</code>, <code>PushblockActions</code> and <code>PushBlockWithInputPlayerController</code> missing <code>UnityEngine.InputSystem</code> and <code>Unity.MLAgents.Extensions.Input</code> with the note &quot;(are you missing a using directive or an assembly reference?)&quot;. This; however, did not happen in the aforementioned tutorials.</p>
<p>Although they make the majority of errors, they are not exclusively about assembly references. Other errors, which may or may not be about assembly references, are</p>
<ul>
<li>error CS0115: 'Match3Board.GetCurrentBoardSize()': no suitable method found to override</li>
<li>error CS0535: 'SensorBase' does not implement interface member 'ISensor.GetCompressionType()'</li>
</ul>
<p><a href=""https://i.stack.imgur.com/dH4Gj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dH4Gj.jpg"" alt=""enter image description here"" /></a></p>
<h2>The things I've tried</h2>
<h3>Python</h3>
<p>I've followed the <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a> from Unity's ml-agents git and was successful in installing a Python 3.7 environment with Anaconda, PyTorch with Cuda, and the Unity's ml-agents python package via pip. When trying to verify the ml-agents python package works with <code>mlagents-learn --help</code>, I first got an exception but could resolve that by updating <code>protobuf==3.20.1</code> as per <a href=""https://discuss.streamlit.io/t/typeerror-descriptors-cannot-not-be-created-directly/25639/3"" rel=""nofollow noreferrer"">suggestion from a forum</a> (just mentioning this in case it is relevant).</p>
<h3>Unity</h3>
<p>I downloaded the C# package from Unity's package manager and tried it for several versions (<code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.1.0 (Preview) -- lastest</code>). After which I'm able to select ML-Agents from the 'Add Component' menu in the Inspector.</p>
<p>I've also tried to create a new Unity project with the <code>ml-agent package 1.9.1 (Preview)</code> with the right Barracuda version, and the release 19 branch of Unity's ml-agents git, without success (now it's 53 warnings and 70 errors). Now also the Actuators are not found, which seems to be a more common problem on its own.</p>
<h3>VS Code</h3>
<p>I'm using VS Code as opposed to VS as was recommended <a href=""https://stackoverflow.com/a/63301180/2451456"">here</a>. I downloaded .Net version 6.0.301 and checked it was installed with 'dotnet --info'. In the VS Code's extension manager, I installed the extensions <a href=""https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp"" rel=""nofollow noreferrer"">C#</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner"" rel=""nofollow noreferrer"">Code Runner</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=Tobiah.unity-tools"" rel=""nofollow noreferrer"">Unity Tools
</a>, <a href=""https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug"" rel=""nofollow noreferrer"">Debugger for Unity</a>.</p>
<h3>Git-Repository</h3>
<p>I have also switched from Unity's ml-agents git's main branch to the <a href=""https://github.com/Unity-Technologies/ml-agents/tree/release_19_branch"" rel=""nofollow noreferrer"">release 19 branch</a> and also tried other versions of the Barracuda package, e.g. <code>Version 3.0.0</code>, which seems to remove the warnings, but not the errors and instead gives these notifications:</p>
<p><a href=""https://i.stack.imgur.com/gQZqW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gQZqW.jpg"" alt=""enter image description here"" /></a></p>
<p>However, warnings still show up in the assets' settings:</p>
<p><a href=""https://i.stack.imgur.com/rQWDb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rQWDb.jpg"" alt=""enter image description here"" /></a></p>
<h2>Course: ML-Agents: Hummingbirds</h2>
<p>I successfully completed the <a href=""https://learn.unity.com/course/ml-agents-hummingbirds?uv=2019.3"" rel=""nofollow noreferrer"">ML-Agents: Hummingbirds</a>. This course doesn't use any of the assets from the ML-agents Git repository. Although, assets for this course are downloaded and added the same way, without issues. This lets me assume that the general setup for ML-agents is working but I specifically can't import the assets.</p>
<h2>My setup</h2>
<ul>
<li>I'm working on a machine with Windows 11</li>
<li><code>Unity Version is 2020.3.32f1 Personal &lt;DX11&gt;</code></li>
<li>The Unity <code>ml-agent package</code> was tried with <code>1.0.8 (Verified), 1.9.1 (Preview), 2.0.1, and 2.0.2 (Preview)</code></li>
<li>The Unity <code>ML Agents Extensions</code> package 0.6.1 (preview)</li>
<li>Python Version is, as per <a href=""https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md"" rel=""nofollow noreferrer"">instructions</a>, <code>3.7 with an Anaconda virtual environment</code></li>
<li>Unity's ml-agents git is <code>main</code>, as well as <code>release_19_branch</code></li>
<li>C# editor would be <code>Visual Studio Code 1.67.2</code></li>
<li>DotNet Version: 6.0.301</li>
</ul>
<h2>Things I found out so far</h2>
<p>This problem seems to be somewhat common, I've found several variations of similar problems over a couple of years, some more specific to the <a href=""https://stackoverflow.com/questions/64085348/unity-ml-agents-package-manager-is-not-importing-actuator-script"">Actuators</a> missing, some more <a href=""https://github.com/Unity-Technologies/ml-agents/issues/3027"" rel=""nofollow noreferrer"">general</a>. Some are posting about <a href=""https://forum.unity.com/threads/cannot-find-unityengine-inputsystem.807645/"" rel=""nofollow noreferrer"">problems with the InputSystem</a> as well, but seemingly different solutions and mixed reactions to the solutions.</p>
<p>There are various suggestions, about version changes for Unity, the ml-agents package, and Visual Studio Code. Other solutions involve downloading additional packages in Visual Studio or Unity. Some suggest editing scripts within the cloned git repository. Although most of these threads are from the past 2 years. I've spent two days trying to set this up and fix this and am just about to give up on the ml-agents package. A shame the installation process for a seemingly great resource seems so infeasible. I'd appreciate further suggestions or directions on additional resources on how to set up this package.</p>
",147,1,2,4,python;c#;unity3d;ml-agent,2022-05-27 23:02:29,2022-05-27 23:02:29,2022-07-11 22:28:00,i m trying to work with  and encountered problems during the setup  when i try to import the assets from  into unity  i get many warnings and errors inside unity  for the purpose of context  i m at the very beginning of learning unity  so i don t know if the errors are due to the ml agents package or user error from my side in how to set everything up  instructions to create a first test scene with assets from unity s ml agents git suggest making a new d project in unity and drag and drop the folder projects assets ml agents into the project s assets  at this point  unity is showing many errors and warnings in the terminal  it still has the examples in the assets but every element in the scene is full of warnings  in my case the majority of the warnings seem to come from pushblockwithinput  pushblockactions and pushblockwithinputplayercontroller missing unityengine inputsystem and unity mlagents extensions input with the note   are you missing a using directive or an assembly reference     this  however  did not happen in the aforementioned tutorials  although they make the majority of errors  they are not exclusively about assembly references  other errors  which may or may not be about assembly references  are  i ve followed the  from unity s ml agents git and was successful in installing a python   environment with anaconda  pytorch with cuda  and the unity s ml agents python package via pip  when trying to verify the ml agents python package works with mlagents learn   help  i first got an exception but could resolve that by updating protobuf     as per   just mentioning this in case it is relevant   i downloaded the c  package from unity s package manager and tried it for several versions      verified       preview       and     preview     lastest   after which i m able to select ml agents from the  add component  menu in the inspector  i ve also tried to create a new unity project with the ml agent package     preview  with the right barracuda version  and the release  branch of unity s ml agents git  without success  now it s  warnings and  errors   now also the actuators are not found  which seems to be a more common problem on its own  i have also switched from unity s ml agents git s main branch to the  and also tried other versions of the barracuda package  e g  version     which seems to remove the warnings  but not the errors and instead gives these notifications   however  warnings still show up in the assets  settings   i successfully completed the   this course doesn t use any of the assets from the ml agents git repository  although  assets for this course are downloaded and added the same way  without issues  this lets me assume that the general setup for ml agents is working but i specifically can t import the assets  this problem seems to be somewhat common  i ve found several variations of similar problems over a couple of years  some more specific to the  missing  some more   some are posting about  as well  but seemingly different solutions and mixed reactions to the solutions  there are various suggestions  about version changes for unity  the ml agents package  and visual studio code  other solutions involve downloading additional packages in visual studio or unity  some suggest editing scripts within the cloned git repository  although most of these threads are from the past  years  i ve spent two days trying to set this up and fix this and am just about to give up on the ml agents package  a shame the installation process for a seemingly great resource seems so infeasible  i d appreciate further suggestions or directions on additional resources on how to set up this package ,unity   s ml agents assets throw warnings and errors  pushblockwithinput  actuator  barracuda ,trying work encountered problems setup try import assets unity get many warnings errors inside unity purpose context beginning learning unity know errors due ml agents package user error side set everything instructions create first test scene assets unity ml agents git suggest making project unity drag drop folder projects assets ml agents project assets point unity showing many errors warnings terminal still examples assets every element scene full warnings case majority warnings seem come pushblockwithinput pushblockactions pushblockwithinputplayercontroller missing unityengine inputsystem unity mlagents extensions input note missing using directive assembly reference however happen aforementioned tutorials although make majority errors exclusively assembly references errors may may assembly references followed unity ml agents git successful installing python environment anaconda pytorch cuda unity ml agents python package via pip trying verify ml agents python package works mlagents learn help first got exception could resolve updating protobuf per mentioning case relevant downloaded c package unity package manager tried several versions verified preview preview lastest able select ml agents component menu inspector also tried create unity project ml agent package preview right barracuda version release branch unity ml agents git without success warnings errors also actuators found seems common problem also switched unity ml agents git main branch also tried versions barracuda package e g version seems remove warnings errors instead gives notifications however warnings still show assets settings successfully completed course use assets ml agents git repository although assets course downloaded added way without issues lets assume general setup ml agents working specifically import assets problem seems somewhat common found several variations similar problems couple years specific missing posting well seemingly different solutions mixed reactions solutions various suggestions version changes unity ml agents package visual studio code solutions involve downloading additional packages visual studio unity suggest editing scripts within cloned git repository although threads past years spent two days trying set fix give ml agents package shame installation process seemingly great resource seems infeasible appreciate suggestions directions additional resources set package,unity ml agents assets throw warnings errors pushblockwithinput actuator barracuda,unity ml agents assets throw warnings errors pushblockwithinput actuator barracudatrying work encountered problems setup try import assets unity get many warnings errors inside unity purpose context beginning learning unity know errors due ml agents package user error side set everything instructions create first test scene assets unity ml agents git suggest making project unity drag drop folder projects assets ml agents project assets point unity showing many errors warnings terminal still examples assets every element scene full warnings case majority warnings seem come pushblockwithinput pushblockactions pushblockwithinputplayercontroller missing unityengine inputsystem unity mlagents extensions input note missing using directive assembly reference however happen aforementioned tutorials although make majority errors exclusively assembly references errors may may assembly references followed unity ml agents git successful installing python environment anaconda pytorch cuda unity ml agents python package via pip trying verify ml agents python package works mlagents learn help first got exception could resolve updating protobuf per mentioning case relevant downloaded c package unity package manager tried several versions verified preview preview lastest able select ml agents component menu inspector also tried create unity project ml agent package preview right barracuda version release branch unity ml agents git without success warnings errors also actuators found seems common problem also switched unity ml agents git main branch also tried versions barracuda package e g version seems remove warnings errors instead gives notifications however warnings still show assets settings successfully completed course use assets ml agents git repository although assets course downloaded added way without issues lets assume general setup ml agents working specifically import assets problem seems somewhat common found several variations similar problems couple years specific missing posting well seemingly different solutions mixed reactions solutions various suggestions version changes unity ml agents package visual studio code solutions involve downloading additional packages visual studio unity suggest editing scripts within cloned git repository although threads past years spent two days trying set fix give ml agents package shame installation process seemingly great resource seems infeasible appreciate suggestions directions additional resources set package,"['unity', 'ml', 'agents', 'assets', 'throw', 'warnings', 'errors', 'pushblockwithinput', 'actuator', 'barracudatrying', 'work', 'encountered', 'problems', 'setup', 'try', 'import', 'assets', 'unity', 'get', 'many', 'warnings', 'errors', 'inside', 'unity', 'purpose', 'context', 'beginning', 'learning', 'unity', 'know', 'errors', 'due', 'ml', 'agents', 'package', 'user', 'error', 'side', 'set', 'everything', 'instructions', 'create', 'first', 'test', 'scene', 'assets', 'unity', 'ml', 'agents', 'git', 'suggest', 'making', 'project', 'unity', 'drag', 'drop', 'folder', 'projects', 'assets', 'ml', 'agents', 'project', 'assets', 'point', 'unity', 'showing', 'many', 'errors', 'warnings', 'terminal', 'still', 'examples', 'assets', 'every', 'element', 'scene', 'full', 'warnings', 'case', 'majority', 'warnings', 'seem', 'come', 'pushblockwithinput', 'pushblockactions', 'pushblockwithinputplayercontroller', 'missing', 'unityengine', 'inputsystem', 'unity', 'mlagents', 'extensions', 'input', 'note', 'missing', 'using', 'directive', 'assembly', 'reference', 'however', 'happen', 'aforementioned', 'tutorials', 'although', 'make', 'majority', 'errors', 'exclusively', 'assembly', 'references', 'errors', 'may', 'may', 'assembly', 'references', 'followed', 'unity', 'ml', 'agents', 'git', 'successful', 'installing', 'python', 'environment', 'anaconda', 'pytorch', 'cuda', 'unity', 'ml', 'agents', 'python', 'package', 'via', 'pip', 'trying', 'verify', 'ml', 'agents', 'python', 'package', 'works', 'mlagents', 'learn', 'help', 'first', 'got', 'exception', 'could', 'resolve', 'updating', 'protobuf', 'per', 'mentioning', 'case', 'relevant', 'downloaded', 'c', 'package', 'unity', 'package', 'manager', 'tried', 'several', 'versions', 'verified', 'preview', 'preview', 'lastest', 'able', 'select', 'ml', 'agents', 'component', 'menu', 'inspector', 'also', 'tried', 'create', 'unity', 'project', 'ml', 'agent', 'package', 'preview', 'right', 'barracuda', 'version', 'release', 'branch', 'unity', 'ml', 'agents', 'git', 'without', 'success', 'warnings', 'errors', 'also', 'actuators', 'found', 'seems', 'common', 'problem', 'also', 'switched', 'unity', 'ml', 'agents', 'git', 'main', 'branch', 'also', 'tried', 'versions', 'barracuda', 'package', 'e', 'g', 'version', 'seems', 'remove', 'warnings', 'errors', 'instead', 'gives', 'notifications', 'however', 'warnings', 'still', 'show', 'assets', 'settings', 'successfully', 'completed', 'course', 'use', 'assets', 'ml', 'agents', 'git', 'repository', 'although', 'assets', 'course', 'downloaded', 'added', 'way', 'without', 'issues', 'lets', 'assume', 'general', 'setup', 'ml', 'agents', 'working', 'specifically', 'import', 'assets', 'problem', 'seems', 'somewhat', 'common', 'found', 'several', 'variations', 'similar', 'problems', 'couple', 'years', 'specific', 'missing', 'posting', 'well', 'seemingly', 'different', 'solutions', 'mixed', 'reactions', 'solutions', 'various', 'suggestions', 'version', 'changes', 'unity', 'ml', 'agents', 'package', 'visual', 'studio', 'code', 'solutions', 'involve', 'downloading', 'additional', 'packages', 'visual', 'studio', 'unity', 'suggest', 'editing', 'scripts', 'within', 'cloned', 'git', 'repository', 'although', 'threads', 'past', 'years', 'spent', 'two', 'days', 'trying', 'set', 'fix', 'give', 'ml', 'agents', 'package', 'shame', 'installation', 'process', 'seemingly', 'great', 'resource', 'seems', 'infeasible', 'appreciate', 'suggestions', 'directions', 'additional', 'resources', 'set', 'package']","['uniti', 'ml', 'agent', 'asset', 'throw', 'warn', 'error', 'pushblockwithinput', 'actuat', 'barracudatri', 'work', 'encount', 'problem', 'setup', 'tri', 'import', 'asset', 'uniti', 'get', 'mani', 'warn', 'error', 'insid', 'uniti', 'purpos', 'context', 'begin', 'learn', 'uniti', 'know', 'error', 'due', 'ml', 'agent', 'packag', 'user', 'error', 'side', 'set', 'everyth', 'instruct', 'creat', 'first', 'test', 'scene', 'asset', 'uniti', 'ml', 'agent', 'git', 'suggest', 'make', 'project', 'uniti', 'drag', 'drop', 'folder', 'project', 'asset', 'ml', 'agent', 'project', 'asset', 'point', 'uniti', 'show', 'mani', 'error', 'warn', 'termin', 'still', 'exampl', 'asset', 'everi', 'element', 'scene', 'full', 'warn', 'case', 'major', 'warn', 'seem', 'come', 'pushblockwithinput', 'pushblockact', 'pushblockwithinputplayercontrol', 'miss', 'unityengin', 'inputsystem', 'uniti', 'mlagent', 'extens', 'input', 'note', 'miss', 'use', 'direct', 'assembl', 'refer', 'howev', 'happen', 'aforement', 'tutori', 'although', 'make', 'major', 'error', 'exclus', 'assembl', 'refer', 'error', 'may', 'may', 'assembl', 'refer', 'follow', 'uniti', 'ml', 'agent', 'git', 'success', 'instal', 'python', 'environ', 'anaconda', 'pytorch', 'cuda', 'uniti', 'ml', 'agent', 'python', 'packag', 'via', 'pip', 'tri', 'verifi', 'ml', 'agent', 'python', 'packag', 'work', 'mlagent', 'learn', 'help', 'first', 'got', 'except', 'could', 'resolv', 'updat', 'protobuf', 'per', 'mention', 'case', 'relev', 'download', 'c', 'packag', 'uniti', 'packag', 'manag', 'tri', 'sever', 'version', 'verifi', 'preview', 'preview', 'lastest', 'abl', 'select', 'ml', 'agent', 'compon', 'menu', 'inspector', 'also', 'tri', 'creat', 'uniti', 'project', 'ml', 'agent', 'packag', 'preview', 'right', 'barracuda', 'version', 'releas', 'branch', 'uniti', 'ml', 'agent', 'git', 'without', 'success', 'warn', 'error', 'also', 'actuat', 'found', 'seem', 'common', 'problem', 'also', 'switch', 'uniti', 'ml', 'agent', 'git', 'main', 'branch', 'also', 'tri', 'version', 'barracuda', 'packag', 'e', 'g', 'version', 'seem', 'remov', 'warn', 'error', 'instead', 'give', 'notif', 'howev', 'warn', 'still', 'show', 'asset', 'set', 'success', 'complet', 'cours', 'use', 'asset', 'ml', 'agent', 'git', 'repositori', 'although', 'asset', 'cours', 'download', 'ad', 'way', 'without', 'issu', 'let', 'assum', 'gener', 'setup', 'ml', 'agent', 'work', 'specif', 'import', 'asset', 'problem', 'seem', 'somewhat', 'common', 'found', 'sever', 'variat', 'similar', 'problem', 'coupl', 'year', 'specif', 'miss', 'post', 'well', 'seemingli', 'differ', 'solut', 'mix', 'reaction', 'solut', 'variou', 'suggest', 'version', 'chang', 'uniti', 'ml', 'agent', 'packag', 'visual', 'studio', 'code', 'solut', 'involv', 'download', 'addit', 'packag', 'visual', 'studio', 'uniti', 'suggest', 'edit', 'script', 'within', 'clone', 'git', 'repositori', 'although', 'thread', 'past', 'year', 'spent', 'two', 'day', 'tri', 'set', 'fix', 'give', 'ml', 'agent', 'packag', 'shame', 'instal', 'process', 'seemingli', 'great', 'resourc', 'seem', 'infeas', 'appreci', 'suggest', 'direct', 'addit', 'resourc', 'set', 'packag']"
46,51,51,345660,7044808,"Using R to download gzipped data file, extract, and import data","<p>A follow up to <a href=""https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data"">this question</a>: How can I download and uncompress a gzipped file using R?  For example (from <a href=""http://archive.ics.uci.edu/ml/datasets"" rel=""noreferrer"">the UCI Machine Learning Repository</a>), I have a <a href=""http://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29"" rel=""noreferrer"">file of insurance data</a>.  How can I download it using R?</p>

<p>Here is the data url: <code>http://archive.ics.uci.edu/ml/databases/tic/tic.tar.gz</code>.</p>
",8732,4,11,3,r;zip;connection,2011-08-13 00:10:45,2011-08-13 00:10:45,2022-07-11 21:36:21,a follow up to   how can i download and uncompress a gzipped file using r   for example  from    i have a    how can i download it using r  here is the data url  http   archive ics uci edu ml databases tic tic tar gz ,using r to download gzipped data file  extract  and import data,follow download uncompress gzipped file using r example download using r data url http archive ics uci edu ml databases tic tic tar gz,using r download gzipped data file extract import data,using r download gzipped data file extract import datafollow download uncompress gzipped file using r example download using r data url http archive ics uci edu ml databases tic tic tar gz,"['using', 'r', 'download', 'gzipped', 'data', 'file', 'extract', 'import', 'datafollow', 'download', 'uncompress', 'gzipped', 'file', 'using', 'r', 'example', 'download', 'using', 'r', 'data', 'url', 'http', 'archive', 'ics', 'uci', 'edu', 'ml', 'databases', 'tic', 'tic', 'tar', 'gz']","['use', 'r', 'download', 'gzip', 'data', 'file', 'extract', 'import', 'datafollow', 'download', 'uncompress', 'gzip', 'file', 'use', 'r', 'exampl', 'download', 'use', 'r', 'data', 'url', 'http', 'archiv', 'ic', 'uci', 'edu', 'ml', 'databas', 'tic', 'tic', 'tar', 'gz']"
47,52,52,14415252,72938511,Detect a logo and compare it with original logo with percentage of matching,"<p>I need to make a machine learning model which will detect if a particular image contains the logo, and if it contains the logo then up to what percentage it matches with the original logo.</p>
<ol>
<li>Which libraries should I use?</li>
<li>Which algorithms do I need to learn?</li>
<li>Which algorithms and libraries will be quicker to learn for this
purpose?</li>
</ol>
",40,1,-4,5,python;tensorflow;machine-learning;image-processing;deep-learning,2022-07-11 17:51:37,2022-07-11 17:51:37,2022-07-11 20:21:51,i need to make a machine learning model which will detect if a particular image contains the logo  and if it contains the logo then up to what percentage it matches with the original logo ,detect a logo and compare it with original logo with percentage of matching,need make machine learning model detect particular image contains logo contains logo percentage matches original logo,detect logo compare original logo percentage matching,detect logo compare original logo percentage matchingneed make machine learning model detect particular image contains logo contains logo percentage matches original logo,"['detect', 'logo', 'compare', 'original', 'logo', 'percentage', 'matchingneed', 'make', 'machine', 'learning', 'model', 'detect', 'particular', 'image', 'contains', 'logo', 'contains', 'logo', 'percentage', 'matches', 'original', 'logo']","['detect', 'logo', 'compar', 'origin', 'logo', 'percentag', 'matchingne', 'make', 'machin', 'learn', 'model', 'detect', 'particular', 'imag', 'contain', 'logo', 'contain', 'logo', 'percentag', 'match', 'origin', 'logo']"
48,53,53,17957100,72938442,How do I return all pages from a rest API request without manually specifying the page number?,"<p>I'm retrieving data from a paginated API and converting it to .JSON format and I'd like to retrieve all pages in the response, without having to specify the page number in the URL. The API accepts page number and results per page (max. 250) as inputs.</p>
<p>I understand that the typical solution is to loop through pages using a key that specifies the address of the next page. However, it appears as though this API doesn't include a next page parameter in the output (see example response below). I can only think that the last page (i.e. total pages) parameter could be useful here? How can I scrape all of the pages without specifying the page number?</p>
<p><strong>My script:</strong></p>
<pre><code>  import requests
  import json

  url = &quot;https://api-v2.pitchbook.com/deals/search?keywords=machine learning accelerator&amp;perPage=250&quot;

  payload={}
  headers = {
      'Authorization': 'PB-Token 1234567'
   }

  response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload)

  data = response.json()

  print(data)                                                                                                                                                                                 
</code></pre>
<p><strong>Example response</strong></p>
<p>{'stats': {'total': 2, 'perPage': 250, 'page': 1, 'lastPage': 1}, 'items': [{'dealId': '98982-
28T', 'companyId': '162120-79', 'companyName': 'companyA'}, {'dealId': '112532-05T',
'companyId': '233527-87', 'companyName': 'companyB'}]}</p>
",27,1,0,5,python;json;loops;rest;pagination,2022-07-11 17:46:07,2022-07-11 17:46:07,2022-07-11 19:05:55,i m retrieving data from a paginated api and converting it to  json format and i d like to retrieve all pages in the response  without having to specify the page number in the url  the api accepts page number and results per page  max    as inputs  i understand that the typical solution is to loop through pages using a key that specifies the address of the next page  however  it appears as though this api doesn t include a next page parameter in the output  see example response below   i can only think that the last page  i e  total pages  parameter could be useful here  how can i scrape all of the pages without specifying the page number  my script  example response,how do i return all pages from a rest api request without manually specifying the page number ,retrieving data paginated api converting json format like retrieve pages response without specify page number url api accepts page number results per page max inputs understand typical solution loop pages using key specifies address next page however appears though api include next page parameter output see example response think last page e total pages parameter could useful scrape pages without specifying page number script example response,return pages rest api request without manually specifying page number,return pages rest api request without manually specifying page numberretrieving data paginated api converting json format like retrieve pages response without specify page number url api accepts page number results per page max inputs understand typical solution loop pages using key specifies address next page however appears though api include next page parameter output see example response think last page e total pages parameter could useful scrape pages without specifying page number script example response,"['return', 'pages', 'rest', 'api', 'request', 'without', 'manually', 'specifying', 'page', 'numberretrieving', 'data', 'paginated', 'api', 'converting', 'json', 'format', 'like', 'retrieve', 'pages', 'response', 'without', 'specify', 'page', 'number', 'url', 'api', 'accepts', 'page', 'number', 'results', 'per', 'page', 'max', 'inputs', 'understand', 'typical', 'solution', 'loop', 'pages', 'using', 'key', 'specifies', 'address', 'next', 'page', 'however', 'appears', 'though', 'api', 'include', 'next', 'page', 'parameter', 'output', 'see', 'example', 'response', 'think', 'last', 'page', 'e', 'total', 'pages', 'parameter', 'could', 'useful', 'scrape', 'pages', 'without', 'specifying', 'page', 'number', 'script', 'example', 'response']","['return', 'page', 'rest', 'api', 'request', 'without', 'manual', 'specifi', 'page', 'numberretriev', 'data', 'pagin', 'api', 'convert', 'json', 'format', 'like', 'retriev', 'page', 'respons', 'without', 'specifi', 'page', 'number', 'url', 'api', 'accept', 'page', 'number', 'result', 'per', 'page', 'max', 'input', 'understand', 'typic', 'solut', 'loop', 'page', 'use', 'key', 'specifi', 'address', 'next', 'page', 'howev', 'appear', 'though', 'api', 'includ', 'next', 'page', 'paramet', 'output', 'see', 'exampl', 'respons', 'think', 'last', 'page', 'e', 'total', 'page', 'paramet', 'could', 'use', 'scrape', 'page', 'without', 'specifi', 'page', 'number', 'script', 'exampl', 'respons']"
49,54,54,19436129,72939406,SQL Server Vector Index,"<p>I would like to create a vector index for a database table in SQL Server. In my case, I have 256-dimensional vectors (numerical representations of the text contents of each row, derived from some NLP methods). However, SQL Server unfortunately does not seem to include array functionality (numerical vector data type).</p>
<p>Some potential solutions are offered here: <a href=""https://www.sqlshack.com/implement-array-like-functionality-sql-server/"" rel=""nofollow noreferrer"">https://www.sqlshack.com/implement-array-like-functionality-sql-server/</a>. For instance, I could create a table with one column for each entry of the vector. Is there a more elegant (performant) way?</p>
<hr />
<p>Background: I am implementing a semantic search feature for my database using MS Machine Learning Services (Python), that is, I want to be able to compare vector representations of query texts to the index (using cosine similarity). I know that there is also a full-text search feature for SQL server. Currently, I do not use it because it provides too little control over the pre-processing of the rather messy text fields in my database.</p>
",27,0,0,4,sql-server;python-3.x;indexing;full-text-search,2022-07-11 19:01:21,2022-07-11 19:01:21,2022-07-11 19:01:21,i would like to create a vector index for a database table in sql server  in my case  i have  dimensional vectors  numerical representations of the text contents of each row  derived from some nlp methods   however  sql server unfortunately does not seem to include array functionality  numerical vector data type   some potential solutions are offered here    for instance  i could create a table with one column for each entry of the vector  is there a more elegant  performant  way  background  i am implementing a semantic search feature for my database using ms machine learning services  python   that is  i want to be able to compare vector representations of query texts to the index  using cosine similarity   i know that there is also a full text search feature for sql server  currently  i do not use it because it provides too little control over the pre processing of the rather messy text fields in my database ,sql server vector index,would like create vector index database table sql server case dimensional vectors numerical representations text contents row derived nlp methods however sql server unfortunately seem include array functionality numerical vector data type potential solutions offered instance could create table one column entry vector elegant performant way background implementing semantic search feature database using ms machine learning services python want able compare vector representations query texts index using cosine similarity know also full text search feature sql server currently use provides little control pre processing rather messy text fields database,sql server vector index,sql server vector indexwould like create vector index database table sql server case dimensional vectors numerical representations text contents row derived nlp methods however sql server unfortunately seem include array functionality numerical vector data type potential solutions offered instance could create table one column entry vector elegant performant way background implementing semantic search feature database using ms machine learning services python want able compare vector representations query texts index using cosine similarity know also full text search feature sql server currently use provides little control pre processing rather messy text fields database,"['sql', 'server', 'vector', 'indexwould', 'like', 'create', 'vector', 'index', 'database', 'table', 'sql', 'server', 'case', 'dimensional', 'vectors', 'numerical', 'representations', 'text', 'contents', 'row', 'derived', 'nlp', 'methods', 'however', 'sql', 'server', 'unfortunately', 'seem', 'include', 'array', 'functionality', 'numerical', 'vector', 'data', 'type', 'potential', 'solutions', 'offered', 'instance', 'could', 'create', 'table', 'one', 'column', 'entry', 'vector', 'elegant', 'performant', 'way', 'background', 'implementing', 'semantic', 'search', 'feature', 'database', 'using', 'ms', 'machine', 'learning', 'services', 'python', 'want', 'able', 'compare', 'vector', 'representations', 'query', 'texts', 'index', 'using', 'cosine', 'similarity', 'know', 'also', 'full', 'text', 'search', 'feature', 'sql', 'server', 'currently', 'use', 'provides', 'little', 'control', 'pre', 'processing', 'rather', 'messy', 'text', 'fields', 'database']","['sql', 'server', 'vector', 'indexwould', 'like', 'creat', 'vector', 'index', 'databas', 'tabl', 'sql', 'server', 'case', 'dimension', 'vector', 'numer', 'represent', 'text', 'content', 'row', 'deriv', 'nlp', 'method', 'howev', 'sql', 'server', 'unfortun', 'seem', 'includ', 'array', 'function', 'numer', 'vector', 'data', 'type', 'potenti', 'solut', 'offer', 'instanc', 'could', 'creat', 'tabl', 'one', 'column', 'entri', 'vector', 'eleg', 'perform', 'way', 'background', 'implement', 'semant', 'search', 'featur', 'databas', 'use', 'ms', 'machin', 'learn', 'servic', 'python', 'want', 'abl', 'compar', 'vector', 'represent', 'queri', 'text', 'index', 'use', 'cosin', 'similar', 'know', 'also', 'full', 'text', 'search', 'featur', 'sql', 'server', 'current', 'use', 'provid', 'littl', 'control', 'pre', 'process', 'rather', 'messi', 'text', 'field', 'databas']"
50,55,55,17668281,72938908,MS Azure Machine Learning - Notebook extremely slow in comparison to GC,"<p>I work with a jupyter notebook on different platforms.</p>
<p>The notebook includes a detectron2 algorithm.</p>
<p>The runtime for the training process on different platforms differs significantl (Same Code, Same parameters, same data)</p>
<p><strong>Google Colab</strong></p>
<ul>
<li>K80 GPU (Successfully used while training)</li>
<li>Data stored on Google Drive</li>
<li>Torch: 1.9.0+cu111</li>
<li><strong>Runtime: 4,5h</strong></li>
</ul>
<hr />
<p><strong>Microsoft Azure - Machine Learning</strong></p>
<ul>
<li>K80 GPU (Successfully used while training)</li>
<li>STANDARD_NC6</li>
<li>Data stored an Azure Files</li>
<li>Torch: 1.9.0+cu111</li>
<li><strong>Runtime: 15h</strong></li>
</ul>
<p>Why is the expensive Azure ML three times slower than free Google Colab and is there a possibility to improve that?</p>
",15,0,0,5,azure;deep-learning;jupyter-notebook;google-colaboratory;object-detection,2022-07-11 18:21:35,2022-07-11 18:21:35,2022-07-11 18:21:35,i work with a jupyter notebook on different platforms  the notebook includes a detectron algorithm  the runtime for the training process on different platforms differs significantl  same code  same parameters  same data  google colab microsoft azure   machine learning why is the expensive azure ml three times slower than free google colab and is there a possibility to improve that ,ms azure machine learning   notebook extremely slow in comparison to gc,work jupyter notebook different platforms notebook includes detectron algorithm runtime training process different platforms differs significantl code parameters data google colab microsoft azure machine learning expensive azure ml three times slower free google colab possibility improve,ms azure machine learning notebook extremely slow comparison gc,ms azure machine learning notebook extremely slow comparison gcwork jupyter notebook different platforms notebook includes detectron algorithm runtime training process different platforms differs significantl code parameters data google colab microsoft azure machine learning expensive azure ml three times slower free google colab possibility improve,"['ms', 'azure', 'machine', 'learning', 'notebook', 'extremely', 'slow', 'comparison', 'gcwork', 'jupyter', 'notebook', 'different', 'platforms', 'notebook', 'includes', 'detectron', 'algorithm', 'runtime', 'training', 'process', 'different', 'platforms', 'differs', 'significantl', 'code', 'parameters', 'data', 'google', 'colab', 'microsoft', 'azure', 'machine', 'learning', 'expensive', 'azure', 'ml', 'three', 'times', 'slower', 'free', 'google', 'colab', 'possibility', 'improve']","['ms', 'azur', 'machin', 'learn', 'notebook', 'extrem', 'slow', 'comparison', 'gcwork', 'jupyt', 'notebook', 'differ', 'platform', 'notebook', 'includ', 'detectron', 'algorithm', 'runtim', 'train', 'process', 'differ', 'platform', 'differ', 'significantl', 'code', 'paramet', 'data', 'googl', 'colab', 'microsoft', 'azur', 'machin', 'learn', 'expens', 'azur', 'ml', 'three', 'time', 'slower', 'free', 'googl', 'colab', 'possibl', 'improv']"
51,56,56,6151951,72911756,GCP Pipeline for ML Images in Cloud Storage,"<p>I am setting up a Machine Learning training instance on the GKE. My images live in Cloud Storage. How can I access those images without first downloading them onto the GKE? They are considered private (medical data), and so I do not want to risk them being exposed by transferring them to a GKE instance first.</p>
<p>Is there a recommended way to do this?</p>
",34,0,-1,3,machine-learning;google-cloud-storage;google-kubernetes-engine,2022-07-08 18:25:28,2022-07-08 18:25:28,2022-07-11 17:30:20,i am setting up a machine learning training instance on the gke  my images live in cloud storage  how can i access those images without first downloading them onto the gke  they are considered private  medical data   and so i do not want to risk them being exposed by transferring them to a gke instance first  is there a recommended way to do this ,gcp pipeline for ml images in cloud storage,setting machine learning training instance gke images live cloud storage access images without first downloading onto gke considered private medical data want risk exposed transferring gke instance first recommended way,gcp pipeline ml images cloud storage,gcp pipeline ml images cloud storagesetting machine learning training instance gke images live cloud storage access images without first downloading onto gke considered private medical data want risk exposed transferring gke instance first recommended way,"['gcp', 'pipeline', 'ml', 'images', 'cloud', 'storagesetting', 'machine', 'learning', 'training', 'instance', 'gke', 'images', 'live', 'cloud', 'storage', 'access', 'images', 'without', 'first', 'downloading', 'onto', 'gke', 'considered', 'private', 'medical', 'data', 'want', 'risk', 'exposed', 'transferring', 'gke', 'instance', 'first', 'recommended', 'way']","['gcp', 'pipelin', 'ml', 'imag', 'cloud', 'storageset', 'machin', 'learn', 'train', 'instanc', 'gke', 'imag', 'live', 'cloud', 'storag', 'access', 'imag', 'without', 'first', 'download', 'onto', 'gke', 'consid', 'privat', 'medic', 'data', 'want', 'risk', 'expos', 'transfer', 'gke', 'instanc', 'first', 'recommend', 'way']"
52,57,57,11143347,72935277,upload a pre-trained model locally into databricks,"<p>Is it possible to upload a pre-trained machine learning model that was trained on a different environment on databricks, and serve it? Or is it impossible on Databricks ?</p>
",38,1,0,3,machine-learning;databricks;mlflow,2022-07-11 13:20:31,2022-07-11 13:20:31,2022-07-11 17:28:05,is it possible to upload a pre trained machine learning model that was trained on a different environment on databricks  and serve it  or is it impossible on databricks  ,upload a pre trained model locally into databricks,possible upload pre trained machine learning model trained different environment databricks serve impossible databricks,upload pre trained model locally databricks,upload pre trained model locally databrickspossible upload pre trained machine learning model trained different environment databricks serve impossible databricks,"['upload', 'pre', 'trained', 'model', 'locally', 'databrickspossible', 'upload', 'pre', 'trained', 'machine', 'learning', 'model', 'trained', 'different', 'environment', 'databricks', 'serve', 'impossible', 'databricks']","['upload', 'pre', 'train', 'model', 'local', 'databricksposs', 'upload', 'pre', 'train', 'machin', 'learn', 'model', 'train', 'differ', 'environ', 'databrick', 'serv', 'imposs', 'databrick']"
53,58,58,8614508,72937870,"The method is not allowed for the requested URL - flask, model deployment","<p>I was trying to build a web application for my machine learning model deployment.</p>
<p>First I was testing the connection using the below code:</p>
<pre><code>import pickle  ## read .bin file
from distutils.log import debug
from pyexpat import model
from urllib import response
from flask import Flask, request, jsonify
from model_file.ml_model import predict_mpg

app = Flask(&quot;mpg_prediction&quot;)

@app.route('/', methods=['GET'])  ## test if the app is working
def ping():
    return &quot;Pinging Model Application!!&quot;
</code></pre>
<p>The above worked fine when I went to the server and the web app reads <code>Pinging Model Application!!</code></p>
<p>The problem was when I tried to &quot;fit&quot; my machine learning model to the app. I've pickled my trained model into the model.bin file and I used the below codes:</p>
<pre><code>@app.route('/', methods=['POST'])
def predict():
    vehicle_config = request.get_json()

    with open('./model_file/model.bin', 'rb') as f_in:
        model = pickle.load(f_in)
        f_in.close()

        predictions = predict_mpg(vehicle_config, model)

        response = {
            'mpg_predictions': list(predictions)
        }
        return jsonify()
</code></pre>
<p>After I ran the above codes, and tested the web app, I came across the error message:
<code>The method is not allowed for the requested URL</code></p>
<p>This is my first time using Flask, so I've no idea what went wrong.</p>
<p>I'd appreciate any help on this. Thanks in advance.</p>
",23,0,-1,4,python;flask;deployment;pickle,2022-07-11 17:00:42,2022-07-11 17:00:42,2022-07-11 17:00:42,i was trying to build a web application for my machine learning model deployment  first i was testing the connection using the below code  the above worked fine when i went to the server and the web app reads pinging model application   the problem was when i tried to  fit  my machine learning model to the app  i ve pickled my trained model into the model bin file and i used the below codes  this is my first time using flask  so i ve no idea what went wrong  i d appreciate any help on this  thanks in advance ,the method is not allowed for the requested url   flask  model deployment,trying build web application machine learning model deployment first testing connection using code worked fine went server web app reads pinging model application problem tried fit machine learning model app pickled trained model model bin file used codes first time using flask idea went wrong appreciate help thanks advance,method allowed requested url flask model deployment,method allowed requested url flask model deploymenttrying build web application machine learning model deployment first testing connection using code worked fine went server web app reads pinging model application problem tried fit machine learning model app pickled trained model model bin file used codes first time using flask idea went wrong appreciate help thanks advance,"['method', 'allowed', 'requested', 'url', 'flask', 'model', 'deploymenttrying', 'build', 'web', 'application', 'machine', 'learning', 'model', 'deployment', 'first', 'testing', 'connection', 'using', 'code', 'worked', 'fine', 'went', 'server', 'web', 'app', 'reads', 'pinging', 'model', 'application', 'problem', 'tried', 'fit', 'machine', 'learning', 'model', 'app', 'pickled', 'trained', 'model', 'model', 'bin', 'file', 'used', 'codes', 'first', 'time', 'using', 'flask', 'idea', 'went', 'wrong', 'appreciate', 'help', 'thanks', 'advance']","['method', 'allow', 'request', 'url', 'flask', 'model', 'deploymenttri', 'build', 'web', 'applic', 'machin', 'learn', 'model', 'deploy', 'first', 'test', 'connect', 'use', 'code', 'work', 'fine', 'went', 'server', 'web', 'app', 'read', 'ping', 'model', 'applic', 'problem', 'tri', 'fit', 'machin', 'learn', 'model', 'app', 'pickl', 'train', 'model', 'model', 'bin', 'file', 'use', 'code', 'first', 'time', 'use', 'flask', 'idea', 'went', 'wrong', 'appreci', 'help', 'thank', 'advanc']"
54,59,59,11619848,72936419,Getting error 404 in azure machine learning service,"<p>I am new to the cloud and i've been trying to follow a tutorial. I am trying to create a simple pipeline and understand how the <strong>Execute python script</strong> component works. So far this is what i'm trying to execute.</p>
<p><a href=""https://i.stack.imgur.com/Kkn9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kkn9z.png"" alt=""Pipeline structure"" /></a></p>
<p>This is the code i have within the execute block:</p>
<pre><code>import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
    dataframe1['Dollar/HP'] = dataframe1.price / dataframe1.horsepower
    return dataframe1
</code></pre>
<p>No matter what i try i get the same error and i can't find anything about it on the internet.</p>
<p><strong>Job preparation failed: HTTP Error 404: Unable to find snapshot with id: ...</strong></p>
",15,0,0,1,azure-pipelines,2022-07-11 15:04:48,2022-07-11 15:04:48,2022-07-11 15:04:48,i am new to the cloud and i ve been trying to follow a tutorial  i am trying to create a simple pipeline and understand how the execute python script component works  so far this is what i m trying to execute   this is the code i have within the execute block  no matter what i try i get the same error and i can t find anything about it on the internet  job preparation failed  http error   unable to find snapshot with id     ,getting error  in azure machine learning service,cloud trying follow tutorial trying create simple pipeline understand execute python script component works far trying execute code within execute block matter try get error find anything internet job preparation failed http error unable find snapshot id,getting error azure machine learning service,getting error azure machine learning servicecloud trying follow tutorial trying create simple pipeline understand execute python script component works far trying execute code within execute block matter try get error find anything internet job preparation failed http error unable find snapshot id,"['getting', 'error', 'azure', 'machine', 'learning', 'servicecloud', 'trying', 'follow', 'tutorial', 'trying', 'create', 'simple', 'pipeline', 'understand', 'execute', 'python', 'script', 'component', 'works', 'far', 'trying', 'execute', 'code', 'within', 'execute', 'block', 'matter', 'try', 'get', 'error', 'find', 'anything', 'internet', 'job', 'preparation', 'failed', 'http', 'error', 'unable', 'find', 'snapshot', 'id']","['get', 'error', 'azur', 'machin', 'learn', 'servicecloud', 'tri', 'follow', 'tutori', 'tri', 'creat', 'simpl', 'pipelin', 'understand', 'execut', 'python', 'script', 'compon', 'work', 'far', 'tri', 'execut', 'code', 'within', 'execut', 'block', 'matter', 'tri', 'get', 'error', 'find', 'anyth', 'internet', 'job', 'prepar', 'fail', 'http', 'error', 'unabl', 'find', 'snapshot', 'id']"
55,60,60,1361737,72927724,Why do I get &#39;c50 code called exit with value 1&#39; in R?,"<p>I am using RStudio 2021.09.0 &quot;Ghost Orchid&quot; Release for macOS.</p>
<p>I am learning to use to C5.0 algorithm in R. For this I am following <em>'Machine Learning in R'</em> by Brett Lantz. The dataset I am using is a modified version of one relating to loans obtained from a credit agency in Germany.</p>
<p>The data has no missing values, and no empty factor levels (this has caused the same error in other posts I have viewed). I have split the data into training and test tibbles using the <code>initial_split()</code> function in <code>rsample</code> package. The structure of the data is:</p>
<pre><code>str(credit_train)

tibble [900 × 21] (S3: tbl_df/tbl/data.frame)
 $ checking_balance    : Factor w/ 4 levels &quot;&lt; 0 DM&quot;,&quot;&gt; 200 DM&quot;,..: 4 1 4 3 3 4 3 4 1 1 ...
 $ months_loan_duration: Factor w/ 33 levels &quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,..: 18 22 18 16 30 18 9 9 14 9 ...
 $ credit_history      : Factor w/ 5 levels &quot;critical&quot;,&quot;delayed&quot;,..: 1 1 1 5 4 2 5 5 5 5 ...
 $ purpose             : Factor w/ 10 levels &quot;business&quot;,&quot;car (new)&quot;,..: 8 3 3 1 1 1 8 1 2 2 ...
 $ amount              : num [1:900] 2611 6187 2197 2767 6416 ...
 $ savings_balance     : Factor w/ 5 levels &quot;&lt; 100 DM&quot;,&quot;&gt; 1000 DM&quot;,..: 1 3 5 3 1 1 1 1 3 1 ...
 $ employment_length   : Factor w/ 5 levels &quot;&gt; 7 yrs&quot;,&quot;0 - 1 yrs&quot;,..: 1 4 4 1 1 3 1 4 4 3 ...
 $ installment_rate    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 4 1 4 4 4 1 3 2 4 4 ...
 $ personal_status     : Factor w/ 4 levels &quot;divorced male&quot;,..: 3 3 4 1 2 4 3 4 4 2 ...
 $ other_debtors       : Factor w/ 3 levels &quot;co-applicant&quot;,..: 1 3 3 3 3 3 2 3 3 2 ...
 $ residence_history   : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 4 4 2 3 2 3 4 3 4 ...
 $ property            : Factor w/ 4 levels &quot;building society savings&quot;,..: 3 2 2 2 4 4 3 2 2 1 ...
 $ age                 : num [1:900] 46 24 43 61 59 32 40 36 30 29 ...
 $ installment_plan    : Factor w/ 3 levels &quot;bank&quot;,&quot;none&quot;,..: 2 2 2 1 2 2 1 2 2 2 ...
 $ housing             : Factor w/ 3 levels &quot;for free&quot;,&quot;own&quot;,..: 2 3 2 3 3 1 2 2 2 2 ...
 $ existing_credits    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 2 2 2 1 1 2 1 1 1 ...
 $ default             : Factor w/ 2 levels &quot;paid&quot;,&quot;default&quot;: 1 1 1 2 2 1 1 1 1 1 ...
 $ dependents          : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 2 1 1 1 1 1 2 1 ...
 $ telephone           : Factor w/ 2 levels &quot;none&quot;,&quot;yes&quot;: 1 1 2 1 1 1 1 2 2 2 ...
 $ foreign_worker      : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...
 $ job                 : Factor w/ 4 levels &quot;management self-employed&quot;,..: 2 2 2 4 2 2 4 2 1 2 ...

</code></pre>
<p>My issue is specifically when I try to fit a model using a cost matrix. Without this cost matrix, the model does <strong>not</strong> throw this error. This is how I have created the cost matrix:</p>
<pre><code>error_cost &lt;- matrix(nrow = 2, 
                     ncol = 2,
                     dimnames = list(c('predict_paid','predict_default'), #rows
                                     c('actual_paid','actual_default')), #columns
                     data = c(0, 1, 4, 0))  
</code></pre>
<p>I must also point out that I have tried several ways to create this matrix, including literally copying the exact method given in the Lantz book, and they all result in this same error.</p>
<p>Here is the code I am using to try and fit the model.</p>
<pre><code>c5_boostTree &lt;- C5.0(default ~.,
                     credit_train,
                     trials = 3,
                     costs = error_cost)
</code></pre>
<p>However, this also happens if I use the <code>x = credit_train %&gt;% select(-default), y = credit_train$default</code> rather than the formula approach, and any similar approaches I can find or think of. I am at a complete loss as to why I am getting this error.</p>
<pre><code>c50 code called exit with value 1
</code></pre>
<p>Anyone have any ideas???</p>
<p>====================================</p>
<p>In response to a request for <code>dput(credit_train</code>, here is the output for dput(head(credit_train)), it seems too large otherwise:</p>
<pre><code>structure(list(checking_balance = structure(c(4L, 1L, 4L, 3L, 
3L, 4L), .Label = c(&quot;&lt; 0 DM&quot;, &quot;&gt; 200 DM&quot;, &quot;1 - 200 DM&quot;, &quot;unknown&quot;
), class = &quot;factor&quot;), months_loan_duration = structure(c(18L, 
22L, 18L, 16L, 30L, 18L), .Label = c(&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, 
&quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;18&quot;, &quot;20&quot;, &quot;21&quot;, 
&quot;22&quot;, &quot;24&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;30&quot;, &quot;33&quot;, &quot;36&quot;, &quot;39&quot;, &quot;40&quot;, &quot;42&quot;, 
&quot;45&quot;, &quot;47&quot;, &quot;48&quot;, &quot;54&quot;, &quot;60&quot;, &quot;72&quot;), class = &quot;factor&quot;), credit_history = structure(c(1L, 
1L, 1L, 5L, 4L, 2L), .Label = c(&quot;critical&quot;, &quot;delayed&quot;, &quot;fully repaid&quot;, 
&quot;fully repaid this bank&quot;, &quot;repaid&quot;), class = &quot;factor&quot;), purpose = structure(c(8L, 
3L, 3L, 1L, 1L, 1L), .Label = c(&quot;business&quot;, &quot;car (new)&quot;, &quot;car (used)&quot;, 
&quot;domestic appliances&quot;, &quot;education&quot;, &quot;furniture&quot;, &quot;others&quot;, &quot;radio/tv&quot;, 
&quot;repairs&quot;, &quot;retraining&quot;), class = &quot;factor&quot;), amount = c(2611, 
6187, 2197, 2767, 6416, 3863), savings_balance = structure(c(1L, 
3L, 5L, 3L, 1L, 1L), .Label = c(&quot;&lt; 100 DM&quot;, &quot;&gt; 1000 DM&quot;, &quot;101 - 500 DM&quot;, 
&quot;501 - 1000 DM&quot;, &quot;unknown&quot;), class = &quot;factor&quot;), employment_length = structure(c(1L, 
4L, 4L, 1L, 1L, 3L), .Label = c(&quot;&gt; 7 yrs&quot;, &quot;0 - 1 yrs&quot;, &quot;1 - 4 yrs&quot;, 
&quot;4 - 7 yrs&quot;, &quot;unemployed&quot;), class = &quot;factor&quot;), installment_rate = structure(c(4L, 
1L, 4L, 4L, 4L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    personal_status = structure(c(3L, 3L, 4L, 1L, 2L, 4L), .Label = c(&quot;divorced male&quot;, 
    &quot;female&quot;, &quot;married male&quot;, &quot;single male&quot;), class = &quot;factor&quot;), 
    other_debtors = structure(c(1L, 3L, 3L, 3L, 3L, 3L), .Label = c(&quot;co-applicant&quot;, 
    &quot;guarantor&quot;, &quot;none&quot;), class = &quot;factor&quot;), residence_history = structure(c(3L, 
    4L, 4L, 2L, 3L, 2L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    property = structure(c(3L, 2L, 2L, 2L, 4L, 4L), .Label = c(&quot;building society savings&quot;, 
    &quot;other&quot;, &quot;real estate&quot;, &quot;unknown/none&quot;), class = &quot;factor&quot;), 
    age = c(46, 24, 43, 61, 59, 32), installment_plan = structure(c(2L, 
    2L, 2L, 1L, 2L, 2L), .Label = c(&quot;bank&quot;, &quot;none&quot;, &quot;stores&quot;), class = &quot;factor&quot;), 
    housing = structure(c(2L, 3L, 2L, 3L, 3L, 1L), .Label = c(&quot;for free&quot;, 
    &quot;own&quot;, &quot;rent&quot;), class = &quot;factor&quot;), existing_credits = structure(c(2L, 
    2L, 2L, 2L, 1L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    default = structure(c(1L, 1L, 1L, 2L, 2L, 1L), .Label = c(&quot;paid&quot;, 
    &quot;default&quot;), class = &quot;factor&quot;), dependents = structure(c(1L, 
    1L, 2L, 1L, 1L, 1L), .Label = c(&quot;1&quot;, &quot;2&quot;), class = &quot;factor&quot;), 
    telephone = structure(c(1L, 1L, 2L, 1L, 1L, 1L), .Label = c(&quot;none&quot;, 
    &quot;yes&quot;), class = &quot;factor&quot;), foreign_worker = structure(c(2L, 
    2L, 2L, 2L, 2L, 2L), .Label = c(&quot;no&quot;, &quot;yes&quot;), class = &quot;factor&quot;), 
    job = structure(c(2L, 2L, 2L, 4L, 2L, 2L), .Label = c(&quot;management self-employed&quot;, 
    &quot;skilled employee&quot;, &quot;unemployed non-resident&quot;, &quot;unskilled resident&quot;
    ), class = &quot;factor&quot;)), row.names = c(NA, -6L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
",42,1,0,4,r;machine-learning;classification;c5.0,2022-07-10 15:47:02,2022-07-10 15:47:02,2022-07-11 14:34:35,i am using rstudio     ghost orchid  release for macos  i am learning to use to c  algorithm in r  for this i am following  machine learning in r  by brett lantz  the dataset i am using is a modified version of one relating to loans obtained from a credit agency in germany  the data has no missing values  and no empty factor levels  this has caused the same error in other posts i have viewed   i have split the data into training and test tibbles using the initial_split   function in rsample package  the structure of the data is  my issue is specifically when i try to fit a model using a cost matrix  without this cost matrix  the model does not throw this error  this is how i have created the cost matrix  i must also point out that i have tried several ways to create this matrix  including literally copying the exact method given in the lantz book  and they all result in this same error  here is the code i am using to try and fit the model  however  this also happens if i use the x   credit_train   gt   select  default   y   credit_train default rather than the formula approach  and any similar approaches i can find or think of  i am at a complete loss as to why i am getting this error  anyone have any ideas                                         in response to a request for dput credit_train  here is the output for dput head credit_train    it seems too large otherwise ,why do i get    c code called exit with value     in r ,using rstudio ghost orchid release macos learning use c algorithm r following machine learning r brett lantz dataset using modified version one relating loans obtained credit agency germany data missing values empty factor levels caused error posts viewed split data training test tibbles using initial_split function rsample package structure data issue specifically try fit model using cost matrix without cost matrix model throw error created cost matrix must also point tried several ways create matrix including literally copying exact method given lantz book result error code using try fit model however also happens use x credit_train gt select default credit_train default rather formula approach similar approaches find think complete loss getting error anyone ideas response request dput credit_train output dput head credit_train seems large otherwise,get c code called exit value r,get c code called exit value rusing rstudio ghost orchid release macos learning use c algorithm r following machine learning r brett lantz dataset using modified version one relating loans obtained credit agency germany data missing values empty factor levels caused error posts viewed split data training test tibbles using initial_split function rsample package structure data issue specifically try fit model using cost matrix without cost matrix model throw error created cost matrix must also point tried several ways create matrix including literally copying exact method given lantz book result error code using try fit model however also happens use x credit_train gt select default credit_train default rather formula approach similar approaches find think complete loss getting error anyone ideas response request dput credit_train output dput head credit_train seems large otherwise,"['get', 'c', 'code', 'called', 'exit', 'value', 'rusing', 'rstudio', 'ghost', 'orchid', 'release', 'macos', 'learning', 'use', 'c', 'algorithm', 'r', 'following', 'machine', 'learning', 'r', 'brett', 'lantz', 'dataset', 'using', 'modified', 'version', 'one', 'relating', 'loans', 'obtained', 'credit', 'agency', 'germany', 'data', 'missing', 'values', 'empty', 'factor', 'levels', 'caused', 'error', 'posts', 'viewed', 'split', 'data', 'training', 'test', 'tibbles', 'using', 'initial_split', 'function', 'rsample', 'package', 'structure', 'data', 'issue', 'specifically', 'try', 'fit', 'model', 'using', 'cost', 'matrix', 'without', 'cost', 'matrix', 'model', 'throw', 'error', 'created', 'cost', 'matrix', 'must', 'also', 'point', 'tried', 'several', 'ways', 'create', 'matrix', 'including', 'literally', 'copying', 'exact', 'method', 'given', 'lantz', 'book', 'result', 'error', 'code', 'using', 'try', 'fit', 'model', 'however', 'also', 'happens', 'use', 'x', 'credit_train', 'gt', 'select', 'default', 'credit_train', 'default', 'rather', 'formula', 'approach', 'similar', 'approaches', 'find', 'think', 'complete', 'loss', 'getting', 'error', 'anyone', 'ideas', 'response', 'request', 'dput', 'credit_train', 'output', 'dput', 'head', 'credit_train', 'seems', 'large', 'otherwise']","['get', 'c', 'code', 'call', 'exit', 'valu', 'ruse', 'rstudio', 'ghost', 'orchid', 'releas', 'maco', 'learn', 'use', 'c', 'algorithm', 'r', 'follow', 'machin', 'learn', 'r', 'brett', 'lantz', 'dataset', 'use', 'modifi', 'version', 'one', 'relat', 'loan', 'obtain', 'credit', 'agenc', 'germani', 'data', 'miss', 'valu', 'empti', 'factor', 'level', 'caus', 'error', 'post', 'view', 'split', 'data', 'train', 'test', 'tibbl', 'use', 'initial_split', 'function', 'rsampl', 'packag', 'structur', 'data', 'issu', 'specif', 'tri', 'fit', 'model', 'use', 'cost', 'matrix', 'without', 'cost', 'matrix', 'model', 'throw', 'error', 'creat', 'cost', 'matrix', 'must', 'also', 'point', 'tri', 'sever', 'way', 'creat', 'matrix', 'includ', 'liter', 'copi', 'exact', 'method', 'given', 'lantz', 'book', 'result', 'error', 'code', 'use', 'tri', 'fit', 'model', 'howev', 'also', 'happen', 'use', 'x', 'credit_train', 'gt', 'select', 'default', 'credit_train', 'default', 'rather', 'formula', 'approach', 'similar', 'approach', 'find', 'think', 'complet', 'loss', 'get', 'error', 'anyon', 'idea', 'respons', 'request', 'dput', 'credit_train', 'output', 'dput', 'head', 'credit_train', 'seem', 'larg', 'otherwis']"
56,61,61,614157,31610971,Spark - repartition() vs coalesce(),"<p>According to Learning Spark</p>

<blockquote>
  <p>Keep in mind that repartitioning your data is a fairly expensive operation.
  Spark also has an optimized version of <code>repartition()</code> called <code>coalesce()</code> that allows avoiding data movement, but only if you are decreasing the number of RDD partitions.</p>
</blockquote>

<p>One difference I get is that with <code>repartition()</code> the number of partitions can be increased/decreased, but with <code>coalesce()</code> the number of partitions can only be decreased.</p>

<p>If the partitions are spread across multiple machines and <code>coalesce()</code> is run, how can it avoid data movement?</p>
",324244,20,374,3,apache-spark;distributed-computing;rdd,2015-07-24 18:19:19,2015-07-24 18:19:19,2022-07-11 11:53:03,according to learning spark one difference i get is that with repartition   the number of partitions can be increased decreased  but with coalesce   the number of partitions can only be decreased  if the partitions are spread across multiple machines and coalesce   is run  how can it avoid data movement ,spark   repartition   vs coalesce  ,according learning spark one difference get repartition number partitions increased decreased coalesce number partitions decreased partitions spread across multiple machines coalesce run avoid data movement,spark repartition vs coalesce,spark repartition vs coalesceaccording learning spark one difference get repartition number partitions increased decreased coalesce number partitions decreased partitions spread across multiple machines coalesce run avoid data movement,"['spark', 'repartition', 'vs', 'coalesceaccording', 'learning', 'spark', 'one', 'difference', 'get', 'repartition', 'number', 'partitions', 'increased', 'decreased', 'coalesce', 'number', 'partitions', 'decreased', 'partitions', 'spread', 'across', 'multiple', 'machines', 'coalesce', 'run', 'avoid', 'data', 'movement']","['spark', 'repartit', 'vs', 'coalesceaccord', 'learn', 'spark', 'one', 'differ', 'get', 'repartit', 'number', 'partit', 'increas', 'decreas', 'coalesc', 'number', 'partit', 'decreas', 'partit', 'spread', 'across', 'multipl', 'machin', 'coalesc', 'run', 'avoid', 'data', 'movement']"
57,63,63,14837550,65324610,Python Machine Learning - Rule based match,"<p>I am new to machine learning and need help on the best approach.</p>
<p>I have a master dataset with millions of rows with columns:</p>
<pre><code>Customer first name, 
last name, 
SSN , 
address,
Unique cust id 
</code></pre>
<p>Input is a new customer details with same columns.I want to create a machine learning model with following rules</p>
<pre><code>If new customer matches any customer on SSN then return cust ids of
    matching customers  
else if customer matches any customer on First +
    Last name + zip then return cust ids of matching customers  
else
    create new cust id
</code></pre>
<p>The other issue is that name and address could have spelling errors, so exact match is not an option</p>
<p>what is the best approach and what model will work</p>
",137,1,0,2,python;match,2020-12-16 19:21:58,2020-12-16 19:21:58,2022-07-11 09:05:43,i am new to machine learning and need help on the best approach  i have a master dataset with millions of rows with columns  input is a new customer details with same columns i want to create a machine learning model with following rules the other issue is that name and address could have spelling errors  so exact match is not an option what is the best approach and what model will work,python machine learning   rule based match,machine learning need help best approach master dataset millions rows columns input customer details columns want create machine learning model following rules issue name address could spelling errors exact match option best approach model work,python machine learning rule based match,python machine learning rule based matchmachine learning need help best approach master dataset millions rows columns input customer details columns want create machine learning model following rules issue name address could spelling errors exact match option best approach model work,"['python', 'machine', 'learning', 'rule', 'based', 'matchmachine', 'learning', 'need', 'help', 'best', 'approach', 'master', 'dataset', 'millions', 'rows', 'columns', 'input', 'customer', 'details', 'columns', 'want', 'create', 'machine', 'learning', 'model', 'following', 'rules', 'issue', 'name', 'address', 'could', 'spelling', 'errors', 'exact', 'match', 'option', 'best', 'approach', 'model', 'work']","['python', 'machin', 'learn', 'rule', 'base', 'matchmachin', 'learn', 'need', 'help', 'best', 'approach', 'master', 'dataset', 'million', 'row', 'column', 'input', 'custom', 'detail', 'column', 'want', 'creat', 'machin', 'learn', 'model', 'follow', 'rule', 'issu', 'name', 'address', 'could', 'spell', 'error', 'exact', 'match', 'option', 'best', 'approach', 'model', 'work']"
58,64,64,9744542,72932518,How to deal with a skewed Time series data,"<p>I have hourly data of no. of minutes spent online by people for 2 years. Hence the values are distributed between 0 and 60 and also most data is either 0 or 60. My goal is to predict the number of minutes the person will spend online in the future (next day/hour/month etc.). What kind of approach or machine learning model can I use to predict this data? Can this be modelled into a regression/forecasting problem in spite of the skewness?<a href=""https://i.stack.imgur.com/Vk93I.png"" rel=""nofollow noreferrer"">hourly data</a></p>
",22,1,-2,2,machine-learning;time-series,2022-07-11 04:59:36,2022-07-11 04:59:36,2022-07-11 06:52:05,i have hourly data of no  of minutes spent online by people for  years  hence the values are distributed between  and  and also most data is either  or   my goal is to predict the number of minutes the person will spend online in the future  next day hour month etc    what kind of approach or machine learning model can i use to predict this data  can this be modelled into a regression forecasting problem in spite of the skewness ,how to deal with a skewed time series data,hourly data minutes spent online people years hence values distributed also data either goal predict number minutes person spend online future next day hour month etc kind approach machine learning model use predict data modelled regression forecasting problem spite skewness,deal skewed time series data,deal skewed time series datahourly data minutes spent online people years hence values distributed also data either goal predict number minutes person spend online future next day hour month etc kind approach machine learning model use predict data modelled regression forecasting problem spite skewness,"['deal', 'skewed', 'time', 'series', 'datahourly', 'data', 'minutes', 'spent', 'online', 'people', 'years', 'hence', 'values', 'distributed', 'also', 'data', 'either', 'goal', 'predict', 'number', 'minutes', 'person', 'spend', 'online', 'future', 'next', 'day', 'hour', 'month', 'etc', 'kind', 'approach', 'machine', 'learning', 'model', 'use', 'predict', 'data', 'modelled', 'regression', 'forecasting', 'problem', 'spite', 'skewness']","['deal', 'skew', 'time', 'seri', 'datahourli', 'data', 'minut', 'spent', 'onlin', 'peopl', 'year', 'henc', 'valu', 'distribut', 'also', 'data', 'either', 'goal', 'predict', 'number', 'minut', 'person', 'spend', 'onlin', 'futur', 'next', 'day', 'hour', 'month', 'etc', 'kind', 'approach', 'machin', 'learn', 'model', 'use', 'predict', 'data', 'model', 'regress', 'forecast', 'problem', 'spite', 'skew']"
59,65,65,14515155,72931509,Symbol not found error while importing tensorflow in M1 Macbook Pro,"<p>I have installed tensorflow in my M1 Macbook Pro using these commands in a conda environment:</p>
<pre><code>conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
python -m pip install tensorflow-datasets
conda install jupyter pandas numpy matplotlib scikit-learn
</code></pre>
<p>I did it following this instruction - <a href=""https://github.com/mrdbourke/m1-machine-learning-test"" rel=""nofollow noreferrer"">https://github.com/mrdbourke/m1-machine-learning-test</a></p>
<p>But when I import the packages I installed it gives me an error message.</p>
<p>Here:</p>
<pre><code>import numpy as np
import pandas as pd
import sklearn
import tensorflow as tf
import matplotlib.pyplot as plt

# Check for TensorFlow GPU access
print(f&quot;TensorFlow has access to the following devices:\n{tf.config.list_physical_devices()}&quot;)

# See TensorFlow version
print(f&quot;TensorFlow version: {tf.__version__}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
Input In [1], in &lt;cell line: 4&gt;()
      2 import pandas as pd
      3 import sklearn
----&gt; 4 import tensorflow as tf
      5 import matplotlib.pyplot as plt
      7 # Check for TensorFlow GPU access

File ~/miniforge3/lib/python3.9/site-packages/tensorflow/__init__.py:443, in &lt;module&gt;
    441 _plugin_dir = _os.path.join(_s, 'tensorflow-plugins')
    442 if _os.path.exists(_plugin_dir):
--&gt; 443   _ll.load_library(_plugin_dir)
    444   # Load Pluggable Device Library
    445   _ll.load_pluggable_device_library(_plugin_dir)

File ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:151, in load_library(library_location)
    148     kernel_libraries = [library_location]
    150   for lib in kernel_libraries:
--&gt; 151     py_tf.TF_LoadLibrary(lib)
    153 else:
    154   raise OSError(
    155       errno.ENOENT,
    156       'The file or folder to load kernel libraries from does not exist.',
    157       library_location)

NotFoundError: dlopen(/Users/arannya/miniforge3/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: __ZNKSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEE3strEv
  Referenced from: /Users/arannya/miniforge3/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib (which was built for Mac OS X 12.3)
  Expected in: /usr/lib/libc++.1.dylib
</code></pre>
<p>Please help me solve this issue. Thank you.</p>
",22,0,0,3,python;tensorflow;apple-m1,2022-07-11 01:29:45,2022-07-11 01:29:45,2022-07-11 01:29:45,i have installed tensorflow in my m macbook pro using these commands in a conda environment  i did it following this instruction    but when i import the packages i installed it gives me an error message  here  error  please help me solve this issue  thank you ,symbol not found error while importing tensorflow in m macbook pro,installed tensorflow macbook pro using commands conda environment following instruction import packages installed gives error message error please help solve issue thank,symbol found error importing tensorflow macbook pro,symbol found error importing tensorflow macbook proinstalled tensorflow macbook pro using commands conda environment following instruction import packages installed gives error message error please help solve issue thank,"['symbol', 'found', 'error', 'importing', 'tensorflow', 'macbook', 'proinstalled', 'tensorflow', 'macbook', 'pro', 'using', 'commands', 'conda', 'environment', 'following', 'instruction', 'import', 'packages', 'installed', 'gives', 'error', 'message', 'error', 'please', 'help', 'solve', 'issue', 'thank']","['symbol', 'found', 'error', 'import', 'tensorflow', 'macbook', 'proinstal', 'tensorflow', 'macbook', 'pro', 'use', 'command', 'conda', 'environ', 'follow', 'instruct', 'import', 'packag', 'instal', 'give', 'error', 'messag', 'error', 'pleas', 'help', 'solv', 'issu', 'thank']"
60,66,66,17573228,72930868,errors in the tutorial (Interpreting Machine Learning Models with the iml Package),"<p>I am getting the following error when trying to execute the following code in section entitled &quot;Replication requirements&quot; (<a href=""https://uc-r.github.io/iml-pkg"" rel=""nofollow noreferrer"">https://uc-r.github.io/iml-pkg</a>):</p>
<pre><code>#classification data
df &lt;- rsample::attrition %&gt;%
mutate_if(is.ordered, factor, ordered = FALSE) %&gt;%
mutate(Attrition = recode(Attrition, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;0&quot;) %&gt;% factor(levels = c(&quot;1&quot;, &quot;0&quot;)))

&gt; Error: 'attrition' is not an exported object from 'namespace:rsample'
</code></pre>
<p>The problem was solved using the following code:</p>
<pre><code>#data
library(modeldata)
data(&quot;attrition&quot;, package = &quot;modeldata&quot;)
#classification data
df &lt;- attrition %&gt;%
mutate_if(is.ordered, factor, ordered = FALSE) %&gt;%
mutate(Attrition = recode(Attrition, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;0&quot;) %&gt;% factor(levels = c(&quot;1&quot;, &quot;0&quot;)))
</code></pre>
<p>Unfortunately, I got another error after trying to execute the following code (section entitled &quot;Global interpretation/Feature importance&quot; (<a href=""https://uc-r.github.io/iml-pkg"" rel=""nofollow noreferrer"">https://uc-r.github.io/iml-pkg</a>):</p>
<pre><code>#compute feature importance with specified loss metric
imp.glm &lt;- FeatureImp$new(predictor.glm, loss = &quot;mse&quot;)
imp.rf &lt;- FeatureImp$new(predictor.rf, loss = &quot;mse&quot;)
imp.gbm &lt;- FeatureImp$new(predictor.gbm, loss = &quot;mse&quot;)

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected

&gt; Error in [.data.frame(prediction, , self$class, drop = FALSE) : undefined columns selected
</code></pre>
<p>I use R 4.2.0/ Win10</p>
",20,0,0,2,r;iml,2022-07-10 23:51:37,2022-07-10 23:51:37,2022-07-10 23:51:37,i am getting the following error when trying to execute the following code in section entitled  replication requirements      the problem was solved using the following code  unfortunately  i got another error after trying to execute the following code  section entitled  global interpretation feature importance      i use r     win,errors in the tutorial  interpreting machine learning models with the iml package ,getting following error trying execute following code section entitled replication requirements problem solved using following code unfortunately got another error trying execute following code section entitled global interpretation feature importance use r win,errors tutorial interpreting machine learning models iml package,errors tutorial interpreting machine learning models iml packagegetting following error trying execute following code section entitled replication requirements problem solved using following code unfortunately got another error trying execute following code section entitled global interpretation feature importance use r win,"['errors', 'tutorial', 'interpreting', 'machine', 'learning', 'models', 'iml', 'packagegetting', 'following', 'error', 'trying', 'execute', 'following', 'code', 'section', 'entitled', 'replication', 'requirements', 'problem', 'solved', 'using', 'following', 'code', 'unfortunately', 'got', 'another', 'error', 'trying', 'execute', 'following', 'code', 'section', 'entitled', 'global', 'interpretation', 'feature', 'importance', 'use', 'r', 'win']","['error', 'tutori', 'interpret', 'machin', 'learn', 'model', 'iml', 'packageget', 'follow', 'error', 'tri', 'execut', 'follow', 'code', 'section', 'entitl', 'replic', 'requir', 'problem', 'solv', 'use', 'follow', 'code', 'unfortun', 'got', 'anoth', 'error', 'tri', 'execut', 'follow', 'code', 'section', 'entitl', 'global', 'interpret', 'featur', 'import', 'use', 'r', 'win']"
61,67,67,13605647,72929325,nginx: [emerg] invalid number of arguments in &quot;include&quot; directive,"<p>I am learning how to deploy AWS for the first time. I am following this guide here: <a href=""https://www.youtube.com/watch?v=HtWgb_vbyvY"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=HtWgb_vbyvY</a>.</p>
<p>I am using windows machine while the person in the video uses mac.</p>
<p>I am getting the following error when running ngnix commands on the windows terminal: &quot;nginx: [emerg] CreateFile() &quot;C:/nginx-1.23.0/nginx-1.23.0/mime.types&quot; failed (3: The system cannot find the path specified) in C:\Users\Shi Jie\Downloads\nginx-1.23.0\nginx-1.23.0/conf/nginx.conf:12&quot;</p>
<p>i think it is how i write the path on my ngnix.conf which i write as such</p>
<pre><code>worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include \Users\Shi Jie\Downloads\nginx-1.23.0\nginx-1.23.0\mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                      '$status $body_bytes_sent &quot;$http_referer&quot; '
                      '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    client_body_buffer_size 100k;
    client_header_buffer_size 1k;
    client_max_body_size 100k;
    large_client_header_buffers 2 1k;
    client_body_timeout 10;
    client_header_timeout 10;
    keepalive_timeout 5 5;
    send_timeout 10;
    server_tokens off;
    #gzip  on; on;

    include /etc/nginx/conf.d/*.conf;
}


</code></pre>
<p>Can anyone point the way to teach me how to write the path properly? Thank you.</p>
",19,1,0,2,amazon-web-services;amazon-ec2,2022-07-10 20:12:01,2022-07-10 20:12:01,2022-07-10 23:08:04,i am learning how to deploy aws for the first time  i am following this guide here    i am using windows machine while the person in the video uses mac  i am getting the following error when running ngnix commands on the windows terminal   nginx   emerg  createfile    c  nginx    nginx    mime types  failed    the system cannot find the path specified  in c  users shi jie downloads nginx    nginx    conf nginx conf   i think it is how i write the path on my ngnix conf which i write as such can anyone point the way to teach me how to write the path properly  thank you ,nginx   emerg  invalid number of arguments in  include  directive,learning deploy aws first time following guide using windows machine person video uses mac getting following error running ngnix commands windows terminal nginx emerg createfile c nginx nginx mime types failed system cannot find path specified c users shi jie downloads nginx nginx conf nginx conf think write path ngnix conf write anyone point way teach write path properly thank,nginx emerg invalid number arguments include directive,nginx emerg invalid number arguments include directivelearning deploy aws first time following guide using windows machine person video uses mac getting following error running ngnix commands windows terminal nginx emerg createfile c nginx nginx mime types failed system cannot find path specified c users shi jie downloads nginx nginx conf nginx conf think write path ngnix conf write anyone point way teach write path properly thank,"['nginx', 'emerg', 'invalid', 'number', 'arguments', 'include', 'directivelearning', 'deploy', 'aws', 'first', 'time', 'following', 'guide', 'using', 'windows', 'machine', 'person', 'video', 'uses', 'mac', 'getting', 'following', 'error', 'running', 'ngnix', 'commands', 'windows', 'terminal', 'nginx', 'emerg', 'createfile', 'c', 'nginx', 'nginx', 'mime', 'types', 'failed', 'system', 'can', 'not', 'find', 'path', 'specified', 'c', 'users', 'shi', 'jie', 'downloads', 'nginx', 'nginx', 'conf', 'nginx', 'conf', 'think', 'write', 'path', 'ngnix', 'conf', 'write', 'anyone', 'point', 'way', 'teach', 'write', 'path', 'properly', 'thank']","['nginx', 'emerg', 'invalid', 'number', 'argument', 'includ', 'directivelearn', 'deploy', 'aw', 'first', 'time', 'follow', 'guid', 'use', 'window', 'machin', 'person', 'video', 'use', 'mac', 'get', 'follow', 'error', 'run', 'ngnix', 'command', 'window', 'termin', 'nginx', 'emerg', 'createfil', 'c', 'nginx', 'nginx', 'mime', 'type', 'fail', 'system', 'can', 'not', 'find', 'path', 'specifi', 'c', 'user', 'shi', 'jie', 'download', 'nginx', 'nginx', 'conf', 'nginx', 'conf', 'think', 'write', 'path', 'ngnix', 'conf', 'write', 'anyon', 'point', 'way', 'teach', 'write', 'path', 'properli', 'thank']"
62,68,68,16421594,72919440,I need guidance for first published Article/paper,"<p>I have a novel idea to use machine learning and object detection to distinguish between two similar skin diseases. I need guidance where to publish this article/paper.</p>
",40,0,-3,3,machine-learning;computer-vision;medical-imaging,2022-07-09 12:02:38,2022-07-09 12:02:38,2022-07-10 18:14:00,i have a novel idea to use machine learning and object detection to distinguish between two similar skin diseases  i need guidance where to publish this article paper ,i need guidance for first published article paper,novel idea use machine learning object detection distinguish two similar skin diseases need guidance publish article paper,need guidance first published article paper,need guidance first published article papernovel idea use machine learning object detection distinguish two similar skin diseases need guidance publish article paper,"['need', 'guidance', 'first', 'published', 'article', 'papernovel', 'idea', 'use', 'machine', 'learning', 'object', 'detection', 'distinguish', 'two', 'similar', 'skin', 'diseases', 'need', 'guidance', 'publish', 'article', 'paper']","['need', 'guidanc', 'first', 'publish', 'articl', 'papernovel', 'idea', 'use', 'machin', 'learn', 'object', 'detect', 'distinguish', 'two', 'similar', 'skin', 'diseas', 'need', 'guidanc', 'publish', 'articl', 'paper']"
63,69,69,19519627,72926799,How to use the rear camera instead of the front camera on a teachable machine,"
<br>




    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image
<pre><code>// the link to your model provided by Teachable Machine export panel
const URL = &quot;./my_model/&quot;;

let model, webcam, labelContainer, maxPredictions;

// Load the image model and setup the webcam
async function init() {
    const modelURL = URL + &quot;model.json&quot;;
    const metadataURL = URL + &quot;metadata.json&quot;;
   

    // load the model and metadata
    // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
    // or files from your local hard drive
    // Note: the pose library adds &quot;tmImage&quot; object to your window (window.tmImage)
    model = await tmImage.load(modelURL, metadataURL);
    maxPredictions = model.getTotalClasses();
    
    // Convenience function to setup a webcam

    const size = 350;
    const flip = true; // whether to flip the webcam
    webcam = new tmImage.Webcam(size, size, flip); // width, height, flip
    await webcam.setup(); // request access to the webcam
    await webcam.play();
    window.requestAnimationFrame(loop);

    // append elements to the DOM
    document.getElementById(&quot;webcam-container&quot;).appendChild(webcam.canvas);
    labelContainer = document.getElementById(&quot;label-container&quot;);
    for (let i = 0; i &lt; maxPredictions; i++) { // and class labels
        labelContainer.appendChild(document.createElement(&quot;div&quot;));
    }
}

async function loop() {
    webcam.update(); // update the webcam frame
    await predict();
    window.requestAnimationFrame(loop);
}
</code></pre>
<p>Currently, it is producing a program that can produce food-related information by filming food with a mobile phone rear camera using a learning machine.</p>
<p>I want to use the back camera in this part, but only the front camera keeps coming out. I'd appreciate it if you could help me :)</p>
",14,0,0,4,javascript;camera;webcam;teachable-machine,2022-07-10 12:51:46,2022-07-10 12:51:46,2022-07-10 12:51:46,currently  it is producing a program that can produce food related information by filming food with a mobile phone rear camera using a learning machine  i want to use the back camera in this part  but only the front camera keeps coming out  i d appreciate it if you could help me   ,how to use the rear camera instead of the front camera on a teachable machine,currently producing program produce food related information filming food mobile phone rear camera using learning machine want use back camera part front camera keeps coming appreciate could help,use rear camera instead front camera teachable machine,use rear camera instead front camera teachable machinecurrently producing program produce food related information filming food mobile phone rear camera using learning machine want use back camera part front camera keeps coming appreciate could help,"['use', 'rear', 'camera', 'instead', 'front', 'camera', 'teachable', 'machinecurrently', 'producing', 'program', 'produce', 'food', 'related', 'information', 'filming', 'food', 'mobile', 'phone', 'rear', 'camera', 'using', 'learning', 'machine', 'want', 'use', 'back', 'camera', 'part', 'front', 'camera', 'keeps', 'coming', 'appreciate', 'could', 'help']","['use', 'rear', 'camera', 'instead', 'front', 'camera', 'teachabl', 'machinecurr', 'produc', 'program', 'produc', 'food', 'relat', 'inform', 'film', 'food', 'mobil', 'phone', 'rear', 'camera', 'use', 'learn', 'machin', 'want', 'use', 'back', 'camera', 'part', 'front', 'camera', 'keep', 'come', 'appreci', 'could', 'help']"
64,70,70,7723320,72926266,Best solution for visualising custom neural network data,"<p>I have essentially built my own machine learning framework from the ground up in c and c#. There are a number of reasons why I have chosen to do this. Mainly because I couldn't find any solutions for unsupervised learning that were suitable for my project needs. Please note I am fully aware of the pros and cons and challenges of creating my own machine learning framework. Furthermore, my framework is already functional at this point.</p>
<p>I am now looking to find ways to visualise the weights, biases, fitness etc. and other metrics while training my networks. Something similar to <a href=""https://www.tensorflow.org/tensorboard"" rel=""nofollow noreferrer"">Tensorboard</a> or <a href=""https://wandb.ai/site"" rel=""nofollow noreferrer"">Weights and Biases.</a> I figured my best option would be to somehow convert my neural network data into an existing ml format such as Keras models. I of course understand the exact file format of my neural network files however, it seems like it would be quite challenging to fully understand or find documentation on the format of a keras file and to convert my data like this.</p>
<p>To give you an idea of how I am currently capturing the weights and biases throughout training in my framework - At each epoch an 'NN' file (my custom file format) is created which holds all of the weight and bias values encoded as a series doubles. Each layers weight and bias values are stored consecutively in a single NN file. Each NN file also has a header containing extra info about the network.</p>
<p>I've also just started exploring other data visualisation tools such as this <a href=""https://observablehq.com"" rel=""nofollow noreferrer"">https://observablehq.com</a> . Which has got me quite excited as this seems like it may be the easiest to implement. However, I could potentially be missing out on all of the machine learning specific data analysis already built into solutions such as Tensorboard.</p>
<p>Any guidance or advice on what would be the best way to proceed with this would be much appreciated.</p>
",24,0,-2,5,tensorflow;machine-learning;keras;neural-network;data-analysis,2022-07-10 10:36:37,2022-07-10 10:36:37,2022-07-10 10:36:37,i have essentially built my own machine learning framework from the ground up in c and c   there are a number of reasons why i have chosen to do this  mainly because i couldn t find any solutions for unsupervised learning that were suitable for my project needs  please note i am fully aware of the pros and cons and challenges of creating my own machine learning framework  furthermore  my framework is already functional at this point  i am now looking to find ways to visualise the weights  biases  fitness etc  and other metrics while training my networks  something similar to  or  i figured my best option would be to somehow convert my neural network data into an existing ml format such as keras models  i of course understand the exact file format of my neural network files however  it seems like it would be quite challenging to fully understand or find documentation on the format of a keras file and to convert my data like this  to give you an idea of how i am currently capturing the weights and biases throughout training in my framework   at each epoch an  nn  file  my custom file format  is created which holds all of the weight and bias values encoded as a series doubles  each layers weight and bias values are stored consecutively in a single nn file  each nn file also has a header containing extra info about the network  i ve also just started exploring other data visualisation tools such as this    which has got me quite excited as this seems like it may be the easiest to implement  however  i could potentially be missing out on all of the machine learning specific data analysis already built into solutions such as tensorboard  any guidance or advice on what would be the best way to proceed with this would be much appreciated ,best solution for visualising custom neural network data,essentially built machine learning framework ground c c number reasons chosen mainly find solutions unsupervised learning suitable project needs please note fully aware pros cons challenges creating machine learning framework furthermore framework already functional point looking find ways visualise weights biases fitness etc metrics training networks something similar figured best option would somehow convert neural network data existing ml format keras models course understand exact file format neural network files however seems like would quite challenging fully understand find documentation format keras file convert data like give idea currently capturing weights biases throughout training framework epoch nn file file format created holds weight bias values encoded series doubles layers weight bias values stored consecutively single nn file nn file also header containing extra info network also started exploring data visualisation tools got quite excited seems like may easiest implement however could potentially missing machine learning specific data analysis already built solutions tensorboard guidance advice would best way proceed would much appreciated,best solution visualising neural network data,best solution visualising neural network dataessentially built machine learning framework ground c c number reasons chosen mainly find solutions unsupervised learning suitable project needs please note fully aware pros cons challenges creating machine learning framework furthermore framework already functional point looking find ways visualise weights biases fitness etc metrics training networks something similar figured best option would somehow convert neural network data existing ml format keras models course understand exact file format neural network files however seems like would quite challenging fully understand find documentation format keras file convert data like give idea currently capturing weights biases throughout training framework epoch nn file file format created holds weight bias values encoded series doubles layers weight bias values stored consecutively single nn file nn file also header containing extra info network also started exploring data visualisation tools got quite excited seems like may easiest implement however could potentially missing machine learning specific data analysis already built solutions tensorboard guidance advice would best way proceed would much appreciated,"['best', 'solution', 'visualising', 'neural', 'network', 'dataessentially', 'built', 'machine', 'learning', 'framework', 'ground', 'c', 'c', 'number', 'reasons', 'chosen', 'mainly', 'find', 'solutions', 'unsupervised', 'learning', 'suitable', 'project', 'needs', 'please', 'note', 'fully', 'aware', 'pros', 'cons', 'challenges', 'creating', 'machine', 'learning', 'framework', 'furthermore', 'framework', 'already', 'functional', 'point', 'looking', 'find', 'ways', 'visualise', 'weights', 'biases', 'fitness', 'etc', 'metrics', 'training', 'networks', 'something', 'similar', 'figured', 'best', 'option', 'would', 'somehow', 'convert', 'neural', 'network', 'data', 'existing', 'ml', 'format', 'keras', 'models', 'course', 'understand', 'exact', 'file', 'format', 'neural', 'network', 'files', 'however', 'seems', 'like', 'would', 'quite', 'challenging', 'fully', 'understand', 'find', 'documentation', 'format', 'keras', 'file', 'convert', 'data', 'like', 'give', 'idea', 'currently', 'capturing', 'weights', 'biases', 'throughout', 'training', 'framework', 'epoch', 'nn', 'file', 'file', 'format', 'created', 'holds', 'weight', 'bias', 'values', 'encoded', 'series', 'doubles', 'layers', 'weight', 'bias', 'values', 'stored', 'consecutively', 'single', 'nn', 'file', 'nn', 'file', 'also', 'header', 'containing', 'extra', 'info', 'network', 'also', 'started', 'exploring', 'data', 'visualisation', 'tools', 'got', 'quite', 'excited', 'seems', 'like', 'may', 'easiest', 'implement', 'however', 'could', 'potentially', 'missing', 'machine', 'learning', 'specific', 'data', 'analysis', 'already', 'built', 'solutions', 'tensorboard', 'guidance', 'advice', 'would', 'best', 'way', 'proceed', 'would', 'much', 'appreciated']","['best', 'solut', 'visualis', 'neural', 'network', 'dataessenti', 'built', 'machin', 'learn', 'framework', 'ground', 'c', 'c', 'number', 'reason', 'chosen', 'mainli', 'find', 'solut', 'unsupervis', 'learn', 'suitabl', 'project', 'need', 'pleas', 'note', 'fulli', 'awar', 'pro', 'con', 'challeng', 'creat', 'machin', 'learn', 'framework', 'furthermor', 'framework', 'alreadi', 'function', 'point', 'look', 'find', 'way', 'visualis', 'weight', 'bias', 'fit', 'etc', 'metric', 'train', 'network', 'someth', 'similar', 'figur', 'best', 'option', 'would', 'somehow', 'convert', 'neural', 'network', 'data', 'exist', 'ml', 'format', 'kera', 'model', 'cours', 'understand', 'exact', 'file', 'format', 'neural', 'network', 'file', 'howev', 'seem', 'like', 'would', 'quit', 'challeng', 'fulli', 'understand', 'find', 'document', 'format', 'kera', 'file', 'convert', 'data', 'like', 'give', 'idea', 'current', 'captur', 'weight', 'bias', 'throughout', 'train', 'framework', 'epoch', 'nn', 'file', 'file', 'format', 'creat', 'hold', 'weight', 'bia', 'valu', 'encod', 'seri', 'doubl', 'layer', 'weight', 'bia', 'valu', 'store', 'consecut', 'singl', 'nn', 'file', 'nn', 'file', 'also', 'header', 'contain', 'extra', 'info', 'network', 'also', 'start', 'explor', 'data', 'visualis', 'tool', 'got', 'quit', 'excit', 'seem', 'like', 'may', 'easiest', 'implement', 'howev', 'could', 'potenti', 'miss', 'machin', 'learn', 'specif', 'data', 'analysi', 'alreadi', 'built', 'solut', 'tensorboard', 'guidanc', 'advic', 'would', 'best', 'way', 'proceed', 'would', 'much', 'appreci']"
65,71,71,9721314,72924904,Stratified Sampling in Python without scikit-learn,"<p>I have a vector which contains 10 values of sample 1 and 25 values of sample 2.</p>
<pre><code>Fact = np.array((2,2,2,2,1,2,1,1,2,2,2,1,2,2,2,1,2,2,2,1,2,2,1,1,2,1,2,2,2,2,2,2,1,2,2))
</code></pre>
<p>I want to create a stratified output vector where :</p>
<p>sample 1 is divided in 80% : 8 values of 1 and 20% : 2 values of 0.</p>
<p>sample 2 is divided in 80% : 20 values of 1 and 20% : 5 values of 0.</p>
<p>The expected output will be :</p>
<pre><code>Output = np.array((0,1,1,1,0,1,1,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1))
</code></pre>
<p>How can I automate this ? I can’t use the sampling function from scikit-learn because it is not for a machine learning experience.</p>
",45,1,0,2,python;numpy,2022-07-10 03:35:59,2022-07-10 03:35:59,2022-07-10 07:17:43,i have a vector which contains  values of sample  and  values of sample   i want to create a stratified output vector where   sample  is divided in      values of  and      values of   sample  is divided in      values of  and      values of   the expected output will be   how can i automate this   i can t use the sampling function from scikit learn because it is not for a machine learning experience ,stratified sampling in python without scikit learn,vector contains values sample values sample want create stratified output vector sample divided values values sample divided values values expected output automate use sampling function scikit learn machine learning experience,stratified sampling python without scikit learn,stratified sampling python without scikit learnvector contains values sample values sample want create stratified output vector sample divided values values sample divided values values expected output automate use sampling function scikit learn machine learning experience,"['stratified', 'sampling', 'python', 'without', 'scikit', 'learnvector', 'contains', 'values', 'sample', 'values', 'sample', 'want', 'create', 'stratified', 'output', 'vector', 'sample', 'divided', 'values', 'values', 'sample', 'divided', 'values', 'values', 'expected', 'output', 'automate', 'use', 'sampling', 'function', 'scikit', 'learn', 'machine', 'learning', 'experience']","['stratifi', 'sampl', 'python', 'without', 'scikit', 'learnvector', 'contain', 'valu', 'sampl', 'valu', 'sampl', 'want', 'creat', 'stratifi', 'output', 'vector', 'sampl', 'divid', 'valu', 'valu', 'sampl', 'divid', 'valu', 'valu', 'expect', 'output', 'autom', 'use', 'sampl', 'function', 'scikit', 'learn', 'machin', 'learn', 'experi']"
66,72,72,19303916,72555192,How to generate passwords using PassGAN,"<p>Sorry if this question is very basic and simple, I'm a beginner at programming and especially machine learning.</p>
<p>I'm trying to evaluate the performance of the PassGAN AI by having it generate passwords, and then I compare them to a testing list that contains around a million passwords and see how many matches I get.</p>
<p>I have managed to train the algorithm, but I'm not sure how to get it to generate a password file with generated passwords.</p>
<p>Link to the GitHub source of PassGAN: <a href=""https://github.com/d4ichi/PassGAN"" rel=""nofollow noreferrer"">https://github.com/d4ichi/PassGAN</a>
<br/>
Training &amp; Testing file: <a href=""https://github.com/d4ichi/PassGAN/releases/download/data/rockyou-test.txt"" rel=""nofollow noreferrer"">https://github.com/d4ichi/PassGAN/releases/download/data/rockyou-test.txt</a></p>
<p>*Note: I did have to modify some of the code, and downgrade the TensorFlow version to get it to work.
<br/>
I simply replaced all:
<br/>
<code>import tensorflow as tf</code>
<br/>
with
<br/>
<code>import tensorflow.compat.v1 as tf</code>
<br/>
<code>tf.disable_v2_behavior()</code>
<br/>
in every .py file.</p>
<p>I'd really appreciate any help on this.</p>
",44,1,0,5,python;tensorflow;machine-learning;artificial-intelligence;generative-adversarial-network,2022-06-09 11:09:21,2022-06-09 11:09:21,2022-07-10 01:56:19,sorry if this question is very basic and simple  i m a beginner at programming and especially machine learning  i m trying to evaluate the performance of the passgan ai by having it generate passwords  and then i compare them to a testing list that contains around a million passwords and see how many matches i get  i have managed to train the algorithm  but i m not sure how to get it to generate a password file with generated passwords  i d really appreciate any help on this ,how to generate passwords using passgan,sorry question basic simple beginner programming especially machine learning trying evaluate performance passgan ai generate passwords compare testing contains around million passwords see many matches get managed train algorithm sure get generate password file generated passwords really appreciate help,generate passwords using passgan,generate passwords using passgansorry question basic simple beginner programming especially machine learning trying evaluate performance passgan ai generate passwords compare testing contains around million passwords see many matches get managed train algorithm sure get generate password file generated passwords really appreciate help,"['generate', 'passwords', 'using', 'passgansorry', 'question', 'basic', 'simple', 'beginner', 'programming', 'especially', 'machine', 'learning', 'trying', 'evaluate', 'performance', 'passgan', 'ai', 'generate', 'passwords', 'compare', 'testing', 'contains', 'around', 'million', 'passwords', 'see', 'many', 'matches', 'get', 'managed', 'train', 'algorithm', 'sure', 'get', 'generate', 'password', 'file', 'generated', 'passwords', 'really', 'appreciate', 'help']","['gener', 'password', 'use', 'passgansorri', 'question', 'basic', 'simpl', 'beginn', 'program', 'especi', 'machin', 'learn', 'tri', 'evalu', 'perform', 'passgan', 'ai', 'gener', 'password', 'compar', 'test', 'contain', 'around', 'million', 'password', 'see', 'mani', 'match', 'get', 'manag', 'train', 'algorithm', 'sure', 'get', 'gener', 'password', 'file', 'gener', 'password', 'realli', 'appreci', 'help']"
67,73,73,17169994,72924263,"IndexError: positional indexers are out-of-bounds, Machine Learning Classification","<p>I'm trying to do machine learning classification with a leave-one-out Cross-Validation. This is the first part of the code.</p>
<pre><code>listOfRows = []
dfModelPerf = pd.DataFrame(columns = ['threshold', 'c', 'class_weights', 'RFCacc', 'RFCsens', 'RFCspec', 'RFCprec', 'RFCFscore', 'RFCAUC-ROC'
                                    'SVMacc', 'SVMsens', 'SVMspec', 'SVMprec', 'SVMFscore', 'SVMAUC-ROC'])
thresholds = [0.75]
cs = [1]
class_weights = [{'Control':1, 'SCD':1}, #'MCI':1}, 
                {'Control':1.75, 'SCD':1.5}, #'MCI':1}, 
                {'Control':2, 'SCD':1.75} #'MCI':1}
                ]

#this is the loop used to iterate through param combintions
for threshold in thresholds:
    for c in cs:
        for class_weight in class_weights:
            row = {}
            svmAcc = []
            rfAcc = []
            X=dfClass
            y=df_classification.loc[df_classification['Diagnosis'].isin(['Control', 'SCD'])]
            loo = LeaveOneOut()
            loo.get_n_splits(dfClass)
            # let's count the selected features in a Counter
            featCount = Counter()
            # the LOOCV happens here with the selected parameter combination
            for train_index, test_index in loo.split(dfClass):
                X_train, X_test = X.iloc[train_index], X.iloc[test_index]
                y_train, y_test = y.iloc[train_index], y.iloc[test_index]
                pvalue = []
                x_control = X_train.loc[y_train=='Control', :].astype('float64', copy=False)
                x_scd = X_train.loc[y_train=='SCD', :].astype('float64', copy=False)
</code></pre>
<p>When I do this, I come across this error.</p>
<pre><code>IndexError                                Traceback (most recent call last)
~\anaconda3\lib\site-packages\pandas\core\indexing.py in _get_list_axis(self, key, axis)
   1529         try:
-&gt; 1530             return self.obj._take_with_is_copy(key, axis=axis)
   1531         except IndexError as err:

~\anaconda3\lib\site-packages\pandas\core\generic.py in _take_with_is_copy(self, indices, axis)
   3627         &quot;&quot;&quot;
-&gt; 3628         result = self.take(indices=indices, axis=axis)
   3629         # Maybe set copy if we didn't actually change the index.

~\anaconda3\lib\site-packages\pandas\core\generic.py in take(self, indices, axis, is_copy, **kwargs)
   3615         new_data = self._mgr.take(
-&gt; 3616             indices, axis=self._get_block_manager_axis(axis), verify=True
   3617         )

~\anaconda3\lib\site-packages\pandas\core\internals\managers.py in take(self, indexer, axis, verify)
    861         n = self.shape[axis]
--&gt; 862         indexer = maybe_convert_indices(indexer, n, verify=verify)
    863 

~\anaconda3\lib\site-packages\pandas\core\indexers.py in maybe_convert_indices(indices, n, verify)
    291         if mask.any():
--&gt; 292             raise IndexError(&quot;indices are out-of-bounds&quot;)
    293     return indices

IndexError: indices are out-of-bounds

The above exception was the direct cause of the following exception:

IndexError                                Traceback (most recent call last)
&lt;ipython-input-14-7df8494e8241&gt; in &lt;module&gt;
     27             for train_index, test_index in loo.split(dfClass):
     28                 X_train, X_test = X.iloc[train_index], X.iloc[test_index]
---&gt; 29                 y_train, y_test = y.iloc[train_index], y.iloc[test_index]
     30                 pvalue = []
     31                 x_control = X_train.loc[y_train=='Control', :].astype('float64', copy=False)

~\anaconda3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
    929 
    930             maybe_callable = com.apply_if_callable(key, self.obj)
--&gt; 931             return self._getitem_axis(maybe_callable, axis=axis)
    932 
    933     def _is_scalar_access(self, key: tuple):

~\anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_axis(self, key, axis)
   1555         # a list of integers
   1556         elif is_list_like_indexer(key):
-&gt; 1557             return self._get_list_axis(key, axis=axis)
   1558 
   1559         # a single integer

~\anaconda3\lib\site-packages\pandas\core\indexing.py in _get_list_axis(self, key, axis)
   1531         except IndexError as err:
   1532             # re-raise with different error message
-&gt; 1533             raise IndexError(&quot;positional indexers are out-of-bounds&quot;) from err
   1534 
   1535     def _getitem_axis(self, key, axis: int):

IndexError: positional indexers are out-of-bounds
</code></pre>
<p>This error may be because I'm using .loc and then .iloc, but is there a way around it? I want only certain parts of my dataframe as &quot;y&quot; for the classification. What else am I doing wrong here?</p>
",48,0,0,3,python;pandas;classification,2022-07-10 01:32:32,2022-07-10 01:32:32,2022-07-10 01:32:32,i m trying to do machine learning classification with a leave one out cross validation  this is the first part of the code  when i do this  i come across this error  this error may be because i m using  loc and then  iloc  but is there a way around it  i want only certain parts of my dataframe as  y  for the classification  what else am i doing wrong here ,indexerror  positional indexers are out of bounds  machine learning classification,trying machine learning classification leave one cross validation first part code come across error error may using loc iloc way around want certain parts dataframe classification else wrong,indexerror positional indexers bounds machine learning classification,indexerror positional indexers bounds machine learning classificationtrying machine learning classification leave one cross validation first part code come across error error may using loc iloc way around want certain parts dataframe classification else wrong,"['indexerror', 'positional', 'indexers', 'bounds', 'machine', 'learning', 'classificationtrying', 'machine', 'learning', 'classification', 'leave', 'one', 'cross', 'validation', 'first', 'part', 'code', 'come', 'across', 'error', 'error', 'may', 'using', 'loc', 'iloc', 'way', 'around', 'want', 'certain', 'parts', 'dataframe', 'classification', 'else', 'wrong']","['indexerror', 'posit', 'index', 'bound', 'machin', 'learn', 'classificationtri', 'machin', 'learn', 'classif', 'leav', 'one', 'cross', 'valid', 'first', 'part', 'code', 'come', 'across', 'error', 'error', 'may', 'use', 'loc', 'iloc', 'way', 'around', 'want', 'certain', 'part', 'datafram', 'classif', 'els', 'wrong']"
68,74,74,19502829,72898584,Why is XmlWriter failing to replace characters not supported by the current encoding with equivalent character entities?,"<p>I am attempting to write an <code>XmlDocument</code> to a file using an encoding other than UTF8 (<code>Encoding.ASCII</code> in this case) and have <code>XmlWriter</code> automatically replace characters not supported by the encoding with equivalent character entities.  To do this, I am using the sample code from <em><a href=""https://stackoverflow.com/q/72348095/3744182"">Conversion of the special characters while adding it to the XML innertext in C#</a></em>.  However, for my XML, rather than replacing characters with character entities, it is throwing an exception like the below:</p>
<blockquote>
<p>Unable to translate Unicode character \u2018 at index 5852 to specified code page.Encode_Save</p>
</blockquote>
<p>What is the cause of this exception?  Why aren't the unsupported characters getting escaped as expected?</p>
<p><strong>Code used:</strong></p>
<p>My serialization code:</p>
<pre><code>using (var stream = new FileStream(clsGlobal.outputXMLPath, FileMode.OpenOrCreate))
{
    clsGlobal.XMLDoc.Save(stream, indent: false, encoding: Encoding.ASCII, omitXmlDeclaration: false);
}
</code></pre>
<p>The code for <code>Save()</code> from the linked question:</p>
<pre><code>public static class XmlSerializationHelper
{
    public static string GetOuterXml(this XmlNode node, bool indent = false, Encoding encoding = null, bool omitXmlDeclaration = false)
    {
        if (node == null)
            return null;
        
            var stream = new MemoryStream();

            node.Save(stream, indent: indent, encoding: encoding, omitXmlDeclaration: omitXmlDeclaration, closeOutput: false);
            stream.Position = 0;
            
            var reader = new StreamReader(stream);
            return reader.ReadToEnd();
      
    }

    public static void Save(this XmlNode node, Stream stream, bool indent = false, Encoding encoding = null, bool omitXmlDeclaration = false, bool closeOutput = true) =&gt;
        node.Save(stream, new XmlWriterSettings
        {
            Indent = indent,
            Encoding = encoding,               
            OmitXmlDeclaration = omitXmlDeclaration,
            CloseOutput = closeOutput,    
        });

    public static void Save(this XmlNode node, Stream stream, XmlWriterSettings settings)
    {
        try
        {
            using (var xmlWriter = XmlWriter.Create(stream, settings))
            {
                node.WriteTo(xmlWriter);
            }
        }
        catch (Exception ex)
        {
            clsGlobal.globalErrCount++;
            clsGlobal.WriteLog(ex.Message + &quot;Encode_Save&quot;);
        }
    }
}
</code></pre>
<p><strong>Input XML Data</strong>, stored in <code>clsGlobal.XMLDoc</code>:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;article dtd=&quot;RSCART3.8&quot;&gt;
   &lt;art-admin&gt;
      &lt;ms-id&gt;BK9781839161964-00123&lt;/ms-id&gt;
      &lt;doi&gt;10.1039/9781839165580-00123&lt;/doi&gt;
   &lt;/art-admin&gt;
   &lt;published type=&quot;book&quot;&gt;
      &lt;journalref&gt;
         &lt;title&gt;DNA Photodamage: From Light Absorption to Cellular Responses and Skin Cancer&lt;/title&gt;
         &lt;sercode&gt;BK&lt;/sercode&gt;
         &lt;publisher&gt;
            &lt;orgname&gt;
               &lt;nameelt&gt;Royal Society of Chemistry&lt;/nameelt&gt;
            &lt;/orgname&gt;
         &lt;/publisher&gt;
         &lt;issn type=&quot;isbn&quot; /&gt;
         &lt;cpyrt&gt;© European Society for Photobiology 2022&lt;/cpyrt&gt;
      &lt;/journalref&gt;
      &lt;volumeref&gt;
         &lt;link /&gt;
      &lt;/volumeref&gt;
      &lt;pubfront&gt;
         &lt;fpage&gt;0&lt;/fpage&gt;
         &lt;lpage&gt;0&lt;/lpage&gt;
         &lt;no-of-pages&gt;0&lt;/no-of-pages&gt;
         &lt;date&gt;
            &lt;year&gt;2022&lt;/year&gt;
         &lt;/date&gt;
      &lt;/pubfront&gt;
   &lt;/published&gt;
   &lt;art-front&gt;
      &lt;titlegrp&gt;
         &lt;title&gt;Chapter 2&lt;/title&gt;
         &lt;title&gt;In Silico Tools to Assess Chemical Hazard&lt;/title&gt;
      &lt;/titlegrp&gt;
      &lt;abstract&gt;
         &lt;p&gt;
            Fundamentally, chemical hazard is a function of structure, and the quickest and cheapest way to predict toxicity is to do so from structure alone. Currently, there are many tools available to predict absorption, distribution, metabolism, and excretion (ADME), as well as some key endpoints, such as LD
            &lt;inf&gt;50&lt;/inf&gt;
            (the minimal dose necessary to kill half the animals exposed), mutagenicity, skin sensitization, and ecotoxicity. While quantitative structure–activity relationships (QSARS) and read-across are well established, the field is rapidly changing with the advent of larger data sets and more sophisticated machine learning approaches. As computational power increases, 3D models may become widely available. However, virtually all models have blind spots, and some endpoints (such as developmental toxicity and endocrine disruption) have proven difficult to predict from structure alone – in these cases, it is necessary to use toxicity tests that capture the complexity of a biological system.
         &lt;/p&gt;
      &lt;/abstract&gt;
   &lt;/art-front&gt;
   &lt;art-body&gt;
      &lt;section&gt;
         &lt;no&gt;0.0&lt;/no&gt;
         &lt;title&gt;2.1 Introduction&lt;/title&gt;
         &lt;p&gt;
            “It is obvious that there must exist a relation between the chemical constitution and the physiological action of a substance, but as yet scarcely any attempts have been made to discover what this relation is. . . .”
            &lt;citref idrefs=&quot;cit1&quot;&gt;1&lt;/citref&gt;
            This was written in 1865 by Alexander Crum Brown, a chemist who worked in tandem with a medical student, and represents the very first conjecture of the basic principle that is the foundation of
            &lt;it&gt;in silico&lt;/it&gt;
            toxicology: that, fundamentally, chemical hazard is a function of chemical structure. In theory, then, the quickest and cheapest way to predict toxicity is to do so from structure alone. In practice, as we shall see, this is often challenging – but understanding what we can and cannot predict from structure alone is a good way to understand how chemicals affect biological systems.
         &lt;/p&gt;
         &lt;p&gt;
            At its most basic, a chemical can be said to be hazardous when it has the potential to interact with a biological system in a way that causes harm – or to use the regulatory term, “an adverse outcome.” Sometimes the negative effect is because a chemical is a mutagen –
            &lt;it&gt;e.g&lt;/it&gt;
            . an electrophilic chemical might cause alkylation of DNA, which is nucleophilic, resulting in an error in the genetic code and, potentially, cancer. Or, a chemical might have a structure that so closely mimics a biological molecule that it can interact with a receptor for the endogenous molecule – as happens when chemicals that are large and coplanar, such as diethylstilbestrol, bind to the estrogen receptor and therefore prevent normal endocrine signaling. Similar mechanisms are thought to underlie many of the chemicals that are considered potential endocrine disruptors. A chemical can displace something essential –
            &lt;it&gt;e.g&lt;/it&gt;
            . carbon monoxide (CO) binds more strongly to hemoglobin than oxygen, and in sufficient quantities, it will deprive tissues of oxygen, resulting in cellular death and eventually asphyxiation.
         &lt;/p&gt;
         &lt;p&gt;
            Sometimes hazard is a straightforward result of the chemical properties of a molecule – most strong acids or bases will cause skin and eye irritation. Other times there are several steps –
            &lt;it&gt;e.g&lt;/it&gt;
            . 2,4-dinitrochlorobenzene can easily be absorbed through the skin barrier, and then bind with many proteins in the dermal layer. These altered proteins (“haptens”) are then recognized by the immune system as “foreign material” – and because your immune system is always on the lookout for foreign proteins, it activates immune cells that respond to the hapten, creating an allergic reaction that will persist. In some cases, the chemical itself is not a problem, but once inside the body, it can be metabolized into something problematic, as in the case of acetaminophen.
         &lt;/p&gt;
         &lt;p&gt;
            There are two main components to predicting toxicity. Toxicokinetics refers to how the xenobiotic is absorbed, distributed, metabolized, and excreted. Fundamentally, the balance of these factors determines the biologically effective dose – the amount of a xenobiotic that can cause harm. Toxicodynamics refers to how the chemical reacts in a negative way with biological molecules – proteins, DNA, or the cell membrane. Ultimately, the dose and the manner in which a compound causes harm determines whether there are effects at the cellular level. Severe enough effects at the cellular level eventually cause organ damage – the harmful outcome referred to as an “adverse effect.” (
            &lt;figref idrefs=&quot;fig1&quot;&gt;Figure 2.1&lt;/figref&gt;
            ).
         &lt;/p&gt;
      &lt;/section&gt;      
      &lt;section&gt;
         &lt;no&gt;0.0&lt;/no&gt;
         &lt;title&gt;2.5 Conclusion&lt;/title&gt;
         &lt;p&gt;
            Skin sensitization is the one endpoint that also has multiple
            &lt;it&gt;in silico&lt;/it&gt;
            tools models available, ranging from SAR approaches such as ToxTree to more sophisticated QSARs:
            &lt;it&gt;e.g.&lt;/it&gt;
            PredSkin,
            &lt;citref idrefs=&quot;cit47&quot;&gt;47&lt;/citref&gt;
            which is based on human data and available
            &lt;it&gt;via&lt;/it&gt;
            the web, and the OECD QSAR Toolbox,
            &lt;citref idrefs=&quot;cit48&quot;&gt;48&lt;/citref&gt;
            which has an automated workflow for skin sensitization. In general, most of these models perform well (with the OECD QSAR Toolbox having 80% balanced accuracy) although the models differ in their sensitivity and specificity
            &lt;!--AQ27--&gt;
            . The value of 80% might seem disappointing, but the reality is that the animal test these models are built off of – the LLNA test – is only ≈80% reproducible,
            &lt;citref idrefs=&quot;cit46&quot;&gt;46&lt;/citref&gt;
            and although figures vary, it only predicts human sensitization with a similar level of accuracy.
            &lt;citref idrefs=&quot;cit49&quot;&gt;49&lt;/citref&gt;
            As yet, these models typically predict binary sensitization status, instead of potency, which is a significant drawback – many chemicals that are very weak sensitizers are often predicted as sensitizers although their actual hazard under most exposure conditions might be small. However, the
            &lt;it&gt;in silico&lt;/it&gt;
            models are, at this point, performing about as well as can be expected given the limitations of the data. Because skin sensitization represents an instance where the toxicodynamics are well understood – something we will discuss in Chapter 3 – it also offers an instance where
            &lt;it&gt;in vitro&lt;/it&gt;
            data can be used as an effective supplement in
            &lt;it&gt;in silico&lt;/it&gt;
            models. Further improvement will likely require new ways to think about combining
            &lt;it&gt;in silico&lt;/it&gt;
            ,
            &lt;it&gt;in chemico&lt;/it&gt;
            , and
            &lt;it&gt;in vitro&lt;/it&gt;
            data.
         &lt;/p&gt;
         &lt;p&gt;
            Currently, there are many tools available to predict ADME, as well as some key endpoints, such as LD
            &lt;inf&gt;50&lt;/inf&gt;
            , mutagenicity, skin sensitization, and ecotoxicity. We can predict some important endpoints based on others –
            &lt;it&gt;e.g.&lt;/it&gt;
            it does not take a great leap of imagination to understand that most skin irritants will also be eye irritants, even though the reverse is not always true. Skin sensitization should raise a concern for respiratory sensitization, although not conclusively as there are differences in bioavailability and mechanism that means this is not a universal rule.
            &lt;citref idrefs=&quot;cit50&quot;&gt;50&lt;/citref&gt;
            A chemical that interferes in DNA replication is likely to cause developmental effects should it go through the fetal–placental barrier, but there are many mechanisms by which a chemical can cause developmental effects, and there are no validated models that are considered robust enough for regulatory acceptance. In theory, read-across and QSARs can be used in a well-defined chemical class if the mechanism is known. In practice, given the well-known difficulty of connecting structure to developmental toxicity, this remains an endpoint that requires an
            &lt;it&gt;in vivo&lt;/it&gt;
            study for clarity.
         &lt;/p&gt;
         &lt;p&gt;
            Of course, no model is perfect and there are several caveats that apply to all models broadly. A model is only as good as the data that goes into it, and in many instances the data will have a great deal of noise as well as missing data. Most data sets assembled for predictive models will not cover a diverse area of the chemical space, and are often biased towards positives, for the simple reason that people tend not to gather data on chemicals that are largely biologically inert. However, this can be problematic:
            &lt;it&gt;e.g.&lt;/it&gt;
            if a data set consists of 100 chemicals, and 80% of them are considered skin sensitizers, a model that simply declares every chemical a sensitizer will have 80% accuracy. Therefore, when judging model performance, always look to the sensitivity, specificity, and balanced accuracy. Many models, like structural alerts and read-across, are better at identifying toxic compounds than establishing the absence of toxicity. While this is useful for screening-level approaches that are oriented towards being precautionary, it is problematic when trying to decide between chemical candidates in the R&amp;amp;D phase.
         &lt;/p&gt;
         &lt;p&gt;Passive diffusion is relatively easy to predict, because it depends solely on chemical properties, and because of this we have models that will predict diffusion across skin, intestine, and lung tissue. We can also predict whether a chemical will likely passively diffuse across the blood–brain barrier, but have few models that can identify transporter-mediated absorption. With the exception of the relatively well-studied PGP transporter, this has proven very difficult to model because of the diversity of transporters. The probability of a chemical being metabolized by a Phase I enzyme can also be predicted, even if the prediction of the metabolite is more difficult. Finally, based on physical chemical properties, we can estimate overall distribution, excretion, and half-life.&lt;/p&gt;
         &lt;p&gt;
            In terms of toxicodynamics – predicting biological targets of chemicals and the downstream effects –the search space is more complicated both because of the diversity of targets and the biological variability of the subsequent events. Endpoints with a straightforward connection to chemical structure –
            &lt;it&gt;e.g.&lt;/it&gt;
            mutagenicity and skin sensitization, which are both related to electrophilicity – can be proactively identified with structural alerts, and modeled with QSARs. More complicated endpoints can be predicted with limited success, and most such models should be treated with caution. If you do not truly understand the relationship between chemical structure and toxicity, read-across or QSARs will necessarily be limited – you can never know whether two similar molecules are in fact an activity cliff. Moreover, virtually all models will have some blindspots that will reflect the era in which they were developed as well as the data available, and if not updated will tend to become increasingly outdated.
         &lt;/p&gt;
         &lt;p&gt;
            Finally,
            &lt;it&gt;in silico&lt;/it&gt;
            approaches can only be used on discrete, organic structures
            &lt;!--AQ28--&gt;
            . By a rough estimate, however, that means that 50% of the chemicals within commerce cannot be evaluated with
            &lt;it&gt;in silico&lt;/it&gt;
            tools, as they are mixtures (called UVCBs), metal compounds, or salts, in addition to containing impurities – and even small amounts of impurities can give rise to adverse events (
            &lt;it&gt;e.g.&lt;/it&gt;
            sensitization or mutagenicity
            &lt;!--AQ29--&gt;
            ). Such chemicals are likely to increase as many bio-based chemicals are UVCBs, polymers, and engineered nanomaterials, which cannot be handled easily by existing
            &lt;it&gt;in silico&lt;/it&gt;
            tools.
         &lt;/p&gt;
         &lt;p&gt;Glossary&lt;/p&gt;
         &lt;figure id=&quot;fig1&quot; xsrc=&quot;BK9781839161964-00123-f1.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;
               Toxicokinetics and toxicodynamics together determine whether a xenobiotic will cause a disease. Adapted from ref.
               &lt;citref idrefs=&quot;cit51&quot;&gt;51&lt;/citref&gt;
               , https://doi.org/10.14573/altex.1610101, under the terms of the CC BY 4.0 license,
               &lt;url url=&quot;https://creativecommons.org/licenses/by/4.0/&quot;&gt;https://creativecommons.org/licenses/by/4.0/&lt;/url&gt;
               .
            &lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig2&quot; xsrc=&quot;BK9781839161964-00123-f2.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;Phase I metabolism involves either oxidation or hydrolysis, typically resulting in a more reactive intermediate. Phase II conjugates the compounds either with glutathione, in the case of electrophiles, or sulfation, acetylation, or glucuronidation to make a compound more water soluble.&lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig3&quot; xsrc=&quot;BK9781839161964-00123-f3.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;
               ADME is determined by absorption (ingestion, inhalation, or dermal), distribution primarily
               &lt;it&gt;via&lt;/it&gt;
               The blood and lymph, and excretion
               &lt;!--AQ87--&gt;
               .
            &lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig4&quot; xsrc=&quot;BK9781839161964-00123-f4.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;
               Paracetamol metabolism. Paracetamol can be immediately glucuronidated or sulfated without being metabolized by a Phase I enzyme. However, some will be oxidized
               &lt;it&gt;via&lt;/it&gt;
               CYP2E1 into a reactive intermediate.
            &lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig5&quot; xsrc=&quot;BK9781839161964-00123-f5.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;Phorbol ester structure, from PubChem.&lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig6&quot; xsrc=&quot;BK9781839161964-00123-f6.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;The ultimate rat carcinogen. Reproduced from Ref. 52, DOI:10.2788/6234, under the terms of the CC BY 4.0 license https://creativecommons.org/licenses/by/4.0/.&lt;/title&gt;
         &lt;/figure&gt;
         &lt;figure id=&quot;fig7&quot; xsrc=&quot;BK9781839161964-00123-f7.tif&quot; pos=&quot;float&quot;&gt;
            &lt;title&gt;Structural analogs for Bisphenol A as selected by GenRA. One the left is ToxPrints, on the right Morgan fingerprints.&lt;/title&gt;
         &lt;/figure&gt;         
         &lt;table-entry id=&quot;tab4&quot;&gt;
            &lt;title&gt;Table 2.4 Non-commercial read-across and QSAR&lt;/title&gt;
            &lt;table frame=&quot;topbot&quot;&gt;
               &lt;tgroup cols=&quot;3&quot; align=&quot;left&quot; colsep=&quot;1&quot; rowsep=&quot;1&quot; /&gt;
               &lt;colspec colnum=&quot;1&quot; colname=&quot;c1&quot; /&gt;
               &lt;colspec colnum=&quot;2&quot; colname=&quot;c2&quot; /&gt;
               &lt;colspec colnum=&quot;3&quot; colname=&quot;c3&quot; /&gt;
               &lt;thead /&gt;
               &lt;tbody&gt;
                  &lt;row&gt;
                     &lt;entry&gt;
                        &lt;bo&gt;
                           &lt;it&gt;Software&lt;/it&gt;
                        &lt;/bo&gt;
                     &lt;/entry&gt;
                     &lt;entry&gt;
                        &lt;bo&gt;
                           &lt;it&gt;Models available&lt;/it&gt;
                        &lt;/bo&gt;
                     &lt;/entry&gt;
                     &lt;entry&gt;
                        &lt;bo&gt;
                           &lt;it&gt;Platform&lt;/it&gt;
                        &lt;/bo&gt;
                     &lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry&gt;OECD QSAR Toolbox&lt;/entry&gt;
                     &lt;entry&gt;Read-across, QSARs, QSPR for multiple endpoints&lt;/entry&gt;
                     &lt;entry&gt;Requires Windows&lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry&gt;GenRA&lt;/entry&gt;
                     &lt;entry&gt;Read-across&lt;/entry&gt;
                     &lt;entry&gt;
                        Available
                        &lt;it&gt;via&lt;/it&gt;
                        Web at the EPA Comptox Dashboard
                     &lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry align=&quot;char&quot; char=&quot;.&quot;&gt;T.E.S.T.&lt;/entry&gt;
                     &lt;entry&gt;Global QSAR for acute toxicity, estrogen receptor binding, developmental toxicity, ecotoxicology endpoints&lt;/entry&gt;
                     &lt;entry&gt;
                        Available
                        &lt;it&gt;via&lt;/it&gt;
                        web at the EPA Comptox Dashboard and as stand-alone software
                     &lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry&gt;ECOSAR&lt;/entry&gt;
                     &lt;entry&gt;Ecotoxicology endpoints&lt;/entry&gt;
                     &lt;entry&gt;Available as stand-alone software&lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry&gt;VEGA&lt;/entry&gt;
                     &lt;entry&gt;ADME, Read-across, and QSAR for multiple endpoints&lt;/entry&gt;
                     &lt;entry&gt;Java application for Mac\Linux\&lt;/entry&gt;
                  &lt;/row&gt;
                  &lt;row&gt;
                     &lt;entry&gt;Danish QSAR Database&lt;/entry&gt;
                     &lt;entry&gt;Global QSARs based on existing models for multiple endpoints; applicability domain indicated&lt;/entry&gt;
                     &lt;entry&gt;
                        Available
                        &lt;it&gt;via&lt;/it&gt;
                        the web
                     &lt;/entry&gt;
                  &lt;/row&gt;
               &lt;/tbody&gt;
            &lt;/table&gt;
         &lt;/table-entry&gt;
      &lt;/section&gt;
   &lt;/art-body&gt;
   &lt;art-back&gt;
      &lt;biblist title=&quot;References&quot;&gt;
         &lt;citgroup id=&quot;cit1&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;A. C.&lt;/fname&gt;
                  &lt;surname&gt;Brown&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;T. R.&lt;/fname&gt;
                  &lt;surname&gt;Fraser&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;arttitle&gt;On the Connection between Chemical Constitution and Physiological Action; with special reference to the Physiological Action of the Salts of the Ammonium Bases derived from Strychnia, Brucia, Thebaia, Codeia, Morphia, and Nicotia&lt;/arttitle&gt;
               &lt;title&gt;J. Anat. Physiol.&lt;/title&gt;
               &lt;year&gt;1868&lt;/year&gt;
               &lt;volumeno&gt;2&lt;/volumeno&gt;
               &lt;pages&gt;
                  &lt;fpage&gt;224&lt;/fpage&gt;
                  &lt;lpage&gt;242&lt;/lpage&gt;
               &lt;/pages&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;
         &lt;citgroup id=&quot;cit2&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;C.&lt;/fname&gt;
                  &lt;surname&gt;Lynch&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;title&gt;Anesth. Analg.&lt;/title&gt;
               &lt;year&gt;2008&lt;/year&gt;
               &lt;volumeno&gt;107&lt;/volumeno&gt;
               &lt;pages&gt;
                  &lt;fpage&gt;864&lt;/fpage&gt;
                  &lt;lpage&gt;867&lt;/lpage&gt;
               &lt;/pages&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;
         &lt;citgroup id=&quot;cit3&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;C. A.&lt;/fname&gt;
                  &lt;surname&gt;Lipinski&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;arttitle&gt;Lead- and drug-like compounds: the rule-of-five revolution&lt;/arttitle&gt;
               &lt;title&gt;Drug Discov. Today Technol.&lt;/title&gt;
               &lt;year&gt;2004&lt;/year&gt;
               &lt;volumeno&gt;1&lt;/volumeno&gt;
               &lt;pages&gt;
                  &lt;fpage&gt;337&lt;/fpage&gt;
                  &lt;lpage&gt;341&lt;/lpage&gt;
               &lt;/pages&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;
         &lt;citgroup id=&quot;cit4&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;D.&lt;/fname&gt;
                  &lt;surname&gt;Epel&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;T.&lt;/fname&gt;
                  &lt;surname&gt;Luckenbach&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;C. N.&lt;/fname&gt;
                  &lt;surname&gt;Stevenson&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;L. A.&lt;/fname&gt;
                  &lt;surname&gt;Macmanus-Spencer&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;A.&lt;/fname&gt;
                  &lt;surname&gt;Hamdoun&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;T.&lt;/fname&gt;
                  &lt;surname&gt;Smital&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;arttitle&gt;Efflux transporters: newly appreciated roles in protection against pollutants&lt;/arttitle&gt;
               &lt;title&gt;Environ. Sci. Technol.&lt;/title&gt;
               &lt;year&gt;2008&lt;/year&gt;
               &lt;volumeno&gt;42&lt;/volumeno&gt;
               &lt;pages&gt;
                  &lt;fpage&gt;3914&lt;/fpage&gt;
                  &lt;lpage&gt;3920&lt;/lpage&gt;
               &lt;/pages&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;
         &lt;citgroup id=&quot;cit5&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;L.-A.&lt;/fname&gt;
                  &lt;surname&gt;Clerbaux&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;A.&lt;/fname&gt;
                  &lt;surname&gt;Paini&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;A.&lt;/fname&gt;
                  &lt;surname&gt;Lumen&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;H.&lt;/fname&gt;
                  &lt;surname&gt;Osman-Ponchet&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;A. P.&lt;/fname&gt;
                  &lt;surname&gt;Worth&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;citauth&gt;
                  &lt;fname&gt;O.&lt;/fname&gt;
                  &lt;surname&gt;Fardel&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;arttitle&gt;Membrane transporter data to support kinetically-informed chemical risk assessment using non-animal methods: Scientific and regulatory perspectives&lt;/arttitle&gt;
               &lt;title&gt;Environ. Int.&lt;/title&gt;
               &lt;year&gt;2019&lt;/year&gt;
               &lt;volumeno&gt;126&lt;/volumeno&gt;
               &lt;pages&gt;
                  &lt;fpage&gt;659&lt;/fpage&gt;
                  &lt;lpage&gt;671&lt;/lpage&gt;
               &lt;/pages&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;         
         &lt;citgroup id=&quot;cit54&quot;&gt;
            &lt;journalcit&gt;
               &lt;citauth&gt;
                  &lt;surname&gt;Oecd&lt;/surname&gt;
               &lt;/citauth&gt;
               &lt;arttitle&gt;Data from: EChemPortal: Global portal to information on chemical substances&lt;/arttitle&gt;
               &lt;title&gt;OECD Obs.&lt;/title&gt;
            &lt;/journalcit&gt;
         &lt;/citgroup&gt;
      &lt;/biblist&gt;
      &lt;compoundgrp /&gt;
      &lt;annotationgrp /&gt;
      &lt;datagrp /&gt;
      &lt;resourcegrp /&gt;
   &lt;/art-back&gt;
   &lt;!--MAQ1: AQ: Please insert the expansion for the acronym ‘PGP’ if appropriate for the reader.--&gt;
   &lt;!--MAQ2: CE: The sentence beginning ‘The PGP transporter is expressed.’ has been altered for clarity, please check that the meaning is correct.--&gt;
   &lt;!--MAQ3: &lt;AQ&gt;The sentence beginning ‘The fraction unbound in plasma.’ has been altered for clarity, please check that the meaning is correct.&lt;/AQ&gt;--&gt;
   &lt;!--MAQ5: &lt;AQ&gt;In the sentence beginning ‘In pharmacology studies.’ a word or phrase appears to be missing after ‘is known and the remaining’. Please check this carefully and indicate any changes required here.&lt;/AQ&gt;--&gt;   
&lt;/article&gt;
</code></pre>
",61,2,-1,4,c#;xml;encoding;xmlwriter,2022-07-07 18:49:49,2022-07-07 18:49:49,2022-07-10 00:33:15,i am attempting to write an xmldocument to a file using an encoding other than utf  encoding ascii in this case  and have xmlwriter automatically replace characters not supported by the encoding with equivalent character entities   to do this  i am using the sample code from    however  for my xml  rather than replacing characters with character entities  it is throwing an exception like the below  unable to translate unicode character  u at index  to specified code page encode_save what is the cause of this exception   why aren t the unsupported characters getting escaped as expected  code used  my serialization code  the code for save   from the linked question  input xml data  stored in clsglobal xmldoc ,why is xmlwriter failing to replace characters not supported by the current encoding with equivalent character entities ,attempting write xmldocument file using encoding utf encoding ascii case xmlwriter automatically replace characters supported encoding equivalent character entities using sample code however xml rather replacing characters character entities throwing exception like unable translate unicode character u index specified code page encode_save cause exception unsupported characters getting escaped expected code used serialization code code save linked question input xml data stored clsglobal xmldoc,xmlwriter failing replace characters supported current encoding equivalent character entities,xmlwriter failing replace characters supported current encoding equivalent character entitiesattempting write xmldocument file using encoding utf encoding ascii case xmlwriter automatically replace characters supported encoding equivalent character entities using sample code however xml rather replacing characters character entities throwing exception like unable translate unicode character u index specified code page encode_save cause exception unsupported characters getting escaped expected code used serialization code code save linked question input xml data stored clsglobal xmldoc,"['xmlwriter', 'failing', 'replace', 'characters', 'supported', 'current', 'encoding', 'equivalent', 'character', 'entitiesattempting', 'write', 'xmldocument', 'file', 'using', 'encoding', 'utf', 'encoding', 'ascii', 'case', 'xmlwriter', 'automatically', 'replace', 'characters', 'supported', 'encoding', 'equivalent', 'character', 'entities', 'using', 'sample', 'code', 'however', 'xml', 'rather', 'replacing', 'characters', 'character', 'entities', 'throwing', 'exception', 'like', 'unable', 'translate', 'unicode', 'character', 'u', 'index', 'specified', 'code', 'page', 'encode_save', 'cause', 'exception', 'unsupported', 'characters', 'getting', 'escaped', 'expected', 'code', 'used', 'serialization', 'code', 'code', 'save', 'linked', 'question', 'input', 'xml', 'data', 'stored', 'clsglobal', 'xmldoc']","['xmlwriter', 'fail', 'replac', 'charact', 'support', 'current', 'encod', 'equival', 'charact', 'entitiesattempt', 'write', 'xmldocument', 'file', 'use', 'encod', 'utf', 'encod', 'ascii', 'case', 'xmlwriter', 'automat', 'replac', 'charact', 'support', 'encod', 'equival', 'charact', 'entiti', 'use', 'sampl', 'code', 'howev', 'xml', 'rather', 'replac', 'charact', 'charact', 'entiti', 'throw', 'except', 'like', 'unabl', 'translat', 'unicod', 'charact', 'u', 'index', 'specifi', 'code', 'page', 'encode_sav', 'caus', 'except', 'unsupport', 'charact', 'get', 'escap', 'expect', 'code', 'use', 'serial', 'code', 'code', 'save', 'link', 'question', 'input', 'xml', 'data', 'store', 'clsglobal', 'xmldoc']"
69,75,75,19517141,72923148,The Impact of TV Advertising on Website Traffic,"<p>I need to build a model that measures the impact of TV advertising on website traffic.</p>
<p>I have two datasets: one contains the number of visits to the page and a timestamp, the other contains a timestamp and TV ad data such as a channel, information about whether the ads are shown in the middle, after or before a TV show, and so on.</p>
<p>My problem is that I don't know how to merge these datasets as the timestamps in the datasets have different granularity and then which machine learning method should be appropriate to measure this impact.</p>
<p>If you have any ideas or experiences please let me know.</p>
",18,0,-4,4,python;machine-learning;modeling;web-traffic,2022-07-09 22:16:14,2022-07-09 22:16:14,2022-07-09 23:02:03,i need to build a model that measures the impact of tv advertising on website traffic  i have two datasets  one contains the number of visits to the page and a timestamp  the other contains a timestamp and tv ad data such as a channel  information about whether the ads are shown in the middle  after or before a tv show  and so on  my problem is that i don t know how to merge these datasets as the timestamps in the datasets have different granularity and then which machine learning method should be appropriate to measure this impact  if you have any ideas or experiences please let me know ,the impact of tv advertising on website traffic,need build model measures impact tv advertising website traffic two datasets one contains number visits page timestamp contains timestamp tv ad data channel information whether ads shown middle tv show problem know merge datasets timestamps datasets different granularity machine learning method appropriate measure impact ideas experiences please let know,impact tv advertising website traffic,impact tv advertising website trafficneed build model measures impact tv advertising website traffic two datasets one contains number visits page timestamp contains timestamp tv ad data channel information whether ads shown middle tv show problem know merge datasets timestamps datasets different granularity machine learning method appropriate measure impact ideas experiences please let know,"['impact', 'tv', 'advertising', 'website', 'trafficneed', 'build', 'model', 'measures', 'impact', 'tv', 'advertising', 'website', 'traffic', 'two', 'datasets', 'one', 'contains', 'number', 'visits', 'page', 'timestamp', 'contains', 'timestamp', 'tv', 'ad', 'data', 'channel', 'information', 'whether', 'ads', 'shown', 'middle', 'tv', 'show', 'problem', 'know', 'merge', 'datasets', 'timestamps', 'datasets', 'different', 'granularity', 'machine', 'learning', 'method', 'appropriate', 'measure', 'impact', 'ideas', 'experiences', 'please', 'let', 'know']","['impact', 'tv', 'advertis', 'websit', 'trafficne', 'build', 'model', 'measur', 'impact', 'tv', 'advertis', 'websit', 'traffic', 'two', 'dataset', 'one', 'contain', 'number', 'visit', 'page', 'timestamp', 'contain', 'timestamp', 'tv', 'ad', 'data', 'channel', 'inform', 'whether', 'ad', 'shown', 'middl', 'tv', 'show', 'problem', 'know', 'merg', 'dataset', 'timestamp', 'dataset', 'differ', 'granular', 'machin', 'learn', 'method', 'appropri', 'measur', 'impact', 'idea', 'experi', 'pleas', 'let', 'know']"
70,76,76,14328643,72844991,Code Challange Joining repeated numbers on an array,"<p>my output from a machine learning algorithm is a list of segments, segments are represented by a pair of ids, each segment is part of a track(ideally, there are repeated ids and missing ids), and the following code produces a similar array:</p>
<pre><code>    import numpy as np
    np.random.seed(42)
    #ids pairs are less the further you go on the track
    ids_ammount= np.arange(26,8,-2)
    # the array is a 2 column array  with conected segments
    ids_array = np.zeros((np.sum(ids_ammount),2))
    idx_0 = 0
    idx_1 = 0
    for i, ids in enumerate(ids_ammount):
      idx_1+=ids
      #first column is in order from smaller to largest id
      ids_array[:,0][idx_0:idx_1] = np.sort(np.random.randint(i * 10,10*(i+1),size=ids))
      ids_array[:,1][idx_0:idx_1] = np.random.randint((i + 1) * 10,10*(i+2),size=ids)
      idx_0+= ids
</code></pre>
<p>my goal is to reconstruct the tracks using <strong>ids_array</strong>, I have an implementation but is really inefficient with 3 <strong>if</strong> statements and I need to check manually the max number of repeated ids every time in order to make it work, so I prefer not to include it as I would like to get some fresh ideas on how to solve this as efficient as possible.
My desired output would be 9 arrays, one with ids that have just one connection, the other with 2 connections, and so on until 9 connections, example:</p>
<p>single_conection = [[0,18],[67,73]...],
double_conection = [[1,14,26], [53,68,79],....]
.
.
.
nine_conections = [[3,15,28,36,41,59,66,79,82,99]....]</p>
<p>Some of the problems that I encounter are, that ids are repeated, some ids don't exist, and the maximum number of repeated ids is variable.
If someone has an idea on how to reconstruct these tracks would be amazing, and if anyone knows how to do it efficiently would be even better.
Thanks</p>
",59,1,0,5,python;arrays;algorithm;performance;sorting,2022-07-03 14:01:54,2022-07-03 14:01:54,2022-07-09 19:04:54,my output from a machine learning algorithm is a list of segments  segments are represented by a pair of ids  each segment is part of a track ideally  there are repeated ids and missing ids   and the following code produces a similar array ,code challange joining repeated numbers on an array,output machine learning algorithm segments segments represented pair ids segment part track ideally repeated ids missing ids following code produces similar array,code challange joining repeated numbers array,code challange joining repeated numbers arrayoutput machine learning algorithm segments segments represented pair ids segment part track ideally repeated ids missing ids following code produces similar array,"['code', 'challange', 'joining', 'repeated', 'numbers', 'arrayoutput', 'machine', 'learning', 'algorithm', 'segments', 'segments', 'represented', 'pair', 'ids', 'segment', 'part', 'track', 'ideally', 'repeated', 'ids', 'missing', 'ids', 'following', 'code', 'produces', 'similar', 'array']","['code', 'challang', 'join', 'repeat', 'number', 'arrayoutput', 'machin', 'learn', 'algorithm', 'segment', 'segment', 'repres', 'pair', 'id', 'segment', 'part', 'track', 'ideal', 'repeat', 'id', 'miss', 'id', 'follow', 'code', 'produc', 'similar', 'array']"
71,77,77,14429324,66687941,Openpyxl load_workbook gives me KeyError,"<p>I'm using Jupyter Notebook and I'm trying to convert a file <code>.xlsx</code> into pandas DataFrame but <code>openpyxl</code> keeps giving me error.</p>
<p>My file's name is <code>Dry_Bean_Dataset.xlsx</code>, so what I'm doing is setting the path to that file and then creating the workbook with <code>openpyxl</code>. However, when I try the 'ws' function, it gives me KeyError.</p>
<pre><code>import pandas as pd
from openpyxl import load_workbook
path2= 'C:/Users/nbravo/OneDrive - Schenck Process Group Holding/Documents/13. Master of Information Technology/2. Semestre 1 - 2021/Machine Learning/Lab02/DryBeanDataset/Dry_Bean_Dataset.xlsx'
wb = load_workbook(path2)
ws = wb['Dry_Bean_Dataset']
</code></pre>
<p>And this is the error that I get:</p>
<hr />
<p>KeyError                                  Traceback (most recent call last)
 in 
----&gt; 1 ws = wb['Dry_Bean_Dataset']</p>
<p>~\Anaconda3\lib\site-packages\openpyxl\workbook\workbook.py in __ getitem__(self, key)
271             if sheet.title == key:
272                 return sheet
--&gt; 273         raise KeyError(&quot;Worksheet {0} does not exist.&quot;.format(key))
274
275     def __ delitem__(self, key):</p>
<p>KeyError: 'Worksheet Dry_Bean_Dataset does not exist.'</p>
",147,0,2,4,python;pandas;jupyter-notebook;openpyxl,2021-03-18 14:40:34,2021-03-18 14:40:34,2022-07-09 18:20:35,i m using jupyter notebook and i m trying to convert a file  xlsx into pandas dataframe but openpyxl keeps giving me error  my file s name is dry_bean_dataset xlsx  so what i m doing is setting the path to that file and then creating the workbook with openpyxl  however  when i try the  ws  function  it gives me keyerror  and this is the error that i get  keyerror   worksheet dry_bean_dataset does not exist  ,openpyxl load_workbook gives me keyerror,using jupyter notebook trying convert file xlsx pandas dataframe openpyxl keeps giving error file name dry_bean_dataset xlsx setting path file creating workbook openpyxl however try ws function gives keyerror error get keyerror worksheet dry_bean_dataset exist,openpyxl load_workbook gives keyerror,openpyxl load_workbook gives keyerrorusing jupyter notebook trying convert file xlsx pandas dataframe openpyxl keeps giving error file name dry_bean_dataset xlsx setting path file creating workbook openpyxl however try ws function gives keyerror error get keyerror worksheet dry_bean_dataset exist,"['openpyxl', 'load_workbook', 'gives', 'keyerrorusing', 'jupyter', 'notebook', 'trying', 'convert', 'file', 'xlsx', 'pandas', 'dataframe', 'openpyxl', 'keeps', 'giving', 'error', 'file', 'name', 'dry_bean_dataset', 'xlsx', 'setting', 'path', 'file', 'creating', 'workbook', 'openpyxl', 'however', 'try', 'ws', 'function', 'gives', 'keyerror', 'error', 'get', 'keyerror', 'worksheet', 'dry_bean_dataset', 'exist']","['openpyxl', 'load_workbook', 'give', 'keyerrorus', 'jupyt', 'notebook', 'tri', 'convert', 'file', 'xlsx', 'panda', 'datafram', 'openpyxl', 'keep', 'give', 'error', 'file', 'name', 'dry_bean_dataset', 'xlsx', 'set', 'path', 'file', 'creat', 'workbook', 'openpyxl', 'howev', 'tri', 'ws', 'function', 'give', 'keyerror', 'error', 'get', 'keyerror', 'worksheet', 'dry_bean_dataset', 'exist']"
72,78,78,19349751,72920959,How to increase the score in Kaggle competition?,"<p>I am new to Kaggle and recently I submitted the notebook for the <strong>Titanic - Machine Learning from Disaster</strong> competition. My Logistic Regression model is giving me <strong>80% accuracy</strong> but when I submitted the same CSV file, my Kaggle score is <strong>0</strong>.</p>
<p>So I wanted to know what is going wrong and how to fix this issue.</p>
",26,0,-3,4,python;machine-learning;regression;kaggle,2022-07-09 16:54:51,2022-07-09 16:54:51,2022-07-09 16:54:51,i am new to kaggle and recently i submitted the notebook for the titanic   machine learning from disaster competition  my logistic regression model is giving me   accuracy but when i submitted the same csv file  my kaggle score is   so i wanted to know what is going wrong and how to fix this issue ,how to increase the score in kaggle competition ,kaggle recently submitted notebook titanic machine learning disaster competition logistic regression model giving accuracy submitted csv file kaggle score wanted know going wrong fix issue,increase score kaggle competition,increase score kaggle competitionkaggle recently submitted notebook titanic machine learning disaster competition logistic regression model giving accuracy submitted csv file kaggle score wanted know going wrong fix issue,"['increase', 'score', 'kaggle', 'competitionkaggle', 'recently', 'submitted', 'notebook', 'titanic', 'machine', 'learning', 'disaster', 'competition', 'logistic', 'regression', 'model', 'giving', 'accuracy', 'submitted', 'csv', 'file', 'kaggle', 'score', 'wanted', 'know', 'going', 'wrong', 'fix', 'issue']","['increas', 'score', 'kaggl', 'competitionkaggl', 'recent', 'submit', 'notebook', 'titan', 'machin', 'learn', 'disast', 'competit', 'logist', 'regress', 'model', 'give', 'accuraci', 'submit', 'csv', 'file', 'kaggl', 'score', 'want', 'know', 'go', 'wrong', 'fix', 'issu']"
73,79,79,19387078,72918614,"Overfitting Gradient Boosting, should I increase or decrease learning rate?","<p>I was reading how to regularize gradient boosting by adjusting the learning rate and cant seem to understand why a smaller learning rate can regularize better. In the book Hands-On Machine Learning with Scikit-Learn, keras and Tensorflow, the author suggests decreasing learning rate. However, at the same time the author compares a large learning rate 1, with a small learning rate 0.1, the graph for rate 1 seems far more regularize than 0.1.</p>
<p>Here's my take on this:</p>
<ol>
<li>It will regularize better if you decrease the number of iterations and a smaller learning rate (can't just lower learning rate)</li>
<li>Or just increase the learning rate</li>
</ol>
<p>Is this idea anywhere near correct? Or am I completely wrong. Thanks!</p>
",26,0,-2,3,machine-learning;ensemble-learning;boosting,2022-07-09 08:26:50,2022-07-09 08:26:50,2022-07-09 08:26:50,i was reading how to regularize gradient boosting by adjusting the learning rate and cant seem to understand why a smaller learning rate can regularize better  in the book hands on machine learning with scikit learn  keras and tensorflow  the author suggests decreasing learning rate  however  at the same time the author compares a large learning rate   with a small learning rate    the graph for rate  seems far more regularize than    here s my take on this  is this idea anywhere near correct  or am i completely wrong  thanks ,overfitting gradient boosting  should i increase or decrease learning rate ,reading regularize gradient boosting adjusting learning rate cant seem understand smaller learning rate regularize better book hands machine learning scikit learn keras tensorflow author suggests decreasing learning rate however time author compares large learning rate small learning rate graph rate seems far regularize take idea anywhere near correct completely wrong thanks,overfitting gradient boosting increase decrease learning rate,overfitting gradient boosting increase decrease learning ratereading regularize gradient boosting adjusting learning rate cant seem understand smaller learning rate regularize better book hands machine learning scikit learn keras tensorflow author suggests decreasing learning rate however time author compares large learning rate small learning rate graph rate seems far regularize take idea anywhere near correct completely wrong thanks,"['overfitting', 'gradient', 'boosting', 'increase', 'decrease', 'learning', 'ratereading', 'regularize', 'gradient', 'boosting', 'adjusting', 'learning', 'rate', 'cant', 'seem', 'understand', 'smaller', 'learning', 'rate', 'regularize', 'better', 'book', 'hands', 'machine', 'learning', 'scikit', 'learn', 'keras', 'tensorflow', 'author', 'suggests', 'decreasing', 'learning', 'rate', 'however', 'time', 'author', 'compares', 'large', 'learning', 'rate', 'small', 'learning', 'rate', 'graph', 'rate', 'seems', 'far', 'regularize', 'take', 'idea', 'anywhere', 'near', 'correct', 'completely', 'wrong', 'thanks']","['overfit', 'gradient', 'boost', 'increas', 'decreas', 'learn', 'rateread', 'regular', 'gradient', 'boost', 'adjust', 'learn', 'rate', 'cant', 'seem', 'understand', 'smaller', 'learn', 'rate', 'regular', 'better', 'book', 'hand', 'machin', 'learn', 'scikit', 'learn', 'kera', 'tensorflow', 'author', 'suggest', 'decreas', 'learn', 'rate', 'howev', 'time', 'author', 'compar', 'larg', 'learn', 'rate', 'small', 'learn', 'rate', 'graph', 'rate', 'seem', 'far', 'regular', 'take', 'idea', 'anywher', 'near', 'correct', 'complet', 'wrong', 'thank']"
74,80,80,18726236,72778858,"neural network not optimizing weights of first layer, returning all 1&#39;s for z1","<p>So im building a neural network in python right now losely following Andrew Ng's machine learning course. It has 3 layers (all sigmoid) and works on predicting the MNIST dataset. But it fails to actually predict the dataset, and while the cost is decreasing with every iteration accuracy remains at around 0.1, indicating something isnt working properly.
After playing around and letting the program display the different steps i noticed that z1 (calculated from the train_X@theta1 in the activation layer) is pretty much uniformly 1's, which seems to be hindering the network to function. This is due to the train_X@theta1 being high values, making e^(-z) basically 0 and the sigmoid function returning basically 1. But that also leads to the derivative for theta1 (due to being multiplied by (1-z1) to just be 0's, hindering the network from doing anything.
Things i have tried are:</p>
<p>Regularizing the dataset</p>
<p>Playing around with alpha values (which doesnt do anything, as expected)</p>
<p>Making theta1 values start out really small (all calculated through a uniform distribution, which i then multiplied by like 0.000001): this, while fixing the problem for the first iteration just makes the network optimize theta1 to where it leads to uniformly 1's at z1 again, so i suspect backwards/forward propagation to not be working properly? Even though I basically copied that from one of my working solutions to an exercise in the course in octave, which produced resonable results and ended up with a high accuracy.</p>
<p>Here is my algorithm:</p>
<pre><code>def nn_forwardPropagation(train_X, train_Y, theta1, theta2, theta3):
    accuracy = 0
    J = 0
    #forward propagation, always adding 1's for the bias unit
    train_X = np.c_[ np.ones((np.shape(train_X)[0],1)), train_X]
    z1 = sigmoid(train_X@np.transpose(theta1))
    a1 = np.c_[ np.ones((np.shape(z1)[0],1)), z1]
    z2 = sigmoid(a1@np.transpose(theta2))
    a2 = np.c_[ np.ones((np.shape(z2)[0],1)), z2]
    #last step
    predValues = np.zeros((10,len(train_Y)))
    y1 = predValues.copy()
    predValues2 = y1.copy()
    for j in range(len(train_Y)):
        for i in range(np.shape(theta3)[0]):
            predValues[i,j] = sigmoid(a2[j,:]@np.transpose(theta3[i,:]))
        #making y1 our &quot;target matrix&quot; where we would like to see a 1 at the place of the right number and 0's everywhere else 
        y1[train_Y[j],j] = 1
        J += np.sum((np.transpose(-y1[:,j])@np.log(predValues[:,j]))  -  (1-np.transpose(y1[:,j]))@np.log(1 - predValues[:,j]))/len(train_Y)
        #in order to calculate accuracy, we just assume that the highest value in our predicted values dictates which value is &quot;right&quot; and looks if that's right
        predValues2[np.where(predValues[:,j] == max(predValues[:,j])),j] = 1 
        if np.array_equiv(predValues2[:,j], y1[:,j]):
            accuracy += 1/len(train_Y)
    #calculating the derivatives
    delta3 = predValues - y1  
    delta2 = np.transpose(delta3) @ theta3[:,1:] * z2 * (1-z2)
    delta1 = delta2 @ theta2[:,1:] * z1 * (1-z1)
    D3 = delta3 @ a2 
    D2 = np.transpose(delta2) @ a1
    D1 = np.transpose(delta1) @ train_X
    theta3_grad = D3 / len(train_Y)
    theta2_grad = D2 / len(train_Y)
    theta1_grad = D1 / len(train_Y)
    return J, theta3_grad, theta2_grad, theta1_grad, accuracy
    
def nn_runner(train_X, train_Y, iterations, alpha1, alpha2, alpha3):
    theta1 = np.random.rand(4,785)
    theta2 = np.random.rand(7,5)
    theta3 = np.random.rand(10,8)
    for i in range(1,iterations+1):
        J, theta3_grad, theta2_grad, theta1_grad, accuracy = nn_forwardPropagation(train_X, train_Y, theta1, theta2, theta3)
        theta3 -= alpha3*theta3_grad
        theta2 -= alpha2*theta2_grad
        theta1 -= alpha1*theta1_grad
        if i % 1 == 0:
            print(&quot;At iteration&quot;,i,&quot;:&quot;, J)
    print(&quot;This amounts to a total accuracy on training data of&quot;, accuracy)
    return theta3, theta2, theta1
</code></pre>
<p>Does anybody here know what i did wrong/could look into to make this working?
Thanks</p>
",47,1,0,4,python;machine-learning;neural-network;mnist,2022-06-28 03:52:00,2022-06-28 03:52:00,2022-07-09 04:46:44,regularizing the dataset playing around with alpha values  which doesnt do anything  as expected  making theta values start out really small  all calculated through a uniform distribution  which i then multiplied by like     this  while fixing the problem for the first iteration just makes the network optimize theta to where it leads to uniformly  s at z again  so i suspect backwards forward propagation to not be working properly  even though i basically copied that from one of my working solutions to an exercise in the course in octave  which produced resonable results and ended up with a high accuracy  here is my algorithm ,neural network not optimizing weights of first layer  returning all    s for z,regularizing dataset playing around alpha values doesnt anything expected making theta values start really small calculated uniform distribution multiplied like fixing problem first iteration makes network optimize theta leads uniformly z suspect backwards forward propagation working properly even though basically copied one working solutions exercise course octave produced resonable results ended high accuracy algorithm,neural network optimizing weights first layer returning z,neural network optimizing weights first layer returning zregularizing dataset playing around alpha values doesnt anything expected making theta values start really small calculated uniform distribution multiplied like fixing problem first iteration makes network optimize theta leads uniformly z suspect backwards forward propagation working properly even though basically copied one working solutions exercise course octave produced resonable results ended high accuracy algorithm,"['neural', 'network', 'optimizing', 'weights', 'first', 'layer', 'returning', 'zregularizing', 'dataset', 'playing', 'around', 'alpha', 'values', 'doesnt', 'anything', 'expected', 'making', 'theta', 'values', 'start', 'really', 'small', 'calculated', 'uniform', 'distribution', 'multiplied', 'like', 'fixing', 'problem', 'first', 'iteration', 'makes', 'network', 'optimize', 'theta', 'leads', 'uniformly', 'z', 'suspect', 'backwards', 'forward', 'propagation', 'working', 'properly', 'even', 'though', 'basically', 'copied', 'one', 'working', 'solutions', 'exercise', 'course', 'octave', 'produced', 'resonable', 'results', 'ended', 'high', 'accuracy', 'algorithm']","['neural', 'network', 'optim', 'weight', 'first', 'layer', 'return', 'zregular', 'dataset', 'play', 'around', 'alpha', 'valu', 'doesnt', 'anyth', 'expect', 'make', 'theta', 'valu', 'start', 'realli', 'small', 'calcul', 'uniform', 'distribut', 'multipli', 'like', 'fix', 'problem', 'first', 'iter', 'make', 'network', 'optim', 'theta', 'lead', 'uniformli', 'z', 'suspect', 'backward', 'forward', 'propag', 'work', 'properli', 'even', 'though', 'basic', 'copi', 'one', 'work', 'solut', 'exercis', 'cours', 'octav', 'produc', 'reson', 'result', 'end', 'high', 'accuraci', 'algorithm']"
75,81,81,5930146,48013034,Full &amp; complete uninstall QT Creator,"<p>Question:</p>

<p><strong>How to uninstall QT Creator from Linux machine?</strong></p>

<p>I have a corrupted install on a Debian Jessie machine. I have tried reinstalling, fixing, etc. and with no luck. I was advised to uninstall completely, grab the newest version and reinstall.</p>

<p>The problem is that I'm not sure how to do this and every time I try, QT keeps coming back with the same exact issues, I'm assuming because of some config files that are not getting deleted in the removal process.</p>

<p>Running <code>qmake --version</code> gives me this output:</p>

<pre><code>QMake version 3.0
Using Qt version 5.6.2 in /home/greg/anaconda2/lib
</code></pre>

<p>Running <code>pkg-config --modversion QtCore</code> gives me this output:</p>

<pre><code>Package QtCore was not found in the pkg-config search path.
Perhaps you should add the directory containing `QtCore.pc'
to the PKG_CONFIG_PATH environment variable
No package 'QtCore' found
</code></pre>

<p>The problem is that I've got several versions installed, in different folders and I want to completely start over with a fresh install of QT Creator. Version 5.10 was installed yesterday via the QT maintenence tool, but for some reason won't run. I just need to remove it and start over. QT 5.9.3 and 5.10 are installed at <code>/home/greg</code> and you can see 5.6.2 is installed in my anaconda2 folder. Not sure how this happened. I'm still a newb so still learning.</p>

<p><strong>How can I do a complete/full uninstall and removal of ALL QT creator files without deleting other QT files, such as KDE dependencies, etc., so that I can do a fresh install with new config settings????</strong></p>
",12060,1,4,3,qt;qt-creator;debian-jessie,2017-12-29 00:39:41,2017-12-29 00:39:41,2022-07-09 03:38:05,question  how to uninstall qt creator from linux machine  i have a corrupted install on a debian jessie machine  i have tried reinstalling  fixing  etc  and with no luck  i was advised to uninstall completely  grab the newest version and reinstall  the problem is that i m not sure how to do this and every time i try  qt keeps coming back with the same exact issues  i m assuming because of some config files that are not getting deleted in the removal process  running qmake   version gives me this output  running pkg config   modversion qtcore gives me this output  the problem is that i ve got several versions installed  in different folders and i want to completely start over with a fresh install of qt creator  version   was installed yesterday via the qt maintenence tool  but for some reason won t run  i just need to remove it and start over  qt    and   are installed at  home greg and you can see    is installed in my anaconda folder  not sure how this happened  i m still a newb so still learning  how can i do a complete full uninstall and removal of all qt creator files without deleting other qt files  such as kde dependencies  etc   so that i can do a fresh install with new config settings    ,full  amp  complete uninstall qt creator,question uninstall qt creator linux machine corrupted install debian jessie machine tried reinstalling fixing etc luck advised uninstall completely grab newest version reinstall problem sure every time try qt keeps coming back exact issues assuming config files getting deleted removal process running qmake version gives output running pkg config modversion qtcore gives output problem got several versions installed different folders want completely start fresh install qt creator version installed yesterday via qt maintenence tool reason run need remove start qt installed home greg see installed anaconda folder sure happened still newb still learning complete full uninstall removal qt creator files without deleting qt files kde dependencies etc fresh install config settings,full amp complete uninstall qt creator,full amp complete uninstall qt creatorquestion uninstall qt creator linux machine corrupted install debian jessie machine tried reinstalling fixing etc luck advised uninstall completely grab newest version reinstall problem sure every time try qt keeps coming back exact issues assuming config files getting deleted removal process running qmake version gives output running pkg config modversion qtcore gives output problem got several versions installed different folders want completely start fresh install qt creator version installed yesterday via qt maintenence tool reason run need remove start qt installed home greg see installed anaconda folder sure happened still newb still learning complete full uninstall removal qt creator files without deleting qt files kde dependencies etc fresh install config settings,"['full', 'amp', 'complete', 'uninstall', 'qt', 'creatorquestion', 'uninstall', 'qt', 'creator', 'linux', 'machine', 'corrupted', 'install', 'debian', 'jessie', 'machine', 'tried', 'reinstalling', 'fixing', 'etc', 'luck', 'advised', 'uninstall', 'completely', 'grab', 'newest', 'version', 'reinstall', 'problem', 'sure', 'every', 'time', 'try', 'qt', 'keeps', 'coming', 'back', 'exact', 'issues', 'assuming', 'config', 'files', 'getting', 'deleted', 'removal', 'process', 'running', 'qmake', 'version', 'gives', 'output', 'running', 'pkg', 'config', 'modversion', 'qtcore', 'gives', 'output', 'problem', 'got', 'several', 'versions', 'installed', 'different', 'folders', 'want', 'completely', 'start', 'fresh', 'install', 'qt', 'creator', 'version', 'installed', 'yesterday', 'via', 'qt', 'maintenence', 'tool', 'reason', 'run', 'need', 'remove', 'start', 'qt', 'installed', 'home', 'greg', 'see', 'installed', 'anaconda', 'folder', 'sure', 'happened', 'still', 'newb', 'still', 'learning', 'complete', 'full', 'uninstall', 'removal', 'qt', 'creator', 'files', 'without', 'deleting', 'qt', 'files', 'kde', 'dependencies', 'etc', 'fresh', 'install', 'config', 'settings']","['full', 'amp', 'complet', 'uninstal', 'qt', 'creatorquest', 'uninstal', 'qt', 'creator', 'linux', 'machin', 'corrupt', 'instal', 'debian', 'jessi', 'machin', 'tri', 'reinstal', 'fix', 'etc', 'luck', 'advis', 'uninstal', 'complet', 'grab', 'newest', 'version', 'reinstal', 'problem', 'sure', 'everi', 'time', 'tri', 'qt', 'keep', 'come', 'back', 'exact', 'issu', 'assum', 'config', 'file', 'get', 'delet', 'remov', 'process', 'run', 'qmake', 'version', 'give', 'output', 'run', 'pkg', 'config', 'modvers', 'qtcore', 'give', 'output', 'problem', 'got', 'sever', 'version', 'instal', 'differ', 'folder', 'want', 'complet', 'start', 'fresh', 'instal', 'qt', 'creator', 'version', 'instal', 'yesterday', 'via', 'qt', 'mainten', 'tool', 'reason', 'run', 'need', 'remov', 'start', 'qt', 'instal', 'home', 'greg', 'see', 'instal', 'anaconda', 'folder', 'sure', 'happen', 'still', 'newb', 'still', 'learn', 'complet', 'full', 'uninstal', 'remov', 'qt', 'creator', 'file', 'without', 'delet', 'qt', 'file', 'kde', 'depend', 'etc', 'fresh', 'instal', 'config', 'set']"
76,82,82,19512158,72915205,Flask Error: werkzeug.routing.BuildError: Could not build url for endpoint &#39;prediction&#39;. Did you mean &#39;flower_prediction&#39; instead?,"<p>I'm deploying a machine learning model with python through flask, however I'm getting an error when I input the information for the model to process.</p>
<p><a href=""https://i.stack.imgur.com/t5OIf.png"" rel=""nofollow noreferrer"">Picture of inputting values on website</a></p>
<p>However, after I input the values and press the button to analyze it, it shows me an error. In VS Code where I made the flask back-end programming, it says this:</p>
<pre><code>File &quot;C:\Users\danis\anaconda3\Lib\site-packages\werkzeug\routing.py&quot;, line 2314, in build
raise BuildError(endpoint, values, method, self)
werkzeug.routing.BuildError: Could not build url for endpoint 'prediction'. Did you mean 'flower_prediction' instead?
</code></pre>
<p>I also turned on an option for a debugger named werkzeug.routing.BuildError to pop up in my browser and it said:</p>
<pre><code>werkzeug.routing.BuildError: Could not build url for endpoint 'prediction'. Did you mean 'flower_prediction' instead?
File &quot;C:\Users\danis\Documents\Udemy AI Course Files\TF_2_Notebooks_and_Data\09-Deployment\test.py&quot;, line 44, in index
return redirect(url_for(&quot;prediction&quot;))
</code></pre>
<p>flower_prediction is my function that calls the machine learning model to process the data, so I'm unsure what to do next.</p>
<p>I'm a beginner to flask, so here is the code for the back-end program:</p>
<pre><code>from flask import Flask, render_template,session,url_for,redirect, session
import numpy as np 
from flask_wtf import FlaskForm
from wtforms import TextField,SubmitField
from wtforms.validators import NumberRange
from tensorflow.keras.models import load_model
import joblib 

def return_prediction(model,scaler,sample_json):

    s_len = sample_json[&quot;sepal_length&quot;]
    s_wid = sample_json[&quot;sepal_width&quot;]
    p_len = sample_json[&quot;petal_length&quot;]
    p_wid = sample_json[&quot;petal_width&quot;]

    flower = [[s_len,s_wid,p_len,p_wid]]
    flower = scaler.transform(flower)
    classes = np.array(['setosa','versicolor','virginica'])
    class_ind = np.argmax(model.predict(flower))
    return classes[class_ind][0]

app = Flask(__name__)
app.config['SECRET_KEY'] = 'mysecretkey'

flower_model = load_model(&quot;final_iris_model.h5&quot;)
flower_scaler = joblib.load(&quot;iris_scaler.pkl&quot;)

class FlowerForm(FlaskForm):
    sep_len = TextField(&quot;Sepal Length&quot;)
    sep_wid = TextField(&quot;Sepal Width&quot;)
    pet_len = TextField(&quot;Petal Length&quot;)
    pet_wid = TextField(&quot;Petal Width&quot;)
    submit = SubmitField(&quot;Analyze&quot;)

@app.route(&quot;/&quot;,methods=['GET','POST'])
def index():
    form = FlowerForm()
    if form.validate_on_submit():
        session['sep_len'] = form.sep_len.data
        session['sep_wid'] = form.sep_wid.data
        session['pet_len'] = form.pet_len.data
        session['pet_wid'] = form.pet_wid.data
        
        return redirect(url_for(&quot;prediction&quot;))
    return render_template('home.html',form=form)


@app.route('/prediction',methods=['POST'])
def flower_prediction():
    content = {}
    content['sepal_length'] = float(session['sep_len'])
    content['sepal_width'] = float(session['sep_wid'])
    content['petal_length'] = float(session['pet_len'])
    content['petal_width'] = float(session['pet_wid'])
    
    results = return_prediction(model=flower_model,scaler=flower_scaler,sample_json=content)
    return render_template('prediction.html',results=results)


if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>Why is it not building the URL endpoint for 'prediction'?</p>
",40,0,-1,2,python;flask,2022-07-08 23:22:03,2022-07-08 23:22:03,2022-07-09 02:03:29,i m deploying a machine learning model with python through flask  however i m getting an error when i input the information for the model to process   however  after i input the values and press the button to analyze it  it shows me an error  in vs code where i made the flask back end programming  it says this  i also turned on an option for a debugger named werkzeug routing builderror to pop up in my browser and it said  flower_prediction is my function that calls the machine learning model to process the data  so i m unsure what to do next  i m a beginner to flask  so here is the code for the back end program  why is it not building the url endpoint for  prediction  ,flask error  werkzeug routing builderror  could not build url for endpoint    prediction     did you mean    flower_prediction    instead ,deploying machine learning model python flask however getting error input information model process however input values press button analyze shows error vs code made flask back end programming says also turned option debugger named werkzeug routing builderror pop browser said flower_prediction function calls machine learning model process data unsure next beginner flask code back end program building url endpoint prediction,flask error werkzeug routing builderror could build url endpoint prediction mean flower_prediction instead,flask error werkzeug routing builderror could build url endpoint prediction mean flower_prediction insteaddeploying machine learning model python flask however getting error input information model process however input values press button analyze shows error vs code made flask back end programming says also turned option debugger named werkzeug routing builderror pop browser said flower_prediction function calls machine learning model process data unsure next beginner flask code back end program building url endpoint prediction,"['flask', 'error', 'werkzeug', 'routing', 'builderror', 'could', 'build', 'url', 'endpoint', 'prediction', 'mean', 'flower_prediction', 'insteaddeploying', 'machine', 'learning', 'model', 'python', 'flask', 'however', 'getting', 'error', 'input', 'information', 'model', 'process', 'however', 'input', 'values', 'press', 'button', 'analyze', 'shows', 'error', 'vs', 'code', 'made', 'flask', 'back', 'end', 'programming', 'says', 'also', 'turned', 'option', 'debugger', 'named', 'werkzeug', 'routing', 'builderror', 'pop', 'browser', 'said', 'flower_prediction', 'function', 'calls', 'machine', 'learning', 'model', 'process', 'data', 'unsure', 'next', 'beginner', 'flask', 'code', 'back', 'end', 'program', 'building', 'url', 'endpoint', 'prediction']","['flask', 'error', 'werkzeug', 'rout', 'builderror', 'could', 'build', 'url', 'endpoint', 'predict', 'mean', 'flower_predict', 'insteaddeploy', 'machin', 'learn', 'model', 'python', 'flask', 'howev', 'get', 'error', 'input', 'inform', 'model', 'process', 'howev', 'input', 'valu', 'press', 'button', 'analyz', 'show', 'error', 'vs', 'code', 'made', 'flask', 'back', 'end', 'program', 'say', 'also', 'turn', 'option', 'debugg', 'name', 'werkzeug', 'rout', 'builderror', 'pop', 'browser', 'said', 'flower_predict', 'function', 'call', 'machin', 'learn', 'model', 'process', 'data', 'unsur', 'next', 'beginn', 'flask', 'code', 'back', 'end', 'program', 'build', 'url', 'endpoint', 'predict']"
77,83,83,19510948,72912509,Machine Learning model to be deployed in Server less Azure cloud,"<p>Computer Vision Model I want to deploy into Azure Server less cloud. <code>__init__.py</code> is not responding to my main program.how to write serverless function</p>
",21,0,-1,5,python;azure;deployment;computer-vision;serverless,2022-07-08 19:26:52,2022-07-08 19:26:52,2022-07-08 21:41:47,computer vision model i want to deploy into azure server less cloud  __init__ py is not responding to my main program how to write serverless function,machine learning model to be deployed in server less azure cloud,computer vision model want deploy azure server less cloud __init__ py responding main program write serverless function,machine learning model deployed server less azure cloud,machine learning model deployed server less azure cloudcomputer vision model want deploy azure server less cloud __init__ py responding main program write serverless function,"['machine', 'learning', 'model', 'deployed', 'server', 'less', 'azure', 'cloudcomputer', 'vision', 'model', 'want', 'deploy', 'azure', 'server', 'less', 'cloud', '__init__', 'py', 'responding', 'main', 'program', 'write', 'serverless', 'function']","['machin', 'learn', 'model', 'deploy', 'server', 'less', 'azur', 'cloudcomput', 'vision', 'model', 'want', 'deploy', 'azur', 'server', 'less', 'cloud', '__init__', 'py', 'respond', 'main', 'program', 'write', 'serverless', 'function']"
78,84,84,18067785,72192511,ModuleNotFoundError: No module named &#39;pyarrow.lib&#39;,"<p>This is the full error message.</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\machine learning project.py&quot;, line 3, in &lt;module&gt;
    import streamlit as st
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\__init__.py&quot;, line 70, in &lt;module&gt;
    from streamlit.delta_generator import DeltaGenerator as _DeltaGenerator
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\delta_generator.py&quot;, line 19, in &lt;module&gt;
    from streamlit import cursor, caching
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\cursor.py&quot;, line 18, in &lt;module&gt;
    from streamlit.scriptrunner import get_script_run_ctx
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\scriptrunner\__init__.py&quot;, line 16, in &lt;module&gt;
    from .script_runner import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\scriptrunner\script_runner.py&quot;, line 35, in &lt;module&gt;
    from streamlit.state import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\state\__init__.py&quot;, line 27, in &lt;module&gt;
    from .session_state_proxy import (
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\state\session_state_proxy.py&quot;, line 24, in &lt;module&gt;
    from streamlit.type_util import Key
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\streamlit\type_util.py&quot;, line 22, in &lt;module&gt;
    import pyarrow as pa
  File &quot;C:\Users\adi\OneDrive\Desktop\Python310\lib\site-packages\pyarrow\__init__.py&quot;, line 65, in &lt;module&gt;
    import pyarrow.lib as _lib
</code></pre>
<p>I am working with streamlit for a project but can't work out this problem.
I have tried uninstalling and reinstalling streamlit but that did'nt help.</p>
<p>i using python 3.8</p>
",245,2,1,4,python;pyarrow;streamlit;facebook-prophet,2022-05-11 01:50:03,2022-05-11 01:50:03,2022-07-08 21:29:31,this is the full error message  i using python  ,modulenotfounderror  no module named    pyarrow lib   ,full error message using python,modulenotfounderror module named pyarrow lib,modulenotfounderror module named pyarrow libfull error message using python,"['modulenotfounderror', 'module', 'named', 'pyarrow', 'libfull', 'error', 'message', 'using', 'python']","['modulenotfounderror', 'modul', 'name', 'pyarrow', 'libful', 'error', 'messag', 'use', 'python']"
79,85,85,17259565,72913971,Does my histogram display number of pixels?,"<p>I am doing some machine learning training on images and wanted to display the MSE losses between the train and test images in a histogram. Each image size is (512, 768, 3).</p>
<p>In the code below, pred and test_set are both arrays of the image pixel values and of length 512 each (i.e. 512 images)</p>
<pre><code>rain_loss = tf.keras.losses.mean_squared_error(pred[1], test_set[1])
plt.hist(train_loss, bins=50, alpha=0.5)
plt.xlabel(&quot;Train loss&quot;)
plt.ylabel(&quot;No of pixels&quot;)

plt.show()
</code></pre>
<p>This is the graph formed for image 1:</p>
<p><a href=""https://i.stack.imgur.com/K7NZZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K7NZZ.png"" alt=""1"" /></a></p>
<p>Now, I do not understand what this graph is actually representing. I initially thought it would be the number of pixels but the numbers don't add up, unless I am missing something?</p>
",26,0,-1,4,machine-learning;image-processing;deep-learning;neural-network,2022-07-08 21:21:52,2022-07-08 21:21:52,2022-07-08 21:26:38,i am doing some machine learning training on images and wanted to display the mse losses between the train and test images in a histogram  each image size is         in the code below  pred and test_set are both arrays of the image pixel values and of length  each  i e   images  this is the graph formed for image    now  i do not understand what this graph is actually representing  i initially thought it would be the number of pixels but the numbers don t add up  unless i am missing something ,does my histogram display number of pixels ,machine learning training images wanted display mse losses train test images histogram image size code pred test_set arrays image pixel values length e images graph formed image understand graph actually representing initially thought would number pixels numbers unless missing something,histogram display number pixels,histogram display number pixelsmachine learning training images wanted display mse losses train test images histogram image size code pred test_set arrays image pixel values length e images graph formed image understand graph actually representing initially thought would number pixels numbers unless missing something,"['histogram', 'display', 'number', 'pixelsmachine', 'learning', 'training', 'images', 'wanted', 'display', 'mse', 'losses', 'train', 'test', 'images', 'histogram', 'image', 'size', 'code', 'pred', 'test_set', 'arrays', 'image', 'pixel', 'values', 'length', 'e', 'images', 'graph', 'formed', 'image', 'understand', 'graph', 'actually', 'representing', 'initially', 'thought', 'would', 'number', 'pixels', 'numbers', 'unless', 'missing', 'something']","['histogram', 'display', 'number', 'pixelsmachin', 'learn', 'train', 'imag', 'want', 'display', 'mse', 'loss', 'train', 'test', 'imag', 'histogram', 'imag', 'size', 'code', 'pred', 'test_set', 'array', 'imag', 'pixel', 'valu', 'length', 'e', 'imag', 'graph', 'form', 'imag', 'understand', 'graph', 'actual', 'repres', 'initi', 'thought', 'would', 'number', 'pixel', 'number', 'unless', 'miss', 'someth']"
80,86,86,18298324,72831330,Incorrect Number of Dimensions on Train and Test Dataset,"<p>I am running a machine learning algorithms in a loop. Everything is working perfectly but at where I have to split and train the data, I get error saying incorrect number of dimensions.</p>
<ol>
<li>I have a data from UCI; heart disease</li>
<li>Now I am applying MCAR mechanism and imputations methods</li>
<li>Machine learning algorithms</li>
</ol>
<p>The data can be downloaded using this link (<a href=""https://www.kaggle.com//code//mchtklb//kalebasi-heart-disease-ml-modeling//data?select=heart.csv"" rel=""nofollow noreferrer"">https://www.kaggle.com//code//mchtklb//kalebasi-heart-disease-ml-modeling//data?select=heart.csv</a>)</p>
<p>This is the code from where the loop starts
p  &lt;- c(0.01,0.02) #generating prob of missing data
ip = 2 #number of imputations</p>
<pre><code>#MICE BINS
hd_mcar  &lt;- vector(mode = &quot;list&quot;, length = length(p))
mice_mcar &lt;- vector(mode = &quot;list&quot;, length = length(p))
app1 &lt;- vector(mode = &quot;list&quot;, length = length(p))
min1 &lt;- vector(mode = &quot;list&quot;, length = length(p))
sm1 &lt;- vector(mode = &quot;list&quot;, length = length(p))
final_clean_hd_mcar &lt;- vector(mode = &quot;list&quot;, length = length(p))
mice.mcar &lt;- vector(mode = &quot;list&quot;, length = length(p))


for(i in 1:length(p)){
  hd_mcar[[i]] &lt;- delete_MCAR(heart.ds, p[i])

                    
              #Applying Imputation
    
    
  ###1. Method(MICE)
  mice_mcar[[i]] &lt;- mice(hd_mcar[[i]], m=ip, method = c(&quot;pmm&quot;,&quot;logreg&quot;,&quot;polyreg&quot;,&quot;pmm&quot;,&quot;pmm&quot;,&quot;logreg&quot;,
                                            &quot;polyreg&quot;,&quot;pmm&quot;,&quot;logreg&quot;,&quot;pmm&quot;,&quot;polyreg&quot;,&quot;pmm&quot;,
                                            &quot;polyreg&quot;,&quot;logreg&quot;), maxit = 20)

  #Diagnostic check
  summary(heart.ds$age)
  mice_mcar[[i]]$imp$age


  #Finding the means of the impuatations
  app1[[i]] &lt;- apply(mice_mcar[[i]]$imp$age, MARGIN = 2, FUN = mean)
  min1[[i]] &lt;- abs(app1[[i]]-mean(heart.ds$age))
  
  #Selecting the minimum index
  sm1[[i]] &lt;- which(min1[[i]]==min(min1[[i]]))
  
  #Selecting final imputation
  final_clean_hd_mcar[[i]] =mice::complete(mice_mcar[[i]],sm1[[i]])
  mice.mcar[[i]] &lt;- final_clean_hd_mcar[[i]]


#MLA BINS
control &lt;- vector(mode = &quot;list&quot;, length = length(p))
results &lt;- vector(mode = &quot;list&quot;, length = length(p))
vimp &lt;- vector(mode = &quot;list&quot;, length = length(p))

#Feature Selection Using Recursive Feature Elimination
control[[i]] &lt;- rfeControl(functions=rfFuncs, method=&quot;cv&quot;, number=10)
# run the RFE algorithm
results[[i]] &lt;- rfe(mice.mcar[[i]][,1:13], mice.mcar[[i]][,14], sizes=c(1:13), rfeControl=control[[i]], metric = &quot;Accuracy&quot;)

# list the chosen features
predictors(results[[i]])


#BINS
n.heart.ds &lt;- vector(mode = &quot;list&quot;, length = length(p))
tr.dat &lt;- vector(mode = &quot;list&quot;, length = length(p))
TrainSet &lt;- vector(mode = &quot;list&quot;, length = length(p))
ValidSet &lt;- vector(mode = &quot;list&quot;, length = length(p))
fitControl &lt;- vector(mode = &quot;list&quot;, length = length(p))

#Re-selection from the Feature Selection
n.heart.ds[[i]] &lt;- mice.mcar[[i]][,-c(2,6,7,8,9,11)]

#Data Splitting
set.seed(1237)
tr.dat[[i]] &lt;- sample(nrow(n.heart.ds[[i]]), .7*nrow(n.heart.ds[[i]]), replace = FALSE)

TrainSet[[i]] &lt;- n.heart.ds[tr.dat[[i]],]
ValidSet[[i]] &lt;- n.heart.ds[-tr.dat[[i]],]
}
</code></pre>
<p>Now at the TrainSet, I get an error:</p>
<pre><code>Error in n.heart.ds[tr.dat[[i]], ] : incorrect number of dimensions
In addition: There were 50 or more warnings (use warnings() to see the first 50)
</code></pre>
<p>Please how do i solve the error on the Training and Validation dataset? I mean the dimension errors is printing</p>
",47,0,0,2,r;machine-learning,2022-07-01 20:44:18,2022-07-01 20:44:18,2022-07-08 16:22:37,i am running a machine learning algorithms in a loop  everything is working perfectly but at where i have to split and train the data  i get error saying incorrect number of dimensions  the data can be downloaded using this link    now at the trainset  i get an error  please how do i solve the error on the training and validation dataset  i mean the dimension errors is printing,incorrect number of dimensions on train and test dataset,running machine learning algorithms loop everything working perfectly split train data get error saying incorrect number dimensions data downloaded using link trainset get error please solve error training validation dataset mean dimension errors printing,incorrect number dimensions train test dataset,incorrect number dimensions train test datasetrunning machine learning algorithms loop everything working perfectly split train data get error saying incorrect number dimensions data downloaded using link trainset get error please solve error training validation dataset mean dimension errors printing,"['incorrect', 'number', 'dimensions', 'train', 'test', 'datasetrunning', 'machine', 'learning', 'algorithms', 'loop', 'everything', 'working', 'perfectly', 'split', 'train', 'data', 'get', 'error', 'saying', 'incorrect', 'number', 'dimensions', 'data', 'downloaded', 'using', 'link', 'trainset', 'get', 'error', 'please', 'solve', 'error', 'training', 'validation', 'dataset', 'mean', 'dimension', 'errors', 'printing']","['incorrect', 'number', 'dimens', 'train', 'test', 'datasetrun', 'machin', 'learn', 'algorithm', 'loop', 'everyth', 'work', 'perfectli', 'split', 'train', 'data', 'get', 'error', 'say', 'incorrect', 'number', 'dimens', 'data', 'download', 'use', 'link', 'trainset', 'get', 'error', 'pleas', 'solv', 'error', 'train', 'valid', 'dataset', 'mean', 'dimens', 'error', 'print']"
81,87,87,18432393,72909820,Should I use object detection or segmentation? Detection by position rather than looks?,"<p>I am working on a project that needs to decide which olive tree branches should be cut. The goal is to detect the specific type of branch (watersprouts).</p>
<p>The problem I'm facing is that I'm unsure if I should use object detection (image classification + localization) or image segmentation. The difference between branches is mostly in their position with watersprouts growing mostly vertical to the main branch(there is a very small difference in looks between watersprouts and other branches) while other branches can grow in all ways (mostly parallel to the main branch).</p>
<p>My plan was to use object detection so I can classify watersprouts and localize them in the picture. I think that segmentation is a bit overkill for this problem because I don't see the need for localizing every pixel. The plan was to take pictures of watersprouts as class 1 and other branches as class 2, train them so I can detect them and localize. When I localize them I can now see which of these branches is a watersprout branch and which is a regular branch and then I know that watersprout should be cut.</p>
<p>The other problem I have is with understanding if it is possible for my machine learning project to recognize watersprouts not by their looks but by their position in regards to the main branch and correctly differentiate them from other branches because this is the main difference between watersprouts branch and regular branch. My understending is that the network learns how the object looks like and that position doesn't matter.</p>
<p>Am I on a right track or am I missing something?</p>
",23,0,-1,4,machine-learning;computer-vision;classification;image-segmentation,2022-07-08 15:37:12,2022-07-08 15:37:12,2022-07-08 16:03:25,i am working on a project that needs to decide which olive tree branches should be cut  the goal is to detect the specific type of branch  watersprouts   the problem i m facing is that i m unsure if i should use object detection  image classification   localization  or image segmentation  the difference between branches is mostly in their position with watersprouts growing mostly vertical to the main branch there is a very small difference in looks between watersprouts and other branches  while other branches can grow in all ways  mostly parallel to the main branch   my plan was to use object detection so i can classify watersprouts and localize them in the picture  i think that segmentation is a bit overkill for this problem because i don t see the need for localizing every pixel  the plan was to take pictures of watersprouts as class  and other branches as class   train them so i can detect them and localize  when i localize them i can now see which of these branches is a watersprout branch and which is a regular branch and then i know that watersprout should be cut  the other problem i have is with understanding if it is possible for my machine learning project to recognize watersprouts not by their looks but by their position in regards to the main branch and correctly differentiate them from other branches because this is the main difference between watersprouts branch and regular branch  my understending is that the network learns how the object looks like and that position doesn t matter  am i on a right track or am i missing something ,should i use object detection or segmentation  detection by position rather than looks ,working project needs decide olive tree branches cut goal detect specific type branch watersprouts problem facing unsure use object detection image classification localization image segmentation difference branches mostly position watersprouts growing mostly vertical main branch small difference looks watersprouts branches branches grow ways mostly parallel main branch plan use object detection classify watersprouts localize picture think segmentation bit overkill problem see need localizing every pixel plan take pictures watersprouts class branches class train detect localize localize see branches watersprout branch regular branch know watersprout cut problem understanding possible machine learning project recognize watersprouts looks position regards main branch correctly differentiate branches main difference watersprouts branch regular branch understending network learns object looks like position matter right track missing something,use object detection segmentation detection position rather looks,use object detection segmentation detection position rather looksworking project needs decide olive tree branches cut goal detect specific type branch watersprouts problem facing unsure use object detection image classification localization image segmentation difference branches mostly position watersprouts growing mostly vertical main branch small difference looks watersprouts branches branches grow ways mostly parallel main branch plan use object detection classify watersprouts localize picture think segmentation bit overkill problem see need localizing every pixel plan take pictures watersprouts class branches class train detect localize localize see branches watersprout branch regular branch know watersprout cut problem understanding possible machine learning project recognize watersprouts looks position regards main branch correctly differentiate branches main difference watersprouts branch regular branch understending network learns object looks like position matter right track missing something,"['use', 'object', 'detection', 'segmentation', 'detection', 'position', 'rather', 'looksworking', 'project', 'needs', 'decide', 'olive', 'tree', 'branches', 'cut', 'goal', 'detect', 'specific', 'type', 'branch', 'watersprouts', 'problem', 'facing', 'unsure', 'use', 'object', 'detection', 'image', 'classification', 'localization', 'image', 'segmentation', 'difference', 'branches', 'mostly', 'position', 'watersprouts', 'growing', 'mostly', 'vertical', 'main', 'branch', 'small', 'difference', 'looks', 'watersprouts', 'branches', 'branches', 'grow', 'ways', 'mostly', 'parallel', 'main', 'branch', 'plan', 'use', 'object', 'detection', 'classify', 'watersprouts', 'localize', 'picture', 'think', 'segmentation', 'bit', 'overkill', 'problem', 'see', 'need', 'localizing', 'every', 'pixel', 'plan', 'take', 'pictures', 'watersprouts', 'class', 'branches', 'class', 'train', 'detect', 'localize', 'localize', 'see', 'branches', 'watersprout', 'branch', 'regular', 'branch', 'know', 'watersprout', 'cut', 'problem', 'understanding', 'possible', 'machine', 'learning', 'project', 'recognize', 'watersprouts', 'looks', 'position', 'regards', 'main', 'branch', 'correctly', 'differentiate', 'branches', 'main', 'difference', 'watersprouts', 'branch', 'regular', 'branch', 'understending', 'network', 'learns', 'object', 'looks', 'like', 'position', 'matter', 'right', 'track', 'missing', 'something']","['use', 'object', 'detect', 'segment', 'detect', 'posit', 'rather', 'lookswork', 'project', 'need', 'decid', 'oliv', 'tree', 'branch', 'cut', 'goal', 'detect', 'specif', 'type', 'branch', 'watersprout', 'problem', 'face', 'unsur', 'use', 'object', 'detect', 'imag', 'classif', 'local', 'imag', 'segment', 'differ', 'branch', 'mostli', 'posit', 'watersprout', 'grow', 'mostli', 'vertic', 'main', 'branch', 'small', 'differ', 'look', 'watersprout', 'branch', 'branch', 'grow', 'way', 'mostli', 'parallel', 'main', 'branch', 'plan', 'use', 'object', 'detect', 'classifi', 'watersprout', 'local', 'pictur', 'think', 'segment', 'bit', 'overkil', 'problem', 'see', 'need', 'local', 'everi', 'pixel', 'plan', 'take', 'pictur', 'watersprout', 'class', 'branch', 'class', 'train', 'detect', 'local', 'local', 'see', 'branch', 'watersprout', 'branch', 'regular', 'branch', 'know', 'watersprout', 'cut', 'problem', 'understand', 'possibl', 'machin', 'learn', 'project', 'recogn', 'watersprout', 'look', 'posit', 'regard', 'main', 'branch', 'correctli', 'differenti', 'branch', 'main', 'differ', 'watersprout', 'branch', 'regular', 'branch', 'understend', 'network', 'learn', 'object', 'look', 'like', 'posit', 'matter', 'right', 'track', 'miss', 'someth']"
82,88,88,14022684,72906803,how can I run python file in remote server using Paramiko?,"<p>I'm trying to execute a python script(machine learning) via remote server which has good resources.
And I'm trying to call remote python code in local environment with Paramiko library. (I send command to remote server, and that remote server will execute that command)</p>
<p>code in local environment is below:</p>
<pre><code>cli = paramiko.SSHClient()
cli.set_missing_host_key_policy(paramiko.AutoAddPolicy)
server = config['ip']
user = 'ubuntu'
pwd = config['pw']
cli.connect(server, username=user, password=pwd)
command=&quot;python my_code.py&quot;
stdin, stdout, stderr = cli.exec_command(command)
</code></pre>
<p>I send a command to remote server via above code. But, that doesn't work.
When the command is basic linux command (ex, ls -al, cd, pwd ...) those work.
But the remote server's cell does not recognize other commands like, python ~.py, conda activate, pip install...</p>
<p>My wonder is that, those commands are available when I directly connect to remote server's shell. see below:</p>
<p><a href=""https://i.stack.imgur.com/V2HSL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V2HSL.png"" alt=""python command works"" /></a></p>
<p><a href=""https://i.stack.imgur.com/aaY1F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aaY1F.png"" alt=""pip command works"" /></a></p>
<p>I guess the bash that is opened when I directly connect is different from the bash that Paramiko handles. How can I handles this problem?
thanks sincerely</p>
",38,1,0,3,python;remote-access;paramiko,2022-07-08 10:26:11,2022-07-08 10:26:11,2022-07-08 12:49:14,code in local environment is below  my wonder is that  those commands are available when i directly connect to remote server s shell  see below   ,how can i run python file in remote server using paramiko ,code local environment wonder commands available directly connect remote server shell see,run python file remote server using paramiko,run python file remote server using paramikocode local environment wonder commands available directly connect remote server shell see,"['run', 'python', 'file', 'remote', 'server', 'using', 'paramikocode', 'local', 'environment', 'wonder', 'commands', 'available', 'directly', 'connect', 'remote', 'server', 'shell', 'see']","['run', 'python', 'file', 'remot', 'server', 'use', 'paramikocod', 'local', 'environ', 'wonder', 'command', 'avail', 'directli', 'connect', 'remot', 'server', 'shell', 'see']"
83,90,90,3317122,64418032,How do I reshape this Pandas dataframe?,"<p>I have the first dataframe in Pandas, which I'm trying to reshape to the second dataframe for supervised machine learning purposes. <code>[foo,bar]</code> represents a datapoint; each <code>id</code> has a definite label <code>[dog,cat]</code> and multiple datapoints. The final dataframe includes up to 3 datapoints in the order that they're initially given, using either truncation or zero-padding to achieve this goal.</p>
<pre class=""lang-py prettyprint-override""><code>   foo  bar  dog  cat   id
0  1.1  1.6    0    1   12
1  2.3  2.4    0    1   12
2  4.5  4.2    0    1   12
3  2.3  1.2    0    1   12
4  4.2  3.8    1    0  535
5  1.6  4.1    1    0  535
...
</code></pre>
<pre class=""lang-py prettyprint-override""><code> id  foo1  bar1  foo2  bar2  foo3  bar3  dog  cat
 12   1.1   1.6   2.3   2.4   4.5   4.2    0    1
535   4.2   3.8   1.6   4.1     0     0    1    0
...
</code></pre>
<p>I've tried calling <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>pd.pivot()</code></a>, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>pd.stack()</code></a>, and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>pd.unstack()</code></a>, but I haven't gotten anywhere. I also haven't been able to find what I'm trying to do on the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html"" rel=""nofollow noreferrer"">Pandas reshaping docs</a>.</p>
",53,1,-2,3,python;pandas;reshape,2020-10-19 02:00:49,2020-10-19 02:00:49,2022-07-08 10:37:07,i have the first dataframe in pandas  which i m trying to reshape to the second dataframe for supervised machine learning purposes   foo bar  represents a datapoint  each id has a definite label  dog cat  and multiple datapoints  the final dataframe includes up to  datapoints in the order that they re initially given  using either truncation or zero padding to achieve this goal  i ve tried calling     and   but i haven t gotten anywhere  i also haven t been able to find what i m trying to do on the  ,how do i reshape this pandas dataframe ,first dataframe pandas trying reshape second dataframe supervised machine learning purposes foo bar represents datapoint id definite label dog cat multiple datapoints final dataframe includes datapoints order initially given using either truncation zero padding achieve goal tried calling gotten anywhere also able find trying,reshape pandas dataframe,reshape pandas dataframefirst dataframe pandas trying reshape second dataframe supervised machine learning purposes foo bar represents datapoint id definite label dog cat multiple datapoints final dataframe includes datapoints order initially given using either truncation zero padding achieve goal tried calling gotten anywhere also able find trying,"['reshape', 'pandas', 'dataframefirst', 'dataframe', 'pandas', 'trying', 'reshape', 'second', 'dataframe', 'supervised', 'machine', 'learning', 'purposes', 'foo', 'bar', 'represents', 'datapoint', 'id', 'definite', 'label', 'dog', 'cat', 'multiple', 'datapoints', 'final', 'dataframe', 'includes', 'datapoints', 'order', 'initially', 'given', 'using', 'either', 'truncation', 'zero', 'padding', 'achieve', 'goal', 'tried', 'calling', 'gotten', 'anywhere', 'also', 'able', 'find', 'trying']","['reshap', 'panda', 'dataframefirst', 'datafram', 'panda', 'tri', 'reshap', 'second', 'datafram', 'supervis', 'machin', 'learn', 'purpos', 'foo', 'bar', 'repres', 'datapoint', 'id', 'definit', 'label', 'dog', 'cat', 'multipl', 'datapoint', 'final', 'datafram', 'includ', 'datapoint', 'order', 'initi', 'given', 'use', 'either', 'truncat', 'zero', 'pad', 'achiev', 'goal', 'tri', 'call', 'gotten', 'anywher', 'also', 'abl', 'find', 'tri']"
84,91,91,19507573,72906861,grid of mtry values while training random forests with ranger,"<p>I am working with a subset of the 'Ames Housing' dataset and have originally 17 features. Using the 'recipes' package, I have engineered the original set of features and created dummy variables for nominal predictors with the following code. That has resulted in 35 features in the 'baked_train' dataset below.</p>
<pre><code>blueprint &lt;- recipe(Sale_Price ~ ., data = _train) %&gt;%
      step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;%
      step_impute_knn(Gr_Liv_Area) %&gt;%
      step_integer(Overall_Qual) %&gt;%
      step_normalize(all_numeric_predictors()) %&gt;%
      step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;%
      step_dummy(all_nominal_predictors(), one_hot = FALSE)

prepare &lt;- prep(blueprint, data = ames_train)

baked_train &lt;- bake(prepare, new_data = ames_train)

baked_test &lt;- bake(prepare, new_data = ames_test)
</code></pre>
<p>Now, I am trying to train random forests with the 'ranger' package using the following code.</p>
<pre><code>cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5)

param_grid_rf &lt;- expand.grid(mtry = seq(1, 35, 1),
                             splitrule = &quot;variance&quot;,
                             min.node.size = 2)

rf_cv &lt;- train(blueprint,
           data = ames_train,
           method = &quot;ranger&quot;,
           trControl = cv_specs,
           tuneGrid = param_grid_rf,
           metric = &quot;RMSE&quot;)
</code></pre>
<p>I have set the grid of 'mtry' values based on the number of features in the 'baked_train' data. It is my understanding that 'caret' will apply the blueprint within each resample of 'ames_train' creating a baked version at each CV step.</p>
<p>The text Hands-On Machine Learning with R by Boehmke &amp; Greenwell says on section 3.8.3,</p>
<p><strong>Consequently, the goal is to develop our blueprint, then within each resample iteration we want to apply prep() and bake() to our resample training and validation data. Luckily, the caret package simplifies this process. We only need to specify the blueprint and caret will automatically prepare and bake within each resample.</strong></p>
<p>However, when I run the code above I get an error,</p>
<p><em>mtry can not be larger than number of variables in data. Ranger will EXIT now.</em></p>
<p>I get the same error when I specify 'tuneLength = 20' instead of the 'tuneGrid'. Although the code works fine when the grid of 'mtry' values is specified to be from 1 to 17 (the number of features in the original training data 'ames_train').</p>
<p>When I specify a grid of 'mtry' values from 1 to 17, info about the final model after CV is shown below. Notice that it mentions <strong>Number of independent variables: 35</strong> which corresponds to the 'baked_train' data, although specifying a grid from 1 to 35 throws an error.</p>
<pre><code>Type:                             Regression 
Number of trees:                  500 
Sample size:                      618 
Number of independent variables:  35 
Mtry:                             15 
Target node size:                 2 
Variable importance mode:         impurity 
Splitrule:                        variance 
OOB prediction error (MSE):       995351989 
R squared (OOB):                  0.8412147 
</code></pre>
<p>What am I missing here? Specifically, why do I have to specify the number of features in 'ames_train' instead of 'baked_train' when essentially 'caret' is supposed to create a baked version before fitting and evaluating the model for each resample?</p>
<p>Thanks.</p>
",15,0,0,4,random-forest;r-caret;r-recipes;r-ranger,2022-07-08 10:34:12,2022-07-08 10:34:12,2022-07-08 10:34:12,i am working with a subset of the  ames housing  dataset and have originally  features  using the  recipes  package  i have engineered the original set of features and created dummy variables for nominal predictors with the following code  that has resulted in  features in the  baked_train  dataset below  now  i am trying to train random forests with the  ranger  package using the following code  i have set the grid of  mtry  values based on the number of features in the  baked_train  data  it is my understanding that  caret  will apply the blueprint within each resample of  ames_train  creating a baked version at each cv step  the text hands on machine learning with r by boehmke  amp  greenwell says on section     consequently  the goal is to develop our blueprint  then within each resample iteration we want to apply prep   and bake   to our resample training and validation data  luckily  the caret package simplifies this process  we only need to specify the blueprint and caret will automatically prepare and bake within each resample  however  when i run the code above i get an error  mtry can not be larger than number of variables in data  ranger will exit now  i get the same error when i specify  tunelength     instead of the  tunegrid   although the code works fine when the grid of  mtry  values is specified to be from  to   the number of features in the original training data  ames_train    when i specify a grid of  mtry  values from  to   info about the final model after cv is shown below  notice that it mentions number of independent variables   which corresponds to the  baked_train  data  although specifying a grid from  to  throws an error  what am i missing here  specifically  why do i have to specify the number of features in  ames_train  instead of  baked_train  when essentially  caret  is supposed to create a baked version before fitting and evaluating the model for each resample  thanks ,grid of mtry values while training random forests with ranger,working subset ames housing dataset originally features using recipes package engineered original set features created dummy variables nominal predictors following code resulted features baked_train dataset trying train random forests ranger package using following code set grid mtry values based number features baked_train data understanding caret apply blueprint within resample ames_train creating baked version cv step text hands machine learning r boehmke amp greenwell says section consequently goal develop blueprint within resample iteration want apply prep bake resample training validation data luckily caret package simplifies process need specify blueprint caret automatically prepare bake within resample however run code get error mtry larger number variables data ranger exit get error specify tunelength instead tunegrid although code works fine grid mtry values specified number features original training data ames_train specify grid mtry values info final model cv shown notice mentions number independent variables corresponds baked_train data although specifying grid throws error missing specifically specify number features ames_train instead baked_train essentially caret supposed create baked version fitting evaluating model resample thanks,grid mtry values training random forests ranger,grid mtry values training random forests rangerworking subset ames housing dataset originally features using recipes package engineered original set features created dummy variables nominal predictors following code resulted features baked_train dataset trying train random forests ranger package using following code set grid mtry values based number features baked_train data understanding caret apply blueprint within resample ames_train creating baked version cv step text hands machine learning r boehmke amp greenwell says section consequently goal develop blueprint within resample iteration want apply prep bake resample training validation data luckily caret package simplifies process need specify blueprint caret automatically prepare bake within resample however run code get error mtry larger number variables data ranger exit get error specify tunelength instead tunegrid although code works fine grid mtry values specified number features original training data ames_train specify grid mtry values info final model cv shown notice mentions number independent variables corresponds baked_train data although specifying grid throws error missing specifically specify number features ames_train instead baked_train essentially caret supposed create baked version fitting evaluating model resample thanks,"['grid', 'mtry', 'values', 'training', 'random', 'forests', 'rangerworking', 'subset', 'ames', 'housing', 'dataset', 'originally', 'features', 'using', 'recipes', 'package', 'engineered', 'original', 'set', 'features', 'created', 'dummy', 'variables', 'nominal', 'predictors', 'following', 'code', 'resulted', 'features', 'baked_train', 'dataset', 'trying', 'train', 'random', 'forests', 'ranger', 'package', 'using', 'following', 'code', 'set', 'grid', 'mtry', 'values', 'based', 'number', 'features', 'baked_train', 'data', 'understanding', 'caret', 'apply', 'blueprint', 'within', 'resample', 'ames_train', 'creating', 'baked', 'version', 'cv', 'step', 'text', 'hands', 'machine', 'learning', 'r', 'boehmke', 'amp', 'greenwell', 'says', 'section', 'consequently', 'goal', 'develop', 'blueprint', 'within', 'resample', 'iteration', 'want', 'apply', 'prep', 'bake', 'resample', 'training', 'validation', 'data', 'luckily', 'caret', 'package', 'simplifies', 'process', 'need', 'specify', 'blueprint', 'caret', 'automatically', 'prepare', 'bake', 'within', 'resample', 'however', 'run', 'code', 'get', 'error', 'mtry', 'larger', 'number', 'variables', 'data', 'ranger', 'exit', 'get', 'error', 'specify', 'tunelength', 'instead', 'tunegrid', 'although', 'code', 'works', 'fine', 'grid', 'mtry', 'values', 'specified', 'number', 'features', 'original', 'training', 'data', 'ames_train', 'specify', 'grid', 'mtry', 'values', 'info', 'final', 'model', 'cv', 'shown', 'notice', 'mentions', 'number', 'independent', 'variables', 'corresponds', 'baked_train', 'data', 'although', 'specifying', 'grid', 'throws', 'error', 'missing', 'specifically', 'specify', 'number', 'features', 'ames_train', 'instead', 'baked_train', 'essentially', 'caret', 'supposed', 'create', 'baked', 'version', 'fitting', 'evaluating', 'model', 'resample', 'thanks']","['grid', 'mtri', 'valu', 'train', 'random', 'forest', 'rangerwork', 'subset', 'ame', 'hous', 'dataset', 'origin', 'featur', 'use', 'recip', 'packag', 'engin', 'origin', 'set', 'featur', 'creat', 'dummi', 'variabl', 'nomin', 'predictor', 'follow', 'code', 'result', 'featur', 'baked_train', 'dataset', 'tri', 'train', 'random', 'forest', 'ranger', 'packag', 'use', 'follow', 'code', 'set', 'grid', 'mtri', 'valu', 'base', 'number', 'featur', 'baked_train', 'data', 'understand', 'caret', 'appli', 'blueprint', 'within', 'resampl', 'ames_train', 'creat', 'bake', 'version', 'cv', 'step', 'text', 'hand', 'machin', 'learn', 'r', 'boehmk', 'amp', 'greenwel', 'say', 'section', 'consequ', 'goal', 'develop', 'blueprint', 'within', 'resampl', 'iter', 'want', 'appli', 'prep', 'bake', 'resampl', 'train', 'valid', 'data', 'luckili', 'caret', 'packag', 'simplifi', 'process', 'need', 'specifi', 'blueprint', 'caret', 'automat', 'prepar', 'bake', 'within', 'resampl', 'howev', 'run', 'code', 'get', 'error', 'mtri', 'larger', 'number', 'variabl', 'data', 'ranger', 'exit', 'get', 'error', 'specifi', 'tunelength', 'instead', 'tunegrid', 'although', 'code', 'work', 'fine', 'grid', 'mtri', 'valu', 'specifi', 'number', 'featur', 'origin', 'train', 'data', 'ames_train', 'specifi', 'grid', 'mtri', 'valu', 'info', 'final', 'model', 'cv', 'shown', 'notic', 'mention', 'number', 'independ', 'variabl', 'correspond', 'baked_train', 'data', 'although', 'specifi', 'grid', 'throw', 'error', 'miss', 'specif', 'specifi', 'number', 'featur', 'ames_train', 'instead', 'baked_train', 'essenti', 'caret', 'suppos', 'creat', 'bake', 'version', 'fit', 'evalu', 'model', 'resampl', 'thank']"
85,92,92,17271390,70802006,Clean Up Azure Machine Learning Blob Storage,"<p>I manage a frequently used Azure Machine Learning workspace. With several Experiments and active pipelines. Everything is working good so far. My problem is to get rid of old data from runs, experiments and pipelines. Over the last year the blob storage grew to enourmus size, because every pipeline data is stored.</p>
<p>I have deleted older runs from experimnents by using the gui, but the actual pipeline data on the blob store is not deleted. Is there a smart way to clean up data on the blob store from runs which have been deleted ?</p>
<p>On one of the countless Microsoft support pages, I found the following not very helpfull post:</p>
<p>*Azure does not automatically delete intermediate data written with OutputFileDatasetConfig. To avoid storage charges for large amounts of unneeded data, you should either:</p>
<ol>
<li>Programmatically delete intermediate data at the end of a pipeline
run, when it is no longer needed</li>
<li>Use blob storage with a short-term storage policy for intermediate data (see Optimize costs by automating Azure Blob Storage access tiers)</li>
<li>Regularly review and delete no-longer-needed data*</li>
</ol>
<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed</a></p>
<p>Any idea is welcome.</p>
",253,1,4,2,azure-blob-storage;azure-machine-learning-service,2022-01-21 18:53:46,2022-01-21 18:53:46,2022-07-08 09:37:43,i manage a frequently used azure machine learning workspace  with several experiments and active pipelines  everything is working good so far  my problem is to get rid of old data from runs  experiments and pipelines  over the last year the blob storage grew to enourmus size  because every pipeline data is stored  i have deleted older runs from experimnents by using the gui  but the actual pipeline data on the blob store is not deleted  is there a smart way to clean up data on the blob store from runs which have been deleted   on one of the countless microsoft support pages  i found the following not very helpfull post   azure does not automatically delete intermediate data written with outputfiledatasetconfig  to avoid storage charges for large amounts of unneeded data  you should either   any idea is welcome ,clean up azure machine learning blob storage,manage frequently used azure machine learning workspace several experiments active pipelines everything working good far problem get rid old data runs experiments pipelines last year blob storage grew enourmus size every pipeline data stored deleted older runs experimnents using gui actual pipeline data blob store deleted smart way clean data blob store runs deleted one countless microsoft support pages found following helpfull post azure automatically delete intermediate data written outputfiledatasetconfig avoid storage charges large amounts unneeded data either idea welcome,clean azure machine learning blob storage,clean azure machine learning blob storagemanage frequently used azure machine learning workspace several experiments active pipelines everything working good far problem get rid old data runs experiments pipelines last year blob storage grew enourmus size every pipeline data stored deleted older runs experimnents using gui actual pipeline data blob store deleted smart way clean data blob store runs deleted one countless microsoft support pages found following helpfull post azure automatically delete intermediate data written outputfiledatasetconfig avoid storage charges large amounts unneeded data either idea welcome,"['clean', 'azure', 'machine', 'learning', 'blob', 'storagemanage', 'frequently', 'used', 'azure', 'machine', 'learning', 'workspace', 'several', 'experiments', 'active', 'pipelines', 'everything', 'working', 'good', 'far', 'problem', 'get', 'rid', 'old', 'data', 'runs', 'experiments', 'pipelines', 'last', 'year', 'blob', 'storage', 'grew', 'enourmus', 'size', 'every', 'pipeline', 'data', 'stored', 'deleted', 'older', 'runs', 'experimnents', 'using', 'gui', 'actual', 'pipeline', 'data', 'blob', 'store', 'deleted', 'smart', 'way', 'clean', 'data', 'blob', 'store', 'runs', 'deleted', 'one', 'countless', 'microsoft', 'support', 'pages', 'found', 'following', 'helpfull', 'post', 'azure', 'automatically', 'delete', 'intermediate', 'data', 'written', 'outputfiledatasetconfig', 'avoid', 'storage', 'charges', 'large', 'amounts', 'unneeded', 'data', 'either', 'idea', 'welcome']","['clean', 'azur', 'machin', 'learn', 'blob', 'storagemanag', 'frequent', 'use', 'azur', 'machin', 'learn', 'workspac', 'sever', 'experi', 'activ', 'pipelin', 'everyth', 'work', 'good', 'far', 'problem', 'get', 'rid', 'old', 'data', 'run', 'experi', 'pipelin', 'last', 'year', 'blob', 'storag', 'grew', 'enourmu', 'size', 'everi', 'pipelin', 'data', 'store', 'delet', 'older', 'run', 'experimn', 'use', 'gui', 'actual', 'pipelin', 'data', 'blob', 'store', 'delet', 'smart', 'way', 'clean', 'data', 'blob', 'store', 'run', 'delet', 'one', 'countless', 'microsoft', 'support', 'page', 'found', 'follow', 'helpful', 'post', 'azur', 'automat', 'delet', 'intermedi', 'data', 'written', 'outputfiledatasetconfig', 'avoid', 'storag', 'charg', 'larg', 'amount', 'unneed', 'data', 'either', 'idea', 'welcom']"
86,93,93,14038854,72905185,How to classify an unknown class from the test dataset if the training dataset never had that particular class in supervised machine learning,"<p>I have a training dataset whose features describe 2 classes (say dog and cat). But, my test dataset has more than 2 classes which are unknown. If I train and run SVM and Random forest on the training data, then it will not be able to find anything other species other than cat or dog on the test data.</p>
<p>Is there any algorithm to predict cat, dog or other species out of the test data? But still should be accurate enough to determine the cat and dog?</p>
<p>Sorry, if similar questions were asked. I checked quite some posts, but they were around 3 years old. So looking for any updated and efficient solutions.</p>
",15,0,-3,4,python;machine-learning;classification;supervised-learning,2022-07-08 04:52:11,2022-07-08 04:52:11,2022-07-08 04:52:11,i have a training dataset whose features describe  classes  say dog and cat   but  my test dataset has more than  classes which are unknown  if i train and run svm and random forest on the training data  then it will not be able to find anything other species other than cat or dog on the test data  is there any algorithm to predict cat  dog or other species out of the test data  but still should be accurate enough to determine the cat and dog  sorry  if similar questions were asked  i checked quite some posts  but they were around  years old  so looking for any updated and efficient solutions ,how to classify an unknown class from the test dataset if the training dataset never had that particular class in supervised machine learning,training dataset whose features describe classes say dog cat test dataset classes unknown train run svm random forest training data able find anything species cat dog test data algorithm predict cat dog species test data still accurate enough determine cat dog sorry similar questions asked checked quite posts around years old looking updated efficient solutions,classify unknown class test dataset training dataset never particular class supervised machine learning,classify unknown class test dataset training dataset never particular class supervised machine learningtraining dataset whose features describe classes say dog cat test dataset classes unknown train run svm random forest training data able find anything species cat dog test data algorithm predict cat dog species test data still accurate enough determine cat dog sorry similar questions asked checked quite posts around years old looking updated efficient solutions,"['classify', 'unknown', 'class', 'test', 'dataset', 'training', 'dataset', 'never', 'particular', 'class', 'supervised', 'machine', 'learningtraining', 'dataset', 'whose', 'features', 'describe', 'classes', 'say', 'dog', 'cat', 'test', 'dataset', 'classes', 'unknown', 'train', 'run', 'svm', 'random', 'forest', 'training', 'data', 'able', 'find', 'anything', 'species', 'cat', 'dog', 'test', 'data', 'algorithm', 'predict', 'cat', 'dog', 'species', 'test', 'data', 'still', 'accurate', 'enough', 'determine', 'cat', 'dog', 'sorry', 'similar', 'questions', 'asked', 'checked', 'quite', 'posts', 'around', 'years', 'old', 'looking', 'updated', 'efficient', 'solutions']","['classifi', 'unknown', 'class', 'test', 'dataset', 'train', 'dataset', 'never', 'particular', 'class', 'supervis', 'machin', 'learningtrain', 'dataset', 'whose', 'featur', 'describ', 'class', 'say', 'dog', 'cat', 'test', 'dataset', 'class', 'unknown', 'train', 'run', 'svm', 'random', 'forest', 'train', 'data', 'abl', 'find', 'anyth', 'speci', 'cat', 'dog', 'test', 'data', 'algorithm', 'predict', 'cat', 'dog', 'speci', 'test', 'data', 'still', 'accur', 'enough', 'determin', 'cat', 'dog', 'sorri', 'similar', 'question', 'ask', 'check', 'quit', 'post', 'around', 'year', 'old', 'look', 'updat', 'effici', 'solut']"
87,94,94,19457312,72826975,"I am working with Dask, what are the advantages of using Datashader for the dataviz instead of the classic Seaborn in Python?","<p>It is the first time I am working on a Machine Learning model with Dask, but before splitting the data I have to produce some visualisations of basic descriptive statistics. I have read that Datashader is &quot;smoother&quot; than Seaborn when working with Dask. What I don't understand is what makes that library optimal when parallelising with Dask. Is there any substantive advantage in terms of performance? By the way, I'm using the describe() method to calculate the descriptive statistics.</p>
<p>Thanks in advance.</p>
",25,1,-1,2,python;dask,2022-07-01 14:41:58,2022-07-01 14:41:58,2022-07-08 04:33:49,it is the first time i am working on a machine learning model with dask  but before splitting the data i have to produce some visualisations of basic descriptive statistics  i have read that datashader is  smoother  than seaborn when working with dask  what i don t understand is what makes that library optimal when parallelising with dask  is there any substantive advantage in terms of performance  by the way  i m using the describe   method to calculate the descriptive statistics  thanks in advance ,i am working with dask  what are the advantages of using datashader for the dataviz instead of the classic seaborn in python ,first time working machine learning model dask splitting data produce visualisations basic descriptive statistics read datashader smoother seaborn working dask understand makes library optimal parallelising dask substantive advantage terms performance way using describe method calculate descriptive statistics thanks advance,working dask advantages using datashader dataviz instead classic seaborn python,working dask advantages using datashader dataviz instead classic seaborn pythonfirst time working machine learning model dask splitting data produce visualisations basic descriptive statistics read datashader smoother seaborn working dask understand makes library optimal parallelising dask substantive advantage terms performance way using describe method calculate descriptive statistics thanks advance,"['working', 'dask', 'advantages', 'using', 'datashader', 'dataviz', 'instead', 'classic', 'seaborn', 'pythonfirst', 'time', 'working', 'machine', 'learning', 'model', 'dask', 'splitting', 'data', 'produce', 'visualisations', 'basic', 'descriptive', 'statistics', 'read', 'datashader', 'smoother', 'seaborn', 'working', 'dask', 'understand', 'makes', 'library', 'optimal', 'parallelising', 'dask', 'substantive', 'advantage', 'terms', 'performance', 'way', 'using', 'describe', 'method', 'calculate', 'descriptive', 'statistics', 'thanks', 'advance']","['work', 'dask', 'advantag', 'use', 'datashad', 'dataviz', 'instead', 'classic', 'seaborn', 'pythonfirst', 'time', 'work', 'machin', 'learn', 'model', 'dask', 'split', 'data', 'produc', 'visualis', 'basic', 'descript', 'statist', 'read', 'datashad', 'smoother', 'seaborn', 'work', 'dask', 'understand', 'make', 'librari', 'optim', 'parallelis', 'dask', 'substant', 'advantag', 'term', 'perform', 'way', 'use', 'describ', 'method', 'calcul', 'descript', 'statist', 'thank', 'advanc']"
88,95,95,18311567,72904193,Why is time taken by O(NlogN) algorithm same as that of O(N^2)?,"<p>I wrote two functions <code>maxSubSum2</code> and <code>maxSubSum3</code>, both of which try to find the maximum continuous sum of a sub-sequence in a given sequence.</p>
<h2><code>maxSubSum2()</code></h2>
<ul>
<li>Loops through the entire vector and sets a beginning marker <code>i</code> on each iteration.</li>
<li>For each beginning marker <code>i</code> it sets a corresponding ending marker <code>j</code>.</li>
<li>The elements bound between these two markers form a valid sub-sequence; therefore, it calculates its sum.</li>
<li>And checks if it is greater than the previous highest sum.</li>
</ul>
<pre class=""lang-cpp prettyprint-override""><code>int maxSubSum2(const std::vector&lt;int&gt; &amp;v)
{
    int maxSum = 0;
    for(std::size_t begin = 0; begin &lt; v.size(); ++begin)
    {
        int thisSum = 0;
        for(std::size_t end = begin; end &lt; v.size(); ++end)
        {
            thisSum += v[end];
            if(thisSum &gt; maxSum)
                maxSum = thisSum;
        }
    }
    return maxSum;
}
</code></pre>
<h2><code>maxSubSum3()</code></h2>
<ul>
<li>Is itself a driver function to the recursive function <code>maxSumRec()</code>.</li>
<li><code>maxSumRec</code> uses divide-and-conquer to calculate the maximum sub-sequence sum.</li>
<li>The maximum sub-sequence sum can be either in the entirety of the left part, or right part, or it can between the two (in which case, it is the sum of maximum sum in left part which <strong>includes</strong> its border: <code>center</code>, and the maximum sum in right part which also <strong>includes</strong> its border: <code>center + 1</code>.</li>
</ul>
<pre class=""lang-cpp prettyprint-override""><code>int maxSubSum3(const std::vector&lt;int&gt; &amp;v)
{
    return maxSumRec(v, 0, v.size() - 1);
}

int maxSumRec(const std::vector&lt;int&gt; &amp;v, std::vector&lt;int&gt;::size_type left, std::vector&lt;int&gt;::size_type right)
{
    if(left == right)
        if(v[left] &gt; 0)
            return v[left];
        else
            return 0;
    std::vector&lt;int&gt;::size_type center = (left + right) / 2;
    int maxLeftSum = maxSumRec(v, left, center);
    int maxRightSum = maxSumRec(v, center + 1, right);
    int maxLeftBorderSum = 0;
    int leftBorderSum = 0;
    for(std::vector&lt;int&gt;::size_type idx = center; idx &lt; v.size(); --idx)
    {
        leftBorderSum += v[idx];
        if(leftBorderSum &gt; maxLeftBorderSum)
            maxLeftBorderSum = leftBorderSum;
    }
    int maxRightBorderSum = 0;
    int rightBorderSum = 0;
    for(std::vector&lt;int&gt;::size_type idx = center + 1; idx &lt;= right; ++idx)
    {
        rightBorderSum += v[idx];
        if(rightBorderSum &gt; maxRightBorderSum)
            maxRightBorderSum = rightBorderSum;
    }
    return max3(maxLeftSum, maxRightSum, maxLeftBorderSum + maxRightBorderSum);
}

int max3(int n1, int n2, int n3)
{
    if(n1 &gt;= n2 &amp;&amp; n1 &gt;= n3) return n1;
    if(n2 &gt;= n1 &amp;&amp; n2 &gt;= n3) return n2;
    if(n3 &gt;= n1 &amp;&amp; n3 &gt;= n2) return n3;
    return 0; // &lt;--- Should never happen
}
</code></pre>
<p>Because <code>maxSubSum2()</code> has a double nested <code>for</code> loops, its time complexity needs to be <strong>O(N)</strong>, and because <code>maxSubSum3()</code> uses divide and conquer, its time complexity needs to be <strong>O(NlogN)</strong>.</p>
<p>However, I created a simple running time calculation function <code>stopwatch()</code> to measure the actual running time <code>runtime</code> for each function. It looks like the following:</p>
<pre class=""lang-cpp prettyprint-override""><code>void stopwatch(int (*maxSubSumN)(const std::vector&lt;int&gt;&amp;), const std::vector&lt;int&gt; &amp;v)
{
    std::chrono::time_point start = std::chrono::steady_clock::now();
    maxSubSumN(v);
    std::chrono::time_point end = std::chrono::steady_clock::now();
    std::chrono::duration&lt;double&gt; runtime = end - start;
    std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(9) &lt;&lt; std::left &lt;&lt; std::setw(9) &lt;&lt; runtime.count();
}
</code></pre>
<p>Before the program begins, I populate two vectors <code>small</code> and <code>big</code> with 1000 and 10000 randomly generated <code>int</code> respectively; like this:</p>
<pre class=""lang-cpp prettyprint-override""><code>int randInt()
{
    return std::rand() % 101 - 50;
}

void populate(std::vector&lt;int&gt; &amp;v)
{
    for(int &amp;i : v)
        i = randInt();
}

int main()
{
    std::srand(std::time(NULL));
    std::vector&lt;int&gt; small(1000);
    std::vector&lt;int&gt; big(10000);
    populate(small);
    populate(big);
    std::cout &lt;&lt; &quot;[OPTIMIZED BRUTE FORCE] \t: &quot;;
    stopwatch(maxSubSum2, small);
    std::cout &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;[OPTIMIZED BRUTE FORCE] \t: &quot;;
    stopwatch(maxSubSum2, big);
    std::cout &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;[DIVIDE AND CONQUER] \t\t: &quot;;
    stopwatch(maxSubSum3, small);
    std::cout &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;[DIVIDE AND CONQUER] \t\t: &quot;;
    stopwatch(maxSubSum3, big);
    std::cout &lt;&lt; std::endl;
    return 0;
}
</code></pre>
<p>However, after running the program several times, the execution time for both <code>maxSubSum2</code> and <code>maxSubSum3</code> is almost identical. Below are the results on my machine.</p>
<h2><code>small.size() == 10 &amp;&amp; big.size() == 100</code></h2>
<pre><code>[OPTIMIZED BRUTE FORCE]         : 0.000001015
[OPTIMIZED BRUTE FORCE]         : 0.000041509
[DIVIDE AND CONQUER]            : 0.000001789
[DIVIDE AND CONQUER]            : 0.000058110
</code></pre>
<h2><code>small.size() == 100 &amp;&amp; big.size() == 1000</code></h2>
<pre><code>[OPTIMIZED BRUTE FORCE]         : 0.000042093
[OPTIMIZED BRUTE FORCE]         : 0.003870208
[DIVIDE AND CONQUER]            : 0.000053203
[DIVIDE AND CONQUER]            : 0.003899243
</code></pre>
<h2><code>small.size() == 1000 &amp;&amp; big.size() == 10000</code></h2>
<pre><code>[OPTIMIZED BRUTE FORCE]         : 0.002765456
[OPTIMIZED BRUTE FORCE]         : 0.271172096
[DIVIDE AND CONQUER]            : 0.002931273
[DIVIDE AND CONQUER]            : 0.274476880
</code></pre>
<h2><code>small.size() == 1000 &amp;&amp; big.size() == 1000000</code></h2>
<pre><code>[OPTIMIZED BRUTE FORCE]         : 0.002730444
[OPTIMIZED BRUTE FORCE]         : 26.383375030
[DIVIDE AND CONQUER]            : 0.002903615
[DIVIDE AND CONQUER]            : 26.508168165
</code></pre>
<ul>
<li>Is there something that I did wrong in calculation of time complexities?</li>
<li>Did I make some mistake in the implementation of <code>maxSumRec</code> <em>it gives the right results though</em>?</li>
<li>Is there some other bottleneck in the implementation that I did not consider?</li>
<li>Or, is there something that I am missing or did not understand?</li>
</ul>
<p>Any help would be highly appreciated. For reference, I am learning from the book: <a href=""https://www.pearson.com/us/higher-education/program/Weiss-Data-Structures-and-Algorithm-Analysis-in-C-4th-Edition/PGM148299.html"" rel=""nofollow noreferrer"">Data Structures and Algorithms in C++ (4th Edition)</a>.</p>
<h2>Problem solved. Problematic part: use of overflowing integer</h2>
<p>See <a href=""https://stackoverflow.com/a/72904384/18311567"">interjay's answer below</a></p>
<pre class=""lang-cpp prettyprint-override""><code>for(std::vector&lt;int&gt;::size_type idx = center; idx &lt; v.size(); --idx)
</code></pre>
<blockquote>
<p>It makes a difference because the loop is only supposed to go down to left but you made it go down to 0. This changes the recursive call's runtime to O(n) instead of O(right - left), and the total runtime to O(n^2) because there are a total of O(n) recursive calls</p>
</blockquote>
<p><strong>Credits:</strong> <a href=""https://stackoverflow.com/users/189205/interjay"">interjay</a></p>
",100,1,2,4,c++;algorithm;c++11;time-complexity,2022-07-08 02:34:59,2022-07-08 02:34:59,2022-07-08 03:14:40,i wrote two functions maxsubsum and maxsubsum  both of which try to find the maximum continuous sum of a sub sequence in a given sequence  because maxsubsum   has a double nested for loops  its time complexity needs to be o n   and because maxsubsum   uses divide and conquer  its time complexity needs to be o nlogn   however  i created a simple running time calculation function stopwatch   to measure the actual running time runtime for each function  it looks like the following  before the program begins  i populate two vectors small and big with  and  randomly generated int respectively  like this  however  after running the program several times  the execution time for both maxsubsum and maxsubsum is almost identical  below are the results on my machine  any help would be highly appreciated  for reference  i am learning from the book    see  it makes a difference because the loop is only supposed to go down to left but you made it go down to   this changes the recursive call s runtime to o n  instead of o right   left   and the total runtime to o n   because there are a total of o n  recursive calls credits  ,why is time taken by o nlogn  algorithm same as that of o n   ,wrote two functions maxsubsum maxsubsum try find maximum continuous sum sub sequence given sequence maxsubsum double nested loops time complexity needs n maxsubsum uses divide conquer time complexity needs nlogn however created simple running time calculation function stopwatch measure actual running time runtime function looks like following program begins populate two vectors small big randomly generated int respectively like however running program several times execution time maxsubsum maxsubsum almost identical results machine help would highly appreciated reference learning book see makes difference loop supposed go left made go changes recursive call runtime n instead right left total runtime n total n recursive calls credits,time taken nlogn algorithm n,time taken nlogn algorithm nwrote two functions maxsubsum maxsubsum try find maximum continuous sum sub sequence given sequence maxsubsum double nested loops time complexity needs n maxsubsum uses divide conquer time complexity needs nlogn however created simple running time calculation function stopwatch measure actual running time runtime function looks like following program begins populate two vectors small big randomly generated int respectively like however running program several times execution time maxsubsum maxsubsum almost identical results machine help would highly appreciated reference learning book see makes difference loop supposed go left made go changes recursive call runtime n instead right left total runtime n total n recursive calls credits,"['time', 'taken', 'nlogn', 'algorithm', 'nwrote', 'two', 'functions', 'maxsubsum', 'maxsubsum', 'try', 'find', 'maximum', 'continuous', 'sum', 'sub', 'sequence', 'given', 'sequence', 'maxsubsum', 'double', 'nested', 'loops', 'time', 'complexity', 'needs', 'n', 'maxsubsum', 'uses', 'divide', 'conquer', 'time', 'complexity', 'needs', 'nlogn', 'however', 'created', 'simple', 'running', 'time', 'calculation', 'function', 'stopwatch', 'measure', 'actual', 'running', 'time', 'runtime', 'function', 'looks', 'like', 'following', 'program', 'begins', 'populate', 'two', 'vectors', 'small', 'big', 'randomly', 'generated', 'int', 'respectively', 'like', 'however', 'running', 'program', 'several', 'times', 'execution', 'time', 'maxsubsum', 'maxsubsum', 'almost', 'identical', 'results', 'machine', 'help', 'would', 'highly', 'appreciated', 'reference', 'learning', 'book', 'see', 'makes', 'difference', 'loop', 'supposed', 'go', 'left', 'made', 'go', 'changes', 'recursive', 'call', 'runtime', 'n', 'instead', 'right', 'left', 'total', 'runtime', 'n', 'total', 'n', 'recursive', 'calls', 'credits']","['time', 'taken', 'nlogn', 'algorithm', 'nwrote', 'two', 'function', 'maxsubsum', 'maxsubsum', 'tri', 'find', 'maximum', 'continu', 'sum', 'sub', 'sequenc', 'given', 'sequenc', 'maxsubsum', 'doubl', 'nest', 'loop', 'time', 'complex', 'need', 'n', 'maxsubsum', 'use', 'divid', 'conquer', 'time', 'complex', 'need', 'nlogn', 'howev', 'creat', 'simpl', 'run', 'time', 'calcul', 'function', 'stopwatch', 'measur', 'actual', 'run', 'time', 'runtim', 'function', 'look', 'like', 'follow', 'program', 'begin', 'popul', 'two', 'vector', 'small', 'big', 'randomli', 'gener', 'int', 'respect', 'like', 'howev', 'run', 'program', 'sever', 'time', 'execut', 'time', 'maxsubsum', 'maxsubsum', 'almost', 'ident', 'result', 'machin', 'help', 'would', 'highli', 'appreci', 'refer', 'learn', 'book', 'see', 'make', 'differ', 'loop', 'suppos', 'go', 'left', 'made', 'go', 'chang', 'recurs', 'call', 'runtim', 'n', 'instead', 'right', 'left', 'total', 'runtim', 'n', 'total', 'n', 'recurs', 'call', 'credit']"
89,96,96,17264096,72818690,How to identify curved lines in an image and take their lengths based on a scale,"<p>I am trying to automate the measurement of several curved lines based on a scale (see the example image). I have several images like the example one that I have to extract the measurement of each line. I have managed to binarize the image and have been searching for a solution with opencv in python. The procedure that I thought the algorithm should follow is something like:</p>
<ol>
<li>binarize the image</li>
<li>find the scale and set it for measurements</li>
<li>identify lines to be measured</li>
<li>measure lines based on scale</li>
<li>store measurements in a dataframe</li>
</ol>
<p>I am newbie with programming and been thinking in doing it with python. Should I apply a machine learning algorithm to segment each region containing each line and get their measurement? Or is there a simpler and more intuitive way of doing it without having to train a ML algorithm?</p>
<p><a href=""https://i.stack.imgur.com/JniYS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JniYS.png"" alt=""lines image"" /></a></p>
",75,2,1,5,python;image;opencv;image-processing;computer-vision,2022-06-30 21:20:58,2022-06-30 21:20:58,2022-07-08 00:16:39,i am trying to automate the measurement of several curved lines based on a scale  see the example image   i have several images like the example one that i have to extract the measurement of each line  i have managed to binarize the image and have been searching for a solution with opencv in python  the procedure that i thought the algorithm should follow is something like  i am newbie with programming and been thinking in doing it with python  should i apply a machine learning algorithm to segment each region containing each line and get their measurement  or is there a simpler and more intuitive way of doing it without having to train a ml algorithm  ,how to identify curved lines in an image and take their lengths based on a scale,trying automate measurement several curved lines based scale see example image several images like example one extract measurement line managed binarize image searching solution opencv python procedure thought algorithm follow something like newbie programming thinking python apply machine learning algorithm segment region containing line get measurement simpler intuitive way without train ml algorithm,identify curved lines image take lengths based scale,identify curved lines image take lengths based scaletrying automate measurement several curved lines based scale see example image several images like example one extract measurement line managed binarize image searching solution opencv python procedure thought algorithm follow something like newbie programming thinking python apply machine learning algorithm segment region containing line get measurement simpler intuitive way without train ml algorithm,"['identify', 'curved', 'lines', 'image', 'take', 'lengths', 'based', 'scaletrying', 'automate', 'measurement', 'several', 'curved', 'lines', 'based', 'scale', 'see', 'example', 'image', 'several', 'images', 'like', 'example', 'one', 'extract', 'measurement', 'line', 'managed', 'binarize', 'image', 'searching', 'solution', 'opencv', 'python', 'procedure', 'thought', 'algorithm', 'follow', 'something', 'like', 'newbie', 'programming', 'thinking', 'python', 'apply', 'machine', 'learning', 'algorithm', 'segment', 'region', 'containing', 'line', 'get', 'measurement', 'simpler', 'intuitive', 'way', 'without', 'train', 'ml', 'algorithm']","['identifi', 'curv', 'line', 'imag', 'take', 'length', 'base', 'scaletri', 'autom', 'measur', 'sever', 'curv', 'line', 'base', 'scale', 'see', 'exampl', 'imag', 'sever', 'imag', 'like', 'exampl', 'one', 'extract', 'measur', 'line', 'manag', 'binar', 'imag', 'search', 'solut', 'opencv', 'python', 'procedur', 'thought', 'algorithm', 'follow', 'someth', 'like', 'newbi', 'program', 'think', 'python', 'appli', 'machin', 'learn', 'algorithm', 'segment', 'region', 'contain', 'line', 'get', 'measur', 'simpler', 'intuit', 'way', 'without', 'train', 'ml', 'algorithm']"
90,97,97,19316680,72902042,How to determine the community affiliation of a set of nodes from a set of known communities containing the rest of nodes in a network?,"<p>I have a dictionary that contains the nodes and edges in an activity network diagram. I also have a dictionary that specifies the community corresponding to each node. Specifically, there is a community called &quot;Unknown&quot; that contains a good fraction of the nodes. This community is named as such since some of the nodes are not known to belong to any one of the other known communities. Do I need a machine learning algorithm to assign a community label (from the available ones) for those nodes that are missing community affiliation?</p>
<p>I would appreciate if you can put me on a right track for this problem.</p>
",23,0,0,5,machine-learning;graph;deep-learning;neural-network;network-analysis,2022-07-07 22:58:07,2022-07-07 22:58:07,2022-07-08 00:12:53,i have a dictionary that contains the nodes and edges in an activity network diagram  i also have a dictionary that specifies the community corresponding to each node  specifically  there is a community called  unknown  that contains a good fraction of the nodes  this community is named as such since some of the nodes are not known to belong to any one of the other known communities  do i need a machine learning algorithm to assign a community label  from the available ones  for those nodes that are missing community affiliation  i would appreciate if you can put me on a right track for this problem ,how to determine the community affiliation of a set of nodes from a set of known communities containing the rest of nodes in a network ,dictionary contains nodes edges activity network diagram also dictionary specifies community corresponding node specifically community called unknown contains good fraction nodes community named since nodes known belong one known communities need machine learning algorithm assign community label available ones nodes missing community affiliation would appreciate put right track problem,determine community affiliation set nodes set known communities containing rest nodes network,determine community affiliation set nodes set known communities containing rest nodes networkdictionary contains nodes edges activity network diagram also dictionary specifies community corresponding node specifically community called unknown contains good fraction nodes community named since nodes known belong one known communities need machine learning algorithm assign community label available ones nodes missing community affiliation would appreciate put right track problem,"['determine', 'community', 'affiliation', 'set', 'nodes', 'set', 'known', 'communities', 'containing', 'rest', 'nodes', 'networkdictionary', 'contains', 'nodes', 'edges', 'activity', 'network', 'diagram', 'also', 'dictionary', 'specifies', 'community', 'corresponding', 'node', 'specifically', 'community', 'called', 'unknown', 'contains', 'good', 'fraction', 'nodes', 'community', 'named', 'since', 'nodes', 'known', 'belong', 'one', 'known', 'communities', 'need', 'machine', 'learning', 'algorithm', 'assign', 'community', 'label', 'available', 'ones', 'nodes', 'missing', 'community', 'affiliation', 'would', 'appreciate', 'put', 'right', 'track', 'problem']","['determin', 'commun', 'affili', 'set', 'node', 'set', 'known', 'commun', 'contain', 'rest', 'node', 'networkdictionari', 'contain', 'node', 'edg', 'activ', 'network', 'diagram', 'also', 'dictionari', 'specifi', 'commun', 'correspond', 'node', 'specif', 'commun', 'call', 'unknown', 'contain', 'good', 'fraction', 'node', 'commun', 'name', 'sinc', 'node', 'known', 'belong', 'one', 'known', 'commun', 'need', 'machin', 'learn', 'algorithm', 'assign', 'commun', 'label', 'avail', 'one', 'node', 'miss', 'commun', 'affili', 'would', 'appreci', 'put', 'right', 'track', 'problem']"
91,98,98,19503683,72899698,How can I check the gameobject that the script is attached to in order to use the same script on multiple gameobjects?,"<p>I've just started learning to use Unity and so far I've managed to solve the problems I've encountered. I tried not to post, but I couldn't find what I was looking for. </p>
<p>In my game, I have a machine that changes the players' sprites. I wanted to use the same script for multiple machines. The idea was to detect the game object (machine) and change the sprite accordingly. For example, the script detects that it is a green machine and changes the sprites to green or detects that it is a red machine and changes the sprites to red.</p>
<p>I hope I made myself clear. I tend to overcomplicate things...</p>
",38,1,0,2,c#;unity3d,2022-07-07 20:04:59,2022-07-07 20:04:59,2022-07-07 23:22:36,i ve just started learning to use unity and so far i ve managed to solve the problems i ve encountered  i tried not to post  but i couldn t find what i was looking for   in my game  i have a machine that changes the players  sprites  i wanted to use the same script for multiple machines  the idea was to detect the game object  machine  and change the sprite accordingly  for example  the script detects that it is a green machine and changes the sprites to green or detects that it is a red machine and changes the sprites to red  i hope i made myself clear  i tend to overcomplicate things   ,how can i check the gameobject that the script is attached to in order to use the same script on multiple gameobjects ,started learning use unity far managed solve problems encountered tried post find looking game machine changes players sprites wanted use script multiple machines idea detect game object machine change sprite accordingly example script detects green machine changes sprites green detects red machine changes sprites red hope made clear tend overcomplicate things,check gameobject script attached order use script multiple gameobjects,check gameobject script attached order use script multiple gameobjectsstarted learning use unity far managed solve problems encountered tried post find looking game machine changes players sprites wanted use script multiple machines idea detect game object machine change sprite accordingly example script detects green machine changes sprites green detects red machine changes sprites red hope made clear tend overcomplicate things,"['check', 'gameobject', 'script', 'attached', 'order', 'use', 'script', 'multiple', 'gameobjectsstarted', 'learning', 'use', 'unity', 'far', 'managed', 'solve', 'problems', 'encountered', 'tried', 'post', 'find', 'looking', 'game', 'machine', 'changes', 'players', 'sprites', 'wanted', 'use', 'script', 'multiple', 'machines', 'idea', 'detect', 'game', 'object', 'machine', 'change', 'sprite', 'accordingly', 'example', 'script', 'detects', 'green', 'machine', 'changes', 'sprites', 'green', 'detects', 'red', 'machine', 'changes', 'sprites', 'red', 'hope', 'made', 'clear', 'tend', 'overcomplicate', 'things']","['check', 'gameobject', 'script', 'attach', 'order', 'use', 'script', 'multipl', 'gameobjectsstart', 'learn', 'use', 'uniti', 'far', 'manag', 'solv', 'problem', 'encount', 'tri', 'post', 'find', 'look', 'game', 'machin', 'chang', 'player', 'sprite', 'want', 'use', 'script', 'multipl', 'machin', 'idea', 'detect', 'game', 'object', 'machin', 'chang', 'sprite', 'accordingli', 'exampl', 'script', 'detect', 'green', 'machin', 'chang', 'sprite', 'green', 'detect', 'red', 'machin', 'chang', 'sprite', 'red', 'hope', 'made', 'clear', 'tend', 'overcompl', 'thing']"
92,99,99,17394856,72897473,Can I separately train a classifier (e.g. SVM) with two different types of features and combine the results later?,"<p>I am a student and working on my first simple machine learning project. The project is about classifying articles into fake and true. I want to use SVM as classification algorithm and two different types of features:</p>
<ol>
<li>TF-IDF</li>
<li>Lexical Features like the count of exclamation marks and numbers</li>
</ol>
<p>I have figured out how to use the lexical features and TF-IDF as a features separately. However, I have not managed to figure out, how to combine them.</p>
<p><strong>Is it possible, to train and test two separate learning algorithms (one with TF-IDF and the other one with lexical features) and later combine the results?</strong></p>
<p>For example, can I calculate Accuracy, Precision and Recall for both separately and then take the average?</p>
",24,1,0,1,classification,2022-07-07 17:29:36,2022-07-07 17:29:36,2022-07-07 23:03:16,i am a student and working on my first simple machine learning project  the project is about classifying articles into fake and true  i want to use svm as classification algorithm and two different types of features  i have figured out how to use the lexical features and tf idf as a features separately  however  i have not managed to figure out  how to combine them  is it possible  to train and test two separate learning algorithms  one with tf idf and the other one with lexical features  and later combine the results  for example  can i calculate accuracy  precision and recall for both separately and then take the average ,can i separately train a classifier  e g  svm  with two different types of features and combine the results later ,student working first simple machine learning project project classifying articles fake true want use svm classification algorithm two different types features figured use lexical features tf idf features separately however managed figure combine possible train test two separate learning algorithms one tf idf one lexical features later combine results example calculate accuracy precision recall separately take average,separately train classifier e g svm two different types features combine results later,separately train classifier e g svm two different types features combine results laterstudent working first simple machine learning project project classifying articles fake true want use svm classification algorithm two different types features figured use lexical features tf idf features separately however managed figure combine possible train test two separate learning algorithms one tf idf one lexical features later combine results example calculate accuracy precision recall separately take average,"['separately', 'train', 'classifier', 'e', 'g', 'svm', 'two', 'different', 'types', 'features', 'combine', 'results', 'laterstudent', 'working', 'first', 'simple', 'machine', 'learning', 'project', 'project', 'classifying', 'articles', 'fake', 'true', 'want', 'use', 'svm', 'classification', 'algorithm', 'two', 'different', 'types', 'features', 'figured', 'use', 'lexical', 'features', 'tf', 'idf', 'features', 'separately', 'however', 'managed', 'figure', 'combine', 'possible', 'train', 'test', 'two', 'separate', 'learning', 'algorithms', 'one', 'tf', 'idf', 'one', 'lexical', 'features', 'later', 'combine', 'results', 'example', 'calculate', 'accuracy', 'precision', 'recall', 'separately', 'take', 'average']","['separ', 'train', 'classifi', 'e', 'g', 'svm', 'two', 'differ', 'type', 'featur', 'combin', 'result', 'laterstud', 'work', 'first', 'simpl', 'machin', 'learn', 'project', 'project', 'classifi', 'articl', 'fake', 'true', 'want', 'use', 'svm', 'classif', 'algorithm', 'two', 'differ', 'type', 'featur', 'figur', 'use', 'lexic', 'featur', 'tf', 'idf', 'featur', 'separ', 'howev', 'manag', 'figur', 'combin', 'possibl', 'train', 'test', 'two', 'separ', 'learn', 'algorithm', 'one', 'tf', 'idf', 'one', 'lexic', 'featur', 'later', 'combin', 'result', 'exampl', 'calcul', 'accuraci', 'precis', 'recal', 'separ', 'take', 'averag']"
93,100,100,19238612,72900440,How RandomOverSampler from imblearn is oversampling the data?,"<p>I am working on a machine learning problem in which I should determine if a particular asteroid correspond to a certain class, among 6 classes, or to another.
The dataset I am using is very unbalanced with respect to one class and if I use the RandomOverSampler class from the Imblearn model on python it works really well.
The point is that I would like to dig more how this class (RandomOverSampler) works, I have also read the paper that they link in the documentation (<a href=""https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler"" rel=""nofollow noreferrer"">https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler</a>) in the reference 1, but I didn't get if on default it is working oversampling the data just by duplicating or more or if there is a not naive way to do that.
When I say default it means just using the class with no parameters inside, I know it is not using for sure the SMOTE and the ADASYN techniques because there are separete classes for that, but I'd like to know if it is using the ROSE technique explained in the paper they link (<a href=""https://link.springer.com/article/10.1007/s10618-012-0295-5"" rel=""nofollow noreferrer"">https://link.springer.com/article/10.1007/s10618-012-0295-5</a>).</p>
<p>Thank you so much for your answer!</p>
",16,0,-1,5,python;dataset;random-forest;imbalanced-data;oversampling,2022-07-07 20:51:40,2022-07-07 20:51:40,2022-07-07 20:51:40,thank you so much for your answer ,how randomoversampler from imblearn is oversampling the data ,thank much answer,randomoversampler imblearn oversampling data,randomoversampler imblearn oversampling datathank much answer,"['randomoversampler', 'imblearn', 'oversampling', 'datathank', 'much', 'answer']","['randomoversampl', 'imblearn', 'oversampl', 'datathank', 'much', 'answer']"
94,101,101,17499387,72897875,"Deep Reinforcement Learning, how to make an agent that control many machines","<p>Good morning, Im facing a &quot;RL&quot; problem, which have many constraints, the main idea is that my agent will control many different machines with for example ordering them to go out for doing their missions (we don't give importance for the mission), or ordering them to enter to the depot and choosing for them the right place where they should sit (depending from constraints).
The problem is: the agent will take decision at periods of time that are defined, for each periode we know which of actions (go out, go in) are allowed. He will for example at 8oclock decide to order for 4 machines to go out, and at 14oclock decide to bring back 2 machines(with choosing for them the right place).</p>
<p>In literature i show many ideas which refers to BDQ, but is it recquired for my problem ? Im thinking about having actions like [chooseMachine1, chooseMachine2,chooseMachine3...chooseMachineN, goOut, goInPlace1, goInPlace2, goInPlace3, goInPlace4]. And in the code specifying the logic that depending of the period we are, i expose for the begening a number M&lt;=N of the machines to choose (with giving 0 probability to those actions that aren't possible for the moment' if it is 14oclock you know that only the machines that are out are concerned with the agent decision'), if the agent choose Machine1, so he will access to only the possible actions from choosing it.</p>
<p>So, my question is, do you think that my ideas are right ? (am beginner), my idea is to make a DQN with giving the logic for the possible/impossible actions,
Do you think that a BDQ is more accurate with my problem ? like having N branchs for N machines which have the same possible actions (brach1(Machine1) : go out, goPlace1, goPlace2 ...)
If it is the case is there any implementation examples ?</p>
<p>If you have ressources to advise me, i will be glad of checking them.</p>
<p>Thank You</p>
",48,1,0,5,deep-learning;artificial-intelligence;reinforcement-learning;dqn;multi-agent-reinforcement-learning,2022-07-07 18:01:59,2022-07-07 18:01:59,2022-07-07 20:38:44,in literature i show many ideas which refers to bdq  but is it recquired for my problem   im thinking about having actions like  choosemachine  choosemachine choosemachine   choosemachinen  goout  goinplace  goinplace  goinplace  goinplace   and in the code specifying the logic that depending of the period we are  i expose for the begening a number m lt  n of the machines to choose  with giving  probability to those actions that aren t possible for the moment  if it is oclock you know that only the machines that are out are concerned with the agent decision    if the agent choose machine  so he will access to only the possible actions from choosing it  if you have ressources to advise me  i will be glad of checking them  thank you,deep reinforcement learning  how to make an agent that control many machines,literature show many ideas refers bdq recquired problem im thinking actions like choosemachine choosemachine choosemachine choosemachinen goout goinplace goinplace goinplace goinplace code specifying logic depending period expose begening number lt n machines choose giving probability actions possible moment oclock know machines concerned agent decision agent choose machine access possible actions choosing ressources advise glad checking thank,deep reinforcement learning make agent control many machines,deep reinforcement learning make agent control many machinesliterature show many ideas refers bdq recquired problem im thinking actions like choosemachine choosemachine choosemachine choosemachinen goout goinplace goinplace goinplace goinplace code specifying logic depending period expose begening number lt n machines choose giving probability actions possible moment oclock know machines concerned agent decision agent choose machine access possible actions choosing ressources advise glad checking thank,"['deep', 'reinforcement', 'learning', 'make', 'agent', 'control', 'many', 'machinesliterature', 'show', 'many', 'ideas', 'refers', 'bdq', 'recquired', 'problem', 'im', 'thinking', 'actions', 'like', 'choosemachine', 'choosemachine', 'choosemachine', 'choosemachinen', 'goout', 'goinplace', 'goinplace', 'goinplace', 'goinplace', 'code', 'specifying', 'logic', 'depending', 'period', 'expose', 'begening', 'number', 'lt', 'n', 'machines', 'choose', 'giving', 'probability', 'actions', 'possible', 'moment', 'oclock', 'know', 'machines', 'concerned', 'agent', 'decision', 'agent', 'choose', 'machine', 'access', 'possible', 'actions', 'choosing', 'ressources', 'advise', 'glad', 'checking', 'thank']","['deep', 'reinforc', 'learn', 'make', 'agent', 'control', 'mani', 'machinesliteratur', 'show', 'mani', 'idea', 'refer', 'bdq', 'recquir', 'problem', 'im', 'think', 'action', 'like', 'choosemachin', 'choosemachin', 'choosemachin', 'choosemachinen', 'goout', 'goinplac', 'goinplac', 'goinplac', 'goinplac', 'code', 'specifi', 'logic', 'depend', 'period', 'expos', 'begen', 'number', 'lt', 'n', 'machin', 'choos', 'give', 'probabl', 'action', 'possibl', 'moment', 'oclock', 'know', 'machin', 'concern', 'agent', 'decis', 'agent', 'choos', 'machin', 'access', 'possibl', 'action', 'choos', 'ressourc', 'advis', 'glad', 'check', 'thank']"
95,102,102,7196912,72899661,Zorin Permanently change bluetooth headset name,"<p>So I have Zorin OS 16.1 and I am trying to permanently change the name of my bluetooth headset. I want to change it as we have several users that use the same model headset and it would help if we could change the name of the headset on their specific machine.</p>
<p>I tried to follow the answer from <a href=""https://askubuntu.com/questions/80960/how-to-change-bluetooth-broadcast-device-name"">https://askubuntu.com/questions/80960/how-to-change-bluetooth-broadcast-device-name</a></p>
<p>I get it to change the name, but as soon as I disconnect it then reconnect the headset it reverts back to the original name. Is what I am looking to do possible in Zorin?</p>
<p>I have tried changing the name withing /etc/bluetooth and /var/lib/bluetooth. Even tried creating a file called machine-info. Could use some guidance. I am still learning Linux as a whole.</p>
",13,0,0,1,bluetooth,2022-07-07 20:02:06,2022-07-07 20:02:06,2022-07-07 20:02:06,so i have zorin os   and i am trying to permanently change the name of my bluetooth headset  i want to change it as we have several users that use the same model headset and it would help if we could change the name of the headset on their specific machine  i tried to follow the answer from  i get it to change the name  but as soon as i disconnect it then reconnect the headset it reverts back to the original name  is what i am looking to do possible in zorin  i have tried changing the name withing  etc bluetooth and  var lib bluetooth  even tried creating a file called machine info  could use some guidance  i am still learning linux as a whole ,zorin permanently change bluetooth headset name,zorin os trying permanently change name bluetooth headset want change several users use model headset would help could change name headset specific machine tried follow answer get change name soon disconnect reconnect headset reverts back original name looking possible zorin tried changing name withing etc bluetooth var lib bluetooth even tried creating file called machine info could use guidance still learning linux whole,zorin permanently change bluetooth headset name,zorin permanently change bluetooth headset namezorin os trying permanently change name bluetooth headset want change several users use model headset would help could change name headset specific machine tried follow answer get change name soon disconnect reconnect headset reverts back original name looking possible zorin tried changing name withing etc bluetooth var lib bluetooth even tried creating file called machine info could use guidance still learning linux whole,"['zorin', 'permanently', 'change', 'bluetooth', 'headset', 'namezorin', 'os', 'trying', 'permanently', 'change', 'name', 'bluetooth', 'headset', 'want', 'change', 'several', 'users', 'use', 'model', 'headset', 'would', 'help', 'could', 'change', 'name', 'headset', 'specific', 'machine', 'tried', 'follow', 'answer', 'get', 'change', 'name', 'soon', 'disconnect', 'reconnect', 'headset', 'reverts', 'back', 'original', 'name', 'looking', 'possible', 'zorin', 'tried', 'changing', 'name', 'withing', 'etc', 'bluetooth', 'var', 'lib', 'bluetooth', 'even', 'tried', 'creating', 'file', 'called', 'machine', 'info', 'could', 'use', 'guidance', 'still', 'learning', 'linux', 'whole']","['zorin', 'perman', 'chang', 'bluetooth', 'headset', 'namezorin', 'os', 'tri', 'perman', 'chang', 'name', 'bluetooth', 'headset', 'want', 'chang', 'sever', 'user', 'use', 'model', 'headset', 'would', 'help', 'could', 'chang', 'name', 'headset', 'specif', 'machin', 'tri', 'follow', 'answer', 'get', 'chang', 'name', 'soon', 'disconnect', 'reconnect', 'headset', 'revert', 'back', 'origin', 'name', 'look', 'possibl', 'zorin', 'tri', 'chang', 'name', 'with', 'etc', 'bluetooth', 'var', 'lib', 'bluetooth', 'even', 'tri', 'creat', 'file', 'call', 'machin', 'info', 'could', 'use', 'guidanc', 'still', 'learn', 'linux', 'whole']"
96,103,103,14350322,72896686,Cloud GPU for CNN projects,"<p>A project I worked on involved recognizing the arrow on a manometer and calculating the steam machine's pressure using OpenCV. Everything was live, so I used a web cam. Since accuracy was solely dependent upon external factors such as sharpness of the picture, light, darkness, etc., I was not satisfied with the results.</p>
<p>Now I plan to implement a deep learning solution for that case, i.e. train a neural network (probably CNN type) and use it the same way . However, the question is whether cloud GPU solutions are viable, especially when working with external devices, such as cameras, in real time. Due to security concerns, I've heard it's difficult to use web cam in Google Colab. Are there any other options? Thanks in advance.</p>
",13,0,-1,3,deep-learning;conv-neural-network;cloud,2022-07-07 16:33:22,2022-07-07 16:33:22,2022-07-07 17:36:52,a project i worked on involved recognizing the arrow on a manometer and calculating the steam machine s pressure using opencv  everything was live  so i used a web cam  since accuracy was solely dependent upon external factors such as sharpness of the picture  light  darkness  etc   i was not satisfied with the results  now i plan to implement a deep learning solution for that case  i e  train a neural network  probably cnn type  and use it the same way   however  the question is whether cloud gpu solutions are viable  especially when working with external devices  such as cameras  in real time  due to security concerns  i ve heard it s difficult to use web cam in google colab  are there any other options  thanks in advance ,cloud gpu for cnn projects,project worked involved recognizing arrow manometer calculating steam machine pressure using opencv everything live used web cam since accuracy solely dependent upon external factors sharpness picture light darkness etc satisfied results plan implement deep learning solution case e train neural network probably cnn type use way however question whether cloud gpu solutions viable especially working external devices cameras real time due security concerns heard difficult use web cam google colab options thanks advance,cloud gpu cnn projects,cloud gpu cnn projectsproject worked involved recognizing arrow manometer calculating steam machine pressure using opencv everything live used web cam since accuracy solely dependent upon external factors sharpness picture light darkness etc satisfied results plan implement deep learning solution case e train neural network probably cnn type use way however question whether cloud gpu solutions viable especially working external devices cameras real time due security concerns heard difficult use web cam google colab options thanks advance,"['cloud', 'gpu', 'cnn', 'projectsproject', 'worked', 'involved', 'recognizing', 'arrow', 'manometer', 'calculating', 'steam', 'machine', 'pressure', 'using', 'opencv', 'everything', 'live', 'used', 'web', 'cam', 'since', 'accuracy', 'solely', 'dependent', 'upon', 'external', 'factors', 'sharpness', 'picture', 'light', 'darkness', 'etc', 'satisfied', 'results', 'plan', 'implement', 'deep', 'learning', 'solution', 'case', 'e', 'train', 'neural', 'network', 'probably', 'cnn', 'type', 'use', 'way', 'however', 'question', 'whether', 'cloud', 'gpu', 'solutions', 'viable', 'especially', 'working', 'external', 'devices', 'cameras', 'real', 'time', 'due', 'security', 'concerns', 'heard', 'difficult', 'use', 'web', 'cam', 'google', 'colab', 'options', 'thanks', 'advance']","['cloud', 'gpu', 'cnn', 'projectsproject', 'work', 'involv', 'recogn', 'arrow', 'manomet', 'calcul', 'steam', 'machin', 'pressur', 'use', 'opencv', 'everyth', 'live', 'use', 'web', 'cam', 'sinc', 'accuraci', 'sole', 'depend', 'upon', 'extern', 'factor', 'sharp', 'pictur', 'light', 'dark', 'etc', 'satisfi', 'result', 'plan', 'implement', 'deep', 'learn', 'solut', 'case', 'e', 'train', 'neural', 'network', 'probabl', 'cnn', 'type', 'use', 'way', 'howev', 'question', 'whether', 'cloud', 'gpu', 'solut', 'viabl', 'especi', 'work', 'extern', 'devic', 'camera', 'real', 'time', 'due', 'secur', 'concern', 'heard', 'difficult', 'use', 'web', 'cam', 'googl', 'colab', 'option', 'thank', 'advanc']"
97,104,104,14027466,72896426,How to stabilize NN-model for timeseries prediction,"<p>I'm new to Machine Learning and Neural Nets and am experimenting with a configuration I found in a <a href=""https://iq.opengenus.org/time-series-prediction-techniques/"" rel=""nofollow noreferrer"">blog post</a>. I try to build a model to predict the value 4 time steps ahead of the last value fed to the model as a feature (window-size is flexibel). E.g. I give the model weeks 1-5 and want to get a prediction for week 9.</p>
<p>Currently I run into the challenge that the model is quite unstable. With this I mean that the predictions differ quite a lot if repeating the steps of data preparation, building a dataset, building and training the model and finally doing the forecasting.</p>
<p>Sometimes the forecast seems to fit quite nice, sometimes it seems to be inverted via a negativ sign and sometimes just the amplitude doesn't match while the directions are right (see the following screenshots).</p>
<p><a href=""https://i.stack.imgur.com/12Lxk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/12Lxk.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Wauis.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wauis.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/P64gg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P64gg.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/jkF8M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jkF8M.png"" alt=""enter image description here"" /></a></p>
<p>I thought that might has to something with the shuffling and size of shuffle_buffer, but this also happens for different sizes of shuffle_buffer, e.g. 20.</p>
<p><a href=""https://i.stack.imgur.com/EfQak.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EfQak.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/iqDWO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iqDWO.png"" alt=""enter image description here"" /></a></p>
<p>What is the reason for this behaviour and how to prevent this?</p>
<p>The code:</p>
<pre><code>#%% Initializing
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras

#%% Defining the series
series = np.array([ 218,  555,  550,  563, 2492, 2848, 4041, 1302, 1040, 1073, 1392, 2093,
 1870, 2328, 2102, 2844, 1730, 2431, 1974, 2450, 1975, 1415, 2568, 2831,
 3011, 2576, 2825, 3327, 3539, 3392, 2949, 3283, 3854, 3918, 2639, 3826,
 3980, 3134, 3997, 2708, 3257, 3435, 3337, 2571, 3370, 4277, 3482, 2804,
 3253, 2979, 2458, 2306, 2482, 3209, 3915, 1292,  931, 2748, 2874, 2089,
 2660, 3205, 3093, 1389,  834, 1914, 2568, 2831, 2129, 3138, 2841, 2318,
 2653, 1598, 1779, 1529, 2190, 2180, 1737, 1845, 2511, 1922, 3679, 3277,
 2633, 2064, 2802, 2853, 2220, 1987, 2491, 1867, 3593, 1998, 2425, 3226,
 2143, 3466, 3327, 3283, 3011, 2552, 2844, 2501, 1575, 1829, 3086, 3345,
 1905, 1192, 2772, 3667, 4223, 4117, 2113, 2312, 2615, 3126, 2581, 3265,
 3682, 3355, 1820, 2989, 2806, 3333, 2395, 2777, 2189, 2628, 2379, 1867])


time = range(1, len(series)+1)

#%% Prepare Data for NN 
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  dataset = tf.data.Dataset.from_tensor_slices(series)
  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-4], window[-1])) # :-4 because we want to predict the value in 4 weeks, not for the next week
  dataset = dataset.batch(batch_size).prefetch(1)
  return dataset

split_time = len(time)-16
time_train = time[:split_time]
x_train = series[:split_time]

# For normalization based on training data, so that model does not &quot;see&quot; the validation data before validation
train_mean = x_train.mean()
train_std = x_train.std()

x_train_norm = (x_train - train_mean) / train_std
series_norm = (series - train_mean) / train_std


time_valid = time[split_time:]
x_valid = series[split_time:]


def plot_series(time, series, format=&quot;-&quot;, start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel(&quot;Time&quot;)
    plt.ylabel(&quot;Value&quot;)
    plt.grid(True)

plt.figure(figsize=(10, 6))
plot_series(time_train, x_train)
plt.show()

plt.figure(figsize=(10, 6))
plot_series(time_valid, x_valid)
plt.show()

#%% Build the dataset
w_size = 4
b_size = 4
sb_size = 1
w_size_adjusted = w_size - 3

dataset = windowed_dataset(series_norm, window_size = w_size, batch_size = b_size, shuffle_buffer = sb_size)

#%% Build and train the model


l0 = tf.keras.layers.Dense(1, input_shape=[w_size_adjusted])
model = tf.keras.models.Sequential([l0])

model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9)) # original: learning_rat = 1e-6, also trid [... ].optimizers.Adam(learning_rate=1e-6)
model.fit(dataset,epochs=100,verbose=0)

#%% Forecast data
forecast = []

for time_step in range(len(series) - w_size_adjusted +1):
  forecast.append(model.predict(series_norm[time_step:time_step + w_size_adjusted][np.newaxis]))

forecast_subset = forecast[split_time - w_size_adjusted + 1:]
results_norm = np.array(forecast_subset)[:, 0, 0]

#%% Unnormalize

results = (results_norm * train_std)+train_mean


#%% Plot

plt.figure(figsize=(10, 6))

plot_series(time_valid, x_valid)
plot_series(time_valid, results)
plt.title('window_size: {0}, batch_size: {1}, shuffle_buffer: {2}'.format(w_size, b_size, sb_size))
</code></pre>
",39,1,1,3,python;tensorflow;machine-learning,2022-07-07 16:15:37,2022-07-07 16:15:37,2022-07-07 17:01:50,i m new to machine learning and neural nets and am experimenting with a configuration i found in a   i try to build a model to predict the value  time steps ahead of the last value fed to the model as a feature  window size is flexibel   e g  i give the model weeks   and want to get a prediction for week   currently i run into the challenge that the model is quite unstable  with this i mean that the predictions differ quite a lot if repeating the steps of data preparation  building a dataset  building and training the model and finally doing the forecasting  sometimes the forecast seems to fit quite nice  sometimes it seems to be inverted via a negativ sign and sometimes just the amplitude doesn t match while the directions are right  see the following screenshots   i thought that might has to something with the shuffling and size of shuffle_buffer  but this also happens for different sizes of shuffle_buffer  e g    what is the reason for this behaviour and how to prevent this  the code ,how to stabilize nn model for timeseries prediction,machine learning neural nets experimenting configuration found try build model predict value time steps ahead last value fed model feature window size flexibel e g give model weeks want get prediction week currently run challenge model quite unstable mean predictions differ quite lot repeating steps data preparation building dataset building training model finally forecasting sometimes forecast seems fit quite nice sometimes seems inverted via negativ sign sometimes amplitude match directions right see following screenshots thought might something shuffling size shuffle_buffer also happens different sizes shuffle_buffer e g reason behaviour prevent code,stabilize nn model timeseries prediction,stabilize nn model timeseries predictionmachine learning neural nets experimenting configuration found try build model predict value time steps ahead last value fed model feature window size flexibel e g give model weeks want get prediction week currently run challenge model quite unstable mean predictions differ quite lot repeating steps data preparation building dataset building training model finally forecasting sometimes forecast seems fit quite nice sometimes seems inverted via negativ sign sometimes amplitude match directions right see following screenshots thought might something shuffling size shuffle_buffer also happens different sizes shuffle_buffer e g reason behaviour prevent code,"['stabilize', 'nn', 'model', 'timeseries', 'predictionmachine', 'learning', 'neural', 'nets', 'experimenting', 'configuration', 'found', 'try', 'build', 'model', 'predict', 'value', 'time', 'steps', 'ahead', 'last', 'value', 'fed', 'model', 'feature', 'window', 'size', 'flexibel', 'e', 'g', 'give', 'model', 'weeks', 'want', 'get', 'prediction', 'week', 'currently', 'run', 'challenge', 'model', 'quite', 'unstable', 'mean', 'predictions', 'differ', 'quite', 'lot', 'repeating', 'steps', 'data', 'preparation', 'building', 'dataset', 'building', 'training', 'model', 'finally', 'forecasting', 'sometimes', 'forecast', 'seems', 'fit', 'quite', 'nice', 'sometimes', 'seems', 'inverted', 'via', 'negativ', 'sign', 'sometimes', 'amplitude', 'match', 'directions', 'right', 'see', 'following', 'screenshots', 'thought', 'might', 'something', 'shuffling', 'size', 'shuffle_buffer', 'also', 'happens', 'different', 'sizes', 'shuffle_buffer', 'e', 'g', 'reason', 'behaviour', 'prevent', 'code']","['stabil', 'nn', 'model', 'timeseri', 'predictionmachin', 'learn', 'neural', 'net', 'experi', 'configur', 'found', 'tri', 'build', 'model', 'predict', 'valu', 'time', 'step', 'ahead', 'last', 'valu', 'fed', 'model', 'featur', 'window', 'size', 'flexibel', 'e', 'g', 'give', 'model', 'week', 'want', 'get', 'predict', 'week', 'current', 'run', 'challeng', 'model', 'quit', 'unstabl', 'mean', 'predict', 'differ', 'quit', 'lot', 'repeat', 'step', 'data', 'prepar', 'build', 'dataset', 'build', 'train', 'model', 'final', 'forecast', 'sometim', 'forecast', 'seem', 'fit', 'quit', 'nice', 'sometim', 'seem', 'invert', 'via', 'negativ', 'sign', 'sometim', 'amplitud', 'match', 'direct', 'right', 'see', 'follow', 'screenshot', 'thought', 'might', 'someth', 'shuffl', 'size', 'shuffle_buff', 'also', 'happen', 'differ', 'size', 'shuffle_buff', 'e', 'g', 'reason', 'behaviour', 'prevent', 'code']"
98,105,105,15393588,72896025,How do researchers calculate the differentiation of the energy function in the conference/journal papers?,"<p>I am a computer vision/graphics guy. Although nowadays machine learning solutions dominate the mainstream methods, sometimes we need to deal with classical solutions. For example, SLAM (simultaneous localization and mapping) and accurate 3D reconstructions.</p>
<p>When implementing conference papers such as CVPR (Computer Vision and Pattern Recognition) and SIGGRAPH, the biggest trouble for me is to calculate the derivatives of the energy function while the number of variables may be thousands.</p>
<p>I know analytical differentiation is a standard method to calculate the derivatives, but the energy functions can be very complex for programmers without strong math background to adopt analytical differentiation.</p>
<p>I was using numerical differentiation to calculate the derivatives of the energy functions. Numerical differentiation is not accurate but easy to implement.<br />
Sometimes you have to build a custom system while the existing software doesn't meet the requirement. I wonder how the other programmers and the paper authors implement the derivatives of energy functions. If they manually do the analytical differentiation, that's amazing.</p>
",9,0,0,3,computer-vision;derivative;differentiation,2022-07-07 15:42:07,2022-07-07 15:42:07,2022-07-07 15:42:07,i am a computer vision graphics guy  although nowadays machine learning solutions dominate the mainstream methods  sometimes we need to deal with classical solutions  for example  slam  simultaneous localization and mapping  and accurate d reconstructions  when implementing conference papers such as cvpr  computer vision and pattern recognition  and siggraph  the biggest trouble for me is to calculate the derivatives of the energy function while the number of variables may be thousands  i know analytical differentiation is a standard method to calculate the derivatives  but the energy functions can be very complex for programmers without strong math background to adopt analytical differentiation ,how do researchers calculate the differentiation of the energy function in the conference journal papers ,computer vision graphics guy although nowadays machine learning solutions dominate mainstream methods sometimes need deal classical solutions example slam simultaneous localization mapping accurate reconstructions implementing conference papers cvpr computer vision pattern recognition siggraph biggest trouble calculate derivatives energy function number variables may thousands know analytical differentiation standard method calculate derivatives energy functions complex programmers without strong math background adopt analytical differentiation,researchers calculate differentiation energy function conference journal papers,researchers calculate differentiation energy function conference journal paperscomputer vision graphics guy although nowadays machine learning solutions dominate mainstream methods sometimes need deal classical solutions example slam simultaneous localization mapping accurate reconstructions implementing conference papers cvpr computer vision pattern recognition siggraph biggest trouble calculate derivatives energy function number variables may thousands know analytical differentiation standard method calculate derivatives energy functions complex programmers without strong math background adopt analytical differentiation,"['researchers', 'calculate', 'differentiation', 'energy', 'function', 'conference', 'journal', 'paperscomputer', 'vision', 'graphics', 'guy', 'although', 'nowadays', 'machine', 'learning', 'solutions', 'dominate', 'mainstream', 'methods', 'sometimes', 'need', 'deal', 'classical', 'solutions', 'example', 'slam', 'simultaneous', 'localization', 'mapping', 'accurate', 'reconstructions', 'implementing', 'conference', 'papers', 'cvpr', 'computer', 'vision', 'pattern', 'recognition', 'siggraph', 'biggest', 'trouble', 'calculate', 'derivatives', 'energy', 'function', 'number', 'variables', 'may', 'thousands', 'know', 'analytical', 'differentiation', 'standard', 'method', 'calculate', 'derivatives', 'energy', 'functions', 'complex', 'programmers', 'without', 'strong', 'math', 'background', 'adopt', 'analytical', 'differentiation']","['research', 'calcul', 'differenti', 'energi', 'function', 'confer', 'journal', 'paperscomput', 'vision', 'graphic', 'guy', 'although', 'nowaday', 'machin', 'learn', 'solut', 'domin', 'mainstream', 'method', 'sometim', 'need', 'deal', 'classic', 'solut', 'exampl', 'slam', 'simultan', 'local', 'map', 'accur', 'reconstruct', 'implement', 'confer', 'paper', 'cvpr', 'comput', 'vision', 'pattern', 'recognit', 'siggraph', 'biggest', 'troubl', 'calcul', 'deriv', 'energi', 'function', 'number', 'variabl', 'may', 'thousand', 'know', 'analyt', 'differenti', 'standard', 'method', 'calcul', 'deriv', 'energi', 'function', 'complex', 'programm', 'without', 'strong', 'math', 'background', 'adopt', 'analyt', 'differenti']"
99,106,106,17765060,72893883,Is regularization in machine learning and deep learning same,"<p>As there are L1 , L2 , etc out and other technique are those all same for machine learning and deep learning while using Ml algorithm and DL algorithm</p>
",23,0,-1,2,machine-learning;deep-learning,2022-07-07 13:01:22,2022-07-07 13:01:22,2022-07-07 13:20:39,as there are l   l   etc out and other technique are those all same for machine learning and deep learning while using ml algorithm and dl algorithm,is regularization in machine learning and deep learning same,l l etc technique machine learning deep learning using ml algorithm dl algorithm,regularization machine learning deep learning,regularization machine learning deep learningl l etc technique machine learning deep learning using ml algorithm dl algorithm,"['regularization', 'machine', 'learning', 'deep', 'learningl', 'l', 'etc', 'technique', 'machine', 'learning', 'deep', 'learning', 'using', 'ml', 'algorithm', 'dl', 'algorithm']","['regular', 'machin', 'learn', 'deep', 'learningl', 'l', 'etc', 'techniqu', 'machin', 'learn', 'deep', 'learn', 'use', 'ml', 'algorithm', 'dl', 'algorithm']"
100,107,107,19338925,72889705,supervised machine learning prediction datatype,"<p>/* how can one set the datatype of the label column while using classifier to make prediction. I have a dataset whose label's column datatype is of int64 but I want my model to predict values in object datatype. I have tried to change the label columns datatype from int64 to object datatype by using astype() but when I make predictions my model returns me values in int64 and not in object datatype.  */</p>
<pre><code>convert_dict = {'Outcome': object
                
             }
  
df1 = df1.astype(convert_dict)
print(df1.dtypes)
</code></pre>
<p>/* following is the result*/</p>
<pre><code>Pregnancies                   int64
Glucose                       int64
BloodPressure                 int64
SkinThickness               float64
Insulin                       int64
BMI                         float64
DiabetesPedigreeFunction    float64
Age                           int64
Outcome                      object
dtype: object
       
</code></pre>
<p>/* but when I check the datatype of my predicted value after executing astype() I still get int64 datatype. */</p>
<pre><code>prediction=LogReg.predict([[0,13,40,35,168,43.1,2.288,33]])
print(prediction)
prediction.dtype
</code></pre>
<p>/* following is the result*/</p>
<pre><code>[0]
dtype('int64')
</code></pre>
<p>/* I want this predicted value to be object datatype every time I made a prediction. am I doing something wrong here. please guide*/</p>
",18,0,0,1,types,2022-07-07 02:20:38,2022-07-07 02:20:38,2022-07-07 12:27:54,   how can one set the datatype of the label column while using classifier to make prediction  i have a dataset whose label s column datatype is of int but i want my model to predict values in object datatype  i have tried to change the label columns datatype from int to object datatype by using astype   but when i make predictions my model returns me values in int and not in object datatype         following is the result      but when i check the datatype of my predicted value after executing astype   i still get int datatype        following is the result      i want this predicted value to be object datatype every time i made a prediction  am i doing something wrong here  please guide  ,supervised machine learning prediction datatype,one set datatype label column using classifier make prediction dataset whose label column datatype int want model predict values object datatype tried change label columns datatype int object datatype using astype make predictions model returns values int object datatype following result check datatype predicted value executing astype still get int datatype following result want predicted value object datatype every time made prediction something wrong please guide,supervised machine learning prediction datatype,supervised machine learning prediction datatypeone set datatype label column using classifier make prediction dataset whose label column datatype int want model predict values object datatype tried change label columns datatype int object datatype using astype make predictions model returns values int object datatype following result check datatype predicted value executing astype still get int datatype following result want predicted value object datatype every time made prediction something wrong please guide,"['supervised', 'machine', 'learning', 'prediction', 'datatypeone', 'set', 'datatype', 'label', 'column', 'using', 'classifier', 'make', 'prediction', 'dataset', 'whose', 'label', 'column', 'datatype', 'int', 'want', 'model', 'predict', 'values', 'object', 'datatype', 'tried', 'change', 'label', 'columns', 'datatype', 'int', 'object', 'datatype', 'using', 'astype', 'make', 'predictions', 'model', 'returns', 'values', 'int', 'object', 'datatype', 'following', 'result', 'check', 'datatype', 'predicted', 'value', 'executing', 'astype', 'still', 'get', 'int', 'datatype', 'following', 'result', 'want', 'predicted', 'value', 'object', 'datatype', 'every', 'time', 'made', 'prediction', 'something', 'wrong', 'please', 'guide']","['supervis', 'machin', 'learn', 'predict', 'datatypeon', 'set', 'datatyp', 'label', 'column', 'use', 'classifi', 'make', 'predict', 'dataset', 'whose', 'label', 'column', 'datatyp', 'int', 'want', 'model', 'predict', 'valu', 'object', 'datatyp', 'tri', 'chang', 'label', 'column', 'datatyp', 'int', 'object', 'datatyp', 'use', 'astyp', 'make', 'predict', 'model', 'return', 'valu', 'int', 'object', 'datatyp', 'follow', 'result', 'check', 'datatyp', 'predict', 'valu', 'execut', 'astyp', 'still', 'get', 'int', 'datatyp', 'follow', 'result', 'want', 'predict', 'valu', 'object', 'datatyp', 'everi', 'time', 'made', 'predict', 'someth', 'wrong', 'pleas', 'guid']"
101,108,108,17668281,72814539,Detectron2 - Same Code&amp;Data // Different platforms // highly divergent results,"<p>I use different hardware to benchmark multiple possibilites. The Code runs in a jupyter Notebook.</p>
<p>When i evaluate the different losses i get highly divergent results.</p>
<p>I also checked the full .cfg with <code>cfg.dump()</code> - it is completely consistent.</p>
<p><strong>Detectron2 Parameters:</strong></p>
<pre><code>cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-Detection/retinanet_R_101_FPN_3x.yaml&quot;))
cfg.DATASETS.TRAIN = (&quot;dataset_train&quot;,)
cfg.DATASETS.TEST = (&quot;dataset_test&quot;,)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-Detection/retinanet_R_101_FPN_3x.yaml&quot;)  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025  # 0.00125 pick a good LR
cfg.SOLVER.MAX_ITER = 1200    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)
#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 25  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
cfg.MODEL.RETINANET.NUM_CLASSES = 3
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.
cfg.OUTPUT_DIR = &quot;/content/drive/MyDrive/Colab_Notebooks/testrun/output&quot;
cfg.TEST.EVAL_PERIOD = 25
cfg.SEED=5
</code></pre>
<hr />
<p><strong>1. Environment: Azure</strong></p>
<pre><code>Microsoft Azure - Machine Learning
STANDARD_NC6
Torch: 1.9.0+cu111
</code></pre>
<p><em><strong>Results</strong></em>:</p>
<p><a href=""https://i.stack.imgur.com/oKXID.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oKXID.png"" alt=""Results Azure"" /></a></p>
<p><em><strong>Training Log:</strong></em> <a href=""https://pastebin.com/rBv2PQqY"" rel=""nofollow noreferrer"">Log Azure</a></p>
<hr />
<p><strong>2. Environment: Colab</strong></p>
<pre><code>GoogleColab free

Torch: 1.9.0+cu111 
</code></pre>
<p><em><strong>Results</strong></em>:</p>
<p><a href=""https://i.stack.imgur.com/uYQ6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uYQ6y.png"" alt=""Results GoogleColab"" /></a></p>
<p><em><strong>Training Log:</strong></em> <a href=""https://pastebin.com/egVVUupP"" rel=""nofollow noreferrer"">Log Colab</a></p>
<hr />
<p><strong>EDIT:</strong></p>
<p><strong>3. Environment: Ubuntu</strong></p>
<pre><code>Ubuntu 22.04
RTX 3080
Torch: 1.9.0+cu111
</code></pre>
<p><em><strong>Results:</strong></em></p>
<p><a href=""https://i.stack.imgur.com/SzrG6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SzrG6.jpg"" alt=""enter image description here"" /></a></p>
<p><em><strong>Training Log:</strong></em> <a href=""https://pastebin.com/PwXMz4hY"" rel=""nofollow noreferrer"">https://pastebin.com/PwXMz4hY</a></p>
<hr />
<p><strong>New dataset</strong></p>
<p>Issue is not reproducible with a larger dataset:</p>
<p><a href=""https://i.stack.imgur.com/2IFaR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2IFaR.png"" alt=""enter image description here"" /></a></p>
",70,0,3,5,machine-learning;computer-vision;conv-neural-network;object-detection;detectron,2022-06-30 16:22:53,2022-06-30 16:22:53,2022-07-07 12:19:54,i use different hardware to benchmark multiple possibilites  the code runs in a jupyter notebook  when i evaluate the different losses i get highly divergent results  i also checked the full  cfg with cfg dump     it is completely consistent  detectron parameters    environment  azure results   training log     environment  colab results   training log   edit    environment  ubuntu results   training log   new dataset issue is not reproducible with a larger dataset  ,detectron   same code amp data    different platforms    highly divergent results,use different hardware benchmark multiple possibilites code runs jupyter notebook evaluate different losses get highly divergent results also checked full cfg cfg dump completely consistent detectron parameters environment azure results training log environment colab results training log edit environment ubuntu results training log dataset issue reproducible larger dataset,detectron code amp data different platforms highly divergent results,detectron code amp data different platforms highly divergent resultsuse different hardware benchmark multiple possibilites code runs jupyter notebook evaluate different losses get highly divergent results also checked full cfg cfg dump completely consistent detectron parameters environment azure results training log environment colab results training log edit environment ubuntu results training log dataset issue reproducible larger dataset,"['detectron', 'code', 'amp', 'data', 'different', 'platforms', 'highly', 'divergent', 'resultsuse', 'different', 'hardware', 'benchmark', 'multiple', 'possibilites', 'code', 'runs', 'jupyter', 'notebook', 'evaluate', 'different', 'losses', 'get', 'highly', 'divergent', 'results', 'also', 'checked', 'full', 'cfg', 'cfg', 'dump', 'completely', 'consistent', 'detectron', 'parameters', 'environment', 'azure', 'results', 'training', 'log', 'environment', 'colab', 'results', 'training', 'log', 'edit', 'environment', 'ubuntu', 'results', 'training', 'log', 'dataset', 'issue', 'reproducible', 'larger', 'dataset']","['detectron', 'code', 'amp', 'data', 'differ', 'platform', 'highli', 'diverg', 'resultsus', 'differ', 'hardwar', 'benchmark', 'multipl', 'possibilit', 'code', 'run', 'jupyt', 'notebook', 'evalu', 'differ', 'loss', 'get', 'highli', 'diverg', 'result', 'also', 'check', 'full', 'cfg', 'cfg', 'dump', 'complet', 'consist', 'detectron', 'paramet', 'environ', 'azur', 'result', 'train', 'log', 'environ', 'colab', 'result', 'train', 'log', 'edit', 'environ', 'ubuntu', 'result', 'train', 'log', 'dataset', 'issu', 'reproduc', 'larger', 'dataset']"
102,109,109,19332126,72890588,extract rectangular area from noisy image,"<p>extract rectangular area from noisy image</p>
<p>I want to get the clean figure (right) from the noisy figure (left):</p>
<p><a href=""https://i.stack.imgur.com/cTDWN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cTDWN.png"" alt="""" /></a></p>
<p>To do this, I have a dataset with the noisy figure and its corresponding clean figures. When I put the algorithm into a noisy figure, I want it to estimate the clean figure. Which machine learning algorithm should I use to achieve this?</p>
<p>I know I will use Python, but I am unsure how to input this dataset and to which algorithm? I would appreciate any help provided.</p>
",20,1,-1,2,python;machine-learning,2022-07-07 04:14:34,2022-07-07 04:14:34,2022-07-07 04:21:27,extract rectangular area from noisy image i want to get the clean figure  right  from the noisy figure  left    to do this  i have a dataset with the noisy figure and its corresponding clean figures  when i put the algorithm into a noisy figure  i want it to estimate the clean figure  which machine learning algorithm should i use to achieve this  i know i will use python  but i am unsure how to input this dataset and to which algorithm  i would appreciate any help provided ,extract rectangular area from noisy image,extract rectangular area noisy image want get clean figure right noisy figure left dataset noisy figure corresponding clean figures put algorithm noisy figure want estimate clean figure machine learning algorithm use achieve know use python unsure input dataset algorithm would appreciate help provided,extract rectangular area noisy image,extract rectangular area noisy imageextract rectangular area noisy image want get clean figure right noisy figure left dataset noisy figure corresponding clean figures put algorithm noisy figure want estimate clean figure machine learning algorithm use achieve know use python unsure input dataset algorithm would appreciate help provided,"['extract', 'rectangular', 'area', 'noisy', 'imageextract', 'rectangular', 'area', 'noisy', 'image', 'want', 'get', 'clean', 'figure', 'right', 'noisy', 'figure', 'left', 'dataset', 'noisy', 'figure', 'corresponding', 'clean', 'figures', 'put', 'algorithm', 'noisy', 'figure', 'want', 'estimate', 'clean', 'figure', 'machine', 'learning', 'algorithm', 'use', 'achieve', 'know', 'use', 'python', 'unsure', 'input', 'dataset', 'algorithm', 'would', 'appreciate', 'help', 'provided']","['extract', 'rectangular', 'area', 'noisi', 'imageextract', 'rectangular', 'area', 'noisi', 'imag', 'want', 'get', 'clean', 'figur', 'right', 'noisi', 'figur', 'left', 'dataset', 'noisi', 'figur', 'correspond', 'clean', 'figur', 'put', 'algorithm', 'noisi', 'figur', 'want', 'estim', 'clean', 'figur', 'machin', 'learn', 'algorithm', 'use', 'achiev', 'know', 'use', 'python', 'unsur', 'input', 'dataset', 'algorithm', 'would', 'appreci', 'help', 'provid']"
103,110,110,13200965,72876754,It it possible to have data leakage from the training set to the test set in a Machine Learning data preprocessing?,"<p>I'm building this flowchart for a machine learning pipeline and during the data preprocessing I found myself wondering if is it possible to have data leakage (besides from test to train) also from train to test. I'd appreciate if someone could analyze my flowchart and say what you think.</p>
<p><a href=""https://i.stack.imgur.com/60jFu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/60jFu.png"" alt=""enter image description here"" /></a></p>
",18,0,0,2,machine-learning;preprocessor,2022-07-06 05:50:23,2022-07-06 05:50:23,2022-07-07 04:03:27,i m building this flowchart for a machine learning pipeline and during the data preprocessing i found myself wondering if is it possible to have data leakage  besides from test to train  also from train to test  i d appreciate if someone could analyze my flowchart and say what you think  ,it it possible to have data leakage from the training set to the test set in a machine learning data preprocessing ,building flowchart machine learning pipeline data preprocessing found wondering possible data leakage besides test train also train test appreciate someone could analyze flowchart say think,possible data leakage training set test set machine learning data preprocessing,possible data leakage training set test set machine learning data preprocessingbuilding flowchart machine learning pipeline data preprocessing found wondering possible data leakage besides test train also train test appreciate someone could analyze flowchart say think,"['possible', 'data', 'leakage', 'training', 'set', 'test', 'set', 'machine', 'learning', 'data', 'preprocessingbuilding', 'flowchart', 'machine', 'learning', 'pipeline', 'data', 'preprocessing', 'found', 'wondering', 'possible', 'data', 'leakage', 'besides', 'test', 'train', 'also', 'train', 'test', 'appreciate', 'someone', 'could', 'analyze', 'flowchart', 'say', 'think']","['possibl', 'data', 'leakag', 'train', 'set', 'test', 'set', 'machin', 'learn', 'data', 'preprocessingbuild', 'flowchart', 'machin', 'learn', 'pipelin', 'data', 'preprocess', 'found', 'wonder', 'possibl', 'data', 'leakag', 'besid', 'test', 'train', 'also', 'train', 'test', 'appreci', 'someon', 'could', 'analyz', 'flowchart', 'say', 'think']"
104,111,111,10755032,72884667,ML - How to avoid estimating prices that are more than 25 dollars off of the actual price in Machine Learning model?,"<p>I am currently working on a case study where I have to estimate how much a person makes by giving their property for rent. They provided me with a constraint which is as follows:</p>
<blockquote>
<p>&quot;avoid estimating prices that are more than 25 dollars off of the
actual price&quot;</p>
</blockquote>
<p>At first, I tried modeling without considering the constraint but failed miserably since the score I was getting is around 0.25. I did the training by using RandomForestRegressor along with RandomizedSearchCV. I used <code>.score</code> at last which gave me 0.25</p>
<p>So, I guess that constraint should be implemented for sure. As I am somewhat of a novice, I did not come across such a case before, therefore having no idea how to approach it.</p>
<p>The dataset I am using is: <a href=""https://www.kaggle.com/datasets/karthikbhandary2/property-rentals"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/karthikbhandary2/property-rentals</a></p>
<p>For the sake of context I am sharing with you the full details of the case study:</p>
<blockquote>
<p>You have been hired by Inn the Neighborhood, an online platform that
allows people to rent out their properties for short stays. Currently,
the webpage for renters has a conversion rate of 2%. This means that
most people leave the platform without signing up.</p>
<p>The product manager would like to increase this conversion rate. They
are interested in developing an application to help people estimate
the money they could earn renting out their living space. They hope
that this would make people more likely to sign up.</p>
<p>The company has provided you with a dataset that includes details
about each property rented, as well as the price charged per night.
<strong>They want to avoid estimating prices that are more than 25 dollars off
of the actual price</strong>, as this may discourage people.</p>
</blockquote>
",42,0,-3,3,python;machine-learning;prediction,2022-07-06 19:14:09,2022-07-06 19:14:09,2022-07-06 22:00:38,i am currently working on a case study where i have to estimate how much a person makes by giving their property for rent  they provided me with a constraint which is as follows  at first  i tried modeling without considering the constraint but failed miserably since the score i was getting is around    i did the training by using randomforestregressor along with randomizedsearchcv  i used  score at last which gave me   so  i guess that constraint should be implemented for sure  as i am somewhat of a novice  i did not come across such a case before  therefore having no idea how to approach it  the dataset i am using is   for the sake of context i am sharing with you the full details of the case study ,ml   how to avoid estimating prices that are more than  dollars off of the actual price in machine learning model ,currently working case study estimate much person makes giving property rent provided constraint follows first tried modeling without considering constraint failed miserably since score getting around training using randomforestregressor along randomizedsearchcv used score last gave guess constraint implemented sure somewhat novice come across case therefore idea approach dataset using sake context sharing full details case study,ml avoid estimating prices dollars actual price machine learning model,ml avoid estimating prices dollars actual price machine learning modelcurrently working case study estimate much person makes giving property rent provided constraint follows first tried modeling without considering constraint failed miserably since score getting around training using randomforestregressor along randomizedsearchcv used score last gave guess constraint implemented sure somewhat novice come across case therefore idea approach dataset using sake context sharing full details case study,"['ml', 'avoid', 'estimating', 'prices', 'dollars', 'actual', 'price', 'machine', 'learning', 'modelcurrently', 'working', 'case', 'study', 'estimate', 'much', 'person', 'makes', 'giving', 'property', 'rent', 'provided', 'constraint', 'follows', 'first', 'tried', 'modeling', 'without', 'considering', 'constraint', 'failed', 'miserably', 'since', 'score', 'getting', 'around', 'training', 'using', 'randomforestregressor', 'along', 'randomizedsearchcv', 'used', 'score', 'last', 'gave', 'guess', 'constraint', 'implemented', 'sure', 'somewhat', 'novice', 'come', 'across', 'case', 'therefore', 'idea', 'approach', 'dataset', 'using', 'sake', 'context', 'sharing', 'full', 'details', 'case', 'study']","['ml', 'avoid', 'estim', 'price', 'dollar', 'actual', 'price', 'machin', 'learn', 'modelcurr', 'work', 'case', 'studi', 'estim', 'much', 'person', 'make', 'give', 'properti', 'rent', 'provid', 'constraint', 'follow', 'first', 'tri', 'model', 'without', 'consid', 'constraint', 'fail', 'miser', 'sinc', 'score', 'get', 'around', 'train', 'use', 'randomforestregressor', 'along', 'randomizedsearchcv', 'use', 'score', 'last', 'gave', 'guess', 'constraint', 'implement', 'sure', 'somewhat', 'novic', 'come', 'across', 'case', 'therefor', 'idea', 'approach', 'dataset', 'use', 'sake', 'context', 'share', 'full', 'detail', 'case', 'studi']"
105,112,112,18021400,71930838,Protect python source code that runs as API,"<p>The company has built a python API with Machine Learning modules and we want to install this API on our customers' local server.</p>
<p>Of course, we don't want them to read the code, but when I try to use pyinstaller or pyarmor the fastAPI server can't understand the code anymore.</p>
<p>Is there a way to obfuscate or compile the python code and make it work with a server like fastapi with the uvicorn command?</p>
",89,0,0,5,python;api;compilation;obfuscation;fastapi,2022-04-20 01:46:42,2022-04-20 01:46:42,2022-07-06 21:53:42,the company has built a python api with machine learning modules and we want to install this api on our customers  local server  of course  we don t want them to read the code  but when i try to use pyinstaller or pyarmor the fastapi server can t understand the code anymore  is there a way to obfuscate or compile the python code and make it work with a server like fastapi with the uvicorn command ,protect python source code that runs as api,company built python api machine learning modules want install api customers local server course want read code try use pyinstaller pyarmor fastapi server understand code anymore way obfuscate compile python code make work server like fastapi uvicorn command,protect python source code runs api,protect python source code runs apicompany built python api machine learning modules want install api customers local server course want read code try use pyinstaller pyarmor fastapi server understand code anymore way obfuscate compile python code make work server like fastapi uvicorn command,"['protect', 'python', 'source', 'code', 'runs', 'apicompany', 'built', 'python', 'api', 'machine', 'learning', 'modules', 'want', 'install', 'api', 'customers', 'local', 'server', 'course', 'want', 'read', 'code', 'try', 'use', 'pyinstaller', 'pyarmor', 'fastapi', 'server', 'understand', 'code', 'anymore', 'way', 'obfuscate', 'compile', 'python', 'code', 'make', 'work', 'server', 'like', 'fastapi', 'uvicorn', 'command']","['protect', 'python', 'sourc', 'code', 'run', 'apicompani', 'built', 'python', 'api', 'machin', 'learn', 'modul', 'want', 'instal', 'api', 'custom', 'local', 'server', 'cours', 'want', 'read', 'code', 'tri', 'use', 'pyinstal', 'pyarmor', 'fastapi', 'server', 'understand', 'code', 'anymor', 'way', 'obfusc', 'compil', 'python', 'code', 'make', 'work', 'server', 'like', 'fastapi', 'uvicorn', 'command']"
106,113,113,18216019,72886394,Making a AI that guess the number that the user is thinking python,"<p>The goal is that the user enters a number between 1-10 and AI tries to figure out what number the player picked.
For example I take the number: 6 and AI then tries to find out which number with 3 attempts.</p>
<p>AI learns which numbers are frequently chosen and tries them.</p>
<p>The problem is. I'm new to machine learning and thought this would be a good project to learn about machine learning. I can't find such a project anywhere and I don't know where to start. I've already asked this question on other platforms but it looks like nobody can help me.</p>
<p>I thought I use python, keras and tensorflow for that</p>
<p><strong>Has anyone done this before or know where it is?</strong></p>
<p>Thanks :)</p>
",41,1,-1,2,python;artificial-intelligence,2022-07-06 21:09:09,2022-07-06 21:09:09,2022-07-06 21:34:10,ai learns which numbers are frequently chosen and tries them  the problem is  i m new to machine learning and thought this would be a good project to learn about machine learning  i can t find such a project anywhere and i don t know where to start  i ve already asked this question on other platforms but it looks like nobody can help me  i thought i use python  keras and tensorflow for that has anyone done this before or know where it is  thanks   ,making a ai that guess the number that the user is thinking python,ai learns numbers frequently chosen tries problem machine learning thought would good project learn machine learning find project anywhere know start already asked question platforms looks like nobody help thought use python keras tensorflow anyone done know thanks,making ai guess number user thinking python,making ai guess number user thinking pythonai learns numbers frequently chosen tries problem machine learning thought would good project learn machine learning find project anywhere know start already asked question platforms looks like nobody help thought use python keras tensorflow anyone done know thanks,"['making', 'ai', 'guess', 'number', 'user', 'thinking', 'pythonai', 'learns', 'numbers', 'frequently', 'chosen', 'tries', 'problem', 'machine', 'learning', 'thought', 'would', 'good', 'project', 'learn', 'machine', 'learning', 'find', 'project', 'anywhere', 'know', 'start', 'already', 'asked', 'question', 'platforms', 'looks', 'like', 'nobody', 'help', 'thought', 'use', 'python', 'keras', 'tensorflow', 'anyone', 'done', 'know', 'thanks']","['make', 'ai', 'guess', 'number', 'user', 'think', 'pythonai', 'learn', 'number', 'frequent', 'chosen', 'tri', 'problem', 'machin', 'learn', 'thought', 'would', 'good', 'project', 'learn', 'machin', 'learn', 'find', 'project', 'anywher', 'know', 'start', 'alreadi', 'ask', 'question', 'platform', 'look', 'like', 'nobodi', 'help', 'thought', 'use', 'python', 'kera', 'tensorflow', 'anyon', 'done', 'know', 'thank']"
107,114,114,12587364,72882549,Comparing the predicted class for each instance of test data from different models,"<p>My test set data contains about 50,000 instances. I trained different machine learning models. Now I want to do some comparison to see for example if for every instance <code>x_i</code> that model A predicted as 0, models B and C also predicted that instance as 0.</p>
<p>For example, below are the first 5 predictions by the models.</p>
<pre><code>import pandas as pd

data = {'true_class': [3.0, 3.0, 3.0, 3.0, 3.0],
 'rf_pred': [3.0, 0.0, 0.0, 0.0, 0.0],
 'mlp_pred': [3.0, 0.0, 0.0, 0.0, 0.0],
 'knn_pred': [3.0, 0.0, 0.0, 0.0, 0.0],
 'lg_pred': [3.0, 0.0, 0.0, 0.0, 0.0],
 'ada_pred': [2.0, 2.0, 2.0, 2.0, 2.0]}

df = pd.DataFrame(data)
df
 true_class rf_pred mlp_pred knn_pred lg_pred ada_pred
0   3.0     3.0     3.0      3.0      3.0      2.0
1   3.0     0.0     0.0      0.0      0.0      2.0
2   3.0     0.0     0.0      0.0      0.0      2.0
3   3.0     0.0     0.0      0.0      0.0      2.0
4   3.0     0.0     0.0      0.0      0.0      2.0
</code></pre>
<p>Clearly predictions of <code>rf_pred, mlp_pred, knn_pred</code> &amp; <code>lg_pred</code> are the same for these five instances.</p>
<p>Is there any way to perform such analysis, per haps visually?</p>
",37,2,-1,5,python;python-3.x;machine-learning;classification;supervised-learning,2022-07-06 16:40:28,2022-07-06 16:40:28,2022-07-06 20:27:09,my test set data contains about   instances  i trained different machine learning models  now i want to do some comparison to see for example if for every instance x_i that model a predicted as   models b and c also predicted that instance as   for example  below are the first  predictions by the models  clearly predictions of rf_pred  mlp_pred  knn_pred  amp  lg_pred are the same for these five instances  is there any way to perform such analysis  per haps visually ,comparing the predicted class for each instance of test data from different models,test set data contains instances trained different machine learning models want comparison see example every instance x_i model predicted models b c also predicted instance example first predictions models clearly predictions rf_pred mlp_pred knn_pred amp lg_pred five instances way perform analysis per haps visually,comparing predicted class instance test data different models,comparing predicted class instance test data different modelstest set data contains instances trained different machine learning models want comparison see example every instance x_i model predicted models b c also predicted instance example first predictions models clearly predictions rf_pred mlp_pred knn_pred amp lg_pred five instances way perform analysis per haps visually,"['comparing', 'predicted', 'class', 'instance', 'test', 'data', 'different', 'modelstest', 'set', 'data', 'contains', 'instances', 'trained', 'different', 'machine', 'learning', 'models', 'want', 'comparison', 'see', 'example', 'every', 'instance', 'x_i', 'model', 'predicted', 'models', 'b', 'c', 'also', 'predicted', 'instance', 'example', 'first', 'predictions', 'models', 'clearly', 'predictions', 'rf_pred', 'mlp_pred', 'knn_pred', 'amp', 'lg_pred', 'five', 'instances', 'way', 'perform', 'analysis', 'per', 'haps', 'visually']","['compar', 'predict', 'class', 'instanc', 'test', 'data', 'differ', 'modelstest', 'set', 'data', 'contain', 'instanc', 'train', 'differ', 'machin', 'learn', 'model', 'want', 'comparison', 'see', 'exampl', 'everi', 'instanc', 'x_i', 'model', 'predict', 'model', 'b', 'c', 'also', 'predict', 'instanc', 'exampl', 'first', 'predict', 'model', 'clearli', 'predict', 'rf_pred', 'mlp_pred', 'knn_pred', 'amp', 'lg_pred', 'five', 'instanc', 'way', 'perform', 'analysi', 'per', 'hap', 'visual']"
108,115,115,6545834,38179829,How to load a .json file with python nltk,"<p>I'm trying to load a .json file from an output of an application so I can feed it into different machine learning algorithms so I can classify the text, problem is I can't seem to figure out why NLTK is not loading my .json file, even if I try it with their own .json file, it doesn't seem to work. From what I gather based on the book, I should only need to import 'nltk' and I can use the function 'load' from 'nltk.data'. Can somebody help me realise what I am doing wrong?</p>

<p>Below is the code I used to try loading my the file from nltk.</p>

<pre><code>import nltk
nltk.data.load('corpora/twitter_samples/negative_tweets.json')
</code></pre>

<p>After trying that out I got an error from it.</p>

<pre><code>C:\Python34\python.exe ""C:/Users/JarvinLi/PycharmProjects/ThesisTrial1/Trial Loading.py""
Traceback (most recent call last):
   File ""C:/Users/JarvinLi/PycharmProjects/ThesisTrial1/Trial Loading.py"", line 7, in &lt;module&gt;
     nltk.data.load('corpora/twitter_samples/negative_tweets.json')
  File ""C:\Python34\lib\site-packages\nltk\data.py"", line 810, in load
    resource_val = json.load(opened_resource)
  File ""C:\Python34\lib\json\__init__.py"", line 268, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""C:\Python34\lib\json\__init__.py"", line 312, in loads
    s.__class__.__name__))
TypeError: the JSON object must be str, not 'bytes'

Process finished with exit code 1
</code></pre>

<p>EDIT #1 : I'm using Python 3.4.1 and NLTK 3.</p>

<p>EDIT #2 : Below is another try I did but now using json.load()</p>

<pre><code>  import json
  json.load('corpora/twitter_samples/negative_tweets.json')
</code></pre>

<p>But I encountered a similar error</p>

<pre><code>C:\Python34\python.exe ""C:/Users/JarvinLi/PycharmProjects/ThesisTrial1/Trial Loading.py""
Traceback (most recent call last):
  File ""C:/Users/JarvinLi/PycharmProjects/ThesisTrial1/Trial Loading.py"", line 5, in &lt;module&gt;
    json.load('corpora/twitter_samples/quotefileNeg.json')
  File ""C:\Python34\lib\json\__init__.py"", line 265, in load
    return loads(fp.read(),
AttributeError: 'str' object has no attribute 'read'

Process finished with exit code 1
</code></pre>
",2406,1,0,3,python;json;nltk,2016-07-04 13:46:30,2016-07-04 13:46:30,2022-07-06 20:21:24,i m trying to load a  json file from an output of an application so i can feed it into different machine learning algorithms so i can classify the text  problem is i can t seem to figure out why nltk is not loading my  json file  even if i try it with their own  json file  it doesn t seem to work  from what i gather based on the book  i should only need to import  nltk  and i can use the function  load  from  nltk data   can somebody help me realise what i am doing wrong  below is the code i used to try loading my the file from nltk  after trying that out i got an error from it  edit     i m using python    and nltk   edit     below is another try i did but now using json load   but i encountered a similar error,how to load a  json file with python nltk,trying load json file output application feed different machine learning algorithms classify text problem seem figure nltk loading json file even try json file seem work gather based book need import nltk use function load nltk data somebody help realise wrong code used try loading file nltk trying got error edit using python nltk edit another try using json load encountered similar error,load json file python nltk,load json file python nltktrying load json file output application feed different machine learning algorithms classify text problem seem figure nltk loading json file even try json file seem work gather based book need import nltk use function load nltk data somebody help realise wrong code used try loading file nltk trying got error edit using python nltk edit another try using json load encountered similar error,"['load', 'json', 'file', 'python', 'nltktrying', 'load', 'json', 'file', 'output', 'application', 'feed', 'different', 'machine', 'learning', 'algorithms', 'classify', 'text', 'problem', 'seem', 'figure', 'nltk', 'loading', 'json', 'file', 'even', 'try', 'json', 'file', 'seem', 'work', 'gather', 'based', 'book', 'need', 'import', 'nltk', 'use', 'function', 'load', 'nltk', 'data', 'somebody', 'help', 'realise', 'wrong', 'code', 'used', 'try', 'loading', 'file', 'nltk', 'trying', 'got', 'error', 'edit', 'using', 'python', 'nltk', 'edit', 'another', 'try', 'using', 'json', 'load', 'encountered', 'similar', 'error']","['load', 'json', 'file', 'python', 'nltktri', 'load', 'json', 'file', 'output', 'applic', 'feed', 'differ', 'machin', 'learn', 'algorithm', 'classifi', 'text', 'problem', 'seem', 'figur', 'nltk', 'load', 'json', 'file', 'even', 'tri', 'json', 'file', 'seem', 'work', 'gather', 'base', 'book', 'need', 'import', 'nltk', 'use', 'function', 'load', 'nltk', 'data', 'somebodi', 'help', 'realis', 'wrong', 'code', 'use', 'tri', 'load', 'file', 'nltk', 'tri', 'got', 'error', 'edit', 'use', 'python', 'nltk', 'edit', 'anoth', 'tri', 'use', 'json', 'load', 'encount', 'similar', 'error']"
109,116,116,10868740,72884585,How to synchronize production and developement database,"<p>We want to implement a database for development purposes. We are currently developing on the production database (MSSQL 2019).
The application that accesses the database is a data intensive machine Learning process that runs every morning for ~5h. We have the following requirements:</p>
<p>Our desired architecture looks like this:</p>
<p><a href=""https://i.stack.imgur.com/tP6IX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tP6IX.jpg"" alt=""enter image description here"" /></a></p>
<p>We want:</p>
<ul>
<li>To have the customers data on both servers</li>
<li>Maintainers input quick fixes in production, these quick fixes are also applied to the development database</li>
<li>The developers add new tables and add further data to the database. On-Demand they deploy their changes to the production database.</li>
<li>Within both databases we have ETL tasks. We want to test our ETL tasks as well.</li>
</ul>
<p>We tried implementing this with SQL server replication. We transfered the production database with transactional replication to the development database.
However, this lead to key violations because the transactional replication from production and other tasks during developement led to the same primary key.
Is there a way to avoid these violations? Do you have other ideas to implement this?</p>
",33,0,-1,2,sql-server;database,2022-07-06 19:09:29,2022-07-06 19:09:29,2022-07-06 19:09:29,our desired architecture looks like this   we want ,how to synchronize production and developement database,desired architecture looks like want,synchronize production developement database,synchronize production developement databasedesired architecture looks like want,"['synchronize', 'production', 'developement', 'databasedesired', 'architecture', 'looks', 'like', 'want']","['synchron', 'product', 'develop', 'databasedesir', 'architectur', 'look', 'like', 'want']"
110,117,117,14022554,72883416,Elegant way or common practice for passing a large number of parameters and file paths to python scripts?,"<p>At the moment I either use simple command line arguments, when there is only around 3-4 parameters, or I use separate input json files, when the parameter list becomes too bloated. The former seems user friendly when I have to pass the script to people that are unfamiliar with its functions, while the latter is good for reproducibility, as I can just copy the input json into the output folder along with the results. However, non of them feels very professional to me, nor efficient. The command line arguments simply turn chaotic when there are 5 different file locations and 10 different parameters needed (e.g.: for machine learning models), while for the json files, I'm not sure that I can expect non-IT users to edit those for the preferred parameters.</p>
<p>Is there a better way? Something more common practice?</p>
",21,0,0,3,python;json;shell,2022-07-06 17:46:26,2022-07-06 17:46:26,2022-07-06 17:46:26,at the moment i either use simple command line arguments  when there is only around   parameters  or i use separate input json files  when the parameter list becomes too bloated  the former seems user friendly when i have to pass the script to people that are unfamiliar with its functions  while the latter is good for reproducibility  as i can just copy the input json into the output folder along with the results  however  non of them feels very professional to me  nor efficient  the command line arguments simply turn chaotic when there are  different file locations and  different parameters needed  e g   for machine learning models   while for the json files  i m not sure that i can expect non it users to edit those for the preferred parameters  is there a better way  something more common practice ,elegant way or common practice for passing a large number of parameters and file paths to python scripts ,moment either use simple command line arguments around parameters use separate input json files parameter becomes bloated former seems user friendly pass script people unfamiliar functions latter good reproducibility copy input json output folder along results however non feels professional efficient command line arguments simply turn chaotic different file locations different parameters needed e g machine learning models json files sure expect non users edit preferred parameters better way something common practice,elegant way common practice passing large number parameters file paths python scripts,elegant way common practice passing large number parameters file paths python scriptsmoment either use simple command line arguments around parameters use separate input json files parameter becomes bloated former seems user friendly pass script people unfamiliar functions latter good reproducibility copy input json output folder along results however non feels professional efficient command line arguments simply turn chaotic different file locations different parameters needed e g machine learning models json files sure expect non users edit preferred parameters better way something common practice,"['elegant', 'way', 'common', 'practice', 'passing', 'large', 'number', 'parameters', 'file', 'paths', 'python', 'scriptsmoment', 'either', 'use', 'simple', 'command', 'line', 'arguments', 'around', 'parameters', 'use', 'separate', 'input', 'json', 'files', 'parameter', 'becomes', 'bloated', 'former', 'seems', 'user', 'friendly', 'pass', 'script', 'people', 'unfamiliar', 'functions', 'latter', 'good', 'reproducibility', 'copy', 'input', 'json', 'output', 'folder', 'along', 'results', 'however', 'non', 'feels', 'professional', 'efficient', 'command', 'line', 'arguments', 'simply', 'turn', 'chaotic', 'different', 'file', 'locations', 'different', 'parameters', 'needed', 'e', 'g', 'machine', 'learning', 'models', 'json', 'files', 'sure', 'expect', 'non', 'users', 'edit', 'preferred', 'parameters', 'better', 'way', 'something', 'common', 'practice']","['eleg', 'way', 'common', 'practic', 'pass', 'larg', 'number', 'paramet', 'file', 'path', 'python', 'scriptsmoment', 'either', 'use', 'simpl', 'command', 'line', 'argument', 'around', 'paramet', 'use', 'separ', 'input', 'json', 'file', 'paramet', 'becom', 'bloat', 'former', 'seem', 'user', 'friendli', 'pass', 'script', 'peopl', 'unfamiliar', 'function', 'latter', 'good', 'reproduc', 'copi', 'input', 'json', 'output', 'folder', 'along', 'result', 'howev', 'non', 'feel', 'profession', 'effici', 'command', 'line', 'argument', 'simpli', 'turn', 'chaotic', 'differ', 'file', 'locat', 'differ', 'paramet', 'need', 'e', 'g', 'machin', 'learn', 'model', 'json', 'file', 'sure', 'expect', 'non', 'user', 'edit', 'prefer', 'paramet', 'better', 'way', 'someth', 'common', 'practic']"
111,119,119,13165659,72882083,What are drawbacks of serializing custom objects along with their definition?,"<p>My question is what future repercussions are conceivable when I &quot;force&quot; Python class/function definitions to be serialized along with the objects, by &quot;re-declaring&quot; them in <code>__main__</code> just before serialization.</p>
<p><strong>Details</strong></p>
<p>It is a common gotcha that Python libraries such as pickle and dill do not serialize class or function definitions along with the objects, if the definitions are not located in <code>__main__</code>.</p>
<p>As a result, when deserializing an object, its dependencies must be found in the same location as during serialization. This adds some overhead/inflexibility to deployment, as the definitions must be maintained in a separate package which must be versioned and present in the (production) environment.</p>
<p>I sometimes use the workaround of &quot;mainifying&quot; objects before serializing them, as described for instance by Oege Dijk <a href=""https://oegedijk.github.io/blog/pickle/dill/python/2020/11/10/serializing-dill-references.html"" rel=""nofollow noreferrer"">here</a>. It essentially redeclares the object's definition in <code>__main__</code> so that it will be serialized. The code I use is listed below.</p>
<p>So far this approach has worked well for all my (machine learning) workflows, for quite a while. Yet, it seems quite hacky, and I wonder whether it might cause problems down the line, and which. Of course, the ability to easily modify the serialized definitions is removed (e.g. bugfix). But that is something I can live with. Are there other dangers I am unaware of?</p>
<pre><code>import inspect
import types

def mainify(obj):
   
    if obj.__module__ != '__main__':                                                
        
        import __main__       
        is_func = True if isinstance(obj, types.FunctionType) else False                                                            
                                
        # Get source code and compile
        source = inspect.getsource(obj if is_func else obj.__class__)
        compiled = compile(source, '&lt;string&gt;', 'exec')                    

        # &quot;Declare&quot; in __main__ and keep track which key
        # of __main__ dict is new 
        pre = list(__main__.__dict__.keys()) 
        exec(compiled, __main__.__dict__)
        post = list(__main__.__dict__.keys())                        
        new_in_main = list(set(post) - set(pre))[0]
        
        # for function return mainified version, else assign new
        # class to obj and return object
        if is_func:
            obj = __main__.__dict__[new_in_main]            
        else:            
            obj.__class__ = __main__.__dict__[new_in_main]
                
    return obj
</code></pre>
",14,0,0,4,python;serialization;pickle;dill,2022-07-06 16:10:08,2022-07-06 16:10:08,2022-07-06 16:14:11,my question is what future repercussions are conceivable when i  force  python class function definitions to be serialized along with the objects  by  re declaring  them in __main__ just before serialization  details it is a common gotcha that python libraries such as pickle and dill do not serialize class or function definitions along with the objects  if the definitions are not located in __main__  as a result  when deserializing an object  its dependencies must be found in the same location as during serialization  this adds some overhead inflexibility to deployment  as the definitions must be maintained in a separate package which must be versioned and present in the  production  environment  i sometimes use the workaround of  mainifying  objects before serializing them  as described for instance by oege dijk   it essentially redeclares the object s definition in __main__ so that it will be serialized  the code i use is listed below  so far this approach has worked well for all my  machine learning  workflows  for quite a while  yet  it seems quite hacky  and i wonder whether it might cause problems down the line  and which  of course  the ability to easily modify the serialized definitions is removed  e g  bugfix   but that is something i can live with  are there other dangers i am unaware of ,what are drawbacks of serializing custom objects along with their definition ,question future repercussions conceivable force python class function definitions serialized along objects declaring __main__ serialization details common gotcha python libraries pickle dill serialize class function definitions along objects definitions located __main__ result deserializing object dependencies must found location serialization adds overhead inflexibility deployment definitions must maintained separate package must versioned present production environment sometimes use workaround mainifying objects serializing described instance oege dijk essentially redeclares object definition __main__ serialized code use listed far approach worked well machine learning workflows quite yet seems quite hacky wonder whether might cause problems line course ability easily modify serialized definitions removed e g bugfix something live dangers unaware,drawbacks serializing objects along definition,drawbacks serializing objects along definitionquestion future repercussions conceivable force python class function definitions serialized along objects declaring __main__ serialization details common gotcha python libraries pickle dill serialize class function definitions along objects definitions located __main__ result deserializing object dependencies must found location serialization adds overhead inflexibility deployment definitions must maintained separate package must versioned present production environment sometimes use workaround mainifying objects serializing described instance oege dijk essentially redeclares object definition __main__ serialized code use listed far approach worked well machine learning workflows quite yet seems quite hacky wonder whether might cause problems line course ability easily modify serialized definitions removed e g bugfix something live dangers unaware,"['drawbacks', 'serializing', 'objects', 'along', 'definitionquestion', 'future', 'repercussions', 'conceivable', 'force', 'python', 'class', 'function', 'definitions', 'serialized', 'along', 'objects', 'declaring', '__main__', 'serialization', 'details', 'common', 'gotcha', 'python', 'libraries', 'pickle', 'dill', 'serialize', 'class', 'function', 'definitions', 'along', 'objects', 'definitions', 'located', '__main__', 'result', 'deserializing', 'object', 'dependencies', 'must', 'found', 'location', 'serialization', 'adds', 'overhead', 'inflexibility', 'deployment', 'definitions', 'must', 'maintained', 'separate', 'package', 'must', 'versioned', 'present', 'production', 'environment', 'sometimes', 'use', 'workaround', 'mainifying', 'objects', 'serializing', 'described', 'instance', 'oege', 'dijk', 'essentially', 'redeclares', 'object', 'definition', '__main__', 'serialized', 'code', 'use', 'listed', 'far', 'approach', 'worked', 'well', 'machine', 'learning', 'workflows', 'quite', 'yet', 'seems', 'quite', 'hacky', 'wonder', 'whether', 'might', 'cause', 'problems', 'line', 'course', 'ability', 'easily', 'modify', 'serialized', 'definitions', 'removed', 'e', 'g', 'bugfix', 'something', 'live', 'dangers', 'unaware']","['drawback', 'serial', 'object', 'along', 'definitionquest', 'futur', 'repercuss', 'conceiv', 'forc', 'python', 'class', 'function', 'definit', 'serial', 'along', 'object', 'declar', '__main__', 'serial', 'detail', 'common', 'gotcha', 'python', 'librari', 'pickl', 'dill', 'serial', 'class', 'function', 'definit', 'along', 'object', 'definit', 'locat', '__main__', 'result', 'deseri', 'object', 'depend', 'must', 'found', 'locat', 'serial', 'add', 'overhead', 'inflex', 'deploy', 'definit', 'must', 'maintain', 'separ', 'packag', 'must', 'version', 'present', 'product', 'environ', 'sometim', 'use', 'workaround', 'mainifi', 'object', 'serial', 'describ', 'instanc', 'oeg', 'dijk', 'essenti', 'redeclar', 'object', 'definit', '__main__', 'serial', 'code', 'use', 'list', 'far', 'approach', 'work', 'well', 'machin', 'learn', 'workflow', 'quit', 'yet', 'seem', 'quit', 'hacki', 'wonder', 'whether', 'might', 'caus', 'problem', 'line', 'cours', 'abil', 'easili', 'modifi', 'serial', 'definit', 'remov', 'e', 'g', 'bugfix', 'someth', 'live', 'danger', 'unawar']"
112,121,121,9490832,57888291,How to properly pickle sklearn pipeline when using custom transformer,"<p>I am trying to pickle a sklearn machine-learning model, and load it in another project. The model is wrapped in pipeline that does feature encoding, scaling etc. The problem starts when i want to use self-written transformers in the pipeline for more advanced tasks. </p>

<p>Let's say I have 2 projects: </p>

<ul>
<li>train_project: it has the custom transformers in src.feature_extraction.transformers.py</li>
<li>use_project: it has other things in src, or has no src catalog at all</li>
</ul>

<p>If in ""train_project"" I save the pipeline with joblib.dump(), and then in ""use_project"" i load it with joblib.load() it will not find something such as ""src.feature_extraction.transformers"" and throw exception:</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'src.feature_extraction'</p>
</blockquote>

<p>I should also add that my intention from the beginning was to simplify usage of the model, so programist can load the model as any other model, pass very simple, human readable features, and all ""magic"" preprocessing of features for actual model (e.g. gradient boosting) is happening inside.</p>

<p>I thought of creating /dependencies/xxx_model/ catalog in root of both projects, and store all needed classes and functions in there (copy code from ""train_project"" to ""use_project""), so structure of projects is equal and transformers can be loaded. I find this solution extremely inelegant, because it would force the structure of any project where the model would be used.</p>

<p>I thought of just recreating the pipeline and all transformers inside ""use_project"" and somehow loading fitted values of transformers from ""train_project"".</p>

<p>The best possible solution would be if dumped file contained all needed info and needed no dependencies, and I am honestly shocked that sklearn.Pipelines seem to not have that possibility - what's the point of fitting a pipeline if i can not load fitted object later? Yes it would work if i used only sklearn classes, and not create custom ones, but non-custom ones do not have all needed functionality.</p>

<p>Example code:</p>

<p>train_project</p>

<p>src.feature_extraction.transformers.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.pipeline import TransformerMixin
class FilterOutBigValuesTransformer(TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        self.biggest_value = X.c1.max()
        return self

    def transform(self, X):
        return X.loc[X.c1 &lt;= self.biggest_value]
</code></pre>

<p>train_project</p>

<p>main.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.externals import joblib
from sklearn.preprocessing import MinMaxScaler
from src.feature_extraction.transformers import FilterOutBigValuesTransformer

pipeline = Pipeline([
    ('filter', FilterOutBigValuesTransformer()),
    ('encode', MinMaxScaler()),
])
X=load_some_pandas_dataframe()
pipeline.fit(X)
joblib.dump(pipeline, 'path.x')
</code></pre>

<p>test_project</p>

<p>main.py</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.externals import joblib

pipeline = joblib.load('path.x')
</code></pre>

<p>The expected result is pipeline loaded correctly with transform method possible to use.</p>

<p>Actual result is exception when loading the file.</p>
",13242,7,24,5,python;scikit-learn;persistence;pipeline;joblib,2019-09-11 17:06:03,2019-09-11 17:06:03,2022-07-06 15:41:48,i am trying to pickle a sklearn machine learning model  and load it in another project  the model is wrapped in pipeline that does feature encoding  scaling etc  the problem starts when i want to use self written transformers in the pipeline for more advanced tasks   let s say i have  projects   if in train_project i save the pipeline with joblib dump    and then in use_project i load it with joblib load   it will not find something such as src feature_extraction transformers and throw exception  modulenotfounderror  no module named  src feature_extraction  i should also add that my intention from the beginning was to simplify usage of the model  so programist can load the model as any other model  pass very simple  human readable features  and all magic preprocessing of features for actual model  e g  gradient boosting  is happening inside  i thought of creating  dependencies xxx_model  catalog in root of both projects  and store all needed classes and functions in there  copy code from train_project to use_project   so structure of projects is equal and transformers can be loaded  i find this solution extremely inelegant  because it would force the structure of any project where the model would be used  i thought of just recreating the pipeline and all transformers inside use_project and somehow loading fitted values of transformers from train_project  the best possible solution would be if dumped file contained all needed info and needed no dependencies  and i am honestly shocked that sklearn pipelines seem to not have that possibility   what s the point of fitting a pipeline if i can not load fitted object later  yes it would work if i used only sklearn classes  and not create custom ones  but non custom ones do not have all needed functionality  example code  train_project src feature_extraction transformers py train_project main py test_project main py the expected result is pipeline loaded correctly with transform method possible to use  actual result is exception when loading the file ,how to properly pickle sklearn pipeline when using custom transformer,trying pickle sklearn machine learning model load another project model wrapped pipeline feature encoding scaling etc problem starts want use self written transformers pipeline advanced tasks let say projects train_project save pipeline joblib dump use_project load joblib load find something src feature_extraction transformers throw exception modulenotfounderror module named src feature_extraction also intention beginning simplify usage model programist load model model pass simple human readable features magic preprocessing features actual model e g gradient boosting happening inside thought creating dependencies xxx_model catalog root projects store needed classes functions copy code train_project use_project structure projects equal transformers loaded find solution extremely inelegant would force structure project model would used thought recreating pipeline transformers inside use_project somehow loading fitted values transformers train_project best possible solution would dumped file contained needed info needed dependencies honestly shocked sklearn pipelines seem possibility point fitting pipeline load fitted object later yes would work used sklearn classes create ones non ones needed functionality example code train_project src feature_extraction transformers py train_project main py test_project main py expected result pipeline loaded correctly transform method possible use actual result exception loading file,properly pickle sklearn pipeline using transformer,properly pickle sklearn pipeline using transformertrying pickle sklearn machine learning model load another project model wrapped pipeline feature encoding scaling etc problem starts want use self written transformers pipeline advanced tasks let say projects train_project save pipeline joblib dump use_project load joblib load find something src feature_extraction transformers throw exception modulenotfounderror module named src feature_extraction also intention beginning simplify usage model programist load model model pass simple human readable features magic preprocessing features actual model e g gradient boosting happening inside thought creating dependencies xxx_model catalog root projects store needed classes functions copy code train_project use_project structure projects equal transformers loaded find solution extremely inelegant would force structure project model would used thought recreating pipeline transformers inside use_project somehow loading fitted values transformers train_project best possible solution would dumped file contained needed info needed dependencies honestly shocked sklearn pipelines seem possibility point fitting pipeline load fitted object later yes would work used sklearn classes create ones non ones needed functionality example code train_project src feature_extraction transformers py train_project main py test_project main py expected result pipeline loaded correctly transform method possible use actual result exception loading file,"['properly', 'pickle', 'sklearn', 'pipeline', 'using', 'transformertrying', 'pickle', 'sklearn', 'machine', 'learning', 'model', 'load', 'another', 'project', 'model', 'wrapped', 'pipeline', 'feature', 'encoding', 'scaling', 'etc', 'problem', 'starts', 'want', 'use', 'self', 'written', 'transformers', 'pipeline', 'advanced', 'tasks', 'let', 'say', 'projects', 'train_project', 'save', 'pipeline', 'joblib', 'dump', 'use_project', 'load', 'joblib', 'load', 'find', 'something', 'src', 'feature_extraction', 'transformers', 'throw', 'exception', 'modulenotfounderror', 'module', 'named', 'src', 'feature_extraction', 'also', 'intention', 'beginning', 'simplify', 'usage', 'model', 'programist', 'load', 'model', 'model', 'pass', 'simple', 'human', 'readable', 'features', 'magic', 'preprocessing', 'features', 'actual', 'model', 'e', 'g', 'gradient', 'boosting', 'happening', 'inside', 'thought', 'creating', 'dependencies', 'xxx_model', 'catalog', 'root', 'projects', 'store', 'needed', 'classes', 'functions', 'copy', 'code', 'train_project', 'use_project', 'structure', 'projects', 'equal', 'transformers', 'loaded', 'find', 'solution', 'extremely', 'inelegant', 'would', 'force', 'structure', 'project', 'model', 'would', 'used', 'thought', 'recreating', 'pipeline', 'transformers', 'inside', 'use_project', 'somehow', 'loading', 'fitted', 'values', 'transformers', 'train_project', 'best', 'possible', 'solution', 'would', 'dumped', 'file', 'contained', 'needed', 'info', 'needed', 'dependencies', 'honestly', 'shocked', 'sklearn', 'pipelines', 'seem', 'possibility', 'point', 'fitting', 'pipeline', 'load', 'fitted', 'object', 'later', 'yes', 'would', 'work', 'used', 'sklearn', 'classes', 'create', 'ones', 'non', 'ones', 'needed', 'functionality', 'example', 'code', 'train_project', 'src', 'feature_extraction', 'transformers', 'py', 'train_project', 'main', 'py', 'test_project', 'main', 'py', 'expected', 'result', 'pipeline', 'loaded', 'correctly', 'transform', 'method', 'possible', 'use', 'actual', 'result', 'exception', 'loading', 'file']","['properli', 'pickl', 'sklearn', 'pipelin', 'use', 'transformertri', 'pickl', 'sklearn', 'machin', 'learn', 'model', 'load', 'anoth', 'project', 'model', 'wrap', 'pipelin', 'featur', 'encod', 'scale', 'etc', 'problem', 'start', 'want', 'use', 'self', 'written', 'transform', 'pipelin', 'advanc', 'task', 'let', 'say', 'project', 'train_project', 'save', 'pipelin', 'joblib', 'dump', 'use_project', 'load', 'joblib', 'load', 'find', 'someth', 'src', 'feature_extract', 'transform', 'throw', 'except', 'modulenotfounderror', 'modul', 'name', 'src', 'feature_extract', 'also', 'intent', 'begin', 'simplifi', 'usag', 'model', 'programist', 'load', 'model', 'model', 'pass', 'simpl', 'human', 'readabl', 'featur', 'magic', 'preprocess', 'featur', 'actual', 'model', 'e', 'g', 'gradient', 'boost', 'happen', 'insid', 'thought', 'creat', 'depend', 'xxx_model', 'catalog', 'root', 'project', 'store', 'need', 'class', 'function', 'copi', 'code', 'train_project', 'use_project', 'structur', 'project', 'equal', 'transform', 'load', 'find', 'solut', 'extrem', 'ineleg', 'would', 'forc', 'structur', 'project', 'model', 'would', 'use', 'thought', 'recreat', 'pipelin', 'transform', 'insid', 'use_project', 'somehow', 'load', 'fit', 'valu', 'transform', 'train_project', 'best', 'possibl', 'solut', 'would', 'dump', 'file', 'contain', 'need', 'info', 'need', 'depend', 'honestli', 'shock', 'sklearn', 'pipelin', 'seem', 'possibl', 'point', 'fit', 'pipelin', 'load', 'fit', 'object', 'later', 'ye', 'would', 'work', 'use', 'sklearn', 'class', 'creat', 'one', 'non', 'one', 'need', 'function', 'exampl', 'code', 'train_project', 'src', 'feature_extract', 'transform', 'py', 'train_project', 'main', 'py', 'test_project', 'main', 'py', 'expect', 'result', 'pipelin', 'load', 'correctli', 'transform', 'method', 'possibl', 'use', 'actual', 'result', 'except', 'load', 'file']"
113,122,122,4678218,72879855,how to classified and digitalized huge amount of paper using python,"<p>I have an archive papers in a company representing different business operation form different sections.
I want to scan all these documents and after that I want a way to classify all these scanned document into different category and sub-category based on custom preference such as (name, age, section, ..etc).</p>
<p>I want the end result to be digital files categorized according to the preferences that I set.</p>
<p>How can I do this using <strong>Python NLP</strong> or any other <strong>machine learning approach</strong></p>
",27,1,-2,3,python;nlp;document-classification,2022-07-06 13:32:50,2022-07-06 13:32:50,2022-07-06 15:12:59,i want the end result to be digital files categorized according to the preferences that i set  how can i do this using python nlp or any other machine learning approach,how to classified and digitalized huge amount of paper using python,want end result digital files categorized according preferences set using python nlp machine learning approach,classified digitalized huge amount paper using python,classified digitalized huge amount paper using pythonwant end result digital files categorized according preferences set using python nlp machine learning approach,"['classified', 'digitalized', 'huge', 'amount', 'paper', 'using', 'pythonwant', 'end', 'result', 'digital', 'files', 'categorized', 'according', 'preferences', 'set', 'using', 'python', 'nlp', 'machine', 'learning', 'approach']","['classifi', 'digit', 'huge', 'amount', 'paper', 'use', 'pythonw', 'end', 'result', 'digit', 'file', 'categor', 'accord', 'prefer', 'set', 'use', 'python', 'nlp', 'machin', 'learn', 'approach']"
114,123,123,12969291,72880184,SciPy optimization to a machine learning model,"<p>Recently I have been facing a problem which I think that SciPy might be a good candidate to solve. However, I have not been able to properly apply it. Not sure if I am missing something or if what I am looking for is actually not possible at all.</p>
<p>This is a fictitious example which I made to makes things more clear and easier to visualize. My case is way more complicated. However, what I want to find out is how many dogs and cats would there be given a specific number of rats?</p>
<pre><code>from sklearn.svm import SVR
from scipy.optimize import minimize
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import pandas as pd

n_dogs = [10, 5, 5, 2, 19, 12, 1,2]
n_cats = [5, 100, 5, 3, 1000, 0, 1,2]
n_rats = [100, 0, 50, 30, 0, 1000, 10, 5]


X = np.array([n_dogs, n_cats]).T
y = np.array([n_rats]).T 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)    
model = RandomForestRegressor()
model.fit(X,y)

#Boundaries of condition
bnds = [(100,500), (100,150)]
#Initial guesses
x0 = [300,125]
#Predict from condition
residual_plant = minimize(lambda x: model.predict([[12,8]]), x0, method='SLSQP',bounds=bnds,options = {'eps': np.radians(5.0)})
print(residual_plant)```

</code></pre>
",30,0,0,4,python;numpy;scikit-learn;scipy,2022-07-06 13:57:37,2022-07-06 13:57:37,2022-07-06 13:57:37,recently i have been facing a problem which i think that scipy might be a good candidate to solve  however  i have not been able to properly apply it  not sure if i am missing something or if what i am looking for is actually not possible at all  this is a fictitious example which i made to makes things more clear and easier to visualize  my case is way more complicated  however  what i want to find out is how many dogs and cats would there be given a specific number of rats ,scipy optimization to a machine learning model,recently facing problem think scipy might good candidate solve however able properly apply sure missing something looking actually possible fictitious example made makes things clear easier visualize case way complicated however want find many dogs cats would given specific number rats,scipy optimization machine learning model,scipy optimization machine learning modelrecently facing problem think scipy might good candidate solve however able properly apply sure missing something looking actually possible fictitious example made makes things clear easier visualize case way complicated however want find many dogs cats would given specific number rats,"['scipy', 'optimization', 'machine', 'learning', 'modelrecently', 'facing', 'problem', 'think', 'scipy', 'might', 'good', 'candidate', 'solve', 'however', 'able', 'properly', 'apply', 'sure', 'missing', 'something', 'looking', 'actually', 'possible', 'fictitious', 'example', 'made', 'makes', 'things', 'clear', 'easier', 'visualize', 'case', 'way', 'complicated', 'however', 'want', 'find', 'many', 'dogs', 'cats', 'would', 'given', 'specific', 'number', 'rats']","['scipi', 'optim', 'machin', 'learn', 'modelrec', 'face', 'problem', 'think', 'scipi', 'might', 'good', 'candid', 'solv', 'howev', 'abl', 'properli', 'appli', 'sure', 'miss', 'someth', 'look', 'actual', 'possibl', 'fictiti', 'exampl', 'made', 'make', 'thing', 'clear', 'easier', 'visual', 'case', 'way', 'complic', 'howev', 'want', 'find', 'mani', 'dog', 'cat', 'would', 'given', 'specif', 'number', 'rat']"
115,125,125,19491907,72878566,AttributeError: module &#39;keras.api._v2.keras.experimental&#39; has no attribute &#39;export_saved_model&#39;,"<p>can anyone here help me fix this? i cant find the solution to this in the internet. also i am not good in python/machine learning</p>
<pre><code>tf.keras.experimental.export_saved_model(model, SHOE_SAVED_MODEL)
shoe_model = tf.keras.experimental.load_from_saved_model(SHOE_SAVED_MODEL,
custom_objects={'KerasLayer':hub.KerasLayer})



AttributeError                            Traceback (most recent call last)
&lt;ipython-input-20-82a8de315a24&gt; in &lt;module&gt;()
      2 
      3 SHOE_SAVED_MODEL = &quot;saved_models/shoe&quot;
----&gt; 4 tf.keras.experimental.export_saved_model(model, SHOE_SAVED_MODEL)
      5 shoe_model = tf.keras.experimental.load_from_saved_model(SHOE_SAVED_MODEL,
      6 custom_objects={'KerasLayer':hub.KerasLayer})

AttributeError: module 'keras.api._v2.keras.experimental' has no attribute 'export_saved_model'```
</code></pre>
",15,1,0,4,python;tensorflow;machine-learning;keras,2022-07-06 11:30:09,2022-07-06 11:30:09,2022-07-06 11:39:10,can anyone here help me fix this  i cant find the solution to this in the internet  also i am not good in python machine learning,attributeerror  module    keras api _v keras experimental    has no attribute    export_saved_model   ,anyone help fix cant find solution internet also good python machine learning,attributeerror module keras api _v keras experimental attribute export_saved_model,attributeerror module keras api _v keras experimental attribute export_saved_modelanyone help fix cant find solution internet also good python machine learning,"['attributeerror', 'module', 'keras', 'api', '_v', 'keras', 'experimental', 'attribute', 'export_saved_modelanyone', 'help', 'fix', 'cant', 'find', 'solution', 'internet', 'also', 'good', 'python', 'machine', 'learning']","['attributeerror', 'modul', 'kera', 'api', '_v', 'kera', 'experiment', 'attribut', 'export_saved_modelanyon', 'help', 'fix', 'cant', 'find', 'solut', 'internet', 'also', 'good', 'python', 'machin', 'learn']"
116,126,126,17350567,72876127,Machine Learning: multiple steps per turn,"<p>I have a question of understanding. I have an environment that passes possible actions and the state of the board to the agent. The agent chooses an action and stores it, together with the state, in a collection.</p>
<p>Now I have the problem that I have a board game where a chosen move might lead to possible other moves in the same turn. To make it clearer, let's say that I take an action and place my piece on square A. From the rules of the game it then follows that the piece from square A may go to square B or C in the same turn. So suddenly I have two new possibilities - in the same turn.</p>
<p>I have solved it as follows. The actions that are passed to the agent are only the next field. As soon as the agent has selected an action, it is stored in the collection together with the board. Afterwards, the environment is looked at to see if the piece can continue from the new position in the same round. If so, the agent decides which action he then selects, then saves this too and this continues until the figure can no longer move - then the next round begins.</p>
<p>Is it stupid to have implemented it this way? Doesn't the agent learn properly then?</p>
",24,1,1,2,machine-learning;reinforcement-learning,2022-07-06 03:50:16,2022-07-06 03:50:16,2022-07-06 03:59:15,i have a question of understanding  i have an environment that passes possible actions and the state of the board to the agent  the agent chooses an action and stores it  together with the state  in a collection  now i have the problem that i have a board game where a chosen move might lead to possible other moves in the same turn  to make it clearer  let s say that i take an action and place my piece on square a  from the rules of the game it then follows that the piece from square a may go to square b or c in the same turn  so suddenly i have two new possibilities   in the same turn  i have solved it as follows  the actions that are passed to the agent are only the next field  as soon as the agent has selected an action  it is stored in the collection together with the board  afterwards  the environment is looked at to see if the piece can continue from the new position in the same round  if so  the agent decides which action he then selects  then saves this too and this continues until the figure can no longer move   then the next round begins  is it stupid to have implemented it this way  doesn t the agent learn properly then ,machine learning  multiple steps per turn,question understanding environment passes possible actions state board agent agent chooses action stores together state collection problem board game chosen move might lead possible moves turn make clearer let say take action place piece square rules game follows piece square may go square b c turn suddenly two possibilities turn solved follows actions passed agent next field soon agent selected action stored collection together board afterwards environment looked see piece continue position round agent decides action selects saves continues figure longer move next round begins stupid implemented way agent learn properly,machine learning multiple steps per turn,machine learning multiple steps per turnquestion understanding environment passes possible actions state board agent agent chooses action stores together state collection problem board game chosen move might lead possible moves turn make clearer let say take action place piece square rules game follows piece square may go square b c turn suddenly two possibilities turn solved follows actions passed agent next field soon agent selected action stored collection together board afterwards environment looked see piece continue position round agent decides action selects saves continues figure longer move next round begins stupid implemented way agent learn properly,"['machine', 'learning', 'multiple', 'steps', 'per', 'turnquestion', 'understanding', 'environment', 'passes', 'possible', 'actions', 'state', 'board', 'agent', 'agent', 'chooses', 'action', 'stores', 'together', 'state', 'collection', 'problem', 'board', 'game', 'chosen', 'move', 'might', 'lead', 'possible', 'moves', 'turn', 'make', 'clearer', 'let', 'say', 'take', 'action', 'place', 'piece', 'square', 'rules', 'game', 'follows', 'piece', 'square', 'may', 'go', 'square', 'b', 'c', 'turn', 'suddenly', 'two', 'possibilities', 'turn', 'solved', 'follows', 'actions', 'passed', 'agent', 'next', 'field', 'soon', 'agent', 'selected', 'action', 'stored', 'collection', 'together', 'board', 'afterwards', 'environment', 'looked', 'see', 'piece', 'continue', 'position', 'round', 'agent', 'decides', 'action', 'selects', 'saves', 'continues', 'figure', 'longer', 'move', 'next', 'round', 'begins', 'stupid', 'implemented', 'way', 'agent', 'learn', 'properly']","['machin', 'learn', 'multipl', 'step', 'per', 'turnquest', 'understand', 'environ', 'pass', 'possibl', 'action', 'state', 'board', 'agent', 'agent', 'choos', 'action', 'store', 'togeth', 'state', 'collect', 'problem', 'board', 'game', 'chosen', 'move', 'might', 'lead', 'possibl', 'move', 'turn', 'make', 'clearer', 'let', 'say', 'take', 'action', 'place', 'piec', 'squar', 'rule', 'game', 'follow', 'piec', 'squar', 'may', 'go', 'squar', 'b', 'c', 'turn', 'suddenli', 'two', 'possibl', 'turn', 'solv', 'follow', 'action', 'pass', 'agent', 'next', 'field', 'soon', 'agent', 'select', 'action', 'store', 'collect', 'togeth', 'board', 'afterward', 'environ', 'look', 'see', 'piec', 'continu', 'posit', 'round', 'agent', 'decid', 'action', 'select', 'save', 'continu', 'figur', 'longer', 'move', 'next', 'round', 'begin', 'stupid', 'implement', 'way', 'agent', 'learn', 'properli']"
117,127,127,19489959,72875600,Machine Learning question (Solving ValueError: could not convert string to float:),"<p><strong>I am running the example code Below:</strong></p>
<pre><code>import pandas as pd

from sklearn.tree import DecisionTreeClassifier

from sklearn.preprocessing import OneHotEncoder
</code></pre>
<p><a href=""https://i.stack.imgur.com/Hw0eT.png"" rel=""nofollow noreferrer"">How the CSV looks</a></p>
<pre><code>url_data = pd.read_csv('phishing_site_urls.csv')

url_data.drop_duplicates(inplace = True)
print(url_data.shape)


#X = input Data (Urls) // Y = output (Wether its Bad or Good)

X = url_data.drop(columns=['Label'])
y = url_data['Label']

model = DecisionTreeClassifier()

model.fit(X, y)
predictions = model.predict([[&quot;Paste suspected Phishy Link here&quot;]])

print(predictions)
</code></pre>
<p>**-Using a csv with the name phishing_site_urls.csv, that has two columns one named &quot;URL&quot; and the other &quot;Label&quot;. Where the URL column holds links that are either phishy or valid and the label column has a corresponding &quot;bad&quot; or &quot;good&quot; for determining which link in the URL column is phishy or valid.</p>
<p>-My question is I keep getting the error: &quot;ValueError: could not convert string to float:&quot; I assume there has to be some way of encoding the links from strings to floats so the model can run? If so I would appreciate some insight on how I can do this.**</p>
",41,1,1,2,python;machine-learning,2022-07-06 02:43:28,2022-07-06 02:43:28,2022-07-06 03:30:43,i am running the example code below      using a csv with the name phishing_site_urls csv  that has two columns one named  url  and the other  label   where the url column holds links that are either phishy or valid and the label column has a corresponding  bad  or  good  for determining which link in the url column is phishy or valid   my question is i keep getting the error   valueerror  could not convert string to float   i assume there has to be some way of encoding the links from strings to floats so the model can run  if so i would appreciate some insight on how i can do this   ,machine learning question  solving valueerror  could not convert string to float  ,running example code using csv name phishing_site_urls csv two columns one named url label url column holds links either phishy valid label column corresponding bad good determining link url column phishy valid question keep getting error valueerror could convert string float assume way encoding links strings floats model run would appreciate insight,machine learning question solving valueerror could convert string float,machine learning question solving valueerror could convert string floatrunning example code using csv name phishing_site_urls csv two columns one named url label url column holds links either phishy valid label column corresponding bad good determining link url column phishy valid question keep getting error valueerror could convert string float assume way encoding links strings floats model run would appreciate insight,"['machine', 'learning', 'question', 'solving', 'valueerror', 'could', 'convert', 'string', 'floatrunning', 'example', 'code', 'using', 'csv', 'name', 'phishing_site_urls', 'csv', 'two', 'columns', 'one', 'named', 'url', 'label', 'url', 'column', 'holds', 'links', 'either', 'phishy', 'valid', 'label', 'column', 'corresponding', 'bad', 'good', 'determining', 'link', 'url', 'column', 'phishy', 'valid', 'question', 'keep', 'getting', 'error', 'valueerror', 'could', 'convert', 'string', 'float', 'assume', 'way', 'encoding', 'links', 'strings', 'floats', 'model', 'run', 'would', 'appreciate', 'insight']","['machin', 'learn', 'question', 'solv', 'valueerror', 'could', 'convert', 'string', 'floatrun', 'exampl', 'code', 'use', 'csv', 'name', 'phishing_site_url', 'csv', 'two', 'column', 'one', 'name', 'url', 'label', 'url', 'column', 'hold', 'link', 'either', 'phishi', 'valid', 'label', 'column', 'correspond', 'bad', 'good', 'determin', 'link', 'url', 'column', 'phishi', 'valid', 'question', 'keep', 'get', 'error', 'valueerror', 'could', 'convert', 'string', 'float', 'assum', 'way', 'encod', 'link', 'string', 'float', 'model', 'run', 'would', 'appreci', 'insight']"
118,128,128,19489602,72875251,How to deal this task with machine learning?,"<h1>Question</h1>
<p>I'm trying to deal following task with machine learning, but the performance is not so good. Actually I'm not familiar with around machine learning and data science, so I don't have much acknowledgement. Don't you know there was some similar tasks in the past at like Kaggle?</p>
<h1>Task</h1>
<ul>
<li>The dataset is several queries and list of contents respect to each queries.</li>
<li>Contents in each queries have 0 or 1 as a label.</li>
<li>Almost all contents in each queries has 0 as a label.</li>
<li>There is only 1 or 0 content which has 1 as a label in each queries.</li>
<li>I wanna give the highest output from the model to the content which has 1 as a label in each queries.</li>
<li>I don't care about the order, difference of output from the model among contents with label 0, I just wanna bring content with label 1 at No.1 in each queries.</li>
<li>When the model gives exactly the same output to the all of contents, the content with label 1 will be No.1 of the ranking in the query, duh. But this doesn't have no meaning.</li>
</ul>
<h1>What I did</h1>
<p>At first, I didn't look the dataset by each queries, and I treated as a classification task to classify into 0 or 1. Let's say the model could classify a content with label 1 as 1, but sometimes there was content with label 0 classified as 1 with higher score than the content whose label is 1 in the same query. Actually the order (content whose label is 1 comes No.1 in each queries) is the most important thing, so I'm using &quot;Learn to Rank&quot; now.</p>
<h1>Problems I'm facing</h1>
<p>I'm visiting many website which describes on Learn to Rank, but I can't find the case like this, I don't know how can I call, like binary ranking.
Actually I'm using &quot;LambdaRank&quot; method, which scale the differential of loss function(cross entropy) because I'm expecting this method will contribute to bring the content with label 1 to the top of the list in each queries. And I'm using LightGBM or PyTorchBut now I'm facing several problems like this.</p>
<ul>
<li>Because almost all contents has 0 as their label, so the model can make loss small with predicting all of them are 0. Then, the slope of loss function is almost 0, so the training will not progress. Then, all of the contents is No.1 in the ranking.</li>
<li>(In PyTorch,) training is depends on the beginning of the training. In many cases, the model will predict all of the content is 0 at the 1st epoch of training. Then, the training will not progress as I said before. But, I'm not sure the reason, but sometimes, there is the content with label 1 at the top of ranking with about 10% of queries. In this case, the training will progress.</li>
<li>(Confirming with PyTorch is not yet) After training, about 80% of contents with label 1 is No.1 in their queries. But there is several queries who has no content with label 1, only has contents with label 0. Actually I wanna cut such queries, so I did score-cut but it was not effective. So, I guess there is no consistency of prediction among queries. Let's say there is 2 queries and predictions on each contents is like, query1[A:0.9, B:0.6, C:0.1] query2[D:0.7, E:0.2], then A is more relevant than D respect to each query?</li>
</ul>
<h1>Ideas but I've not tried yet</h1>
<p>Training will not progress due to a lot of contents with label 0.</p>
<ul>
<li>Use any other loss function like focal loss.</li>
<li>Use the differential of loss function at the prediction on content with label 1 for updating model parameters.
To reduce contents with label 0 at the No.1 in the ranking even if there is also the content with label 1 at the No.1,</li>
<li>Create custom metric which reduces them.</li>
<li>Create custom metric which compares the prediction of the content with label 1 and the content with the highest score in the list except for the content with label 1.</li>
<li>But I guess these metrics are not differentiable, but I think I can use for scaling differential of loss function instead of NDCG in LambdaRank method.</li>
</ul>
",23,0,0,3,machine-learning;deep-learning;kaggle,2022-07-06 02:05:37,2022-07-06 02:05:37,2022-07-06 02:05:37,i m trying to deal following task with machine learning  but the performance is not so good  actually i m not familiar with around machine learning and data science  so i don t have much acknowledgement  don t you know there was some similar tasks in the past at like kaggle  at first  i didn t look the dataset by each queries  and i treated as a classification task to classify into  or   let s say the model could classify a content with label  as   but sometimes there was content with label  classified as  with higher score than the content whose label is  in the same query  actually the order  content whose label is  comes no  in each queries  is the most important thing  so i m using  learn to rank  now  training will not progress due to a lot of contents with label  ,how to deal this task with machine learning ,trying deal following task machine learning performance good actually familiar around machine learning data science much acknowledgement know similar tasks past like kaggle first look dataset queries treated classification task classify let say model could classify content label sometimes content label classified higher score content whose label query actually order content whose label comes queries important thing using learn rank training progress due lot contents label,deal task machine learning,deal task machine learningtrying deal following task machine learning performance good actually familiar around machine learning data science much acknowledgement know similar tasks past like kaggle first look dataset queries treated classification task classify let say model could classify content label sometimes content label classified higher score content whose label query actually order content whose label comes queries important thing using learn rank training progress due lot contents label,"['deal', 'task', 'machine', 'learningtrying', 'deal', 'following', 'task', 'machine', 'learning', 'performance', 'good', 'actually', 'familiar', 'around', 'machine', 'learning', 'data', 'science', 'much', 'acknowledgement', 'know', 'similar', 'tasks', 'past', 'like', 'kaggle', 'first', 'look', 'dataset', 'queries', 'treated', 'classification', 'task', 'classify', 'let', 'say', 'model', 'could', 'classify', 'content', 'label', 'sometimes', 'content', 'label', 'classified', 'higher', 'score', 'content', 'whose', 'label', 'query', 'actually', 'order', 'content', 'whose', 'label', 'comes', 'queries', 'important', 'thing', 'using', 'learn', 'rank', 'training', 'progress', 'due', 'lot', 'contents', 'label']","['deal', 'task', 'machin', 'learningtri', 'deal', 'follow', 'task', 'machin', 'learn', 'perform', 'good', 'actual', 'familiar', 'around', 'machin', 'learn', 'data', 'scienc', 'much', 'acknowledg', 'know', 'similar', 'task', 'past', 'like', 'kaggl', 'first', 'look', 'dataset', 'queri', 'treat', 'classif', 'task', 'classifi', 'let', 'say', 'model', 'could', 'classifi', 'content', 'label', 'sometim', 'content', 'label', 'classifi', 'higher', 'score', 'content', 'whose', 'label', 'queri', 'actual', 'order', 'content', 'whose', 'label', 'come', 'queri', 'import', 'thing', 'use', 'learn', 'rank', 'train', 'progress', 'due', 'lot', 'content', 'label']"
119,129,129,17758340,72874722,How to configure SQL Server Machine Learning Services on Mac?,"<p>I'm trying to run Python within SQL Server using Machine Learning Services, however, I am having a really hard time getting started. I have SQL Server 2019 running/set up on Mac using Docker according to this <a href=""https://www.youtube.com/watch?v=glxE7w4D8v8&amp;t=301s"" rel=""nofollow noreferrer"">video</a>.</p>
<p>What I've tried to get Machine Learning Services working:</p>
<ul>
<li><p>Installing the Machine Learning extension for Azure Data Studio, including following listed prerequisites:</p>
<p>-Specifying the local path to a preexisting Python installation under Settings</p>
<p>-Microsoft ODBC driver 17 for SQL Server for macOS</p>
</li>
<li><p>Running the script (which runs successfully):</p>
</li>
</ul>
<pre><code>EXEC sp_configure 'external scripts enabled', 1 
RECONFIGURE WITH OVERRIDE
</code></pre>
<ul>
<li>After running the above script: disconnecting from the server, quitting Azure Data Studio, and either stopping or restarting the container</li>
</ul>
<p>However, whenever I attempt to run a piece of code like this:</p>
<pre><code>EXEC sp_execute_external_script
  @language = N'Python',
  @script=N'print(&quot;Hello World!&quot;)'
GO
</code></pre>
<p>I keep receiving the error:</p>
<blockquote>
<p>Msg 39111, Level 16, State 1, Procedure sp_execute_external_script, Line 1
The SQL Server Machine Learning Services End-User License Agreement (EULA) has not been accepted.</p>
</blockquote>
<p>I've followed other solutions, though all seem to be Windows oriented, such as restarting SQL Server services or Launchpad via Configuration Manager. Any help is much appreciated, thank you.</p>
",28,0,0,2,sql-server;sql-server-2019,2022-07-06 01:12:16,2022-07-06 01:12:16,2022-07-06 01:13:55,i m trying to run python within sql server using machine learning services  however  i am having a really hard time getting started  i have sql server  running set up on mac using docker according to this   what i ve tried to get machine learning services working  installing the machine learning extension for azure data studio  including following listed prerequisites   specifying the local path to a preexisting python installation under settings  microsoft odbc driver  for sql server for macos running the script  which runs successfully   however  whenever i attempt to run a piece of code like this  i keep receiving the error  i ve followed other solutions  though all seem to be windows oriented  such as restarting sql server services or launchpad via configuration manager  any help is much appreciated  thank you ,how to configure sql server machine learning services on mac ,trying run python within sql server using machine learning services however really hard time getting started sql server running set mac using docker according tried get machine learning services working installing machine learning extension azure data studio including following listed prerequisites specifying local path preexisting python installation settings microsoft odbc driver sql server macos running script runs successfully however whenever attempt run piece code like keep receiving error followed solutions though seem windows oriented restarting sql server services launchpad via configuration manager help much appreciated thank,configure sql server machine learning services mac,configure sql server machine learning services mactrying run python within sql server using machine learning services however really hard time getting started sql server running set mac using docker according tried get machine learning services working installing machine learning extension azure data studio including following listed prerequisites specifying local path preexisting python installation settings microsoft odbc driver sql server macos running script runs successfully however whenever attempt run piece code like keep receiving error followed solutions though seem windows oriented restarting sql server services launchpad via configuration manager help much appreciated thank,"['configure', 'sql', 'server', 'machine', 'learning', 'services', 'mactrying', 'run', 'python', 'within', 'sql', 'server', 'using', 'machine', 'learning', 'services', 'however', 'really', 'hard', 'time', 'getting', 'started', 'sql', 'server', 'running', 'set', 'mac', 'using', 'docker', 'according', 'tried', 'get', 'machine', 'learning', 'services', 'working', 'installing', 'machine', 'learning', 'extension', 'azure', 'data', 'studio', 'including', 'following', 'listed', 'prerequisites', 'specifying', 'local', 'path', 'preexisting', 'python', 'installation', 'settings', 'microsoft', 'odbc', 'driver', 'sql', 'server', 'macos', 'running', 'script', 'runs', 'successfully', 'however', 'whenever', 'attempt', 'run', 'piece', 'code', 'like', 'keep', 'receiving', 'error', 'followed', 'solutions', 'though', 'seem', 'windows', 'oriented', 'restarting', 'sql', 'server', 'services', 'launchpad', 'via', 'configuration', 'manager', 'help', 'much', 'appreciated', 'thank']","['configur', 'sql', 'server', 'machin', 'learn', 'servic', 'mactri', 'run', 'python', 'within', 'sql', 'server', 'use', 'machin', 'learn', 'servic', 'howev', 'realli', 'hard', 'time', 'get', 'start', 'sql', 'server', 'run', 'set', 'mac', 'use', 'docker', 'accord', 'tri', 'get', 'machin', 'learn', 'servic', 'work', 'instal', 'machin', 'learn', 'extens', 'azur', 'data', 'studio', 'includ', 'follow', 'list', 'prerequisit', 'specifi', 'local', 'path', 'preexist', 'python', 'instal', 'set', 'microsoft', 'odbc', 'driver', 'sql', 'server', 'maco', 'run', 'script', 'run', 'success', 'howev', 'whenev', 'attempt', 'run', 'piec', 'code', 'like', 'keep', 'receiv', 'error', 'follow', 'solut', 'though', 'seem', 'window', 'orient', 'restart', 'sql', 'server', 'servic', 'launchpad', 'via', 'configur', 'manag', 'help', 'much', 'appreci', 'thank']"
120,130,130,19489442,72874254,"ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required","<p>So I'm new at programming and machine learning, and I'm using this code I found from a journal for spam detection. When I try to use it, the result turns out to be error, even though I already prepared the data correctly. The error message is 'ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required.'
Can anyone please help me out with this issue?
[The link for the complete code is here] (<a href=""https://github.com/ijdutse/spd"" rel=""nofollow noreferrer"">https://github.com/ijdutse/spd</a>)</p>
<pre><code>#!/usr/bin/env python3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
from datetime import datetime
import preprocessor as p
import random, os, utils, smart_open, json, codecs, pickle, time
import gensim
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.fftpack import fft

data_sources = ['I:\Data Penelitian\Iphone/iphone.json']

def main():
    spd = Spd(data_sources) #class instantiation
    start = time.process_time()
    relevant_tweets = spd.detector(data_sources)
    stop = time.process_time()
    return relevant_tweets




class Spd:
    &quot;&quot;&quot; some functions to accept raw files, extract relevant fields and filter our irrelevent content&quot;&quot;&quot;
    def __init__(self, data_sources):
        self.data_sources = data_sources
    pass
        
    # first function in the class:
    def extractor(self, data_sources): # accept list of files consisting of raw tweets in form of json object
        data_extracts = {'TweetID':[],'ScreenName':[],'RawTweets':[],'CreatedAt':[],'RetweetCount':[],\
                         'FollowersCount':[],'FriendsCount':[], 'StatusesCount':[],'FavouritesCount':[],\
                         'UserName':[],'Location':[],'AccountCreated':[],'Language':[],'Description':[],\
                         'UserURL':[],'VerifiedAccount':[],'CleanTweets':[],'UserID':[], 'TimeZone':[],'TweetFavouriteCount':[]}
        non_english_tweets = 0 # keep track of the non-English tweets
        with codecs.open('I:\Data Penelitian\Iphone/iphone.json', 'r') as f: # data_source is read from extractor() function
            for line in f.readlines():
                non_English = 0
                try:
                    line = json.loads(line)
                    if line['lang'] in ['en','en-gb','en-GB','en-AU','en-IN','en_US']:
                        data_extracts['Language'].append(line['Language'])
                        data_extracts['TweetID'].append(line['TweetID'])
                        data_extracts['RawTweets'].append(line['RawTweets'])
                        data_extracts['CleanTweets'].append(p.clean(line['RawTweets']))
                        data_extracts['CreatedAt'].append(line['CreatedAt'])
                        data_extracts['AccountCreated'].append(line['AccountCreated'])                       
                        data_extracts['ScreenName'].append(line['ScreenName'])                          
                        data_extracts['RetweetCount'].append(line['RetweetCount'])
                        data_extracts['FollowersCount'].append(line['FollowersCount'])
                        data_extracts['FriendsCount'].append(line['FriendsCount'])
                        data_extracts['StatusesCount'].append(line['StatusesCount'])
                        data_extracts['FavouritesCount'].append(line['FavouritesCount'])
                        data_extracts['UserName'].append(line['UserName'])
                        data_extracts['Location'].append(line['Location'])
                        data_extracts['Description'].append(line['Description'])
                        data_extracts['UserURL'].append(line['UserURL'])
                        data_extracts['VerifiedAccount'].append(line['VerifiedAccount'])
                        data_extracts['UserID'].append(line['UserID'])
                        data_extracts['TimeZone'].append(line['TimeZone'])
                        data_extracts['TweetFavouriteCount'].append(line['TweetFavouriteCount'])
                    else:
                        non_english_tweets +=1
                except:
                    continue
            df0 = pd.DataFrame(data_extracts) #convert data extracts to pandas DataFrame
            df0['CreatedAt']=pd.to_datetime(data_extracts['CreatedAt'],errors='coerce') # convert to datetime
            df0['AccountCreated']=pd.to_datetime(data_extracts['AccountCreated'],errors='coerce')
            df0 = df0.dropna(subset=['AccountCreated','CreatedAt']) # drop na in datetime
            AccountAge = [] # compute the account age of accounts
            date_format = &quot;%Y-%m-%d  %H:%M:%S&quot;
            for dr,dc in zip(df0.CreatedAt, df0.AccountCreated):
                #try:
                dr = str(dr)
                dc = str(dc)
                d1 = datetime.strptime(dr,date_format)
                d2 = datetime.strptime(dc,date_format)
                dif = d1 - d2
                AccountAge.append(dif.days)
                #except:
                    #continue
            df0['AccountAge']=AccountAge
            # add/define additional features ...
            df0['Retweets'] = df0.RawTweets.apply(lambda x: str(x).split()[0]=='RT' )
            df0['RawTweetsLen'] = df0.RawTweets.apply(lambda x: len(str(x))) # modified
            df0['DescriptionLen'] = df0.Description.apply(lambda x: len(str(x)))
            df0['UserNameLen'] = df0.UserName.apply(lambda x: len(str(x)))
            df0['ScreenNameLen'] = df0.ScreenName.apply(lambda x: len(str(x)))
            df0['LocationLen'] = df0.Location.apply(lambda x: len(str(x)))
            df0['Activeness'] = df0.StatusesCount.truediv(df0.AccountAge)
            df0['Friendship'] = df0.FriendsCount.truediv(df0.FollowersCount)
            df0['Followership'] = df0.FollowersCount.truediv(df0.FriendsCount)
            df0['Interestingness'] = df0.FavouritesCount.truediv(df0.StatusesCount)
            df0['BidirFriendship'] = (df0.FriendsCount + df0.FollowersCount).truediv(df0.FriendsCount)
            df0['BidirFollowership'] = (df0.FriendsCount + df0.FollowersCount).truediv(df0.FollowersCount)
            df0['NamesRatio'] = df0.ScreenNameLen.truediv(df0.UserNameLen)
            df0['CleanTweetsLen'] = df0.CleanTweets.apply(lambda x: len(str(x)))
            df0['LexRichness'] = df0.CleanTweetsLen.truediv(df0.RawTweetsLen)       
            # Remove all RTs, set UserID as index and save relevant files:
            df0 = df0[df0.Retweets.values==False] # remove retweets
            df0 = df0.set_index('UserID')
            df0 = df0[~df0.index.duplicated()] # remove duplicates in the tweet
            #df0.to_csv(data_source[:15]+'all_extracts.csv') #save all extracts as csv
            df0.to_csv(data_sources[:5]+'all_extracts.csv') #save all extracts as csv 
            with open(data_sources[:5]+'non_English.txt','w') as d: # save count of non-English tweets
                d.write('{}'.format(non_english_tweets))
                d.close()
        return df0

    
    def detector(self, data_sources): # accept list of raw tweets as json objects
        self.data_sources = data_sources
        for data_sources in data_sources:
            self.data_sources = data_sources
            df0 = self.extractor(data_sources)
            #drop fields not required for predicition
            X = df0.drop(['Language','TweetID','RawTweets','CleanTweets','CreatedAt','AccountCreated','ScreenName',\
                 'Retweets','UserName','Location','Description','UserURL','VerifiedAccount','RetweetCount','TimeZone','TweetFavouriteCount'], axis=1)
            X = X.replace([np.inf,-np.inf],np.nan) # replace infinity values to avoid 0 division ...
            X = X.dropna()
            # reload the trained model for use:
            spd_filter=pickle.load(open('trained_rf.pkl','rb'))
            PredictedClass = spd_filter.predict(X) # Predict spam or automated accounts/tweets:
            X['PredictedClass'] = PredictedClass # include the predicted class in the dataframe
            nonspam = df0.loc[X.PredictedClass.values==1] # sort out the nonspam accounts
            spam = df0.loc[X.PredictedClass.values==0] # sort out spam/automated accounts
            #relevant_tweets = nonspam[['CreatedAt', 'CleanTweets']]
            relevant_tweets = nonspam[['CreatedAt','AccountCreated','ScreenName','Location','TimeZone','Description','VerifiedAccount','RawTweets', 'CleanTweets','TweetFavouriteCount','Retweets']]
            relevant_tweets = relevant_tweets.reset_index() # reset index and remove it from the dataframe
            #relevant_tweets = relevant_tweets.drop('UserID', axis=1) 
            # save files:
            X.to_csv(data_source[:5]+'_all_predicted_classes.csv') #save all extracts as csv, used to be 15
            nonspam.to_csv(data_source[:5]+'_nonspam_accounts.csv')
            spam.to_csv(data_source[:5]+'_spam_accounts.csv')
            relevant_tweets.to_csv(data_source[:5]+'_relevant_tweets.csv') # relevant tweets for subsequent analysis
        return relevant_tweets # or return relevant_tweets, nonspam, spam

if __name__ =='__main__':
    main()
</code></pre>
<p>The traceback error is as follow</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-2-5dc56f49d005&gt; in &lt;module&gt;
    142 
    143 if __name__ =='__main__':
--&gt; 144     main()

&lt;ipython-input-2-5dc56f49d005&gt; in main()
     18     spd = Spd(data_sources) #class instantiation
     19     start = time.process_time()
---&gt; 20     relevant_tweets = spd.detector(data_sources)
     21     stop = time.process_time()
     22     return relevant_tweets

&lt;ipython-input-2-5dc56f49d005&gt; in detector(self, data_sources)
    126             # reload the trained model for use:
    127             spd_filter=pickle.load(open('trained_rf.pkl','rb'))
--&gt; 128             PredictedClass = spd_filter.predict(X) # Predict spam or automated accounts/tweets:
    129             X['PredictedClass'] = PredictedClass # include the predicted class in the dataframe
    130             nonspam = df0.loc[X.PredictedClass.values==1] # sort out the nonspam accounts

~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in predict(self, X)
    543             The predicted classes.
    544         &quot;&quot;&quot;
--&gt; 545         proba = self.predict_proba(X)
    546 
    547         if self.n_outputs_ == 1:

~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in predict_proba(self, X)
    586         check_is_fitted(self, 'estimators_')
    587         # Check data
--&gt; 588         X = self._validate_X_predict(X)
    589 
    590         # Assign chunk of trees to jobs

~\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py in _validate_X_predict(self, X)
    357                                  &quot;call `fit` before exploiting the model.&quot;)
    358 
--&gt; 359         return self.estimators_[0]._validate_X_predict(X, check_input=True)
    360 
    361     @property

~\Anaconda3\lib\site-packages\sklearn\tree\tree.py in _validate_X_predict(self, X, check_input)
    389         &quot;&quot;&quot;Validate X whenever one tries to predict, apply, predict_proba&quot;&quot;&quot;
    390         if check_input:
--&gt; 391             X = check_array(X, dtype=DTYPE, accept_sparse=&quot;csr&quot;)
    392             if issparse(X) and (X.indices.dtype != np.intc or
    393                                 X.indptr.dtype != np.intc):

~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    548                              &quot; minimum of %d is required%s.&quot;
    549                              % (n_samples, array.shape, ensure_min_samples,
--&gt; 550                                 context))
    551 
    552     if ensure_min_features &gt; 0 and array.ndim == 2:

ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required.
</code></pre>
",52,0,0,2,python;scikit-learn,2022-07-06 00:22:11,2022-07-06 00:22:11,2022-07-06 01:09:02,the traceback error is as follow,valueerror  found array with  sample s   shape       while a minimum of  is required,traceback error follow,valueerror found array sample shape minimum required,valueerror found array sample shape minimum requiredtraceback error follow,"['valueerror', 'found', 'array', 'sample', 'shape', 'minimum', 'requiredtraceback', 'error', 'follow']","['valueerror', 'found', 'array', 'sampl', 'shape', 'minimum', 'requiredtraceback', 'error', 'follow']"
121,131,131,19489272,72874070,How do I predict on Keras LSTM model?,"<p>I'm trying to implement time-series forecasting using an LSTM model. My input data are timestamps, where x is the start time of a cycle and y is the end time of a cycle. (Example: timestamps for <code>2021-12-31 19:16:14.337738</code> as start time and <code>2021-12-31 19:16:23.768758</code> for end time).</p>
<p>I've split my training set to be 80% of total data and am passing it in as parameters in model.fit(), where my model is defined as:</p>
<pre><code>model = Sequential()
model.add(LSTM(64, activation='relu', return_sequences=True, input_shape=(2, 2)))
model.add(BatchNormalization())
model.add(Dense(10))
model.compile(optimizer='adam', loss='mse', metrics=[&quot;accuracy&quot;])
</code></pre>
<p>Afterwards, I use <code>model.predict()</code> on my x-values (cycle start times) of my testing set which is 20% of my data. I am trying to obtain y-values (cycle end times) for the given x-values but all I'm getting are nans:</p>
<pre><code>yhat = model.predict(np.array(x_input))
print(yhat)
</code></pre>
<p>Output:</p>
<pre><code>[[[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 ...

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]

 [[nan nan nan ... nan nan nan]
  [nan nan nan ... nan nan nan]]]
</code></pre>
<p>How do I get the model to predict the y-values I need and not nans? I'm very new to machine learning and have never worked with it before so I'm not really sure what I'm doing or where I'm going wrong.</p>
<p>Thank you for any help!</p>
",19,0,0,3,python;dataframe;nan,2022-07-06 00:03:11,2022-07-06 00:03:11,2022-07-06 00:06:31,i m trying to implement time series forecasting using an lstm model  my input data are timestamps  where x is the start time of a cycle and y is the end time of a cycle   example  timestamps for        as start time and        for end time   i ve split my training set to be   of total data and am passing it in as parameters in model fit    where my model is defined as  afterwards  i use model predict   on my x values  cycle start times  of my testing set which is   of my data  i am trying to obtain y values  cycle end times  for the given x values but all i m getting are nans  output  how do i get the model to predict the y values i need and not nans  i m very new to machine learning and have never worked with it before so i m not really sure what i m doing or where i m going wrong  thank you for any help ,how do i predict on keras lstm model ,trying implement time series forecasting using lstm model input data timestamps x start time cycle end time cycle example timestamps start time end time split training set total data passing parameters model fit model defined afterwards use model predict x values cycle start times testing set data trying obtain values cycle end times given x values getting nans output get model predict values need nans machine learning never worked really sure going wrong thank help,predict keras lstm model,predict keras lstm modeltrying implement time series forecasting using lstm model input data timestamps x start time cycle end time cycle example timestamps start time end time split training set total data passing parameters model fit model defined afterwards use model predict x values cycle start times testing set data trying obtain values cycle end times given x values getting nans output get model predict values need nans machine learning never worked really sure going wrong thank help,"['predict', 'keras', 'lstm', 'modeltrying', 'implement', 'time', 'series', 'forecasting', 'using', 'lstm', 'model', 'input', 'data', 'timestamps', 'x', 'start', 'time', 'cycle', 'end', 'time', 'cycle', 'example', 'timestamps', 'start', 'time', 'end', 'time', 'split', 'training', 'set', 'total', 'data', 'passing', 'parameters', 'model', 'fit', 'model', 'defined', 'afterwards', 'use', 'model', 'predict', 'x', 'values', 'cycle', 'start', 'times', 'testing', 'set', 'data', 'trying', 'obtain', 'values', 'cycle', 'end', 'times', 'given', 'x', 'values', 'getting', 'nans', 'output', 'get', 'model', 'predict', 'values', 'need', 'nans', 'machine', 'learning', 'never', 'worked', 'really', 'sure', 'going', 'wrong', 'thank', 'help']","['predict', 'kera', 'lstm', 'modeltri', 'implement', 'time', 'seri', 'forecast', 'use', 'lstm', 'model', 'input', 'data', 'timestamp', 'x', 'start', 'time', 'cycl', 'end', 'time', 'cycl', 'exampl', 'timestamp', 'start', 'time', 'end', 'time', 'split', 'train', 'set', 'total', 'data', 'pass', 'paramet', 'model', 'fit', 'model', 'defin', 'afterward', 'use', 'model', 'predict', 'x', 'valu', 'cycl', 'start', 'time', 'test', 'set', 'data', 'tri', 'obtain', 'valu', 'cycl', 'end', 'time', 'given', 'x', 'valu', 'get', 'nan', 'output', 'get', 'model', 'predict', 'valu', 'need', 'nan', 'machin', 'learn', 'never', 'work', 'realli', 'sure', 'go', 'wrong', 'thank', 'help']"
122,132,132,9749124,72872978,Split text into sentences without NLTK,"<p>I want to split large text into sentences.
I know how to do that with NLTK but I do not know how to do that without it.</p>
<p>This is my text, it has 8 sentences:</p>
<pre><code>import re
import nltk

text = &quot;&quot;&quot;Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. 
        It is seen as a part of artificial intelligence. 
        Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. 
        Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. 
        A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. 
        The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. 
        Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. As of 2020, many sources continue to assert that ML remains a subfield of AI. 
        Others have the view that not all ML is part of AI, but only an 'intelligent subset' of ML should be considered AI.&quot;&quot;&quot;


sent_num = len(re.split(&quot;(?&lt;=[^A-Z].[.?])(\s|\n)+(?=[A-Z])&quot;, text))
print(&quot;Number of sentences with regex:&quot;, sent_num)  #15

sent_num = len(nltk.sent_tokenize(text))
print(&quot;Number of sentences with NLTK:&quot;, sent_num)  #8
</code></pre>
<p>I have wrote a regex that can split text based on condition:
If word ends with punctuation (.!?) and if there is empty space or new line after punctuation and if word after empty space has first capital letter, then split it.</p>
<p>But Im getting bad results, NLTK gives 8 (correct), and my regex gives 15 instead of 8.</p>
",35,2,1,1,python,2022-07-05 22:18:23,2022-07-05 22:18:23,2022-07-05 23:13:14,this is my text  it has  sentences  but im getting bad results  nltk gives   correct   and my regex gives  instead of  ,split text into sentences without nltk,text sentences im getting bad results nltk gives correct regex gives instead,split text sentences without nltk,split text sentences without nltktext sentences im getting bad results nltk gives correct regex gives instead,"['split', 'text', 'sentences', 'without', 'nltktext', 'sentences', 'im', 'getting', 'bad', 'results', 'nltk', 'gives', 'correct', 'regex', 'gives', 'instead']","['split', 'text', 'sentenc', 'without', 'nltktext', 'sentenc', 'im', 'get', 'bad', 'result', 'nltk', 'give', 'correct', 'regex', 'give', 'instead']"
123,133,133,19488951,72873412,How to solve error of regexflag object not subscriptable,"<p>It's a machine learning python error where imputer.fit is giving error. I tried importing re and all but still showing error.<a href=""https://i.stack.imgur.com/z4XXP.jpg"" rel=""nofollow noreferrer"">this is what error and not able to resolve since so long</a></p>
",13,0,0,4,machine-learning;data-science;sklearn-pandas;imputation,2022-07-05 22:58:39,2022-07-05 22:58:39,2022-07-05 22:58:39,it s a machine learning python error where imputer fit is giving error  i tried importing re and all but still showing error ,how to solve error of regexflag object not subscriptable,machine learning python error imputer fit giving error tried importing still showing error,solve error regexflag object subscriptable,solve error regexflag object subscriptablemachine learning python error imputer fit giving error tried importing still showing error,"['solve', 'error', 'regexflag', 'object', 'subscriptablemachine', 'learning', 'python', 'error', 'imputer', 'fit', 'giving', 'error', 'tried', 'importing', 'still', 'showing', 'error']","['solv', 'error', 'regexflag', 'object', 'subscriptablemachin', 'learn', 'python', 'error', 'imput', 'fit', 'give', 'error', 'tri', 'import', 'still', 'show', 'error']"
124,134,134,12283890,72873088,ModuleNotFoundError: No module named &#39;tensorflow&#39; Command skipped related to Databrics notebook (Azure databrics labs),"<blockquote>
<p>This issue is mainly related to &quot;Databrics notebook&quot; provided for Azure machine learning lab notebooks</p>
</blockquote>
",7,1,-1,2,tensorflow;jupyter-notebook,2022-07-05 22:29:37,2022-07-05 22:29:37,2022-07-05 22:29:37,this issue is mainly related to  databrics notebook  provided for azure machine learning lab notebooks,modulenotfounderror  no module named    tensorflow    command skipped related to databrics notebook  azure databrics labs ,issue mainly related databrics notebook provided azure machine learning lab notebooks,modulenotfounderror module named tensorflow command skipped related databrics notebook azure databrics labs,modulenotfounderror module named tensorflow command skipped related databrics notebook azure databrics labsissue mainly related databrics notebook provided azure machine learning lab notebooks,"['modulenotfounderror', 'module', 'named', 'tensorflow', 'command', 'skipped', 'related', 'databrics', 'notebook', 'azure', 'databrics', 'labsissue', 'mainly', 'related', 'databrics', 'notebook', 'provided', 'azure', 'machine', 'learning', 'lab', 'notebooks']","['modulenotfounderror', 'modul', 'name', 'tensorflow', 'command', 'skip', 'relat', 'databr', 'notebook', 'azur', 'databr', 'labsissu', 'mainli', 'relat', 'databr', 'notebook', 'provid', 'azur', 'machin', 'learn', 'lab', 'notebook']"
125,135,135,15673855,72332126,Variable input and output size for Keras,"<p>Before I start, I am quite new to Keras and machine learning. I know the theory quite well but the syntax less so.</p>
<p>I am trying to create a reinforcement learning neural network using Keras. The problem to be solved is essentially the travelling salesman problem. The problem is, is that the network is fed in its location and the environment, which is a randomly created network of points such as [[0,5],[30,17],[19,83]..., and as the agent travels through this network, it changes as a point cannot be visited again. So if the agent goes from [0,0] to [0,5] then [30,17], the input would look like [0,5],[30,17],[19,83] to [30,17],[19,83] to [19,83]. There is a similar issue with the output, which is just the index of the possible locations to move to. This means that there could be any number of outputs.</p>
<p>The size of the input is initially 100, and the output could also be anywhere between 0 and 100. Methods like padding the inputs with a number would not work as the network would be fed a location impossible to get to, and there is a similar problem with padding the output with a number - the network can just stay in the same position whilst 'moving' ([0,0] to [0,0] etc). The agent also has limited time, so even with filling with random numbers it could just travel to locations which don't actually exist which doesn't solve the problem at hand.</p>
<p>How would I dynamically change the input and output sizes? Is it even possible, and if not, how should it be done?</p>
<p>edit: code because someone wanted it. Quite unintelligible but in essence a class containing the actions able to be done, the input in the form of self.state, and the enviroment in self.point_space. Reward is calculated as the distance travelled at each step and when complete, the distance compared to a random loop. The more important thing is if i can change the input and output sizes.</p>
<pre><code>class GraphEnv(Env):
  def __init__(self):
      self.point_space = createpoints()
      self.action_space = Discrete(len(self.point_space))
      self.observation_space = self.point_space.copy()
      self.state = [[0,0]]
     
      for i in self.observation_space:
        self.state.append(i)
      self.length = len(self.point_space)
      self.totallen = 0
      self.unchangedpoint_space = self.point_space.copy()
  def step(self, action):
    oldstate = self.state[0]
    self.state = []
    self.state.append(list(self.point_space[action-1]))
    try:
      del self.point_space[action-1]
    except:
      pass
    self.observation_space = self.point_space.copy()
    for i in self.observation_space:
        self.state.append(i)
    self.action_space = Discrete(len(self.point_space))
    #print(&quot;self.state = &quot;, self.state)

    reward = int(-math.sqrt((oldstate[0] - self.state[0][0])**2 + (oldstate[1] - self.state[0][1])**2))
    self.totallen += reward

    self.length -= 1
    if self.length &lt;= 0:
      #print(&quot;unchanged =&quot;, self.unchangedpoint_space)
      randomscore = scoreforrandom(self.unchangedpoint_space)
      reward = self.totallen - randomscore
      #print(&quot;totallen =&quot;,self.totallen)
      #print(&quot;randomscore =&quot;, randomscore)
      #print(&quot;reward&quot;, reward)
      done = True
    else:
      done = False


    info = {}

    return(self.state,reward,done,info)
  def render(self):
    
    pass
  def reset(self):
    self.state = [[0,0]]
    self.length = 100
    self.point_space = createpoints()
    self.observation_space = self.point_space.copy()
    self.state.append(self.observation_space)
    self.unchangedpoint_space = self.point_space.copy()
    #print(&quot;unchanged on init&quot;, self.unchangedpoint_space)
    self.action_space = Discrete(len(self.point_space))
    self.totallen = 0
    pass
</code></pre>
<p>The video i used as help: <a href=""https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte</a></p>
",51,1,0,5,machine-learning;keras;deep-learning;neural-network;reinforcement-learning,2022-05-21 23:41:40,2022-05-21 23:41:40,2022-07-05 22:04:32,before i start  i am quite new to keras and machine learning  i know the theory quite well but the syntax less so  i am trying to create a reinforcement learning neural network using keras  the problem to be solved is essentially the travelling salesman problem  the problem is  is that the network is fed in its location and the environment  which is a randomly created network of points such as                  and as the agent travels through this network  it changes as a point cannot be visited again  so if the agent goes from     to     then      the input would look like             to         to      there is a similar issue with the output  which is just the index of the possible locations to move to  this means that there could be any number of outputs  the size of the input is initially   and the output could also be anywhere between  and   methods like padding the inputs with a number would not work as the network would be fed a location impossible to get to  and there is a similar problem with padding the output with a number   the network can just stay in the same position whilst  moving       to     etc   the agent also has limited time  so even with filling with random numbers it could just travel to locations which don t actually exist which doesn t solve the problem at hand  how would i dynamically change the input and output sizes  is it even possible  and if not  how should it be done  edit  code because someone wanted it  quite unintelligible but in essence a class containing the actions able to be done  the input in the form of self state  and the enviroment in self point_space  reward is calculated as the distance travelled at each step and when complete  the distance compared to a random loop  the more important thing is if i can change the input and output sizes  the video i used as help  ,variable input and output size for keras,start quite keras machine learning know theory quite well syntax less trying create reinforcement learning neural network using keras problem solved essentially travelling salesman problem problem network fed location environment randomly created network points agent travels network changes point cannot visited agent goes input would look like similar issue output index possible locations move means could number outputs size input initially output could also anywhere methods like padding inputs number would work network would fed location impossible get similar problem padding output number network stay position whilst moving etc agent also limited time even filling random numbers could travel locations actually exist solve problem hand would dynamically change input output sizes even possible done edit code someone wanted quite unintelligible essence class containing actions able done input form self state enviroment self point_space reward calculated distance travelled step complete distance compared random loop important thing change input output sizes video used help,variable input output size keras,variable input output size kerasstart quite keras machine learning know theory quite well syntax less trying create reinforcement learning neural network using keras problem solved essentially travelling salesman problem problem network fed location environment randomly created network points agent travels network changes point cannot visited agent goes input would look like similar issue output index possible locations move means could number outputs size input initially output could also anywhere methods like padding inputs number would work network would fed location impossible get similar problem padding output number network stay position whilst moving etc agent also limited time even filling random numbers could travel locations actually exist solve problem hand would dynamically change input output sizes even possible done edit code someone wanted quite unintelligible essence class containing actions able done input form self state enviroment self point_space reward calculated distance travelled step complete distance compared random loop important thing change input output sizes video used help,"['variable', 'input', 'output', 'size', 'kerasstart', 'quite', 'keras', 'machine', 'learning', 'know', 'theory', 'quite', 'well', 'syntax', 'less', 'trying', 'create', 'reinforcement', 'learning', 'neural', 'network', 'using', 'keras', 'problem', 'solved', 'essentially', 'travelling', 'salesman', 'problem', 'problem', 'network', 'fed', 'location', 'environment', 'randomly', 'created', 'network', 'points', 'agent', 'travels', 'network', 'changes', 'point', 'can', 'not', 'visited', 'agent', 'goes', 'input', 'would', 'look', 'like', 'similar', 'issue', 'output', 'index', 'possible', 'locations', 'move', 'means', 'could', 'number', 'outputs', 'size', 'input', 'initially', 'output', 'could', 'also', 'anywhere', 'methods', 'like', 'padding', 'inputs', 'number', 'would', 'work', 'network', 'would', 'fed', 'location', 'impossible', 'get', 'similar', 'problem', 'padding', 'output', 'number', 'network', 'stay', 'position', 'whilst', 'moving', 'etc', 'agent', 'also', 'limited', 'time', 'even', 'filling', 'random', 'numbers', 'could', 'travel', 'locations', 'actually', 'exist', 'solve', 'problem', 'hand', 'would', 'dynamically', 'change', 'input', 'output', 'sizes', 'even', 'possible', 'done', 'edit', 'code', 'someone', 'wanted', 'quite', 'unintelligible', 'essence', 'class', 'containing', 'actions', 'able', 'done', 'input', 'form', 'self', 'state', 'enviroment', 'self', 'point_space', 'reward', 'calculated', 'distance', 'travelled', 'step', 'complete', 'distance', 'compared', 'random', 'loop', 'important', 'thing', 'change', 'input', 'output', 'sizes', 'video', 'used', 'help']","['variabl', 'input', 'output', 'size', 'kerasstart', 'quit', 'kera', 'machin', 'learn', 'know', 'theori', 'quit', 'well', 'syntax', 'less', 'tri', 'creat', 'reinforc', 'learn', 'neural', 'network', 'use', 'kera', 'problem', 'solv', 'essenti', 'travel', 'salesman', 'problem', 'problem', 'network', 'fed', 'locat', 'environ', 'randomli', 'creat', 'network', 'point', 'agent', 'travel', 'network', 'chang', 'point', 'can', 'not', 'visit', 'agent', 'goe', 'input', 'would', 'look', 'like', 'similar', 'issu', 'output', 'index', 'possibl', 'locat', 'move', 'mean', 'could', 'number', 'output', 'size', 'input', 'initi', 'output', 'could', 'also', 'anywher', 'method', 'like', 'pad', 'input', 'number', 'would', 'work', 'network', 'would', 'fed', 'locat', 'imposs', 'get', 'similar', 'problem', 'pad', 'output', 'number', 'network', 'stay', 'posit', 'whilst', 'move', 'etc', 'agent', 'also', 'limit', 'time', 'even', 'fill', 'random', 'number', 'could', 'travel', 'locat', 'actual', 'exist', 'solv', 'problem', 'hand', 'would', 'dynam', 'chang', 'input', 'output', 'size', 'even', 'possibl', 'done', 'edit', 'code', 'someon', 'want', 'quit', 'unintellig', 'essenc', 'class', 'contain', 'action', 'abl', 'done', 'input', 'form', 'self', 'state', 'enviro', 'self', 'point_spac', 'reward', 'calcul', 'distanc', 'travel', 'step', 'complet', 'distanc', 'compar', 'random', 'loop', 'import', 'thing', 'chang', 'input', 'output', 'size', 'video', 'use', 'help']"
126,136,136,15319497,72871434,how can i edit a local html file via the browser such as how jupyter notebooks work,"<p>So i have a bunch of data and i need to loop through it all and label it. the data is Natural language that needs labels for Machine learning. so i wanted to build a template with all the possible labels as javascript buttons so that as i loop through the data i can click the label and have the html save permanently to the html file. ive looked at chrome work station but you still have to edit the file in a side panel. downloading the file seems like a way but i would still like to know if i can do it my way. The only good example i have found is such as jupyter notebooks. on there you load the browser from terminal and then in the browser you can edit the file, add files, directories, save the file and even run python so i know its possible i just dont know how.</p>
<pre><code>&lt;!DOCTYPE HTML&gt;

&lt;html&gt;
&lt;head&gt;
&lt;meta content=&quot;text/html;charset=utf-8&quot; http-equiv=&quot;content-type&quot;/&gt;
&lt;title&gt;Title here&lt;/title&gt;
&lt;style&gt;
    p.customer{color: red;}
    p.assitant{color: black;  }
    p.tech{color: blue; }
    p.fixed { font-size: 20px;}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;p id=&quot;demo&quot;&gt;&lt;/p&gt;
&lt;p class=&quot;customer&quot;&gt;customer says they have a problem&lt;/p&gt;
&lt;p class=&quot;assitant&quot;&gt;what have you tried so far&lt;/p&gt;
&lt;p class=&quot;tech&quot;&gt;thats going to cost 100 dollars to fix&lt;/p&gt;
&lt;p class=&quot;customer&quot;&gt;Customer Text Here&lt;/p&gt;
&lt;p class=&quot;customer&quot;&gt;Customer Text Here&lt;/p&gt;
&lt;p class=&quot;customer&quot;&gt;Customer make us rich&lt;/p&gt;
&lt;p class=&quot;customer fixed&quot;&gt;Customer make us&lt;/p&gt;
&lt;p class=&quot;customer fixed&quot;&gt;Customer make us rich&lt;/p&gt;
&lt;button onclick=&quot;document.getElementById('demo').innerHTML = Date()&quot; type=&quot;button&quot;&gt;
click this button to label this issue as resolved&lt;/button&gt;
&lt;button onclick=&quot;document.getElementById('demo').innerHTML = Date()&quot; type=&quot;button&quot;&gt;
click this button to label this issue as not resolved&lt;/button&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>i want to be able to open this document in a browser(or something that can handle javascript) which would be stored on my machine. on clicking the javascript buttons the script would edit the html in some way and subsequently save the file to is original location.</p>
",17,0,0,3,python;html;jupyter-notebook,2022-07-05 20:20:30,2022-07-05 20:20:30,2022-07-05 22:03:55,so i have a bunch of data and i need to loop through it all and label it  the data is natural language that needs labels for machine learning  so i wanted to build a template with all the possible labels as javascript buttons so that as i loop through the data i can click the label and have the html save permanently to the html file  ive looked at chrome work station but you still have to edit the file in a side panel  downloading the file seems like a way but i would still like to know if i can do it my way  the only good example i have found is such as jupyter notebooks  on there you load the browser from terminal and then in the browser you can edit the file  add files  directories  save the file and even run python so i know its possible i just dont know how  i want to be able to open this document in a browser or something that can handle javascript  which would be stored on my machine  on clicking the javascript buttons the script would edit the html in some way and subsequently save the file to is original location ,how can i edit a local html file via the browser such as how jupyter notebooks work,bunch data need loop label data natural language needs labels machine learning wanted build template possible labels javascript buttons loop data click label html save permanently html file ive looked chrome work station still edit file side panel downloading file seems like way would still like know way good example found jupyter notebooks load browser terminal browser edit file files directories save file even run python know possible dont know want able open document browser something handle javascript would stored machine clicking javascript buttons script would edit html way subsequently save file original location,edit local html file via browser jupyter notebooks work,edit local html file via browser jupyter notebooks workbunch data need loop label data natural language needs labels machine learning wanted build template possible labels javascript buttons loop data click label html save permanently html file ive looked chrome work station still edit file side panel downloading file seems like way would still like know way good example found jupyter notebooks load browser terminal browser edit file files directories save file even run python know possible dont know want able open document browser something handle javascript would stored machine clicking javascript buttons script would edit html way subsequently save file original location,"['edit', 'local', 'html', 'file', 'via', 'browser', 'jupyter', 'notebooks', 'workbunch', 'data', 'need', 'loop', 'label', 'data', 'natural', 'language', 'needs', 'labels', 'machine', 'learning', 'wanted', 'build', 'template', 'possible', 'labels', 'javascript', 'buttons', 'loop', 'data', 'click', 'label', 'html', 'save', 'permanently', 'html', 'file', 'ive', 'looked', 'chrome', 'work', 'station', 'still', 'edit', 'file', 'side', 'panel', 'downloading', 'file', 'seems', 'like', 'way', 'would', 'still', 'like', 'know', 'way', 'good', 'example', 'found', 'jupyter', 'notebooks', 'load', 'browser', 'terminal', 'browser', 'edit', 'file', 'files', 'directories', 'save', 'file', 'even', 'run', 'python', 'know', 'possible', 'dont', 'know', 'want', 'able', 'open', 'document', 'browser', 'something', 'handle', 'javascript', 'would', 'stored', 'machine', 'clicking', 'javascript', 'buttons', 'script', 'would', 'edit', 'html', 'way', 'subsequently', 'save', 'file', 'original', 'location']","['edit', 'local', 'html', 'file', 'via', 'browser', 'jupyt', 'notebook', 'workbunch', 'data', 'need', 'loop', 'label', 'data', 'natur', 'languag', 'need', 'label', 'machin', 'learn', 'want', 'build', 'templat', 'possibl', 'label', 'javascript', 'button', 'loop', 'data', 'click', 'label', 'html', 'save', 'perman', 'html', 'file', 'ive', 'look', 'chrome', 'work', 'station', 'still', 'edit', 'file', 'side', 'panel', 'download', 'file', 'seem', 'like', 'way', 'would', 'still', 'like', 'know', 'way', 'good', 'exampl', 'found', 'jupyt', 'notebook', 'load', 'browser', 'termin', 'browser', 'edit', 'file', 'file', 'directori', 'save', 'file', 'even', 'run', 'python', 'know', 'possibl', 'dont', 'know', 'want', 'abl', 'open', 'document', 'browser', 'someth', 'handl', 'javascript', 'would', 'store', 'machin', 'click', 'javascript', 'button', 'script', 'would', 'edit', 'html', 'way', 'subsequ', 'save', 'file', 'origin', 'locat']"
127,137,137,14894805,72871669,"If you proceed with the bash command, the ec2 connection is disconnected, but if you go directly to the terminal and proceed, there is no problem","<p>I wrote the code to start ec2 locally using bash and run.
Below is a part of the code.</p>
<pre><code>ssh -i $AWS_KEY -t ubuntu@$INSTANCE_DNS 'cd /home/ubuntu/Hardware-Data-Collect/&amp;&amp; sudo bash ./settings.sh &amp;&amp; sudo bash ./Run_gpu.sh'
</code></pre>
<p>It proceeds normally until settings.sh is installed.</p>
<p>However, as soon as I run Run_gpu.sh to run machine learning, the ec2 connection is disconnected.</p>
<blockquote>
<p>Connection to ec2-18-236-69-227.us-west-2.compute.amazonaws.com closed.</p>
</blockquote>
<p>If i connect directly to ec2 with ssh and then execute the run_gpu.sh, there is no error.</p>
<p>I want to know why ec2 connection is disconnected in terminal (The instance is not terminated)</p>
",21,0,-2,5,bash;amazon-web-services;amazon-ec2;ssh;nohup,2022-07-05 20:35:41,2022-07-05 20:35:41,2022-07-05 20:35:41,it proceeds normally until settings sh is installed  however  as soon as i run run_gpu sh to run machine learning  the ec connection is disconnected  connection to ec     us west  compute amazonaws com closed  if i connect directly to ec with ssh and then execute the run_gpu sh  there is no error  i want to know why ec connection is disconnected in terminal  the instance is not terminated ,if you proceed with the bash command  the ec connection is disconnected  but if you go directly to the terminal and proceed  there is no problem,proceeds normally settings sh installed however soon run run_gpu sh run machine learning ec connection disconnected connection ec us west compute amazonaws com closed connect directly ec ssh execute run_gpu sh error want know ec connection disconnected terminal instance terminated,proceed bash command ec connection disconnected go directly terminal proceed problem,proceed bash command ec connection disconnected go directly terminal proceed problemproceeds normally settings sh installed however soon run run_gpu sh run machine learning ec connection disconnected connection ec us west compute amazonaws com closed connect directly ec ssh execute run_gpu sh error want know ec connection disconnected terminal instance terminated,"['proceed', 'bash', 'command', 'ec', 'connection', 'disconnected', 'go', 'directly', 'terminal', 'proceed', 'problemproceeds', 'normally', 'settings', 'sh', 'installed', 'however', 'soon', 'run', 'run_gpu', 'sh', 'run', 'machine', 'learning', 'ec', 'connection', 'disconnected', 'connection', 'ec', 'us', 'west', 'compute', 'amazonaws', 'com', 'closed', 'connect', 'directly', 'ec', 'ssh', 'execute', 'run_gpu', 'sh', 'error', 'want', 'know', 'ec', 'connection', 'disconnected', 'terminal', 'instance', 'terminated']","['proceed', 'bash', 'command', 'ec', 'connect', 'disconnect', 'go', 'directli', 'termin', 'proceed', 'problemproce', 'normal', 'set', 'sh', 'instal', 'howev', 'soon', 'run', 'run_gpu', 'sh', 'run', 'machin', 'learn', 'ec', 'connect', 'disconnect', 'connect', 'ec', 'us', 'west', 'comput', 'amazonaw', 'com', 'close', 'connect', 'directli', 'ec', 'ssh', 'execut', 'run_gpu', 'sh', 'error', 'want', 'know', 'ec', 'connect', 'disconnect', 'termin', 'instanc', 'termin']"
128,138,138,17260574,72871653,Create release pipeline/artifact for ML training (Azureml),"<p>I have the following pipeline which runs the machine learning training however I want to create a release pipeline. The pipeline should build an artifact which then the release pipeline should be able to deploy.</p>
<pre><code>trigger: 
  none
  
pool:
  vmImage: &quot;ubuntu-latest&quot;


variables:
  - group: variable group
    

steps:

  # Set the version of Python that the build agent should use
- task: UsePythonVersion@0
  displayName: Use Python 3.8
  inputs:
    versionSpec: '3.8'
    addToPath: true 

  # Run Pip installer for any prerequisite libraries
- bash: pip3 install -r requirements.txt
  displayName: Install pip package requirements
  workingDirectory: workingDirectory/Scripts


- task: AzureCLI@2
  inputs:
    azureSubscription: 'azureSubscription:'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: 'python run-training.py'
    workingDirectory: 'workingDirectory/Scripts'
</code></pre>
<p>Use the following as research but still not sure how to do this:
<a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/build-artifacts?view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/build-artifacts?view=azure-devops&amp;tabs=yaml</a>
<a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/publish-build-artifacts?view=azure-devops"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/publish-build-artifacts?view=azure-devops</a></p>
",15,0,-1,4,azure-pipelines;azure-pipelines-release-pipeline;mlops;azureml-python-sdk,2022-07-05 20:34:31,2022-07-05 20:34:31,2022-07-05 20:34:31,i have the following pipeline which runs the machine learning training however i want to create a release pipeline  the pipeline should build an artifact which then the release pipeline should be able to deploy ,create release pipeline artifact for ml training  azureml ,following pipeline runs machine learning training however want create release pipeline pipeline build artifact release pipeline able deploy,create release pipeline artifact ml training azureml,create release pipeline artifact ml training azuremlfollowing pipeline runs machine learning training however want create release pipeline pipeline build artifact release pipeline able deploy,"['create', 'release', 'pipeline', 'artifact', 'ml', 'training', 'azuremlfollowing', 'pipeline', 'runs', 'machine', 'learning', 'training', 'however', 'want', 'create', 'release', 'pipeline', 'pipeline', 'build', 'artifact', 'release', 'pipeline', 'able', 'deploy']","['creat', 'releas', 'pipelin', 'artifact', 'ml', 'train', 'azuremlfollow', 'pipelin', 'run', 'machin', 'learn', 'train', 'howev', 'want', 'creat', 'releas', 'pipelin', 'pipelin', 'build', 'artifact', 'releas', 'pipelin', 'abl', 'deploy']"
129,139,139,16770626,72865453,How do you display data when you have multiple machine learning models,"<p>I have developed a program that finds the probabilities of around 500 classes based on some training data that involves a few thousand features.</p>
<p>It works by training about 500 logistic regression models that take these few thousand features and find the probability of a single class. Each model finds the probability for a different class, so I am able to find to find the probability for each of the classes.</p>
<p>Since these are all different models, I have been able to find the accuracy for each model and have the mean accuracy by averaging all of these values.</p>
<p>My problem is that right now I have these 500 or so data points and the average and I don't really know how to represent them graphically. I can't really plot them with a line graph since there isn't much relation between the classes and the ROC curves don't work since this isn't a binary classification.</p>
<p>Does anyone have any suggestions on ways I can graph this data? Thank you!</p>
",33,1,-2,4,python;machine-learning;graph;artificial-intelligence,2022-07-05 12:52:36,2022-07-05 12:52:36,2022-07-05 19:03:58,i have developed a program that finds the probabilities of around  classes based on some training data that involves a few thousand features  it works by training about  logistic regression models that take these few thousand features and find the probability of a single class  each model finds the probability for a different class  so i am able to find to find the probability for each of the classes  since these are all different models  i have been able to find the accuracy for each model and have the mean accuracy by averaging all of these values  my problem is that right now i have these  or so data points and the average and i don t really know how to represent them graphically  i can t really plot them with a line graph since there isn t much relation between the classes and the roc curves don t work since this isn t a binary classification  does anyone have any suggestions on ways i can graph this data  thank you ,how do you display data when you have multiple machine learning models,developed program finds probabilities around classes based training data involves thousand features works training logistic regression models take thousand features find probability single class model finds probability different class able find find probability classes since different models able find accuracy model mean accuracy averaging values problem right data points average really know represent graphically really plot line graph since much relation classes roc curves work since binary classification anyone suggestions ways graph data thank,display data multiple machine learning models,display data multiple machine learning modelsdeveloped program finds probabilities around classes based training data involves thousand features works training logistic regression models take thousand features find probability single class model finds probability different class able find find probability classes since different models able find accuracy model mean accuracy averaging values problem right data points average really know represent graphically really plot line graph since much relation classes roc curves work since binary classification anyone suggestions ways graph data thank,"['display', 'data', 'multiple', 'machine', 'learning', 'modelsdeveloped', 'program', 'finds', 'probabilities', 'around', 'classes', 'based', 'training', 'data', 'involves', 'thousand', 'features', 'works', 'training', 'logistic', 'regression', 'models', 'take', 'thousand', 'features', 'find', 'probability', 'single', 'class', 'model', 'finds', 'probability', 'different', 'class', 'able', 'find', 'find', 'probability', 'classes', 'since', 'different', 'models', 'able', 'find', 'accuracy', 'model', 'mean', 'accuracy', 'averaging', 'values', 'problem', 'right', 'data', 'points', 'average', 'really', 'know', 'represent', 'graphically', 'really', 'plot', 'line', 'graph', 'since', 'much', 'relation', 'classes', 'roc', 'curves', 'work', 'since', 'binary', 'classification', 'anyone', 'suggestions', 'ways', 'graph', 'data', 'thank']","['display', 'data', 'multipl', 'machin', 'learn', 'modelsdevelop', 'program', 'find', 'probabl', 'around', 'class', 'base', 'train', 'data', 'involv', 'thousand', 'featur', 'work', 'train', 'logist', 'regress', 'model', 'take', 'thousand', 'featur', 'find', 'probabl', 'singl', 'class', 'model', 'find', 'probabl', 'differ', 'class', 'abl', 'find', 'find', 'probabl', 'class', 'sinc', 'differ', 'model', 'abl', 'find', 'accuraci', 'model', 'mean', 'accuraci', 'averag', 'valu', 'problem', 'right', 'data', 'point', 'averag', 'realli', 'know', 'repres', 'graphic', 'realli', 'plot', 'line', 'graph', 'sinc', 'much', 'relat', 'class', 'roc', 'curv', 'work', 'sinc', 'binari', 'classif', 'anyon', 'suggest', 'way', 'graph', 'data', 'thank']"
130,140,140,13990977,72831076,How can I use a sequence of numbers to predict a single number in Tensorflow?,"<p>I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.</p>
<p>You can imagine my dataset to look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>x data</th>
<th>y data</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>np.ndarray(shape (1209278,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>1</td>
<td><code>np.ndarray(shape (1211140,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>2</td>
<td><code>np.ndarray(shape (1418411,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>3</td>
<td><code>np.ndarray(shape (1077132,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p><strong>This was my first attempt:</strong></p>
<p>I tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata, so something like this:</p>
<pre class=""lang-py prettyprint-override""><code>array([
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
])
</code></pre>
<p>My y data is a numpy ndarray containing floats, which looks something like this</p>
<pre class=""lang-py prettyprint-override""><code>array([2.9864411, 3.0562437, ... , 2.7750807, 2.8712902], dtype=float32)
</code></pre>
<p>But when I tried to train the model using <code>model.fit()</code> it yields this error:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>
<p>I was able to solve this error by asking a question related to this:
<a href=""https://stackoverflow.com/questions/72815591/how-can-i-have-a-series-of-numpy-ndarrays-as-the-input-data-to-train-a-tensorflo"">How can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?</a></p>
<p><strong>My latest attempt:</strong>
Because Tensorflow does not seem to be able to convert a ndarray of ndarrays to a tensor, I tried to convert my x data to a list of ndarrays like this:</p>
<pre class=""lang-py prettyprint-override""><code>[
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
]
</code></pre>
<p>I left my y data untouched, so as a ndarray of floats.
Sadly my attempt of using a list of ndarrays instead of a ndarray of ndarrays yielded this error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Data cardinality is ambiguous:
  x sizes: 1304593, 1209278, 1407624, ...
  y sizes: 46
Make sure all arrays contain the same number of samples.
</code></pre>
<p>As you can see, my x data consists of arrays which all have a different shape.
But I don't think that this should be a problem.</p>
<p><strong>Question:</strong></p>
<p>My guess is that Tensorflow tries to use my list of arrays as <em>multiple</em> inputs.
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">Tensorflow fit() documentation</a></p>
<p>But I don't want to use my x data as multiple inputs.
Easily said I just want my model to predict a number from a sequence of numbers.
For example like this:</p>
<ul>
<li>array([3.59280851, 3.60459062, 3.60459062, ...]) =&gt; 2.8989773</li>
<li>array([3.54752101, 3.56740332, 3.56740332, ...]) =&gt; 3.0893357</li>
<li>...</li>
</ul>
<p><strong>How can I use a sequence of numbers to predict a single number in Tensorflow?</strong></p>
<p><strong>EDIT</strong>
Maybe I should have added that I want to use a RNN, especially a LSTM.
I have had a look at the Keras documentation, and in their simplest example they are using a <code>Embedding</code> layer. But I don't really know what to do.</p>
<p>All in all I think that my question ist pretty general and should be easy to answer if you know how to tackle this problem, unlike me.
Thanks in advance!</p>
",111,1,5,5,python;arrays;numpy;tensorflow;keras,2022-07-01 20:23:16,2022-07-01 20:23:16,2022-07-05 18:51:44,i am trying to build a machine learning model which predicts a single number from a series of numbers  i am using a sequential model from the keras api of tensorflow  you can imagine my dataset to look something like this  this was my first attempt  i tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata  so something like this  my y data is a numpy ndarray containing floats  which looks something like this but when i tried to train the model using model fit   it yields this error  question  how can i use a sequence of numbers to predict a single number in tensorflow ,how can i use a sequence of numbers to predict a single number in tensorflow ,trying build machine learning model predicts single number series numbers using sequential model keras api tensorflow imagine dataset look something like first attempt tried using numpy ndarray contains numpy ndarrays finally contain floats xdata something like data numpy ndarray containing floats looks something like tried train model using model fit yields error question use sequence numbers predict single number tensorflow,use sequence numbers predict single number tensorflow,use sequence numbers predict single number tensorflowtrying build machine learning model predicts single number series numbers using sequential model keras api tensorflow imagine dataset look something like first attempt tried using numpy ndarray contains numpy ndarrays finally contain floats xdata something like data numpy ndarray containing floats looks something like tried train model using model fit yields error question use sequence numbers predict single number tensorflow,"['use', 'sequence', 'numbers', 'predict', 'single', 'number', 'tensorflowtrying', 'build', 'machine', 'learning', 'model', 'predicts', 'single', 'number', 'series', 'numbers', 'using', 'sequential', 'model', 'keras', 'api', 'tensorflow', 'imagine', 'dataset', 'look', 'something', 'like', 'first', 'attempt', 'tried', 'using', 'numpy', 'ndarray', 'contains', 'numpy', 'ndarrays', 'finally', 'contain', 'floats', 'xdata', 'something', 'like', 'data', 'numpy', 'ndarray', 'containing', 'floats', 'looks', 'something', 'like', 'tried', 'train', 'model', 'using', 'model', 'fit', 'yields', 'error', 'question', 'use', 'sequence', 'numbers', 'predict', 'single', 'number', 'tensorflow']","['use', 'sequenc', 'number', 'predict', 'singl', 'number', 'tensorflowtri', 'build', 'machin', 'learn', 'model', 'predict', 'singl', 'number', 'seri', 'number', 'use', 'sequenti', 'model', 'kera', 'api', 'tensorflow', 'imagin', 'dataset', 'look', 'someth', 'like', 'first', 'attempt', 'tri', 'use', 'numpi', 'ndarray', 'contain', 'numpi', 'ndarray', 'final', 'contain', 'float', 'xdata', 'someth', 'like', 'data', 'numpi', 'ndarray', 'contain', 'float', 'look', 'someth', 'like', 'tri', 'train', 'model', 'use', 'model', 'fit', 'yield', 'error', 'question', 'use', 'sequenc', 'number', 'predict', 'singl', 'number', 'tensorflow']"
131,141,141,4903479,72806592,Model deployment failing in azure machine learning,"<p>I am following the procedure as described <a href=""https://www.analyticsvidhya.com/blog/2022/02/deploy-your-ml-model-as-a-web-service-in-microsoft-azure-cloud/"" rel=""nofollow noreferrer"">here</a>.<br />
I am trying to register models and deploy them in Azure machine learning. I have the following script:</p>
<pre class=""lang-py prettyprint-override""><code>    import pandas as pd
    import sklearn
    from sklearn.svm import SVC
    import pickle
    import joblib
    from sklearn.model_selection import train_test_split
    dataset = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;, header = None, names= colnames )
    dataset = dataset.replace({&quot;class&quot;:  {&quot;Iris-setosa&quot;:1,&quot;Iris-versicolor&quot;:2, &quot;Iris-virginica&quot;:3}})
    X = dataset.drop(['class'], axis=1)[:,0]
    y = dataset['class']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
    classifier = SVC(kernel = 'linear', random_state = 0)
    #Fit the model on training data
    classifier.fit(X_train, y_train)
    #Make the prediction
    y_pred = classifier.predict(X_test)
    ## Save as a pickle file
    filename= 'final_mod_v1.pkl'
    joblib.dump(classifier,open(filename, 'wb'))
</code></pre>
<p>In the models tab I registered the model. Then I tried to deploy the model as web service. The following is the scoring script file.</p>
<pre class=""lang-py prettyprint-override""><code>    import json
    import numpy as np
    import os
    import pickle
    import joblib
    from sklearn.svm import SVC
    from azureml.core import Model
    
    def init():
        global model
        model_name = 'classifier'
        path = Model.get_model_path(model_name)
        model = joblib.load(path)
    
    def run(data):
        try:
            data = json.loads(data)
            result = model.predict(data['data'])
            return {'data' : result.tolist() , 'message' : &quot;Successfully classified Iris&quot;}
    
        except Exception as e:
            error = str(e)
            return {'data' : error , 'message' : 'Failed to classify iris'}
</code></pre>
<p>The following is the <strong>conda_dependencies.yml</strong>:</p>
<pre><code>channels:
- anaconda
- conda-forge
dependencies:
- python=3.6.2
- pip:
  - pandas==1.1.5
  - azureml-defaults
  - joblib==0.17.0
- scikit-learn==0.23.2
name: azureml_2d0fd20031db3baaed8684d5f08fe619
</code></pre>
<p>I am confused about the last line in the above script <code>azureml_2d0fd20031db3baaed8684d5f08fe619</code></p>
<pre><code> name: azureml_2d0fd20031db3baaed8684d5f08fe619
</code></pre>
<p>The deployment is failing. Deployment log shows:</p>
<pre><code> container &quot;classifier&quot; in pod &quot;wk-caas-9a4c565844b043cfa9d8ba246af11ff5-517e6d8f74175a01ffc43147e5dd8133-pod&quot; is waiting to start: PodInitializing
</code></pre>
<p>It would be helpful if I can get a guidance on this.</p>
",70,0,0,4,azure;machine-learning;azure-devops;azure-web-app-service,2022-06-30 00:34:34,2022-06-30 00:34:34,2022-07-05 18:09:59,in the models tab i registered the model  then i tried to deploy the model as web service  the following is the scoring script file  the following is the conda_dependencies yml  i am confused about the last line in the above script azureml_dfddbbaaeddffe the deployment is failing  deployment log shows  it would be helpful if i can get a guidance on this ,model deployment failing in azure machine learning,models tab registered model tried deploy model web service following scoring script file following conda_dependencies yml confused last line script azureml_dfddbbaaeddffe deployment failing deployment log shows would helpful get guidance,model deployment failing azure machine learning,model deployment failing azure machine learningmodels tab registered model tried deploy model web service following scoring script file following conda_dependencies yml confused last line script azureml_dfddbbaaeddffe deployment failing deployment log shows would helpful get guidance,"['model', 'deployment', 'failing', 'azure', 'machine', 'learningmodels', 'tab', 'registered', 'model', 'tried', 'deploy', 'model', 'web', 'service', 'following', 'scoring', 'script', 'file', 'following', 'conda_dependencies', 'yml', 'confused', 'last', 'line', 'script', 'azureml_dfddbbaaeddffe', 'deployment', 'failing', 'deployment', 'log', 'shows', 'would', 'helpful', 'get', 'guidance']","['model', 'deploy', 'fail', 'azur', 'machin', 'learningmodel', 'tab', 'regist', 'model', 'tri', 'deploy', 'model', 'web', 'servic', 'follow', 'score', 'script', 'file', 'follow', 'conda_depend', 'yml', 'confus', 'last', 'line', 'script', 'azureml_dfddbbaaeddff', 'deploy', 'fail', 'deploy', 'log', 'show', 'would', 'help', 'get', 'guidanc']"
132,142,142,4871597,67160576,YoloV5 killed at first epoch,"<p>I'm using a virtual machine on Windows 10 with this config:</p>
<pre><code>Memory 7.8 GiB
Processor Intel® Core™ i5-6600K CPU @ 3.50GHz × 3
Graphics llvmpipe (LLVM 11.0.0, 256 bits)
Disk Capcity 80.5 GB
OS Ubuntu 20.10 64 Bit
Virtualization Oracle
</code></pre>
<p>I installed docker for Ubuntu as described in <a href=""https://docs.docker.com/engine/install/ubuntu/"" rel=""nofollow noreferrer"">the official documentation</a>.<br>
I pulled the docker image as described on the <a href=""https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart"" rel=""nofollow noreferrer"">yolo github section for docker</a>.<br>
Since I have no NVIDIA GPU I could not install a driver or CUDA.
I pulled the aquarium from <a href=""https://public.roboflow.com/object-detection/aquarium"" rel=""nofollow noreferrer"">roboflow</a> and installed it on a folde aquarium.
I ran this command to start the image and have my aquarium folder mounted</p>
<pre><code>sudo docker run --ipc=host -it -v &quot;$(pwd)&quot;/Desktop/yolo/aquarium:/usr/src/app/aquarium ultralytics/yolov5:latest
</code></pre>
<p>And was greeted with this banner</p>
<blockquote>
<h1>=============
== PyTorch ==</h1>
<p>NVIDIA Release 21.03 (build 21060478) PyTorch Version 1.9.0a0+df837d0</p>
<p>Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights
reserved.</p>
<p>Copyright (c) 2014-2021 Facebook Inc. Copyright (c) 2011-2014 Idiap
Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind
Technologies    (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC
Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU<br />
(Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America
(Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright
(c) 2006      Idiap Research Institute (Samy Bengio) Copyright (c)
2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio,
Johnny Mariethoz) Copyright (c) 2015      Google Inc. Copyright (c)
2015      Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.</p>
<p>NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA
CORPORATION.  All rights reserved.</p>
<p>Various files include modifications (c) NVIDIA CORPORATION.  All
rights reserved.</p>
<p>This container image and its contents are governed by the NVIDIA Deep
Learning Container License. By pulling and using the container, you
accept the terms and conditions of this license:
<a href=""https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license"" rel=""nofollow noreferrer"">https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license</a></p>
<p>WARNING: The NVIDIA Driver was not detected.  GPU functionality will
not be available.    Use 'nvidia-docker run' to start this container;
see    <a href=""https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker</a> .</p>
<p>NOTE: MOFED driver for multi-node communication was not detected.
Multi-node communication performance may be reduced.</p>
</blockquote>
<p>So no error there.<br>
I installed pip and with pip wandb I added wandb. I used <code>wandb login</code> and set my API key.<br><br>
I ran following command:</p>
<pre><code># python train.py --img 640 --batch 16 --epochs 10 --data ./aquarium/data.yaml --weights yolov5s.pt --project ip5 --name aquarium5 --nosave --cache
</code></pre>
<p>And received this output:</p>
<pre><code>github: skipping check (Docker image)
YOLOv5 🚀 v5.0-14-g238583b torch 1.9.0a0+df837d0 CPU

Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='./aquarium/data.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='aquarium5', noautoanchor=False, nosave=True, notest=False, project='ip5', quad=False, rect=False, resume=False, save_dir='ip5/aquarium5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)
tensorboard: Start with 'tensorboard --logdir ip5', view at http://localhost:6006/
hyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0
wandb: Currently logged in as: pebs (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.10.26
wandb: Syncing run aquarium5
wandb: ⭐️ View project at https://wandb.ai/pebs/ip5
wandb: 🚀 View run at https://wandb.ai/pebs/ip5/runs/1c2j80ii
wandb: Run data is saved locally in /usr/src/app/wandb/run-20210419_102642-1c2j80ii
wandb: Run `wandb offline` to turn off syncing.

Overriding model.yaml nc=80 with nc=7

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        
  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.
Model Summary: 283 layers, 7079724 parameters, 7079724 gradients, 16.4 GFLOPS

Transferred 356/362 items from yolov5s.pt
Scaled weight_decay = 0.0005
Optimizer groups: 62 .bias, 62 conv.weight, 59 other
train: Scanning '/usr/src/app/aquarium/train/labels.cache' images and labels... 448 found, 0 missing, 1 empty, 0 corrupted: 100%|█| 448/448 [00:00&lt;?, ?
train: Caching images (0.4GB): 100%|████████████████████████████████████████████████████████████████████████████████| 448/448 [00:01&lt;00:00, 313.77it/s]
val: Scanning '/usr/src/app/aquarium/valid/labels.cache' images and labels... 127 found, 0 missing, 0 empty, 0 corrupted: 100%|█| 127/127 [00:00&lt;?, ?it
val: Caching images (0.1GB): 100%|██████████████████████████████████████████████████████████████████████████████████| 127/127 [00:00&lt;00:00, 141.31it/s]
Plotting labels... 

autoanchor: Analyzing anchors... anchors/target = 5.17, Best Possible Recall (BPR) = 0.9997
Image sizes 640 train, 640 test
Using 3 dataloader workers
Logging results to ip5/aquarium5
Starting training for 10 epochs...

     Epoch   gpu_mem       box       obj       cls     total    labels  img_size
  0%|                                                                                                                           | 0/28 [00:00&lt;?, ?it/s]Killed
root@cf40a6498016:~# /opt/conda/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
</code></pre>
<p>From this output I would think that there were 0 epochs completed.<br>
My data.yaml contains this code:</p>
<pre><code>train: /usr/src/app/aquarium/train/images
val: /usr/src/app/aquarium/valid/images

nc: 7
names: ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']
</code></pre>
<p><a href=""https://wandb.ai/"" rel=""nofollow noreferrer"">wandb.ai</a> does not display any metrics, but I have the files config.yaml, requirements.txt, wandb-metadata.json and wandb-summary.json.</p>
<p>Why am I not getting any output?<br>
Has there in fact be no training at all?<br>
If there was a training, how can I use my model?</p>
",1619,2,2,5,python;docker;pytorch;yolov5;wandb,2021-04-19 16:19:10,2021-04-19 16:19:10,2022-07-05 16:44:56,i m using a virtual machine on windows  with this config  and was greeted with this banner nvidia release    build   pytorch version   a dfd and received this output   does not display any metrics  but i have the files config yaml  requirements txt  wandb metadata json and wandb summary json ,yolov killed at first epoch,using virtual machine windows config greeted banner nvidia release build pytorch version dfd received output display metrics files config yaml requirements txt wandb metadata json wandb summary json,yolov killed first epoch,yolov killed first epochusing virtual machine windows config greeted banner nvidia release build pytorch version dfd received output display metrics files config yaml requirements txt wandb metadata json wandb summary json,"['yolov', 'killed', 'first', 'epochusing', 'virtual', 'machine', 'windows', 'config', 'greeted', 'banner', 'nvidia', 'release', 'build', 'pytorch', 'version', 'dfd', 'received', 'output', 'display', 'metrics', 'files', 'config', 'yaml', 'requirements', 'txt', 'wandb', 'metadata', 'json', 'wandb', 'summary', 'json']","['yolov', 'kill', 'first', 'epochus', 'virtual', 'machin', 'window', 'config', 'greet', 'banner', 'nvidia', 'releas', 'build', 'pytorch', 'version', 'dfd', 'receiv', 'output', 'display', 'metric', 'file', 'config', 'yaml', 'requir', 'txt', 'wandb', 'metadata', 'json', 'wandb', 'summari', 'json']"
133,143,143,45843,72867109,What is PyTorch Dataset supposed to return?,"<p>I'm trying to get PyTorch to work with DataLoader, this being said to be the easiest way to handle mini batches, which are in some cases necessary for best performance.</p>
<p>DataLoader wants a Dataset as input.</p>
<p>Most of the documentation on Dataset assumes you are working with an off-the-shelf standard data set e.g. MNIST, or at least with images, and can use existing machinery as a black box. I'm working with non-image data I'm generating myself. My best current attempt to distill the documentation about how to do that, down to a minimal test case, is:</p>
<pre><code>import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader


class Dataset1(Dataset):
    def __init__(self):
        pass

    def __len__(self):
        return 80

    def __getitem__(self, i):
        # actual data is blank, just to test the mechanics of Dataset
        return [0.0, 0.0, 0.0], 1.0


train_dataloader = DataLoader(Dataset1(), batch_size=8)

for X, y in train_dataloader:
    print(f&quot;X: {X}&quot;)
    print(f&quot;y: {y.shape} {y.dtype} {y}&quot;)
    break


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(3, 10),
            nn.ReLU(),
            nn.Linear(10, 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        return self.layers(x)


device = torch.device(&quot;cpu&quot;)
model = Net().to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

for epoch in range(10):
    for X, y in train_dataloader:
        X, y = X.to(device), y.to(device)

        pred = model(X)
        loss = criterion(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<p>The output of the above program is:</p>
<pre><code>X: [tensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)]
y: torch.Size([8]) torch.float64 tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)
Traceback (most recent call last):
  File &quot;C:\ml\test_dataloader.py&quot;, line 47, in &lt;module&gt;
    X, y = X.to(device), y.to(device)
AttributeError: 'list' object has no attribute 'to'
</code></pre>
<p>In all the example code I can find, <code>X, y = X.to(device), y.to(device)</code> succeeds, because <code>X</code> is indeed a tensor (whereas it is not in my version). Now I'm trying to find out what exactly converts <code>X</code> to a tensor, because either the example code e.g. <a href=""https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html</a> does not do so, or I am failing to understand how and where it does.</p>
<p>Does Dataset itself convert things to tensors? The answer seems to be 'sort of'.</p>
<p>It has converted <code>y</code> to a tensor, a column of the <code>y</code> value for every example in the batch. That much, makes sense, though it has used type float64, whereas in machine learning, we usually prefer float32. I am used to the idea that Python always represents scalars in double precision, so the conversion from double to single precision happens at the time of forming a tensor, and that this can be insured by specifying the <code>dtype</code> parameter. But in this case Dataset seems to have formed the tensor implicitly. Is there a place or way to specify the <code>dtype</code> parameter?</p>
<p><code>X</code> is not a tensor, but a list thereof. It would make intuitive sense if it were a list of the examples in the batch, but instead of a list of 8 elements each containing 3 elements, it's the other way around. So Dataset has transposed the input data, which would make sense if it is forming a tensor to match the shape of <code>y</code>, but instead of making a single 2d tensor, it has made a list of 1d tensors. (And, again, in double precision.) Why? Is there a way to change this behavior?</p>
<p>The answer posted so far to <a href=""https://stackoverflow.com/questions/67416496/does-pytorch-dataset-getitem-have-to-return-a-dict"">Does pytorch Dataset.__getitem__ have to return a dict?</a> says <code>__getitem__</code> can return anything. Okay, but then how does the anything get converted to the form the training procedure requires?</p>
",31,2,1,2,pytorch;pytorch-dataloader,2022-07-05 15:02:55,2022-07-05 15:02:55,2022-07-05 15:56:24,i m trying to get pytorch to work with dataloader  this being said to be the easiest way to handle mini batches  which are in some cases necessary for best performance  dataloader wants a dataset as input  most of the documentation on dataset assumes you are working with an off the shelf standard data set e g  mnist  or at least with images  and can use existing machinery as a black box  i m working with non image data i m generating myself  my best current attempt to distill the documentation about how to do that  down to a minimal test case  is  the output of the above program is  in all the example code i can find  x  y   x to device   y to device  succeeds  because x is indeed a tensor  whereas it is not in my version   now i m trying to find out what exactly converts x to a tensor  because either the example code e g   does not do so  or i am failing to understand how and where it does  does dataset itself convert things to tensors  the answer seems to be  sort of   it has converted y to a tensor  a column of the y value for every example in the batch  that much  makes sense  though it has used type float  whereas in machine learning  we usually prefer float  i am used to the idea that python always represents scalars in double precision  so the conversion from double to single precision happens at the time of forming a tensor  and that this can be insured by specifying the dtype parameter  but in this case dataset seems to have formed the tensor implicitly  is there a place or way to specify the dtype parameter  x is not a tensor  but a list thereof  it would make intuitive sense if it were a list of the examples in the batch  but instead of a list of  elements each containing  elements  it s the other way around  so dataset has transposed the input data  which would make sense if it is forming a tensor to match the shape of y  but instead of making a single d tensor  it has made a list of d tensors   and  again  in double precision   why  is there a way to change this behavior  the answer posted so far to  says __getitem__ can return anything  okay  but then how does the anything get converted to the form the training procedure requires ,what is pytorch dataset supposed to return ,trying get pytorch work dataloader said easiest way handle mini batches cases necessary best performance dataloader wants dataset input documentation dataset assumes working shelf standard data set e g mnist least images use existing machinery black box working non image data generating best current attempt distill documentation minimal test case output program example code find x x device device succeeds x indeed tensor whereas version trying find exactly converts x tensor either example code e g failing understand dataset convert things tensors answer seems sort converted tensor column value every example batch much makes sense though used type float whereas machine learning usually prefer float used idea python always represents scalars double precision conversion double single precision happens time forming tensor insured specifying dtype parameter case dataset seems formed tensor implicitly place way specify dtype parameter x tensor thereof would make intuitive sense examples batch instead elements containing elements way around dataset transposed input data would make sense forming tensor match shape instead making single tensor made tensors double precision way change behavior answer posted far says __getitem__ return anything okay anything get converted form training procedure requires,pytorch dataset supposed return,pytorch dataset supposed returntrying get pytorch work dataloader said easiest way handle mini batches cases necessary best performance dataloader wants dataset input documentation dataset assumes working shelf standard data set e g mnist least images use existing machinery black box working non image data generating best current attempt distill documentation minimal test case output program example code find x x device device succeeds x indeed tensor whereas version trying find exactly converts x tensor either example code e g failing understand dataset convert things tensors answer seems sort converted tensor column value every example batch much makes sense though used type float whereas machine learning usually prefer float used idea python always represents scalars double precision conversion double single precision happens time forming tensor insured specifying dtype parameter case dataset seems formed tensor implicitly place way specify dtype parameter x tensor thereof would make intuitive sense examples batch instead elements containing elements way around dataset transposed input data would make sense forming tensor match shape instead making single tensor made tensors double precision way change behavior answer posted far says __getitem__ return anything okay anything get converted form training procedure requires,"['pytorch', 'dataset', 'supposed', 'returntrying', 'get', 'pytorch', 'work', 'dataloader', 'said', 'easiest', 'way', 'handle', 'mini', 'batches', 'cases', 'necessary', 'best', 'performance', 'dataloader', 'wants', 'dataset', 'input', 'documentation', 'dataset', 'assumes', 'working', 'shelf', 'standard', 'data', 'set', 'e', 'g', 'mnist', 'least', 'images', 'use', 'existing', 'machinery', 'black', 'box', 'working', 'non', 'image', 'data', 'generating', 'best', 'current', 'attempt', 'distill', 'documentation', 'minimal', 'test', 'case', 'output', 'program', 'example', 'code', 'find', 'x', 'x', 'device', 'device', 'succeeds', 'x', 'indeed', 'tensor', 'whereas', 'version', 'trying', 'find', 'exactly', 'converts', 'x', 'tensor', 'either', 'example', 'code', 'e', 'g', 'failing', 'understand', 'dataset', 'convert', 'things', 'tensors', 'answer', 'seems', 'sort', 'converted', 'tensor', 'column', 'value', 'every', 'example', 'batch', 'much', 'makes', 'sense', 'though', 'used', 'type', 'float', 'whereas', 'machine', 'learning', 'usually', 'prefer', 'float', 'used', 'idea', 'python', 'always', 'represents', 'scalars', 'double', 'precision', 'conversion', 'double', 'single', 'precision', 'happens', 'time', 'forming', 'tensor', 'insured', 'specifying', 'dtype', 'parameter', 'case', 'dataset', 'seems', 'formed', 'tensor', 'implicitly', 'place', 'way', 'specify', 'dtype', 'parameter', 'x', 'tensor', 'thereof', 'would', 'make', 'intuitive', 'sense', 'examples', 'batch', 'instead', 'elements', 'containing', 'elements', 'way', 'around', 'dataset', 'transposed', 'input', 'data', 'would', 'make', 'sense', 'forming', 'tensor', 'match', 'shape', 'instead', 'making', 'single', 'tensor', 'made', 'tensors', 'double', 'precision', 'way', 'change', 'behavior', 'answer', 'posted', 'far', 'says', '__getitem__', 'return', 'anything', 'okay', 'anything', 'get', 'converted', 'form', 'training', 'procedure', 'requires']","['pytorch', 'dataset', 'suppos', 'returntri', 'get', 'pytorch', 'work', 'dataload', 'said', 'easiest', 'way', 'handl', 'mini', 'batch', 'case', 'necessari', 'best', 'perform', 'dataload', 'want', 'dataset', 'input', 'document', 'dataset', 'assum', 'work', 'shelf', 'standard', 'data', 'set', 'e', 'g', 'mnist', 'least', 'imag', 'use', 'exist', 'machineri', 'black', 'box', 'work', 'non', 'imag', 'data', 'gener', 'best', 'current', 'attempt', 'distil', 'document', 'minim', 'test', 'case', 'output', 'program', 'exampl', 'code', 'find', 'x', 'x', 'devic', 'devic', 'succe', 'x', 'inde', 'tensor', 'wherea', 'version', 'tri', 'find', 'exactli', 'convert', 'x', 'tensor', 'either', 'exampl', 'code', 'e', 'g', 'fail', 'understand', 'dataset', 'convert', 'thing', 'tensor', 'answer', 'seem', 'sort', 'convert', 'tensor', 'column', 'valu', 'everi', 'exampl', 'batch', 'much', 'make', 'sens', 'though', 'use', 'type', 'float', 'wherea', 'machin', 'learn', 'usual', 'prefer', 'float', 'use', 'idea', 'python', 'alway', 'repres', 'scalar', 'doubl', 'precis', 'convers', 'doubl', 'singl', 'precis', 'happen', 'time', 'form', 'tensor', 'insur', 'specifi', 'dtype', 'paramet', 'case', 'dataset', 'seem', 'form', 'tensor', 'implicitli', 'place', 'way', 'specifi', 'dtype', 'paramet', 'x', 'tensor', 'thereof', 'would', 'make', 'intuit', 'sens', 'exampl', 'batch', 'instead', 'element', 'contain', 'element', 'way', 'around', 'dataset', 'transpos', 'input', 'data', 'would', 'make', 'sens', 'form', 'tensor', 'match', 'shape', 'instead', 'make', 'singl', 'tensor', 'made', 'tensor', 'doubl', 'precis', 'way', 'chang', 'behavior', 'answer', 'post', 'far', 'say', '__getitem__', 'return', 'anyth', 'okay', 'anyth', 'get', 'convert', 'form', 'train', 'procedur', 'requir']"
134,144,144,19417433,72861688,Using PCA on Part of Dataframe,"<p>I want to use a clustering algorithm to a dataframe that contains a lot of features (32 columns).</p>
<p>A part of the features are encoded using one hot encoder.</p>
<p>I want to use PCA ( Principal Component analysis ) to reduce the dimension and  make the machine learning process easier.</p>
<p>Is it possible to use the PCA just for some columns of the data frame and keep the other columns as they are then use machine learning model.</p>
<p>Or it is obligatory to use PCA for all the dataframe before clustering.</p>
",26,1,-1,4,dataframe;machine-learning;artificial-intelligence;pca,2022-07-05 01:50:57,2022-07-05 01:50:57,2022-07-05 13:43:41,i want to use a clustering algorithm to a dataframe that contains a lot of features   columns   a part of the features are encoded using one hot encoder  i want to use pca   principal component analysis   to reduce the dimension and  make the machine learning process easier  is it possible to use the pca just for some columns of the data frame and keep the other columns as they are then use machine learning model  or it is obligatory to use pca for all the dataframe before clustering ,using pca on part of dataframe,want use clustering algorithm dataframe contains lot features columns part features encoded using one hot encoder want use pca principal component analysis reduce dimension make machine learning process easier possible use pca columns data frame keep columns use machine learning model obligatory use pca dataframe clustering,using pca part dataframe,using pca part dataframewant use clustering algorithm dataframe contains lot features columns part features encoded using one hot encoder want use pca principal component analysis reduce dimension make machine learning process easier possible use pca columns data frame keep columns use machine learning model obligatory use pca dataframe clustering,"['using', 'pca', 'part', 'dataframewant', 'use', 'clustering', 'algorithm', 'dataframe', 'contains', 'lot', 'features', 'columns', 'part', 'features', 'encoded', 'using', 'one', 'hot', 'encoder', 'want', 'use', 'pca', 'principal', 'component', 'analysis', 'reduce', 'dimension', 'make', 'machine', 'learning', 'process', 'easier', 'possible', 'use', 'pca', 'columns', 'data', 'frame', 'keep', 'columns', 'use', 'machine', 'learning', 'model', 'obligatory', 'use', 'pca', 'dataframe', 'clustering']","['use', 'pca', 'part', 'dataframew', 'use', 'cluster', 'algorithm', 'datafram', 'contain', 'lot', 'featur', 'column', 'part', 'featur', 'encod', 'use', 'one', 'hot', 'encod', 'want', 'use', 'pca', 'princip', 'compon', 'analysi', 'reduc', 'dimens', 'make', 'machin', 'learn', 'process', 'easier', 'possibl', 'use', 'pca', 'column', 'data', 'frame', 'keep', 'column', 'use', 'machin', 'learn', 'model', 'obligatori', 'use', 'pca', 'datafram', 'cluster']"
135,145,145,11377545,72862092,"WebRTC: ICE failed in Firefox, but working in MS Edge","<p>I want to develop a WebRTC based streaming application and I am learning about the protocol starting from the basics. In particular, I followed this extremely simple <a href=""https://github.com/hnasr/javascript_playground/tree/master/webrtc"" rel=""nofollow noreferrer"">example on GitHub</a> (also shown in this <a href=""https://www.youtube.com/watch?v=FExZvpVvYxA&amp;t=48m00s"" rel=""nofollow noreferrer"">YouTube video</a>) that just opens a WebRTC channel  between two browser tabs, and was able to run it on Microsoft Edge in my local machine. However, if I use the same JavaScript code in Firefox (version 102.0 on Windows 11), I receive an error <code>WebRTC: ICE failed, add a STUN server and see about:webrtc for more details</code> on both sender and receiver tabs.</p>
<p>I have therefore tried to include the STUN, TURN and TURNS servers provided by <a href=""https://www.metered.ca/tools/openrelay/"" rel=""nofollow noreferrer"">Open Relay</a> in the configuration, but the same error message always appears.</p>
<p>I would like my application to work on the most used internet browsers, so I am wondering how this can be fixed before digging deeper. Similar questions seemed a bit outdated or related to different errors.</p>
<p>Thank you all in advance for your help and suggestion.</p>
<hr />
<p>Considering the simplest case with no STUN nor TURN server (which should work in a local network or at least on the same localhost), WebRTC's connection registry contains this:</p>
<pre><code>+++++++ BEGIN (process id 21692) ++++++++
(generic/CRIT) PR_Connect failed: -5980
(ice/WARNING) /builds/worker/checkouts/gecko/dom/media/webrtc/transport/third_party/nICEr/src/net/nr_socket_multi_tcp.c:639 function nr_socket_multi_tcp_listen failed with error 3
(ice/WARNING) ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): failed to create passive TCP host candidate: 3
(ice/INFO) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): All candidates initialized
(generic/CRIT) PR_Connect failed: -5980
(ice/WARNING) /builds/worker/checkouts/gecko/dom/media/webrtc/transport/third_party/nICEr/src/net/nr_socket_multi_tcp.c:639 function nr_socket_multi_tcp_listen failed with error 3
(ice/WARNING) ICE-STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb): failed to create passive TCP host candidate: 3
(ice/INFO) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) starting grace period timer for 5000 ms
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) no streams with non-empty check lists
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) no streams with pre-answer requests
(ice/INFO) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) no checks to start, but gathering is not done yet, cancelling grace period timer
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) pairing local trickle ICE candidate host(IP4:192.168.1.28:56443/UDP)
(ice/INFO) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) starting grace period timer for 5000 ms
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) pairing local trickle ICE candidate host(IP4:192.168.1.28:63946/TCP) active
(ice/INFO) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): All candidates initialized
(ice/INFO) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): peer (PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default) Trickle grace period is over; marking every component with only failed pairs as failed.
(ice/INFO) ICE-PEER(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default)/STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb)/COMP(1): All pairs are failed, and grace period has elapsed. Marking component as failed.
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home))/STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb): state dump
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home))/ICE-STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb): Local component 1 - dumping candidates
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home))/ICE-STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb)/CAND(5fk0): host(IP4:192.168.1.28:56443/UDP)
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home))/ICE-STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb)/CAND(vE/5): host(IP4:192.168.1.28:63946/TCP) active
(ice/ERR) ICE-PEER(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default)/STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb): state dump
(ice/ERR) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home))/ICE-STREAM(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home) transport-id=transport_0 - c71730bb:54ae3990a18fe7c5e8e67bddb6f56bbb): Remote component 1 in state 3 - dumping candidates
(ice/INFO) ICE-PEER(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home):default): all checks completed success=0 fail=1
(ice/INFO) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): peer (PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default) starting grace period timer for 5000 ms
(ice/NOTICE) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): peer (PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default) no streams with non-empty check lists
(ice/NOTICE) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): peer (PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default) no streams with pre-answer requests
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/CAND-PAIR(S2jI): setting pair to state FROZEN: S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)
(ice/INFO) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/CAND-PAIR(S2jI): Pairing candidate IP4:192.168.1.28:56005/UDP (7e7f00ff):IP4:192.168.1.28:56443/UDP (7e7f00ff) priority=9115005270282338815 (7e7f00fffcfe01ff)
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): Starting check timer for stream.
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/CAND-PAIR(S2jI): setting pair to state WAITING: S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/CAND-PAIR(S2jI): setting pair to state IN_PROGRESS: S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)
(ice/NOTICE) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): peer (PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default) is now checking
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default): no FROZEN/WAITING pairs for PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(ice/INFO) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home)): peer (PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default) Trickle grace period is over; marking every component with only failed pairs as failed.
(stun/INFO) Responding with error 400: ICE Failure
(ice/NOTICE) ICE(PC:{f1eb3850-ea2c-4c5d-8699-1bd8bcdbe09b} 1656968377394000 (id=2147483690 url=about:home)): Message does not correspond to any registered stun ctx
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Received response; processing
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): nr_stun_process_error_response failed
(stun/WARNING) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Error processing response: Retry may be possible, stun error code 400.
(stun/INFO) STUN-CLIENT(S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)): Timed out
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/CAND-PAIR(S2jI): setting pair to state FAILED: S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host)
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf)/COMP(1): All pairs are failed, and grace period has elapsed. Marking component as failed.
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): state dump
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): Local component 1 - dumping candidates
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf)/CAND(R6E4): host(IP4:192.168.1.28:56005/UDP)
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf)/CAND(xSO3): host(IP4:192.168.1.28:60680/TCP) active
(ice/ERR) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default)/STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): state dump
(ice/ERR) CAND-PAIR(S2jI): pair S2jI|IP4:192.168.1.28:56005/UDP|IP4:192.168.1.28:56443/UDP(host(IP4:192.168.1.28:56005/UDP)|candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host): state=FAILED, priority=0x7e7f00fffcfe01ff
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf): Remote component 1 in state 3 - dumping candidates
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf)/CAND(giJB): candidate:0 1 UDP 2122252543 192.168.1.28 56443 typ host
(ice/ERR) ICE(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home))/ICE-STREAM(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home) transport-id=transport_0 - a9c7cadd:7ce62f845d4f5b83df2a70e1bac037bf)/CAND(eRB0): candidate:1 1 TCP 2105524479 192.168.1.28 9 typ host tcptype active
(ice/INFO) ICE-PEER(PC:{a3e94488-2d82-4739-9c98-ce3de577714b} 1656968349764000 (id=2147483688 url=about:home):default): all checks completed success=0 fail=1
+++++++ END (process id 21692) ++++++++
</code></pre>
<p>while local and remote SDPs respectively look like this:</p>
<pre><code>v=0
o=mozilla...THIS_IS_SDPARTA-99.0 1501128418895448605 0 IN IP4 0.0.0.0
s=-
t=0 0
a=fingerprint:sha-256 26:27:49:F1:75:8E:96:56:D6:A9:3F:E9:28:A9:D8:3A:E3:99:CA:52:4B:22:3C:6C:9D:02:FD:C7:AE:21:17:3F
a=group:BUNDLE 0
a=ice-options:trickle
a=msid-semantic:WMS *
m=application 9 UDP/DTLS/SCTP webrtc-datachannel
c=IN IP4 0.0.0.0
a=sendrecv
a=ice-pwd:7ce62f845d4f5b83df2a70e1bac037bf
a=ice-ufrag:a9c7cadd
a=mid:0
a=setup:actpass
a=sctp-port:5000
a=max-message-size:1073741823
</code></pre>
<pre><code>v=0
o=mozilla...THIS_IS_SDPARTA-99.0 5475832728403267594 0 IN IP4 0.0.0.0
s=-
t=0 0
a=sendrecv
a=fingerprint:sha-256 DC:E2:A0:CE:08:9B:25:D6:63:E0:6A:D8:94:05:82:1A:9B:27:47:4B:83:30:95:72:B4:BD:0B:62:2F:44:7C:9D
a=group:BUNDLE 0
a=ice-options:trickle
a=msid-semantic:WMS *
m=application 9 UDP/DTLS/SCTP webrtc-datachannel
c=IN IP4 0.0.0.0
a=candidate:0 1 UDP 2122252543 eb5fb3b1-f3d1-4add-afff-e9cde1bf07a2.local 56443 typ host
a=candidate:1 1 TCP 2105524479 eb5fb3b1-f3d1-4add-afff-e9cde1bf07a2.local 9 typ host tcptype active
a=sendrecv
a=end-of-candidates
a=ice-pwd:54ae3990a18fe7c5e8e67bddb6f56bbb
a=ice-ufrag:c71730bb
a=mid:0
a=setup:active
a=sctp-port:5000
a=max-message-size:1073741823
</code></pre>
<hr />
<p>Strangely, connection can be established between tabs of different browsers (sender in Edge and receiver in Firefox, or vice versa). Remote SDPs of the Firefox client (therefore describing Edge session) look like this:</p>
<pre><code>v=0
o=- 6799550255512083034 2 IN IP4 127.0.0.1
s=-
t=0 0
a=group:BUNDLE 0
a=extmap-allow-mixed
a=msid-semantic: WMS
m=application 9 UDP/DTLS/SCTP webrtc-datachannel
c=IN IP4 0.0.0.0
a=candidate:2606571665 1 udp 2113937151 5b5fd4e1-64b5-421c-b103-ca4a29c106dc.local 62776 typ host generation 0 network-cost 999
a=ice-ufrag:Gc38
a=ice-pwd:zxGRR6Dlm+usOA+Xx4BmuN2x
a=ice-options:trickle
a=fingerprint:sha-256 6C:23:49:63:D3:60:78:D2:AF:1E:68:4F:7C:EC:19:1C:29:5C:64:A0:5B:29:7B:D3:5B:5F:CC:81:7E:EA:56:97
a=setup:actpass
a=mid:0
a=sctp-port:5000
a=max-message-size:262144
</code></pre>
",42,1,0,5,javascript;firefox;networking;webrtc;sdp,2022-07-05 02:45:17,2022-07-05 02:45:17,2022-07-05 12:56:34,i want to develop a webrtc based streaming application and i am learning about the protocol starting from the basics  in particular  i followed this extremely simple   also shown in this   that just opens a webrtc channel  between two browser tabs  and was able to run it on microsoft edge in my local machine  however  if i use the same javascript code in firefox  version   on windows    i receive an error webrtc  ice failed  add a stun server and see about webrtc for more details on both sender and receiver tabs  i have therefore tried to include the stun  turn and turns servers provided by  in the configuration  but the same error message always appears  i would like my application to work on the most used internet browsers  so i am wondering how this can be fixed before digging deeper  similar questions seemed a bit outdated or related to different errors  thank you all in advance for your help and suggestion  considering the simplest case with no stun nor turn server  which should work in a local network or at least on the same localhost   webrtc s connection registry contains this  while local and remote sdps respectively look like this  strangely  connection can be established between tabs of different browsers  sender in edge and receiver in firefox  or vice versa   remote sdps of the firefox client  therefore describing edge session  look like this ,webrtc  ice failed in firefox  but working in ms edge,want develop webrtc based streaming application learning protocol starting basics particular followed extremely simple also shown opens webrtc channel two browser tabs able run microsoft edge local machine however use javascript code firefox version windows receive error webrtc ice failed stun server see webrtc details sender receiver tabs therefore tried include stun turn turns servers provided configuration error message always appears would like application work used internet browsers wondering fixed digging deeper similar questions seemed bit outdated related different errors thank advance help suggestion considering simplest case stun turn server work local network least localhost webrtc connection registry contains local remote sdps respectively look like strangely connection established tabs different browsers sender edge receiver firefox vice versa remote sdps firefox client therefore describing edge session look like,webrtc ice failed firefox working ms edge,webrtc ice failed firefox working ms edgewant develop webrtc based streaming application learning protocol starting basics particular followed extremely simple also shown opens webrtc channel two browser tabs able run microsoft edge local machine however use javascript code firefox version windows receive error webrtc ice failed stun server see webrtc details sender receiver tabs therefore tried include stun turn turns servers provided configuration error message always appears would like application work used internet browsers wondering fixed digging deeper similar questions seemed bit outdated related different errors thank advance help suggestion considering simplest case stun turn server work local network least localhost webrtc connection registry contains local remote sdps respectively look like strangely connection established tabs different browsers sender edge receiver firefox vice versa remote sdps firefox client therefore describing edge session look like,"['webrtc', 'ice', 'failed', 'firefox', 'working', 'ms', 'edgewant', 'develop', 'webrtc', 'based', 'streaming', 'application', 'learning', 'protocol', 'starting', 'basics', 'particular', 'followed', 'extremely', 'simple', 'also', 'shown', 'opens', 'webrtc', 'channel', 'two', 'browser', 'tabs', 'able', 'run', 'microsoft', 'edge', 'local', 'machine', 'however', 'use', 'javascript', 'code', 'firefox', 'version', 'windows', 'receive', 'error', 'webrtc', 'ice', 'failed', 'stun', 'server', 'see', 'webrtc', 'details', 'sender', 'receiver', 'tabs', 'therefore', 'tried', 'include', 'stun', 'turn', 'turns', 'servers', 'provided', 'configuration', 'error', 'message', 'always', 'appears', 'would', 'like', 'application', 'work', 'used', 'internet', 'browsers', 'wondering', 'fixed', 'digging', 'deeper', 'similar', 'questions', 'seemed', 'bit', 'outdated', 'related', 'different', 'errors', 'thank', 'advance', 'help', 'suggestion', 'considering', 'simplest', 'case', 'stun', 'turn', 'server', 'work', 'local', 'network', 'least', 'localhost', 'webrtc', 'connection', 'registry', 'contains', 'local', 'remote', 'sdps', 'respectively', 'look', 'like', 'strangely', 'connection', 'established', 'tabs', 'different', 'browsers', 'sender', 'edge', 'receiver', 'firefox', 'vice', 'versa', 'remote', 'sdps', 'firefox', 'client', 'therefore', 'describing', 'edge', 'session', 'look', 'like']","['webrtc', 'ice', 'fail', 'firefox', 'work', 'ms', 'edgew', 'develop', 'webrtc', 'base', 'stream', 'applic', 'learn', 'protocol', 'start', 'basic', 'particular', 'follow', 'extrem', 'simpl', 'also', 'shown', 'open', 'webrtc', 'channel', 'two', 'browser', 'tab', 'abl', 'run', 'microsoft', 'edg', 'local', 'machin', 'howev', 'use', 'javascript', 'code', 'firefox', 'version', 'window', 'receiv', 'error', 'webrtc', 'ice', 'fail', 'stun', 'server', 'see', 'webrtc', 'detail', 'sender', 'receiv', 'tab', 'therefor', 'tri', 'includ', 'stun', 'turn', 'turn', 'server', 'provid', 'configur', 'error', 'messag', 'alway', 'appear', 'would', 'like', 'applic', 'work', 'use', 'internet', 'browser', 'wonder', 'fix', 'dig', 'deeper', 'similar', 'question', 'seem', 'bit', 'outdat', 'relat', 'differ', 'error', 'thank', 'advanc', 'help', 'suggest', 'consid', 'simplest', 'case', 'stun', 'turn', 'server', 'work', 'local', 'network', 'least', 'localhost', 'webrtc', 'connect', 'registri', 'contain', 'local', 'remot', 'sdp', 'respect', 'look', 'like', 'strang', 'connect', 'establish', 'tab', 'differ', 'browser', 'sender', 'edg', 'receiv', 'firefox', 'vice', 'versa', 'remot', 'sdp', 'firefox', 'client', 'therefor', 'describ', 'edg', 'session', 'look', 'like']"
136,146,146,806160,72865447,Compute similarity in pyspark,"<p>I have a <code>csv</code> file contains some data, I want select the similar data with an input.
my data is like:</p>
<pre><code>H1      | H2      | H3
--------+---------+----------
A       | 1       | 7
B       | 5       | 3
C       | 7       | 2
</code></pre>
<p>And the data point that I want find data similar to that in my <code>csv</code> is like : <code>[6, 8]</code>.</p>
<p>Actually I want find rows that <code>H2</code> and <code>H3</code> of data set is similar to input, and It return <code>H1</code>.</p>
<p>I want use pyspark and some similarity measure like Euclidean Distance, Manhattan Distance, Cosine Similarity or machine learning algorithm.</p>
",39,0,0,3,pyspark;similarity;sentence-similarity,2022-07-05 12:52:19,2022-07-05 12:52:19,2022-07-05 12:52:19,and the data point that i want find data similar to that in my csv is like         actually i want find rows that h and h of data set is similar to input  and it return h  i want use pyspark and some similarity measure like euclidean distance  manhattan distance  cosine similarity or machine learning algorithm ,compute similarity in pyspark,data point want find data similar csv like actually want find rows h h data set similar input return h want use pyspark similarity measure like euclidean distance manhattan distance cosine similarity machine learning algorithm,compute similarity pyspark,compute similarity pysparkdata point want find data similar csv like actually want find rows h h data set similar input return h want use pyspark similarity measure like euclidean distance manhattan distance cosine similarity machine learning algorithm,"['compute', 'similarity', 'pysparkdata', 'point', 'want', 'find', 'data', 'similar', 'csv', 'like', 'actually', 'want', 'find', 'rows', 'h', 'h', 'data', 'set', 'similar', 'input', 'return', 'h', 'want', 'use', 'pyspark', 'similarity', 'measure', 'like', 'euclidean', 'distance', 'manhattan', 'distance', 'cosine', 'similarity', 'machine', 'learning', 'algorithm']","['comput', 'similar', 'pysparkdata', 'point', 'want', 'find', 'data', 'similar', 'csv', 'like', 'actual', 'want', 'find', 'row', 'h', 'h', 'data', 'set', 'similar', 'input', 'return', 'h', 'want', 'use', 'pyspark', 'similar', 'measur', 'like', 'euclidean', 'distanc', 'manhattan', 'distanc', 'cosin', 'similar', 'machin', 'learn', 'algorithm']"
137,147,147,9098088,72862237,MATLAB string to integer conversion,"<p>I am a newbie at matlab. I have deployed a machine learning model(developed using python) using flask. From matlab, I have called the API and received a string response. The response is like: '[0.8]'. but matlab is showing the size of the string is 1. I need only the value 0.8. My code:</p>
<pre><code>import matlab.net.http.*
import matlab.net.http.field.*

request = RequestMessage( 'POST', ...
    [ContentTypeField( 'application/vnd.api+json' ), AcceptField('application/vnd.api+json')], ...
    '{&quot;meta&quot;: {&quot;Speed_RPM_PU&quot;: 0.2}}' );
response = request.send( 'http://127.0.0.1:5000/predict' );
ans=response.Body.Data
length(ans) % equals to 1
% for i = 1:length(ans)
%  
%    fprintf('%c ',ans(i))
%  
%    %disp(String(i))
%  
% end
</code></pre>
<p>Here, ans='[0.8]'</p>
",32,1,1,2,string;matlab,2022-07-05 03:08:43,2022-07-05 03:08:43,2022-07-05 12:45:53,i am a newbie at matlab  i have deployed a machine learning model developed using python  using flask  from matlab  i have called the api and received a string response  the response is like         but matlab is showing the size of the string is   i need only the value    my code  here  ans      ,matlab string to integer conversion,newbie matlab deployed machine learning model developed using python using flask matlab called api received string response response like matlab showing size string need value code ans,matlab string integer conversion,matlab string integer conversionnewbie matlab deployed machine learning model developed using python using flask matlab called api received string response response like matlab showing size string need value code ans,"['matlab', 'string', 'integer', 'conversionnewbie', 'matlab', 'deployed', 'machine', 'learning', 'model', 'developed', 'using', 'python', 'using', 'flask', 'matlab', 'called', 'api', 'received', 'string', 'response', 'response', 'like', 'matlab', 'showing', 'size', 'string', 'need', 'value', 'code', 'ans']","['matlab', 'string', 'integ', 'conversionnewbi', 'matlab', 'deploy', 'machin', 'learn', 'model', 'develop', 'use', 'python', 'use', 'flask', 'matlab', 'call', 'api', 'receiv', 'string', 'respons', 'respons', 'like', 'matlab', 'show', 'size', 'string', 'need', 'valu', 'code', 'an']"
138,148,148,12698225,72863635,Getting error message when pinging target machine through ssh with ansible inventory file,"<p>I'm learning Ansible. I created an inventory file to ping my target hosts through it.</p>
<p>Every time i am doing it I am getting the error message:</p>
<pre><code>ansible [core 2.12.7]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/ubuntu/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /home/ubuntu/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible
  python version = 3.10.4 (main, Apr  2 2022, 09:04:19) [GCC 11.2.0]
  jinja version = 3.0.3
  libyaml = True
Using /etc/ansible/ansible.cfg as config file
host_list declined parsing /home/ubuntu/vprofile/exercise1/inventory as it did not pass its verify_file() method
script declined parsing /home/ubuntu/vprofile/exercise1/inventory as it did not pass its verify_file() method
auto declined parsing /home/ubuntu/vprofile/exercise1/inventory as it did not pass its verify_file() method
Parsed /home/ubuntu/vprofile/exercise1/inventory inventory source with ini plugin
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.
META: ran handlers
&lt;172.31.84.210&gt; ESTABLISH SSH CONNECTION FOR USER: amazon
&lt;172.31.84.210&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o 'IdentityFile=&quot;vprofile-key.pem&quot;' -o KbdInteractivi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User=&quot;amazon&quot;' -o ConnectTimeout=10 -o 'ControlPath=&quot;/home/ubuntu/.ansible/cp/715n &amp;&amp; sleep 0'&quot;'&quot;''
&lt;172.31.84.210&gt; (255, b'', b'amazon@172.31.84.210: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\r\n')
web01 | UNREACHABLE! =&gt; {
    &quot;changed&quot;: false,
    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: amazon@172.31.84.210: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).&quot;,
    &quot;unreachable&quot;: true





I tried SSH with my host machine to target machine its working fine.
</code></pre>
<p>PS: Im using Ubuntu server as my host and target machine and I have put ansible_user=amazon</p>
<p>Please help me out.</p>
",23,0,0,4,amazon-web-services;amazon-ec2;ansible;ansible-inventory,2022-07-05 08:41:25,2022-07-05 08:41:25,2022-07-05 09:26:21,i m learning ansible  i created an inventory file to ping my target hosts through it  every time i am doing it i am getting the error message  ps  im using ubuntu server as my host and target machine and i have put ansible_user amazon please help me out ,getting error message when pinging target machine through ssh with ansible inventory file,learning ansible created inventory file ping target hosts every time getting error message ps im using ubuntu server host target machine put ansible_user amazon please help,getting error message pinging target machine ssh ansible inventory file,getting error message pinging target machine ssh ansible inventory filelearning ansible created inventory file ping target hosts every time getting error message ps im using ubuntu server host target machine put ansible_user amazon please help,"['getting', 'error', 'message', 'pinging', 'target', 'machine', 'ssh', 'ansible', 'inventory', 'filelearning', 'ansible', 'created', 'inventory', 'file', 'ping', 'target', 'hosts', 'every', 'time', 'getting', 'error', 'message', 'ps', 'im', 'using', 'ubuntu', 'server', 'host', 'target', 'machine', 'put', 'ansible_user', 'amazon', 'please', 'help']","['get', 'error', 'messag', 'ping', 'target', 'machin', 'ssh', 'ansibl', 'inventori', 'filelearn', 'ansibl', 'creat', 'inventori', 'file', 'ping', 'target', 'host', 'everi', 'time', 'get', 'error', 'messag', 'ps', 'im', 'use', 'ubuntu', 'server', 'host', 'target', 'machin', 'put', 'ansible_us', 'amazon', 'pleas', 'help']"
139,149,149,224059,7673509,Automatically built regex expressions that fit set of strings,"<p>We have written the system to analyse log messages from the large network. The system takes log messages from lots of different network elements, and analyses it by regex expressions. For example user may have written two rules:</p>

<pre><code>^cron/script\.sh.*
.*script\.sh [0-9]+$
</code></pre>

<p>In this case only logs that match given patterns will be selected. The reason of the filtering is that there may be really lots of log messages, up to 1 GB per day.</p>

<p>Now the main part of my question. Since there is lots of network elements, and several types of them, and every one of them has different parameters in path... Is there any way to automatically generate set of regexes that will somehow group the logs? The system can learn on historical data, e.g. from the last week. Generated regex must not be very accurate, it is supposed to be the hint for user to add such new rule into system.</p>

<p>I was thinking about unsupervised machine learning to divide input into groups and then in each group find proper regex. Is there any other way, maybe faster or better? And, last but not least, <em>how to</em> find regex that matches all strings in obtained group?  (Non-trivial, so <code>.*</code> is not the answer.)</p>

<hr>

<p><strong>Edit</strong> After some thinking I'll try to simplify the problem. Suppose I have already grouped logs. I'd like to find (at most) three largest substrings (at least one) common to all the strings in set. For example:</p>

<pre><code>Set of strings:
cron/script1.sh -abc 1243 all
cron/script2.sh 1
bin/script1.sh -asdf 15

Obtained groups:
/script
.sh 
</code></pre>

<p>Now I can build some simple regex by concatenating these groups with <code>.*?</code>. In this example it would be <code>.*?(/script).*?(\.sh ).*?</code>. It seems to be simpler solution.</p>
",4157,4,8,4,regex;string;algorithm;grammar-induction,2011-10-06 16:48:39,2011-10-06 16:48:39,2022-07-05 02:16:17,we have written the system to analyse log messages from the large network  the system takes log messages from lots of different network elements  and analyses it by regex expressions  for example user may have written two rules  in this case only logs that match given patterns will be selected  the reason of the filtering is that there may be really lots of log messages  up to  gb per day  now the main part of my question  since there is lots of network elements  and several types of them  and every one of them has different parameters in path    is there any way to automatically generate set of regexes that will somehow group the logs  the system can learn on historical data  e g  from the last week  generated regex must not be very accurate  it is supposed to be the hint for user to add such new rule into system  i was thinking about unsupervised machine learning to divide input into groups and then in each group find proper regex  is there any other way  maybe faster or better  and  last but not least  how to find regex that matches all strings in obtained group    non trivial  so    is not the answer   edit after some thinking i ll try to simplify the problem  suppose i have already grouped logs  i d like to find  at most  three largest substrings  at least one  common to all the strings in set  for example  now i can build some simple regex by concatenating these groups with      in this example it would be      script       sh       it seems to be simpler solution ,automatically built regex expressions that fit set of strings,written system analyse log messages large network system takes log messages lots different network elements analyses regex expressions example user may written two rules case logs match given patterns selected reason filtering may really lots log messages gb per day main part question since lots network elements several types every one different parameters path way automatically generate set regexes somehow group logs system learn historical data e g last week generated regex must accurate supposed hint user rule system thinking unsupervised machine learning divide input groups group find proper regex way maybe faster better last least find regex matches strings obtained group non trivial answer edit thinking try simplify problem suppose already grouped logs like find three largest substrings least one common strings set example build simple regex concatenating groups example would script sh seems simpler solution,automatically built regex expressions fit set strings,automatically built regex expressions fit set stringswritten system analyse log messages large network system takes log messages lots different network elements analyses regex expressions example user may written two rules case logs match given patterns selected reason filtering may really lots log messages gb per day main part question since lots network elements several types every one different parameters path way automatically generate set regexes somehow group logs system learn historical data e g last week generated regex must accurate supposed hint user rule system thinking unsupervised machine learning divide input groups group find proper regex way maybe faster better last least find regex matches strings obtained group non trivial answer edit thinking try simplify problem suppose already grouped logs like find three largest substrings least one common strings set example build simple regex concatenating groups example would script sh seems simpler solution,"['automatically', 'built', 'regex', 'expressions', 'fit', 'set', 'stringswritten', 'system', 'analyse', 'log', 'messages', 'large', 'network', 'system', 'takes', 'log', 'messages', 'lots', 'different', 'network', 'elements', 'analyses', 'regex', 'expressions', 'example', 'user', 'may', 'written', 'two', 'rules', 'case', 'logs', 'match', 'given', 'patterns', 'selected', 'reason', 'filtering', 'may', 'really', 'lots', 'log', 'messages', 'gb', 'per', 'day', 'main', 'part', 'question', 'since', 'lots', 'network', 'elements', 'several', 'types', 'every', 'one', 'different', 'parameters', 'path', 'way', 'automatically', 'generate', 'set', 'regexes', 'somehow', 'group', 'logs', 'system', 'learn', 'historical', 'data', 'e', 'g', 'last', 'week', 'generated', 'regex', 'must', 'accurate', 'supposed', 'hint', 'user', 'rule', 'system', 'thinking', 'unsupervised', 'machine', 'learning', 'divide', 'input', 'groups', 'group', 'find', 'proper', 'regex', 'way', 'maybe', 'faster', 'better', 'last', 'least', 'find', 'regex', 'matches', 'strings', 'obtained', 'group', 'non', 'trivial', 'answer', 'edit', 'thinking', 'try', 'simplify', 'problem', 'suppose', 'already', 'grouped', 'logs', 'like', 'find', 'three', 'largest', 'substrings', 'least', 'one', 'common', 'strings', 'set', 'example', 'build', 'simple', 'regex', 'concatenating', 'groups', 'example', 'would', 'script', 'sh', 'seems', 'simpler', 'solution']","['automat', 'built', 'regex', 'express', 'fit', 'set', 'stringswritten', 'system', 'analys', 'log', 'messag', 'larg', 'network', 'system', 'take', 'log', 'messag', 'lot', 'differ', 'network', 'element', 'analys', 'regex', 'express', 'exampl', 'user', 'may', 'written', 'two', 'rule', 'case', 'log', 'match', 'given', 'pattern', 'select', 'reason', 'filter', 'may', 'realli', 'lot', 'log', 'messag', 'gb', 'per', 'day', 'main', 'part', 'question', 'sinc', 'lot', 'network', 'element', 'sever', 'type', 'everi', 'one', 'differ', 'paramet', 'path', 'way', 'automat', 'gener', 'set', 'regex', 'somehow', 'group', 'log', 'system', 'learn', 'histor', 'data', 'e', 'g', 'last', 'week', 'gener', 'regex', 'must', 'accur', 'suppos', 'hint', 'user', 'rule', 'system', 'think', 'unsupervis', 'machin', 'learn', 'divid', 'input', 'group', 'group', 'find', 'proper', 'regex', 'way', 'mayb', 'faster', 'better', 'last', 'least', 'find', 'regex', 'match', 'string', 'obtain', 'group', 'non', 'trivial', 'answer', 'edit', 'think', 'tri', 'simplifi', 'problem', 'suppos', 'alreadi', 'group', 'log', 'like', 'find', 'three', 'largest', 'substr', 'least', 'one', 'common', 'string', 'set', 'exampl', 'build', 'simpl', 'regex', 'concaten', 'group', 'exampl', 'would', 'script', 'sh', 'seem', 'simpler', 'solut']"
140,151,151,6527049,70663003,MongoServerSelectionError: connection &lt;monitor&gt; to &lt;MyIP&gt; closed,"<p>I have started learning MongoDB, I am just trying to connect to the database using MongoDB Shell.
I am using the below command.</p>
<pre><code>mongosh &quot;mongodb+srv://cluster0.12345.mongodb.net/myFirstDatabase&quot; --username viveknuna
</code></pre>
<p>I have added my current IP Address to IP Access List. But getting this error.</p>
<blockquote>
<p>MongoServerSelectionError: connection  to  closed</p>
</blockquote>
<p>I have referred to <a href=""https://stackoverflow.com/q/60431996/6527049"">this</a> question and added 0.0.0.0/0 (Allow access from AnyWhere) in the IP Access List. and it works as expected. But this is a security risk, Is there any way without allowing all IPs.</p>
<p><strong>FYI</strong>: I am inside Virtual Machine and running this command also from VM.</p>
",893,1,1,3,mongodb;mongodb-atlas;mongo-shell,2022-01-11 12:55:15,2022-01-11 12:55:15,2022-07-04 23:22:05,i have added my current ip address to ip access list  but getting this error  mongoserverselectionerror  connection  to  closed i have referred to  question and added       allow access from anywhere  in the ip access list  and it works as expected  but this is a security risk  is there any way without allowing all ips  fyi  i am inside virtual machine and running this command also from vm ,mongoserverselectionerror  connection  lt monitor gt  to  lt myip gt  closed,added current ip address ip access getting error mongoserverselectionerror connection closed referred question added allow access anywhere ip access works expected security risk way without allowing ips fyi inside virtual machine running command also vm,mongoserverselectionerror connection lt monitor gt lt myip gt closed,mongoserverselectionerror connection lt monitor gt lt myip gt closedadded current ip address ip access getting error mongoserverselectionerror connection closed referred question added allow access anywhere ip access works expected security risk way without allowing ips fyi inside virtual machine running command also vm,"['mongoserverselectionerror', 'connection', 'lt', 'monitor', 'gt', 'lt', 'myip', 'gt', 'closedadded', 'current', 'ip', 'address', 'ip', 'access', 'getting', 'error', 'mongoserverselectionerror', 'connection', 'closed', 'referred', 'question', 'added', 'allow', 'access', 'anywhere', 'ip', 'access', 'works', 'expected', 'security', 'risk', 'way', 'without', 'allowing', 'ips', 'fyi', 'inside', 'virtual', 'machine', 'running', 'command', 'also', 'vm']","['mongoserverselectionerror', 'connect', 'lt', 'monitor', 'gt', 'lt', 'myip', 'gt', 'closedad', 'current', 'ip', 'address', 'ip', 'access', 'get', 'error', 'mongoserverselectionerror', 'connect', 'close', 'refer', 'question', 'ad', 'allow', 'access', 'anywher', 'ip', 'access', 'work', 'expect', 'secur', 'risk', 'way', 'without', 'allow', 'ip', 'fyi', 'insid', 'virtual', 'machin', 'run', 'command', 'also', 'vm']"
141,152,152,9895048,59408524,How to unzip image folder in colab,"<p><strong>I want to train a deep learning model on a Devanagari dataset containing around 9000 images. Since the dataset is huge, I want to use Google colab since it's GPU supported.I uploaded folder from my local machine to Colab in Zip format . But an error is occurred while unzipping file.</strong></p>

<pre><code>    from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file ""{name}"" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
</code></pre>

<p><strong>When I tried following to unzip file</strong></p>

<pre><code>unzip devanagari-character-dataset.zip  
</code></pre>

<p><strong>I have following error.</strong></p>

<pre><code>      File ""&lt;ipython-input-8-92b289004693&gt;"", line 1
        unzip devanagari-character-dataset.zip
                       ^

SyntaxError: invalid syntax
</code></pre>

<p><strong>How to solve above issue.</strong></p>
",826,2,3,2,python;deep-learning,2019-12-19 16:41:33,2019-12-19 16:41:33,2022-07-04 22:05:20,i want to train a deep learning model on a devanagari dataset containing around  images  since the dataset is huge  i want to use google colab since it s gpu supported i uploaded folder from my local machine to colab in zip format   but an error is occurred while unzipping file  when i tried following to unzip file i have following error  how to solve above issue ,how to unzip image folder in colab,want train deep learning model devanagari dataset containing around images since dataset huge want use google colab since gpu supported uploaded folder local machine colab zip format error occurred unzipping file tried following unzip file following error solve issue,unzip image folder colab,unzip image folder colabwant train deep learning model devanagari dataset containing around images since dataset huge want use google colab since gpu supported uploaded folder local machine colab zip format error occurred unzipping file tried following unzip file following error solve issue,"['unzip', 'image', 'folder', 'colabwant', 'train', 'deep', 'learning', 'model', 'devanagari', 'dataset', 'containing', 'around', 'images', 'since', 'dataset', 'huge', 'want', 'use', 'google', 'colab', 'since', 'gpu', 'supported', 'uploaded', 'folder', 'local', 'machine', 'colab', 'zip', 'format', 'error', 'occurred', 'unzipping', 'file', 'tried', 'following', 'unzip', 'file', 'following', 'error', 'solve', 'issue']","['unzip', 'imag', 'folder', 'colabw', 'train', 'deep', 'learn', 'model', 'devanagari', 'dataset', 'contain', 'around', 'imag', 'sinc', 'dataset', 'huge', 'want', 'use', 'googl', 'colab', 'sinc', 'gpu', 'support', 'upload', 'folder', 'local', 'machin', 'colab', 'zip', 'format', 'error', 'occur', 'unzip', 'file', 'tri', 'follow', 'unzip', 'file', 'follow', 'error', 'solv', 'issu']"
142,153,153,14680256,72853315,Font Awesome icon not changing color based on React state,"<p>Font awesome icon is not changing color based on react state, I have an app which shows random quotes and display random color in background, new quote button and twitter icon. On first reload twitter icon color is changing as intended, however, when new quote button is clicked it stays the same. Here is my code</p>
<pre><code>import React from 'react';
import './style.scss';
import randomColor from 'randomcolor';
import '@fortawesome/fontawesome-free/css/all.css';
import '@fortawesome/fontawesome-free/js/all.js';

const quotes = [
    'I do not fear computers. I fear lack of them.',
    'A computer once beat me at chess, but it was no match for me at kick boxing.',
    'Computer Science is no more about computers than astronomy is about telescopes.',
    'The computer was born to solve problems that did not exist before.',
    'Software is like entropy: It is difficult to grasp, weighs nothing, and obeys the Second Law of Thermodynamics; i.e., it always increases.',
    'Software is a gas; it expands to fill its container.',
    &quot;All parts should go together without forcing.  You must remember that the parts you are reassembling were disassembled by you.  Therefore, if you can't get them together again, there must be a reason.  By all means, do not use a hammer.&quot;,
    &quot;Standards are always out of date.  That's what makes them standards.&quot;,
    &quot;Physics is the universe's operating system.&quot;,
    &quot;It's hardware that makes a machine fast.  It's software that makes a fast machine slow.&quot;,
    'Imagination is more important than knowledge.  For knowledge is limited, whereas imagination embraces the entire world.',
    'The greatest enemy of knowledge is not ignorance, it is the illusion of knowledge.',
    'The more you know, the more you realize you know nothing.',
    'Tell me and I forget.  Teach me and I remember.  Involve me and I learn.',
    &quot;Real knowledge is to know the extent of one's ignorance.&quot;,
    'If people never did silly things, nothing intelligent would ever get done.',
    'Getting information off the Internet is like taking a drink from a fire hydrant.',
    'If you think your users are idiots, only idiots will use it.',
    &quot;From a programmer's point of view, the user is a peripheral that types when you issue a read request.&quot;,
    &quot;Where is the 'any' key?&quot;,
    'Computers are good at following instructions, but not at reading your mind.',
    &quot;There is only one problem with common sense; it's not very common.&quot;,
    'Your most unhappy customers are your greatest source of learning.',
    'Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.',
];
const authors = [
    '- Isaac Asimov',
    '- Emo Philips',
    '- Edsger W. Dijkstra',
    '- Bill Gates',
    '- Norman Augustine',
    '- Nathan Myhrvold',
    '- IBM Manual, 1925',
    '- Alan Bennett',
    '- Steven R Garman',
    '- Craig Bruce',
    '- Albert Einstein',
    '- Stephen Hawking',
    '- Socrates',
    '- Benjamin Franklin',
    '- Confucius',
    '- Ludwig Wittgenstein',
    '- Mitchell Kapor',
    '- Linus Torvalds',
    '- P. Williams',
    '- Homer Simpson, in response to the message, “Press any key”',
    '- Donald Knuth',
    '- Milt Bryce',
    '- Bill Gates',
    '- Donald E. Knuth',
];

const random = Math.floor(Math.random() * quotes.length);
class App extends React.Component {
    constructor(props) {
        super(props);
        this.state = {
            quote: quotes[random],
            author: authors[random],
            color: randomColor(),
        };
        this.handleClick = this.handleClick.bind(this);
    }

    handleClick() {
        const clickRandom = Math.floor(Math.random() * quotes.length);
        this.setState({
            quote: quotes[clickRandom],
            author: authors[clickRandom],
            color: randomColor(),
        });
    }

    render() {
        return (
            &lt;div
                id=&quot;bg&quot;
                style={{
                    backgroundColor: this.state.color,
                }}
            &gt;
                &lt;div id=&quot;quote-box&quot;&gt;
                    &lt;p id=&quot;text&quot;&gt;{this.state.quote}&lt;/p&gt;
                    &lt;p id=&quot;author&quot;&gt;{this.state.author}&lt;/p&gt;
                    &lt;a id=&quot;tweet-quote&quot; href=&quot;https://twitter.com/intent/tweet&quot;&gt;
                        &lt;i
                            className=&quot;fa-brands fa-twitter-square fa-2xl&quot;
                            style={{
                                color: this.state.color,
                            }}
                        &gt;&lt;/i&gt;
                    &lt;/a&gt;
                    &lt;div style={{
                        width: 20,
                        height: 20,
                        backgroundColor: this.state.color
                    }}&gt;&lt;/div&gt;
                    &lt;button
                        id=&quot;new-quote&quot;
                        onClick={this.handleClick}
                        style={{
                            backgroundColor: this.state.color,
                            border: 'solid 1px ' + this.state.color,
                        }}
                    &gt;
                        New Quote
                    &lt;/button&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        );
    }
}

export default App;
</code></pre>
<p>To diagnose the problem I have specifically made a div box of 20 x 20px, every element is changing color upon click on new quote button except font awesome twitter icon.</p>
",57,1,0,4,javascript;html;css;reactjs,2022-07-04 13:15:46,2022-07-04 13:15:46,2022-07-04 19:00:12,font awesome icon is not changing color based on react state  i have an app which shows random quotes and display random color in background  new quote button and twitter icon  on first reload twitter icon color is changing as intended  however  when new quote button is clicked it stays the same  here is my code to diagnose the problem i have specifically made a div box of  x px  every element is changing color upon click on new quote button except font awesome twitter icon ,font awesome icon not changing color based on react state,font awesome icon changing color based react state app shows random quotes display random color background quote button twitter icon first reload twitter icon color changing intended however quote button clicked stays code diagnose problem specifically made div box x px every element changing color upon click quote button except font awesome twitter icon,font awesome icon changing color based react state,font awesome icon changing color based react statefont awesome icon changing color based react state app shows random quotes display random color background quote button twitter icon first reload twitter icon color changing intended however quote button clicked stays code diagnose problem specifically made div box x px every element changing color upon click quote button except font awesome twitter icon,"['font', 'awesome', 'icon', 'changing', 'color', 'based', 'react', 'statefont', 'awesome', 'icon', 'changing', 'color', 'based', 'react', 'state', 'app', 'shows', 'random', 'quotes', 'display', 'random', 'color', 'background', 'quote', 'button', 'twitter', 'icon', 'first', 'reload', 'twitter', 'icon', 'color', 'changing', 'intended', 'however', 'quote', 'button', 'clicked', 'stays', 'code', 'diagnose', 'problem', 'specifically', 'made', 'div', 'box', 'x', 'px', 'every', 'element', 'changing', 'color', 'upon', 'click', 'quote', 'button', 'except', 'font', 'awesome', 'twitter', 'icon']","['font', 'awesom', 'icon', 'chang', 'color', 'base', 'react', 'statefont', 'awesom', 'icon', 'chang', 'color', 'base', 'react', 'state', 'app', 'show', 'random', 'quot', 'display', 'random', 'color', 'background', 'quot', 'button', 'twitter', 'icon', 'first', 'reload', 'twitter', 'icon', 'color', 'chang', 'intend', 'howev', 'quot', 'button', 'click', 'stay', 'code', 'diagnos', 'problem', 'specif', 'made', 'div', 'box', 'x', 'px', 'everi', 'element', 'chang', 'color', 'upon', 'click', 'quot', 'button', 'except', 'font', 'awesom', 'twitter', 'icon']"
143,154,154,13788653,72856267,How to split chessboard into tensors so that they can be analyzed as a batch,"<p>I am currently trying to process a chessboard with a machine learning model by splitting the image into 64 squares, and then getting the model prediction from each square. However, I was wondering if there is any way I can somehow split the original image into groups so that they can be analyzed simultaneously as a batch so the code can run much faster.</p>
<p>Here is my current code:</p>
<pre><code>squares = image_processing('/Users/Me/Downloads/Screen Shot 2022-07-04 at 11.09.04 AM.png')
image_transforms = transforms.Compose([  
                    transforms.ToTensor()])

board = []
for square in squares:
    square = Image.fromarray(square)
    square = image_transforms(square).float()
    square = square.unsqueeze(0)

    output = model(square)
    _, predicted = torch.max(output.data, 1)
    board.append(classes[predicted.item()])
</code></pre>
",25,1,0,3,performance;machine-learning;pytorch,2022-07-04 17:22:46,2022-07-04 17:22:46,2022-07-04 17:57:26,i am currently trying to process a chessboard with a machine learning model by splitting the image into  squares  and then getting the model prediction from each square  however  i was wondering if there is any way i can somehow split the original image into groups so that they can be analyzed simultaneously as a batch so the code can run much faster  here is my current code ,how to split chessboard into tensors so that they can be analyzed as a batch,currently trying process chessboard machine learning model splitting image squares getting model prediction square however wondering way somehow split original image groups analyzed simultaneously batch code run much faster current code,split chessboard tensors analyzed batch,split chessboard tensors analyzed batchcurrently trying process chessboard machine learning model splitting image squares getting model prediction square however wondering way somehow split original image groups analyzed simultaneously batch code run much faster current code,"['split', 'chessboard', 'tensors', 'analyzed', 'batchcurrently', 'trying', 'process', 'chessboard', 'machine', 'learning', 'model', 'splitting', 'image', 'squares', 'getting', 'model', 'prediction', 'square', 'however', 'wondering', 'way', 'somehow', 'split', 'original', 'image', 'groups', 'analyzed', 'simultaneously', 'batch', 'code', 'run', 'much', 'faster', 'current', 'code']","['split', 'chessboard', 'tensor', 'analyz', 'batchcurr', 'tri', 'process', 'chessboard', 'machin', 'learn', 'model', 'split', 'imag', 'squar', 'get', 'model', 'predict', 'squar', 'howev', 'wonder', 'way', 'somehow', 'split', 'origin', 'imag', 'group', 'analyz', 'simultan', 'batch', 'code', 'run', 'much', 'faster', 'current', 'code']"
144,155,155,19338925,72854063,making pipeline for machine learning models,"<pre><code>from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

model_params = {           /* creating dictionary of all classifiers with paramters */
     'svm': {
        'model': svm.SVC(gamma='auto'),
         'params' : {
             'svc__C': [1,10,100,1000],
             'svc__kernel': ['rbf','linear']
         }  
     },
    
        'logistic_regression' : {
         'model': LogisticRegression(solver='liblinear',multi_class='auto'),
         'params': {
             'logisticregression__C': [1,5,10]
         }
     },
    
    'random_forest1': {
         'model': RandomForestClassifier(),
         'params' : {
             'randomforestclassifier__n_estimators': [1,5,10]
         }
     },
    
       

      'decision_tree': {
         'model': DecisionTreeClassifier(),
         'params': {
             'decisionTreeClassifier__criterion': [&quot;gini&quot;,&quot;entropy&quot;,&quot;log_loss&quot;]
            
         }
    
       }
}
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

scores = []
best_estimators = {}
import pandas as pd
for algo, mp in model_params.items():
    pipe = make_pipeline(StandardScaler(), mp['model']) /* creating pipeline to scale data and fetching classifiers from dictionary */
    
    clf =  GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)  /* using grid search cv on my classifiers */
   
    clf.fit(features,target)
    scores.append({
        'model': algo,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    best_estimators[algo] = clf.best_estimator_
    
df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
</code></pre>
<p>Error:</p>
<pre><code>Invalid parameter '' for estimator Pipeline(steps=[('standardscaler', StandardScaler()),
                ('decision_tree', DecisionTreeClassifier() ]). Valid parameters are: ['memory', 'steps', 'verbose'].  
</code></pre>
<p>the code works  fine for svm logistic regression and random forest classifiers but throw parameter error for the decision tree classifier. cant figure out whether it is a syntax issue or something else</p>
",38,1,0,5,python;machine-learning;scikit-learn;classification;pipeline,2022-07-04 14:21:41,2022-07-04 14:21:41,2022-07-04 17:10:40,error  the code works  fine for svm logistic regression and random forest classifiers but throw parameter error for the decision tree classifier  cant figure out whether it is a syntax issue or something else,making pipeline for machine learning models,error code works fine svm logistic regression random forest classifiers throw parameter error decision tree classifier cant figure whether syntax issue something else,making pipeline machine learning models,making pipeline machine learning modelserror code works fine svm logistic regression random forest classifiers throw parameter error decision tree classifier cant figure whether syntax issue something else,"['making', 'pipeline', 'machine', 'learning', 'modelserror', 'code', 'works', 'fine', 'svm', 'logistic', 'regression', 'random', 'forest', 'classifiers', 'throw', 'parameter', 'error', 'decision', 'tree', 'classifier', 'cant', 'figure', 'whether', 'syntax', 'issue', 'something', 'else']","['make', 'pipelin', 'machin', 'learn', 'modelserror', 'code', 'work', 'fine', 'svm', 'logist', 'regress', 'random', 'forest', 'classifi', 'throw', 'paramet', 'error', 'decis', 'tree', 'classifi', 'cant', 'figur', 'whether', 'syntax', 'issu', 'someth', 'els']"
145,156,156,17260574,72853790,Azure machine learning compute bicep template naming error,"<p>I have a file called machine-learning.bicep file which contains both resources machine learning workspace and compute(I want to keep both resources together). Getting the following naming error for the compute resource 'incorrect segment lengths. A nested resource type must have identical number of segments as its resource name. A root resource type must have segment length one greater than its resource name'. I also removed cluster in compute name to match segment length but still getting error</p>
<pre><code>resource machineLearning 'Microsoft.MachineLearningServices/workspaces@2022-01-01-preview' = {
  name: 'mlw-${project}-${env}'
  location: loc
  tags: tags
  identity: {
    type: 'SystemAssigned'
  }
  properties: {
    // dependent resources
    applicationInsights: appInsights.id
    containerRegistry: containerRegistry.id
    keyVault: keyVaultId
    storageAccount: storage.id
  }
}

resource amlci 'Microsoft.MachineLearningServices/workspaces/computes@2020-09-01-preview' = {
  name: 'mlw-${project}-${env}-cluster'
  location: loc
  properties: {
    computeType: 'AmlCompute'
    properties: {
      vmSize: 'Standard_DS3_v2'
      subnet: json('null')
      osType: 'Linux'
      scaleSettings: {
        maxNodeCount: 5
        minNodeCount: 0
      }
    }
  }
}
</code></pre>
",29,1,0,4,azure;machine-learning;azure-resource-manager;azure-bicep,2022-07-04 13:59:49,2022-07-04 13:59:49,2022-07-04 15:00:45,i have a file called machine learning bicep file which contains both resources machine learning workspace and compute i want to keep both resources together   getting the following naming error for the compute resource  incorrect segment lengths  a nested resource type must have identical number of segments as its resource name  a root resource type must have segment length one greater than its resource name   i also removed cluster in compute name to match segment length but still getting error,azure machine learning compute bicep template naming error,file called machine learning bicep file contains resources machine learning workspace compute want keep resources together getting following naming error compute resource incorrect segment lengths nested resource type must identical number segments resource name root resource type must segment length one greater resource name also removed cluster compute name match segment length still getting error,azure machine learning compute bicep template naming error,azure machine learning compute bicep template naming errorfile called machine learning bicep file contains resources machine learning workspace compute want keep resources together getting following naming error compute resource incorrect segment lengths nested resource type must identical number segments resource name root resource type must segment length one greater resource name also removed cluster compute name match segment length still getting error,"['azure', 'machine', 'learning', 'compute', 'bicep', 'template', 'naming', 'errorfile', 'called', 'machine', 'learning', 'bicep', 'file', 'contains', 'resources', 'machine', 'learning', 'workspace', 'compute', 'want', 'keep', 'resources', 'together', 'getting', 'following', 'naming', 'error', 'compute', 'resource', 'incorrect', 'segment', 'lengths', 'nested', 'resource', 'type', 'must', 'identical', 'number', 'segments', 'resource', 'name', 'root', 'resource', 'type', 'must', 'segment', 'length', 'one', 'greater', 'resource', 'name', 'also', 'removed', 'cluster', 'compute', 'name', 'match', 'segment', 'length', 'still', 'getting', 'error']","['azur', 'machin', 'learn', 'comput', 'bicep', 'templat', 'name', 'errorfil', 'call', 'machin', 'learn', 'bicep', 'file', 'contain', 'resourc', 'machin', 'learn', 'workspac', 'comput', 'want', 'keep', 'resourc', 'togeth', 'get', 'follow', 'name', 'error', 'comput', 'resourc', 'incorrect', 'segment', 'length', 'nest', 'resourc', 'type', 'must', 'ident', 'number', 'segment', 'resourc', 'name', 'root', 'resourc', 'type', 'must', 'segment', 'length', 'one', 'greater', 'resourc', 'name', 'also', 'remov', 'cluster', 'comput', 'name', 'match', 'segment', 'length', 'still', 'get', 'error']"
146,157,157,1279459,54635355,What does log_prob do?,"<p>In some (e.g. machine learning) libraries, we can find <code>log_prob</code> function. What does it do and how is it different from taking just regular <code>log</code>?</p>

<p>For example, what is the purpose of this code:</p>

<pre><code>dist = Normal(mean, std)
sample = dist.sample()
logprob = dist.log_prob(sample)
</code></pre>

<p>And subsequently, why would we first take a log and then exponentiate the resulting value instead of just evaluating it directly:</p>

<pre><code>prob = torch.exp(dist.log_prob(sample))
</code></pre>
",17870,4,23,2,pytorch;probability-distribution,2019-02-11 22:21:07,2019-02-11 22:21:07,2022-07-04 14:07:51,in some  e g  machine learning  libraries  we can find log_prob function  what does it do and how is it different from taking just regular log  for example  what is the purpose of this code  and subsequently  why would we first take a log and then exponentiate the resulting value instead of just evaluating it directly ,what does log_prob do ,e g machine learning libraries find log_prob function different taking regular log example purpose code subsequently would first take log exponentiate resulting value instead evaluating directly,log_prob,log_probe g machine learning libraries find log_prob function different taking regular log example purpose code subsequently would first take log exponentiate resulting value instead evaluating directly,"['log_probe', 'g', 'machine', 'learning', 'libraries', 'find', 'log_prob', 'function', 'different', 'taking', 'regular', 'log', 'example', 'purpose', 'code', 'subsequently', 'would', 'first', 'take', 'log', 'exponentiate', 'resulting', 'value', 'instead', 'evaluating', 'directly']","['log_prob', 'g', 'machin', 'learn', 'librari', 'find', 'log_prob', 'function', 'differ', 'take', 'regular', 'log', 'exampl', 'purpos', 'code', 'subsequ', 'would', 'first', 'take', 'log', 'exponenti', 'result', 'valu', 'instead', 'evalu', 'directli']"
147,158,158,6891262,48082414,init docker swarm with docker machine: context deadline exceeded,"<p>I'm learing Docker machine while encount some problems.<br>
My computer is mac and use Docker for mac.  I create 2 vm,vm1&amp; vm2 by docker-machine,and try to init a swarm who has nodes-vm1,vm2 and my mac.My steps are below:<br>
1. create an image called ""sprinla/cms:latest"" and a docker-compose.yml</p>

<pre><code>version: ""3""
services:
  web:
    image: sprinla/cms:latest
    deploy:
      replicas: 1
    ports:
      - ""80:80""
    networks:
      - webnet
    command: /data/start.sh
networks:
  webnet:
</code></pre>

<p>2.create 2 vms.Here is vm info:  </p>

<pre><code>yuxrdeMBP:~ yuxr$ docker-machine ls  
NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS  
vm1    -        virtualbox   Running   tcp://192.168.99.100:2376           v17.12.0-ce  
vm2    -        virtualbox   Running   tcp://192.168.99.101:2376           v17.12.0-ce  
</code></pre>

<ol start=""3"">
<li>init swarm on my mac host:  </li>
</ol>

<blockquote>
<pre><code>yuxrdeMBP:~ yuxr$ docker swarm init
Swarm initialized: current node (uf6rg1v91exlwntlskyj8iim7) is now a manager.
To add a worker to this swarm, run the following command:
docker swarm join --token SWMTKN-1-3qb32l84n0s8vl74rj9d6psm7bzdany3piw55ohtrq0q7ly814-c5km5zg3kj9d6vn6vrtt6xxtg 192.168.65.2:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
</code></pre>
</blockquote>

<p>4 join vm1 to swarm,then comes the problem</p>

<pre><code>yuxrdeMBP:~ yuxr$ docker-machine ssh vm1 ""docker swarm join --token SWMTKN-1-3qb32l84n0s8vl74rj9d6psm7bzdany3piw55ohtrq0q7ly814-c5km5zg3kj9d6vn6vrtt6xxtg 192.168.65.2:2377""
Error response from daemon: Timeout was reached before node joined. The attempt to join the swarm will continue in the background. Use the ""docker info"" command to see the current swarm status of your node.
exit status 1
</code></pre>

<p>5.cat the docker log :</p>

<pre><code>time=""2018-01-03T17:13:50.387854642Z"" level=debug msg=""Calling GET /_ping""
time=""2018-01-03T17:13:50.388228524Z"" level=debug msg=""Calling GET /_ping""
time=""2018-01-03T17:13:50.388521374Z"" level=debug msg=""Calling POST /v1.35/swarm/join""
time=""2018-01-03T17:13:50.388583426Z"" level=debug msg=""form data: {\""AdvertiseAddr\"":\""\"",\""Availability\"":\""\"",\""DataPathAddr\"":\""\"",\""JoinToken\"":\""*****\"",\""ListenAddr\"":\""0.0.0.0:2377\"",\""RemoteAddrs\"":[\""192.168.65.2:2377\""]}""
time=""2018-01-03T17:13:55.392578452Z"" level=error msg=""failed to retrieve remote root CA certificate"" error=""rpc error: code = DeadlineExceeded desc = context deadline exceeded"" module=node
time=""2018-01-03T17:14:02.394608777Z"" level=error msg=""failed to retrieve remote root CA certificate"" error=""rpc error: code = DeadlineExceeded desc = context deadline exceeded"" module=node
time=""2018-01-03T17:14:09.395720474Z"" level=error msg=""failed to retrieve remote root CA certificate"" error=""rpc error: code = DeadlineExceeded desc = context deadline exceeded"" module=node
time=""2018-01-03T17:14:10.393743738Z"" level=error msg=""Handler for POST /v1.35/swarm/join returned error: Timeout was reached before node joined. The attempt to join the swarm will continue in the background. Use the \""docker info\"" command to see the current swarm status of your node.""
time=""2018-01-03T17:14:16.398095265Z"" level=error msg=""failed to retrieve remote root CA certificate"" error=""rpc error: code = DeadlineExceeded desc = context deadline exceeded"" module=node
time=""2018-01-03T17:14:23.399587783Z"" level=error msg=""failed to retrieve remote root CA certificate"" error=""rpc error: code = DeadlineExceeded desc = context deadline exceeded"" module=node
time=""2018-01-03T17:14:25.399943337Z"" level=error msg=""cluster exited with error: rpc error: code = DeadlineExceeded desc = context deadline exceeded""
</code></pre>

<ol start=""6"">
<li><p>below is my mac ifconfig info:  </p>

<blockquote>
<pre><code>yuxrdeMBP:~ yuxr$ ifconfig
lo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384
  options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt;
  inet 127.0.0.1 netmask 0xff000000
  inet6 ::1 prefixlen 128
  inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280
stf0: flags=0&lt;&gt; mtu 1280
XHC20: flags=0&lt;&gt; mtu 0
en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
  ether ac:bc:32:81:97:37
  inet6 fe80::4d8:6b2:718a:5d3b%en0 prefixlen 64 secured scopeid 0x5
  inet 192.168.199.169 netmask 0xffffff00 broadcast 192.168.199.255
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
  media: autoselect
  status: active
p2p0: flags=8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; mtu 2304
  ether 0e:bc:32:81:97:37
  media: autoselect
  status: inactive
awdl0: flags=8943&lt;UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1484
  ether 36:9f:65:fd:34:c3
  inet6 fe80::349f:65ff:fefd:34c3%awdl0 prefixlen 64 scopeid 0x7
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
  media: autoselect
  status: active
en1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500
  options=60&lt;TSO4,TSO6&gt;
  ether 6a:00:00:e3:4c:30
  media: autoselect &lt;full-duplex&gt;
  status: inactive
en2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500
  options=60&lt;TSO4,TSO6&gt;
  ether 6a:00:00:e3:4c:31
  media: autoselect &lt;full-duplex&gt;
  status: inactive
bridge0: flags=8822&lt;BROADCAST,SMART,SIMPLEX,MULTICAST&gt; mtu 1500
  options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;
  ether 6a:00:00:e3:4c:30
  Configuration:
      id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
      maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
      root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
      ipfilter disabled flags 0x2
  member: en1 flags=3&lt;LEARNING,DISCOVER&gt;
          ifmaxaddr 0 port 8 priority 0 path cost 0
  member: en2 flags=3&lt;LEARNING,DISCOVER&gt;
          ifmaxaddr 0 port 9 priority 0 path cost 0
  media: &lt;unknown type&gt;
  status: inactive
utun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000
  options=6403&lt;RXCSUM,TXCSUM,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;
  inet6 fe80::441e:c0e3:5429:2abb%utun0 prefixlen 64 scopeid 0xb
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
utun1: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1380
  options=6403&lt;RXCSUM,TXCSUM,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;
  inet6 fe80::7820:5bac:4735:7f82%utun1 prefixlen 64 scopeid 0xc
  inet6 fd44:5cb3:4ab4:5d08:7820:5bac:4735:7f82 prefixlen 64
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
utun2: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1380
  options=6403&lt;RXCSUM,TXCSUM,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;
  inet6 fe80::26f2:e964:8dfb:e884%utun2 prefixlen 64 scopeid 0xd
  nd6 options=201&lt;PERFORMNUD,DAD&gt;
gpd0: flags=8862&lt;BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1400
  ether 02:50:41:00:01:01
vboxnet0: flags=8943&lt;UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500
  ether 0a:00:27:00:00:00
  inet 192.168.99.1 netmask 0xffffff00 broadcast 192.168.99.255
</code></pre>
</blockquote></li>
</ol>

<p>Why????<br>
mac host has ip, 192.168.99.1 ,vm1 has ip 192.168.99.100,vm2 has ip 192.168.99.101,they are in the same network,why can't vm1 nor vm2
join the mac host's swarm?</p>

<p>ANOTHER QUESTION:if i use vm1 as swarm manager,run ""docker swarm join"" commad on the mac host,when join as worker,it can join but can't use;when join as manager will has error:</p>

<pre><code>yuxrdeMBP:~ yuxr$ docker swarm join --token SWMTKN-1-49w1hd28hs1mtj3sgmd0o3q7n59zgppvd18vs0iwhcnjemzmwb-7mk35zdnaslt1p41gninvwlud 192.168.99.100:2377
Error response from daemon: manager stopped: can't initialize raft node: rpc error: code = Unknown desc = could not connect to prospective new cluster member using its advertised address: rpc error: code = Unavailable desc = grpc: the connection is unavailable
</code></pre>

<p>THANK YOU FOR HELP ME !!!</p>
",4864,3,1,3,docker;docker-swarm;docker-machine,2018-01-03 23:09:16,2018-01-03 23:09:16,2022-07-04 12:21:03, create  vms here is vm info     join vm to swarm then comes the problem  cat the docker log   below is my mac ifconfig info    another question if i use vm as swarm manager run docker swarm join commad on the mac host when join as worker it can join but can t use when join as manager will has error  thank you for help me    ,init docker swarm with docker machine  context deadline exceeded,create vms vm info join vm swarm comes problem cat docker log mac ifconfig info another question use vm swarm manager run docker swarm join commad mac host join worker join use join manager error thank help,init docker swarm docker machine context deadline exceeded,init docker swarm docker machine context deadline exceededcreate vms vm info join vm swarm comes problem cat docker log mac ifconfig info another question use vm swarm manager run docker swarm join commad mac host join worker join use join manager error thank help,"['init', 'docker', 'swarm', 'docker', 'machine', 'context', 'deadline', 'exceededcreate', 'vms', 'vm', 'info', 'join', 'vm', 'swarm', 'comes', 'problem', 'cat', 'docker', 'log', 'mac', 'ifconfig', 'info', 'another', 'question', 'use', 'vm', 'swarm', 'manager', 'run', 'docker', 'swarm', 'join', 'commad', 'mac', 'host', 'join', 'worker', 'join', 'use', 'join', 'manager', 'error', 'thank', 'help']","['init', 'docker', 'swarm', 'docker', 'machin', 'context', 'deadlin', 'exceededcr', 'vm', 'vm', 'info', 'join', 'vm', 'swarm', 'come', 'problem', 'cat', 'docker', 'log', 'mac', 'ifconfig', 'info', 'anoth', 'question', 'use', 'vm', 'swarm', 'manag', 'run', 'docker', 'swarm', 'join', 'commad', 'mac', 'host', 'join', 'worker', 'join', 'use', 'join', 'manag', 'error', 'thank', 'help']"
148,159,159,19214553,72846197,Tensorflow virtual environment on Mac in vs code doesn&#39;t work help please,"<p>I followed this guide: <a href=""https://github.com/mrdbourke/m1-machine-learning-test"" rel=""nofollow noreferrer"">https://github.com/mrdbourke/m1-machine-learning-test</a> to create a virtual environment to run TensorFlow on my Mac but I could not get it to run in VS Code. Somehow the kernel is not right I don't know much about this stuff.</p>
<p><a href=""https://i.stack.imgur.com/hYN7m.jpg"" rel=""nofollow noreferrer""><strong>Vs code code doesn't work</strong></a></p>
<p><a href=""https://i.stack.imgur.com/hYN7m.jpg"" rel=""nofollow noreferrer"">2</a>: <a href=""https://i.stack.imgur.com/HVNoD.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/HVNoD.jpg</a> Jupyter code works in browser</p>
<p>here's the terminal log:</p>
<pre><code>Error 14:27:30.924: Error in execution (get message for cell) c [Error]: Kernel base (Python 3.9.13) is not usable. Check the Jupyter output tab for more information.
    at b.createNewKernelSession (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:408801)
    at runNextTicks (node:internal/process/task_queues:61:5)
    at processTimers (node:internal/timers:497:9)
    at b.connect (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:407398)
    at d.startNew (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:415056)
    at a (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:365465)
    at S.createNotebookInstance (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:365663)
    at S.createNotebook (/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/out/extension.node.js:2:366262) {
  category: 'invalidkernel',
  kernelConnectionMetadata: {
    kind: 'startUsingPythonInterpreter',
    kernelSpec: {
      specFile: '/Users/lukasgassner/.vscode/extensions/ms-toolsai.jupyter-2022.4.1021342353/temp/jupyter/kernels/python3913jvsc74a57bd071973c23121dc5e6b00777b455410e06e065f8f26f2253b01b08bb5762db1de7/kernel.json',
      interpreterPath: '/Users/lukasgassner/miniforge3/bin/python',
      isRegisteredByVSC: 'registeredByNewVersionOfExt',
      name: 'python3913jvsc74a57bd071973c23121dc5e6b00777b455410e06e065f8f26f2253b01b08bb5762db1de7',
      argv: [Array],
      language: 'python',
      executable: 'python',
      display_name: &quot;Python 3.9.13 ('base')&quot;,
      metadata: [Object],
      env: {}
    },
    interpreter: {
      id: '/Users/lukasgassner/miniforge3/bin/python',
      sysPrefix: '/Users/lukasgassner/miniforge3',
      envType: 'Conda',
      envName: 'base',
      envPath: [m],
      architecture: 3,
      sysVersion: '3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n' +
        '[Clang 13.0.1 ]',
      version: [Object],
      companyDisplayName: 'Anaconda, Inc.',
      displayName: &quot;Python 3.9.13 ('base')&quot;,
      detailedDisplayName: &quot;Python 3.9.13 ('base': conda)&quot;,
      uri: [m]
    },
    id: '.jvsc74a57bd071973c23121dc5e6b00777b455410e06e065f8f26f2253b01b08bb5762db1de7./Users/lukasgassner/miniforge3/python./Users/lukasgassner/miniforge3/python.-m#ipykernel_launcher'
  }
}
</code></pre>
",26,0,0,4,macos;tensorflow;visual-studio-code;virtualenv,2022-07-03 17:13:00,2022-07-03 17:13:00,2022-07-04 12:07:28,i followed this guide   to create a virtual environment to run tensorflow on my mac but i could not get it to run in vs code  somehow the kernel is not right i don t know much about this stuff      jupyter code works in browser here s the terminal log ,tensorflow virtual environment on mac in vs code doesn   t work help please,followed guide create virtual environment run tensorflow mac could get run vs code somehow kernel right know much stuff jupyter code works browser terminal log,tensorflow virtual environment mac vs code work help please,tensorflow virtual environment mac vs code work help pleasefollowed guide create virtual environment run tensorflow mac could get run vs code somehow kernel right know much stuff jupyter code works browser terminal log,"['tensorflow', 'virtual', 'environment', 'mac', 'vs', 'code', 'work', 'help', 'pleasefollowed', 'guide', 'create', 'virtual', 'environment', 'run', 'tensorflow', 'mac', 'could', 'get', 'run', 'vs', 'code', 'somehow', 'kernel', 'right', 'know', 'much', 'stuff', 'jupyter', 'code', 'works', 'browser', 'terminal', 'log']","['tensorflow', 'virtual', 'environ', 'mac', 'vs', 'code', 'work', 'help', 'pleasefollow', 'guid', 'creat', 'virtual', 'environ', 'run', 'tensorflow', 'mac', 'could', 'get', 'run', 'vs', 'code', 'somehow', 'kernel', 'right', 'know', 'much', 'stuff', 'jupyt', 'code', 'work', 'browser', 'termin', 'log']"
149,160,160,17424122,72852489,Firebase - Machine Learning and interest tracking to create an algorithm for sorting posts,"<p>One of my applications includes user-generated posts and functions in a similar way to Instagram. When a user opens the app they see a feed of posts sorted by date. This works when there just one small demographic using the app, but as the user base becomes more diverse, not everyone is interested in the same posts. This is why apps like TikTok and Instagram have algorithms to decide which posts to show to a user. Where do I even start with this? I understand that there need to be tags on each post for what they are about (this is where I think I can use machine learning) and then each users information needs to include their interests (I’m not sure what can be used to change this as they like or dislike posts). Is there a simple pre-built way of doing this or any examples? It seems fo be a pretty big secret that mostly big tech companies understand and use.</p>
",12,0,0,2,firebase;firebase-machine-learning,2022-07-04 11:53:01,2022-07-04 11:53:01,2022-07-04 11:53:01,one of my applications includes user generated posts and functions in a similar way to instagram  when a user opens the app they see a feed of posts sorted by date  this works when there just one small demographic using the app  but as the user base becomes more diverse  not everyone is interested in the same posts  this is why apps like tiktok and instagram have algorithms to decide which posts to show to a user  where do i even start with this  i understand that there need to be tags on each post for what they are about  this is where i think i can use machine learning  and then each users information needs to include their interests  i m not sure what can be used to change this as they like or dislike posts   is there a simple pre built way of doing this or any examples  it seems fo be a pretty big secret that mostly big tech companies understand and use ,firebase   machine learning and interest tracking to create an algorithm for sorting posts,one applications includes user generated posts functions similar way instagram user opens app see feed posts sorted date works one small demographic using app user base becomes diverse everyone interested posts apps like tiktok instagram algorithms decide posts show user even start understand need tags post think use machine learning users information needs include interests sure used change like dislike posts simple pre built way examples seems fo pretty big secret mostly big tech companies understand use,firebase machine learning interest tracking create algorithm sorting posts,firebase machine learning interest tracking create algorithm sorting postsone applications includes user generated posts functions similar way instagram user opens app see feed posts sorted date works one small demographic using app user base becomes diverse everyone interested posts apps like tiktok instagram algorithms decide posts show user even start understand need tags post think use machine learning users information needs include interests sure used change like dislike posts simple pre built way examples seems fo pretty big secret mostly big tech companies understand use,"['firebase', 'machine', 'learning', 'interest', 'tracking', 'create', 'algorithm', 'sorting', 'postsone', 'applications', 'includes', 'user', 'generated', 'posts', 'functions', 'similar', 'way', 'instagram', 'user', 'opens', 'app', 'see', 'feed', 'posts', 'sorted', 'date', 'works', 'one', 'small', 'demographic', 'using', 'app', 'user', 'base', 'becomes', 'diverse', 'everyone', 'interested', 'posts', 'apps', 'like', 'tiktok', 'instagram', 'algorithms', 'decide', 'posts', 'show', 'user', 'even', 'start', 'understand', 'need', 'tags', 'post', 'think', 'use', 'machine', 'learning', 'users', 'information', 'needs', 'include', 'interests', 'sure', 'used', 'change', 'like', 'dislike', 'posts', 'simple', 'pre', 'built', 'way', 'examples', 'seems', 'fo', 'pretty', 'big', 'secret', 'mostly', 'big', 'tech', 'companies', 'understand', 'use']","['firebas', 'machin', 'learn', 'interest', 'track', 'creat', 'algorithm', 'sort', 'postson', 'applic', 'includ', 'user', 'gener', 'post', 'function', 'similar', 'way', 'instagram', 'user', 'open', 'app', 'see', 'feed', 'post', 'sort', 'date', 'work', 'one', 'small', 'demograph', 'use', 'app', 'user', 'base', 'becom', 'divers', 'everyon', 'interest', 'post', 'app', 'like', 'tiktok', 'instagram', 'algorithm', 'decid', 'post', 'show', 'user', 'even', 'start', 'understand', 'need', 'tag', 'post', 'think', 'use', 'machin', 'learn', 'user', 'inform', 'need', 'includ', 'interest', 'sure', 'use', 'chang', 'like', 'dislik', 'post', 'simpl', 'pre', 'built', 'way', 'exampl', 'seem', 'fo', 'pretti', 'big', 'secret', 'mostli', 'big', 'tech', 'compani', 'understand', 'use']"
150,161,161,18744280,72852267,When to use what classifier,"<p>I am relatively new to machine learning.  I know there are different classifiers. There is no clear answer on when to use what.  But I am sure there are characteristics of different datasets that pre dispose them to one classifier Vs another.  For example, I was told naive Bayes is usually used for text classification.</p>
<p>Is there such a table or paper that can serve as a useful reference guide on what to use?</p>
<p>I have googled and asked different people but have not gotten a satisfactory answer other than &quot;it depends?&quot;</p>
",11,0,-1,1,classification,2022-07-04 11:21:43,2022-07-04 11:21:43,2022-07-04 11:21:43,i am relatively new to machine learning   i know there are different classifiers  there is no clear answer on when to use what   but i am sure there are characteristics of different datasets that pre dispose them to one classifier vs another   for example  i was told naive bayes is usually used for text classification  is there such a table or paper that can serve as a useful reference guide on what to use  i have googled and asked different people but have not gotten a satisfactory answer other than  it depends  ,when to use what classifier,relatively machine learning know different classifiers clear answer use sure characteristics different datasets pre dispose one classifier vs another example told naive bayes usually used text classification table paper serve useful reference guide use googled asked different people gotten satisfactory answer depends,use classifier,use classifierrelatively machine learning know different classifiers clear answer use sure characteristics different datasets pre dispose one classifier vs another example told naive bayes usually used text classification table paper serve useful reference guide use googled asked different people gotten satisfactory answer depends,"['use', 'classifierrelatively', 'machine', 'learning', 'know', 'different', 'classifiers', 'clear', 'answer', 'use', 'sure', 'characteristics', 'different', 'datasets', 'pre', 'dispose', 'one', 'classifier', 'vs', 'another', 'example', 'told', 'naive', 'bayes', 'usually', 'used', 'text', 'classification', 'table', 'paper', 'serve', 'useful', 'reference', 'guide', 'use', 'googled', 'asked', 'different', 'people', 'gotten', 'satisfactory', 'answer', 'depends']","['use', 'classifierrel', 'machin', 'learn', 'know', 'differ', 'classifi', 'clear', 'answer', 'use', 'sure', 'characterist', 'differ', 'dataset', 'pre', 'dispos', 'one', 'classifi', 'vs', 'anoth', 'exampl', 'told', 'naiv', 'bay', 'usual', 'use', 'text', 'classif', 'tabl', 'paper', 'serv', 'use', 'refer', 'guid', 'use', 'googl', 'ask', 'differ', 'peopl', 'gotten', 'satisfactori', 'answer', 'depend']"
151,162,162,1361737,72844445,How do I extract the classification tree from this parsnip model in R?,"<p>I am working through 'Machine Learning &amp; R Expert techniques for predictive modeling' by Brett Lantz. I am using the <code>tidymodels</code> suite as I try the example modeling exercises in R.</p>
<p>I am working through chapter 5 in which you build a decision tree with the C5.0 algorithm. I hav e created the model using the code shown below</p>
<pre><code> c5_v1 &lt;- C5_rules() %&gt;% 
 set_mode('classification') %&gt;% 
 set_engine('C5.0')
  

c5_res_1 &lt;- fit(object = c5_v1, formula = default ~., data = credit_train)
</code></pre>
<p>This has worked successfully:</p>
<pre><code>parsnip model object


Call:
C5.0.default(x = x, y = y, trials = trials, rules = TRUE, control
 = C50::C5.0Control(minCases = minCases, seed = sample.int(10^5, 1), earlyStopping
 = FALSE))

Rule-Based Model
Number of samples: 900 
Number of predictors: 20 

Number of Rules: 22 

Non-standard options: attempt to group attributes
</code></pre>
<p>Try as I might, Google as I do, read <code>parsnips</code> documentation, etc., I <em>cannot</em> find out how to view the decision tree. Can anyone tell me how to view the actual tree it has created?</p>
",47,1,2,5,r;decision-tree;tidymodels;r-parsnip;c5.0,2022-07-03 12:16:56,2022-07-03 12:16:56,2022-07-04 07:10:41,i am working through  machine learning  amp  r expert techniques for predictive modeling  by brett lantz  i am using the tidymodels suite as i try the example modeling exercises in r  i am working through chapter  in which you build a decision tree with the c  algorithm  i hav e created the model using the code shown below this has worked successfully  try as i might  google as i do  read parsnips documentation  etc   i cannot find out how to view the decision tree  can anyone tell me how to view the actual tree it has created ,how do i extract the classification tree from this parsnip model in r ,working machine learning amp r expert techniques predictive modeling brett lantz using tidymodels suite try example modeling exercises r working chapter build decision tree c algorithm hav e created model using code shown worked successfully try might google read parsnips documentation etc cannot find view decision tree anyone tell view actual tree created,extract classification tree parsnip model r,extract classification tree parsnip model rworking machine learning amp r expert techniques predictive modeling brett lantz using tidymodels suite try example modeling exercises r working chapter build decision tree c algorithm hav e created model using code shown worked successfully try might google read parsnips documentation etc cannot find view decision tree anyone tell view actual tree created,"['extract', 'classification', 'tree', 'parsnip', 'model', 'rworking', 'machine', 'learning', 'amp', 'r', 'expert', 'techniques', 'predictive', 'modeling', 'brett', 'lantz', 'using', 'tidymodels', 'suite', 'try', 'example', 'modeling', 'exercises', 'r', 'working', 'chapter', 'build', 'decision', 'tree', 'c', 'algorithm', 'hav', 'e', 'created', 'model', 'using', 'code', 'shown', 'worked', 'successfully', 'try', 'might', 'google', 'read', 'parsnips', 'documentation', 'etc', 'can', 'not', 'find', 'view', 'decision', 'tree', 'anyone', 'tell', 'view', 'actual', 'tree', 'created']","['extract', 'classif', 'tree', 'parsnip', 'model', 'rwork', 'machin', 'learn', 'amp', 'r', 'expert', 'techniqu', 'predict', 'model', 'brett', 'lantz', 'use', 'tidymodel', 'suit', 'tri', 'exampl', 'model', 'exercis', 'r', 'work', 'chapter', 'build', 'decis', 'tree', 'c', 'algorithm', 'hav', 'e', 'creat', 'model', 'use', 'code', 'shown', 'work', 'success', 'tri', 'might', 'googl', 'read', 'parsnip', 'document', 'etc', 'can', 'not', 'find', 'view', 'decis', 'tree', 'anyon', 'tell', 'view', 'actual', 'tree', 'creat']"
152,163,163,19474127,72848936,Generate costum data using machine learning,"<p>Hello I have a dataset containing a word and a distorted version of it, is there a way to train a model to predict from the distorted word the normal version of it? Example: if I have &quot;/--pPython&quot; can the program output python using the dataset <a href=""https://i.stack.imgur.com/v5N7u.png"" rel=""nofollow noreferrer"">dataset.csv</a></p>
",25,0,-3,5,python-3.x;csv;machine-learning;dataset;tensorflow-datasets,2022-07-03 23:53:45,2022-07-03 23:53:45,2022-07-03 23:56:55,hello i have a dataset containing a word and a distorted version of it  is there a way to train a model to predict from the distorted word the normal version of it  example  if i have     ppython  can the program output python using the dataset ,generate costum data using machine learning,hello dataset containing word distorted version way train model predict distorted word normal version example ppython program output python using dataset,generate costum data using machine learning,generate costum data using machine learninghello dataset containing word distorted version way train model predict distorted word normal version example ppython program output python using dataset,"['generate', 'costum', 'data', 'using', 'machine', 'learninghello', 'dataset', 'containing', 'word', 'distorted', 'version', 'way', 'train', 'model', 'predict', 'distorted', 'word', 'normal', 'version', 'example', 'ppython', 'program', 'output', 'python', 'using', 'dataset']","['gener', 'costum', 'data', 'use', 'machin', 'learninghello', 'dataset', 'contain', 'word', 'distort', 'version', 'way', 'train', 'model', 'predict', 'distort', 'word', 'normal', 'version', 'exampl', 'ppython', 'program', 'output', 'python', 'use', 'dataset']"
153,164,164,14384656,72848458,How do I fix ValueError: Classification metrics can&#39;t handle a mix of multiclass and continuous-multioutput targets?,"<p>I am currently learning how sentiment analysis and machine learning works. I am following this <a href=""https://medium.com/mlearning-ai/twitter-sentiment-analysis-with-deep-learning-using-bert-and-hugging-face-830005bcdbbf"" rel=""nofollow noreferrer"">tutorial</a> and this is the <a href=""https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT/blob/main/Notebook.ipynb"" rel=""nofollow noreferrer"">Github</a> source code. When it comes to training the model, I am facing this error:</p>
<pre><code>ValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets
</code></pre>
<p>Upon researching solutions on the internet, I found that it could be due to the presence of continuous values. I have tried to change this line but to no avail:<br />
<code>val_f1 = f1_score_func(predictions, true_vals)</code><br />
to<br />
<code>val_f1 = f1_score_func(predictions, np.round(true_vals))</code></p>
<p>Here is the relevant part of the codes:</p>
<pre><code>def evaluate(dataloader_val):

    #evaluation mode disables the dropout layer 
    model.eval()
    
    #tracking variables
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in tqdm(dataloader_val):
        
        #load into GPU
        batch = tuple(b.to(device) for b in batch)
        
        #define inputs
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2]}

        #compute logits
        with torch.no_grad():        
            outputs = model(**inputs)
        
        #compute loss
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        #compute accuracy
        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    #compute average loss
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals


for epoch in tqdm(range(1, epochs+1)):

    #set model in train mode
    model.train()

    #tracking variable
    loss_train_total = 0
    
    #set up progress bar
    progress_bar = tqdm(dataloader_train, 
                        desc='Epoch {:1d}'.format(epoch), 
                        leave=False, 
                        disable=False)
    
    for batch in progress_bar:
        #set gradient to 0
        model.zero_grad()

        #load into GPU
        batch = tuple(b.to(device) for b in batch)

        #define inputs
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1],
                  'labels': batch[2]}
        
        outputs = model(**inputs)
        loss = outputs[0] #output.loss
        loss_train_total +=loss.item()

        #backward pass to get gradients
        loss.backward()
        
        #clip the norm of the gradients to 1.0 to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        #update optimizer
        optimizer.step()

        #update scheduler
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     
    
    tqdm.write('\nEpoch {epoch}')
    
    #print training result
    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    #evaluate
    val_loss, predictions, true_vals = evaluate(dataloader_val)
    
    #f1 score
    #val_f1 = f1_score_func(predictions, true_vals) #old
    val_f1 = f1_score_func(predictions, np.round(true_vals)) #new
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (weighted): {val_f1}')
</code></pre>
<p>I am new to this and I really wanna know the actual reasoning and working solution behind the error I am facing. Thank you for your help.</p>
",54,0,0,5,python;nlp;sentiment-analysis;bert-language-model;transformer,2022-07-03 22:40:14,2022-07-03 22:40:14,2022-07-03 23:54:34,i am currently learning how sentiment analysis and machine learning works  i am following this  and this is the  source code  when it comes to training the model  i am facing this error  here is the relevant part of the codes  i am new to this and i really wanna know the actual reasoning and working solution behind the error i am facing  thank you for your help ,how do i fix valueerror  classification metrics can   t handle a mix of multiclass and continuous multioutput targets ,currently learning sentiment analysis machine learning works following source code comes training model facing error relevant part codes really wanna know actual reasoning working solution behind error facing thank help,fix valueerror classification metrics handle mix multiclass continuous multioutput targets,fix valueerror classification metrics handle mix multiclass continuous multioutput targetscurrently learning sentiment analysis machine learning works following source code comes training model facing error relevant part codes really wanna know actual reasoning working solution behind error facing thank help,"['fix', 'valueerror', 'classification', 'metrics', 'handle', 'mix', 'multiclass', 'continuous', 'multioutput', 'targetscurrently', 'learning', 'sentiment', 'analysis', 'machine', 'learning', 'works', 'following', 'source', 'code', 'comes', 'training', 'model', 'facing', 'error', 'relevant', 'part', 'codes', 'really', 'wan', 'na', 'know', 'actual', 'reasoning', 'working', 'solution', 'behind', 'error', 'facing', 'thank', 'help']","['fix', 'valueerror', 'classif', 'metric', 'handl', 'mix', 'multiclass', 'continu', 'multioutput', 'targetscurr', 'learn', 'sentiment', 'analysi', 'machin', 'learn', 'work', 'follow', 'sourc', 'code', 'come', 'train', 'model', 'face', 'error', 'relev', 'part', 'code', 'realli', 'wan', 'na', 'know', 'actual', 'reason', 'work', 'solut', 'behind', 'error', 'face', 'thank', 'help']"
154,165,165,19090141,72194943,How to find location of the Microsoft Machine Learning Server installation files in SQL 2019,"<p><a href=""https://i.stack.imgur.com/NRWUA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am stuck here and can't find location of the Microsoft Machine Learning Server installation files. If any one know please answer on this issue.</p>
",98,1,0,2,sql-server;machine-learning,2022-05-11 08:36:51,2022-05-11 08:36:51,2022-07-03 22:56:21, i am stuck here and can t find location of the microsoft machine learning server installation files  if any one know please answer on this issue ,how to find location of the microsoft machine learning server installation files in sql ,stuck find location microsoft machine learning server installation files one know please answer issue,find location microsoft machine learning server installation files sql,find location microsoft machine learning server installation files sqlstuck find location microsoft machine learning server installation files one know please answer issue,"['find', 'location', 'microsoft', 'machine', 'learning', 'server', 'installation', 'files', 'sqlstuck', 'find', 'location', 'microsoft', 'machine', 'learning', 'server', 'installation', 'files', 'one', 'know', 'please', 'answer', 'issue']","['find', 'locat', 'microsoft', 'machin', 'learn', 'server', 'instal', 'file', 'sqlstuck', 'find', 'locat', 'microsoft', 'machin', 'learn', 'server', 'instal', 'file', 'one', 'know', 'pleas', 'answer', 'issu']"
155,167,167,15633016,67088805,"C++ not printing to console during while loop, only after loop is finished","<p><strong>Background</strong><br />
I'm currently writing some code for a naughts and crosses machine learning program based on M.E.N.A.C.E, and I have finished the code for the actual machine learning and for playing against the computer, as well as another computer to play games against it to train it. The user can enter that they'd like to let the computer train itself, and then enter the number of games to play.</p>
<p><strong>Problem</strong><br />
I'm trying to make a % completion display that overwrites itself each time it updates. The issue I have is that for some reason the code won't print anything that should be printed during the while loop, instead waiting until the end to print all of it in one go. I am using '\r' (carriage return) to overwrite the last printed text. If I remove the carriage return, the while loop prints the text on each iteration like it should do. I don't have any idea what's causing this problem as I'm quite new to C++.</p>
<p>I am programming in Repl.it since I'm not able to install an IDE on the computer I'm using.</p>
<p>Here is the subroutine for calculating and displaying the % completion (using namespace std).</p>
<pre><code>void calcCompletion(int a, int b)
{
  int completion = (static_cast&lt;float&gt;(a)/b) * 100;
  cout &lt;&lt; '\r';
  cout &lt;&lt; completion &lt;&lt; &quot;%&quot;;
}
</code></pre>
<p>And here is the start of the while loop where the procedure is called (mode is always 2 when I am testing this).</p>
<pre><code>while(gamesPlayed &lt; gameEnd)
  {
  //permutations();
  if(mode != &quot;1&quot;)
  {
    calcCompletion(gamesPlayed, gameEnd);
  }
</code></pre>
<p>It's a very long while loop so I won't show the whole thing (hence why the curly brackets do not match up).</p>
<p>And here is the output:</p>
<pre><code> clang++-7 -pthread -std=c++17 -o main ai.cpp base3.cpp main.cpp otherai.cpp permutations.cpp winCheck.cpp
 ./main
Enter mode. 
1 - Play the AI 
2 - Train the AI
2
How many games would you like the AI to play?
5

Simulating...
80%
Games complete.
Games played: 5
Games won: 1
Games lost: 0
Games drawn: 4
Win Percentage: 20%
Loss Percentage: 0%
--------------
</code></pre>
<p>It just waits until it is done with the while loop and then prints the last number, instead of printing as it goes.</p>
<p>I have tested trying to overwrite something I've written with no time delay in another code, it works fine so clearly being overwritten too quickly isn't the problem.</p>
",286,1,0,5,c++;while-loop;iostream;cout;carriage-return,2021-04-14 14:40:34,2021-04-14 14:40:34,2022-07-03 19:33:24,i am programming in repl it since i m not able to install an ide on the computer i m using  here is the subroutine for calculating and displaying the   completion  using namespace std   and here is the start of the while loop where the procedure is called  mode is always  when i am testing this   it s a very long while loop so i won t show the whole thing  hence why the curly brackets do not match up   and here is the output  it just waits until it is done with the while loop and then prints the last number  instead of printing as it goes  i have tested trying to overwrite something i ve written with no time delay in another code  it works fine so clearly being overwritten too quickly isn t the problem ,c   not printing to console during while loop  only after loop is finished,programming repl since able install ide computer using subroutine calculating displaying completion using namespace std start loop procedure called mode always testing long loop show whole thing hence curly brackets match output waits done loop prints last number instead printing goes tested trying overwrite something written time delay another code works fine clearly overwritten quickly problem,c printing console loop loop finished,c printing console loop loop finishedprogramming repl since able install ide computer using subroutine calculating displaying completion using namespace std start loop procedure called mode always testing long loop show whole thing hence curly brackets match output waits done loop prints last number instead printing goes tested trying overwrite something written time delay another code works fine clearly overwritten quickly problem,"['c', 'printing', 'console', 'loop', 'loop', 'finishedprogramming', 'repl', 'since', 'able', 'install', 'ide', 'computer', 'using', 'subroutine', 'calculating', 'displaying', 'completion', 'using', 'namespace', 'std', 'start', 'loop', 'procedure', 'called', 'mode', 'always', 'testing', 'long', 'loop', 'show', 'whole', 'thing', 'hence', 'curly', 'brackets', 'match', 'output', 'waits', 'done', 'loop', 'prints', 'last', 'number', 'instead', 'printing', 'goes', 'tested', 'trying', 'overwrite', 'something', 'written', 'time', 'delay', 'another', 'code', 'works', 'fine', 'clearly', 'overwritten', 'quickly', 'problem']","['c', 'print', 'consol', 'loop', 'loop', 'finishedprogram', 'repl', 'sinc', 'abl', 'instal', 'ide', 'comput', 'use', 'subroutin', 'calcul', 'display', 'complet', 'use', 'namespac', 'std', 'start', 'loop', 'procedur', 'call', 'mode', 'alway', 'test', 'long', 'loop', 'show', 'whole', 'thing', 'henc', 'curli', 'bracket', 'match', 'output', 'wait', 'done', 'loop', 'print', 'last', 'number', 'instead', 'print', 'goe', 'test', 'tri', 'overwrit', 'someth', 'written', 'time', 'delay', 'anoth', 'code', 'work', 'fine', 'clearli', 'overwritten', 'quickli', 'problem']"
156,168,168,15535535,72846936,Python package is missed for machine learning and neural networks on windows 10_64bit Enterprise LTSC,"<p>I’m running a neural network code downloaded from github (<a href=""https://github.com/okada39/pinn_wave"" rel=""nofollow noreferrer"">https://github.com/okada39/pinn_wave</a>) that solves the 1D wave equation via PINNs algorithm.
After running my executed neural network code.py by cmd I got:</p>
<p>Traceback (most recent call last):
File &quot;file path&quot;, line 1, in 
import lib.tf_silent
ModuleNotFoundError: No module
named 'lib.tf_silent'</p>
<p>The Python code is as follows:</p>
<pre><code>import lib.tf_silent
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib.gridspec import GridSpec
from lib.pinn import PINN
from lib.network import Network
from lib.optimizer import L_BFGS_B

def u0(tx, c=1, k=2, sd=0.5):
&quot;&quot;&quot;
Initial wave form.
Args:
    tx: variables (t, x) as tf.Tensor.
    c: wave velocity.
    k: wave number.
    sd: standard deviation.
Returns:
    u(t, x) as tf.Tensor.
&quot;&quot;&quot;

t = tx[..., 0, None]
x = tx[..., 1, None]
z = k*x - (c*k)*t
return tf.sin(z) * tf.exp(-(0.5*z/sd)**2)

def du0_dt(tx):
&quot;&quot;&quot;
First derivative of t for the initial wave form.
Args:
    tx: variables (t, x) as tf.Tensor.
Returns:
    du(t, x)/dt as tf.Tensor.
&quot;&quot;&quot;

with tf.GradientTape() as g:
    g.watch(tx)
    u = u0(tx)
du_dt = g.batch_jacobian(u, tx)[..., 0]
return du_dt

if __name__ == '__main__':
&quot;&quot;&quot;
Test the physics informed neural network (PINN) model for the wave equation.
&quot;&quot;&quot;

# number of training samples
num_train_samples = 10000
# number of test samples
num_test_samples = 1000

# build a core network model
network = Network.build()
network.summary()
# build a PINN model
pinn = PINN(network).build()

# create training input
tx_eqn = np.random.rand(num_train_samples, 2)
tx_eqn[..., 0] = 4*tx_eqn[..., 0]                # t =  0 ~ +4
tx_eqn[..., 1] = 2*tx_eqn[..., 1] - 1            # x = -1 ~ +1
tx_ini = np.random.rand(num_train_samples, 2)
tx_ini[..., 0] = 0                               # t = 0
tx_ini[..., 1] = 2*tx_ini[..., 1] - 1            # x = -1 ~ +1
tx_bnd = np.random.rand(num_train_samples, 2)
tx_bnd[..., 0] = 4*tx_bnd[..., 0]                # t =  0 ~ +4
tx_bnd[..., 1] = 2*np.round(tx_bnd[..., 1]) - 1  # x = -1 or +1
# create training output
u_zero = np.zeros((num_train_samples, 1))
u_ini = u0(tf.constant(tx_ini)).numpy()
du_dt_ini = du0_dt(tf.constant(tx_ini)).numpy()

# train the model using L-BFGS-B algorithm
x_train = [tx_eqn, tx_ini, tx_bnd]
y_train = [u_zero, u_ini, du_dt_ini, u_zero]
lbfgs = L_BFGS_B(model=pinn, x_train=x_train, y_train=y_train)
lbfgs.fit()

# predict u(t,x) distribution
t_flat = np.linspace(0, 4, num_test_samples)
x_flat = np.linspace(-1, 1, num_test_samples)
t, x = np.meshgrid(t_flat, x_flat)
tx = np.stack([t.flatten(), x.flatten()], axis=-1)
u = network.predict(tx, batch_size=num_test_samples)
u = u.reshape(t.shape)

# plot u(t,x) distribution as a color-map
fig = plt.figure(figsize=(7,4))
gs = GridSpec(2, 3)
plt.subplot(gs[0, :])
vmin, vmax = -0.5, +0.5
plt.pcolormesh(t, x, u, cmap='rainbow', norm=Normalize(vmin=vmin, vmax=vmax))
plt.xlabel('t')
plt.ylabel('x')
cbar = plt.colorbar(pad=0.05, aspect=10)
cbar.set_label('u(t,x)')
cbar.mappable.set_clim(vmin, vmax)
# plot u(t=const, x) cross-sections
t_cross_sections = [1, 2, 3]
for i, t_cs in enumerate(t_cross_sections):
    plt.subplot(gs[1, i])
    tx = np.stack([np.full(t_flat.shape, t_cs), x_flat], axis=-1)
    u = network.predict(tx, batch_size=num_test_samples)
    plt.plot(x_flat, u)
    plt.title('t={}'.format(t_cs))
    plt.xlabel('x')
    plt.ylabel('u(t,x)')
plt.tight_layout()
plt.savefig('result_img_neumann.png', transparent=True)
plt.show()
</code></pre>
<p>How to fix this? please help?</p>
",34,0,0,3,python;tensorflow;importerror,2022-07-03 19:05:27,2022-07-03 19:05:27,2022-07-03 19:05:27,the python code is as follows  how to fix this  please help ,python package is missed for machine learning and neural networks on windows _bit enterprise ltsc,python code follows fix please help,python package missed machine learning neural networks windows _bit enterprise ltsc,python package missed machine learning neural networks windows _bit enterprise ltscpython code follows fix please help,"['python', 'package', 'missed', 'machine', 'learning', 'neural', 'networks', 'windows', '_bit', 'enterprise', 'ltscpython', 'code', 'follows', 'fix', 'please', 'help']","['python', 'packag', 'miss', 'machin', 'learn', 'neural', 'network', 'window', '_bit', 'enterpris', 'ltscpython', 'code', 'follow', 'fix', 'pleas', 'help']"
157,169,169,19375021,72846088,sklearn is not visible in VSC,"<p>I installed sklearn by <strong>python3 -m pip install scikit-learn</strong></p>
<p>I can see its version, so it's ok:
<strong>python -m pip show scikit-learn</strong>
<em>Name: scikit-learn
Version: 1.1.1
Summary: A set of python modules for machine learning and data mining
Home-page: <a href=""http://scikit-learn.org"" rel=""nofollow noreferrer"">http://scikit-learn.org</a>
Author:
Author-email:
License: new BSD
Location: /Users/imac/opt/anaconda3/lib/python3.9/site-packages
Requires: threadpoolctl, scipy, numpy, joblib
Required-by: sklearn, scikit-learn-intelex</em></p>
<p>but VSC tell me the package it not there. Nothing of the kind with numpy or pandas or other libs.</p>
<p>IDLE tells the same btw((</p>
<p>tried to google but no success. Can anyone help please, I'm just learning Python.</p>
",17,0,0,2,python;scikit-learn,2022-07-03 16:57:03,2022-07-03 16:57:03,2022-07-03 16:58:02,i installed sklearn by python  m pip install scikit learn but vsc tell me the package it not there  nothing of the kind with numpy or pandas or other libs  idle tells the same btw   tried to google but no success  can anyone help please  i m just learning python ,sklearn is not visible in vsc,installed sklearn python pip install scikit learn vsc tell package nothing kind numpy pandas libs idle tells btw tried google success anyone help please learning python,sklearn visible vsc,sklearn visible vscinstalled sklearn python pip install scikit learn vsc tell package nothing kind numpy pandas libs idle tells btw tried google success anyone help please learning python,"['sklearn', 'visible', 'vscinstalled', 'sklearn', 'python', 'pip', 'install', 'scikit', 'learn', 'vsc', 'tell', 'package', 'nothing', 'kind', 'numpy', 'pandas', 'libs', 'idle', 'tells', 'btw', 'tried', 'google', 'success', 'anyone', 'help', 'please', 'learning', 'python']","['sklearn', 'visibl', 'vscinstal', 'sklearn', 'python', 'pip', 'instal', 'scikit', 'learn', 'vsc', 'tell', 'packag', 'noth', 'kind', 'numpi', 'panda', 'lib', 'idl', 'tell', 'btw', 'tri', 'googl', 'success', 'anyon', 'help', 'pleas', 'learn', 'python']"
158,170,170,18155045,72844013,Visualization and Analysis of Clusters for huge data and features,"<p>I am newbie to Machine learning. I am trying to use Clustering Algorithms like K-means on my data (<code>.csv</code>) but it has just too many columns (features) and rows. The objective of using Clustering is to find out new information about the data with it's existing information as base. I am dealing with YAML (<code>.yaml</code>/<code>.yml</code>) files consisting hierarchical syntax and therefore, I am converting each file into a vector. Each vector (1D) generated is passed on to a <code>.csv</code> file.  Let me elaborate my problem here:</p>
<p>I have a <code>.csv</code> generated which contains around 7k+ columns and presumably more than 7k+ rows as well. I took around 3.3K rows (as an experiment) and tried clustering them which of course, took me a while but eventually got clustered. I did also try clustering after using some feature selection techniques such as Information gain etc., which have shown to-some-extent better results (through K-means). However, my problems starts when <strong>I am unable to visualize this huge data</strong>   and therefore, I am to take the manual approach of looking into samples of each cluster. Here are my questions:</p>
<ol>
<li><p>How do I visualize such a huge data? (after clustering)</p>
</li>
<li><p>How do I analyze each cluster?</p>
</li>
</ol>
<p>Thanks in Advance! If you require any extra details, do comment!</p>
<p>P.S. I did try other algorithms such as Agglomerative, BIRCH and GMMs. If you feel some other algorithm may work in my case, do drop by in the comments!</p>
",32,0,0,5,machine-learning;yaml;bigdata;cluster-analysis;visualization,2022-07-03 10:33:47,2022-07-03 10:33:47,2022-07-03 14:11:28,i am newbie to machine learning  i am trying to use clustering algorithms like k means on my data   csv  but it has just too many columns  features  and rows  the objective of using clustering is to find out new information about the data with it s existing information as base  i am dealing with yaml   yaml  yml  files consisting hierarchical syntax and therefore  i am converting each file into a vector  each vector  d  generated is passed on to a  csv file   let me elaborate my problem here  i have a  csv generated which contains around k  columns and presumably more than k  rows as well  i took around  k rows  as an experiment  and tried clustering them which of course  took me a while but eventually got clustered  i did also try clustering after using some feature selection techniques such as information gain etc   which have shown to some extent better results  through k means   however  my problems starts when i am unable to visualize this huge data   and therefore  i am to take the manual approach of looking into samples of each cluster  here are my questions  how do i visualize such a huge data   after clustering  how do i analyze each cluster  thanks in advance  if you require any extra details  do comment  p s  i did try other algorithms such as agglomerative  birch and gmms  if you feel some other algorithm may work in my case  do drop by in the comments ,visualization and analysis of clusters for huge data and features,newbie machine learning trying use clustering algorithms like k means data csv many columns features rows objective using clustering find information data existing information base dealing yaml yaml yml files consisting hierarchical syntax therefore converting file vector vector generated passed csv file let elaborate problem csv generated contains around k columns presumably k rows well took around k rows experiment tried clustering course took eventually got clustered also try clustering using feature selection techniques information gain etc shown extent better results k means however problems starts unable visualize huge data therefore take manual approach looking samples cluster questions visualize huge data clustering analyze cluster thanks advance require extra details comment p try algorithms agglomerative birch gmms feel algorithm may work case drop comments,visualization analysis clusters huge data features,visualization analysis clusters huge data featuresnewbie machine learning trying use clustering algorithms like k means data csv many columns features rows objective using clustering find information data existing information base dealing yaml yaml yml files consisting hierarchical syntax therefore converting file vector vector generated passed csv file let elaborate problem csv generated contains around k columns presumably k rows well took around k rows experiment tried clustering course took eventually got clustered also try clustering using feature selection techniques information gain etc shown extent better results k means however problems starts unable visualize huge data therefore take manual approach looking samples cluster questions visualize huge data clustering analyze cluster thanks advance require extra details comment p try algorithms agglomerative birch gmms feel algorithm may work case drop comments,"['visualization', 'analysis', 'clusters', 'huge', 'data', 'featuresnewbie', 'machine', 'learning', 'trying', 'use', 'clustering', 'algorithms', 'like', 'k', 'means', 'data', 'csv', 'many', 'columns', 'features', 'rows', 'objective', 'using', 'clustering', 'find', 'information', 'data', 'existing', 'information', 'base', 'dealing', 'yaml', 'yaml', 'yml', 'files', 'consisting', 'hierarchical', 'syntax', 'therefore', 'converting', 'file', 'vector', 'vector', 'generated', 'passed', 'csv', 'file', 'let', 'elaborate', 'problem', 'csv', 'generated', 'contains', 'around', 'k', 'columns', 'presumably', 'k', 'rows', 'well', 'took', 'around', 'k', 'rows', 'experiment', 'tried', 'clustering', 'course', 'took', 'eventually', 'got', 'clustered', 'also', 'try', 'clustering', 'using', 'feature', 'selection', 'techniques', 'information', 'gain', 'etc', 'shown', 'extent', 'better', 'results', 'k', 'means', 'however', 'problems', 'starts', 'unable', 'visualize', 'huge', 'data', 'therefore', 'take', 'manual', 'approach', 'looking', 'samples', 'cluster', 'questions', 'visualize', 'huge', 'data', 'clustering', 'analyze', 'cluster', 'thanks', 'advance', 'require', 'extra', 'details', 'comment', 'p', 'try', 'algorithms', 'agglomerative', 'birch', 'gmms', 'feel', 'algorithm', 'may', 'work', 'case', 'drop', 'comments']","['visual', 'analysi', 'cluster', 'huge', 'data', 'featuresnewbi', 'machin', 'learn', 'tri', 'use', 'cluster', 'algorithm', 'like', 'k', 'mean', 'data', 'csv', 'mani', 'column', 'featur', 'row', 'object', 'use', 'cluster', 'find', 'inform', 'data', 'exist', 'inform', 'base', 'deal', 'yaml', 'yaml', 'yml', 'file', 'consist', 'hierarch', 'syntax', 'therefor', 'convert', 'file', 'vector', 'vector', 'gener', 'pass', 'csv', 'file', 'let', 'elabor', 'problem', 'csv', 'gener', 'contain', 'around', 'k', 'column', 'presum', 'k', 'row', 'well', 'took', 'around', 'k', 'row', 'experi', 'tri', 'cluster', 'cours', 'took', 'eventu', 'got', 'cluster', 'also', 'tri', 'cluster', 'use', 'featur', 'select', 'techniqu', 'inform', 'gain', 'etc', 'shown', 'extent', 'better', 'result', 'k', 'mean', 'howev', 'problem', 'start', 'unabl', 'visual', 'huge', 'data', 'therefor', 'take', 'manual', 'approach', 'look', 'sampl', 'cluster', 'question', 'visual', 'huge', 'data', 'cluster', 'analyz', 'cluster', 'thank', 'advanc', 'requir', 'extra', 'detail', 'comment', 'p', 'tri', 'algorithm', 'agglom', 'birch', 'gmm', 'feel', 'algorithm', 'may', 'work', 'case', 'drop', 'comment']"
159,172,172,19469286,72842705,Machine Learning: failed to upload .h5 file to Python Notebook,"<p>After installed Colab in my labtop with Windows system I connected it with Google drive(<a href=""https://drive.google.com/drive/my-drive"" rel=""nofollow noreferrer"">https://drive.google.com/drive/my-drive</a>). Then I opened a Python Notebook from Colab. There I read in file from Notebook with path: /content/mydrive/SVHN_single_grey1.h5</p>
<p>But, I got OSError message:
OSError: Unable to open file (unable to open file: name = '/content/my=drive/SVHN_single_grey1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)</p>
<p>I explored the problem for some time, it seems that the path is correct.</p>
<p>Anyone can suggestion possible solution for it?</p>
<p>Thanks</p>
",29,0,0,1,python,2022-07-03 03:31:45,2022-07-03 03:31:45,2022-07-03 06:11:53,after installed colab in my labtop with windows system i connected it with google drive    then i opened a python notebook from colab  there i read in file from notebook with path   content mydrive svhn_single_grey h i explored the problem for some time  it seems that the path is correct  anyone can suggestion possible solution for it  thanks,machine learning  failed to upload  h file to python notebook,installed colab labtop windows system connected google drive opened python notebook colab read file notebook path content mydrive svhn_single_grey h explored problem time seems path correct anyone suggestion possible solution thanks,machine learning failed upload h file python notebook,machine learning failed upload h file python notebookinstalled colab labtop windows system connected google drive opened python notebook colab read file notebook path content mydrive svhn_single_grey h explored problem time seems path correct anyone suggestion possible solution thanks,"['machine', 'learning', 'failed', 'upload', 'h', 'file', 'python', 'notebookinstalled', 'colab', 'labtop', 'windows', 'system', 'connected', 'google', 'drive', 'opened', 'python', 'notebook', 'colab', 'read', 'file', 'notebook', 'path', 'content', 'mydrive', 'svhn_single_grey', 'h', 'explored', 'problem', 'time', 'seems', 'path', 'correct', 'anyone', 'suggestion', 'possible', 'solution', 'thanks']","['machin', 'learn', 'fail', 'upload', 'h', 'file', 'python', 'notebookinstal', 'colab', 'labtop', 'window', 'system', 'connect', 'googl', 'drive', 'open', 'python', 'notebook', 'colab', 'read', 'file', 'notebook', 'path', 'content', 'mydriv', 'svhn_single_grey', 'h', 'explor', 'problem', 'time', 'seem', 'path', 'correct', 'anyon', 'suggest', 'possibl', 'solut', 'thank']"
160,173,173,5212614,72843042,How can we deploy a Machine Learning Model using Flask?,"<p>I am trying, for the first time ever, to deploy a ML model, using Flask. I'm following the instructions from the link below.</p>
<p><a href=""https://towardsdatascience.com/deploy-a-machine-learning-model-using-flask-da580f84e60c"" rel=""nofollow noreferrer"">https://towardsdatascience.com/deploy-a-machine-learning-model-using-flask-da580f84e60c</a></p>
<p>I created three separate and distinct .py files named 'model.py', 'server.py', and 'request.py'. I open my Anaconda Prompt end entered this: 'C:\Users\ryans&gt;C:\Users\ryans\model.py'</p>
<p>Now, I get this.</p>
<p><a href=""https://i.stack.imgur.com/iuRf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iuRf4.png"" alt=""enter image description here"" /></a></p>
<p>I definitely have Numpy installed! Something must be wrong with my setup, or maybe the way I am starting the process is wrong, but I'm not sure what the issue is. Has anyone encountered this problem before.</p>
",26,1,0,4,python;numpy;anaconda;conda,2022-07-03 04:55:06,2022-07-03 04:55:06,2022-07-03 05:30:18,i am trying  for the first time ever  to deploy a ml model  using flask  i m following the instructions from the link below   i created three separate and distinct  py files named  model py    server py   and  request py   i open my anaconda prompt end entered this   c  users ryans gt c  users ryans model py  now  i get this   i definitely have numpy installed  something must be wrong with my setup  or maybe the way i am starting the process is wrong  but i m not sure what the issue is  has anyone encountered this problem before ,how can we deploy a machine learning model using flask ,trying first time ever deploy ml model using flask following instructions link created three separate distinct py files named model py server py request py open anaconda prompt end entered c users ryans gt c users ryans model py get definitely numpy installed something must wrong setup maybe way starting process wrong sure issue anyone encountered problem,deploy machine learning model using flask,deploy machine learning model using flasktrying first time ever deploy ml model using flask following instructions link created three separate distinct py files named model py server py request py open anaconda prompt end entered c users ryans gt c users ryans model py get definitely numpy installed something must wrong setup maybe way starting process wrong sure issue anyone encountered problem,"['deploy', 'machine', 'learning', 'model', 'using', 'flasktrying', 'first', 'time', 'ever', 'deploy', 'ml', 'model', 'using', 'flask', 'following', 'instructions', 'link', 'created', 'three', 'separate', 'distinct', 'py', 'files', 'named', 'model', 'py', 'server', 'py', 'request', 'py', 'open', 'anaconda', 'prompt', 'end', 'entered', 'c', 'users', 'ryans', 'gt', 'c', 'users', 'ryans', 'model', 'py', 'get', 'definitely', 'numpy', 'installed', 'something', 'must', 'wrong', 'setup', 'maybe', 'way', 'starting', 'process', 'wrong', 'sure', 'issue', 'anyone', 'encountered', 'problem']","['deploy', 'machin', 'learn', 'model', 'use', 'flasktri', 'first', 'time', 'ever', 'deploy', 'ml', 'model', 'use', 'flask', 'follow', 'instruct', 'link', 'creat', 'three', 'separ', 'distinct', 'py', 'file', 'name', 'model', 'py', 'server', 'py', 'request', 'py', 'open', 'anaconda', 'prompt', 'end', 'enter', 'c', 'user', 'ryan', 'gt', 'c', 'user', 'ryan', 'model', 'py', 'get', 'definit', 'numpi', 'instal', 'someth', 'must', 'wrong', 'setup', 'mayb', 'way', 'start', 'process', 'wrong', 'sure', 'issu', 'anyon', 'encount', 'problem']"
161,174,174,19240921,72838009,What does the &#39;CI&#39; terminology mean for ML based modelling?,"<p>I was going through some papers about Machine learning applied to cancer datasets.</p>
<p>I found a specific term with which I was not familiar.</p>
<blockquote>
<p>the AI system achieved an AUROC of 0.976 (95% <strong>CI: 0.972</strong>, 0.980) in identifying ... malignant lesions.</p>
</blockquote>
<p>I dug a bit about this 'CI'... came across another article about <em>Continuous Integration to Machine Learning Projects</em></p>
<p>Link to that: <a href=""https://towardsdatascience.com/how-i-apply-continuous-integration-to-machine-learning-projects-8273274a565a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-i-apply-continuous-integration-to-machine-learning-projects-8273274a565a</a></p>
<p>Are these the same? If yes can someone please elaborate a bit on this topic? I am confused..</p>
",21,0,-1,1,machine-learning,2022-07-02 15:04:20,2022-07-02 15:04:20,2022-07-03 03:13:51,i was going through some papers about machine learning applied to cancer datasets  i found a specific term with which i was not familiar  the ai system achieved an auroc of      ci        in identifying     malignant lesions  i dug a bit about this  ci     came across another article about continuous integration to machine learning projects link to that   are these the same  if yes can someone please elaborate a bit on this topic  i am confused  ,what does the    ci    terminology mean for ml based modelling ,going papers machine learning applied cancer datasets found specific term familiar ai system achieved auroc ci identifying malignant lesions dug bit ci came across another article continuous integration machine learning projects link yes someone please elaborate bit topic confused,ci terminology mean ml based modelling,ci terminology mean ml based modellinggoing papers machine learning applied cancer datasets found specific term familiar ai system achieved auroc ci identifying malignant lesions dug bit ci came across another article continuous integration machine learning projects link yes someone please elaborate bit topic confused,"['ci', 'terminology', 'mean', 'ml', 'based', 'modellinggoing', 'papers', 'machine', 'learning', 'applied', 'cancer', 'datasets', 'found', 'specific', 'term', 'familiar', 'ai', 'system', 'achieved', 'auroc', 'ci', 'identifying', 'malignant', 'lesions', 'dug', 'bit', 'ci', 'came', 'across', 'another', 'article', 'continuous', 'integration', 'machine', 'learning', 'projects', 'link', 'yes', 'someone', 'please', 'elaborate', 'bit', 'topic', 'confused']","['ci', 'terminolog', 'mean', 'ml', 'base', 'modellinggo', 'paper', 'machin', 'learn', 'appli', 'cancer', 'dataset', 'found', 'specif', 'term', 'familiar', 'ai', 'system', 'achiev', 'auroc', 'ci', 'identifi', 'malign', 'lesion', 'dug', 'bit', 'ci', 'came', 'across', 'anoth', 'articl', 'continu', 'integr', 'machin', 'learn', 'project', 'link', 'ye', 'someon', 'pleas', 'elabor', 'bit', 'topic', 'confus']"
162,175,175,19462451,72832840,Trying to process json file returned from an API call in NodeJS - error: undefined - why?,"<p>I am receiving a JSON file from an APIcall (see below). Now, I would like to access the different keys of the JSON file to present the various values separately. However, I get an 'undefined' message in the console. Any ideas?</p>
<p>This is the code to call the API and process JSON file:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>   const input = req.body.var_1;
    console.log('Request Query:' + JSON.stringify({ ""user_input"": input}));
    const articles = [];
    const sent = 'test';
    const api_url = '...';
    const options = {
        method: 'POST',
        body: JSON.stringify({""user_input"": input}),
        headers: {'Content-Type': 'application/json'}
    }

    const response = await fetch(api_url, options);
    const results = await Promise.all([response.json()]);
    console.log(results);
    data = JSON.parse(JSON.stringify(results))
    console.log(data);
    console.log(data.Topic);</code></pre>
</div>
</div>
</p>
<p>This is the console output I get:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Request Query:{""user_input"":""xgboost""}
[
  {
    Topic: {
      '0': 'random forest , Machine Learning and Wine Quality: Finding a good wine using multiple classifications',
      '1': 'decision tree learning , Gradient Boost for Classification',
      '2': 'feature selection , Understanding Multilabel Text Classification and the related process',
      '3': 'decision tree learning , Gradient Boost for Regression',
      '4': '&lt;strong class=""markup--strong markup--h3-strong""&gt;Understanding AdaBoost&lt;/strong&gt; , Anyone starting to learn Boosting technique should start first with AdaBoost or…'
    },
    'URL/PDF': {
      '0': 'https://dev.to//leading-edje/machine-learning-and-wine-quality-finding-a-good-wine-using-multiple-classifications-4kho',
      '1': 'https://dev.to//xsabzal/gradient-boost-for-classification-2f15',
      '2': 'https://dev.to//botreetechnologies/understanding-multilabel-text-classification-and-the-related-process-n66',
      '3': 'https://dev.to//xsabzal/gradient-boost-for-regression-1e42',
      '4': 'https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe'
    },
    Doc_Type: {
      '0': 'article',
      '1': 'article',
      '2': 'article',
      '3': 'article',
      '4': 'article'
    }
  }
]
[
  {
    Topic: {
      '0': 'random forest , Machine Learning and Wine Quality: Finding a good wine using multiple classifications',
      '1': 'decision tree learning , Gradient Boost for Classification',
      '2': 'feature selection , Understanding Multilabel Text Classification and the related process',
      '3': 'decision tree learning , Gradient Boost for Regression',
      '4': '&lt;strong class=""markup--strong markup--h3-strong""&gt;Understanding AdaBoost&lt;/strong&gt; , Anyone starting to learn Boosting technique should start first with AdaBoost or…'
    },
    'URL/PDF': {
      '0': 'https://dev.to//leading-edje/machine-learning-and-wine-quality-finding-a-good-wine-using-multiple-classifications-4kho',
      '1': 'https://dev.to//xsabzal/gradient-boost-for-classification-2f15',
      '2': 'https://dev.to//botreetechnologies/understanding-multilabel-text-classification-and-the-related-process-n66',
      '3': 'https://dev.to//xsabzal/gradient-boost-for-regression-1e42',
      '4': 'https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe'
    },
    Doc_Type: {
      '0': 'article',
      '1': 'article',
      '2': 'article',
      '3': 'article',
      '4': 'article'
    }
  }
]
undefined</code></pre>
</div>
</div>
</p>
<p>Happy for any help, thx</p>
",30,2,1,3,javascript;node.js;api,2022-07-01 23:03:01,2022-07-01 23:03:01,2022-07-03 00:38:14,i am receiving a json file from an apicall  see below   now  i would like to access the different keys of the json file to present the various values separately  however  i get an  undefined  message in the console  any ideas  this is the code to call the api and process json file  this is the console output i get  happy for any help  thx,trying to process json file returned from an api call in nodejs   error  undefined   why ,receiving json file apicall see would like access different keys json file present various values separately however get undefined message console ideas code call api process json file console output get happy help thx,trying process json file returned api call nodejs error undefined,trying process json file returned api call nodejs error undefinedreceiving json file apicall see would like access different keys json file present various values separately however get undefined message console ideas code call api process json file console output get happy help thx,"['trying', 'process', 'json', 'file', 'returned', 'api', 'call', 'nodejs', 'error', 'undefinedreceiving', 'json', 'file', 'apicall', 'see', 'would', 'like', 'access', 'different', 'keys', 'json', 'file', 'present', 'various', 'values', 'separately', 'however', 'get', 'undefined', 'message', 'console', 'ideas', 'code', 'call', 'api', 'process', 'json', 'file', 'console', 'output', 'get', 'happy', 'help', 'thx']","['tri', 'process', 'json', 'file', 'return', 'api', 'call', 'nodej', 'error', 'undefinedreceiv', 'json', 'file', 'apical', 'see', 'would', 'like', 'access', 'differ', 'key', 'json', 'file', 'present', 'variou', 'valu', 'separ', 'howev', 'get', 'undefin', 'messag', 'consol', 'idea', 'code', 'call', 'api', 'process', 'json', 'file', 'consol', 'output', 'get', 'happi', 'help', 'thx']"
163,176,176,3837660,56836530,Auto-activate conda env when changing directory,"<p>I have few conda environments that I use in different projects, say:</p>

<ul>
<li>ml37 (for machine learning)</li>
<li>etl37 (for data pipelines)</li>
</ul>

<p>I have local projects organized in their own directories:</p>

<ul>
<li>apps/some_app</li>
<li>apps/other_app</li>
<li>...</li>
</ul>

<p>Each time I <code>cd</code> to a specific project, I already know which env I would like to use. So I end up doing <code>conda activate [some env]</code> each time I change directories. I feel like there must be a better way. </p>

<p>What would be a clean way to automatize this?</p>

<p>Or is my use of conda environments wrong?</p>
",1046,1,2,1,conda,2019-07-01 18:55:12,2019-07-01 18:55:12,2022-07-02 17:45:53,i have few conda environments that i use in different projects  say  i have local projects organized in their own directories  each time i cd to a specific project  i already know which env i would like to use  so i end up doing conda activate  some env  each time i change directories  i feel like there must be a better way   what would be a clean way to automatize this  or is my use of conda environments wrong ,auto activate conda env when changing directory,conda environments use different projects say local projects organized directories time cd specific project already know env would like use end conda activate env time change directories feel like must better way would clean way automatize use conda environments wrong,auto activate conda env changing directory,auto activate conda env changing directoryconda environments use different projects say local projects organized directories time cd specific project already know env would like use end conda activate env time change directories feel like must better way would clean way automatize use conda environments wrong,"['auto', 'activate', 'conda', 'env', 'changing', 'directoryconda', 'environments', 'use', 'different', 'projects', 'say', 'local', 'projects', 'organized', 'directories', 'time', 'cd', 'specific', 'project', 'already', 'know', 'env', 'would', 'like', 'use', 'end', 'conda', 'activate', 'env', 'time', 'change', 'directories', 'feel', 'like', 'must', 'better', 'way', 'would', 'clean', 'way', 'automatize', 'use', 'conda', 'environments', 'wrong']","['auto', 'activ', 'conda', 'env', 'chang', 'directoryconda', 'environ', 'use', 'differ', 'project', 'say', 'local', 'project', 'organ', 'directori', 'time', 'cd', 'specif', 'project', 'alreadi', 'know', 'env', 'would', 'like', 'use', 'end', 'conda', 'activ', 'env', 'time', 'chang', 'directori', 'feel', 'like', 'must', 'better', 'way', 'would', 'clean', 'way', 'automat', 'use', 'conda', 'environ', 'wrong']"
164,177,177,851249,72837248,what is the difference between database search technique and machine learning search technique?,"<p>We have <code>database</code> table which contains columns like <code>complaint name,complaint category and complaint sub category</code>.If i search for complaint name then i will get required details specific to passed complaint name in database. In <code>machine learning</code> technique, if we make the dataset similar to database table and create a model. If you pass same complaint name to model, it will give the same result. So then why we have to go for machine learning instead of database technique?</p>
",23,0,-1,4,machine-learning;artificial-intelligence;svm;data-mining,2022-07-02 12:53:30,2022-07-02 12:53:30,2022-07-02 13:55:11,we have database table which contains columns like complaint name complaint category and complaint sub category if i search for complaint name then i will get required details specific to passed complaint name in database  in machine learning technique  if we make the dataset similar to database table and create a model  if you pass same complaint name to model  it will give the same result  so then why we have to go for machine learning instead of database technique ,what is the difference between database search technique and machine learning search technique ,database table contains columns like complaint name complaint category complaint sub category search complaint name get required details specific passed complaint name database machine learning technique make dataset similar database table create model pass complaint name model give result go machine learning instead database technique,difference database search technique machine learning search technique,difference database search technique machine learning search techniquedatabase table contains columns like complaint name complaint category complaint sub category search complaint name get required details specific passed complaint name database machine learning technique make dataset similar database table create model pass complaint name model give result go machine learning instead database technique,"['difference', 'database', 'search', 'technique', 'machine', 'learning', 'search', 'techniquedatabase', 'table', 'contains', 'columns', 'like', 'complaint', 'name', 'complaint', 'category', 'complaint', 'sub', 'category', 'search', 'complaint', 'name', 'get', 'required', 'details', 'specific', 'passed', 'complaint', 'name', 'database', 'machine', 'learning', 'technique', 'make', 'dataset', 'similar', 'database', 'table', 'create', 'model', 'pass', 'complaint', 'name', 'model', 'give', 'result', 'go', 'machine', 'learning', 'instead', 'database', 'technique']","['differ', 'databas', 'search', 'techniqu', 'machin', 'learn', 'search', 'techniquedatabas', 'tabl', 'contain', 'column', 'like', 'complaint', 'name', 'complaint', 'categori', 'complaint', 'sub', 'categori', 'search', 'complaint', 'name', 'get', 'requir', 'detail', 'specif', 'pass', 'complaint', 'name', 'databas', 'machin', 'learn', 'techniqu', 'make', 'dataset', 'similar', 'databas', 'tabl', 'creat', 'model', 'pass', 'complaint', 'name', 'model', 'give', 'result', 'go', 'machin', 'learn', 'instead', 'databas', 'techniqu']"
165,178,178,17385780,72834060,Classification in high-dimensional data by extreme learning machine in R,"<p>When I simulate 50 high-dimensional data set from multivariate normal distribution and classify by ELM, I find the AUC scores I get very different from each other in each loop. In some loops it goes below 50%. Where am I doing wrong? I want the deviation in the scores I get to be low for each loop. How can I achieve stable and high scores for each loop? How should I make changes in data generation? I look forward to your valuable contributions.</p>
<pre><code>install.packages(&quot;MASS&quot;)
install.packages(&quot;stats&quot;)
install.packages(&quot;pROC&quot;)
install.packages(&quot;elmNNRcpp&quot;)

library(MASS)
library(stats)
library(pROC)
library(elmNNRcpp)

######################################################
# DATA SIMULATE FUNCTION
######################################################
#  rm(list = ls())
generateData&lt;- function(n,p) {
pr &lt;- seq(0.80, 0.40, length.out = p)
pr[1] &lt;- 1
covmat &lt;- toeplitz(pr)
mu= rep(0,p)
X_ &lt;- data.frame(mvrnorm(n, mu = mu, Sigma = covmat))
X &lt;- unname(as.matrix(sample(X_)))
vCoef = rnorm(ncol(X))
vProb =exp(X%*%vCoef)/(1+exp(X%*%vCoef))
Y &lt;- rbinom(nrow(X), 1, vProb)
mydata= data.frame(cbind(X,Y))
return(mydata)
}

######################################################
# SIMULATED DATA
######################################################
n &lt;- 100
p &lt;- 120
nsim &lt;- 50

set.seed(123)
mydata &lt;- list()
for (k in 1 : nsim ) {
data &lt;- generateData(n , p)
# table(data[ncol(data)])
X &lt;- data[-ncol(data)]
Y &lt;- data[ncol(data)]
mydata[[k]] &lt;- data
}

######################################################
# ELM CLASSIFICATION
######################################################
######################################################
# ELM CLASSIFICATION FUNCTION
######################################################
fELMCLASS &lt;- function(x, col_names){ 
trainIndex &lt;- sample(1:nrow(x), size=0.7*nrow(x))
trainSet &lt;- x[trainIndex,]
testSet &lt;- x[-trainIndex,]

xtrain &lt;- as.matrix(trainSet[, 1:(length(trainSet)-1)])
ytrain &lt;- as.matrix(trainSet[, length(trainSet)])
xtest &lt;- as.matrix(testSet[, 1:(length(testSet)-1)])
ytest &lt;- as.matrix(testSet[, length(testSet)])

model=elm_train(xtrain, ytrain, nhid=25 , actfun='relu')
pred.class=elm_predict(model,xtest,normalize=TRUE)
roc.model=roc(as.factor(ytest) ~ as.numeric(pred.class), direction=c(&quot;auto&quot;))

performance_metrics &lt;-t(data.frame(&quot;AUC&quot; = roc.model$auc))
return(performance_metrics)
}
######################################################
######################################################
# SAVE RESULTS
######################################################
datalist = data.frame()
for (j in 1:nsim) {
data_ELM &lt;- as.data.frame(mydata[[j]])
datalist &lt;- rbind(datalist,fELMCLASS(data_ELM, &quot;data&quot;))
}

datalist
</code></pre>
",21,0,0,5,r;performance;classification;simulation;auc,2022-07-02 01:21:35,2022-07-02 01:21:35,2022-07-02 01:21:35,when i simulate  high dimensional data set from multivariate normal distribution and classify by elm  i find the auc scores i get very different from each other in each loop  in some loops it goes below    where am i doing wrong  i want the deviation in the scores i get to be low for each loop  how can i achieve stable and high scores for each loop  how should i make changes in data generation  i look forward to your valuable contributions ,classification in high dimensional data by extreme learning machine in r,simulate high dimensional data set multivariate normal distribution classify elm find auc scores get different loop loops goes wrong want deviation scores get low loop achieve stable high scores loop make changes data generation look forward valuable contributions,classification high dimensional data extreme learning machine r,classification high dimensional data extreme learning machine rsimulate high dimensional data set multivariate normal distribution classify elm find auc scores get different loop loops goes wrong want deviation scores get low loop achieve stable high scores loop make changes data generation look forward valuable contributions,"['classification', 'high', 'dimensional', 'data', 'extreme', 'learning', 'machine', 'rsimulate', 'high', 'dimensional', 'data', 'set', 'multivariate', 'normal', 'distribution', 'classify', 'elm', 'find', 'auc', 'scores', 'get', 'different', 'loop', 'loops', 'goes', 'wrong', 'want', 'deviation', 'scores', 'get', 'low', 'loop', 'achieve', 'stable', 'high', 'scores', 'loop', 'make', 'changes', 'data', 'generation', 'look', 'forward', 'valuable', 'contributions']","['classif', 'high', 'dimension', 'data', 'extrem', 'learn', 'machin', 'rsimul', 'high', 'dimension', 'data', 'set', 'multivari', 'normal', 'distribut', 'classifi', 'elm', 'find', 'auc', 'score', 'get', 'differ', 'loop', 'loop', 'goe', 'wrong', 'want', 'deviat', 'score', 'get', 'low', 'loop', 'achiev', 'stabl', 'high', 'score', 'loop', 'make', 'chang', 'data', 'gener', 'look', 'forward', 'valuabl', 'contribut']"
166,179,179,17419684,72832970,MLFlow Experiment in Databricks Regressors,"<p>I'm new to Databricks and following a tutorial on mlflow in Databricks:</p>
<p><a href=""https://www.youtube.com/watch?v=_PxEdtAQXME&amp;t=1471s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=_PxEdtAQXME&amp;t=1471s</a></p>
<p>for a simple dataset on bike rentals:</p>
<p><a href=""https://www.kaggle.com/datasets/archit9406/bike-sharing"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/archit9406/bike-sharing</a></p>
<p>Objective is to predict bike rentals or in this case &quot;cnt&quot;</p>
<p>I've created an ml cluster in Databricks and followed the code to as far as cmd 6 where it seems to be referencing other &quot;treeDepth&quot; data.</p>
<p>When I run this I get an AttributeError: 'GBTRegressor' object has no attribute ' setMaxDept'
I've tried to re-enter the variables in my dataframe to be more in line with the code but I cant seem to get it to work.</p>
<p>My overall objective is to get the script running and predicting &quot;cnt&quot;</p>
<p>Can anyone help with this?</p>
<pre><code>    Cmd1
    #install
    %pip install mlflow
</code></pre>
<pre><code>    Cmd 2
    #model name
    modelName = &quot;BikesModelMVP2&quot;
</code></pre>
<pre><code>    Cmd 3
    #importing a variety of different packages
    import mlflow
    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.ml import PipelineModel
    from pyspark.ml import Pipeline
    from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
    from pyspark.ml.feature import VectorAssembler, VectorIndexer, MinMaxScaler
    import mlflow.spark
    #import mlflow.xgboost
    import pyspark.sql.functions as F
    from mlflow.tracking.client import MlflowClient
    from mlflow.entities.model_registry.model_version_status import ModelVersionStatus
</code></pre>
<pre><code>    Cmd 4
    print(mlflow.__version__)
</code></pre>
<pre><code>    Cmd 5
    #reading in some data from dataset that's just abailable inside Databricks
    dataLocation = &quot;/databricks-datasets/bikeSharing/data-001/hour.csv&quot;
    
    df = (spark
         .read
         .option(&quot;header&quot;,&quot;true&quot;)
         .option(&quot;inferSchema&quot;,&quot;true&quot;)
         .csv(dataLocation)
         )
    
    df = df.drop(&quot;instant&quot;,&quot;dteday&quot;,&quot;casual&quot;,&quot;holiday&quot;,&quot;weekday&quot;,&quot;registered&quot;)
    
    df = df.withColumn(&quot;AMPM&quot;, F.when(F.col(&quot;hr&quot;)&lt;12,1).otherwise(0))
</code></pre>
<pre><code>    Cmd 6
    #Function that's going to create a machine learning model for me
    def runModel(_depth,_iter,_ntrees,_modelname):
    #def runModel(season,yr,mnth,hr,workingday,weathersit,temp,atemp,hum,windspeed):
        #with mlflow.start_run(experiment_id=&quot;2526553914207328&quot;):
        with mlflow.start_run():
            seed = 42
            trainDF, testDF = df.randomSplit([0.7,0.3], seed=42)
            
            #clf = RandomForestRegressor()
            clf = GBTRegressor()
            
            clf.setLabelCol(&quot;cnt&quot;)
            
            treeDepth = _depth
            numTrees = _ntrees
            iterations = _iter
            clf.setMaxDept(treeDepth)
            clf.setMaxIter(iterations)
            
            #clf.setNumTrees(numTrees)
            mlflow.log_param(&quot;treeDepth&quot;, treeDepth)
            mlflow.log_param(&quot;numTrees&quot;, numTrees)
            mlflow.log_param(&quot;numIterations&quot;, iterations)
            
            featureCols = df.drop(&quot;cnt&quot;).columns # Removes &quot;cnt&quot;
            vectorAssembler = VectorAssembler(inputCols=featureCols, outputCol=&quot;rawFeatures&quot;)
            vectorIndexer = VectorIndexer(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;, maxCategories=4)
            
            pipeline = Pipeline().setStages([vectorAssembler, vectorIndexer, clf])
            pipelineModel = pipeline.fit(trainDF)
            
            predictionsDF = pipelineModel.transform(testDF)
            
            evaluator = RegressionEvaluator().setMetricName(&quot;rmse&quot;).setPredictionCol(&quot;prediction&quot;).setLabelCol(&quot;cnt&quot;)
            evaluatorR2 = RegressionEvaluator().setMetricName(&quot;r2&quot;).setPredictionCol(&quot;prediction&quot;).setLabelCol(&quot;cnt&quot;)
            
            mlflow.log_metric(&quot;rmse&quot;, evaluator.evaluate(predictionsDF))
            mlflow.log_metric(&quot;r2&quot;, evaluatorR2.evaluate(predictionsDF))
            
            mlflow.log_artifact(&quot;/dbfs&quot;+ dataLocation)
            
            mlflow.spark.log_model(spark_model=pipelineModel, artifact_path=&quot;model&quot; , registered_model_name=_modelname)
            
            run_id = mlflow.active_run().info.run_id
            
            return run_id
    
</code></pre>
",20,0,0,3,regression;databricks;mlflow,2022-07-01 23:17:29,2022-07-01 23:17:29,2022-07-01 23:17:29,i m new to databricks and following a tutorial on mlflow in databricks   for a simple dataset on bike rentals   objective is to predict bike rentals or in this case  cnt  i ve created an ml cluster in databricks and followed the code to as far as cmd  where it seems to be referencing other  treedepth  data  my overall objective is to get the script running and predicting  cnt  can anyone help with this ,mlflow experiment in databricks regressors,databricks following tutorial mlflow databricks simple dataset bike rentals objective predict bike rentals case cnt created ml cluster databricks followed code far cmd seems referencing treedepth data overall objective get script running predicting cnt anyone help,mlflow experiment databricks regressors,mlflow experiment databricks regressorsdatabricks following tutorial mlflow databricks simple dataset bike rentals objective predict bike rentals case cnt created ml cluster databricks followed code far cmd seems referencing treedepth data overall objective get script running predicting cnt anyone help,"['mlflow', 'experiment', 'databricks', 'regressorsdatabricks', 'following', 'tutorial', 'mlflow', 'databricks', 'simple', 'dataset', 'bike', 'rentals', 'objective', 'predict', 'bike', 'rentals', 'case', 'cnt', 'created', 'ml', 'cluster', 'databricks', 'followed', 'code', 'far', 'cmd', 'seems', 'referencing', 'treedepth', 'data', 'overall', 'objective', 'get', 'script', 'running', 'predicting', 'cnt', 'anyone', 'help']","['mlflow', 'experi', 'databrick', 'regressorsdatabrick', 'follow', 'tutori', 'mlflow', 'databrick', 'simpl', 'dataset', 'bike', 'rental', 'object', 'predict', 'bike', 'rental', 'case', 'cnt', 'creat', 'ml', 'cluster', 'databrick', 'follow', 'code', 'far', 'cmd', 'seem', 'referenc', 'treedepth', 'data', 'overal', 'object', 'get', 'script', 'run', 'predict', 'cnt', 'anyon', 'help']"
167,180,180,19461687,72832049,Is it possible to use geodata (latitude &amp; longitude) as a predictor variable?,"<p>I'm a newbie in programming and using machine learning. This is my first post here as I've just recently stumbled upon the first unresolved -of probably many- question.</p>
<p>So I have an extensive database with data on the real-state market of my country and I want to predict the price of the houses -a pretty standard theme ikr- using the latitude and longitude as one variable.</p>
<p>So far I have found Waddell &amp; Besharati-Zadeh's study: <a href=""https://arxiv.org/pdf/2011.14924.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2011.14924.pdf</a> in which they reconstruct the geodata by combining it with other libraries and obtaining string variables as to if certain activities are within a walking distance of 500 meters. So this is a cool alternative but I'm worried there's no accurate data of the walking distance and establishments to do certain activities in my country, not even on google maps. Is there any way in which the combination of the latitude and longitude alone can be used as a predictor variable?</p>
",16,0,0,4,scope;geospatial;latitude-longitude;geo,2022-07-01 21:43:14,2022-07-01 21:43:14,2022-07-01 21:43:14,i m a newbie in programming and using machine learning  this is my first post here as i ve just recently stumbled upon the first unresolved  of probably many  question  so i have an extensive database with data on the real state market of my country and i want to predict the price of the houses  a pretty standard theme ikr  using the latitude and longitude as one variable  so far i have found waddell  amp  besharati zadeh s study   in which they reconstruct the geodata by combining it with other libraries and obtaining string variables as to if certain activities are within a walking distance of  meters  so this is a cool alternative but i m worried there s no accurate data of the walking distance and establishments to do certain activities in my country  not even on google maps  is there any way in which the combination of the latitude and longitude alone can be used as a predictor variable ,is it possible to use geodata  latitude  amp  longitude  as a predictor variable ,newbie programming using machine learning first post recently stumbled upon first unresolved probably many question extensive database data real state market country want predict price houses pretty standard theme ikr using latitude longitude one variable far found waddell amp besharati zadeh study reconstruct geodata combining libraries obtaining string variables certain activities within walking distance meters cool alternative worried accurate data walking distance establishments certain activities country even google maps way combination latitude longitude alone used predictor variable,possible use geodata latitude amp longitude predictor variable,possible use geodata latitude amp longitude predictor variablenewbie programming using machine learning first post recently stumbled upon first unresolved probably many question extensive database data real state market country want predict price houses pretty standard theme ikr using latitude longitude one variable far found waddell amp besharati zadeh study reconstruct geodata combining libraries obtaining string variables certain activities within walking distance meters cool alternative worried accurate data walking distance establishments certain activities country even google maps way combination latitude longitude alone used predictor variable,"['possible', 'use', 'geodata', 'latitude', 'amp', 'longitude', 'predictor', 'variablenewbie', 'programming', 'using', 'machine', 'learning', 'first', 'post', 'recently', 'stumbled', 'upon', 'first', 'unresolved', 'probably', 'many', 'question', 'extensive', 'database', 'data', 'real', 'state', 'market', 'country', 'want', 'predict', 'price', 'houses', 'pretty', 'standard', 'theme', 'ikr', 'using', 'latitude', 'longitude', 'one', 'variable', 'far', 'found', 'waddell', 'amp', 'besharati', 'zadeh', 'study', 'reconstruct', 'geodata', 'combining', 'libraries', 'obtaining', 'string', 'variables', 'certain', 'activities', 'within', 'walking', 'distance', 'meters', 'cool', 'alternative', 'worried', 'accurate', 'data', 'walking', 'distance', 'establishments', 'certain', 'activities', 'country', 'even', 'google', 'maps', 'way', 'combination', 'latitude', 'longitude', 'alone', 'used', 'predictor', 'variable']","['possibl', 'use', 'geodata', 'latitud', 'amp', 'longitud', 'predictor', 'variablenewbi', 'program', 'use', 'machin', 'learn', 'first', 'post', 'recent', 'stumbl', 'upon', 'first', 'unresolv', 'probabl', 'mani', 'question', 'extens', 'databas', 'data', 'real', 'state', 'market', 'countri', 'want', 'predict', 'price', 'hous', 'pretti', 'standard', 'theme', 'ikr', 'use', 'latitud', 'longitud', 'one', 'variabl', 'far', 'found', 'waddel', 'amp', 'besharati', 'zadeh', 'studi', 'reconstruct', 'geodata', 'combin', 'librari', 'obtain', 'string', 'variabl', 'certain', 'activ', 'within', 'walk', 'distanc', 'meter', 'cool', 'altern', 'worri', 'accur', 'data', 'walk', 'distanc', 'establish', 'certain', 'activ', 'countri', 'even', 'googl', 'map', 'way', 'combin', 'latitud', 'longitud', 'alon', 'use', 'predictor', 'variabl']"
168,181,181,18739801,72831813,Uncertainty in target feature in machine learning,"<p>Okay, let's suppose i have a data set with some features, let's say X, and a target one Y with an error('cause this is an experimental measure, and it have an uncertainty), how should I handle this when training some Machine Learning regression algorithm? Both X and Y are continuous.</p>
<p>Does the RMSE say something about the error in the predicted value from the model i trained without the error information? Or should i use another metric to calculate the error? I don't remember the statistics behind these things, so, you guys can recommend me anything to read.</p>
<p>The first thing i was doing was to ignore the error of Y and try to fit the &quot;crude&quot; X and Y, get the model and given some metric(Like RMSE that i was thinking that could work), acquire the error of my predictions, but i don't think it's right.</p>
<p>So, how should i treat my data set with error to create an more accurate model? And how can i take the error information of my predictions?</p>
",32,0,-2,5,python;machine-learning;statistics;data-science;uncertainty,2022-07-01 21:23:42,2022-07-01 21:23:42,2022-07-01 21:25:11,okay  let s suppose i have a data set with some features  let s say x  and a target one y with an error  cause this is an experimental measure  and it have an uncertainty   how should i handle this when training some machine learning regression algorithm  both x and y are continuous  does the rmse say something about the error in the predicted value from the model i trained without the error information  or should i use another metric to calculate the error  i don t remember the statistics behind these things  so  you guys can recommend me anything to read  the first thing i was doing was to ignore the error of y and try to fit the  crude  x and y  get the model and given some metric like rmse that i was thinking that could work   acquire the error of my predictions  but i don t think it s right  so  how should i treat my data set with error to create an more accurate model  and how can i take the error information of my predictions ,uncertainty in target feature in machine learning,okay let suppose data set features let say x target one error cause experimental measure uncertainty handle training machine learning regression algorithm x continuous rmse say something error predicted value model trained without error information use another metric calculate error remember statistics behind things guys recommend anything read first thing ignore error try fit crude x get model given metric like rmse thinking could work acquire error predictions think right treat data set error create accurate model take error information predictions,uncertainty target feature machine learning,uncertainty target feature machine learningokay let suppose data set features let say x target one error cause experimental measure uncertainty handle training machine learning regression algorithm x continuous rmse say something error predicted value model trained without error information use another metric calculate error remember statistics behind things guys recommend anything read first thing ignore error try fit crude x get model given metric like rmse thinking could work acquire error predictions think right treat data set error create accurate model take error information predictions,"['uncertainty', 'target', 'feature', 'machine', 'learningokay', 'let', 'suppose', 'data', 'set', 'features', 'let', 'say', 'x', 'target', 'one', 'error', 'cause', 'experimental', 'measure', 'uncertainty', 'handle', 'training', 'machine', 'learning', 'regression', 'algorithm', 'x', 'continuous', 'rmse', 'say', 'something', 'error', 'predicted', 'value', 'model', 'trained', 'without', 'error', 'information', 'use', 'another', 'metric', 'calculate', 'error', 'remember', 'statistics', 'behind', 'things', 'guys', 'recommend', 'anything', 'read', 'first', 'thing', 'ignore', 'error', 'try', 'fit', 'crude', 'x', 'get', 'model', 'given', 'metric', 'like', 'rmse', 'thinking', 'could', 'work', 'acquire', 'error', 'predictions', 'think', 'right', 'treat', 'data', 'set', 'error', 'create', 'accurate', 'model', 'take', 'error', 'information', 'predictions']","['uncertainti', 'target', 'featur', 'machin', 'learningokay', 'let', 'suppos', 'data', 'set', 'featur', 'let', 'say', 'x', 'target', 'one', 'error', 'caus', 'experiment', 'measur', 'uncertainti', 'handl', 'train', 'machin', 'learn', 'regress', 'algorithm', 'x', 'continu', 'rmse', 'say', 'someth', 'error', 'predict', 'valu', 'model', 'train', 'without', 'error', 'inform', 'use', 'anoth', 'metric', 'calcul', 'error', 'rememb', 'statist', 'behind', 'thing', 'guy', 'recommend', 'anyth', 'read', 'first', 'thing', 'ignor', 'error', 'tri', 'fit', 'crude', 'x', 'get', 'model', 'given', 'metric', 'like', 'rmse', 'think', 'could', 'work', 'acquir', 'error', 'predict', 'think', 'right', 'treat', 'data', 'set', 'error', 'creat', 'accur', 'model', 'take', 'error', 'inform', 'predict']"
169,182,182,15195156,72830619,Machine Learning for RNA Sequence Recognition,"<p>I am looking for a machine learning training model for Natural Language Processing to go through scientific papers and look for Protein Sequences, amino acids and genes. I already have a script that is able to send a 'get' request for the papers and then pre-process the text removing stop words, stemming, and lemmatization. The next step for me would be introducing the NLP and machine learning. If anyone has any good sources or ideas on where to start it would be appreciated.</p>
",21,0,-1,2,machine-learning;nlp,2022-07-01 19:45:34,2022-07-01 19:45:34,2022-07-01 19:45:34,i am looking for a machine learning training model for natural language processing to go through scientific papers and look for protein sequences  amino acids and genes  i already have a script that is able to send a  get  request for the papers and then pre process the text removing stop words  stemming  and lemmatization  the next step for me would be introducing the nlp and machine learning  if anyone has any good sources or ideas on where to start it would be appreciated ,machine learning for rna sequence recognition,looking machine learning training model natural language processing go scientific papers look protein sequences amino acids genes already script able send get request papers pre process text removing stop stemming lemmatization next step would introducing nlp machine learning anyone good sources ideas start would appreciated,machine learning rna sequence recognition,machine learning rna sequence recognitionlooking machine learning training model natural language processing go scientific papers look protein sequences amino acids genes already script able send get request papers pre process text removing stop stemming lemmatization next step would introducing nlp machine learning anyone good sources ideas start would appreciated,"['machine', 'learning', 'rna', 'sequence', 'recognitionlooking', 'machine', 'learning', 'training', 'model', 'natural', 'language', 'processing', 'go', 'scientific', 'papers', 'look', 'protein', 'sequences', 'amino', 'acids', 'genes', 'already', 'script', 'able', 'send', 'get', 'request', 'papers', 'pre', 'process', 'text', 'removing', 'stop', 'stemming', 'lemmatization', 'next', 'step', 'would', 'introducing', 'nlp', 'machine', 'learning', 'anyone', 'good', 'sources', 'ideas', 'start', 'would', 'appreciated']","['machin', 'learn', 'rna', 'sequenc', 'recognitionlook', 'machin', 'learn', 'train', 'model', 'natur', 'languag', 'process', 'go', 'scientif', 'paper', 'look', 'protein', 'sequenc', 'amino', 'acid', 'gene', 'alreadi', 'script', 'abl', 'send', 'get', 'request', 'paper', 'pre', 'process', 'text', 'remov', 'stop', 'stem', 'lemmat', 'next', 'step', 'would', 'introduc', 'nlp', 'machin', 'learn', 'anyon', 'good', 'sourc', 'idea', 'start', 'would', 'appreci']"
170,183,183,19442237,72828671,is python PIP important for machine learning or python programming,"<p>I need to know that does python3 has already installed PIP or it needs to be installed and is it important for machine learning ?</p>
",18,1,-1,1,python-3.x,2022-07-01 17:06:54,2022-07-01 17:06:54,2022-07-01 17:19:12,i need to know that does python has already installed pip or it needs to be installed and is it important for machine learning  ,is python pip important for machine learning or python programming,need know python already installed pip needs installed important machine learning,python pip important machine learning python programming,python pip important machine learning python programmingneed know python already installed pip needs installed important machine learning,"['python', 'pip', 'important', 'machine', 'learning', 'python', 'programmingneed', 'know', 'python', 'already', 'installed', 'pip', 'needs', 'installed', 'important', 'machine', 'learning']","['python', 'pip', 'import', 'machin', 'learn', 'python', 'programmingne', 'know', 'python', 'alreadi', 'instal', 'pip', 'need', 'instal', 'import', 'machin', 'learn']"
171,184,184,19338205,72826443,Assessing molecules geometry similarity with machine learning,"<p>I participate in a project where I am required to compare a set of molecules to a reference molecule, to assess which molecule is the most similar, from geometrical perspective only (neglecting chemical features).</p>
<p>The nature of the problem is unsupervised, because there is no label for each molecule.</p>
<p>My data consist of 1800~ molecules and a reference molecule, each has a table of XYZ coordinates.</p>
<p>I began with feature extraction - computing distances and angels between each point, and calculating the mean, median, variance and skewness as features for each molecule.</p>
<p>Another thought that came to my mind was to use 2d PCA, and use the variance of each PC as feature (Assuming similar mol will result in similar PCA).</p>
<p>I'm trying to think which algo could fit. The first that came to my mind were clustering algos, but I wonder if there are others that could fit?</p>
<p>Thanks you!</p>
",27,0,-2,5,machine-learning;geometry;unsupervised-learning;chemistry;molecule,2022-07-01 13:56:52,2022-07-01 13:56:52,2022-07-01 13:56:52,i participate in a project where i am required to compare a set of molecules to a reference molecule  to assess which molecule is the most similar  from geometrical perspective only  neglecting chemical features   the nature of the problem is unsupervised  because there is no label for each molecule  my data consist of   molecules and a reference molecule  each has a table of xyz coordinates  i began with feature extraction   computing distances and angels between each point  and calculating the mean  median  variance and skewness as features for each molecule  another thought that came to my mind was to use d pca  and use the variance of each pc as feature  assuming similar mol will result in similar pca   i m trying to think which algo could fit  the first that came to my mind were clustering algos  but i wonder if there are others that could fit  thanks you ,assessing molecules geometry similarity with machine learning,participate project required compare set molecules reference molecule assess molecule similar geometrical perspective neglecting chemical features nature problem unsupervised label molecule data consist molecules reference molecule table xyz coordinates began feature extraction computing distances angels point calculating mean median variance skewness features molecule another thought came mind use pca use variance pc feature assuming similar mol result similar pca trying think algo could fit first came mind clustering algos wonder others could fit thanks,assessing molecules geometry similarity machine learning,assessing molecules geometry similarity machine learningparticipate project required compare set molecules reference molecule assess molecule similar geometrical perspective neglecting chemical features nature problem unsupervised label molecule data consist molecules reference molecule table xyz coordinates began feature extraction computing distances angels point calculating mean median variance skewness features molecule another thought came mind use pca use variance pc feature assuming similar mol result similar pca trying think algo could fit first came mind clustering algos wonder others could fit thanks,"['assessing', 'molecules', 'geometry', 'similarity', 'machine', 'learningparticipate', 'project', 'required', 'compare', 'set', 'molecules', 'reference', 'molecule', 'assess', 'molecule', 'similar', 'geometrical', 'perspective', 'neglecting', 'chemical', 'features', 'nature', 'problem', 'unsupervised', 'label', 'molecule', 'data', 'consist', 'molecules', 'reference', 'molecule', 'table', 'xyz', 'coordinates', 'began', 'feature', 'extraction', 'computing', 'distances', 'angels', 'point', 'calculating', 'mean', 'median', 'variance', 'skewness', 'features', 'molecule', 'another', 'thought', 'came', 'mind', 'use', 'pca', 'use', 'variance', 'pc', 'feature', 'assuming', 'similar', 'mol', 'result', 'similar', 'pca', 'trying', 'think', 'algo', 'could', 'fit', 'first', 'came', 'mind', 'clustering', 'algos', 'wonder', 'others', 'could', 'fit', 'thanks']","['assess', 'molecul', 'geometri', 'similar', 'machin', 'learningparticip', 'project', 'requir', 'compar', 'set', 'molecul', 'refer', 'molecul', 'assess', 'molecul', 'similar', 'geometr', 'perspect', 'neglect', 'chemic', 'featur', 'natur', 'problem', 'unsupervis', 'label', 'molecul', 'data', 'consist', 'molecul', 'refer', 'molecul', 'tabl', 'xyz', 'coordin', 'began', 'featur', 'extract', 'comput', 'distanc', 'angel', 'point', 'calcul', 'mean', 'median', 'varianc', 'skew', 'featur', 'molecul', 'anoth', 'thought', 'came', 'mind', 'use', 'pca', 'use', 'varianc', 'pc', 'featur', 'assum', 'similar', 'mol', 'result', 'similar', 'pca', 'tri', 'think', 'algo', 'could', 'fit', 'first', 'came', 'mind', 'cluster', 'algo', 'wonder', 'other', 'could', 'fit', 'thank']"
172,185,185,3025242,72826319,A way to perform voting and select a candidate based on nearest neighbours,"<p>I'm working on a project where I use <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">FAISS</a> to retrieve <em>n</em> neighbouring vectors based on a query vector. The data in question is textual and is being embedded by using a machine learning model to create a vector before it goes into FAISS.</p>
<p>These neighbors each have a category assigned to them, and also have a similarity score to the query, like the following:</p>
<pre><code>Query: Berlin is the capital of Germany
=====
Neighbours output:

5 Neighbour ids: [57, 163, 177, 124, 91]

Text | Category | Similarity

Berlin is a great city to live in | Capital cities | 0.897843
Capital letters are often used to indicate nouns in German | Grammar | 0.803834
Over 3 million people live in Berlin | Capital cities | 0.79434
Germany is a country in Central Europe | Countries | 0.763232
Germany has many big cities | Countries | 0.7304545
</code></pre>
<p>Now, the thing I want to achieve is getting a single category for the query based on the categories of the neighbours, a kind of vector based recommender/suggestion system. What I tried already is just doing simple and weighted (based on similarity) majority voting.</p>
<p>Using simple majority voting, in the above example I would just get &quot;Capital cities&quot; or &quot;Country&quot; category as they both occur 2 out of 5 times. Using weighted voting I would arrive at &quot;Capital cities&quot; as they have higher similarity overall.</p>
<p>Both the approaches seem to work, however I am looking for a slightly more sophisticated approach to combine different signals. I read about the concept of <a href=""https://www.hindawi.com/journals/tswj/2013/704504/"" rel=""nofollow noreferrer"">data fusion</a> in machine learning, but I don't quite know yet how to best apply it here to arrive at one category based on the neighbours.</p>
<p>Any ideas are appreciated!</p>
",12,0,0,5,classification;similarity;recommendation-engine;information-retrieval;nearest-neighbor,2022-07-01 13:46:17,2022-07-01 13:46:17,2022-07-01 13:46:17,i m working on a project where i use  to retrieve n neighbouring vectors based on a query vector  the data in question is textual and is being embedded by using a machine learning model to create a vector before it goes into faiss  these neighbors each have a category assigned to them  and also have a similarity score to the query  like the following  now  the thing i want to achieve is getting a single category for the query based on the categories of the neighbours  a kind of vector based recommender suggestion system  what i tried already is just doing simple and weighted  based on similarity  majority voting  using simple majority voting  in the above example i would just get  capital cities  or  country  category as they both occur  out of  times  using weighted voting i would arrive at  capital cities  as they have higher similarity overall  both the approaches seem to work  however i am looking for a slightly more sophisticated approach to combine different signals  i read about the concept of  in machine learning  but i don t quite know yet how to best apply it here to arrive at one category based on the neighbours  any ideas are appreciated ,a way to perform voting and select a candidate based on nearest neighbours,working project use retrieve n neighbouring vectors based query vector data question textual embedded using machine learning model create vector goes faiss neighbors category assigned also similarity score query like following thing want achieve getting single category query based categories neighbours kind vector based recommender suggestion system tried already simple weighted based similarity majority voting using simple majority voting example would get capital cities country category occur times using weighted voting would arrive capital cities higher similarity overall approaches seem work however looking slightly sophisticated approach combine different signals read concept machine learning quite know yet best apply arrive one category based neighbours ideas appreciated,way perform voting select candidate based nearest neighbours,way perform voting select candidate based nearest neighboursworking project use retrieve n neighbouring vectors based query vector data question textual embedded using machine learning model create vector goes faiss neighbors category assigned also similarity score query like following thing want achieve getting single category query based categories neighbours kind vector based recommender suggestion system tried already simple weighted based similarity majority voting using simple majority voting example would get capital cities country category occur times using weighted voting would arrive capital cities higher similarity overall approaches seem work however looking slightly sophisticated approach combine different signals read concept machine learning quite know yet best apply arrive one category based neighbours ideas appreciated,"['way', 'perform', 'voting', 'select', 'candidate', 'based', 'nearest', 'neighboursworking', 'project', 'use', 'retrieve', 'n', 'neighbouring', 'vectors', 'based', 'query', 'vector', 'data', 'question', 'textual', 'embedded', 'using', 'machine', 'learning', 'model', 'create', 'vector', 'goes', 'faiss', 'neighbors', 'category', 'assigned', 'also', 'similarity', 'score', 'query', 'like', 'following', 'thing', 'want', 'achieve', 'getting', 'single', 'category', 'query', 'based', 'categories', 'neighbours', 'kind', 'vector', 'based', 'recommender', 'suggestion', 'system', 'tried', 'already', 'simple', 'weighted', 'based', 'similarity', 'majority', 'voting', 'using', 'simple', 'majority', 'voting', 'example', 'would', 'get', 'capital', 'cities', 'country', 'category', 'occur', 'times', 'using', 'weighted', 'voting', 'would', 'arrive', 'capital', 'cities', 'higher', 'similarity', 'overall', 'approaches', 'seem', 'work', 'however', 'looking', 'slightly', 'sophisticated', 'approach', 'combine', 'different', 'signals', 'read', 'concept', 'machine', 'learning', 'quite', 'know', 'yet', 'best', 'apply', 'arrive', 'one', 'category', 'based', 'neighbours', 'ideas', 'appreciated']","['way', 'perform', 'vote', 'select', 'candid', 'base', 'nearest', 'neighbourswork', 'project', 'use', 'retriev', 'n', 'neighbour', 'vector', 'base', 'queri', 'vector', 'data', 'question', 'textual', 'embed', 'use', 'machin', 'learn', 'model', 'creat', 'vector', 'goe', 'faiss', 'neighbor', 'categori', 'assign', 'also', 'similar', 'score', 'queri', 'like', 'follow', 'thing', 'want', 'achiev', 'get', 'singl', 'categori', 'queri', 'base', 'categori', 'neighbour', 'kind', 'vector', 'base', 'recommend', 'suggest', 'system', 'tri', 'alreadi', 'simpl', 'weight', 'base', 'similar', 'major', 'vote', 'use', 'simpl', 'major', 'vote', 'exampl', 'would', 'get', 'capit', 'citi', 'countri', 'categori', 'occur', 'time', 'use', 'weight', 'vote', 'would', 'arriv', 'capit', 'citi', 'higher', 'similar', 'overal', 'approach', 'seem', 'work', 'howev', 'look', 'slightli', 'sophist', 'approach', 'combin', 'differ', 'signal', 'read', 'concept', 'machin', 'learn', 'quit', 'know', 'yet', 'best', 'appli', 'arriv', 'one', 'categori', 'base', 'neighbour', 'idea', 'appreci']"
173,186,186,13788653,72824476,How to efficiently run machine learning model over single images?,"<p>I am currently trying to find a way to determine a chess position based on an overhead picture of a chessboard. My model is a retrained resnet50. Here is my code:</p>
<pre><code>classes = ['b','k','n','p','q','r',None,'B','K','N','P','Q','R']

squares = image_processing('/Users/Me/Downloads/s-zoom.file.jpeg')
image_transforms = transforms.Compose([  
                    transforms.Resize((227,227)),
                    transforms.ToTensor()

])

board = []
for square in squares:
    square = Image.fromarray(square)
    square = image_transforms(square).float()
    square = square.unsqueeze(0)

    output = model(square)
    _, predicted = torch.max(output.data, 1)
    board.append(classes[predicted.item()])
</code></pre>
<p>The problem is that this takes a long time to process - over 20 seconds, when ideally it should take under 5. What is a more efficient way of doing this?</p>
",24,0,1,4,performance;machine-learning;computer-vision;conv-neural-network,2022-07-01 10:17:55,2022-07-01 10:17:55,2022-07-01 12:58:27,i am currently trying to find a way to determine a chess position based on an overhead picture of a chessboard  my model is a retrained resnet  here is my code  the problem is that this takes a long time to process   over  seconds  when ideally it should take under   what is a more efficient way of doing this ,how to efficiently run machine learning model over single images ,currently trying find way determine chess position based overhead picture chessboard model retrained resnet code problem takes long time process seconds ideally take efficient way,efficiently run machine learning model single images,efficiently run machine learning model single imagescurrently trying find way determine chess position based overhead picture chessboard model retrained resnet code problem takes long time process seconds ideally take efficient way,"['efficiently', 'run', 'machine', 'learning', 'model', 'single', 'imagescurrently', 'trying', 'find', 'way', 'determine', 'chess', 'position', 'based', 'overhead', 'picture', 'chessboard', 'model', 'retrained', 'resnet', 'code', 'problem', 'takes', 'long', 'time', 'process', 'seconds', 'ideally', 'take', 'efficient', 'way']","['effici', 'run', 'machin', 'learn', 'model', 'singl', 'imagescurr', 'tri', 'find', 'way', 'determin', 'chess', 'posit', 'base', 'overhead', 'pictur', 'chessboard', 'model', 'retrain', 'resnet', 'code', 'problem', 'take', 'long', 'time', 'process', 'second', 'ideal', 'take', 'effici', 'way']"
174,187,187,13002570,72824489,tf.distribute.MirroredStrategy - suggestion for improving test mean_iou for segmentation network using distributed training,"<p>I am using tensorflow 2.5.0 and implemented semantic segmatation network. used DeepLab_v3_plus network with ResNet101 backbone, adam optimizer and Categorical cross entropy loss to train network. I have first build code for single gpu and achieved test accuracy (mean_iou) of 54% trained for 96 epochs. Then added tf MirroredStrategy (one machine) in code to support for multi gpu training. Surprisingly with 2 gpus, training for 48 epochs, test mean_iou is just 27% and training with 4 gpus, for 24 epochs, test mean_iou can around 12% for same dataset.</p>
<ul>
<li>Code I have modified to support multi-gpu training from single-gpu training.</li>
</ul>
<ol>
<li>By following tensorflow blog for distributed training, created mirrored strategy and created model, model compilation and dataset_generator inside strategy scope. As per my understanding, by doing so, model.fit() method will take care of synchronization of gradients and distributing data on each gpus for training. Though code was running without any error, and also training time reduced compared to single gpu for same number of image training, test mean_iou keep getting worst with more number of gpus.</li>
<li>Replaced BatchNormalization with SyncBatchNormalization, but no improvement.</li>
<li>used warmup learning rate with linear scaling of learning rate with number of gpus, but no improvement.</li>
<li>in cross entropy loss, used both losses_utils.ReductionV2.AUTO and losses_utils.ReductionV2.NONE.</li>
</ol>
<pre><code>loss = ce(y_true, y_pred)
# reshape loss for each sample (BxHxWxC -&gt; BxN)
# Normalize loss by number of non zero elements and sum for each sample and mean across all samples.

</code></pre>
<p>using .AUTO/.NONE options, I am not scaling loss by global_batch_size understanding tf will take care of it and I am already normalizing for each gpus. but with both options, didn't get any luck.</p>
<ol start=""5"">
<li>changed data_generator to tf.data.Dataset obj. Though it has helped in training time, but test mean_iou become even worst.</li>
</ol>
<p>I would appreciate if any lead or suggestion for improving test_iou in distributed training.
let me know if you need any additional details.</p>
<p>Thank you</p>
",23,0,0,5,tensorflow;computer-vision;tf.keras;semantic-segmentation;distributed-training,2022-07-01 10:19:28,2022-07-01 10:19:28,2022-07-01 10:59:43,i am using tensorflow    and implemented semantic segmatation network  used deeplab_v_plus network with resnet backbone  adam optimizer and categorical cross entropy loss to train network  i have first build code for single gpu and achieved test accuracy  mean_iou  of   trained for  epochs  then added tf mirroredstrategy  one machine  in code to support for multi gpu training  surprisingly with  gpus  training for  epochs  test mean_iou is just   and training with  gpus  for  epochs  test mean_iou can around   for same dataset  using  auto  none options  i am not scaling loss by global_batch_size understanding tf will take care of it and i am already normalizing for each gpus  but with both options  didn t get any luck  thank you,tf distribute mirroredstrategy   suggestion for improving test mean_iou for segmentation network using distributed training,using tensorflow implemented semantic segmatation network used deeplab_v_plus network resnet backbone adam optimizer categorical cross entropy loss train network first build code single gpu achieved test accuracy mean_iou trained epochs added tf mirroredstrategy one machine code support multi gpu training surprisingly gpus training epochs test mean_iou training gpus epochs test mean_iou around dataset using auto none options scaling loss global_batch_size understanding tf take care already normalizing gpus options get luck thank,tf distribute mirroredstrategy suggestion improving test mean_iou segmentation network using distributed training,tf distribute mirroredstrategy suggestion improving test mean_iou segmentation network using distributed trainingusing tensorflow implemented semantic segmatation network used deeplab_v_plus network resnet backbone adam optimizer categorical cross entropy loss train network first build code single gpu achieved test accuracy mean_iou trained epochs added tf mirroredstrategy one machine code support multi gpu training surprisingly gpus training epochs test mean_iou training gpus epochs test mean_iou around dataset using auto none options scaling loss global_batch_size understanding tf take care already normalizing gpus options get luck thank,"['tf', 'distribute', 'mirroredstrategy', 'suggestion', 'improving', 'test', 'mean_iou', 'segmentation', 'network', 'using', 'distributed', 'trainingusing', 'tensorflow', 'implemented', 'semantic', 'segmatation', 'network', 'used', 'deeplab_v_plus', 'network', 'resnet', 'backbone', 'adam', 'optimizer', 'categorical', 'cross', 'entropy', 'loss', 'train', 'network', 'first', 'build', 'code', 'single', 'gpu', 'achieved', 'test', 'accuracy', 'mean_iou', 'trained', 'epochs', 'added', 'tf', 'mirroredstrategy', 'one', 'machine', 'code', 'support', 'multi', 'gpu', 'training', 'surprisingly', 'gpus', 'training', 'epochs', 'test', 'mean_iou', 'training', 'gpus', 'epochs', 'test', 'mean_iou', 'around', 'dataset', 'using', 'auto', 'none', 'options', 'scaling', 'loss', 'global_batch_size', 'understanding', 'tf', 'take', 'care', 'already', 'normalizing', 'gpus', 'options', 'get', 'luck', 'thank']","['tf', 'distribut', 'mirroredstrategi', 'suggest', 'improv', 'test', 'mean_iou', 'segment', 'network', 'use', 'distribut', 'trainingus', 'tensorflow', 'implement', 'semant', 'segmat', 'network', 'use', 'deeplab_v_plu', 'network', 'resnet', 'backbon', 'adam', 'optim', 'categor', 'cross', 'entropi', 'loss', 'train', 'network', 'first', 'build', 'code', 'singl', 'gpu', 'achiev', 'test', 'accuraci', 'mean_iou', 'train', 'epoch', 'ad', 'tf', 'mirroredstrategi', 'one', 'machin', 'code', 'support', 'multi', 'gpu', 'train', 'surprisingli', 'gpu', 'train', 'epoch', 'test', 'mean_iou', 'train', 'gpu', 'epoch', 'test', 'mean_iou', 'around', 'dataset', 'use', 'auto', 'none', 'option', 'scale', 'loss', 'global_batch_s', 'understand', 'tf', 'take', 'care', 'alreadi', 'normal', 'gpu', 'option', 'get', 'luck', 'thank']"
175,188,188,12882606,72619764,Compress Large Data in R into csv without NULLS or LIST,"<p>FIRST TIME POSTING:</p>
<p>I'm preparing data for <code>arules() read.transactions</code> and need to compress unique Invoice data (500k+ cases) so that each unique Invoice and its associated info fits on a single line like this:</p>
<blockquote>
<p>Invoice001,CustomerID,Country,StockCodeXYZ,StockCode123</p>
<p>Invoice002...etc</p>
</blockquote>
<p>However, the data reads in repeating the Invoice for each <code>StockCode</code> like this:</p>
<blockquote>
<p>Invoice001,CustomerID,Country,StockCodeXYZ</p>
<p>Invoice001,CustomerID,Country,StockCode123</p>
<p>Invoice002....etc</p>
</blockquote>
<p>I've been trying <code>pivot_wider()</code> and then <code>unite()</code>, but it generates 285M+ MOSTLY NULL cells into a LIST which I'm having a hard time resolving and unable to write to csv or read into <code>arules</code>.  I've also tried <code>keep(~!is.null(.)), discard(is.null), compact()</code> without success and am open to any method to achieve the desired outcome above.</p>
<p>However, I feel like I should be able to solve it using the built-in <code>arules() read.transactions() fx</code>, but am getting various errors as I try different things there too.</p>
<p>The data is opensource from University of California, Irvin and found here: <a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx</a></p>
<p>Any help would be greatly appreciated.</p>
<pre><code>library(readxl)
url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx&quot;
destfile &lt;- &quot;Online_20Retail.xlsx&quot;
curl::curl_download(url, destfile)
Online_20Retail &lt;- read_excel(destfile)

trans &lt;- read.transactions(????????????)
</code></pre>
",56,1,0,5,dataframe;dplyr;tidyr;arules;readxl,2022-06-14 21:06:58,2022-06-14 21:06:58,2022-07-01 08:46:06,first time posting  i m preparing data for arules   read transactions and need to compress unique invoice data  k  cases  so that each unique invoice and its associated info fits on a single line like this  invoice customerid country stockcodexyz stockcode invoice   etc however  the data reads in repeating the invoice for each stockcode like this  invoice customerid country stockcodexyz invoice customerid country stockcode invoice    etc i ve been trying pivot_wider   and then unite    but it generates m  mostly null cells into a list which i m having a hard time resolving and unable to write to csv or read into arules   i ve also tried keep   is null      discard is null   compact   without success and am open to any method to achieve the desired outcome above  however  i feel like i should be able to solve it using the built in arules   read transactions   fx  but am getting various errors as i try different things there too  the data is opensource from university of california  irvin and found here   any help would be greatly appreciated ,compress large data in r into csv without nulls or list,first time posting preparing data arules read transactions need compress unique invoice data k cases unique invoice associated info fits single line like invoice customerid country stockcodexyz stockcode invoice etc however data reads repeating invoice stockcode like invoice customerid country stockcodexyz invoice customerid country stockcode invoice etc trying pivot_wider unite generates mostly null cells hard time resolving unable write csv read arules also tried keep null discard null compact without success open method achieve desired outcome however feel like able solve using built arules read transactions fx getting various errors try different things data opensource university california irvin found help would greatly appreciated,compress large data r csv without nulls,compress large data r csv without nullsfirst time posting preparing data arules read transactions need compress unique invoice data k cases unique invoice associated info fits single line like invoice customerid country stockcodexyz stockcode invoice etc however data reads repeating invoice stockcode like invoice customerid country stockcodexyz invoice customerid country stockcode invoice etc trying pivot_wider unite generates mostly null cells hard time resolving unable write csv read arules also tried keep null discard null compact without success open method achieve desired outcome however feel like able solve using built arules read transactions fx getting various errors try different things data opensource university california irvin found help would greatly appreciated,"['compress', 'large', 'data', 'r', 'csv', 'without', 'nullsfirst', 'time', 'posting', 'preparing', 'data', 'arules', 'read', 'transactions', 'need', 'compress', 'unique', 'invoice', 'data', 'k', 'cases', 'unique', 'invoice', 'associated', 'info', 'fits', 'single', 'line', 'like', 'invoice', 'customerid', 'country', 'stockcodexyz', 'stockcode', 'invoice', 'etc', 'however', 'data', 'reads', 'repeating', 'invoice', 'stockcode', 'like', 'invoice', 'customerid', 'country', 'stockcodexyz', 'invoice', 'customerid', 'country', 'stockcode', 'invoice', 'etc', 'trying', 'pivot_wider', 'unite', 'generates', 'mostly', 'null', 'cells', 'hard', 'time', 'resolving', 'unable', 'write', 'csv', 'read', 'arules', 'also', 'tried', 'keep', 'null', 'discard', 'null', 'compact', 'without', 'success', 'open', 'method', 'achieve', 'desired', 'outcome', 'however', 'feel', 'like', 'able', 'solve', 'using', 'built', 'arules', 'read', 'transactions', 'fx', 'getting', 'various', 'errors', 'try', 'different', 'things', 'data', 'opensource', 'university', 'california', 'irvin', 'found', 'help', 'would', 'greatly', 'appreciated']","['compress', 'larg', 'data', 'r', 'csv', 'without', 'nullsfirst', 'time', 'post', 'prepar', 'data', 'arul', 'read', 'transact', 'need', 'compress', 'uniqu', 'invoic', 'data', 'k', 'case', 'uniqu', 'invoic', 'associ', 'info', 'fit', 'singl', 'line', 'like', 'invoic', 'customerid', 'countri', 'stockcodexyz', 'stockcod', 'invoic', 'etc', 'howev', 'data', 'read', 'repeat', 'invoic', 'stockcod', 'like', 'invoic', 'customerid', 'countri', 'stockcodexyz', 'invoic', 'customerid', 'countri', 'stockcod', 'invoic', 'etc', 'tri', 'pivot_wid', 'unit', 'gener', 'mostli', 'null', 'cell', 'hard', 'time', 'resolv', 'unabl', 'write', 'csv', 'read', 'arul', 'also', 'tri', 'keep', 'null', 'discard', 'null', 'compact', 'without', 'success', 'open', 'method', 'achiev', 'desir', 'outcom', 'howev', 'feel', 'like', 'abl', 'solv', 'use', 'built', 'arul', 'read', 'transact', 'fx', 'get', 'variou', 'error', 'tri', 'differ', 'thing', 'data', 'opensourc', 'univers', 'california', 'irvin', 'found', 'help', 'would', 'greatli', 'appreci']"
176,189,189,19456970,72823991,Not able to preview input file in Query (Azure Stream Analytics),"<p>I am trying to set up a basic query in my Azure Stream Analytics. I have set up a Stream Analytics Job as well as a storage account under a Resource Group.</p>
<p>In my storage account, I created an input and output container. I tried to upload a json file as well as a csv file into my input container. The json file was created from a python dataframe of an public dataset (ai4i2020.csv), and the csv file was the exact ai4i2020.csv.</p>
<p>&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv&quot;</p>
<p>I tagged the input container to my Stream Analytics Job Input, but when I went to Query section, Azure was unable to provide me with an input preview. The error message reads</p>
<p>&quot;Unable to connect to input source at the moment. Please check if the input source is available and if it has not hit connection limits.&quot; May I know what might be to cause of this issue?</p>
",16,0,0,1,azure,2022-07-01 08:36:34,2022-07-01 08:36:34,2022-07-01 08:36:34,i am trying to set up a basic query in my azure stream analytics  i have set up a stream analytics job as well as a storage account under a resource group  in my storage account  i created an input and output container  i tried to upload a json file as well as a csv file into my input container  the json file was created from a python dataframe of an public dataset  aii csv   and the csv file was the exact aii csv   https   archive ics uci edu ml machine learning databases  aii csv  i tagged the input container to my stream analytics job input  but when i went to query section  azure was unable to provide me with an input preview  the error message reads  unable to connect to input source at the moment  please check if the input source is available and if it has not hit connection limits   may i know what might be to cause of this issue ,not able to preview input file in query  azure stream analytics ,trying set basic query azure stream analytics set stream analytics job well storage account resource group storage account created input output container tried upload json file well csv file input container json file created python dataframe public dataset aii csv csv file exact aii csv https archive ics uci edu ml machine learning databases aii csv tagged input container stream analytics job input went query section azure unable provide input preview error message reads unable connect input source moment please check input source available hit connection limits may know might cause issue,able preview input file query azure stream analytics,able preview input file query azure stream analyticstrying set basic query azure stream analytics set stream analytics job well storage account resource group storage account created input output container tried upload json file well csv file input container json file created python dataframe public dataset aii csv csv file exact aii csv https archive ics uci edu ml machine learning databases aii csv tagged input container stream analytics job input went query section azure unable provide input preview error message reads unable connect input source moment please check input source available hit connection limits may know might cause issue,"['able', 'preview', 'input', 'file', 'query', 'azure', 'stream', 'analyticstrying', 'set', 'basic', 'query', 'azure', 'stream', 'analytics', 'set', 'stream', 'analytics', 'job', 'well', 'storage', 'account', 'resource', 'group', 'storage', 'account', 'created', 'input', 'output', 'container', 'tried', 'upload', 'json', 'file', 'well', 'csv', 'file', 'input', 'container', 'json', 'file', 'created', 'python', 'dataframe', 'public', 'dataset', 'aii', 'csv', 'csv', 'file', 'exact', 'aii', 'csv', 'https', 'archive', 'ics', 'uci', 'edu', 'ml', 'machine', 'learning', 'databases', 'aii', 'csv', 'tagged', 'input', 'container', 'stream', 'analytics', 'job', 'input', 'went', 'query', 'section', 'azure', 'unable', 'provide', 'input', 'preview', 'error', 'message', 'reads', 'unable', 'connect', 'input', 'source', 'moment', 'please', 'check', 'input', 'source', 'available', 'hit', 'connection', 'limits', 'may', 'know', 'might', 'cause', 'issue']","['abl', 'preview', 'input', 'file', 'queri', 'azur', 'stream', 'analyticstri', 'set', 'basic', 'queri', 'azur', 'stream', 'analyt', 'set', 'stream', 'analyt', 'job', 'well', 'storag', 'account', 'resourc', 'group', 'storag', 'account', 'creat', 'input', 'output', 'contain', 'tri', 'upload', 'json', 'file', 'well', 'csv', 'file', 'input', 'contain', 'json', 'file', 'creat', 'python', 'datafram', 'public', 'dataset', 'aii', 'csv', 'csv', 'file', 'exact', 'aii', 'csv', 'http', 'archiv', 'ic', 'uci', 'edu', 'ml', 'machin', 'learn', 'databas', 'aii', 'csv', 'tag', 'input', 'contain', 'stream', 'analyt', 'job', 'input', 'went', 'queri', 'section', 'azur', 'unabl', 'provid', 'input', 'preview', 'error', 'messag', 'read', 'unabl', 'connect', 'input', 'sourc', 'moment', 'pleas', 'check', 'input', 'sourc', 'avail', 'hit', 'connect', 'limit', 'may', 'know', 'might', 'caus', 'issu']"
177,190,190,6334082,72823016,Machine Learning training with high resolution to increase low resolution data,"<p>If I had some high resolution data. For example weather IR satellite imagery over the CONUS.</p>
<p>And I had some low resolution satellite imagery for the entire globe.</p>
<p>Could I train a machine learning model over one region and apply it globally? If so what ML models should I use and what resources can you recommend.</p>
<p>high_res_conus + low_res_conus -&gt; model</p>
<p>model(low_res_gobal)-&gt;high_res_global</p>
",12,0,-1,1,machine-learning,2022-07-01 05:02:24,2022-07-01 05:02:24,2022-07-01 05:02:24,if i had some high resolution data  for example weather ir satellite imagery over the conus  and i had some low resolution satellite imagery for the entire globe  could i train a machine learning model over one region and apply it globally  if so what ml models should i use and what resources can you recommend  high_res_conus   low_res_conus   gt  model model low_res_gobal   gt high_res_global,machine learning training with high resolution to increase low resolution data,high resolution data example weather ir satellite imagery conus low resolution satellite imagery entire globe could train machine learning model one region apply globally ml models use resources recommend high_res_conus low_res_conus gt model model low_res_gobal gt high_res_global,machine learning training high resolution increase low resolution data,machine learning training high resolution increase low resolution datahigh resolution data example weather ir satellite imagery conus low resolution satellite imagery entire globe could train machine learning model one region apply globally ml models use resources recommend high_res_conus low_res_conus gt model model low_res_gobal gt high_res_global,"['machine', 'learning', 'training', 'high', 'resolution', 'increase', 'low', 'resolution', 'datahigh', 'resolution', 'data', 'example', 'weather', 'ir', 'satellite', 'imagery', 'conus', 'low', 'resolution', 'satellite', 'imagery', 'entire', 'globe', 'could', 'train', 'machine', 'learning', 'model', 'one', 'region', 'apply', 'globally', 'ml', 'models', 'use', 'resources', 'recommend', 'high_res_conus', 'low_res_conus', 'gt', 'model', 'model', 'low_res_gobal', 'gt', 'high_res_global']","['machin', 'learn', 'train', 'high', 'resolut', 'increas', 'low', 'resolut', 'datahigh', 'resolut', 'data', 'exampl', 'weather', 'ir', 'satellit', 'imageri', 'conu', 'low', 'resolut', 'satellit', 'imageri', 'entir', 'globe', 'could', 'train', 'machin', 'learn', 'model', 'one', 'region', 'appli', 'global', 'ml', 'model', 'use', 'resourc', 'recommend', 'high_res_conu', 'low_res_conu', 'gt', 'model', 'model', 'low_res_gob', 'gt', 'high_res_glob']"
178,191,191,8272206,50953685,Agriculture commodity price predictions using machine learning,"<p>I want to create a web application which uses machine learning to predict the price of agriculture commodities before 2-3 months.</p>

<p>Is it really feasible or not?</p>

<p>If yes, then please provide some rough idea about which tools and technologies I can use to implement it.</p>
",312,2,1,3,node.js;machine-learning;prediction,2018-06-20 22:43:06,2018-06-20 22:43:06,2022-07-01 03:46:46,i want to create a web application which uses machine learning to predict the price of agriculture commodities before   months  is it really feasible or not  if yes  then please provide some rough idea about which tools and technologies i can use to implement it ,agriculture commodity price predictions using machine learning,want create web application uses machine learning predict price agriculture commodities months really feasible yes please provide rough idea tools technologies use implement,agriculture commodity price predictions using machine learning,agriculture commodity price predictions using machine learningwant create web application uses machine learning predict price agriculture commodities months really feasible yes please provide rough idea tools technologies use implement,"['agriculture', 'commodity', 'price', 'predictions', 'using', 'machine', 'learningwant', 'create', 'web', 'application', 'uses', 'machine', 'learning', 'predict', 'price', 'agriculture', 'commodities', 'months', 'really', 'feasible', 'yes', 'please', 'provide', 'rough', 'idea', 'tools', 'technologies', 'use', 'implement']","['agricultur', 'commod', 'price', 'predict', 'use', 'machin', 'learningw', 'creat', 'web', 'applic', 'use', 'machin', 'learn', 'predict', 'price', 'agricultur', 'commod', 'month', 'realli', 'feasibl', 'ye', 'pleas', 'provid', 'rough', 'idea', 'tool', 'technolog', 'use', 'implement']"
179,192,192,19454404,72820290,Trouble following Scatter Plot Implementation,"<p>I'm having trouble following along with an example provided by my professor. We're meant to follow along provided examples to understand the code and how the implementation goes and then do a different assignment based on topics covered in examples.</p>
<p>I'm having problems implementing a Scatter plot on the example. The code uses the Adult dataset from the UCI machine learning repository and has the following code.</p>
<pre><code>#install.packages(&quot;ggplot2&quot;)
library(ggplot2)

#import data
adult = read.csv(&quot;adult.DATA&quot;, header = FALSE, stringsAsFactors = TRUE)
summary(adult)
colnames(adult)

#remove similar columns and rename
adult_trim = adult[,-c(3,4,11,12)]
names(adult_trim) &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;Education&quot;, &quot;Marital.Status&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;,
                   &quot;Sex&quot;, &quot;Hours.per.Week&quot;, &quot;Native.Country&quot;, &quot;Income&quot;)

#remove empty values &amp; Race/NativeCountry
adult_trim &lt;- adult_trim[rowSums(adult_trim == &quot;?&quot;) ==0, -c(7,10), drop = FALSE]
</code></pre>
<p>The problem is in the following scatterplot. The data doesnt have any header values for column names so it imports as v1,v2,... etc.</p>
<pre><code>adult$V4 = as.factor(as.character(adult$V4))
levels(adult$V4)
plot(
  jitter(as.numeric(adult$V4),0.5) ~ jitter(as.numeric(adult$V4), 0.5),
  data = adult_trim,
  xlab = &quot;Income&quot;,
  ylab = &quot;Education&quot;,
  pch = 19, 
  cex = 1, 
  bty = &quot;n&quot;,
  xlim = c(1:2),
  col = rgb(180,0,180,30, maxColorValue = 255)
 )
</code></pre>
<p>When trying to implement this plot on my machine it just gives me an error.</p>
<pre><code>Warning message:
In plot.formula(jitter(as.numeric(adult$V4), 0.5) ~ jitter(as.numeric(adult$V4),  :
  c(&quot;the formula 'jitter(as.numeric(adult$V4), 0.5) ~ jitter(as.numeric(adult$V4), ' 
 is treated as 'jitter(as.numeric(adult$V4), 0.5) ~ 1'&quot;, &quot;the formula '    0.5)' 
 is treated as 'jitter(as.numeric(adult$V4), 0.5) ~ 1'&quot;)
</code></pre>
<p>its supposed to look like this graph but with education <a href=""https://i.stack.imgur.com/EPfhX.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/EPfhX.png</a> but I'm just getting the error. Also is there any reason this decides to use the original &quot;adult&quot; instead of &quot;adult_trim&quot; ?</p>
<p>Any help or explanation would be appreciated.</p>
",23,1,0,2,r;ggplot2,2022-06-30 23:41:28,2022-06-30 23:41:28,2022-07-01 00:26:28,i m having trouble following along with an example provided by my professor  we re meant to follow along provided examples to understand the code and how the implementation goes and then do a different assignment based on topics covered in examples  i m having problems implementing a scatter plot on the example  the code uses the adult dataset from the uci machine learning repository and has the following code  the problem is in the following scatterplot  the data doesnt have any header values for column names so it imports as v v     etc  when trying to implement this plot on my machine it just gives me an error  its supposed to look like this graph but with education  but i m just getting the error  also is there any reason this decides to use the original  adult  instead of  adult_trim    any help or explanation would be appreciated ,trouble following scatter plot implementation,trouble following along example provided professor meant follow along provided examples understand code implementation goes different assignment based topics covered examples problems implementing scatter plot example code uses adult dataset uci machine learning repository following code problem following scatterplot data doesnt header values column names imports v v etc trying implement plot machine gives error supposed look like graph education getting error also reason decides use original adult instead adult_trim help explanation would appreciated,trouble following scatter plot implementation,trouble following scatter plot implementationtrouble following along example provided professor meant follow along provided examples understand code implementation goes different assignment based topics covered examples problems implementing scatter plot example code uses adult dataset uci machine learning repository following code problem following scatterplot data doesnt header values column names imports v v etc trying implement plot machine gives error supposed look like graph education getting error also reason decides use original adult instead adult_trim help explanation would appreciated,"['trouble', 'following', 'scatter', 'plot', 'implementationtrouble', 'following', 'along', 'example', 'provided', 'professor', 'meant', 'follow', 'along', 'provided', 'examples', 'understand', 'code', 'implementation', 'goes', 'different', 'assignment', 'based', 'topics', 'covered', 'examples', 'problems', 'implementing', 'scatter', 'plot', 'example', 'code', 'uses', 'adult', 'dataset', 'uci', 'machine', 'learning', 'repository', 'following', 'code', 'problem', 'following', 'scatterplot', 'data', 'doesnt', 'header', 'values', 'column', 'names', 'imports', 'v', 'v', 'etc', 'trying', 'implement', 'plot', 'machine', 'gives', 'error', 'supposed', 'look', 'like', 'graph', 'education', 'getting', 'error', 'also', 'reason', 'decides', 'use', 'original', 'adult', 'instead', 'adult_trim', 'help', 'explanation', 'would', 'appreciated']","['troubl', 'follow', 'scatter', 'plot', 'implementationtroubl', 'follow', 'along', 'exampl', 'provid', 'professor', 'meant', 'follow', 'along', 'provid', 'exampl', 'understand', 'code', 'implement', 'goe', 'differ', 'assign', 'base', 'topic', 'cover', 'exampl', 'problem', 'implement', 'scatter', 'plot', 'exampl', 'code', 'use', 'adult', 'dataset', 'uci', 'machin', 'learn', 'repositori', 'follow', 'code', 'problem', 'follow', 'scatterplot', 'data', 'doesnt', 'header', 'valu', 'column', 'name', 'import', 'v', 'v', 'etc', 'tri', 'implement', 'plot', 'machin', 'give', 'error', 'suppos', 'look', 'like', 'graph', 'educ', 'get', 'error', 'also', 'reason', 'decid', 'use', 'origin', 'adult', 'instead', 'adult_trim', 'help', 'explan', 'would', 'appreci']"
180,193,193,19452872,72817554,Multivariate Times Series Classification using Machine Learning Algorithms,"<p>I am fairly new to machine learning and am currently working on a way to classify time series data. In order to do so, I would like to get a better understanding of how time series data can be fed into machine learning algorithms.</p>
<p>Further information:</p>
<ul>
<li><p>Each sample is a time series consisting of 2000 time points. For each time point, there are several variables, like temperature, speed, acceleration, etc. The data can be represented like this:
<a href=""https://i.stack.imgur.com/mxCPO.png"" rel=""nofollow noreferrer"">data structure for one time series sample</a></p>
</li>
<li><p>The whole dataset consists of 3000 samples. 3000 samples x 2000 data points per sample = 6000000 data points for each variable.</p>
</li>
<li><p>the goal is to classify the samples into classes from 0 to 4.</p>
</li>
</ul>
<p>My first attempt was just feeding the data as an array into the machine learning algorithms.
Let's say, we just focus on temperature. We can now structure the data like this:
<a href=""https://i.stack.imgur.com/lT40S.png"" rel=""nofollow noreferrer"">input training data for a ml-algorithm</a>
. Let X be the training input and y be the training output, the data looks like:</p>
<p>[21,21,22,...]=0</p>
<p>[35,35,35,...]=2</p>
<p>[11,12,12,...]=1</p>
<p>[18,17,18,...]=0</p>
<p>Can I just feed the machine learning algorithm (like SVCs) with array-type time series data like this? How does the algorithm know that the elements in the array are chronological data and not single features?</p>
<p>Here is an example code of what I did so far:</p>
<pre><code>dataframe.head()

     'sample_nr' 'timestamp' 'temperature' 'speed' 'acceleration'
 0   1           0.01        21             -0.43   0.34205 
 1   1           0.02        21             -0.43   0.34205 
 2   1           0.03        22             -0.43   0.34205 
</code></pre>
<p>Create a data_list, which contains all the sample_nr's in a list. Also, the dataframe gets grouped by the sample_nr</p>
<pre><code>data_list = []
for sample_nr, sample_df in dataframe.groupby('sample_nr'):    
    dataframe.groupby('sample_nr'):
    data_list.append(dataframe)
</code></pre>
<p>For a first step, we will only focus on one feature, let's say the temperature:</p>
<pre><code>X_list = []
y_list = []

for sample in data_list:
    temp_X = np.array(sample['temperature'])
    temp_y = sample['label'].unique()[0]

    X_list.append(temp_X)
    y_list.append(temp_y)
</code></pre>
<p>Transform the lists to pandas.Dataframes:</p>
<pre><code>X_df = pd.DataFrame(X_list)
y_df = pd.DataFrame(y_list)
</code></pre>
<p>Now, the X_df is a 3000x2000 list: Each row describes a sample, and the values in the columns are the temperature values for each of the 2000 time steps:</p>
<pre><code>print(X_df)
</code></pre>
<p>....<strong>0</strong>....<strong>1</strong>....<strong>2</strong>....<strong>3</strong></p>
<p><strong>0</strong> 21 21 22 22</p>
<p><strong>1</strong> 35 35 35 36</p>
<p><strong>2</strong> 11 12 12 12</p>
<p>Also, for the output value:</p>
<pre><code>print(y_df)
</code></pre>
<p>....<strong>0</strong></p>
<p><strong>0</strong> 0</p>
<p><strong>1</strong> 2</p>
<p><strong>2</strong> 1</p>
<p>Now split up the dataframe to train and test data:</p>
<pre><code>X_train_array, X_test_array, y_train_array, y_test_array = train_test_split(X_df, y_df, test_size=0.2, shuffle=True, random_state=42)
X_train_df = pd.DataFrame(X_train_array)
X_test_df = pd.DataFrame(X_test_array)
y_train_df = pd.DataFrame(y_train_array)
y_test_df = pd.DataFrame(y_test_array)

from sklearn import svm
clf = svm.SVC()
clf.fit(X_train_df, y_train_df)
</code></pre>
",25,0,0,5,python;machine-learning;time-series;classification;multivariate-time-series,2022-06-30 19:57:10,2022-06-30 19:57:10,2022-06-30 22:19:40,i am fairly new to machine learning and am currently working on a way to classify time series data  in order to do so  i would like to get a better understanding of how time series data can be fed into machine learning algorithms  further information  the whole dataset consists of  samples   samples x  data points per sample    data points for each variable  the goal is to classify the samples into classes from  to                                           can i just feed the machine learning algorithm  like svcs  with array type time series data like this  how does the algorithm know that the elements in the array are chronological data and not single features  here is an example code of what i did so far  create a data_list  which contains all the sample_nr s in a list  also  the dataframe gets grouped by the sample_nr for a first step  we will only focus on one feature  let s say the temperature  transform the lists to pandas dataframes  now  the x_df is a x list  each row describes a sample  and the values in the columns are the temperature values for each of the  time steps                                  also  for the output value             now split up the dataframe to train and test data ,multivariate times series classification using machine learning algorithms,fairly machine learning currently working way classify time series data order would like get better understanding time series data fed machine learning algorithms information whole dataset consists samples samples x data points per sample data points variable goal classify samples classes feed machine learning algorithm like svcs array type time series data like algorithm know elements array chronological data single features example code far create data_list contains sample_nr also dataframe gets grouped sample_nr first step focus one feature let say temperature transform lists pandas dataframes x_df x row describes sample values columns temperature values time steps also output value split dataframe train test data,multivariate times series classification using machine learning algorithms,multivariate times series classification using machine learning algorithmsfairly machine learning currently working way classify time series data order would like get better understanding time series data fed machine learning algorithms information whole dataset consists samples samples x data points per sample data points variable goal classify samples classes feed machine learning algorithm like svcs array type time series data like algorithm know elements array chronological data single features example code far create data_list contains sample_nr also dataframe gets grouped sample_nr first step focus one feature let say temperature transform lists pandas dataframes x_df x row describes sample values columns temperature values time steps also output value split dataframe train test data,"['multivariate', 'times', 'series', 'classification', 'using', 'machine', 'learning', 'algorithmsfairly', 'machine', 'learning', 'currently', 'working', 'way', 'classify', 'time', 'series', 'data', 'order', 'would', 'like', 'get', 'better', 'understanding', 'time', 'series', 'data', 'fed', 'machine', 'learning', 'algorithms', 'information', 'whole', 'dataset', 'consists', 'samples', 'samples', 'x', 'data', 'points', 'per', 'sample', 'data', 'points', 'variable', 'goal', 'classify', 'samples', 'classes', 'feed', 'machine', 'learning', 'algorithm', 'like', 'svcs', 'array', 'type', 'time', 'series', 'data', 'like', 'algorithm', 'know', 'elements', 'array', 'chronological', 'data', 'single', 'features', 'example', 'code', 'far', 'create', 'data_list', 'contains', 'sample_nr', 'also', 'dataframe', 'gets', 'grouped', 'sample_nr', 'first', 'step', 'focus', 'one', 'feature', 'let', 'say', 'temperature', 'transform', 'lists', 'pandas', 'dataframes', 'x_df', 'x', 'row', 'describes', 'sample', 'values', 'columns', 'temperature', 'values', 'time', 'steps', 'also', 'output', 'value', 'split', 'dataframe', 'train', 'test', 'data']","['multivari', 'time', 'seri', 'classif', 'use', 'machin', 'learn', 'algorithmsfairli', 'machin', 'learn', 'current', 'work', 'way', 'classifi', 'time', 'seri', 'data', 'order', 'would', 'like', 'get', 'better', 'understand', 'time', 'seri', 'data', 'fed', 'machin', 'learn', 'algorithm', 'inform', 'whole', 'dataset', 'consist', 'sampl', 'sampl', 'x', 'data', 'point', 'per', 'sampl', 'data', 'point', 'variabl', 'goal', 'classifi', 'sampl', 'class', 'feed', 'machin', 'learn', 'algorithm', 'like', 'svc', 'array', 'type', 'time', 'seri', 'data', 'like', 'algorithm', 'know', 'element', 'array', 'chronolog', 'data', 'singl', 'featur', 'exampl', 'code', 'far', 'creat', 'data_list', 'contain', 'sample_nr', 'also', 'datafram', 'get', 'group', 'sample_nr', 'first', 'step', 'focu', 'one', 'featur', 'let', 'say', 'temperatur', 'transform', 'list', 'panda', 'datafram', 'x_df', 'x', 'row', 'describ', 'sampl', 'valu', 'column', 'temperatur', 'valu', 'time', 'step', 'also', 'output', 'valu', 'split', 'datafram', 'train', 'test', 'data']"
181,194,194,7373787,62409303,How to handle missing values (NaN) in categorical data when using scikit-learn OneHotEncoder?,"<p>I have recently started learning python to develop a predictive model for a research project using machine learning methods. I have a large dataset comprised of both numerical and categorical data. The dataset has lots of missing values. I am currently trying to encode the categorical features using OneHotEncoder. When I read about OneHotEncoder, my understanding was that for a missing value (NaN), OneHotEncoder would assign 0s to all the feature's categories, as such:</p>

<pre><code>0     Male 
1     Female
2     NaN
</code></pre>

<p>After applying OneHotEncoder:</p>

<pre><code>0     10 
1     01
2     00
</code></pre>

<p>However, when running the following code:</p>

<pre class=""lang-py prettyprint-override""><code>    # Encoding categorical data
    from sklearn.compose import ColumnTransformer
    from sklearn.preprocessing import OneHotEncoder


    ct = ColumnTransformer([('encoder', OneHotEncoder(handle_unknown='ignore'), [1])],
                           remainder='passthrough')
    obj_df = np.array(ct.fit_transform(obj_df))
    print(obj_df)

</code></pre>

<p>I am getting the error <strong>ValueError: Input contains NaN</strong></p>

<p>So I am guessing my previous understanding of how OneHotEncoder handles missing values is wrong. 
Is there a way for me to get the functionality described above? I know imputing the missing values before encoding will resolve this issue, but I am reluctant to do this as I am dealing with medical data and fear that imputation may decrease the predictive accuracy of my model. </p>

<p>I found this <a href=""https://stackoverflow.com/questions/58222008/nan-giving-valueerror-in-onehotencoder-in-scikit-learn"">question</a> that is similar but the answer doesn't offer a detailed enough solution on how to deal with the NaN values.</p>

<p>Let me know what your thoughts are, thanks.</p>
",11867,5,15,3,python;machine-learning;scikit-learn,2020-06-16 18:39:16,2020-06-16 18:39:16,2022-06-30 21:06:48,i have recently started learning python to develop a predictive model for a research project using machine learning methods  i have a large dataset comprised of both numerical and categorical data  the dataset has lots of missing values  i am currently trying to encode the categorical features using onehotencoder  when i read about onehotencoder  my understanding was that for a missing value  nan   onehotencoder would assign s to all the feature s categories  as such  after applying onehotencoder  however  when running the following code  i am getting the error valueerror  input contains nan i found this  that is similar but the answer doesn t offer a detailed enough solution on how to deal with the nan values  let me know what your thoughts are  thanks ,how to handle missing values  nan  in categorical data when using scikit learn onehotencoder ,recently started learning python develop predictive model research project using machine learning methods large dataset comprised numerical categorical data dataset lots missing values currently trying encode categorical features using onehotencoder read onehotencoder understanding missing value nan onehotencoder would assign feature categories applying onehotencoder however running following code getting error valueerror input contains nan found similar answer offer detailed enough solution deal nan values let know thoughts thanks,handle missing values nan categorical data using scikit learn onehotencoder,handle missing values nan categorical data using scikit learn onehotencoderrecently started learning python develop predictive model research project using machine learning methods large dataset comprised numerical categorical data dataset lots missing values currently trying encode categorical features using onehotencoder read onehotencoder understanding missing value nan onehotencoder would assign feature categories applying onehotencoder however running following code getting error valueerror input contains nan found similar answer offer detailed enough solution deal nan values let know thoughts thanks,"['handle', 'missing', 'values', 'nan', 'categorical', 'data', 'using', 'scikit', 'learn', 'onehotencoderrecently', 'started', 'learning', 'python', 'develop', 'predictive', 'model', 'research', 'project', 'using', 'machine', 'learning', 'methods', 'large', 'dataset', 'comprised', 'numerical', 'categorical', 'data', 'dataset', 'lots', 'missing', 'values', 'currently', 'trying', 'encode', 'categorical', 'features', 'using', 'onehotencoder', 'read', 'onehotencoder', 'understanding', 'missing', 'value', 'nan', 'onehotencoder', 'would', 'assign', 'feature', 'categories', 'applying', 'onehotencoder', 'however', 'running', 'following', 'code', 'getting', 'error', 'valueerror', 'input', 'contains', 'nan', 'found', 'similar', 'answer', 'offer', 'detailed', 'enough', 'solution', 'deal', 'nan', 'values', 'let', 'know', 'thoughts', 'thanks']","['handl', 'miss', 'valu', 'nan', 'categor', 'data', 'use', 'scikit', 'learn', 'onehotencoderrec', 'start', 'learn', 'python', 'develop', 'predict', 'model', 'research', 'project', 'use', 'machin', 'learn', 'method', 'larg', 'dataset', 'compris', 'numer', 'categor', 'data', 'dataset', 'lot', 'miss', 'valu', 'current', 'tri', 'encod', 'categor', 'featur', 'use', 'onehotencod', 'read', 'onehotencod', 'understand', 'miss', 'valu', 'nan', 'onehotencod', 'would', 'assign', 'featur', 'categori', 'appli', 'onehotencod', 'howev', 'run', 'follow', 'code', 'get', 'error', 'valueerror', 'input', 'contain', 'nan', 'found', 'similar', 'answer', 'offer', 'detail', 'enough', 'solut', 'deal', 'nan', 'valu', 'let', 'know', 'thought', 'thank']"
182,195,195,11967549,72815510,"Matplotlib - When I format ticks, they are not replaced and instead added to the plot","<p>Hello I am using Matplotlib to plot some curves for machine learning. I have the problem that when I format my x_ticks , instead of replacing the old ones they are added to the plot. See the first plot on the image below on the left where percent and numbers are plotted:</p>
<p><a href=""https://i.stack.imgur.com/yJd6d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yJd6d.png"" alt=""enter image description here"" /></a></p>
<p>This is the code from scikit learn. I use <code>axes[0].xaxis.set_major_formatter(PercentFormatter(xmax=5))</code> to format the first plot which seems to work, but like I said the labels are just added on top. I had the same problem with other matplotlib implementations in geopandas for example so I thought maybe someone knows what to do.</p>
<pre><code>from matplotlib.ticker import PercentFormatter
fig, axes = plt.subplots(3, 2, figsize=(10, 15))

plot_learning_curve(...)


def plot_learning_curve(
    estimator,
    title,
    X,
    y,
    axes=None,
    ylim=None,
    cv=None,
    n_jobs=None,
    train_sizes=np.linspace(0.1, 1.0, 5),
):
    &quot;&quot;&quot;
    Generate 3 plots: the test and training learning curve, the training
    samples vs fit times curve, the fit times vs score curve.

    Parameters
    ----------
    estimator : estimator instance
        An estimator instance implementing `fit` and `predict` methods which
        will be cloned for each validation.

    title : str
        Title for the chart.

    X : array-like of shape (n_samples, n_features)
        Training vector, where ``n_samples`` is the number of samples and
        ``n_features`` is the number of features.

    y : array-like of shape (n_samples) or (n_samples, n_features)
        Target relative to ``X`` for classification or regression;
        None for unsupervised learning.

    axes : array-like of shape (3,), default=None
        Axes to use for plotting the curves.

    ylim : tuple of shape (2,), default=None
        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).

    cv : int, cross-validation generator or an iterable, default=None
        Determines the cross-validation splitting strategy.
        Possible inputs for cv are:

          - None, to use the default 5-fold cross-validation,
          - integer, to specify the number of folds.
          - :term:`CV splitter`,
          - An iterable yielding (train, test) splits as arrays of indices.

        For integer/None inputs, if ``y`` is binary or multiclass,
        :class:`StratifiedKFold` used. If the estimator is not a classifier
        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.

        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
        cross-validators that can be used here.

    n_jobs : int or None, default=None
        Number of jobs to run in parallel.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
        for more details.

    train_sizes : array-like of shape (n_ticks,)
        Relative or absolute numbers of training examples that will be used to
        generate the learning curve. If the ``dtype`` is float, it is regarded
        as a fraction of the maximum size of the training set (that is
        determined by the selected validation method), i.e. it has to be within
        (0, 1]. Otherwise it is interpreted as absolute sizes of the training
        sets. Note that for classification the number of samples usually have
        to be big enough to contain at least one sample from each class.
        (default: np.linspace(0.1, 1.0, 5))
    &quot;&quot;&quot;
    if axes is None:
        _, axes = plt.subplots(1, 3, figsize=(20, 5))

    axes[0].set_title(title)
    if ylim is not None:
        axes[0].set_ylim(*ylim)
    axes[0].set_xlabel(&quot;Training examples&quot;)
    axes[0].set_ylabel(&quot;Score&quot;)

    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(
        estimator,
        X,
        y,
        cv=cv,
        n_jobs=n_jobs,
        train_sizes=train_sizes,
        return_times=True,
    )
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fit_times_mean = np.mean(fit_times, axis=1)
    fit_times_std = np.std(fit_times, axis=1)

    # Plot learning curve
    axes[0].grid()
    axes[0].fill_between(
        train_sizes,
        train_scores_mean - train_scores_std,
        train_scores_mean + train_scores_std,
        alpha=0.1,
        color=&quot;r&quot;,
    )
    axes[0].fill_between(
        train_sizes,
        test_scores_mean - test_scores_std,
        test_scores_mean + test_scores_std,
        alpha=0.1,
        color=&quot;g&quot;,
    )
    axes[0].plot(
        train_sizes, train_scores_mean, &quot;o-&quot;, color=&quot;r&quot;, label=&quot;Training score&quot;
    )
    axes[0].plot(
        train_sizes, test_scores_mean, &quot;o-&quot;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;
    )
    axes[0].legend(loc=&quot;best&quot;)
    axes[0].xaxis.set_major_formatter(PercentFormatter(xmax=5))
    
    # Plot n_samples vs fit_times
    axes[1].grid()
    axes[1].plot(train_sizes, fit_times_mean, &quot;o-&quot;)
    axes[1].fill_between(
        train_sizes,
        fit_times_mean - fit_times_std,
        fit_times_mean + fit_times_std,
        alpha=0.1,
    )
    axes[1].set_xlabel(&quot;Training examples&quot;)
    axes[1].set_ylabel(&quot;fit_times&quot;)
    axes[1].set_title(&quot;Scalability of the model&quot;)

    # Plot fit_time vs score
    fit_time_argsort = fit_times_mean.argsort()
    fit_time_sorted = fit_times_mean[fit_time_argsort]
    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]
    test_scores_std_sorted = test_scores_std[fit_time_argsort]
    axes[2].grid()
    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, &quot;o-&quot;)
    axes[2].fill_between(
        fit_time_sorted,
        test_scores_mean_sorted - test_scores_std_sorted,
        test_scores_mean_sorted + test_scores_std_sorted,
        alpha=0.1,
    )
    axes[2].set_xlabel(&quot;fit_times&quot;)
    axes[2].set_ylabel(&quot;Score&quot;)
    axes[2].set_title(&quot;Performance of the model&quot;)

    return plt
</code></pre>
",30,1,-1,4,python;matplotlib;plot;scikit-learn,2022-06-30 17:35:12,2022-06-30 17:35:12,2022-06-30 18:52:00,hello i am using matplotlib to plot some curves for machine learning  i have the problem that when i format my x_ticks   instead of replacing the old ones they are added to the plot  see the first plot on the image below on the left where percent and numbers are plotted   this is the code from scikit learn  i use axes   xaxis set_major_formatter percentformatter xmax    to format the first plot which seems to work  but like i said the labels are just added on top  i had the same problem with other matplotlib implementations in geopandas for example so i thought maybe someone knows what to do ,matplotlib   when i format ticks  they are not replaced and instead added to the plot,hello using matplotlib plot curves machine learning problem format x_ticks instead replacing old ones added plot see first plot image left percent numbers plotted code scikit learn use axes xaxis set_major_formatter percentformatter xmax format first plot seems work like said labels added top problem matplotlib implementations geopandas example thought maybe someone knows,matplotlib format ticks replaced instead added plot,matplotlib format ticks replaced instead added plothello using matplotlib plot curves machine learning problem format x_ticks instead replacing old ones added plot see first plot image left percent numbers plotted code scikit learn use axes xaxis set_major_formatter percentformatter xmax format first plot seems work like said labels added top problem matplotlib implementations geopandas example thought maybe someone knows,"['matplotlib', 'format', 'ticks', 'replaced', 'instead', 'added', 'plothello', 'using', 'matplotlib', 'plot', 'curves', 'machine', 'learning', 'problem', 'format', 'x_ticks', 'instead', 'replacing', 'old', 'ones', 'added', 'plot', 'see', 'first', 'plot', 'image', 'left', 'percent', 'numbers', 'plotted', 'code', 'scikit', 'learn', 'use', 'axes', 'xaxis', 'set_major_formatter', 'percentformatter', 'xmax', 'format', 'first', 'plot', 'seems', 'work', 'like', 'said', 'labels', 'added', 'top', 'problem', 'matplotlib', 'implementations', 'geopandas', 'example', 'thought', 'maybe', 'someone', 'knows']","['matplotlib', 'format', 'tick', 'replac', 'instead', 'ad', 'plothello', 'use', 'matplotlib', 'plot', 'curv', 'machin', 'learn', 'problem', 'format', 'x_tick', 'instead', 'replac', 'old', 'one', 'ad', 'plot', 'see', 'first', 'plot', 'imag', 'left', 'percent', 'number', 'plot', 'code', 'scikit', 'learn', 'use', 'axe', 'xaxi', 'set_major_formatt', 'percentformatt', 'xmax', 'format', 'first', 'plot', 'seem', 'work', 'like', 'said', 'label', 'ad', 'top', 'problem', 'matplotlib', 'implement', 'geopanda', 'exampl', 'thought', 'mayb', 'someon', 'know']"
183,196,196,17031310,72816363,"Python List [:,3]","<p>i'm new to machine learning and I have found this code from github and currently i'm trying to replicate it.</p>
<p>However the code has some runtime errors particularly the way it is accessing the list.</p>
<pre><code>trainx, trainy = train[:, 1], train[:, 3]
</code></pre>
<p>Take this block of code for example, python throws an error
TypeError: list indices must be integers or slices, not tuple</p>
<p>I understand that this is not the code i've written however, I don't understand the reference [:,1]</p>
<p>The code is littered with [:,1] . [:,2]
Is he trying to access the first and second column of the csv?
I believe he is trying to achieve that.</p>
<p>Can anyone explain what he is trying to access from the list.</p>
<p>The github code is located here</p>
<pre><code>https://github.com/anthonymorast/lstm-lstm/blob/master/model/lstm.py
</code></pre>
",31,0,-1,2,python;list,2022-06-30 18:35:51,2022-06-30 18:35:51,2022-06-30 18:35:51,i m new to machine learning and i have found this code from github and currently i m trying to replicate it  however the code has some runtime errors particularly the way it is accessing the list  i understand that this is not the code i ve written however  i don t understand the reference      can anyone explain what he is trying to access from the list  the github code is located here,python list     ,machine learning found code github currently trying replicate however code runtime errors particularly way accessing understand code written however understand reference anyone explain trying access github code located,python,pythonmachine learning found code github currently trying replicate however code runtime errors particularly way accessing understand code written however understand reference anyone explain trying access github code located,"['pythonmachine', 'learning', 'found', 'code', 'github', 'currently', 'trying', 'replicate', 'however', 'code', 'runtime', 'errors', 'particularly', 'way', 'accessing', 'understand', 'code', 'written', 'however', 'understand', 'reference', 'anyone', 'explain', 'trying', 'access', 'github', 'code', 'located']","['pythonmachin', 'learn', 'found', 'code', 'github', 'current', 'tri', 'replic', 'howev', 'code', 'runtim', 'error', 'particularli', 'way', 'access', 'understand', 'code', 'written', 'howev', 'understand', 'refer', 'anyon', 'explain', 'tri', 'access', 'github', 'code', 'locat']"
184,197,197,19146827,72816169,Shiny app (leafviz) in docker container not loading in browser,"<p>I am building a docker container to use the Leafcutter and Leafviz analysis tools. The final visualization commanded from an R file uses shiny app, it works fine on my machine but when ran in the container I can't connect to it.</p>
<p>I run the container with the following line of code:</p>
<pre><code>docker run -dit --name leaf --rm -p 1234:1234 leafcutter
</code></pre>
<p>All is in order when I check with docker ps:</p>
<pre><code>CONTAINER ID   IMAGE        COMMAND   CREATED         STATUS         PORTS                                       NAMES
783f550df965   leafcutter   &quot;bash&quot;    2 minutes ago   Up 2 minutes   0.0.0.0:1234-&gt;1234/tcp, :::1234-&gt;1234/tcp   leaf
</code></pre>
<p>Executing the pipeline in the container calls several snakefiles, and the last rule of the last one calls the R file:</p>
<pre><code>Rscript /path/to/run_leafviz.R -i {input}
</code></pre>
<p>That script calls several functions before calling leafviz() which basically just runs the shiny app, I've modified the options like so:</p>
<pre><code>shiny::runApp(launch.browser=FALSE, appDir = system.file(&quot;application&quot;, package = &quot;leafviz&quot;), host = &quot;0.0.0.0&quot;, port = 1234)
</code></pre>
<p>All the shiny files and packages are installed with the R leafviz package. When I execute my container all goes fine, the output shows &quot;Listening on <a href=""http://0.0.0.0:1234%22"" rel=""nofollow noreferrer"">http://0.0.0.0:1234&quot;</a>, but when I try to connect I get the &quot;connexion failed&quot; page.</p>
<p>My Dockerfile is fairly large but here are the most relevant parts:</p>
<pre><code>FROM ubuntu:22.04
ENV DEBIAN_FRONTEND noninteractive

#Install Ubuntu packages
RUN apt-get update &amp;&amp; apt-get install \
    -y cmake -y curl \
    -y default-jre \
    -y gdebi-core \
    -y less -y libarchive13 -y libbz2-dev -y libcairo2-dev -y libcurl4-openssl-dev -y libgsl-dev \
    -y liblzma-dev -y libncurses5-dev -y libncursesw5-dev -y libssl-dev -y libxml2-dev -y libxt-dev \
    -y pandoc -y pandoc-citeproc -y python-pip \
    -y tabix \
    -y unzip \
    -y wget \
    -y zlib1g-dev \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

#Install conda, snakemake, samtools, regtools...
#[...]

#Install R
LABEL org.label-schema.license=&quot;GPL-2.0&quot; \
      org.label-schema.vcs-url=&quot;https://github.com/rocker-org/r-apt&quot; \
      org.label-schema.vendor=&quot;Rocker Project&quot; \
      maintainer=&quot;Dirk Eddelbuettel &lt;edd@debian.org&gt;&quot;
RUN useradd docker \
    &amp;&amp; mkdir /home/docker \
    &amp;&amp; chown docker:docker /home/docker \
    &amp;&amp; addgroup docker staff
RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
    littler r-base r-base-dev r-recommended r-cran-docopt
RUN ln -s /usr/lib/R/site-library/littler/examples/install.r /usr/local/bin/install.r \
    &amp;&amp; ln -s /usr/lib/R/site-library/littler/examples/install2.r /usr/local/bin/install2.r \
    &amp;&amp; ln -s /usr/lib/R/site-library/littler/examples/installGithub.r /usr/local/bin/installGithub.r \
    &amp;&amp; ln -s /usr/lib/R/site-library/littler/examples/testInstalled.r /usr/local/bin/testInstalled.r \
    &amp;&amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

#Install shiny server (I have tried with and without this RUN command)
RUN wget --no-verbose https://s3.amazonaws.com/rstudio-shiny-server-os-build/ubuntu-12.04/x86_64/VERSION -O &quot;version.txt&quot; &amp;&amp; \
    VERSION=$(cat version.txt)  &amp;&amp; \
    wget --no-verbose &quot;https://s3.amazonaws.com/rstudio-shiny-server-os-build/ubuntu-12.04/x86_64/shiny-server-$VERSION-amd64.deb&quot; -O ss-latest.deb &amp;&amp; \
    gdebi -n ss-latest.deb &amp;&amp; \
    rm -f version.txt ss-latest.deb

EXPOSE 1234

#Install R packages
COPY install_packages.R /tmp/install_packages.R 
RUN Rscript /tmp/install_packages.R 

CMD [&quot;bash&quot;]
</code></pre>
<p>I've been learning docker quite chaotically so I don't know if some elements are clashing, but I have exposed the container port and published it to the host and no error message seems to indicate there is a problem coming from somewhere else. I would really appreciate some help.</p>
",21,0,0,3,docker;shiny;dockerfile,2022-06-30 18:21:11,2022-06-30 18:21:11,2022-06-30 18:21:11,i am building a docker container to use the leafcutter and leafviz analysis tools  the final visualization commanded from an r file uses shiny app  it works fine on my machine but when ran in the container i can t connect to it  i run the container with the following line of code  all is in order when i check with docker ps  executing the pipeline in the container calls several snakefiles  and the last rule of the last one calls the r file  that script calls several functions before calling leafviz   which basically just runs the shiny app  i ve modified the options like so  all the shiny files and packages are installed with the r leafviz package  when i execute my container all goes fine  the output shows  listening on   but when i try to connect i get the  connexion failed  page  my dockerfile is fairly large but here are the most relevant parts  i ve been learning docker quite chaotically so i don t know if some elements are clashing  but i have exposed the container port and published it to the host and no error message seems to indicate there is a problem coming from somewhere else  i would really appreciate some help ,shiny app  leafviz  in docker container not loading in browser,building docker container use leafcutter leafviz analysis tools final visualization commanded r file uses shiny app works fine machine ran container connect run container following line code order check docker ps executing pipeline container calls several snakefiles last rule last one calls r file script calls several functions calling leafviz basically runs shiny app modified options like shiny files packages installed r leafviz package execute container goes fine output shows listening try connect get connexion failed page dockerfile fairly large relevant parts learning docker quite chaotically know elements clashing exposed container port published host error message seems indicate problem coming somewhere else would really appreciate help,shiny app leafviz docker container loading browser,shiny app leafviz docker container loading browserbuilding docker container use leafcutter leafviz analysis tools final visualization commanded r file uses shiny app works fine machine ran container connect run container following line code order check docker ps executing pipeline container calls several snakefiles last rule last one calls r file script calls several functions calling leafviz basically runs shiny app modified options like shiny files packages installed r leafviz package execute container goes fine output shows listening try connect get connexion failed page dockerfile fairly large relevant parts learning docker quite chaotically know elements clashing exposed container port published host error message seems indicate problem coming somewhere else would really appreciate help,"['shiny', 'app', 'leafviz', 'docker', 'container', 'loading', 'browserbuilding', 'docker', 'container', 'use', 'leafcutter', 'leafviz', 'analysis', 'tools', 'final', 'visualization', 'commanded', 'r', 'file', 'uses', 'shiny', 'app', 'works', 'fine', 'machine', 'ran', 'container', 'connect', 'run', 'container', 'following', 'line', 'code', 'order', 'check', 'docker', 'ps', 'executing', 'pipeline', 'container', 'calls', 'several', 'snakefiles', 'last', 'rule', 'last', 'one', 'calls', 'r', 'file', 'script', 'calls', 'several', 'functions', 'calling', 'leafviz', 'basically', 'runs', 'shiny', 'app', 'modified', 'options', 'like', 'shiny', 'files', 'packages', 'installed', 'r', 'leafviz', 'package', 'execute', 'container', 'goes', 'fine', 'output', 'shows', 'listening', 'try', 'connect', 'get', 'connexion', 'failed', 'page', 'dockerfile', 'fairly', 'large', 'relevant', 'parts', 'learning', 'docker', 'quite', 'chaotically', 'know', 'elements', 'clashing', 'exposed', 'container', 'port', 'published', 'host', 'error', 'message', 'seems', 'indicate', 'problem', 'coming', 'somewhere', 'else', 'would', 'really', 'appreciate', 'help']","['shini', 'app', 'leafviz', 'docker', 'contain', 'load', 'browserbuild', 'docker', 'contain', 'use', 'leafcutt', 'leafviz', 'analysi', 'tool', 'final', 'visual', 'command', 'r', 'file', 'use', 'shini', 'app', 'work', 'fine', 'machin', 'ran', 'contain', 'connect', 'run', 'contain', 'follow', 'line', 'code', 'order', 'check', 'docker', 'ps', 'execut', 'pipelin', 'contain', 'call', 'sever', 'snakefil', 'last', 'rule', 'last', 'one', 'call', 'r', 'file', 'script', 'call', 'sever', 'function', 'call', 'leafviz', 'basic', 'run', 'shini', 'app', 'modifi', 'option', 'like', 'shini', 'file', 'packag', 'instal', 'r', 'leafviz', 'packag', 'execut', 'contain', 'goe', 'fine', 'output', 'show', 'listen', 'tri', 'connect', 'get', 'connexion', 'fail', 'page', 'dockerfil', 'fairli', 'larg', 'relev', 'part', 'learn', 'docker', 'quit', 'chaotic', 'know', 'element', 'clash', 'expos', 'contain', 'port', 'publish', 'host', 'error', 'messag', 'seem', 'indic', 'problem', 'come', 'somewher', 'els', 'would', 'realli', 'appreci', 'help']"
185,198,198,13990977,72815591,How can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?,"<p>I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.</p>
<p>Basically my x data is a Pandas series which contains numpy ndarrays, which contain floats.
My y data is a series of numpy ndarrays of shape (1,1), so basically just a single float value.</p>
<p>You can imagine my dataset to look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>x data (pandas series)</th>
<th>y data (pandas series)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>np.ndarray(shape (1209278,) )</code></td>
<td><code>np.ndarray(shape = () )</code></td>
</tr>
<tr>
<td>1</td>
<td><code>np.ndarray(shape (1211140,) )</code></td>
<td><code>np.ndarray(shape = () )</code></td>
</tr>
<tr>
<td>2</td>
<td><code>np.ndarray(shape (1418411,) )</code></td>
<td><code>np.ndarray(shape = () )</code></td>
</tr>
<tr>
<td>3</td>
<td><code>np.ndarray(shape (1077132,) )</code></td>
<td><code>np.ndarray(shape = () )</code></td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>The type of my x data and y data is, as stated above, a pandas series.
When I try to train my model using the fit function it yields this error:</p>
<p><em>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)</em></p>
<p>I also tried converting the pandas series to a numpy array, but this did not help.
As it seems, the fact that I have a series of differently shaped ndarrays as my input data is the problem itself.</p>
<p>I don't really know what I can do, to fix this error.
Which leads me to my question:</p>
<p><strong>How can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?</strong></p>
",39,1,1,5,python;pandas;numpy;tensorflow;keras,2022-06-30 17:40:33,2022-06-30 17:40:33,2022-06-30 17:57:00,i am trying to build a machine learning model which predicts a single number from a series of numbers  i am using a sequential model from the keras api of tensorflow  you can imagine my dataset to look something like this  valueerror  failed to convert a numpy array to a tensor  unsupported object type numpy ndarray  how can i have a series of numpy ndarrays as the input data to train a tensorflow machine learning model ,how can i have a series of numpy ndarrays as the input data to train a tensorflow machine learning model ,trying build machine learning model predicts single number series numbers using sequential model keras api tensorflow imagine dataset look something like valueerror failed convert numpy array tensor unsupported object type numpy ndarray series numpy ndarrays input data train tensorflow machine learning model,series numpy ndarrays input data train tensorflow machine learning model,series numpy ndarrays input data train tensorflow machine learning modeltrying build machine learning model predicts single number series numbers using sequential model keras api tensorflow imagine dataset look something like valueerror failed convert numpy array tensor unsupported object type numpy ndarray series numpy ndarrays input data train tensorflow machine learning model,"['series', 'numpy', 'ndarrays', 'input', 'data', 'train', 'tensorflow', 'machine', 'learning', 'modeltrying', 'build', 'machine', 'learning', 'model', 'predicts', 'single', 'number', 'series', 'numbers', 'using', 'sequential', 'model', 'keras', 'api', 'tensorflow', 'imagine', 'dataset', 'look', 'something', 'like', 'valueerror', 'failed', 'convert', 'numpy', 'array', 'tensor', 'unsupported', 'object', 'type', 'numpy', 'ndarray', 'series', 'numpy', 'ndarrays', 'input', 'data', 'train', 'tensorflow', 'machine', 'learning', 'model']","['seri', 'numpi', 'ndarray', 'input', 'data', 'train', 'tensorflow', 'machin', 'learn', 'modeltri', 'build', 'machin', 'learn', 'model', 'predict', 'singl', 'number', 'seri', 'number', 'use', 'sequenti', 'model', 'kera', 'api', 'tensorflow', 'imagin', 'dataset', 'look', 'someth', 'like', 'valueerror', 'fail', 'convert', 'numpi', 'array', 'tensor', 'unsupport', 'object', 'type', 'numpi', 'ndarray', 'seri', 'numpi', 'ndarray', 'input', 'data', 'train', 'tensorflow', 'machin', 'learn', 'model']"
186,199,199,18276526,72815361,Using Bootstrap tab view in Django,"<p>I am trying to implement tabs using Django and Bootstrap.
The following code does not switch tabs properly. Tab switching is not working even thought URL is changing
Please let me know how I can switch tabs without any problems.</p>
<p>Code</p>
<pre><code>        &lt;div class = &quot;company-info-tab&quot;&gt;
            &lt;div class=&quot;container&quot;&gt;
              &lt;!-- Nav tabs --&gt;
              &lt;ul class=&quot;nav nav-tabs&quot;&gt;
                &lt;li class=&quot;nav-item&quot;&gt;
                  &lt;a class=&quot;nav-link active&quot; data-toggle=&quot;tab&quot; href=&quot;#home&quot;&gt;Home&lt;/a&gt;
                &lt;/li&gt;
                &lt;li class=&quot;nav-item&quot;&gt;
                  &lt;a class=&quot;nav-link&quot; data-toggle=&quot;tab&quot; href=&quot;#menu1&quot;&gt;Baby computer Man&lt;/a&gt;
                &lt;/li&gt;
                &lt;li class=&quot;nav-item&quot;&gt;
                  &lt;a class=&quot;nav-link&quot; data-toggle=&quot;tab&quot; href=&quot;#menu2&quot;&gt;Menu 2&lt;/a&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
              &lt;!-- Tab panes --&gt;
              &lt;div class=&quot;tab-content&quot;&gt;
                &lt;div class=&quot;tab-pane container active&quot; id=&quot;home&quot;&gt;A &quot;Hello, World!&quot; program generally is a computer program that outputs or displays the message &quot;Hello, World!&quot;. Such a program is very simple in most programming languages, and is often used to illustrate the basic syntax of a programming language. It is often the first program written by people learning to code.&lt;/div&gt;
                &lt;div class=&quot;tab-pane container fade&quot; id=&quot;menu1&quot;&gt;The Manchester Baby, also known as the Small-Scale Experimental Machine, was the world's first electronic stored-program computer. It was built at the University of Manchester, UK, by Frederic C. Williams, Tom Kilburn, and Geoff Tootill, and ran its first program on 21 June 1948, seventy-one years ago&lt;/div&gt;
                &lt;div class=&quot;tab-pane container fade&quot; id=&quot;menu2&quot;&gt;...&lt;/div&gt;
              &lt;/div&gt;
            &lt;/div&gt;
      &lt;/div&gt; 
</code></pre>
<p>Screen</p>
<p><a href=""https://i.stack.imgur.com/Mc7pH.png"" rel=""nofollow noreferrer"">Screen</a></p>
",27,1,0,3,html;django;bootstrap-4,2022-06-30 17:25:20,2022-06-30 17:25:20,2022-06-30 17:49:51,code screen ,using bootstrap tab view in django,code screen,using bootstrap tab view django,using bootstrap tab view djangocode screen,"['using', 'bootstrap', 'tab', 'view', 'djangocode', 'screen']","['use', 'bootstrap', 'tab', 'view', 'djangocod', 'screen']"
187,200,200,1332991,72814901,How do machine learning algorithms speak to each other?,"<p>My understanding is that if a company creates a machine learning application, they use an API to ensure that applications talk to one another, which is what I found in my initial research (<a href=""https://www.ibm.com/cloud/learn/api"" rel=""nofollow noreferrer"">https://www.ibm.com/cloud/learn/api</a>). However, is this correct - is there an inner working within the machine learning application that enable two algorithms to connect? And how does this work in practice?</p>
",15,0,-1,2,api;machine-learning,2022-06-30 16:51:49,2022-06-30 16:51:49,2022-06-30 16:51:49,my understanding is that if a company creates a machine learning application  they use an api to ensure that applications talk to one another  which is what i found in my initial research     however  is this correct   is there an inner working within the machine learning application that enable two algorithms to connect  and how does this work in practice ,how do machine learning algorithms speak to each other ,understanding company creates machine learning application use api ensure applications talk one another found initial research however correct inner working within machine learning application enable two algorithms connect work practice,machine learning algorithms speak,machine learning algorithms speakunderstanding company creates machine learning application use api ensure applications talk one another found initial research however correct inner working within machine learning application enable two algorithms connect work practice,"['machine', 'learning', 'algorithms', 'speakunderstanding', 'company', 'creates', 'machine', 'learning', 'application', 'use', 'api', 'ensure', 'applications', 'talk', 'one', 'another', 'found', 'initial', 'research', 'however', 'correct', 'inner', 'working', 'within', 'machine', 'learning', 'application', 'enable', 'two', 'algorithms', 'connect', 'work', 'practice']","['machin', 'learn', 'algorithm', 'speakunderstand', 'compani', 'creat', 'machin', 'learn', 'applic', 'use', 'api', 'ensur', 'applic', 'talk', 'one', 'anoth', 'found', 'initi', 'research', 'howev', 'correct', 'inner', 'work', 'within', 'machin', 'learn', 'applic', 'enabl', 'two', 'algorithm', 'connect', 'work', 'practic']"
188,201,201,15682594,72805521,Getting an index error while training machine learning model for spam detection,"<p>I am creating a spam detection ML model using Naive Bayes from scratch and for that i need the likelihood of all classes(or P(feature|not spam)). For that, I've created a function:</p>
<pre><code>import numpy as np
def get_likelihood(term_document_matrix, label_index, smoothing=0):
    likelihood = {}
    for label, index in label_index.items():
        likelihood[label]=term_document_matrix[index, :].sum(axis=0) + smoothing
        likelihood[label]= np.asarray(likelihood[label])[0]
        total_count = likelihood[label].sum()
        likelihood[label] = likelihood[label]/float(total_count)
    return likelihood
</code></pre>
<p>And then I've implemented this function using the following call:</p>
<pre><code>smoothing = 1
likelihood = get_likelihood(term_docs,label_index,smoothing)
</code></pre>
<p>But I seem to be getting this error:</p>
<pre><code>IndexError                                Traceback (most recent call last)
c:\Users\SUKHMAN\Desktop\Spam detection project\spamdetection.ipynb Cell 9' in &lt;cell line: 2&gt;()
  1 smoothing=1
----&gt; 2 likelihood = get_likelihood(term_docs,label_index,smoothing)

c:\Users\SUKHMAN\Desktop\Spam detection project\spamdetection.ipynb Cell 8' in get_likelihood(term_document_matrix, label_index, smoothing)
  3 likelihood = {}
  4 for label, index in label_index.items():
----&gt; 5     likelihood[label]=term_document_matrix[index, :].sum(axis=0) + smoothing
  6     likelihood[label]= np.asarray(likelihood[label])[0]
  7     total_count = likelihood[label].sum()

File ~\AppData\Roaming\Python\Python310\site-packages\scipy\sparse\_index.py:47, in IndexMixin.__getitem__(self, key)
 46 def __getitem__(self, key):
---&gt; 47     row, col = self._validate_indices(key)
 49     # Dispatch to specialized methods.
 50     if isinstance(row, INT_TYPES):

File ~\AppData\Roaming\Python\Python310\site-packages\scipy\sparse\_index.py:159, in IndexMixin._validate_indices(self, key)
157         row += M
158 elif not isinstance(row, slice):
--&gt; 159     row = self._asindices(row, M)
161 if isintlike(col):
162     col = int(col)
...
--&gt; 191     raise IndexError('index (%d) out of range' % max_indx)
193 min_indx = x.min()
194 if min_indx &lt; 0:

IndexError: index (1499) out of range
</code></pre>
",27,1,0,5,python;machine-learning;deep-learning;jupyter-notebook;naivebayes,2022-06-29 22:53:21,2022-06-29 22:53:21,2022-06-30 16:46:55,i am creating a spam detection ml model using naive bayes from scratch and for that i need the likelihood of all classes or p feature not spam    for that  i ve created a function  and then i ve implemented this function using the following call  but i seem to be getting this error ,getting an index error while training machine learning model for spam detection,creating spam detection ml model using naive bayes scratch need likelihood classes p feature spam created function implemented function using following call seem getting error,getting index error training machine learning model spam detection,getting index error training machine learning model spam detectioncreating spam detection ml model using naive bayes scratch need likelihood classes p feature spam created function implemented function using following call seem getting error,"['getting', 'index', 'error', 'training', 'machine', 'learning', 'model', 'spam', 'detectioncreating', 'spam', 'detection', 'ml', 'model', 'using', 'naive', 'bayes', 'scratch', 'need', 'likelihood', 'classes', 'p', 'feature', 'spam', 'created', 'function', 'implemented', 'function', 'using', 'following', 'call', 'seem', 'getting', 'error']","['get', 'index', 'error', 'train', 'machin', 'learn', 'model', 'spam', 'detectioncr', 'spam', 'detect', 'ml', 'model', 'use', 'naiv', 'bay', 'scratch', 'need', 'likelihood', 'class', 'p', 'featur', 'spam', 'creat', 'function', 'implement', 'function', 'use', 'follow', 'call', 'seem', 'get', 'error']"
189,202,202,3976008,72559416,Swift - Remove image background with CoreML,"<p>I am using CoreML with the DeepLabV3 model to remove the background from an image: <a href=""https://developer.apple.com/machine-learning/models/"" rel=""nofollow noreferrer"">https://developer.apple.com/machine-learning/models/</a></p>
<p>This is working well for removing the background from photos where the subject it a person/dog/car, but for other cases, such as skylines and some objects on a table (please see example images), it is unable to detect the object from the background.</p>
<p>Should I be using a different method for this?</p>
<p>Thank you</p>
<pre><code>var imageSegmentationModel = DeepLabV3()
var request :  VNCoreMLRequest?

func setUpModel() {
        if let visionModel = try? VNCoreMLModel(for: imageSegmentationModel.model) {
                request = VNCoreMLRequest(model: visionModel, completionHandler: visionRequestDidComplete)
                request?.imageCropAndScaleOption = .scaleFill
        }
        else {
                fatalError()
        }
}

func predict() {
        DispatchQueue.global(qos: .userInitiated).async {
            guard let request = self.request else { fatalError() }
            let handler = VNImageRequestHandler(cgImage: (self.originalImage?.cgImage)!, options: [:])
            do {
                try handler.perform([request])
            }catch {
                print(error)
            }
        }
   }

func visionRequestDidComplete(request: VNRequest, error: Error?) {
        DispatchQueue.main.async {
            if let observations = request.results as? [VNCoreMLFeatureValueObservation],
                let segmentationmap = observations.first?.featureValue.multiArrayValue {
                
                self.maskImage = segmentationmap.image(min: 0, max: 255)
                print(self.maskImage!.size)
                
                self.maskImage = self.maskImage?.resizedImage(for: self.originalImage!.size)
                if let image:UIImage = self.maskOriginalImage(){
                    print(&quot;Success&quot;)
                    self.outputImageView.image = image
                }
            }
        }
            
    }

func maskOriginalImage() -&gt; UIImage? {
        if(self.maskImage != nil &amp;&amp; self.originalImage != nil){
            let maskReference = self.maskImage?.cgImage!
            let imageMask = CGImage(maskWidth: maskReference!.width,
                                    height: maskReference!.height,
                                    bitsPerComponent: maskReference!.bitsPerComponent,
                                    bitsPerPixel: maskReference!.bitsPerPixel,
                                    bytesPerRow: maskReference!.bytesPerRow,
                                    provider: maskReference!.dataProvider!, decode: nil, shouldInterpolate: true)
            
            let maskedReference = self.originalImage?.cgImage!.masking(imageMask!)
            return UIImage(cgImage: maskedReference!)
            
        }
        return nil
    }
</code></pre>
<p><a href=""https://i.stack.imgur.com/lSOLm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSOLm.jpg"" alt=""Good Image Example"" /></a></p>
<p><a href=""https://i.stack.imgur.com/e8VQ0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e8VQ0.jpg"" alt=""Bad Image Example"" /></a></p>
",97,1,1,5,ios;swift;coreml;apple-vision;deeplab,2022-06-09 16:51:15,2022-06-09 16:51:15,2022-06-30 15:39:32,i am using coreml with the deeplabv model to remove the background from an image   this is working well for removing the background from photos where the subject it a person dog car  but for other cases  such as skylines and some objects on a table  please see example images   it is unable to detect the object from the background  should i be using a different method for this  thank you  ,swift   remove image background with coreml,using coreml deeplabv model remove background image working well removing background photos subject person dog car cases skylines objects table please see example images unable detect object background using different method thank,swift remove image background coreml,swift remove image background coremlusing coreml deeplabv model remove background image working well removing background photos subject person dog car cases skylines objects table please see example images unable detect object background using different method thank,"['swift', 'remove', 'image', 'background', 'coremlusing', 'coreml', 'deeplabv', 'model', 'remove', 'background', 'image', 'working', 'well', 'removing', 'background', 'photos', 'subject', 'person', 'dog', 'car', 'cases', 'skylines', 'objects', 'table', 'please', 'see', 'example', 'images', 'unable', 'detect', 'object', 'background', 'using', 'different', 'method', 'thank']","['swift', 'remov', 'imag', 'background', 'coremlus', 'coreml', 'deeplabv', 'model', 'remov', 'background', 'imag', 'work', 'well', 'remov', 'background', 'photo', 'subject', 'person', 'dog', 'car', 'case', 'skylin', 'object', 'tabl', 'pleas', 'see', 'exampl', 'imag', 'unabl', 'detect', 'object', 'background', 'use', 'differ', 'method', 'thank']"
190,203,203,12051503,72471691,How to deploy multiple models to an endpoint using Azure Machine Learning CLI v2?,"<p>At the GA of <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/azure-machine-learning-release-notes-cli-v2#2022-05-24"" rel=""nofollow noreferrer"">az ml cli v2</a>, we've been working on some POC using <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-deployment-managed-online"" rel=""nofollow noreferrer"">yml online deployment</a> on top of managed endpoint and it all went well for single model, until when there's certain scenario where there is requirement to deploy multiple trained and registered models to one managed endpoint, it seems there is no documentations on how to achieve that.</p>
<p>Previously using Python SDK, it was able to deploy list of models to AKS cluster.</p>
<p>Checking if there's any limitation or could be some docs I might have missed?</p>
",78,1,0,2,azure-cli;azure-machine-learning-service,2022-06-02 12:04:23,2022-06-02 12:04:23,2022-06-30 14:52:33,at the ga of   we ve been working on some poc using  on top of managed endpoint and it all went well for single model  until when there s certain scenario where there is requirement to deploy multiple trained and registered models to one managed endpoint  it seems there is no documentations on how to achieve that  previously using python sdk  it was able to deploy list of models to aks cluster  checking if there s any limitation or could be some docs i might have missed ,how to deploy multiple models to an endpoint using azure machine learning cli v ,ga working poc using top managed endpoint went well single model certain scenario requirement deploy multiple trained registered models one managed endpoint seems documentations achieve previously using python sdk able deploy models aks cluster checking limitation could docs might missed,deploy multiple models endpoint using azure machine learning cli v,deploy multiple models endpoint using azure machine learning cli vga working poc using top managed endpoint went well single model certain scenario requirement deploy multiple trained registered models one managed endpoint seems documentations achieve previously using python sdk able deploy models aks cluster checking limitation could docs might missed,"['deploy', 'multiple', 'models', 'endpoint', 'using', 'azure', 'machine', 'learning', 'cli', 'vga', 'working', 'poc', 'using', 'top', 'managed', 'endpoint', 'went', 'well', 'single', 'model', 'certain', 'scenario', 'requirement', 'deploy', 'multiple', 'trained', 'registered', 'models', 'one', 'managed', 'endpoint', 'seems', 'documentations', 'achieve', 'previously', 'using', 'python', 'sdk', 'able', 'deploy', 'models', 'aks', 'cluster', 'checking', 'limitation', 'could', 'docs', 'might', 'missed']","['deploy', 'multipl', 'model', 'endpoint', 'use', 'azur', 'machin', 'learn', 'cli', 'vga', 'work', 'poc', 'use', 'top', 'manag', 'endpoint', 'went', 'well', 'singl', 'model', 'certain', 'scenario', 'requir', 'deploy', 'multipl', 'train', 'regist', 'model', 'one', 'manag', 'endpoint', 'seem', 'document', 'achiev', 'previous', 'use', 'python', 'sdk', 'abl', 'deploy', 'model', 'ak', 'cluster', 'check', 'limit', 'could', 'doc', 'might', 'miss']"
191,204,204,10883094,72804704,Reduce fastText memory usage for big models,"<p>I trained a machine learning sentence classification model that uses, among other features, also the vectors obtained from a pretrained fastText model (like <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">these</a>) which is 7Gb.  I use the pretrained fastText Italian model: I am using this word embedding only to get some semantic features to feed into the effective ML model.</p>
<p>I built a simple API based on fastText that, at prediction time, computes the vectors needed by the effective ML model. Under the hood, this API receives a string as input and calls <code>get_sentence_vector</code>. When the API starts, it loads the fastText model into memory.</p>
<p><strong>How can I reduce the memory footprint of fastText, which is loaded into RAM?</strong></p>
<p>Constraints:</p>
<ul>
<li>My model works fine, training was time-consuming and expensive, so I wouldn't want to retrain it using smaller vectors</li>
<li>I need the fastText ability to handle out-of-vocabulary words, so I can't use just vectors but I need the full model</li>
<li>I should reduce the RAM usage, even at the expense of a reduction in speed.</li>
</ul>
<p>At the moment, I'm starting to experiment with <a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext</a>...</p>
<p><strong>Please share your suggestions and thoughts even if they do not represent full-fledged solutions.</strong></p>
",26,0,0,5,python;machine-learning;optimization;nlp;fasttext,2022-06-29 21:44:26,2022-06-29 21:44:26,2022-06-30 14:12:26,i trained a machine learning sentence classification model that uses  among other features  also the vectors obtained from a pretrained fasttext model  like   which is gb   i use the pretrained fasttext italian model  i am using this word embedding only to get some semantic features to feed into the effective ml model  i built a simple api based on fasttext that  at prediction time  computes the vectors needed by the effective ml model  under the hood  this api receives a string as input and calls get_sentence_vector  when the api starts  it loads the fasttext model into memory  how can i reduce the memory footprint of fasttext  which is loaded into ram  constraints  at the moment  i m starting to experiment with     please share your suggestions and thoughts even if they do not represent full fledged solutions ,reduce fasttext memory usage for big models,trained machine learning sentence classification model uses among features also vectors obtained pretrained fasttext model like gb use pretrained fasttext italian model using word embedding get semantic features feed effective ml model built simple api based fasttext prediction time computes vectors needed effective ml model hood api receives string input calls get_sentence_vector api starts loads fasttext model memory reduce memory footprint fasttext loaded ram constraints moment starting experiment please share suggestions thoughts even represent full fledged solutions,reduce fasttext memory usage big models,reduce fasttext memory usage big modelstrained machine learning sentence classification model uses among features also vectors obtained pretrained fasttext model like gb use pretrained fasttext italian model using word embedding get semantic features feed effective ml model built simple api based fasttext prediction time computes vectors needed effective ml model hood api receives string input calls get_sentence_vector api starts loads fasttext model memory reduce memory footprint fasttext loaded ram constraints moment starting experiment please share suggestions thoughts even represent full fledged solutions,"['reduce', 'fasttext', 'memory', 'usage', 'big', 'modelstrained', 'machine', 'learning', 'sentence', 'classification', 'model', 'uses', 'among', 'features', 'also', 'vectors', 'obtained', 'pretrained', 'fasttext', 'model', 'like', 'gb', 'use', 'pretrained', 'fasttext', 'italian', 'model', 'using', 'word', 'embedding', 'get', 'semantic', 'features', 'feed', 'effective', 'ml', 'model', 'built', 'simple', 'api', 'based', 'fasttext', 'prediction', 'time', 'computes', 'vectors', 'needed', 'effective', 'ml', 'model', 'hood', 'api', 'receives', 'string', 'input', 'calls', 'get_sentence_vector', 'api', 'starts', 'loads', 'fasttext', 'model', 'memory', 'reduce', 'memory', 'footprint', 'fasttext', 'loaded', 'ram', 'constraints', 'moment', 'starting', 'experiment', 'please', 'share', 'suggestions', 'thoughts', 'even', 'represent', 'full', 'fledged', 'solutions']","['reduc', 'fasttext', 'memori', 'usag', 'big', 'modelstrain', 'machin', 'learn', 'sentenc', 'classif', 'model', 'use', 'among', 'featur', 'also', 'vector', 'obtain', 'pretrain', 'fasttext', 'model', 'like', 'gb', 'use', 'pretrain', 'fasttext', 'italian', 'model', 'use', 'word', 'embed', 'get', 'semant', 'featur', 'feed', 'effect', 'ml', 'model', 'built', 'simpl', 'api', 'base', 'fasttext', 'predict', 'time', 'comput', 'vector', 'need', 'effect', 'ml', 'model', 'hood', 'api', 'receiv', 'string', 'input', 'call', 'get_sentence_vector', 'api', 'start', 'load', 'fasttext', 'model', 'memori', 'reduc', 'memori', 'footprint', 'fasttext', 'load', 'ram', 'constraint', 'moment', 'start', 'experi', 'pleas', 'share', 'suggest', 'thought', 'even', 'repres', 'full', 'fledg', 'solut']"
192,205,205,19436733,72809575,How to include the weighting factors of a survey when doing machine learning?,"<p>I am applying a Lasso regression for an analysis in machine learning. But I have to consider the weighting factors of the survey that I am using.</p>
<p>How to include the weighting factors of a survey when doing machine learning? where in the code is this entered? I would appreciate if you have any python code examples on this.
Thank you very much for the advice,</p>
",24,0,-1,2,python;machine-learning,2022-06-30 07:33:51,2022-06-30 07:33:51,2022-06-30 07:33:51,i am applying a lasso regression for an analysis in machine learning  but i have to consider the weighting factors of the survey that i am using ,how to include the weighting factors of a survey when doing machine learning ,applying lasso regression analysis machine learning consider weighting factors survey using,include weighting factors survey machine learning,include weighting factors survey machine learningapplying lasso regression analysis machine learning consider weighting factors survey using,"['include', 'weighting', 'factors', 'survey', 'machine', 'learningapplying', 'lasso', 'regression', 'analysis', 'machine', 'learning', 'consider', 'weighting', 'factors', 'survey', 'using']","['includ', 'weight', 'factor', 'survey', 'machin', 'learningappli', 'lasso', 'regress', 'analysi', 'machin', 'learn', 'consid', 'weight', 'factor', 'survey', 'use']"
193,206,206,8833267,72809158,"AttributeError: transform not found in Countvectorizer , using sklearn","<p>Trying to load my spam text detection Machine Learning model in API using Django framework.</p>
<p>My error when sending request with text:&quot;Ahmed,,, Awwad.. Test&quot;</p>
<pre><code>File &quot;path/to/API.py&quot;, line 39, in predict_text
    vector = selector.transform(clean_text)
  File &quot;/usr/lib/python3/dist-packages/scipy/sparse/base.py&quot;, line 687, in __getattr__
    raise AttributeError(attr + &quot; not found&quot;)
AttributeError: transform not found
[30/Jun/2022 00:11:23] &quot;POST /api/spamurai HTTP/1.1&quot; 500 103644
</code></pre>
<p>This my model:</p>
<pre><code>#convert a collection of text to amatrix of tokens (Bag of words)
from sklearn.feature_extraction.text import CountVectorizer
from joblib import dump
import pickle
#process_text previously defined function to preprocess text in model
msg_vector= CountVectorizer(analyzer=process_text)
message_bow = msg_vector.fit_transform(df['text'])
pickle.dump(message_bow, open(&quot;vector_text.pkl&quot;, &quot;wb&quot;))
dump(MultinomialNB,'spam_model_text.joblib')
</code></pre>
<blockquote>
<p>I was using joblib in both dumping but i got many errors with countervectorizer and joblib so i am trying pickle and joblib</p>
</blockquote>
<p>In my API.py file I load my text vector and MultinomialNB file as follows:</p>
<pre><code>from joblib import load
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer

MultinomialNB_file = os.path.join(BASE_DIR, 'tutorials/data', 'spam_model_text.joblib')
CountVectorize_file = os.path.join(BASE_DIR, 'tutorials/data','vector_text.pkl')

txt_model = load(MultinomialNB_file)
selector = pickle.load(open(CountVectorize_file, &quot;rb&quot;))

# # pre processing email text to be used in ML Model without affecting accuracy of model
def process_text(text):
     #processing_code
      return clean_text

# # function uses trainig utilities used in TEXT ML Model
def predict_text(text):
# call pre processing email text in here
    # text is my input 
    # text:  Ahmed,,, Awwad.. Test
    clean_text = process_text(text)
    # clean_TXT: ['Ahmed', 'Awwad', 'Test'] 
    #clean_txt is my input after preprocessing
    vector = selector.transform(clean_text)
    result = txt_model.predict(vector)
</code></pre>
<p>My thoughts may help : 'vector_text.pkl' is not probably readable as countvectorizer dumped file</p>
",35,0,0,5,python;python-3.x;django;machine-learning;scikit-learn,2022-06-30 05:56:41,2022-06-30 05:56:41,2022-06-30 05:56:41,trying to load my spam text detection machine learning model in api using django framework  my error when sending request with text  ahmed    awwad   test  this my model  i was using joblib in both dumping but i got many errors with countervectorizer and joblib so i am trying pickle and joblib in my api py file i load my text vector and multinomialnb file as follows  my thoughts may help    vector_text pkl  is not probably readable as countvectorizer dumped file,attributeerror  transform not found in countvectorizer   using sklearn,trying load spam text detection machine learning model api using django framework error sending request text ahmed awwad test model using joblib dumping got many errors countervectorizer joblib trying pickle joblib api py file load text vector multinomialnb file follows thoughts may help vector_text pkl probably readable countvectorizer dumped file,attributeerror transform found countvectorizer using sklearn,attributeerror transform found countvectorizer using sklearntrying load spam text detection machine learning model api using django framework error sending request text ahmed awwad test model using joblib dumping got many errors countervectorizer joblib trying pickle joblib api py file load text vector multinomialnb file follows thoughts may help vector_text pkl probably readable countvectorizer dumped file,"['attributeerror', 'transform', 'found', 'countvectorizer', 'using', 'sklearntrying', 'load', 'spam', 'text', 'detection', 'machine', 'learning', 'model', 'api', 'using', 'django', 'framework', 'error', 'sending', 'request', 'text', 'ahmed', 'awwad', 'test', 'model', 'using', 'joblib', 'dumping', 'got', 'many', 'errors', 'countervectorizer', 'joblib', 'trying', 'pickle', 'joblib', 'api', 'py', 'file', 'load', 'text', 'vector', 'multinomialnb', 'file', 'follows', 'thoughts', 'may', 'help', 'vector_text', 'pkl', 'probably', 'readable', 'countvectorizer', 'dumped', 'file']","['attributeerror', 'transform', 'found', 'countvector', 'use', 'sklearntri', 'load', 'spam', 'text', 'detect', 'machin', 'learn', 'model', 'api', 'use', 'django', 'framework', 'error', 'send', 'request', 'text', 'ahm', 'awwad', 'test', 'model', 'use', 'joblib', 'dump', 'got', 'mani', 'error', 'countervector', 'joblib', 'tri', 'pickl', 'joblib', 'api', 'py', 'file', 'load', 'text', 'vector', 'multinomialnb', 'file', 'follow', 'thought', 'may', 'help', 'vector_text', 'pkl', 'probabl', 'readabl', 'countvector', 'dump', 'file']"
194,207,207,18906734,72807512,Machine Learning - How to compare two models?,"<p>I am working on loan eligibility supervised project. I used logistic regression and a support vector machine as two models. For the logistic regression, accuracy is 0.84 and the loss function is 0.45. For SVM accuracy is 0.80 and loss function is 0.33. Which of these algorithms performs better? How do we compare two models (by the accuracy or by loss function)?</p>
",25,1,0,3,machine-learning;svm;logistic-regression,2022-06-30 02:01:49,2022-06-30 02:01:49,2022-06-30 03:34:43,i am working on loan eligibility supervised project  i used logistic regression and a support vector machine as two models  for the logistic regression  accuracy is   and the loss function is    for svm accuracy is   and loss function is    which of these algorithms performs better  how do we compare two models  by the accuracy or by loss function  ,machine learning   how to compare two models ,working loan eligibility supervised project used logistic regression support vector machine two models logistic regression accuracy loss function svm accuracy loss function algorithms performs better compare two models accuracy loss function,machine learning compare two models,machine learning compare two modelsworking loan eligibility supervised project used logistic regression support vector machine two models logistic regression accuracy loss function svm accuracy loss function algorithms performs better compare two models accuracy loss function,"['machine', 'learning', 'compare', 'two', 'modelsworking', 'loan', 'eligibility', 'supervised', 'project', 'used', 'logistic', 'regression', 'support', 'vector', 'machine', 'two', 'models', 'logistic', 'regression', 'accuracy', 'loss', 'function', 'svm', 'accuracy', 'loss', 'function', 'algorithms', 'performs', 'better', 'compare', 'two', 'models', 'accuracy', 'loss', 'function']","['machin', 'learn', 'compar', 'two', 'modelswork', 'loan', 'elig', 'supervis', 'project', 'use', 'logist', 'regress', 'support', 'vector', 'machin', 'two', 'model', 'logist', 'regress', 'accuraci', 'loss', 'function', 'svm', 'accuraci', 'loss', 'function', 'algorithm', 'perform', 'better', 'compar', 'two', 'model', 'accuraci', 'loss', 'function']"
195,208,208,1999833,72797783,Machine Learning to predict time-series multi-class signal changes,"<p>I would like to predict the switching behavior of time-dependent signals. Currently the signal has 3 states (1, 2, 3), but it could be that this will change in the future. For the moment, however, it is absolutely okay to assume three states.</p>
<p>I can make the following assumptions about these states (see picture):</p>
<ol>
<li>the signals repeat periodically, possibly with variations concerning the time of day.</li>
<li>the duration of state 2 is always constant and relatively short for all signals.</li>
<li>the duration of states 1 and 3 are also constant, but vary for the different signals.</li>
<li>the switching sequence is always the same: 1 --&gt; 2 --&gt; 3 --&gt; 2 --&gt; 1 --&gt; [...]</li>
<li>there is a constant but unknown time reference between the different signals.</li>
<li>There is no constant time reference between my observations for the different signals. They are simply measured one after the other, but always at different times.</li>
<li>I am able to rebuild my model periodically after i obtained more samples.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/0EMVo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0EMVo.png"" alt=""Sketch of signal model and observations"" /></a></p>
<p>I have the following problems:</p>
<ol>
<li>I can only observe one signal at a time.</li>
<li>I can only observe the signals at different times.</li>
<li>I cannot trigger my measurement with the state transition. That means, when I measure, I am always &quot;in the middle&quot; of a state. Therefore I don't know when this state has started and also not exactly when this state will end.</li>
<li>I cannot observe a certain signal for a long duration. So, i am not able to observe a complete period.</li>
<li>My samples (observations) are widespread in time.</li>
<li>I would like to get a prediction either for the state change or the current state for the current time. It is likely to happen that i will never have measured my signals for that requested time.</li>
</ol>
<p>So far I have tested the TimeSeriesPredictor from the ML.NET Toolbox, as it seemed suitable to me. However, in my opinion, this algorithm requires that you always pass only the data of one signal. This means that assumption 5 is not included in the prediction, which is probably suboptimal. Also, in this case I had problems with the prediction not changing, which should actually happen time-dependently when I query multiple predictions. This behavior led me to believe that only the order of the values entered the model, but not the associated timestamp. If I have understood everything correctly, then exactly this timestamp is my most important &quot;feature&quot;...
So far, i did not do any tests on Regression-based approaches, e.g. FastTree, since my data is not linear, but keeps changing states. Maybe this assumption is not valid and regression-based methods could also be suitable?</p>
<p>I also don't know if a multiclassifier is required, because I had understood that the TimeSeriesPredictor would also be suitable for this, since it works with the single data type. Whether the prediction is 1.3 or exactly 1.0 would be fine for me.</p>
<p><strong>To sum it up:</strong>
I am looking for a algorithm which is able to recognize the switching patterns based on lose and widespread samples. It would be okay to define boundaries, e.g. state duration 3 of signal 1 will never last longer than 30s or state duration 1 of signal 3 will never last longer 60s.
Then, after the algorithm has obtained an approximate model of the switching behaviour, i would like to request a prediction of a certain signal state for a certain time.</p>
<p>Which methods can I use to get the best prediction, preferably using the ML.NET toolbox or based on matlab?</p>
",49,1,-1,5,matlab;machine-learning;time-series;prediction;ml.net,2022-06-29 13:29:52,2022-06-29 13:29:52,2022-06-30 01:32:48,i would like to predict the switching behavior of time dependent signals  currently the signal has  states         but it could be that this will change in the future  for the moment  however  it is absolutely okay to assume three states  i can make the following assumptions about these states  see picture    i have the following problems  i also don t know if a multiclassifier is required  because i had understood that the timeseriespredictor would also be suitable for this  since it works with the single data type  whether the prediction is   or exactly   would be fine for me  which methods can i use to get the best prediction  preferably using the ml net toolbox or based on matlab ,machine learning to predict time series multi class signal changes,would like predict switching behavior time dependent signals currently signal states could change future moment however absolutely okay assume three states make following assumptions states see picture following problems also know multiclassifier required understood timeseriespredictor would also suitable since works single data type whether prediction exactly would fine methods use get best prediction preferably using ml net toolbox based matlab,machine learning predict time series multi class signal changes,machine learning predict time series multi class signal changeswould like predict switching behavior time dependent signals currently signal states could change future moment however absolutely okay assume three states make following assumptions states see picture following problems also know multiclassifier required understood timeseriespredictor would also suitable since works single data type whether prediction exactly would fine methods use get best prediction preferably using ml net toolbox based matlab,"['machine', 'learning', 'predict', 'time', 'series', 'multi', 'class', 'signal', 'changeswould', 'like', 'predict', 'switching', 'behavior', 'time', 'dependent', 'signals', 'currently', 'signal', 'states', 'could', 'change', 'future', 'moment', 'however', 'absolutely', 'okay', 'assume', 'three', 'states', 'make', 'following', 'assumptions', 'states', 'see', 'picture', 'following', 'problems', 'also', 'know', 'multiclassifier', 'required', 'understood', 'timeseriespredictor', 'would', 'also', 'suitable', 'since', 'works', 'single', 'data', 'type', 'whether', 'prediction', 'exactly', 'would', 'fine', 'methods', 'use', 'get', 'best', 'prediction', 'preferably', 'using', 'ml', 'net', 'toolbox', 'based', 'matlab']","['machin', 'learn', 'predict', 'time', 'seri', 'multi', 'class', 'signal', 'changeswould', 'like', 'predict', 'switch', 'behavior', 'time', 'depend', 'signal', 'current', 'signal', 'state', 'could', 'chang', 'futur', 'moment', 'howev', 'absolut', 'okay', 'assum', 'three', 'state', 'make', 'follow', 'assumpt', 'state', 'see', 'pictur', 'follow', 'problem', 'also', 'know', 'multiclassifi', 'requir', 'understood', 'timeseriespredictor', 'would', 'also', 'suitabl', 'sinc', 'work', 'singl', 'data', 'type', 'whether', 'predict', 'exactli', 'would', 'fine', 'method', 'use', 'get', 'best', 'predict', 'prefer', 'use', 'ml', 'net', 'toolbox', 'base', 'matlab']"
196,209,209,2068931,62359175,Pytorch says that CUDA is not available,"<p>I'm trying to run Pytorch on a laptop that I have. It's an older model but it does have an Nvidia graphics card. I realize it is probably not going to be sufficient for real machine learning but I am trying to do it so I can learn the process of getting CUDA installed.</p>

<p>I have followed the steps on the <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html"" rel=""noreferrer"">installation guide</a> for Ubuntu 18.04 (my specific distribution is Xubuntu).</p>

<p>My graphics card is a GeForce 845M, verified by <code>lspci | grep nvidia</code>:</p>

<pre><code>01:00.0 3D controller: NVIDIA Corporation GM107M [GeForce 845M] (rev a2)
01:00.1 Audio device: NVIDIA Corporation Device 0fbc (rev a1)
</code></pre>

<p>I also have gcc 7.5 installed, verified by <code>gcc --version</code></p>

<pre><code>gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>

<p>And I have the correct headers installed, verified by trying to install them with <code>sudo apt-get install linux-headers-$(uname -r)</code>:</p>

<pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
linux-headers-4.15.0-106-generic is already the newest version (4.15.0-106.107).
</code></pre>

<p>I then followed the installation instructions using a local .deb for version 10.1.</p>

<p>Npw, when I run <code>nvidia-smi</code>, I get:</p>

<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce 845M        On   | 00000000:01:00.0 Off |                  N/A |
| N/A   40C    P0    N/A /  N/A |     88MiB /  2004MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       982      G   /usr/lib/xorg/Xorg                            87MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<p>and I run <code>nvcc -V</code> I get:</p>

<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>

<p>I then performed the post-installation instructions from <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#mandatory-post"" rel=""noreferrer"">section 6.1</a>, and so as a result, <code>echo $PATH</code> looks like this:</p>

<pre><code>/home/isaek/anaconda3/envs/stylegan2_pytorch/bin:/home/isaek/anaconda3/bin:/home/isaek/anaconda3/condabin:/usr/local/cuda-10.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
</code></pre>

<p><code>echo $LD_LIBRARY_PATH</code> looks like this:</p>

<pre><code>/usr/local/cuda-10.1/lib64
</code></pre>

<p>and my <code>/etc/udev/rules.d/40-vm-hotadd.rules</code> file looks like this:</p>

<pre><code># On Hyper-V and Xen Virtual Machines we want to add memory and cpus as soon as they appear
ATTR{[dmi/id]sys_vendor}==""Microsoft Corporation"", ATTR{[dmi/id]product_name}==""Virtual Machine"", GOTO=""vm_hotadd_apply""
ATTR{[dmi/id]sys_vendor}==""Xen"", GOTO=""vm_hotadd_apply""
GOTO=""vm_hotadd_end""

LABEL=""vm_hotadd_apply""

# Memory hotadd request

# CPU hotadd request
SUBSYSTEM==""cpu"", ACTION==""add"", DEVPATH==""/devices/system/cpu/cpu[0-9]*"", TEST==""online"", ATTR{online}=""1""

LABEL=""vm_hotadd_end""
</code></pre>

<p>After all of this, I even compiled and ran the samples. <code>./deviceQuery</code> returns:</p>

<pre><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce 845M""
  CUDA Driver Version / Runtime Version          10.1 / 10.1
  CUDA Capability Major/Minor version number:    5.0
  Total amount of global memory:                 2004 MBytes (2101870592 bytes)
  ( 4) Multiprocessors, (128) CUDA Cores/MP:     512 CUDA Cores
  GPU Max Clock rate:                            863 MHz (0.86 GHz)
  Memory Clock rate:                             1001 Mhz
  Memory Bus Width:                              64-bit
  L2 Cache Size:                                 1048576 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            No
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1
Result = PASS
</code></pre>

<p>and <code>./bandwidthTest</code> returns:</p>

<pre><code>[CUDA Bandwidth Test] - Starting...
Running on...

 Device 0: GeForce 845M
 Quick Mode

 Host to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         11.7

 Device to Host Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         11.8

 Device to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         14.5

Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre>

<p>But after all of this, this Python snippet (in a conda environment with all dependencies installed):</p>

<pre><code>import torch
torch.cuda.is_available()
</code></pre>

<p>returns <code>False</code></p>

<p>Does anybody have any idea about how to resolve this? I've tried to add <code>/usr/local/cuda-10.1/bin</code> to <code>etc/environment</code> like this:</p>

<pre><code>PATH=""/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games""
PATH=$PATH:/usr/local/cuda-10.1/bin
</code></pre>

<p>And restarting the terminal, but that didn't fix it. I really don't know what else to try.</p>

<h1>EDIT - Results of collect_env for @kHarshit</h1>

<pre><code>Collecting environment information...
PyTorch version: 1.5.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce 845M
Nvidia driver version: 418.87.00
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.18.5
[pip] pytorch-ranger==0.1.1
[pip] stylegan2-pytorch==0.12.0
[pip] torch==1.5.0
[pip] torch-optimizer==0.0.1a12
[pip] torchvision==0.6.0
[pip] vector-quantize-pytorch==0.0.2
[conda] numpy                     1.18.5                   pypi_0    pypi
[conda] pytorch-ranger            0.1.1                    pypi_0    pypi
[conda] stylegan2-pytorch         0.12.0                   pypi_0    pypi
[conda] torch                     1.5.0                    pypi_0    pypi
[conda] torch-optimizer           0.0.1a12                 pypi_0    pypi
[conda] torchvision               0.6.0                    pypi_0    pypi
[conda] vector-quantize-pytorch   0.0.2                    pypi_0    pypi
</code></pre>
",114230,4,45,3,linux;pytorch;ubuntu-18.04,2020-06-13 17:06:19,2020-06-13 17:06:19,2022-06-30 01:25:16,i m trying to run pytorch on a laptop that i have  it s an older model but it does have an nvidia graphics card  i realize it is probably not going to be sufficient for real machine learning but i am trying to do it so i can learn the process of getting cuda installed  i have followed the steps on the  for ubuntu    my specific distribution is xubuntu   my graphics card is a geforce m  verified by lspci   grep nvidia  i also have gcc   installed  verified by gcc   version and i have the correct headers installed  verified by trying to install them with sudo apt get install linux headers   uname  r   i then followed the installation instructions using a local  deb for version    npw  when i run nvidia smi  i get  and i run nvcc  v i get  i then performed the post installation instructions from   and so as a result  echo  path looks like this  echo  ld_library_path looks like this  and my  etc udev rules d  vm hotadd rules file looks like this  after all of this  i even compiled and ran the samples    devicequery returns  and   bandwidthtest returns  but after all of this  this python snippet  in a conda environment with all dependencies installed   returns false does anybody have any idea about how to resolve this  i ve tried to add  usr local cuda   bin to etc environment like this  and restarting the terminal  but that didn t fix it  i really don t know what else to try ,pytorch says that cuda is not available,trying run pytorch laptop older model nvidia graphics card realize probably going sufficient real machine learning trying learn process getting cuda installed followed steps ubuntu specific distribution xubuntu graphics card geforce verified lspci grep nvidia also gcc installed verified gcc version correct headers installed verified trying install sudo apt get install linux headers uname r followed installation instructions using local deb version npw run nvidia smi get run nvcc v get performed post installation instructions result echo path looks like echo ld_library_path looks like etc udev rules vm hotadd rules file looks like even compiled ran samples devicequery returns bandwidthtest returns python snippet conda environment dependencies installed returns false anybody idea resolve tried usr local cuda bin etc environment like restarting terminal fix really know else try,pytorch says cuda available,pytorch says cuda availabletrying run pytorch laptop older model nvidia graphics card realize probably going sufficient real machine learning trying learn process getting cuda installed followed steps ubuntu specific distribution xubuntu graphics card geforce verified lspci grep nvidia also gcc installed verified gcc version correct headers installed verified trying install sudo apt get install linux headers uname r followed installation instructions using local deb version npw run nvidia smi get run nvcc v get performed post installation instructions result echo path looks like echo ld_library_path looks like etc udev rules vm hotadd rules file looks like even compiled ran samples devicequery returns bandwidthtest returns python snippet conda environment dependencies installed returns false anybody idea resolve tried usr local cuda bin etc environment like restarting terminal fix really know else try,"['pytorch', 'says', 'cuda', 'availabletrying', 'run', 'pytorch', 'laptop', 'older', 'model', 'nvidia', 'graphics', 'card', 'realize', 'probably', 'going', 'sufficient', 'real', 'machine', 'learning', 'trying', 'learn', 'process', 'getting', 'cuda', 'installed', 'followed', 'steps', 'ubuntu', 'specific', 'distribution', 'xubuntu', 'graphics', 'card', 'geforce', 'verified', 'lspci', 'grep', 'nvidia', 'also', 'gcc', 'installed', 'verified', 'gcc', 'version', 'correct', 'headers', 'installed', 'verified', 'trying', 'install', 'sudo', 'apt', 'get', 'install', 'linux', 'headers', 'uname', 'r', 'followed', 'installation', 'instructions', 'using', 'local', 'deb', 'version', 'npw', 'run', 'nvidia', 'smi', 'get', 'run', 'nvcc', 'v', 'get', 'performed', 'post', 'installation', 'instructions', 'result', 'echo', 'path', 'looks', 'like', 'echo', 'ld_library_path', 'looks', 'like', 'etc', 'udev', 'rules', 'vm', 'hotadd', 'rules', 'file', 'looks', 'like', 'even', 'compiled', 'ran', 'samples', 'devicequery', 'returns', 'bandwidthtest', 'returns', 'python', 'snippet', 'conda', 'environment', 'dependencies', 'installed', 'returns', 'false', 'anybody', 'idea', 'resolve', 'tried', 'usr', 'local', 'cuda', 'bin', 'etc', 'environment', 'like', 'restarting', 'terminal', 'fix', 'really', 'know', 'else', 'try']","['pytorch', 'say', 'cuda', 'availabletri', 'run', 'pytorch', 'laptop', 'older', 'model', 'nvidia', 'graphic', 'card', 'realiz', 'probabl', 'go', 'suffici', 'real', 'machin', 'learn', 'tri', 'learn', 'process', 'get', 'cuda', 'instal', 'follow', 'step', 'ubuntu', 'specif', 'distribut', 'xubuntu', 'graphic', 'card', 'geforc', 'verifi', 'lspci', 'grep', 'nvidia', 'also', 'gcc', 'instal', 'verifi', 'gcc', 'version', 'correct', 'header', 'instal', 'verifi', 'tri', 'instal', 'sudo', 'apt', 'get', 'instal', 'linux', 'header', 'unam', 'r', 'follow', 'instal', 'instruct', 'use', 'local', 'deb', 'version', 'npw', 'run', 'nvidia', 'smi', 'get', 'run', 'nvcc', 'v', 'get', 'perform', 'post', 'instal', 'instruct', 'result', 'echo', 'path', 'look', 'like', 'echo', 'ld_library_path', 'look', 'like', 'etc', 'udev', 'rule', 'vm', 'hotadd', 'rule', 'file', 'look', 'like', 'even', 'compil', 'ran', 'sampl', 'devicequeri', 'return', 'bandwidthtest', 'return', 'python', 'snippet', 'conda', 'environ', 'depend', 'instal', 'return', 'fals', 'anybodi', 'idea', 'resolv', 'tri', 'usr', 'local', 'cuda', 'bin', 'etc', 'environ', 'like', 'restart', 'termin', 'fix', 'realli', 'know', 'els', 'tri']"
197,210,210,15445155,72806504,Multi-input Model,"<p>I'm trying to implement a machine learning model (variational autoencoder) where the architecture of the model is</p>
<pre><code>        start_filter_num = 32
        kernel_size = 3
        latent_dim = 16

        x = tf.keras.Input(shape=(width,1))

        conv_seq1 = conv_block_seq_res(x, start_filter_num, kernel_size, 1, &quot;conv_seq1&quot;)
        pool1 = tf.keras.layers.MaxPooling1D(name=&quot;pool1&quot;)(conv_seq1)

        conv_seq2 = conv_block_seq_res(pool1, int(start_filter_num*1.5), kernel_size, 1, &quot;conv_seq2&quot;)
        pool2 = tf.keras.layers.MaxPooling1D(name=&quot;pool2&quot;)(conv_seq2)

        conv_seq3 = conv_block_seq_res(pool2, start_filter_num*3, kernel_size, 1, &quot;conv_seq3&quot;)
        pool3 = tf.keras.layers.MaxPooling1D(name=&quot;pool3&quot;)(conv_seq3)

        conv_seq4 = conv_block_seq_res(pool3, int(start_filter_num*4.5), kernel_size, 1, &quot;conv_seq4&quot;)
        pool4 = tf.keras.layers.MaxPooling1D(name=&quot;pool4&quot;)(conv_seq4)

        conv_seq5 = conv_block_seq_res(pool4, start_filter_num*6, kernel_size, 1, &quot;conv_seq5&quot;)
        pool5 = tf.keras.layers.MaxPooling1D(name=&quot;pool5&quot;)(conv_seq5)

        conv_seq6 = conv_block_seq_res(pool5, int(start_filter_num*7.5), kernel_size, 1, &quot;conv_seq6&quot;, In=False)
        pool6 = tf.keras.layers.MaxPooling1D(name=&quot;pool6&quot;)(conv_seq6)

        conv_seq7 = conv_block_seq_res(pool6, start_filter_num*9, kernel_size, 1, &quot;conv_seq7&quot;, In=False)

        flatten1 = tf.keras.layers.Flatten()(conv_seq7)

        z_mu = tf.keras.layers.Dense(latent_dim, name=&quot;z_mu&quot;)(flatten1)
        z_log_var = tf.keras.layers.Dense(latent_dim, name=&quot;z_log_var&quot;)(flatten1)

        ###############################################################################
        # normalize log variance to std dev
        z_sigma = tf.keras.layers.Lambda(lambda t: K.exp(.5*t), name=&quot;z_sigma&quot;)(z_log_var)
        eps = tf.keras.Input(tensor=K.random_normal(shape=(K.shape(x)[0], latent_dim)), name=&quot;eps&quot;)

        z_eps = tf.keras.layers.Multiply(name=&quot;z_eps&quot;)([z_sigma, eps])
        z = tf.keras.layers.Add(name=&quot;z&quot;)([z_mu, z_eps])

        #latent_conv = tf.keras.layers.Dense(width//64, name=&quot;latent_conv&quot;)(z)
        reshape1 = tf.keras.layers.Reshape([width//64,1], name=&quot;reshape1&quot;)(z)

        ###############################################################################
        #New for conditional VAE
        dconv_seq4 = conv_block_seq_res(reshape1, start_filter_num*9, kernel_size, 1, &quot;dconv_seq4&quot;, In=False)
        dconc5 = tf.keras.layers.concatenate([dconv_seq4, conv_seq7], name=&quot;dconc5&quot;)
        deconv1 = Conv1DTranspose(dconc5, start_filter_num*2, kernel_size=3, strides=2, padding='same')

        dconv_seq5 = conv_block_seq_res(deconv1, int(start_filter_num*7.5), kernel_size, 1, &quot;dconv_seq5&quot;, In=False)
        dconc7 = tf.keras.layers.concatenate([dconv_seq5, conv_seq6], name=&quot;dconc7&quot;)
        deconv2 = Conv1DTranspose(dconc7, start_filter_num, kernel_size=3, strides=2, padding='same')

        dconv_seq6 = conv_block_seq_res(deconv2, start_filter_num*6, kernel_size, 1, &quot;dconv_seq6&quot;, In=False)
        dconc9 = tf.keras.layers.concatenate([dconv_seq6, conv_seq5], name=&quot;dconc9&quot;)
        deconv3 = Conv1DTranspose(dconc9, start_filter_num, kernel_size=3, strides=2, padding='same')

        dconv_seq7 = conv_block_seq_res(deconv3, int(start_filter_num*4.5), kernel_size, 1, &quot;dconv_seq7&quot;, In=False)
        dconc11 = tf.keras.layers.concatenate([dconv_seq7, conv_seq4], name=&quot;dconc11&quot;)
        deconv4 = Conv1DTranspose(dconc11, start_filter_num, kernel_size=3, strides=2, padding='same')

        dconv_seq8 = conv_block_seq_res(deconv4, start_filter_num*3, kernel_size, 1, &quot;dconv_seq8&quot;, In=False)
        dconc13 = tf.keras.layers.concatenate([dconv_seq8, conv_seq3], name=&quot;dconc13&quot;)
        deconv5 = Conv1DTranspose(dconc13, start_filter_num, kernel_size=3, strides=2, padding='same')

        dconv_seq9 = conv_block_seq_res(deconv5, int(start_filter_num*1.5), kernel_size, 1, &quot;dconv_seq9&quot;, In=False)
        dconc15 = tf.keras.layers.concatenate([dconv_seq9, conv_seq2], name=&quot;dconc15&quot;)
        deconv6 = Conv1DTranspose(dconc15, start_filter_num, kernel_size=3, strides=2, padding='same')

        dconv_seq10 = conv_block_seq_res(deconv6, start_filter_num, kernel_size, 1, &quot;dconv_seq10&quot;, In=False)
        dconc17 = tf.keras.layers.concatenate([dconv_seq10, conv_seq1], name=&quot;dconc17&quot;)

        x_pred = tf.keras.layers.Conv1D(1, 3, padding=&quot;same&quot;, activation=&quot;relu&quot;, name=&quot;x_pred&quot;)(dconc17)

        model = tf.keras.Model(inputs=[x, eps], outputs=x_pred)
        model.summary()
</code></pre>
<p>when I try to fit the model use this</p>
<pre><code>history = model.fit((x_train-main_mean)/main_std, (y_train-app_mean)/app_std, shuffle=True, validation_split=nilm[&quot;training&quot;][&quot;validation_split&quot;],
                            epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)
</code></pre>
<p>which of course gave me this error</p>
<p><strong>ValueError: Layer &quot;model&quot; expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 1024, 1) dtype=float32&gt;]</strong></p>
<p>I tried this</p>
<pre><code>        latent_dim = 16
        eps = tf.keras.Input(tensor = K.random_normal(shape=(K.shape(x_train)[0], latent_dim)))
        history = model.fit([(x_train-main_mean)/main_std, eps], (y_train-app_mean)/app_std, shuffle=True,# validation_split=nilm[&quot;training&quot;][&quot;validation_split&quot;],
                            epochs=epochs, batch_size=batch_size, callbacks=list_callbacks, verbose=1, initial_epoch=0)
</code></pre>
<p>but it gives me an error</p>
<p><strong>TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=&quot;created by layer 'tf.cast_4'&quot;), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as <code>tf.cond</code>, <code>tf.function</code>, gradient tapes, or <code>tf.map_fn</code>. Keras Functional model construction only supports TF API calls that <em>do</em> support dispatching, such as <code>tf.math.add</code> or <code>tf.reshape</code>. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer <code>call</code> and calling that layer on this symbolic input/output.</strong></p>
<p>I don't know how to solve it. I appreciate the help
thanks in advance</p>
",30,0,0,4,tensorflow;keras;autoencoder;multiple-input,2022-06-30 00:27:47,2022-06-30 00:27:47,2022-06-30 00:27:47,i m trying to implement a machine learning model  variational autoencoder  where the architecture of the model is when i try to fit the model use this which of course gave me this error valueerror  layer  model  expects  input s   but it received  input tensors  inputs received    lt tf tensor  iteratorgetnext   shape  none      dtype float gt   i tried this but it gives me an error typeerror  you are passing kerastensor type_spec tensorspec shape     dtype tf float  name none   name  placeholder    description  created by layer  tf cast_     an intermediate keras symbolic input output  to a tf api that does not allow registering custom dispatchers  such as tf cond  tf function  gradient tapes  or tf map_fn  keras functional model construction only supports tf api calls that do support dispatching  such as tf math add or tf reshape  other apis cannot be called directly on symbolic kerasinputs outputs  you can work around this limitation by putting the operation in a custom keras layer call and calling that layer on this symbolic input output ,multi input model,trying implement machine learning model variational autoencoder architecture model try fit model use course gave error valueerror layer model expects input received input tensors inputs received lt tf tensor iteratorgetnext shape none dtype float gt tried gives error typeerror passing kerastensor type_spec tensorspec shape dtype tf float name none name placeholder description created layer tf cast_ intermediate keras symbolic input output tf api allow registering dispatchers tf cond tf function gradient tapes tf map_fn keras functional model construction supports tf api calls support dispatching tf math tf reshape apis cannot called directly symbolic kerasinputs outputs work around limitation putting operation keras layer call calling layer symbolic input output,multi input model,multi input modeltrying implement machine learning model variational autoencoder architecture model try fit model use course gave error valueerror layer model expects input received input tensors inputs received lt tf tensor iteratorgetnext shape none dtype float gt tried gives error typeerror passing kerastensor type_spec tensorspec shape dtype tf float name none name placeholder description created layer tf cast_ intermediate keras symbolic input output tf api allow registering dispatchers tf cond tf function gradient tapes tf map_fn keras functional model construction supports tf api calls support dispatching tf math tf reshape apis cannot called directly symbolic kerasinputs outputs work around limitation putting operation keras layer call calling layer symbolic input output,"['multi', 'input', 'modeltrying', 'implement', 'machine', 'learning', 'model', 'variational', 'autoencoder', 'architecture', 'model', 'try', 'fit', 'model', 'use', 'course', 'gave', 'error', 'valueerror', 'layer', 'model', 'expects', 'input', 'received', 'input', 'tensors', 'inputs', 'received', 'lt', 'tf', 'tensor', 'iteratorgetnext', 'shape', 'none', 'dtype', 'float', 'gt', 'tried', 'gives', 'error', 'typeerror', 'passing', 'kerastensor', 'type_spec', 'tensorspec', 'shape', 'dtype', 'tf', 'float', 'name', 'none', 'name', 'placeholder', 'description', 'created', 'layer', 'tf', 'cast_', 'intermediate', 'keras', 'symbolic', 'input', 'output', 'tf', 'api', 'allow', 'registering', 'dispatchers', 'tf', 'cond', 'tf', 'function', 'gradient', 'tapes', 'tf', 'map_fn', 'keras', 'functional', 'model', 'construction', 'supports', 'tf', 'api', 'calls', 'support', 'dispatching', 'tf', 'math', 'tf', 'reshape', 'apis', 'can', 'not', 'called', 'directly', 'symbolic', 'kerasinputs', 'outputs', 'work', 'around', 'limitation', 'putting', 'operation', 'keras', 'layer', 'call', 'calling', 'layer', 'symbolic', 'input', 'output']","['multi', 'input', 'modeltri', 'implement', 'machin', 'learn', 'model', 'variat', 'autoencod', 'architectur', 'model', 'tri', 'fit', 'model', 'use', 'cours', 'gave', 'error', 'valueerror', 'layer', 'model', 'expect', 'input', 'receiv', 'input', 'tensor', 'input', 'receiv', 'lt', 'tf', 'tensor', 'iteratorgetnext', 'shape', 'none', 'dtype', 'float', 'gt', 'tri', 'give', 'error', 'typeerror', 'pass', 'kerastensor', 'type_spec', 'tensorspec', 'shape', 'dtype', 'tf', 'float', 'name', 'none', 'name', 'placehold', 'descript', 'creat', 'layer', 'tf', 'cast_', 'intermedi', 'kera', 'symbol', 'input', 'output', 'tf', 'api', 'allow', 'regist', 'dispatch', 'tf', 'cond', 'tf', 'function', 'gradient', 'tape', 'tf', 'map_fn', 'kera', 'function', 'model', 'construct', 'support', 'tf', 'api', 'call', 'support', 'dispatch', 'tf', 'math', 'tf', 'reshap', 'api', 'can', 'not', 'call', 'directli', 'symbol', 'kerasinput', 'output', 'work', 'around', 'limit', 'put', 'oper', 'kera', 'layer', 'call', 'call', 'layer', 'symbol', 'input', 'output']"
198,211,211,17156297,72598952,Azure Machine Learning REST API request limits,"<p>My application sends requests to Azure Machine Learning REST API in order to invoke a batch endpoint and start scoring jobs as described <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-batch-with-rest#invoke-the-batch-endpoint-to-start-a-batch-scoring-job"" rel=""nofollow noreferrer"">here</a>. It works well for small number of requests, but if the app sends many concurrent requests the REST API sometimes responds with status code 429 &quot;TooManyRequests&quot; and message &quot;Received too many requests in a short amount of time. Retry again after 1 seconds.&quot;. For example, it happened after sending 77 requests at once.</p>
<p>The message is pretty clear and the best solution I can think about is to throttle outgoing requests. That is making sure the app doesn't exceed limits when it sends concurrent requests. But the problem is I don't know what are the request limits for Azure Machine Learning REST API. Looking through the Microsoft documentation I could only find <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints"" rel=""nofollow noreferrer"">this article</a> which provides limits for Managed online endpoints whereas I'm looking for Batch endpoints.</p>
<p>I would really appreciate if someone helped me to find the Azure ML REST API request limits or suggested a better solution. Thanks.</p>
<p><strong>UPDATE 20 Jun 2022</strong>:
I couldn't find out how many concurrent requests are allowed by Azure Machine Learning batch endpoints. So I ended with a limit of 10 outgoing requests which solved the &quot;TooManyRequests&quot; problem. In order to throttle requests I used SemaphoreSlim as described <a href=""https://codeburst.io/throttling-concurrent-outgoing-http-requests-in-net-core-404b5acd987b"" rel=""nofollow noreferrer"">here</a>.</p>
",80,2,0,4,azure;limit;azure-machine-learning-service;azure-rest-api,2022-06-13 12:44:37,2022-06-13 12:44:37,2022-06-29 22:32:27,my application sends requests to azure machine learning rest api in order to invoke a batch endpoint and start scoring jobs as described   it works well for small number of requests  but if the app sends many concurrent requests the rest api sometimes responds with status code   toomanyrequests  and message  received too many requests in a short amount of time  retry again after  seconds    for example  it happened after sending  requests at once  the message is pretty clear and the best solution i can think about is to throttle outgoing requests  that is making sure the app doesn t exceed limits when it sends concurrent requests  but the problem is i don t know what are the request limits for azure machine learning rest api  looking through the microsoft documentation i could only find  which provides limits for managed online endpoints whereas i m looking for batch endpoints  i would really appreciate if someone helped me to find the azure ml rest api request limits or suggested a better solution  thanks ,azure machine learning rest api request limits,application sends requests azure machine learning rest api order invoke batch endpoint start scoring jobs described works well small number requests app sends many concurrent requests rest api sometimes responds status code toomanyrequests message received many requests short amount time retry seconds example happened sending requests message pretty clear best solution think throttle outgoing requests making sure app exceed limits sends concurrent requests problem know request limits azure machine learning rest api looking microsoft documentation could find provides limits managed online endpoints whereas looking batch endpoints would really appreciate someone helped find azure ml rest api request limits suggested better solution thanks,azure machine learning rest api request limits,azure machine learning rest api request limitsapplication sends requests azure machine learning rest api order invoke batch endpoint start scoring jobs described works well small number requests app sends many concurrent requests rest api sometimes responds status code toomanyrequests message received many requests short amount time retry seconds example happened sending requests message pretty clear best solution think throttle outgoing requests making sure app exceed limits sends concurrent requests problem know request limits azure machine learning rest api looking microsoft documentation could find provides limits managed online endpoints whereas looking batch endpoints would really appreciate someone helped find azure ml rest api request limits suggested better solution thanks,"['azure', 'machine', 'learning', 'rest', 'api', 'request', 'limitsapplication', 'sends', 'requests', 'azure', 'machine', 'learning', 'rest', 'api', 'order', 'invoke', 'batch', 'endpoint', 'start', 'scoring', 'jobs', 'described', 'works', 'well', 'small', 'number', 'requests', 'app', 'sends', 'many', 'concurrent', 'requests', 'rest', 'api', 'sometimes', 'responds', 'status', 'code', 'toomanyrequests', 'message', 'received', 'many', 'requests', 'short', 'amount', 'time', 'retry', 'seconds', 'example', 'happened', 'sending', 'requests', 'message', 'pretty', 'clear', 'best', 'solution', 'think', 'throttle', 'outgoing', 'requests', 'making', 'sure', 'app', 'exceed', 'limits', 'sends', 'concurrent', 'requests', 'problem', 'know', 'request', 'limits', 'azure', 'machine', 'learning', 'rest', 'api', 'looking', 'microsoft', 'documentation', 'could', 'find', 'provides', 'limits', 'managed', 'online', 'endpoints', 'whereas', 'looking', 'batch', 'endpoints', 'would', 'really', 'appreciate', 'someone', 'helped', 'find', 'azure', 'ml', 'rest', 'api', 'request', 'limits', 'suggested', 'better', 'solution', 'thanks']","['azur', 'machin', 'learn', 'rest', 'api', 'request', 'limitsappl', 'send', 'request', 'azur', 'machin', 'learn', 'rest', 'api', 'order', 'invok', 'batch', 'endpoint', 'start', 'score', 'job', 'describ', 'work', 'well', 'small', 'number', 'request', 'app', 'send', 'mani', 'concurr', 'request', 'rest', 'api', 'sometim', 'respond', 'statu', 'code', 'toomanyrequest', 'messag', 'receiv', 'mani', 'request', 'short', 'amount', 'time', 'retri', 'second', 'exampl', 'happen', 'send', 'request', 'messag', 'pretti', 'clear', 'best', 'solut', 'think', 'throttl', 'outgo', 'request', 'make', 'sure', 'app', 'exceed', 'limit', 'send', 'concurr', 'request', 'problem', 'know', 'request', 'limit', 'azur', 'machin', 'learn', 'rest', 'api', 'look', 'microsoft', 'document', 'could', 'find', 'provid', 'limit', 'manag', 'onlin', 'endpoint', 'wherea', 'look', 'batch', 'endpoint', 'would', 'realli', 'appreci', 'someon', 'help', 'find', 'azur', 'ml', 'rest', 'api', 'request', 'limit', 'suggest', 'better', 'solut', 'thank']"
199,212,212,1901071,72698110,Python translate matplotlib to a plotnine chart,"<p>I am currently working through the book <a href=""https://rads.stackoverflow.com/amzn/click/com/1492032646"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Hands On Machine Learning</a> and am trying to replicate a visualization where we plot the lat and lon co-ordinates on a scatter plot of San Diego. I have taken the plot code from the book which uses the code below (matplotlib method). I would like to replicate the same visualization using <a href=""https://plotnine.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">plotnine</a>. Could someone help me with the translation.</p>
<h3>matplotlib method</h3>
<pre><code># DATA INGEST -------------------------------------------------------------    
# Import the file from github
url = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv&quot; # Make sure the url is the raw version of the file on GitHub
download = requests.get(url).content

# Reading the downloaded content and turning it into a pandas dataframe
housing = pd.read_csv(io.StringIO(download.decode('utf-8')))

# Then plot
import matplotlib.pyplot as plt

# The size is now related to population divided by 100
# the colour is related to the median house value
housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, 
              s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7),
              c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True)
plt.legend()
plt.show()

</code></pre>
<h3>plotnine method</h3>
<pre><code>from plotnine import ggplot, geom_point, aes, stat_smooth, scale_color_cmap

# Lets try the same thing in ggplot
(ggplot(housing, aes('longitude', 'latitude', size = &quot;population&quot;, color = &quot;median_house_value&quot;))
 + geom_point(alpha = 0.1)
 + scale_color_cmap(name=&quot;jet&quot;))
 
</code></pre>
",38,1,1,4,python;matplotlib;ggplot2;plotnine,2022-06-21 14:40:43,2022-06-21 14:40:43,2022-06-29 22:17:33,i am currently working through the book  and am trying to replicate a visualization where we plot the lat and lon co ordinates on a scatter plot of san diego  i have taken the plot code from the book which uses the code below  matplotlib method   i would like to replicate the same visualization using   could someone help me with the translation ,python translate matplotlib to a plotnine chart,currently working book trying replicate visualization plot lat lon co ordinates scatter plot san diego taken plot code book uses code matplotlib method would like replicate visualization using could someone help translation,python translate matplotlib plotnine chart,python translate matplotlib plotnine chartcurrently working book trying replicate visualization plot lat lon co ordinates scatter plot san diego taken plot code book uses code matplotlib method would like replicate visualization using could someone help translation,"['python', 'translate', 'matplotlib', 'plotnine', 'chartcurrently', 'working', 'book', 'trying', 'replicate', 'visualization', 'plot', 'lat', 'lon', 'co', 'ordinates', 'scatter', 'plot', 'san', 'diego', 'taken', 'plot', 'code', 'book', 'uses', 'code', 'matplotlib', 'method', 'would', 'like', 'replicate', 'visualization', 'using', 'could', 'someone', 'help', 'translation']","['python', 'translat', 'matplotlib', 'plotnin', 'chartcurr', 'work', 'book', 'tri', 'replic', 'visual', 'plot', 'lat', 'lon', 'co', 'ordin', 'scatter', 'plot', 'san', 'diego', 'taken', 'plot', 'code', 'book', 'use', 'code', 'matplotlib', 'method', 'would', 'like', 'replic', 'visual', 'use', 'could', 'someon', 'help', 'translat']"
200,213,213,15181029,72804986,SnapML model issue on Lens Studio,"<p>I built a custom model for classifying images of cars using Tensorflow and Keras, to use it for building a Snap lens powered by machine learning. Lens Studio only accepts quantized models; the model had to go through the quantization process using the TFLite module.</p>
<p>However, the problem is that the model passed into Lens Studio is unable to function properly. It only displays classification results for the first time after its initiation;  then the results (and even the probability numbers behind classification) remain static despite image/video changes.</p>
<p>Any tips on how to solve this issue would be appreciated. The configurations for input image setups remain identical as the provided template by Snap.</p>
",12,0,0,5,tensorflow;keras;quantization;snapchat;tflite,2022-06-29 22:07:02,2022-06-29 22:07:02,2022-06-29 22:07:02,i built a custom model for classifying images of cars using tensorflow and keras  to use it for building a snap lens powered by machine learning  lens studio only accepts quantized models  the model had to go through the quantization process using the tflite module  however  the problem is that the model passed into lens studio is unable to function properly  it only displays classification results for the first time after its initiation   then the results  and even the probability numbers behind classification  remain static despite image video changes  any tips on how to solve this issue would be appreciated  the configurations for input image setups remain identical as the provided template by snap ,snapml model issue on lens studio,built model classifying images cars using tensorflow keras use building snap lens powered machine learning lens studio accepts quantized models model go quantization process using tflite module however problem model passed lens studio unable function properly displays classification results first time initiation results even probability numbers behind classification remain static despite image video changes tips solve issue would appreciated configurations input image setups remain identical provided template snap,snapml model issue lens studio,snapml model issue lens studiobuilt model classifying images cars using tensorflow keras use building snap lens powered machine learning lens studio accepts quantized models model go quantization process using tflite module however problem model passed lens studio unable function properly displays classification results first time initiation results even probability numbers behind classification remain static despite image video changes tips solve issue would appreciated configurations input image setups remain identical provided template snap,"['snapml', 'model', 'issue', 'lens', 'studiobuilt', 'model', 'classifying', 'images', 'cars', 'using', 'tensorflow', 'keras', 'use', 'building', 'snap', 'lens', 'powered', 'machine', 'learning', 'lens', 'studio', 'accepts', 'quantized', 'models', 'model', 'go', 'quantization', 'process', 'using', 'tflite', 'module', 'however', 'problem', 'model', 'passed', 'lens', 'studio', 'unable', 'function', 'properly', 'displays', 'classification', 'results', 'first', 'time', 'initiation', 'results', 'even', 'probability', 'numbers', 'behind', 'classification', 'remain', 'static', 'despite', 'image', 'video', 'changes', 'tips', 'solve', 'issue', 'would', 'appreciated', 'configurations', 'input', 'image', 'setups', 'remain', 'identical', 'provided', 'template', 'snap']","['snapml', 'model', 'issu', 'len', 'studiobuilt', 'model', 'classifi', 'imag', 'car', 'use', 'tensorflow', 'kera', 'use', 'build', 'snap', 'len', 'power', 'machin', 'learn', 'len', 'studio', 'accept', 'quantiz', 'model', 'model', 'go', 'quantiz', 'process', 'use', 'tflite', 'modul', 'howev', 'problem', 'model', 'pass', 'len', 'studio', 'unabl', 'function', 'properli', 'display', 'classif', 'result', 'first', 'time', 'initi', 'result', 'even', 'probabl', 'number', 'behind', 'classif', 'remain', 'static', 'despit', 'imag', 'video', 'chang', 'tip', 'solv', 'issu', 'would', 'appreci', 'configur', 'input', 'imag', 'setup', 'remain', 'ident', 'provid', 'templat', 'snap']"
201,214,214,15493107,72767023,Fullscreen mode not working in D3D12 raytracing samples,"<p>Presently I'm learning the basics of real-time raytracing with the DXR API in DirectX 12 Ultimate. I'm studying the D3D12 raytracing samples on the official GitHub and am using an i9/Intel Iris Xe/RTX3070 laptop and building the programs in VS2022.</p>
<p>Since the samples were written for Windows 10 and I'm using a hybrid graphics PC, a Debug build will run in Windows 11 after adding <code>D3D12_MESSAGE_ID_RESOURCE_BARRIER_MISMATCHING_COMMAND_LIST_TYPE</code> to <strong>D3D12_INFO_QUEUE_FILTER</strong> during device creation (see <a href=""https://stackoverflow.com/questions/69805245/directx-12-application-is-crashing-in-windows-11"">DirectX 12 application is crashing in Windows 11</a>). The only trouble is that none of the sample programs change to fullscreen (i.e. borderless windowed) mode when pressing the Alt+Enter key combination. The programs always stay in windowed mode.</p>
<p>This hasn't worried me so far, because I've been copying the raytracing code over to a template (based on DirectX Tool Kit for Windows desktop) where fullscreen toggling works properly. In this way, I was able to run the <em>HelloWorld</em> and <em>SimpleLighting</em> samples successfully in both windowed mode and fullscreen (2560x1440 monitor resolution).</p>
<p>However, this hasn't been so successful for the <em>ProceduralGeometry</em> sample, which introduces intersection shaders. Once again, the original sample program renders the scene properly, but only in a bordered window. But when the code is reproduced in the template where I can toggle to fullscreen, the raytraced scene does not render properly.</p>
<p>In the scene, the triangle geometry used for the ground plane of the scene renders ok, but a translucent bounding box around the fractal pyramid is visible, and all other procedural geometry also appears translucent. Every couple of seconds, the bounding box for the metaballs also appears briefly, then vanishes.</p>
<p>I was able to determine that by freezing the scene, the reason for the translucency was that the following frames were being presented in sequence:</p>
<ul>
<li>triangle ground plane quad only</li>
<li>floor geometry plus opaque fractal pyramid bounding box</li>
<li>all of the above plus opaque metaball bounding box</li>
<li>completed scene with opaque geometry and no bounding boxes</li>
</ul>
<p>At the native framerate (165Hz on my machine), this results in both the procedural geometry and bounding boxes always being visible, but 'see-through' due to all the partially complete frames being presented to the display. This happens in both windowed and fullscreen modes, but it's worse in fullscreen, because the scene gets affected by random image corruption not seen in windowed mode.</p>
<p>I've been grappling with this issue for a few days and can't work out the problem. The only changes I've made to the sample program are the Windows 11 fix, and using a template for proper fullscreen rendering, which the original sample ignores or doesn't implement properly.</p>
<p>Hopefully someone can shed light on this perplexing issue!</p>
",47,1,1,5,visual-studio-2022;raytracing;windows-11;directx-12;direct3d12,2022-06-27 10:03:11,2022-06-27 10:03:11,2022-06-29 21:47:04,presently i m learning the basics of real time raytracing with the dxr api in directx  ultimate  i m studying the dd raytracing samples on the official github and am using an i intel iris xe rtx laptop and building the programs in vs  since the samples were written for windows  and i m using a hybrid graphics pc  a debug build will run in windows  after adding dd_message_id_resource_barrier_mismatching_command_list_type to dd_info_queue_filter during device creation  see    the only trouble is that none of the sample programs change to fullscreen  i e  borderless windowed  mode when pressing the alt enter key combination  the programs always stay in windowed mode  this hasn t worried me so far  because i ve been copying the raytracing code over to a template  based on directx tool kit for windows desktop  where fullscreen toggling works properly  in this way  i was able to run the helloworld and simplelighting samples successfully in both windowed mode and fullscreen  x monitor resolution   however  this hasn t been so successful for the proceduralgeometry sample  which introduces intersection shaders  once again  the original sample program renders the scene properly  but only in a bordered window  but when the code is reproduced in the template where i can toggle to fullscreen  the raytraced scene does not render properly  in the scene  the triangle geometry used for the ground plane of the scene renders ok  but a translucent bounding box around the fractal pyramid is visible  and all other procedural geometry also appears translucent  every couple of seconds  the bounding box for the metaballs also appears briefly  then vanishes  i was able to determine that by freezing the scene  the reason for the translucency was that the following frames were being presented in sequence  at the native framerate  hz on my machine   this results in both the procedural geometry and bounding boxes always being visible  but  see through  due to all the partially complete frames being presented to the display  this happens in both windowed and fullscreen modes  but it s worse in fullscreen  because the scene gets affected by random image corruption not seen in windowed mode  i ve been grappling with this issue for a few days and can t work out the problem  the only changes i ve made to the sample program are the windows  fix  and using a template for proper fullscreen rendering  which the original sample ignores or doesn t implement properly  hopefully someone can shed light on this perplexing issue ,fullscreen mode not working in dd raytracing samples,presently learning basics real time raytracing dxr api directx ultimate studying dd raytracing samples official github using intel iris xe rtx laptop building programs vs since samples written windows using hybrid graphics pc debug build run windows adding dd_message_id_resource_barrier_mismatching_command_list_type dd_info_queue_filter device creation see trouble none sample programs change fullscreen e borderless windowed mode pressing alt enter key combination programs always stay windowed mode worried far copying raytracing code template based directx tool kit windows desktop fullscreen toggling works properly way able run helloworld simplelighting samples successfully windowed mode fullscreen x monitor resolution however successful proceduralgeometry sample introduces intersection shaders original sample program renders scene properly bordered window code reproduced template toggle fullscreen raytraced scene render properly scene triangle geometry used ground plane scene renders ok translucent bounding box around fractal pyramid visible procedural geometry also appears translucent every couple seconds bounding box metaballs also appears briefly vanishes able determine freezing scene reason translucency following frames presented sequence native framerate hz machine results procedural geometry bounding boxes always visible see due partially complete frames presented display happens windowed fullscreen modes worse fullscreen scene gets affected random image corruption seen windowed mode grappling issue days work problem changes made sample program windows fix using template proper fullscreen rendering original sample ignores implement properly hopefully someone shed light perplexing issue,fullscreen mode working dd raytracing samples,fullscreen mode working dd raytracing samplespresently learning basics real time raytracing dxr api directx ultimate studying dd raytracing samples official github using intel iris xe rtx laptop building programs vs since samples written windows using hybrid graphics pc debug build run windows adding dd_message_id_resource_barrier_mismatching_command_list_type dd_info_queue_filter device creation see trouble none sample programs change fullscreen e borderless windowed mode pressing alt enter key combination programs always stay windowed mode worried far copying raytracing code template based directx tool kit windows desktop fullscreen toggling works properly way able run helloworld simplelighting samples successfully windowed mode fullscreen x monitor resolution however successful proceduralgeometry sample introduces intersection shaders original sample program renders scene properly bordered window code reproduced template toggle fullscreen raytraced scene render properly scene triangle geometry used ground plane scene renders ok translucent bounding box around fractal pyramid visible procedural geometry also appears translucent every couple seconds bounding box metaballs also appears briefly vanishes able determine freezing scene reason translucency following frames presented sequence native framerate hz machine results procedural geometry bounding boxes always visible see due partially complete frames presented display happens windowed fullscreen modes worse fullscreen scene gets affected random image corruption seen windowed mode grappling issue days work problem changes made sample program windows fix using template proper fullscreen rendering original sample ignores implement properly hopefully someone shed light perplexing issue,"['fullscreen', 'mode', 'working', 'dd', 'raytracing', 'samplespresently', 'learning', 'basics', 'real', 'time', 'raytracing', 'dxr', 'api', 'directx', 'ultimate', 'studying', 'dd', 'raytracing', 'samples', 'official', 'github', 'using', 'intel', 'iris', 'xe', 'rtx', 'laptop', 'building', 'programs', 'vs', 'since', 'samples', 'written', 'windows', 'using', 'hybrid', 'graphics', 'pc', 'debug', 'build', 'run', 'windows', 'adding', 'dd_message_id_resource_barrier_mismatching_command_list_type', 'dd_info_queue_filter', 'device', 'creation', 'see', 'trouble', 'none', 'sample', 'programs', 'change', 'fullscreen', 'e', 'borderless', 'windowed', 'mode', 'pressing', 'alt', 'enter', 'key', 'combination', 'programs', 'always', 'stay', 'windowed', 'mode', 'worried', 'far', 'copying', 'raytracing', 'code', 'template', 'based', 'directx', 'tool', 'kit', 'windows', 'desktop', 'fullscreen', 'toggling', 'works', 'properly', 'way', 'able', 'run', 'helloworld', 'simplelighting', 'samples', 'successfully', 'windowed', 'mode', 'fullscreen', 'x', 'monitor', 'resolution', 'however', 'successful', 'proceduralgeometry', 'sample', 'introduces', 'intersection', 'shaders', 'original', 'sample', 'program', 'renders', 'scene', 'properly', 'bordered', 'window', 'code', 'reproduced', 'template', 'toggle', 'fullscreen', 'raytraced', 'scene', 'render', 'properly', 'scene', 'triangle', 'geometry', 'used', 'ground', 'plane', 'scene', 'renders', 'ok', 'translucent', 'bounding', 'box', 'around', 'fractal', 'pyramid', 'visible', 'procedural', 'geometry', 'also', 'appears', 'translucent', 'every', 'couple', 'seconds', 'bounding', 'box', 'metaballs', 'also', 'appears', 'briefly', 'vanishes', 'able', 'determine', 'freezing', 'scene', 'reason', 'translucency', 'following', 'frames', 'presented', 'sequence', 'native', 'framerate', 'hz', 'machine', 'results', 'procedural', 'geometry', 'bounding', 'boxes', 'always', 'visible', 'see', 'due', 'partially', 'complete', 'frames', 'presented', 'display', 'happens', 'windowed', 'fullscreen', 'modes', 'worse', 'fullscreen', 'scene', 'gets', 'affected', 'random', 'image', 'corruption', 'seen', 'windowed', 'mode', 'grappling', 'issue', 'days', 'work', 'problem', 'changes', 'made', 'sample', 'program', 'windows', 'fix', 'using', 'template', 'proper', 'fullscreen', 'rendering', 'original', 'sample', 'ignores', 'implement', 'properly', 'hopefully', 'someone', 'shed', 'light', 'perplexing', 'issue']","['fullscreen', 'mode', 'work', 'dd', 'raytrac', 'samplespres', 'learn', 'basic', 'real', 'time', 'raytrac', 'dxr', 'api', 'directx', 'ultim', 'studi', 'dd', 'raytrac', 'sampl', 'offici', 'github', 'use', 'intel', 'iri', 'xe', 'rtx', 'laptop', 'build', 'program', 'vs', 'sinc', 'sampl', 'written', 'window', 'use', 'hybrid', 'graphic', 'pc', 'debug', 'build', 'run', 'window', 'ad', 'dd_message_id_resource_barrier_mismatching_command_list_typ', 'dd_info_queue_filt', 'devic', 'creation', 'see', 'troubl', 'none', 'sampl', 'program', 'chang', 'fullscreen', 'e', 'borderless', 'window', 'mode', 'press', 'alt', 'enter', 'key', 'combin', 'program', 'alway', 'stay', 'window', 'mode', 'worri', 'far', 'copi', 'raytrac', 'code', 'templat', 'base', 'directx', 'tool', 'kit', 'window', 'desktop', 'fullscreen', 'toggl', 'work', 'properli', 'way', 'abl', 'run', 'helloworld', 'simplelight', 'sampl', 'success', 'window', 'mode', 'fullscreen', 'x', 'monitor', 'resolut', 'howev', 'success', 'proceduralgeometri', 'sampl', 'introduc', 'intersect', 'shader', 'origin', 'sampl', 'program', 'render', 'scene', 'properli', 'border', 'window', 'code', 'reproduc', 'templat', 'toggl', 'fullscreen', 'raytrac', 'scene', 'render', 'properli', 'scene', 'triangl', 'geometri', 'use', 'ground', 'plane', 'scene', 'render', 'ok', 'transluc', 'bound', 'box', 'around', 'fractal', 'pyramid', 'visibl', 'procedur', 'geometri', 'also', 'appear', 'transluc', 'everi', 'coupl', 'second', 'bound', 'box', 'metabal', 'also', 'appear', 'briefli', 'vanish', 'abl', 'determin', 'freez', 'scene', 'reason', 'transluc', 'follow', 'frame', 'present', 'sequenc', 'nativ', 'framer', 'hz', 'machin', 'result', 'procedur', 'geometri', 'bound', 'box', 'alway', 'visibl', 'see', 'due', 'partial', 'complet', 'frame', 'present', 'display', 'happen', 'window', 'fullscreen', 'mode', 'wors', 'fullscreen', 'scene', 'get', 'affect', 'random', 'imag', 'corrupt', 'seen', 'window', 'mode', 'grappl', 'issu', 'day', 'work', 'problem', 'chang', 'made', 'sampl', 'program', 'window', 'fix', 'use', 'templat', 'proper', 'fullscreen', 'render', 'origin', 'sampl', 'ignor', 'implement', 'properli', 'hope', 'someon', 'shed', 'light', 'perplex', 'issu']"
202,215,215,19281672,72804036,Compositional data preprocessing for machine learning,"<p>I have this compositional data from a metagenomics project, in which each row is a sample (60 rows) and each column is a taxonomical Genus (473 columns):
<a href=""https://i.stack.imgur.com/Py6wo.png"" rel=""nofollow noreferrer"">Compositional data (extract)</a></p>
<p>In addition, I transformed these data in R with the &quot;clr&quot; transformation:</p>
<pre><code>phy_gen_clr &lt;- microbiome::transform(phy_gen_comp, 'clr')
</code></pre>
<p>After which I obtained the following data points, which are relative abundances:
<a href=""https://i.stack.imgur.com/84FLk.png"" rel=""nofollow noreferrer"">clr transformed compositional data (extract)</a></p>
<p>Given this data, I want to create a Random Forest model, which should be able to predict, given these transformed abundances, which group each sample belongs to. I am firstly doing this without partitioning the data in training/test sets. This is the code I have developed in Python, using a pipeline:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV,KFold
from sklearn.metrics import make_scorer
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, balanced_accuracy_score
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
import warnings

data = pd.read_csv('clr_abundances.csv', decimal=&quot;.&quot;,delimiter=&quot;;&quot;)
X = data.values[ :, 1:-2].astype(np.float64)
y = (data.values[ :, -1 ])

print(X.shape)

warnings.filterwarnings(&quot;ignore&quot;, category=Warning)
pipeline_rf = Pipeline([(&quot;sel&quot;,SelectFromModel(LogisticRegression(penalty='l1',solver='liblinear'))),(&quot;RF&quot;,RandomForestClassifier())])
param_grid_rf = {&quot;sel__estimator__C&quot;:np.arange(0.001,1.1,0.1).astype(np.float),
         &quot;RF__n_estimators&quot;:np.arange(1,102,10).astype(np.int),
         &quot;RF__criterion&quot;:[&quot;gini&quot;,&quot;entropy&quot;,&quot;log_loss&quot;],
         &quot;RF__max_depth&quot;:[2,3,4],
         &quot;RF__max_features&quot;:[1,20]}
skfold_rf=KFold(n_splits=X.shape[0])
gridcv_rf=GridSearchCV(pipeline_rf,param_grid_rf,cv=skfold_rf)
result_rf=gridcv_rf.fit(X, y)
print(&quot;Best parameters:&quot;,gridcv_rf.best_params_)
print(&quot;Score:&quot;,gridcv_rf.score(X, y))
</code></pre>
<p>As you can see, I first import the data, then define X, which are all the data points, and y, which a list with the group each sample belongs to:</p>
<pre><code>['Group 2' 'Group 1' 'Group 2' 'Group 2' 'Group 2' 'Group 2' 'Group 1'
'Group 2' 'Group 1' 'Group 1' 'Group 1' 'Group 2' 'Group 1' 'Group 1'
'Group 2' 'Group 1' 'Group 2' 'Group 1' 'Group 2' 'Group 2' 'Group 1'
'Group 2' 'Group 1' 'Group 2' 'Group 2' 'Group 1' 'Group 1' 'Group 2'
'Group 2' 'Group 2' 'Group 2' 'Group 2' 'Group 2' 'Group 2' 'Group 1'
'Group 2' 'Group 2' 'Group 1' 'Group 1' 'Group 2' 'Group 1' 'Group 2'
'Group 1' 'Group 2' 'Group 1' 'Group 2' 'Group 1' 'Group 1' 'Group 1'
'Group 1' 'Group 1' 'Group 2' 'Group 1' 'Group 2' 'Group 2' 'Group 2'
'Group 1' 'Group 2' 'Group 1' 'Group 2']
</code></pre>
<p>After this, in a pipeline, I am performing first a LASSO feature selection, whose parameter &quot;C&quot; I am tuning in the &quot;param_grid_rf&quot; variable, after which I define the Random Forest Classifier, whose parameters I am also tuning in that variable. Following this, I want to perform a Leave One Out Cross Validation (LOOCV), which I am doing with &quot;KFold(<em>number of samples</em>)&quot;. Then, I search for the best hyperparameter values with &quot;GridSearchCV&quot;, and I fit this model with my clr-transformed abundances (X) and groups (y).</p>
<p>This code should return the best hyperparameter values and the score obtained from this model given these values.</p>
<p>The problem from this code is that it does not throw any errors, but <strong>it never stops executing</strong>. Is there something I should have done to avoid this?</p>
<p>Before the feature selection process (SelectFromModel), should I <strong>standardize</strong>/<strong>normalize</strong> the already clr-transformed data?</p>
<p>Thank you in advance.</p>
",19,0,0,5,python;machine-learning;scikit-learn;gridsearchcv;leave-one-out,2022-06-29 20:56:09,2022-06-29 20:56:09,2022-06-29 20:56:09,in addition  i transformed these data in r with the  clr  transformation  given this data  i want to create a random forest model  which should be able to predict  given these transformed abundances  which group each sample belongs to  i am firstly doing this without partitioning the data in training test sets  this is the code i have developed in python  using a pipeline  as you can see  i first import the data  then define x  which are all the data points  and y  which a list with the group each sample belongs to  after this  in a pipeline  i am performing first a lasso feature selection  whose parameter  c  i am tuning in the  param_grid_rf  variable  after which i define the random forest classifier  whose parameters i am also tuning in that variable  following this  i want to perform a leave one out cross validation  loocv   which i am doing with  kfold number of samples    then  i search for the best hyperparameter values with  gridsearchcv   and i fit this model with my clr transformed abundances  x  and groups  y   this code should return the best hyperparameter values and the score obtained from this model given these values  the problem from this code is that it does not throw any errors  but it never stops executing  is there something i should have done to avoid this  before the feature selection process  selectfrommodel   should i standardize normalize the already clr transformed data  thank you in advance ,compositional data preprocessing for machine learning,addition transformed data r clr transformation given data want create random forest model able predict given transformed abundances group sample belongs firstly without partitioning data training test sets code developed python using pipeline see first import data define x data points group sample belongs pipeline performing first lasso feature selection whose parameter c tuning param_grid_rf variable define random forest classifier whose parameters also tuning variable following want perform leave one cross validation loocv kfold number samples search best hyperparameter values gridsearchcv fit model clr transformed abundances x groups code return best hyperparameter values score obtained model given values problem code throw errors never stops executing something done avoid feature selection process selectfrommodel standardize normalize already clr transformed data thank advance,compositional data preprocessing machine learning,compositional data preprocessing machine learningaddition transformed data r clr transformation given data want create random forest model able predict given transformed abundances group sample belongs firstly without partitioning data training test sets code developed python using pipeline see first import data define x data points group sample belongs pipeline performing first lasso feature selection whose parameter c tuning param_grid_rf variable define random forest classifier whose parameters also tuning variable following want perform leave one cross validation loocv kfold number samples search best hyperparameter values gridsearchcv fit model clr transformed abundances x groups code return best hyperparameter values score obtained model given values problem code throw errors never stops executing something done avoid feature selection process selectfrommodel standardize normalize already clr transformed data thank advance,"['compositional', 'data', 'preprocessing', 'machine', 'learningaddition', 'transformed', 'data', 'r', 'clr', 'transformation', 'given', 'data', 'want', 'create', 'random', 'forest', 'model', 'able', 'predict', 'given', 'transformed', 'abundances', 'group', 'sample', 'belongs', 'firstly', 'without', 'partitioning', 'data', 'training', 'test', 'sets', 'code', 'developed', 'python', 'using', 'pipeline', 'see', 'first', 'import', 'data', 'define', 'x', 'data', 'points', 'group', 'sample', 'belongs', 'pipeline', 'performing', 'first', 'lasso', 'feature', 'selection', 'whose', 'parameter', 'c', 'tuning', 'param_grid_rf', 'variable', 'define', 'random', 'forest', 'classifier', 'whose', 'parameters', 'also', 'tuning', 'variable', 'following', 'want', 'perform', 'leave', 'one', 'cross', 'validation', 'loocv', 'kfold', 'number', 'samples', 'search', 'best', 'hyperparameter', 'values', 'gridsearchcv', 'fit', 'model', 'clr', 'transformed', 'abundances', 'x', 'groups', 'code', 'return', 'best', 'hyperparameter', 'values', 'score', 'obtained', 'model', 'given', 'values', 'problem', 'code', 'throw', 'errors', 'never', 'stops', 'executing', 'something', 'done', 'avoid', 'feature', 'selection', 'process', 'selectfrommodel', 'standardize', 'normalize', 'already', 'clr', 'transformed', 'data', 'thank', 'advance']","['composit', 'data', 'preprocess', 'machin', 'learningaddit', 'transform', 'data', 'r', 'clr', 'transform', 'given', 'data', 'want', 'creat', 'random', 'forest', 'model', 'abl', 'predict', 'given', 'transform', 'abund', 'group', 'sampl', 'belong', 'firstli', 'without', 'partit', 'data', 'train', 'test', 'set', 'code', 'develop', 'python', 'use', 'pipelin', 'see', 'first', 'import', 'data', 'defin', 'x', 'data', 'point', 'group', 'sampl', 'belong', 'pipelin', 'perform', 'first', 'lasso', 'featur', 'select', 'whose', 'paramet', 'c', 'tune', 'param_grid_rf', 'variabl', 'defin', 'random', 'forest', 'classifi', 'whose', 'paramet', 'also', 'tune', 'variabl', 'follow', 'want', 'perform', 'leav', 'one', 'cross', 'valid', 'loocv', 'kfold', 'number', 'sampl', 'search', 'best', 'hyperparamet', 'valu', 'gridsearchcv', 'fit', 'model', 'clr', 'transform', 'abund', 'x', 'group', 'code', 'return', 'best', 'hyperparamet', 'valu', 'score', 'obtain', 'model', 'given', 'valu', 'problem', 'code', 'throw', 'error', 'never', 'stop', 'execut', 'someth', 'done', 'avoid', 'featur', 'select', 'process', 'selectfrommodel', 'standard', 'normal', 'alreadi', 'clr', 'transform', 'data', 'thank', 'advanc']"
203,216,216,19436129,72798225,Remote Connection fails in setup of Python data-science client for SQL Server Machine Learning Services,"<p>I am trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services following this guide: <a href=""https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql</a> (section 6).
Running the following script</p>

<pre class=""lang-py prettyprint-override""><code>def send_this_func_to_sql():
    from revoscalepy import RxSqlServerData, rx_import
    from pandas.tools.plotting import scatter_matrix
    import matplotlib.pyplot as plt
    import io
    
    # remember the scope of the variables in this func are within our SQL Server Python Runtime
    connection_string = &quot;Driver=SQL Server;Server=localhost\instance02;Database=testmlsiris;Trusted_Connection=Yes;&quot;
    
    # specify a query and load into pandas dataframe df
    sql_query = RxSqlServerData(connection_string=connection_string, sql_query = &quot;select * from iris_data&quot;)
    df = rx_import(sql_query)
    
    scatter_matrix(df)
    
    # return bytestream of image created by scatter_matrix
    buf = io.BytesIO()
    plt.savefig(buf, format=&quot;png&quot;)
    buf.seek(0)
    
    return buf.getvalue()

new_db_name = &quot;testmlsiris&quot;
connection_string = &quot;driver={sql server};server=sqlrzs\instance02;database=%s;trusted_connection=yes;&quot; 

from revoscalepy import RxInSqlServer, rx_exec

# create a remote compute context with connection to SQL Server
sql_compute_context = RxInSqlServer(connection_string=connection_string%new_db_name)

# use rx_exec to send the function execution to SQL Server
image = rx_exec(send_this_func_to_sql, compute_context=sql_compute_context)[0]
</code></pre>
<p>yields the following error message returned by rx_exec (stored in the <em>image</em> variable)</p>
<pre class=""lang-py prettyprint-override""><code>connection_string: &quot;driver={sql server};server=sqlrzs\instance02;database=testmlsiris;trusted_connection=yes;&quot;
num_tasks: 1
execution_timeout_seconds: 0
wait: True
console_output: False
auto_cleanup: True
packages_to_load: []
description: &quot;sqlserver&quot;
version: &quot;1.0&quot;
XXX lineno: 2, opcode: 0
Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;
  File &quot;E:\SQL\MSSQL15.INSTANCE02\PYTHON_SERVICES\lib\site-packages\revoscalepy\computecontext\RxInSqlServer.py&quot;, line 664, in rx_sql_satellite_pool_call
    exec(inputfile.read())
  File &quot;&lt;string&gt;&quot;, line 34, in &lt;module&gt;
  File &quot;E:\SQL\MSSQL15.INSTANCE02\PYTHON_SERVICES\lib\site-packages\revoscalepy\computecontext\RxInSqlServer.py&quot;, line 886, in rx_remote_call
    results = rx_resumeexecution(state_file = inputfile, patched_server_name=args[&quot;hostname&quot;])
  File &quot;E:\SQL\MSSQL15.INSTANCE02\PYTHON_SERVICES\lib\site-packages\revoscalepy\computecontext\RxInSqlServer.py&quot;, line 135, in rx_resumeexecution
    return _state[&quot;function&quot;](**_state[&quot;args&quot;])
  File &quot;C:\Users\username\sendtosql.py&quot;, line 2, in send_this_func_to_sql
SystemError: unknown opcode
====== sqlrzs ( process 0 ) has started run at 2022-06-29 13:47:04 W. Europe Daylight Time ======
{'local_state': {}, 'args': {}, 'function': &lt;function send_this_func_to_sql at 0x0000020F5810F1E0&gt;}
</code></pre>
<p>What is going wrong here? Line 2 in the script is just an import (which works when testing Python scripts on SQL Server directly). Any help is appreciated - thanks.</p>
",39,1,0,4,python;sql-server;azure-machine-learning-studio;microsoft-machine-learning-server,2022-06-29 14:02:20,2022-06-29 14:02:20,2022-06-29 20:07:17,yields the following error message returned by rx_exec  stored in the image variable  what is going wrong here  line  in the script is just an import  which works when testing python scripts on sql server directly   any help is appreciated   thanks ,remote connection fails in setup of python data science client for sql server machine learning services,yields following error message returned rx_exec stored image variable going wrong line script import works testing python scripts sql server directly help appreciated thanks,remote connection fails setup python data science client sql server machine learning services,remote connection fails setup python data science client sql server machine learning servicesyields following error message returned rx_exec stored image variable going wrong line script import works testing python scripts sql server directly help appreciated thanks,"['remote', 'connection', 'fails', 'setup', 'python', 'data', 'science', 'client', 'sql', 'server', 'machine', 'learning', 'servicesyields', 'following', 'error', 'message', 'returned', 'rx_exec', 'stored', 'image', 'variable', 'going', 'wrong', 'line', 'script', 'import', 'works', 'testing', 'python', 'scripts', 'sql', 'server', 'directly', 'help', 'appreciated', 'thanks']","['remot', 'connect', 'fail', 'setup', 'python', 'data', 'scienc', 'client', 'sql', 'server', 'machin', 'learn', 'servicesyield', 'follow', 'error', 'messag', 'return', 'rx_exec', 'store', 'imag', 'variabl', 'go', 'wrong', 'line', 'script', 'import', 'work', 'test', 'python', 'script', 'sql', 'server', 'directli', 'help', 'appreci', 'thank']"
204,217,217,17230057,72789246,Combine numerical and categorical data within the same feature for machine learning,"<p>I want to predict concept labels given to specific records, but have trouble handling the variation these records have, in particularly the presence (or absence) of values for specific features because of mixed data.</p>
<p>I have a dataset in which we want to cluster data and label these clusters for easier retrieval and eventually further detailed processing. We therefore group by the parameter label provided which is a short description of the concept, but with a large variation. We therefore want to include summary statistics such as mean values and most frequent values to improve these as concept-specific words are often repeated in meta-records which are not important.</p>
<p>Unfortunately, the underlying data is sometimes numeric, sometimes categorical, resulting in mixed data for the aggregated features of most frequent values, and missing values for features such as the mean value. Imputing these missing values seems impossible as they should not have a mean to begin with. The most frequent value has mixed data with frequently large gaps between numerical values (e.g. parameters close to 0 - 10, but sometimes closer to 100 - 500, or even &gt; 20k) so binning seems impractical as well.</p>
<p>Example data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text_description</th>
<th>timestamp</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text example one describing data</td>
<td>0</td>
<td>80</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>2</td>
<td>70</td>
</tr>
<tr>
<td>Text example one describing data</td>
<td>3</td>
<td>2000</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>0</td>
<td>Home</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>1</td>
<td>Transit</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>2</td>
<td>Parents</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>3</td>
<td>Transit</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>0</td>
<td>1.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>1</td>
<td>0.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>2</td>
<td>1.0</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>3</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div>
<p>Example aggregated dataframe:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text_description</th>
<th>summary_mean</th>
<th>summary_most_frequent</th>
<th>target_concept_label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text example one describing data</td>
<td>1000</td>
<td>80</td>
<td>Label_One</td>
</tr>
<tr>
<td>Text example two describing data</td>
<td>NaN</td>
<td>Transit</td>
<td>Label_Two</td>
</tr>
<tr>
<td>Text example three describing data</td>
<td>0.8</td>
<td>1.0</td>
<td>Label_Three</td>
</tr>
</tbody>
</table>
</div>
<p>This lower table will be used as input to try to infer the target_concept_label. I've also thought of other approaches like separating the dataset and training three different models for each datatype, but that would mean losing a lot of training data for the text description parts. How would you approach this problem?</p>
",15,0,-2,2,machine-learning;data-processing,2022-06-28 20:56:14,2022-06-28 20:56:14,2022-06-29 19:35:40,i want to predict concept labels given to specific records  but have trouble handling the variation these records have  in particularly the presence  or absence  of values for specific features because of mixed data  i have a dataset in which we want to cluster data and label these clusters for easier retrieval and eventually further detailed processing  we therefore group by the parameter label provided which is a short description of the concept  but with a large variation  we therefore want to include summary statistics such as mean values and most frequent values to improve these as concept specific words are often repeated in meta records which are not important  unfortunately  the underlying data is sometimes numeric  sometimes categorical  resulting in mixed data for the aggregated features of most frequent values  and missing values for features such as the mean value  imputing these missing values seems impossible as they should not have a mean to begin with  the most frequent value has mixed data with frequently large gaps between numerical values  e g  parameters close to      but sometimes closer to      or even  gt  k  so binning seems impractical as well  example data  example aggregated dataframe  this lower table will be used as input to try to infer the target_concept_label  i ve also thought of other approaches like separating the dataset and training three different models for each datatype  but that would mean losing a lot of training data for the text description parts  how would you approach this problem ,combine numerical and categorical data within the same feature for machine learning,want predict concept labels given specific records trouble handling variation records particularly presence absence values specific features mixed data dataset want cluster data label clusters easier retrieval eventually detailed processing therefore group parameter label provided short description concept large variation therefore want include summary statistics mean values frequent values improve concept specific often repeated meta records important unfortunately underlying data sometimes numeric sometimes categorical resulting mixed data aggregated features frequent values missing values features mean value imputing missing values seems impossible mean begin frequent value mixed data frequently large gaps numerical values e g parameters close sometimes closer even gt k binning seems impractical well example data example aggregated dataframe lower table used input try infer target_concept_label also thought approaches like separating dataset training three different models datatype would mean losing lot training data text description parts would approach problem,combine numerical categorical data within feature machine learning,combine numerical categorical data within feature machine learningwant predict concept labels given specific records trouble handling variation records particularly presence absence values specific features mixed data dataset want cluster data label clusters easier retrieval eventually detailed processing therefore group parameter label provided short description concept large variation therefore want include summary statistics mean values frequent values improve concept specific often repeated meta records important unfortunately underlying data sometimes numeric sometimes categorical resulting mixed data aggregated features frequent values missing values features mean value imputing missing values seems impossible mean begin frequent value mixed data frequently large gaps numerical values e g parameters close sometimes closer even gt k binning seems impractical well example data example aggregated dataframe lower table used input try infer target_concept_label also thought approaches like separating dataset training three different models datatype would mean losing lot training data text description parts would approach problem,"['combine', 'numerical', 'categorical', 'data', 'within', 'feature', 'machine', 'learningwant', 'predict', 'concept', 'labels', 'given', 'specific', 'records', 'trouble', 'handling', 'variation', 'records', 'particularly', 'presence', 'absence', 'values', 'specific', 'features', 'mixed', 'data', 'dataset', 'want', 'cluster', 'data', 'label', 'clusters', 'easier', 'retrieval', 'eventually', 'detailed', 'processing', 'therefore', 'group', 'parameter', 'label', 'provided', 'short', 'description', 'concept', 'large', 'variation', 'therefore', 'want', 'include', 'summary', 'statistics', 'mean', 'values', 'frequent', 'values', 'improve', 'concept', 'specific', 'often', 'repeated', 'meta', 'records', 'important', 'unfortunately', 'underlying', 'data', 'sometimes', 'numeric', 'sometimes', 'categorical', 'resulting', 'mixed', 'data', 'aggregated', 'features', 'frequent', 'values', 'missing', 'values', 'features', 'mean', 'value', 'imputing', 'missing', 'values', 'seems', 'impossible', 'mean', 'begin', 'frequent', 'value', 'mixed', 'data', 'frequently', 'large', 'gaps', 'numerical', 'values', 'e', 'g', 'parameters', 'close', 'sometimes', 'closer', 'even', 'gt', 'k', 'binning', 'seems', 'impractical', 'well', 'example', 'data', 'example', 'aggregated', 'dataframe', 'lower', 'table', 'used', 'input', 'try', 'infer', 'target_concept_label', 'also', 'thought', 'approaches', 'like', 'separating', 'dataset', 'training', 'three', 'different', 'models', 'datatype', 'would', 'mean', 'losing', 'lot', 'training', 'data', 'text', 'description', 'parts', 'would', 'approach', 'problem']","['combin', 'numer', 'categor', 'data', 'within', 'featur', 'machin', 'learningw', 'predict', 'concept', 'label', 'given', 'specif', 'record', 'troubl', 'handl', 'variat', 'record', 'particularli', 'presenc', 'absenc', 'valu', 'specif', 'featur', 'mix', 'data', 'dataset', 'want', 'cluster', 'data', 'label', 'cluster', 'easier', 'retriev', 'eventu', 'detail', 'process', 'therefor', 'group', 'paramet', 'label', 'provid', 'short', 'descript', 'concept', 'larg', 'variat', 'therefor', 'want', 'includ', 'summari', 'statist', 'mean', 'valu', 'frequent', 'valu', 'improv', 'concept', 'specif', 'often', 'repeat', 'meta', 'record', 'import', 'unfortun', 'underli', 'data', 'sometim', 'numer', 'sometim', 'categor', 'result', 'mix', 'data', 'aggreg', 'featur', 'frequent', 'valu', 'miss', 'valu', 'featur', 'mean', 'valu', 'imput', 'miss', 'valu', 'seem', 'imposs', 'mean', 'begin', 'frequent', 'valu', 'mix', 'data', 'frequent', 'larg', 'gap', 'numer', 'valu', 'e', 'g', 'paramet', 'close', 'sometim', 'closer', 'even', 'gt', 'k', 'bin', 'seem', 'impract', 'well', 'exampl', 'data', 'exampl', 'aggreg', 'datafram', 'lower', 'tabl', 'use', 'input', 'tri', 'infer', 'target_concept_label', 'also', 'thought', 'approach', 'like', 'separ', 'dataset', 'train', 'three', 'differ', 'model', 'datatyp', 'would', 'mean', 'lose', 'lot', 'train', 'data', 'text', 'descript', 'part', 'would', 'approach', 'problem']"
205,218,218,19437952,72792101,Are there any models to extract specific data from pdf files?,"<p>For the purpose of my project, I am given large pdfs and need to manually extract one specific value (commission). I am looking for ay machine learning or AI model that would be able to automate this process. The structure of the pdfs vary, so ideally the model would be able to scan the pdf and return the commission percent for any type of pdf. For example the value can be provided in such ways:</p>
<ol>
<li><p>Commission Rate = 20%</p>
</li>
<li><p>The commission rate for this transaction is 20%.</p>
</li>
<li><p>Premium             Commission           Net</p>
<p>50000               20%                  40000</p>
</li>
</ol>
",25,1,-2,3,machine-learning;artificial-intelligence;textdecoder,2022-06-29 00:53:03,2022-06-29 00:53:03,2022-06-29 19:34:24,for the purpose of my project  i am given large pdfs and need to manually extract one specific value  commission   i am looking for ay machine learning or ai model that would be able to automate this process  the structure of the pdfs vary  so ideally the model would be able to scan the pdf and return the commission percent for any type of pdf  for example the value can be provided in such ways  commission rate     the commission rate for this transaction is    premium             commission           net                                   ,are there any models to extract specific data from pdf files ,purpose project given large pdfs need manually extract one specific value commission looking ay machine learning ai model would able automate process structure pdfs vary ideally model would able scan pdf return commission percent type pdf example value provided ways commission rate commission rate transaction premium commission net,models extract specific data pdf files,models extract specific data pdf filespurpose project given large pdfs need manually extract one specific value commission looking ay machine learning ai model would able automate process structure pdfs vary ideally model would able scan pdf return commission percent type pdf example value provided ways commission rate commission rate transaction premium commission net,"['models', 'extract', 'specific', 'data', 'pdf', 'filespurpose', 'project', 'given', 'large', 'pdfs', 'need', 'manually', 'extract', 'one', 'specific', 'value', 'commission', 'looking', 'ay', 'machine', 'learning', 'ai', 'model', 'would', 'able', 'automate', 'process', 'structure', 'pdfs', 'vary', 'ideally', 'model', 'would', 'able', 'scan', 'pdf', 'return', 'commission', 'percent', 'type', 'pdf', 'example', 'value', 'provided', 'ways', 'commission', 'rate', 'commission', 'rate', 'transaction', 'premium', 'commission', 'net']","['model', 'extract', 'specif', 'data', 'pdf', 'filespurpos', 'project', 'given', 'larg', 'pdf', 'need', 'manual', 'extract', 'one', 'specif', 'valu', 'commiss', 'look', 'ay', 'machin', 'learn', 'ai', 'model', 'would', 'abl', 'autom', 'process', 'structur', 'pdf', 'vari', 'ideal', 'model', 'would', 'abl', 'scan', 'pdf', 'return', 'commiss', 'percent', 'type', 'pdf', 'exampl', 'valu', 'provid', 'way', 'commiss', 'rate', 'commiss', 'rate', 'transact', 'premium', 'commiss', 'net']"
206,219,219,19425906,72794079,Python (sklearn) train_test_split: choosing which data to train and which data to test,"<p>I want to use sklearn's train_test_split to manually split data into train and test categories. Specifically, in my .csv file, I want to use all the rows of data until the last row to train, and the last row to test. </p> The reason I'm doing this is because I need to launch a machine learning model but am incredibly short on time. I thought the best way would be to use predictions rather than deploying it using IBM Watson. I don't need it to be live. </p> My code so far looks like this:</p>
<pre><code>df=pd.read_csv('Book5.csv', names=['Amiability', 'Email'])

from sklearn.model_selection import train_test_split

df_x = df['Amiability']
df_y = df['Email']

x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=4) 
</code></pre>
<p>Then,</p>
<pre><code>len(df)
</code></pre>
<p>Produces</p>
<pre><code>331
</code></pre>
<p>I want to train with rows 0-330, and test with row 331. How can I do this?</p>
",39,1,-1,3,python;pandas;scikit-learn,2022-06-29 04:36:45,2022-06-29 04:36:45,2022-06-29 19:31:32,i want to use sklearn s train_test_split to manually split data into train and test categories  specifically  in my  csv file  i want to use all the rows of data until the last row to train  and the last row to test   then  produces i want to train with rows    and test with row   how can i do this ,python  sklearn  train_test_split  choosing which data to train and which data to test,want use sklearn train_test_split manually split data train test categories specifically csv file want use rows data last row train last row test produces want train rows test row,python sklearn train_test_split choosing data train data test,python sklearn train_test_split choosing data train data testwant use sklearn train_test_split manually split data train test categories specifically csv file want use rows data last row train last row test produces want train rows test row,"['python', 'sklearn', 'train_test_split', 'choosing', 'data', 'train', 'data', 'testwant', 'use', 'sklearn', 'train_test_split', 'manually', 'split', 'data', 'train', 'test', 'categories', 'specifically', 'csv', 'file', 'want', 'use', 'rows', 'data', 'last', 'row', 'train', 'last', 'row', 'test', 'produces', 'want', 'train', 'rows', 'test', 'row']","['python', 'sklearn', 'train_test_split', 'choos', 'data', 'train', 'data', 'testwant', 'use', 'sklearn', 'train_test_split', 'manual', 'split', 'data', 'train', 'test', 'categori', 'specif', 'csv', 'file', 'want', 'use', 'row', 'data', 'last', 'row', 'train', 'last', 'row', 'test', 'produc', 'want', 'train', 'row', 'test', 'row']"
207,221,221,15797456,72794761,Displaying an Image with Matplotlib in deeplearning,"<p>Running this piece of code:</p>
<pre><code>#loading test images
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
test_path,image_size=(img_height,img_width), label_mode='int', 
batch_size=batch_size)
                                                       
import matplotlib.pyplot as plt

plt.figure(figsize=(12,12))
for img ,label in val_ds.take(1):
    for i in range(12):
        ax = plt.subplot(4,3,i + 1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(class_name[label[i]])
        plt.axis('off')
</code></pre>
<p>I got this error :</p>
<blockquote>
<p>NotFoundError: NewRandomAccessFile failed to Create/Open: D:\Machine
Learning\DPL\Deep Learning for Computer Vision with TensorFlow
2[TutsNode.com] - Deep Learning for Computer Vision with TensorFlow
2\3. Convolutional Neural Networks\15.1
covid19\covid19\test\Covid\auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg
: The system cannot find the path specified. ; No such process<br />
[[{{node ReadFile}}]] [Op:IteratorGetNext]</p>
</blockquote>
<p>Any help ??</p>
",24,0,0,4,python;tensorflow;keras;deep-learning,2022-06-29 06:52:37,2022-06-29 06:52:37,2022-06-29 18:48:36,running this piece of code  i got this error   any help   ,displaying an image with matplotlib in deeplearning,running piece code got error help,displaying image matplotlib deeplearning,displaying image matplotlib deeplearningrunning piece code got error help,"['displaying', 'image', 'matplotlib', 'deeplearningrunning', 'piece', 'code', 'got', 'error', 'help']","['display', 'imag', 'matplotlib', 'deeplearningrun', 'piec', 'code', 'got', 'error', 'help']"
208,222,222,19436129,72789256,Jupyter-notebook fails to start | PyForMLS,"<p>I am currently setting up a Python data-science client for SQL Server Machine Learning Services following this guide: <a href=""https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15</a></p>
<p>Unfortunately, running Jupyter notebooks for this distribution does not seem to work for me: Typing <code>.\Scripts\jupyter-notebook</code> in the distribution folder, or directly running jupyter-notebook.exe from the Scripts sub-folder does not start Jupyter. In the terminal, the command exits with no ouput.
Afterwards, https://localhost:8889/tree is not reachable as should be the case according to the tutorial above.</p>
<p>Any suggestions? (I already checked <a href=""https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html"" rel=""nofollow noreferrer"">https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html</a> for solutions). Thank you!</p>
<hr />
<p>Update: At least <code>.\Scripts\jupyter-console</code> is running, though it is not the same experience.</p>
",48,1,0,2,jupyter-notebook;azure-machine-learning-service,2022-06-28 20:56:50,2022-06-28 20:56:50,2022-06-29 18:03:35,i am currently setting up a python data science client for sql server machine learning services following this guide   any suggestions   i already checked  for solutions   thank you  update  at least   scripts jupyter console is running  though it is not the same experience ,jupyter notebook fails to start   pyformls,currently setting python data science client sql server machine learning services following guide suggestions already checked solutions thank update least scripts jupyter console running though experience,jupyter notebook fails start pyformls,jupyter notebook fails start pyformlscurrently setting python data science client sql server machine learning services following guide suggestions already checked solutions thank update least scripts jupyter console running though experience,"['jupyter', 'notebook', 'fails', 'start', 'pyformlscurrently', 'setting', 'python', 'data', 'science', 'client', 'sql', 'server', 'machine', 'learning', 'services', 'following', 'guide', 'suggestions', 'already', 'checked', 'solutions', 'thank', 'update', 'least', 'scripts', 'jupyter', 'console', 'running', 'though', 'experience']","['jupyt', 'notebook', 'fail', 'start', 'pyformlscurr', 'set', 'python', 'data', 'scienc', 'client', 'sql', 'server', 'machin', 'learn', 'servic', 'follow', 'guid', 'suggest', 'alreadi', 'check', 'solut', 'thank', 'updat', 'least', 'script', 'jupyt', 'consol', 'run', 'though', 'experi']"
209,223,223,16982484,72801120,why does scaling improve the accuracy of my model when all my features are in the same range,"<p>so i am new to machine learning and i learnt that scaling is used when two features have different scales and hence one comes of as more weighted than others, but in MNIST dataset, all the features have values from 0 to 255, then why does standard scaling improve accuracy?</p>
",32,1,0,3,machine-learning;data-science;data-preprocessing,2022-06-29 17:33:15,2022-06-29 17:33:15,2022-06-29 17:39:38,so i am new to machine learning and i learnt that scaling is used when two features have different scales and hence one comes of as more weighted than others  but in mnist dataset  all the features have values from  to   then why does standard scaling improve accuracy ,why does scaling improve the accuracy of my model when all my features are in the same range,machine learning learnt scaling used two features different scales hence one comes weighted others mnist dataset features values standard scaling improve accuracy,scaling improve accuracy model features range,scaling improve accuracy model features rangemachine learning learnt scaling used two features different scales hence one comes weighted others mnist dataset features values standard scaling improve accuracy,"['scaling', 'improve', 'accuracy', 'model', 'features', 'rangemachine', 'learning', 'learnt', 'scaling', 'used', 'two', 'features', 'different', 'scales', 'hence', 'one', 'comes', 'weighted', 'others', 'mnist', 'dataset', 'features', 'values', 'standard', 'scaling', 'improve', 'accuracy']","['scale', 'improv', 'accuraci', 'model', 'featur', 'rangemachin', 'learn', 'learnt', 'scale', 'use', 'two', 'featur', 'differ', 'scale', 'henc', 'one', 'come', 'weight', 'other', 'mnist', 'dataset', 'featur', 'valu', 'standard', 'scale', 'improv', 'accuraci']"
210,224,224,283538,72799969,install tensorflow-decision-forests in windows,"<p>I have to install tensorflow-decision-forests in windows. I tried:</p>
<pre><code>pip install tensorflow-decision-forests
pip3 install tensorflow-decision-forests
pip3 install tensorflow_decision_forests --upgrade
</code></pre>
<p>I get:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement tensorflow-decision-forests (from versions: none)
ERROR: No matching distribution found for tensorflow-decision-forests
</code></pre>
<p>I have (pip show tensorflow):</p>
<pre><code>Name: tensorflow
Version: 2.9.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
</code></pre>
<p>AFIK this is the latest version amy ideas?</p>
",39,1,1,4,python;tensorflow;pip;tensorflow-decision-forests,2022-06-29 16:07:17,2022-06-29 16:07:17,2022-06-29 16:35:17,i have to install tensorflow decision forests in windows  i tried  i get  i have  pip show tensorflow   afik this is the latest version amy ideas ,install tensorflow decision forests in windows,install tensorflow decision forests windows tried get pip show tensorflow afik latest version amy ideas,install tensorflow decision forests windows,install tensorflow decision forests windowsinstall tensorflow decision forests windows tried get pip show tensorflow afik latest version amy ideas,"['install', 'tensorflow', 'decision', 'forests', 'windowsinstall', 'tensorflow', 'decision', 'forests', 'windows', 'tried', 'get', 'pip', 'show', 'tensorflow', 'afik', 'latest', 'version', 'amy', 'ideas']","['instal', 'tensorflow', 'decis', 'forest', 'windowsinstal', 'tensorflow', 'decis', 'forest', 'window', 'tri', 'get', 'pip', 'show', 'tensorflow', 'afik', 'latest', 'version', 'ami', 'idea']"
211,225,225,11572260,70576271,Relationship between Automata theory and Dynamic programming,"<p>I am learning automata theory. I think that there must be some relationship between state machine theory and dynamic programming. for the reason that the essence of dynamic programming is state transition equations. And automata theory is also deal with the problems about the transition between different states. Furthermore, since we can analysis string matching algorithms through the method using the state machine, I vaguely think that state machine can also help us understand DP more clearly.</p>
<p>However, I find there are few materials talking about this. I think that if we can apply automata theory to the process of learning dynamic programming techniques, it can help us understand so many DP problems which are usually too difficult and complex for us to understand.</p>
<p>Can anyone recommend some materials which are talking about the applications of automata theory in dynamic programming algorithms' designing or the analysis of DP problems using state machine theory?</p>
<p>ps. the terminology 'dynamic programming' here just refers to our normal understanding of that algorithm designing tricks, not that mathematics field which is created by Bellman.</p>
",73,0,2,3,dynamic-programming;state-machine;automata-theory,2022-01-04 14:27:16,2022-01-04 14:27:16,2022-06-29 15:30:08,i am learning automata theory  i think that there must be some relationship between state machine theory and dynamic programming  for the reason that the essence of dynamic programming is state transition equations  and automata theory is also deal with the problems about the transition between different states  furthermore  since we can analysis string matching algorithms through the method using the state machine  i vaguely think that state machine can also help us understand dp more clearly  however  i find there are few materials talking about this  i think that if we can apply automata theory to the process of learning dynamic programming techniques  it can help us understand so many dp problems which are usually too difficult and complex for us to understand  can anyone recommend some materials which are talking about the applications of automata theory in dynamic programming algorithms  designing or the analysis of dp problems using state machine theory  ps  the terminology  dynamic programming  here just refers to our normal understanding of that algorithm designing tricks  not that mathematics field which is created by bellman ,relationship between automata theory and dynamic programming,learning automata theory think must relationship state machine theory dynamic programming reason essence dynamic programming state transition equations automata theory also deal problems transition different states furthermore since analysis string matching algorithms method using state machine vaguely think state machine also help us understand dp clearly however find materials talking think apply automata theory process learning dynamic programming techniques help us understand many dp problems usually difficult complex us understand anyone recommend materials talking applications automata theory dynamic programming algorithms designing analysis dp problems using state machine theory ps terminology dynamic programming refers normal understanding algorithm designing tricks mathematics field created bellman,relationship automata theory dynamic programming,relationship automata theory dynamic programminglearning automata theory think must relationship state machine theory dynamic programming reason essence dynamic programming state transition equations automata theory also deal problems transition different states furthermore since analysis string matching algorithms method using state machine vaguely think state machine also help us understand dp clearly however find materials talking think apply automata theory process learning dynamic programming techniques help us understand many dp problems usually difficult complex us understand anyone recommend materials talking applications automata theory dynamic programming algorithms designing analysis dp problems using state machine theory ps terminology dynamic programming refers normal understanding algorithm designing tricks mathematics field created bellman,"['relationship', 'automata', 'theory', 'dynamic', 'programminglearning', 'automata', 'theory', 'think', 'must', 'relationship', 'state', 'machine', 'theory', 'dynamic', 'programming', 'reason', 'essence', 'dynamic', 'programming', 'state', 'transition', 'equations', 'automata', 'theory', 'also', 'deal', 'problems', 'transition', 'different', 'states', 'furthermore', 'since', 'analysis', 'string', 'matching', 'algorithms', 'method', 'using', 'state', 'machine', 'vaguely', 'think', 'state', 'machine', 'also', 'help', 'us', 'understand', 'dp', 'clearly', 'however', 'find', 'materials', 'talking', 'think', 'apply', 'automata', 'theory', 'process', 'learning', 'dynamic', 'programming', 'techniques', 'help', 'us', 'understand', 'many', 'dp', 'problems', 'usually', 'difficult', 'complex', 'us', 'understand', 'anyone', 'recommend', 'materials', 'talking', 'applications', 'automata', 'theory', 'dynamic', 'programming', 'algorithms', 'designing', 'analysis', 'dp', 'problems', 'using', 'state', 'machine', 'theory', 'ps', 'terminology', 'dynamic', 'programming', 'refers', 'normal', 'understanding', 'algorithm', 'designing', 'tricks', 'mathematics', 'field', 'created', 'bellman']","['relationship', 'automata', 'theori', 'dynam', 'programminglearn', 'automata', 'theori', 'think', 'must', 'relationship', 'state', 'machin', 'theori', 'dynam', 'program', 'reason', 'essenc', 'dynam', 'program', 'state', 'transit', 'equat', 'automata', 'theori', 'also', 'deal', 'problem', 'transit', 'differ', 'state', 'furthermor', 'sinc', 'analysi', 'string', 'match', 'algorithm', 'method', 'use', 'state', 'machin', 'vagu', 'think', 'state', 'machin', 'also', 'help', 'us', 'understand', 'dp', 'clearli', 'howev', 'find', 'materi', 'talk', 'think', 'appli', 'automata', 'theori', 'process', 'learn', 'dynam', 'program', 'techniqu', 'help', 'us', 'understand', 'mani', 'dp', 'problem', 'usual', 'difficult', 'complex', 'us', 'understand', 'anyon', 'recommend', 'materi', 'talk', 'applic', 'automata', 'theori', 'dynam', 'program', 'algorithm', 'design', 'analysi', 'dp', 'problem', 'use', 'state', 'machin', 'theori', 'ps', 'terminolog', 'dynam', 'program', 'refer', 'normal', 'understand', 'algorithm', 'design', 'trick', 'mathemat', 'field', 'creat', 'bellman']"
212,226,226,19016900,72798613,Unmet Dependencies while trying to install the Microsoft mssql-mlservices-packages-py Machine Learning package on Ubuntu 21.10,"<p>I followed the instructions from <a href=""https://t.ly/KUFs"" rel=""nofollow noreferrer"">Install MS ML</a> to install the Microsoft Machine Learning Services on Ubuntu 21.10. I started with a fresh installed version of Ubuntu 21.10, installing the updates and required files. I also installed SQL Server 2019 on another Server and have full access to connect to it via ISQL. Nevertheless, when I reached the point to install the actual ML Installation files, I am met with a File Dependency error:</p>
<pre><code>johnny@JSML:~$ sudo apt-get install mssql-mlservices-mlm-py
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
mssql-mlservices-packages-py : Depends: microsoft-openmpi (&gt;= 3.0.0) but it is not installable
Depends: mssql-server-extensibility (&gt;= 15.0.2000) but it is not installable
E: Unable to correct problems, you have held broken packages.
</code></pre>
<p>Tried to manually install the Depends file to no avail. I am not sure how to repair this issue.  Any guidance would be appreciated.</p>
",24,0,-1,2,sql-server;machine-learning,2022-06-29 14:30:57,2022-06-29 14:30:57,2022-06-29 14:30:57,i followed the instructions from  to install the microsoft machine learning services on ubuntu    i started with a fresh installed version of ubuntu    installing the updates and required files  i also installed sql server  on another server and have full access to connect to it via isql  nevertheless  when i reached the point to install the actual ml installation files  i am met with a file dependency error  tried to manually install the depends file to no avail  i am not sure how to repair this issue   any guidance would be appreciated ,unmet dependencies while trying to install the microsoft mssql mlservices packages py machine learning package on ubuntu  ,followed instructions install microsoft machine learning services ubuntu started fresh installed version ubuntu installing updates required files also installed sql server another server full access connect via isql nevertheless reached point install actual ml installation files met file dependency error tried manually install depends file avail sure repair issue guidance would appreciated,unmet dependencies trying install microsoft mssql mlservices packages py machine learning package ubuntu,unmet dependencies trying install microsoft mssql mlservices packages py machine learning package ubuntufollowed instructions install microsoft machine learning services ubuntu started fresh installed version ubuntu installing updates required files also installed sql server another server full access connect via isql nevertheless reached point install actual ml installation files met file dependency error tried manually install depends file avail sure repair issue guidance would appreciated,"['unmet', 'dependencies', 'trying', 'install', 'microsoft', 'mssql', 'mlservices', 'packages', 'py', 'machine', 'learning', 'package', 'ubuntufollowed', 'instructions', 'install', 'microsoft', 'machine', 'learning', 'services', 'ubuntu', 'started', 'fresh', 'installed', 'version', 'ubuntu', 'installing', 'updates', 'required', 'files', 'also', 'installed', 'sql', 'server', 'another', 'server', 'full', 'access', 'connect', 'via', 'isql', 'nevertheless', 'reached', 'point', 'install', 'actual', 'ml', 'installation', 'files', 'met', 'file', 'dependency', 'error', 'tried', 'manually', 'install', 'depends', 'file', 'avail', 'sure', 'repair', 'issue', 'guidance', 'would', 'appreciated']","['unmet', 'depend', 'tri', 'instal', 'microsoft', 'mssql', 'mlservic', 'packag', 'py', 'machin', 'learn', 'packag', 'ubuntufollow', 'instruct', 'instal', 'microsoft', 'machin', 'learn', 'servic', 'ubuntu', 'start', 'fresh', 'instal', 'version', 'ubuntu', 'instal', 'updat', 'requir', 'file', 'also', 'instal', 'sql', 'server', 'anoth', 'server', 'full', 'access', 'connect', 'via', 'isql', 'nevertheless', 'reach', 'point', 'instal', 'actual', 'ml', 'instal', 'file', 'met', 'file', 'depend', 'error', 'tri', 'manual', 'instal', 'depend', 'file', 'avail', 'sure', 'repair', 'issu', 'guidanc', 'would', 'appreci']"
213,227,227,15797456,72794225,Tensorboard Callback,"<pre><code>from tensorflow.keras.callbacks 
import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

early_stopping =EarlyStopping(patience=5, monitor='val_loss', mode='min')
learning_rate_reduction = ReduceLROnPlateau(patience=4,monitor='val_loss', factor=0.1)

#path for saving the best weights
checkpoint_best_path =r'C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Model\Model best checkpoint'

check_bestpoint = ModelCheckpoint(filepath=checkpoint_best_path, save_weights_only=True, save_freq='epoch',
                                 monitor='val_loss',save_best_only=True, verbose=1)


#tensorboard path
log_dir_path =r'C:\Users\Administrator\Desktop\Desktop\Work\Data Science\Practice\Machine Learning\Deep Learning\Custom Deep Learning\Deep Learning with Computer vision\Basics\Covid 19 Project\Data\logs/' 
log_dir = os.path.join(log_dir_path, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)

callbacks= [early_stopping, learning_rate_reduction,check_bestpoint,tensorboard_callback]
</code></pre>
<p>but I got an error stating :</p>
<blockquote>
<p>NotFoundError: Failed to create a NewWriteableFile:
C:\Users\Administrator\Desktop\Desktop\Work\Data
Science\Practice\Machine Learning\Deep Learning\Custom Deep
Learning\Deep Learning with Computer vision\Basics\Covid 19
Project\Data\logs/20220628-231734\train/events.out.tfevents.1656501454.DESKTOP-1ULGF16.14344.2.v2
: The system cannot find the path specified. ; No such process
Creating writable file
C:\Users\Administrator\Desktop\Desktop\Work\Data
Science\Practice\Machine Learning\Deep Learning\Custom Deep
Learning\Deep Learning with Computer vision\Basics\Covid 19
Project\Data\logs/20220628-231734\train/events.out.tfevents.1656501454.DESKTOP-1ULGF16.14344.2.v2
Could not initialize events writer. [Op:CreateSummaryFileWriter]</p>
</blockquote>
",28,1,0,5,python;tensorflow;machine-learning;keras;deep-learning,2022-06-29 05:02:40,2022-06-29 05:02:40,2022-06-29 14:21:31,but i got an error stating  ,tensorboard callback,got error stating,tensorboard callback,tensorboard callbackgot error stating,"['tensorboard', 'callbackgot', 'error', 'stating']","['tensorboard', 'callbackgot', 'error', 'state']"
214,228,228,19441122,72797110,Issue while installing a lower python version in conda prompt,"<p>I have python version 3.8.8 in conda. I want version 3.5.0 for a Machine learning project but when I ran the command <code>conda install python=3.5.0</code>, following output came:</p>
<p>**Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.</p>
<p>PackagesNotFoundError: The following packages are not available from current channels:</p>
<ul>
<li>python=3.5.0</li>
</ul>
<p>Current channels:</p>
<ul>
<li><a href=""https://repo.anaconda.com/pkgs/main/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/msys2/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/msys2/noarch</a></li>
</ul>
<p>To search for alternate channels that may provide the conda package you're
looking for, navigate to</p>
<pre><code>https://anaconda.org
</code></pre>
<p>and use the search bar at the top of the page.**</p>
<p>I am not able to comprehend this. How can I install the required python version?</p>
",12,1,0,2,python-3.x;anaconda,2022-06-29 12:34:46,2022-06-29 12:34:46,2022-06-29 12:54:22,i have python version    in conda  i want version    for a machine learning project but when i ran the command conda install python     following output came  packagesnotfounderror  the following packages are not available from current channels  current channels  and use the search bar at the top of the page    i am not able to comprehend this  how can i install the required python version ,issue while installing a lower python version in conda prompt,python version conda want version machine learning project ran command conda install python following output came packagesnotfounderror following packages available current channels current channels use search bar top page able comprehend install required python version,issue installing lower python version conda prompt,issue installing lower python version conda promptpython version conda want version machine learning project ran command conda install python following output came packagesnotfounderror following packages available current channels current channels use search bar top page able comprehend install required python version,"['issue', 'installing', 'lower', 'python', 'version', 'conda', 'promptpython', 'version', 'conda', 'want', 'version', 'machine', 'learning', 'project', 'ran', 'command', 'conda', 'install', 'python', 'following', 'output', 'came', 'packagesnotfounderror', 'following', 'packages', 'available', 'current', 'channels', 'current', 'channels', 'use', 'search', 'bar', 'top', 'page', 'able', 'comprehend', 'install', 'required', 'python', 'version']","['issu', 'instal', 'lower', 'python', 'version', 'conda', 'promptpython', 'version', 'conda', 'want', 'version', 'machin', 'learn', 'project', 'ran', 'command', 'conda', 'instal', 'python', 'follow', 'output', 'came', 'packagesnotfounderror', 'follow', 'packag', 'avail', 'current', 'channel', 'current', 'channel', 'use', 'search', 'bar', 'top', 'page', 'abl', 'comprehend', 'instal', 'requir', 'python', 'version']"
215,229,229,15958905,72794914,Survey bot or Survey Chatbot with Flask and HTML,"<p>So now I'm working on a survey chatbot that requires users to answer my questions. After the said user answered the question, I will append their answers to a pandas dataframe and my machine learning model will predict the disease based on the symptoms. My machine learning model has been saved to json and now I'm really confused on how to make it work with Flask and HTML. I've watched so many chatbot tutorials but they usually just predict the user's input and response right away. I want my chatbot to ask questions and append user's inputs in a loop before predict it all. Here's my code snippet so far.</p>
<pre><code>from flask import Flask, render_template, request
from xgboost import XGBClassifier

app = Flask(__name__)

@app.route('/welcome')

def home():
    return render_template('home.html')

@app.route('/chatbot')
def get_user():
    name=request.args.get('name')
    gender=request.form['gender']
    age=request.form['age']
    hypertension=request.form['hypertension']
    heart_disease=request.form['heart_disease']
    ever_married=request.form['ever_married']
    work_type=request.form['work_type']
    residence_type=request.form['residence_type']
    weight=request.form['weight']
    height=request.form['height']
    smoking=request.form['smoking']
    return render_template('fortunecookie.html',name=name)

def predict():
    data=pd.DataFrame({'gender':[gender],
                             'age':[age],
                             'hypertension':[hypertension],
                             'heart_disease':[heart_disease],
                             'ever_married':[ever_married],
                             'work_type':[work_type],
                             'residence_type':[residence_type],
                             'bmi':[weight/(height/100)**2],
                             'smoking':[smoking]})
    model=XGBClassifier.load_model(&quot;xgbimbalance.json&quot;)
    predresult=model.predict(data.values)

if __name__==&quot;__main__&quot;:
    app.run()
</code></pre>
<p>Thank you in advance, I'm sorry if it's a bit confusing but the point is to design a survey bot with machine learning to predict the user's input after all the questions are answered.</p>
",27,0,-1,2,python;flask,2022-06-29 07:18:53,2022-06-29 07:18:53,2022-06-29 09:29:31,so now i m working on a survey chatbot that requires users to answer my questions  after the said user answered the question  i will append their answers to a pandas dataframe and my machine learning model will predict the disease based on the symptoms  my machine learning model has been saved to json and now i m really confused on how to make it work with flask and html  i ve watched so many chatbot tutorials but they usually just predict the user s input and response right away  i want my chatbot to ask questions and append user s inputs in a loop before predict it all  here s my code snippet so far  thank you in advance  i m sorry if it s a bit confusing but the point is to design a survey bot with machine learning to predict the user s input after all the questions are answered ,survey bot or survey chatbot with flask and html,working survey chatbot requires users answer questions said user answered question append answers pandas dataframe machine learning model predict disease based symptoms machine learning model saved json really confused make work flask html watched many chatbot tutorials usually predict user input response right away want chatbot ask questions append user inputs loop predict code snippet far thank advance sorry bit confusing point design survey bot machine learning predict user input questions answered,survey bot survey chatbot flask html,survey bot survey chatbot flask htmlworking survey chatbot requires users answer questions said user answered question append answers pandas dataframe machine learning model predict disease based symptoms machine learning model saved json really confused make work flask html watched many chatbot tutorials usually predict user input response right away want chatbot ask questions append user inputs loop predict code snippet far thank advance sorry bit confusing point design survey bot machine learning predict user input questions answered,"['survey', 'bot', 'survey', 'chatbot', 'flask', 'htmlworking', 'survey', 'chatbot', 'requires', 'users', 'answer', 'questions', 'said', 'user', 'answered', 'question', 'append', 'answers', 'pandas', 'dataframe', 'machine', 'learning', 'model', 'predict', 'disease', 'based', 'symptoms', 'machine', 'learning', 'model', 'saved', 'json', 'really', 'confused', 'make', 'work', 'flask', 'html', 'watched', 'many', 'chatbot', 'tutorials', 'usually', 'predict', 'user', 'input', 'response', 'right', 'away', 'want', 'chatbot', 'ask', 'questions', 'append', 'user', 'inputs', 'loop', 'predict', 'code', 'snippet', 'far', 'thank', 'advance', 'sorry', 'bit', 'confusing', 'point', 'design', 'survey', 'bot', 'machine', 'learning', 'predict', 'user', 'input', 'questions', 'answered']","['survey', 'bot', 'survey', 'chatbot', 'flask', 'htmlwork', 'survey', 'chatbot', 'requir', 'user', 'answer', 'question', 'said', 'user', 'answer', 'question', 'append', 'answer', 'panda', 'datafram', 'machin', 'learn', 'model', 'predict', 'diseas', 'base', 'symptom', 'machin', 'learn', 'model', 'save', 'json', 'realli', 'confus', 'make', 'work', 'flask', 'html', 'watch', 'mani', 'chatbot', 'tutori', 'usual', 'predict', 'user', 'input', 'respons', 'right', 'away', 'want', 'chatbot', 'ask', 'question', 'append', 'user', 'input', 'loop', 'predict', 'code', 'snippet', 'far', 'thank', 'advanc', 'sorri', 'bit', 'confus', 'point', 'design', 'survey', 'bot', 'machin', 'learn', 'predict', 'user', 'input', 'question', 'answer']"
216,232,232,18204980,72767066,Stratified sampling getting odd proportion of factor levels,"<p>I'm experimenting with the Dry Bean dataset from UCI machine learning repo. I want to iterate through the dataset, repeatedly removing samples and running classifiers to see how accuracy changes as sample size decreases, but first I make the loop and check it works.</p>
<p>The dataset:</p>
<pre><code>&gt; summary(DryBean)
      Area          Perimeter      MajorAxisLength MinorAxisLength  AspectRation    Eccentricity      ConvexArea     EquivDiameter       Extent          Solidity     
 Min.   : 20420   Min.   : 524.7   Min.   :183.6   Min.   :122.5   Min.   :1.025   Min.   :0.2190   Min.   : 20684   Min.   :161.2   Min.   :0.5553   Min.   :0.9192  
 1st Qu.: 36328   1st Qu.: 703.5   1st Qu.:253.3   1st Qu.:175.8   1st Qu.:1.432   1st Qu.:0.7159   1st Qu.: 36715   1st Qu.:215.1   1st Qu.:0.7186   1st Qu.:0.9857  
 Median : 44652   Median : 794.9   Median :296.9   Median :192.4   Median :1.551   Median :0.7644   Median : 45178   Median :238.4   Median :0.7599   Median :0.9883  
 Mean   : 53048   Mean   : 855.3   Mean   :320.1   Mean   :202.3   Mean   :1.583   Mean   :0.7509   Mean   : 53768   Mean   :253.1   Mean   :0.7497   Mean   :0.9871  
 3rd Qu.: 61332   3rd Qu.: 977.2   3rd Qu.:376.5   3rd Qu.:217.0   3rd Qu.:1.707   3rd Qu.:0.8105   3rd Qu.: 62294   3rd Qu.:279.4   3rd Qu.:0.7869   3rd Qu.:0.9900  
 Max.   :254616   Max.   :1985.4   Max.   :738.9   Max.   :460.2   Max.   :2.430   Max.   :0.9114   Max.   :263261   Max.   :569.4   Max.   :0.8662   Max.   :0.9947  
                                                                                                                                                                      
   roundness       Compactness      ShapeFactor1       ShapeFactor2        ShapeFactor3     ShapeFactor4         Class     
 Min.   :0.4896   Min.   :0.6406   Min.   :0.002778   Min.   :0.0005642   Min.   :0.4103   Min.   :0.9477   BARBUNYA:1322  
 1st Qu.:0.8321   1st Qu.:0.7625   1st Qu.:0.005900   1st Qu.:0.0011535   1st Qu.:0.5814   1st Qu.:0.9937   BOMBAY  : 522  
 Median :0.8832   Median :0.8013   Median :0.006645   Median :0.0016935   Median :0.6420   Median :0.9964   CALI    :1630  
 Mean   :0.8733   Mean   :0.7999   Mean   :0.006564   Mean   :0.0017159   Mean   :0.6436   Mean   :0.9951   DERMASON:3546  
 3rd Qu.:0.9169   3rd Qu.:0.8343   3rd Qu.:0.007271   3rd Qu.:0.0021703   3rd Qu.:0.6960   3rd Qu.:0.9979   HOROZ   :1928  
 Max.   :0.9907   Max.   :0.9873   Max.   :0.010451   Max.   :0.0036650   Max.   :0.9748   Max.   :0.9997   SEKER   :2027  
                                                                                                            SIRA    :2636  
</code></pre>
<p>I set the Class variable to a factor, then found the proportion of levels:</p>
<pre><code>&gt; prop.table(table(DryBean$Class))

  BARBUNYA     BOMBAY       CALI   DERMASON      HOROZ      SEKER       SIRA 
0.09712732 0.03835133 0.11975608 0.26052458 0.14165014 0.14892366 0.19366689
</code></pre>
<p>I created a copy of the data then used a loop to remove samples in those proportions:</p>
<pre><code>while(nrow(beanclone) &gt; 100) {
  mysample &lt;- stratified(beanclone, &quot;Class&quot;, size = c(&quot;BARBUNYA&quot; = 0.09712732 , &quot;BOMBAY&quot; = 0.03835133 , &quot;CALI&quot; = 0.11975608 , &quot;DERMASON&quot; = 0.26052458 , &quot;HOROZ&quot; = 0.14165014 , &quot;SEKER&quot; = 0.14892366 , &quot;SIRA&quot; = 0.19366689 ), keep.rownames = TRUE)
  beanclone &lt;- beanclone[!seq_len(nrow(beanclone)) %in% mysample$rn,]
  print(table(beanclone$Class))
  print(nrow(beanclone))
}
</code></pre>
<p>However, the first few iterations of the loop had this output:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1271      459     1205     2859     1655     1830     2243 
[1] 11522

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1222      404      891     2305     1421     1652     1909 
[1] 9804

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
    1175      356      659     1859     1220     1492     1625 
[1] 8386
</code></pre>
<p>The Barbunya level is losing samples much more slowly than the rest, ultimately ending at this distribution:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
      80        4        1        2        3        5        3 
[1] 98
</code></pre>
<p>Testing with the size being set to different integers for each level showed that the loop was associating the correct size label with the level, i.e. setting &quot;Barbunya&quot; = 3 would result in barbunya losing 3 samples at each iteration, so why is it changing so slowly when going by proportion?</p>
<p>Edit:</p>
<p>Changing some of the proportion decimals has shown some unexpected results.
Barbunya 0.09712732 -&gt; 0.5 ended the loop at:</p>
<pre><code>BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
     89        4        1        2        3        1        3 
[1] 103

BARBUNYA   BOMBAY     CALI DERMASON    HOROZ    SEKER     SIRA 
     86        4        1        2        3        1        3 
[1] 100
</code></pre>
<p>Which is a negligible increase in the amount of Barbunya being removed at each iteration.</p>
",53,0,3,3,r;while-loop;sampling,2022-06-27 10:09:27,2022-06-27 10:09:27,2022-06-29 04:12:48,i m experimenting with the dry bean dataset from uci machine learning repo  i want to iterate through the dataset  repeatedly removing samples and running classifiers to see how accuracy changes as sample size decreases  but first i make the loop and check it works  the dataset  i set the class variable to a factor  then found the proportion of levels  i created a copy of the data then used a loop to remove samples in those proportions  however  the first few iterations of the loop had this output  the barbunya level is losing samples much more slowly than the rest  ultimately ending at this distribution  testing with the size being set to different integers for each level showed that the loop was associating the correct size label with the level  i e  setting  barbunya     would result in barbunya losing  samples at each iteration  so why is it changing so slowly when going by proportion  edit  which is a negligible increase in the amount of barbunya being removed at each iteration ,stratified sampling getting odd proportion of factor levels,experimenting dry bean dataset uci machine learning repo want iterate dataset repeatedly removing samples running classifiers see accuracy changes sample size decreases first make loop check works dataset set class variable factor found proportion levels created copy data used loop remove samples proportions however first iterations loop output barbunya level losing samples much slowly rest ultimately ending distribution testing size set different integers level showed loop associating correct size label level e setting barbunya would result barbunya losing samples iteration changing slowly going proportion edit negligible increase amount barbunya removed iteration,stratified sampling getting odd proportion factor levels,stratified sampling getting odd proportion factor levelsexperimenting dry bean dataset uci machine learning repo want iterate dataset repeatedly removing samples running classifiers see accuracy changes sample size decreases first make loop check works dataset set class variable factor found proportion levels created copy data used loop remove samples proportions however first iterations loop output barbunya level losing samples much slowly rest ultimately ending distribution testing size set different integers level showed loop associating correct size label level e setting barbunya would result barbunya losing samples iteration changing slowly going proportion edit negligible increase amount barbunya removed iteration,"['stratified', 'sampling', 'getting', 'odd', 'proportion', 'factor', 'levelsexperimenting', 'dry', 'bean', 'dataset', 'uci', 'machine', 'learning', 'repo', 'want', 'iterate', 'dataset', 'repeatedly', 'removing', 'samples', 'running', 'classifiers', 'see', 'accuracy', 'changes', 'sample', 'size', 'decreases', 'first', 'make', 'loop', 'check', 'works', 'dataset', 'set', 'class', 'variable', 'factor', 'found', 'proportion', 'levels', 'created', 'copy', 'data', 'used', 'loop', 'remove', 'samples', 'proportions', 'however', 'first', 'iterations', 'loop', 'output', 'barbunya', 'level', 'losing', 'samples', 'much', 'slowly', 'rest', 'ultimately', 'ending', 'distribution', 'testing', 'size', 'set', 'different', 'integers', 'level', 'showed', 'loop', 'associating', 'correct', 'size', 'label', 'level', 'e', 'setting', 'barbunya', 'would', 'result', 'barbunya', 'losing', 'samples', 'iteration', 'changing', 'slowly', 'going', 'proportion', 'edit', 'negligible', 'increase', 'amount', 'barbunya', 'removed', 'iteration']","['stratifi', 'sampl', 'get', 'odd', 'proport', 'factor', 'levelsexperi', 'dri', 'bean', 'dataset', 'uci', 'machin', 'learn', 'repo', 'want', 'iter', 'dataset', 'repeatedli', 'remov', 'sampl', 'run', 'classifi', 'see', 'accuraci', 'chang', 'sampl', 'size', 'decreas', 'first', 'make', 'loop', 'check', 'work', 'dataset', 'set', 'class', 'variabl', 'factor', 'found', 'proport', 'level', 'creat', 'copi', 'data', 'use', 'loop', 'remov', 'sampl', 'proport', 'howev', 'first', 'iter', 'loop', 'output', 'barbunya', 'level', 'lose', 'sampl', 'much', 'slowli', 'rest', 'ultim', 'end', 'distribut', 'test', 'size', 'set', 'differ', 'integ', 'level', 'show', 'loop', 'associ', 'correct', 'size', 'label', 'level', 'e', 'set', 'barbunya', 'would', 'result', 'barbunya', 'lose', 'sampl', 'iter', 'chang', 'slowli', 'go', 'proport', 'edit', 'neglig', 'increas', 'amount', 'barbunya', 'remov', 'iter']"
217,233,233,18201044,72788138,Machine Learning - Train a model using imbalanced data,"<p>I have two classes in my data.</p>
<p>This is how class distribution looks like.</p>
<pre><code>0.0    169072
1.0     84944
</code></pre>
<p>In other words, I have 2:1 class distribution.</p>
<p>I believe I have two choices. Downsample the class <code>0.0</code> or upsample class <code>1.0</code>. If I go with option 1, I'm losing data. If i go with option 2, then I'm using non-real data.</p>
<p>Is there a way, I can train the model without upsample or downsample?</p>
<p>This is how my classification_report looks like.</p>
<pre><code>               precision    recall  f1-score   support

         0.0       0.68      1.00      0.81     51683
         1.0       1.00      0.00      0.00     24522

    accuracy                           0.68     76205
   macro avg       0.84      0.50      0.40     76205
weighted avg       0.78      0.68      0.55     76205  
</code></pre>
",27,2,0,4,scikit-learn;logistic-regression;xgboost;lightgbm,2022-06-28 19:48:30,2022-06-28 19:48:30,2022-06-28 20:09:49,i have two classes in my data  this is how class distribution looks like  in other words  i have   class distribution  i believe i have two choices  downsample the class   or upsample class    if i go with option   i m losing data  if i go with option   then i m using non real data  is there a way  i can train the model without upsample or downsample  this is how my classification_report looks like ,machine learning   train a model using imbalanced data,two classes data class distribution looks like class distribution believe two choices downsample class upsample class go option losing data go option using non real data way train model without upsample downsample classification_report looks like,machine learning train model using imbalanced data,machine learning train model using imbalanced datatwo classes data class distribution looks like class distribution believe two choices downsample class upsample class go option losing data go option using non real data way train model without upsample downsample classification_report looks like,"['machine', 'learning', 'train', 'model', 'using', 'imbalanced', 'datatwo', 'classes', 'data', 'class', 'distribution', 'looks', 'like', 'class', 'distribution', 'believe', 'two', 'choices', 'downsample', 'class', 'upsample', 'class', 'go', 'option', 'losing', 'data', 'go', 'option', 'using', 'non', 'real', 'data', 'way', 'train', 'model', 'without', 'upsample', 'downsample', 'classification_report', 'looks', 'like']","['machin', 'learn', 'train', 'model', 'use', 'imbalanc', 'datatwo', 'class', 'data', 'class', 'distribut', 'look', 'like', 'class', 'distribut', 'believ', 'two', 'choic', 'downsampl', 'class', 'upsampl', 'class', 'go', 'option', 'lose', 'data', 'go', 'option', 'use', 'non', 'real', 'data', 'way', 'train', 'model', 'without', 'upsampl', 'downsampl', 'classification_report', 'look', 'like']"
218,234,234,10029745,72788374,What is a ML model suitable for a nutricionist AI?,"<p>Say a Machine Learning engineer wants to make an AI that predicts the best diet for a person. It takes into account the goal of the diet, profile of the person, calorie intake and possible meals from a database of diets.</p>
<p>Is there an existing model that serves this purpose? Would you recomend using AI to solve this problem as opposed to another algorithm? If no, then why?</p>
<p>The context is I am new to AI and am trying to understand its use cases.</p>
",20,0,-4,3,machine-learning;artificial-intelligence;diet,2022-06-28 20:01:59,2022-06-28 20:01:59,2022-06-28 20:01:59,say a machine learning engineer wants to make an ai that predicts the best diet for a person  it takes into account the goal of the diet  profile of the person  calorie intake and possible meals from a database of diets  is there an existing model that serves this purpose  would you recomend using ai to solve this problem as opposed to another algorithm  if no  then why  the context is i am new to ai and am trying to understand its use cases ,what is a ml model suitable for a nutricionist ai ,say machine learning engineer wants make ai predicts best diet person takes account goal diet profile person calorie intake possible meals database diets existing model serves purpose would recomend using ai solve problem opposed another algorithm context ai trying understand use cases,ml model suitable nutricionist ai,ml model suitable nutricionist aisay machine learning engineer wants make ai predicts best diet person takes account goal diet profile person calorie intake possible meals database diets existing model serves purpose would recomend using ai solve problem opposed another algorithm context ai trying understand use cases,"['ml', 'model', 'suitable', 'nutricionist', 'aisay', 'machine', 'learning', 'engineer', 'wants', 'make', 'ai', 'predicts', 'best', 'diet', 'person', 'takes', 'account', 'goal', 'diet', 'profile', 'person', 'calorie', 'intake', 'possible', 'meals', 'database', 'diets', 'existing', 'model', 'serves', 'purpose', 'would', 'recomend', 'using', 'ai', 'solve', 'problem', 'opposed', 'another', 'algorithm', 'context', 'ai', 'trying', 'understand', 'use', 'cases']","['ml', 'model', 'suitabl', 'nutricionist', 'aisay', 'machin', 'learn', 'engin', 'want', 'make', 'ai', 'predict', 'best', 'diet', 'person', 'take', 'account', 'goal', 'diet', 'profil', 'person', 'calori', 'intak', 'possibl', 'meal', 'databas', 'diet', 'exist', 'model', 'serv', 'purpos', 'would', 'recomend', 'use', 'ai', 'solv', 'problem', 'oppos', 'anoth', 'algorithm', 'context', 'ai', 'tri', 'understand', 'use', 'case']"
219,235,235,10456623,72777047,How to use data without leakage,"<p>I am developing a deep learning model, but the data is sensitive and can only be stored on a remote server. However, coding and debugging on that server is inconvenient.  So, is it possible to use the data in my local machine without data leakage? (make sure I can't download the real data from my side otherwise the person who manages the data won't allow me to do it.)</p>
",16,0,-1,3,deep-learning;privacy;data-security,2022-06-28 00:20:10,2022-06-28 00:20:10,2022-06-28 19:56:09,i am developing a deep learning model  but the data is sensitive and can only be stored on a remote server  however  coding and debugging on that server is inconvenient   so  is it possible to use the data in my local machine without data leakage   make sure i can t download the real data from my side otherwise the person who manages the data won t allow me to do it  ,how to use data without leakage,developing deep learning model data sensitive stored remote server however coding debugging server inconvenient possible use data local machine without data leakage make sure download real data side otherwise person manages data allow,use data without leakage,use data without leakagedeveloping deep learning model data sensitive stored remote server however coding debugging server inconvenient possible use data local machine without data leakage make sure download real data side otherwise person manages data allow,"['use', 'data', 'without', 'leakagedeveloping', 'deep', 'learning', 'model', 'data', 'sensitive', 'stored', 'remote', 'server', 'however', 'coding', 'debugging', 'server', 'inconvenient', 'possible', 'use', 'data', 'local', 'machine', 'without', 'data', 'leakage', 'make', 'sure', 'download', 'real', 'data', 'side', 'otherwise', 'person', 'manages', 'data', 'allow']","['use', 'data', 'without', 'leakagedevelop', 'deep', 'learn', 'model', 'data', 'sensit', 'store', 'remot', 'server', 'howev', 'code', 'debug', 'server', 'inconveni', 'possibl', 'use', 'data', 'local', 'machin', 'without', 'data', 'leakag', 'make', 'sure', 'download', 'real', 'data', 'side', 'otherwis', 'person', 'manag', 'data', 'allow']"
220,236,236,9222360,72786131,Scrape large scale data from Wikipedia,"<p>I am training a large machine learning model and need to scrape a lot of data for the same. I want to train my model on domain specific tasks and hence, given a domain, I will need to scrape Wikipedia pages and pages from the web.</p>
<p>For example, if the domain name is &quot;finance&quot;, I will need to crawl Wikipedia pages that are related to finance and then store the text present in them. Wikipedia Rest API has a limit of 200 requests per second. I will probably need more data than that.</p>
<p>There are wikipedia data dumps but the size of the dumps are too large.</p>
<p>Is there any other way of doing this or working with the API?</p>
<p>On a similar note, if I want to search for pages on the web related to some query, how will I do it?</p>
",20,0,-1,3,web-scraping;web-crawler;mediawiki,2022-06-28 17:36:52,2022-06-28 17:36:52,2022-06-28 18:30:17,i am training a large machine learning model and need to scrape a lot of data for the same  i want to train my model on domain specific tasks and hence  given a domain  i will need to scrape wikipedia pages and pages from the web  for example  if the domain name is  finance   i will need to crawl wikipedia pages that are related to finance and then store the text present in them  wikipedia rest api has a limit of  requests per second  i will probably need more data than that  there are wikipedia data dumps but the size of the dumps are too large  is there any other way of doing this or working with the api  on a similar note  if i want to search for pages on the web related to some query  how will i do it ,scrape large scale data from wikipedia,training large machine learning model need scrape lot data want train model domain specific tasks hence given domain need scrape wikipedia pages pages web example domain name finance need crawl wikipedia pages related finance store text present wikipedia rest api limit requests per second probably need data wikipedia data dumps size dumps large way working api similar note want search pages web related query,scrape large scale data wikipedia,scrape large scale data wikipediatraining large machine learning model need scrape lot data want train model domain specific tasks hence given domain need scrape wikipedia pages pages web example domain name finance need crawl wikipedia pages related finance store text present wikipedia rest api limit requests per second probably need data wikipedia data dumps size dumps large way working api similar note want search pages web related query,"['scrape', 'large', 'scale', 'data', 'wikipediatraining', 'large', 'machine', 'learning', 'model', 'need', 'scrape', 'lot', 'data', 'want', 'train', 'model', 'domain', 'specific', 'tasks', 'hence', 'given', 'domain', 'need', 'scrape', 'wikipedia', 'pages', 'pages', 'web', 'example', 'domain', 'name', 'finance', 'need', 'crawl', 'wikipedia', 'pages', 'related', 'finance', 'store', 'text', 'present', 'wikipedia', 'rest', 'api', 'limit', 'requests', 'per', 'second', 'probably', 'need', 'data', 'wikipedia', 'data', 'dumps', 'size', 'dumps', 'large', 'way', 'working', 'api', 'similar', 'note', 'want', 'search', 'pages', 'web', 'related', 'query']","['scrape', 'larg', 'scale', 'data', 'wikipediatrain', 'larg', 'machin', 'learn', 'model', 'need', 'scrape', 'lot', 'data', 'want', 'train', 'model', 'domain', 'specif', 'task', 'henc', 'given', 'domain', 'need', 'scrape', 'wikipedia', 'page', 'page', 'web', 'exampl', 'domain', 'name', 'financ', 'need', 'crawl', 'wikipedia', 'page', 'relat', 'financ', 'store', 'text', 'present', 'wikipedia', 'rest', 'api', 'limit', 'request', 'per', 'second', 'probabl', 'need', 'data', 'wikipedia', 'data', 'dump', 'size', 'dump', 'larg', 'way', 'work', 'api', 'similar', 'note', 'want', 'search', 'page', 'web', 'relat', 'queri']"
221,237,237,19160479,72700574,Compute the loss of a moving dataset,"<p>I’m new to the world of machine learning, so it could be that my question is trivial or incorrectly posed.</p>
<p>I am using a moving dataset that I have forwarded to an STN network (Spatial Tranformation Network). To the STN I forward each image individually, then restack the whole images together in a tuple.
My problem lies in the loss calculation. My target has a torch.tensor with this size [2,1,64,64]
and my prediction that I want to implement has a torch.tensor [2,1,10,64,64], which means that the prediction and the target are not the same.
Could someone explain an idea to me. The only idea I have is to return the last STN output meaning something like this [2,1,1,64,64] to my prediction and then squeezed to be [2,1,64,64] and then calculate the loss.</p>
<p>Thank you in advance</p>
",39,0,0,5,python;machine-learning;pytorch;mnist;spatial-transformer-network,2022-06-21 17:42:31,2022-06-21 17:42:31,2022-06-28 18:19:39,i m new to the world of machine learning  so it could be that my question is trivial or incorrectly posed  thank you in advance,compute the loss of a moving dataset,world machine learning could question trivial incorrectly posed thank advance,compute loss moving dataset,compute loss moving datasetworld machine learning could question trivial incorrectly posed thank advance,"['compute', 'loss', 'moving', 'datasetworld', 'machine', 'learning', 'could', 'question', 'trivial', 'incorrectly', 'posed', 'thank', 'advance']","['comput', 'loss', 'move', 'datasetworld', 'machin', 'learn', 'could', 'question', 'trivial', 'incorrectli', 'pose', 'thank', 'advanc']"
222,238,238,11719827,72786082,Mixing video and data streams on raspberry pi for machine learning,"<p>What is best way to mix data and video for machine learning on single board computer like raspberry pi?</p>
<p>It should be very common problem. I found there is GPMF and KLV extensions for video formats. But I fail to find someone to implment that for raspberry pi, opencv, etc.</p>
<p>It seems I can solve the problem by mixing 2 data streams in single file and then read that file with my own code but I wonder what is canonical solution for that kind of problem.</p>
",30,0,2,3,machine-learning;raspberry-pi;klvdata,2022-06-28 17:33:13,2022-06-28 17:33:13,2022-06-28 17:33:13,what is best way to mix data and video for machine learning on single board computer like raspberry pi  it should be very common problem  i found there is gpmf and klv extensions for video formats  but i fail to find someone to implment that for raspberry pi  opencv  etc  it seems i can solve the problem by mixing  data streams in single file and then read that file with my own code but i wonder what is canonical solution for that kind of problem ,mixing video and data streams on raspberry pi for machine learning,best way mix data video machine learning single board computer like raspberry pi common problem found gpmf klv extensions video formats fail find someone implment raspberry pi opencv etc seems solve problem mixing data streams single file read file code wonder canonical solution kind problem,mixing video data streams raspberry pi machine learning,mixing video data streams raspberry pi machine learningbest way mix data video machine learning single board computer like raspberry pi common problem found gpmf klv extensions video formats fail find someone implment raspberry pi opencv etc seems solve problem mixing data streams single file read file code wonder canonical solution kind problem,"['mixing', 'video', 'data', 'streams', 'raspberry', 'pi', 'machine', 'learningbest', 'way', 'mix', 'data', 'video', 'machine', 'learning', 'single', 'board', 'computer', 'like', 'raspberry', 'pi', 'common', 'problem', 'found', 'gpmf', 'klv', 'extensions', 'video', 'formats', 'fail', 'find', 'someone', 'implment', 'raspberry', 'pi', 'opencv', 'etc', 'seems', 'solve', 'problem', 'mixing', 'data', 'streams', 'single', 'file', 'read', 'file', 'code', 'wonder', 'canonical', 'solution', 'kind', 'problem']","['mix', 'video', 'data', 'stream', 'raspberri', 'pi', 'machin', 'learningbest', 'way', 'mix', 'data', 'video', 'machin', 'learn', 'singl', 'board', 'comput', 'like', 'raspberri', 'pi', 'common', 'problem', 'found', 'gpmf', 'klv', 'extens', 'video', 'format', 'fail', 'find', 'someon', 'implment', 'raspberri', 'pi', 'opencv', 'etc', 'seem', 'solv', 'problem', 'mix', 'data', 'stream', 'singl', 'file', 'read', 'file', 'code', 'wonder', 'canon', 'solut', 'kind', 'problem']"
223,239,239,18390926,72782753,"How can I resize a image to shape=(None, 321, 321, 3) with name name=None and dtype=tf.float32","<p><strong>Problem</strong>: I am trying to reshape a image to size (None,321,321,3) and also set the name of image to None. I want to match the image dimension requirements to train a machine learning model of specs,</p>
<pre><code>TensorSpec(shape=(None, 321, 321, 3), dtype=tf.float32, name=None)
</code></pre>
<p><strong>What I have done:</strong> I am using <code>PIL</code> to reshape the image. I can convert the image to the required size and RGB band, but have no idea how to make the first argument in the size as <code>None</code> and also how to set the name of image to <code>None</code>. Please help me find a solution to this problem.</p>
",39,1,2,5,python;tensorflow;image-processing;python-imaging-library;image-resizing,2022-06-28 13:31:46,2022-06-28 13:31:46,2022-06-28 14:50:59,problem  i am trying to reshape a image to size  none     and also set the name of image to none  i want to match the image dimension requirements to train a machine learning model of specs  what i have done  i am using pil to reshape the image  i can convert the image to the required size and rgb band  but have no idea how to make the first argument in the size as none and also how to set the name of image to none  please help me find a solution to this problem ,how can i resize a image to shape  none        with name name none and dtype tf float,problem trying reshape image size none also set name image none want match image dimension requirements train machine learning model specs done using pil reshape image convert image required size rgb band idea make first argument size none also set name image none please help find solution problem,resize image shape none name name none dtype tf float,resize image shape none name name none dtype tf floatproblem trying reshape image size none also set name image none want match image dimension requirements train machine learning model specs done using pil reshape image convert image required size rgb band idea make first argument size none also set name image none please help find solution problem,"['resize', 'image', 'shape', 'none', 'name', 'name', 'none', 'dtype', 'tf', 'floatproblem', 'trying', 'reshape', 'image', 'size', 'none', 'also', 'set', 'name', 'image', 'none', 'want', 'match', 'image', 'dimension', 'requirements', 'train', 'machine', 'learning', 'model', 'specs', 'done', 'using', 'pil', 'reshape', 'image', 'convert', 'image', 'required', 'size', 'rgb', 'band', 'idea', 'make', 'first', 'argument', 'size', 'none', 'also', 'set', 'name', 'image', 'none', 'please', 'help', 'find', 'solution', 'problem']","['resiz', 'imag', 'shape', 'none', 'name', 'name', 'none', 'dtype', 'tf', 'floatproblem', 'tri', 'reshap', 'imag', 'size', 'none', 'also', 'set', 'name', 'imag', 'none', 'want', 'match', 'imag', 'dimens', 'requir', 'train', 'machin', 'learn', 'model', 'spec', 'done', 'use', 'pil', 'reshap', 'imag', 'convert', 'imag', 'requir', 'size', 'rgb', 'band', 'idea', 'make', 'first', 'argument', 'size', 'none', 'also', 'set', 'name', 'imag', 'none', 'pleas', 'help', 'find', 'solut', 'problem']"
