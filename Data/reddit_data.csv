,Unnamed: 0,AuthorId,Q_id,Title,Abstract,Answers,Cites,Date,Title_clean,Abstract_clean
0,0,AutoModerator,vfx16r,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",119,17,2022-06-19 20:30:12, d  simple questions thread,please post your questions here instead of creating a new thread  encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread 
1,1,ML_WAYR_bot,vg5kjd,[D] Machine Learning - WAYR (What Are You Reading) - Week 140,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|131-140|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|[Week 131](https://reddit.com/srsu2n)||||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)|[Week 132](https://reddit.com/t2xpfe)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)|[Week 133](https://reddit.com/tdf2gt)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)|[Week 134](https://reddit.com/tpruqj)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)|[Week 135](https://reddit.com/u0pnhf)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|[Week 126](https://reddit.com/r4e8he)|[Week 136](https://reddit.com/ub2xlz)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|[Week 127](https://reddit.com/rez90o)|[Week 137](https://reddit.com/ul9toj)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)|[Week 128](https://reddit.com/ruja9s)|[Week 138](https://reddit.com/uvl3xc)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)|[Week 129](https://reddit.com/s5lg69)|[Week 139](https://reddit.com/v5nggu)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)|[Week 130](https://reddit.com/sgisxq)||

Most upvoted papers two weeks ago:

/u/tetatetata: [Why Philosophers Should Care About Computational Complexity](https://arxiv.org/abs/1108.1791)

Besides that, there are no rules, have fun.",3,59,2022-06-20 03:19:00, d  machine learning   wayr  what are you reading    week ,this is a place to share machine learning research papers  journals  and articles that you re reading this week  if it relates to what you re researching  by all means elaborate and give us your insight  otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page  not the pdf  you can easily access the pdf from the summary page but not the other way around  or any other pertinent links previous weeks                                                                                                                              week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https most upvoted papers two weeks ago  u tetatetata   why philosophers should care about computational complexity  https besides that  there are no rules  have fun 
2,2,pcaversaccio,vmi13r,[P] DALL-E Mini stripped to its bare essentials and converted to PyTorch,,14,183,2022-06-28 13:47:16, p  dall e mini stripped to its bare essentials and converted to pytorch,nan
3,3,bikeskata,vmr8it,[R] Probabilistic Numerics: Computation as Machine Learning (Free Book!),"Abs: Probabilistic numerical computation formalises the connection between machine learning and applied mathematics. Numerical algorithms approximate intractable quantities from computable ones. They estimate integrals from evaluations of the integrand, or the path of a dynamical system described by differential equations from evaluations of the vector field. In other words, they infer a latent quantity from data. This book shows that it is thus formally possible to think of computational routines as learning machines, and to use the notion of Bayesian inference to build more flexible, efficient, or customised algorithms for computation. The text caters for Masters' and PhD students, as well as postgraduate researchers in artificial intelligence, computer science, statistics, and applied mathematics. Extensive background material is provided along with a wealth of figures, worked examples, and exercises (with solutions) to develop intuition. 

Link to book: https://www.probabilistic-numerics.org/textbooks/",2,20,2022-06-28 21:55:51, r  probabilistic numerics  computation as machine learning  free book  ,abs  probabilistic numerical computation formalises the connection between machine learning and applied mathematics  numerical algorithms approximate intractable quantities from computable ones  they estimate integrals from evaluations of the integrand  or the path of a dynamical system described by differential equations from evaluations of the vector field  in other words  they infer a latent quantity from data  this book shows that it is thus formally possible to think of computational routines as learning machines  and to use the notion of bayesian inference to build more flexible  efficient  or customised algorithms for computation  the text caters for masters  and phd students  as well as postgraduate researchers in artificial intelligence  computer science  statistics  and applied mathematics  extensive background material is provided along with a wealth of figures  worked examples  and exercises  with solutions  to develop intuition  link to book  https   www probabilistic numerics org textbooks 
4,4,devzaya,vmn7nt,"[N] Quaterion, a blazingly fast framework for similarity learning.","Just released. Quaterion â€” an open source framework for training and fine-tuning similarity learning models. It enables you to train models significantly (100x) faster, and iterate over experiments in minutes instead of hours even with a laptop GPU. It takes advantage of the PyTorch Lightning backend to make a flexible and scalable learning pipeline. GitHub https://github.com/qdrant/quaterion 

Here is a demo of the caching functionality.  

https://i.redd.it/9qi8gf9n4d891.gif",7,37,2022-06-28 18:53:52, n  quaterion  a blazingly fast framework for similarity learning ,just released  quaterion   an open source framework for training and fine tuning similarity learning models  it enables you to train models significantly  x  faster  and iterate over experiments in minutes instead of hours even with a laptop gpu  it takes advantage of the pytorch lightning backend to make a flexible and scalable learning pipeline  github https here is a demo of the caching functionality   https   i redd it qigfnd gif
5,5,M4mb0,vml5na,[N] PyTorch 1.12 released,"Pytorch 1.12 is available through the [pytorch conda channel](https://anaconda.org/pytorch/pytorch)

- [Release notes](https://github.com/pytorch/pytorch/releases) are still pending.
- [Issue tracker](https://github.com/pytorch/pytorch/milestone/28)

## Changes

- CUDA 11.6 wheels
- [torch.amp](https://pytorch.org/docs/1.12/amp.html) module",0,25,2022-06-28 17:07:42, n  pytorch   released,pytorch   is available through the  pytorch conda channel  https    release notes  https    issue tracker  https    changes  cuda   wheels   torch amp  https   pytorch org docs   amp html  module
6,6,MLknowledge,vme1l7,"[R] Welcome to my continuous, free live machine learning class with intermediate mathematics","Dear all,

Welcome to join my continued ML knowledge dissemination class via Zoom.

I will continue to explain machine learning using an intermediate level mathematics. It happens every second Thursday at GMT at 11:00 (HK7pm / SYD9pm) - the next class is on June 30.

The current topic is:

""Determinantal Point Process""

I'll fully explain its beautiful mathematics over a period of a few sessions. This is a powerful model to model diverse subsets. Yet it is not as commonly used as it should!

You can find my notes on my GitHub site:

[https://github.com/roboticcam/machine-learning-notes/](https://github.com/roboticcam/machine-learning-notes/)

Determinantal Point Process notes is found at:

[https://github.com/roboticcam/machine-learning-notes/blob/master/files/dpp\_new.pdf](https://github.com/roboticcam/machine-learning-notes/blob/master/files/dpp_new.pdf)

You need a solid understanding of linear algebra, calculus, probability and statistics. But if you just want to get a feel of how DPP works for example, and meet like-minded people, please come too!

To join, sign up for one of the meetup groups you see fit:

[https://www.meetup.com/machine-learning-hong-kong/](https://www.meetup.com/machine-learning-hong-kong/)

[https://www.meetup.com/deep-learning-sydney/](https://www.meetup.com/deep-learning-sydney/)

[https://www.meetup.com/Deep-Learning-Melbourne/](https://www.meetup.com/Deep-Learning-Melbourne/)

[https://www.meetup.com/machine-learning-athens/](https://www.meetup.com/machine-learning-athens/)",3,41,2022-06-28 09:38:03, r  welcome to my continuous  free live machine learning class with intermediate mathematics,dear all welcome to join my continued ml knowledge dissemination class via zoom i will continue to explain machine learning using an intermediate level mathematics  it happens every second thursday at gmt at    hkpm   sydpm    the next class is on june  the current topic is determinantal point processi ll fully explain its beautiful mathematics over a period of a few sessions  this is a powerful model to model diverse subsets  yet it is not as commonly used as it should you can find my notes on my github site  https determinantal point process notes is found at  https you need a solid understanding of linear algebra  calculus  probability and statistics  but if you just want to get a feel of how dpp works for example  and meet like minded people  please come too to join  sign up for one of the meetup groups you see fit  https  https  https  https   www meetup com machine learning athens   https   www meetup com machine learning athens  
7,7,alexlyzhov,vm2sti,[N] Inverse Scaling Prize: $250k in prizes for finding tasks where larger language models do worse,"The standard paradigm in natural language processing today is to train large language models to autocomplete random Internet-sourced text. These models are then either frozen and used directly for other tasks (zero-shot/few-shot), or additionally trained on other tasks (fine-tuning).

We're used to finding that task performance scales well with large increases in sizes of language models. But for real-world applications, it's also very meaningful to search for failure cases preemptively to fix the underlying issues. Can you find and convincingly demonstrate these failure cases where zero-shot/few-shot performance of language models scales *inversely*, with larger models behaving worse?

You don't necessarily need to have extra deep knowledge of ML or language models in order to participate and win, because all models are frozen and you only need to come up with the right data.

Check out these resources to learn more! [Announcement Twitter thread](https://twitter.com/EthanJPerez/status/1541454949397041154), [contest details on Github](https://github.com/inverse-scaling/prize).
The deadline for the first round of the contest is August 27, 2022.",32,210,2022-06-28 00:19:44, n  inverse scaling prize   k in prizes for finding tasks where larger language models do worse,the standard paradigm in natural language processing today is to train large language models to autocomplete random internet sourced text  these models are then either frozen and used directly for other tasks  zero shot few shot   or additionally trained on other tasks  fine tuning  we re used to finding that task performance scales well with large increases in sizes of language models  but for real world applications  it s also very meaningful to search for failure cases preemptively to fix the underlying issues  can you find and convincingly demonstrate these failure cases where zero shot few shot performance of language models scales  inversely   with larger models behaving worse you don t necessarily need to have extra deep knowledge of ml or language models in order to participate and win  because all models are frozen and you only need to come up with the right data check out these resources to learn more   announcement twitter thread  https the deadline for the first round of the contest is august    
8,8,InternationalVisito,vml9we,[D]Can a transformer neural network learn to predict sequences longer than it saw?," Simple task: transformer has to repeat a sequence of random integers (0-9) of varied length, like:

sequence **length=7**: input\[ 1, 3 ,5 ,6, 2, 4, 0\] - output\[ 1, 3 ,5 ,6, 2, 4, 0\]  
sequence **length=3**: input\[ 5, 4 ,9 \] - output\[ 5, 4 ,9 \]  
sequence **length=4**: input\[ 6, 3 ,9, 8 \] - output\[ 6, 3 ,9, 8 \]  
...  
Each integer(0-9) can be stored in embedding layer so we can pass it to transformer.  
I trained transformer (generic pytorch model with positional embeddings) on a dataset (1000 examples) of sequences of varied length (1 to 12) and it predicts sequences well within the range of 12 . It fails to predict sequences longer than 12 - 13.  


sequence **length=20**: input\[3, 3, 4, 0, 0, 7, 1, 5, 1, 0, 7, 1, ***9, 0, 9, 1, 5, 2, 3, 6***\]  
.............................. ...- output\[3, 3, 4, 0, 0, 7, 1, 5, 1, 0, 7, 1, ***7, 1, 7, 1, 0, 7, 0, 7***\]

Is it considered an extrapolation task? Are there types of transformers (or other neural networks) that can handle the problem ?  
Same issue with **recurrent neural networks** (RNN, LSTM, GRU).",17,8,2022-06-28 17:14:14, d can a transformer neural network learn to predict sequences longer than it saw , simple task  transformer has to repeat a sequence of random integers     of varied length  like sequence   length     input                    output                   sequence   length     input             output            sequence   length     input               output                   each integer    can be stored in embedding layer so we can pass it to transformer   i trained transformer  generic pytorch model with positional embeddings  on a dataset   examples  of sequences of varied length   to   and it predicts sequences well within the range of    it fails to predict sequences longer than       sequence   length     input                                                                                      output                                                is it considered an extrapolation task  are there types of transformers  or other neural networks  that can handle the problem    same issue with   recurrent neural networks    rnn  lstm  gru  
9,9,ben_cow,vmv54i,[D] [P] Questions about the usability of Shapley values on large feature spaces.," 

Hello! I am planning a research project which involves creating a classification DNN that takes in a frame from a molecular dynamics simulation of a protein which encodes each amino acid's  level of energetic interaction and tries to predict whether that frame came from protein state ""A"" or protein state ""B.""  I want to analyze the feature importance, that is, the importance of amino acid's energetic interaction level for making the classification prediction. Although I have heard some interesting applications with Shapley values to preform such an analysis on feature importance, the input layer structure of the model I am thinking of making would require 100+ neurons as there are 100+ features. The reason why the feature space is so large is because I am investigating how a model learns which amino acids are most important for the model to make a classification prediction for which state a protein is in where the protein is 100+ amino acids in length. Can Shapley methods handle a feature space of a model that large /would the computational cost of such a process be infeasible? Apologies if this question is a little unclear let me know if anything needs to be clarified.

Thanks!",0,2,2022-06-29 00:39:05, d   p  questions about the usability of shapley values on large feature spaces , hello  i am planning a research project which involves creating a classification dnn that takes in a frame from a molecular dynamics simulation of a protein which encodes each amino acid s  level of energetic interaction and tries to predict whether that frame came from protein state a or protein state b   i want to analyze the feature importance  that is  the importance of amino acid s energetic interaction level for making the classification prediction  although i have heard some interesting applications with shapley values to preform such an analysis on feature importance  the input layer structure of the model i am thinking of making would require   neurons as there are   features  the reason why the feature space is so large is because i am investigating how a model learns which amino acids are most important for the model to make a classification prediction for which state a protein is in where the protein is   amino acids in length  can shapley methods handle a feature space of a model that large  would the computational cost of such a process be infeasible  apologies if this question is a little unclear let me know if anything needs to be clarified thanks 
10,10,hegelian_waffle,vm9tki,[D] Laplacian positional encodings,"I just finished reading ""[Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)"" (Dwivedi et al. 2020) and ""[A Generalization of Transformer Networks to Graphs](https://arxiv.org/abs/2012.09699)"" (also Dwivedi et al. 2020), and came across the claim that the eigenvectors of the Laplacian of a graph ""represent a natural generalization of the Transformer (Vaswani et al., 2017) positional encodings (PE)"". Xavier Bresson [tweeted](https://twitter.com/xbresson/status/1273034896517332992?lang=en) the same thing.

So I worked out the eigenvectors of the Laplacian of a path graph (a line of vertices connected by edges like so: v-v-v-...-v), which is the kind of graph used in NLP to represent a sequence of tokens, and found that the ith eigenvector's kth entry is v\_i(k) = cos(Ï€ik/n âˆ’ Ï€i/2n) where n is the number of tokens in the sequence, which is very different from the sinusoidal PEs used in transformers in NLP. I tried working out a change of variables, but nothing's worked so far. Are Laplacian eigenvectors just not the generalizations they're claimed to be, or am I missing something here?",4,48,2022-06-28 05:59:30, d  laplacian positional encodings,i just finished reading  benchmarking graph neural networks  https so i worked out the eigenvectors of the laplacian of a path graph  a line of vertices connected by edges like so  v v v     v   which is the kind of graph used in nlp to represent a sequence of tokens  and found that the ith eigenvector s kth entry is v _i k    cos Ï€ik n   Ï€i n  where n is the number of tokens in the sequence  which is very different from the sinusoidal pes used in transformers in nlp  i tried working out a change of variables  but nothing s worked so far  are laplacian eigenvectors just not the generalizations they re claimed to be  or am i missing something here 
11,11,moschles,vmdl3l,[D] Surface rendering in Diffusion Probability Text-to-Image Generators.,"Two diffusion text-to-image generators are Google's Imagen and openai's DALLE.2.     

DALLE.2 uses a multimodal large language model called CLIP to encode an input text prompt.  The output is produced by a reverse encoder called a diffusion probability model.   Diffusion models have previously seen huge successes in *image super resolution* and denoising.  

One peculiar aspect of DALLE.2's  output is that it is capable of generating light sources in certain (seemingly) 3D locations in the scene, then correctly lighting the objects based off of their implied location.   DALLE.2 can also perform image completions from a starting image prompt.     The two examples below are Spongebob dish sponge in a sink, and Vermeer's famous earring painting.    


https://i.imgur.com/vVI6IOI.png

.


https://i.imgur.com/8h48lTg.png

.

One plausible explanation for these physically perfect surface reflections is that DALLE.2  performs a phase where the image is reverse-encoded into a 3D scene.  That scene is then rendered back into a 2D output image.    However, when consulting the primary literature, no such conversion to a 3D model is seen anywhere along the DALLE.2 workflow.  

The implication is that DALLE.2 must contain a wealth of priors related to light transport, gleaned simply from 2D training images alone.  This means these priors are being applied (mostly correctly) to particular instantiations of objects and surfaces in scenes.  This application is performed even to the point where wet metallic surfaces have correct blurring in reflections.       

Further investigations of this phenomenon would involve finding some user prompts that generated a scene containing light casting a sharp shadow onto a flat surface.  Another would be requesting a reflective object in the text prompt itself.

Your thoughts?",2,26,2022-06-28 09:13:20, d  surface rendering in diffusion probability text to image generators ,two diffusion text to image generators are google s imagen and openai s dalle       dalle  uses a multimodal large language model called clip to encode an input text prompt   the output is produced by a reverse encoder called a diffusion probability model    diffusion models have previously seen huge successes in  image super resolution  and denoising   one peculiar aspect of dalle  s  output is that it is capable of generating light sources in certain  seemingly  d locations in the scene  then correctly lighting the objects based off of their implied location    dalle  can also perform image completions from a starting image prompt      the two examples below are spongebob dish sponge in a sink  and vermeer s famous earring painting     https  https  one plausible explanation for these physically perfect surface reflections is that dalle   performs a phase where the image is reverse encoded into a d scene   that scene is then rendered back into a d output image     however  when consulting the primary literature  no such conversion to a d model is seen anywhere along the dalle  workflow   the implication is that dalle  must contain a wealth of priors related to light transport  gleaned simply from d training images alone   this means these priors are being applied  mostly correctly  to particular instantiations of objects and surfaces in scenes   this application is performed even to the point where wet metallic surfaces have correct blurring in reflections        further investigations of this phenomenon would involve finding some user prompts that generated a scene containing light casting a sharp shadow onto a flat surface   another would be requesting a reflective object in the text prompt itself your thoughts 
12,12,BlockDesigns,vmpenb,[P] Clustering long documents with Transformers in 10 minutes,"Transformers are awesome for so many things in 2022, but one thing I've found them to struggle with is generating embeddings for long documents.

I put together a blog post going through some interesting techniques. Let me know if it helped you!

[Blog post](https://www.notia.ai/articles/clustering-long-documents)",0,3,2022-06-28 20:35:39, p  clustering long documents with transformers in  minutes,transformers are awesome for so many things in   but one thing i ve found them to struggle with is generating embeddings for long documents i put together a blog post going through some interesting techniques  let me know if it helped you  blog post  https   www notia ai articles clustering long documents 
13,13,SnooRecipes1624,vlpnuw,"[D] IBM Zurich Research Plagiarised Our Paper and got it published on CVPR 2022. Is ""copy texts"" is plagiarism, ""copy idea"" is not plagiarism?","I am Xianbiao Qi, a computer vision researcher with more than ten years of research experience. I am writing this blog to complain of a serious case of deliberate plagiarism of our paper by the employees fromÂ **IBM Zurich Research. They did not copy texts, they copied the idea.**

>  
>  
>Our preprint paper on Arxiv is ""Jiaquan Ye, Xianbiao Qi, Yelin He, and etc.""PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML."" arXiv preprint arXiv:2105.01848, May 2021"" and the code was also released.  
>  
>  
>  
>Our paper (Ye et al. arXiv: 2105.01848) was plagiarised by a team in IBM Zurich Research: ""**Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, and Peter Staar, ""TableFormer: Table Structure Understanding with Transformers.""**Â In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4614-4623, June 2022.""  
>  
>  
>  
>Nassar et al. 's paper plagiarized our method, pre-processing, post-processing, visualization, inference, systematic solution, code, pretrained models and etc, but did NOT cite our paper in order to not be captured for the plagiarism.Â  I was notified by many experts in the fields of OCR and table analysis.

Let me explain theÂ plagiarism process step by step.

>**First**,Â Â let me sort out the timeline.  
>  
>**Our Tablemaster,**  
>  
>**2021-04-07,**Â we finish the ICDAR 2021 competition-on-scientific-literature-parsingÂ **hosted by another IBM group in Australia**  
>  
>[https://icdar2021.org/program-2/competitions/competition-on-scientific-literature-parsing/](https://icdar2021.org/program-2/competitions/competition-on-scientific-literature-parsing/)  
>  
>**2021-05-05**, we released our technical report, a preprintÂ version, on Arxiv[https://arxiv.org/pdf/2105.01848.pdf](https://arxiv.org/pdf/2105.01848.pdf)  
>  
>**2021-07-29**, we released our source code, and even our slides on Github,Â [https://github.com/JiaquanYe/TableMASTER-mmocr](https://github.com/JiaquanYe/TableMASTER-mmocr)  
>  
>**2021-09**: we released the Tablemaster pretrained model.  
>  
>**2021-11**Â we even released tablemaster\_mmocr docker environment.  
>  
>For this project, we were naked. We release all these materials to facilitate the community, but you  plagiarized it.  
>  
>  
>  
>**Plagiarized-TableFormer:**  
>  
>**2022-03-02**, the Plagiarized-TableFormer was released on Arxiv[https://arxiv.org/pdf/2203.01017v1.pdf](https://arxiv.org/pdf/2203.01017v1.pdf)Their supplementalÂ to cvpr submission.[https://openaccess.thecvf.com/content/CVPR2022/supplemental/Nassar\_TableFormer\_Table\_Structure\_CVPR\_2022\_supplemental.pdf](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Nassar_TableFormer_Table_Structure_CVPR_2022_supplemental.pdf)  
>  
>**Then,** let meÂ highlightÂ nineÂ clear evidence.Â Â Then I will explain each piece of evidence in detail.

1. Our methodology, youÂ plagiarize
2. Our Pre-processing, youÂ plagiarize
3. Our Post-processing, youÂ plagiarize
4. Our InferenceÂ speedup method,Â Â youÂ plagiarize
5. Our ""tricky"" work, you even plagiarize
6. Our text line detection and text line recognition, youÂ plagiarize
7. Our systematic solution,Â youÂ plagiarize
8. Our visualization,Â Â youÂ plagiarize
9. Misleading the audiences in order to not be captured forÂ plagiarism

&#x200B;

>ToÂ Nassar, Ahmed, Nikolaos Livathinos, Maksym Lysak, and Peter Staar,Â All you need is not ""TableFormer"", All you need is git clone and torch.load  
>  
>git cloneÂ [https://github.com/JiaquanYe/TableMASTER-mmocr](https://github.com/JiaquanYe/TableMASTER-mmocr)  
>  
>torch.load(""our\_pretrained\_model.pth"")Â   
>  
>You will be the rising stars because you have learnt how to use git clone and torch.load.  
>  
>I attach a step-by-step proof below.

&#x200B;

https://preview.redd.it/gexr11d474891.png?width=2480&format=png&auto=webp&s=2a1fe08a20e044bbef8e021abab17b97acf93101

https://preview.redd.it/t95rgpd474891.png?width=2480&format=png&auto=webp&s=cc345024e531edbbae5ed64c1588e1a7d1e7cc7b

https://preview.redd.it/o3fec4d474891.png?width=2480&format=png&auto=webp&s=89ef3646d0b48a2cc75629248c9503deee156969

https://preview.redd.it/u5ywa5d474891.png?width=2480&format=png&auto=webp&s=896f8c0f81d02021098a95902525266be36bad4c

https://preview.redd.it/ic1y75d474891.png?width=2480&format=png&auto=webp&s=a9e9103b576a66981afaec8f86c00acb31fa6e78

https://preview.redd.it/g6ykdod474891.png?width=2480&format=png&auto=webp&s=5aed8320e990841a7813e62d6359579448c2e32f

https://preview.redd.it/jae7j4d474891.png?width=2480&format=png&auto=webp&s=3f008b1c77254434602217facff99b747d977a7d

https://preview.redd.it/pzyln8d474891.png?width=2480&format=png&auto=webp&s=47de343b99f570ce3dac9be256ccd058bb8f52d6

https://preview.redd.it/bpbub9d474891.png?width=2480&format=png&auto=webp&s=b4eb656dfabc68676cec8c941b7e0034a7d29c25

https://preview.redd.it/7oulcad474891.png?width=2480&format=png&auto=webp&s=934af73376e3948d878c7ad15bdd504b075acb20

https://preview.redd.it/qo2aubd474891.png?width=2480&format=png&auto=webp&s=34821a8530c6da6f63b2552fe10aee94b7770e59

https://preview.redd.it/d0mqhdd474891.png?width=2480&format=png&auto=webp&s=4d7f1c95c1ec051d27758cdbb8f95f522e00c113

https://preview.redd.it/8oc6qgd474891.png?width=2480&format=png&auto=webp&s=50caf4ba63d23650a44bfcd830c7337c781b1ecd

https://preview.redd.it/944c1gd474891.png?width=2480&format=png&auto=webp&s=3fb5817bee6b21e6674c519c8954bd69149b92bd

https://preview.redd.it/c83m2gd474891.png?width=2480&format=png&auto=webp&s=125235df4a28cbff8f57956ec871002a6d7782bd

https://preview.redd.it/72awaid474891.png?width=2480&format=png&auto=webp&s=2ad797b986403d8b413289f778b97508e084ee46

https://preview.redd.it/szd7h3d474891.png?width=2480&format=png&auto=webp&s=3efdd2306814be90600b2da19fb032ea4c24ee4c",130,1011,2022-06-27 12:49:04, d  ibm zurich research plagiarised our paper and got it published on cvpr   is copy texts is plagiarism  copy idea is not plagiarism ,i am xianbiao qi  a computer vision researcher with more than ten years of research experience  i am writing this blog to complain of a serious case of deliberate plagiarism of our paper by the employees fromÂ   ibm zurich research  they did not copy texts  they copied the idea          our preprint paper on arxiv is jiaquan ye  xianbiao qi  yelin he  and etc pingan vcgroup s solution for icdar  competition on scientific literature parsing task b  table recognition to html  arxiv preprint arxiv    may  and the code was also released             our paper  ye et al  arxiv     was plagiarised by a team in ibm zurich research    ahmed nassar  nikolaos livathinos  maksym lysak  and peter staar  tableformer  table structure understanding with transformers   Â in proceedings of the ieee cvf conference on computer vision and pattern recognition  pp     june              nassar et al   s paper plagiarized our method  pre processing  post processing  visualization  inference  systematic solution  code  pretrained models and etc  but did not cite our paper in order to not be captured for the plagiarism Â  i was notified by many experts in the fields of ocr and table analysis let me explain theÂ plagiarism process step by step    first   Â Â let me sort out the timeline         our tablemaster                Â we finish the icdar  competition on scientific literature parsingÂ   hosted by another ibm group in australia         https             we released our technical report  a preprintÂ version  on arxiv https             we released our source code  and even our slides on github Â  https            we released the tablemaster pretrained model            Â we even released tablemaster _mmocr docker environment       for this project  we were naked  we release all these materials to facilitate the community  but you  plagiarized it               plagiarized tableformer                 the plagiarized tableformer was released on arxiv https       then    let meÂ highlightÂ nineÂ clear evidence Â Â then i will explain each piece of evidence in detail   our methodology  youÂ plagiarize  our pre processing  youÂ plagiarize  our post processing  youÂ plagiarize  our inferenceÂ speedup method Â Â youÂ plagiarize  our tricky work  you even plagiarize  our text line detection and text line recognition  youÂ plagiarize  our systematic solution Â youÂ plagiarize  our visualization Â Â youÂ plagiarize  misleading the audiences in order to not be captured forÂ plagiarism  xb  toÂ nassar  ahmed  nikolaos livathinos  maksym lysak  and peter staar Â all you need is not tableformer  all you need is git clone and torch load      git cloneÂ  https     torch load our _pretrained _model pth Â       you will be the rising stars because you have learnt how to use git clone and torch load       i attach a step by step proof below   xb https https https https https https https https https https https https https https https https https   preview redd it szdhd png width  format png auto webp s efddbebdafbeaceec
14,14,the_great_magician,vmri2q,[R] Softmax Linear Units,,1,2,2022-06-28 22:07:30, r  softmax linear units,nan
15,15,s7v7nsilver,vm39oe,"""A Path Towards Autonomous Machine Intelligence"" - Yann LeCun",,10,101,2022-06-28 00:39:30,a path towards autonomous machine intelligence   yann lecun,nan
16,16,Farconion,vmr5a5,[D] Have compression techniques every been applied to the likes of GPT-3 & DALLE-2?,"Large language models and the recent spur of diffusion based  text-to-image models are gosh-darn fun to play with, but due to their  size and expensive training costs - they're only accessible via an API  or if you yourself have a access to a large # of GPUs. Yet there are  also a number of compression techniques like pruning and quantization  that can drastically reduce the size (+90%), and thus computational  requirements, of a trained model. Has there been any work looking  appling such techniques to these gigantic models floating around to make  them more accessible?",5,2,2022-06-28 21:51:53, d  have compression techniques every been applied to the likes of gpt    dalle  ,large language models and the recent spur of diffusion based  text to image models are gosh darn fun to play with  but due to their  size and expensive training costs   they re only accessible via an api  or if you yourself have a access to a large   of gpus  yet there are  also a number of compression techniques like pruning and quantization  that can drastically reduce the size       and thus computational  requirements  of a trained model  has there been any work looking  appling such techniques to these gigantic models floating around to make  them more accessible 
17,17,rebataur,vmlu60,[p] RestifyML - AI/ML Tool for Developers to quickly experiment with data and generate AI/ML REST API to consume back into their application," Developers can use RestifyML to

* Create DataScience experiments
* Create Data Source and upload CSV data within the experiment
* Do Data Cleansing and Sanitization
* Visualize raw data using Data Exploration
* Select Features which would help in building models
* Build Model, save or export them
* Finally, deploy Model and expose them as REST API
* Consume Machine Learning REST API from any Application
* Profit!

[https://github.com/rebataur/RestifyML](https://github.com/rebataur/RestifyML)

Feedback/ Feature Request appreciated.",2,4,2022-06-28 17:44:46, p  restifyml   ai ml tool for developers to quickly experiment with data and generate ai ml rest api to consume back into their application, developers can use restifyml to  create datascience experiments  create data source and upload csv data within the experiment  do data cleansing and sanitization  visualize raw data using data exploration  select features which would help in building models  build model  save or export them  finally  deploy model and expose them as rest api  consume machine learning rest api from any application  profit  https feedback  feature request appreciated 
18,18,shreyansh26,vmszee,[R] Annotated KDD 2022 paper - Learning Backward Compatible Embeddings," I read a super interesting KDD 2022 paper recently - ""Learning Backward Compatible Embeddings"".

The paper tackles a common industry problem of ensuring compatibility of newer embeddings with an older downstream model.

An annotated version of the paper - [Annotated-ML-Papers/Learning Backward Compatible Embeddings.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Learning%20Backward%20Compatible%20Embeddings.pdf)",0,0,2022-06-28 23:12:26, r  annotated kdd  paper   learning backward compatible embeddings, i read a super interesting kdd  paper recently   learning backward compatible embeddings the paper tackles a common industry problem of ensuring compatibility of newer embeddings with an older downstream model an annotated version of the paper    annotated ml papers learning backward compatible embeddings pdf  https   github com shreyansh annotated ml papers blob main general dl learning backward compatible embeddings pdf 
19,19,cheptsov,vmroly,[D] Run apps and dev environments in the cloud with a single command,"Hi everyone,  


I'm the creator of dstack, a tool that makes it easier to train models in the cloud. Our tool allows extending it with custom providers to support different languages, frameworks, etc.  
All the built-in providers are also open-source. Today, we've released a new update that extends the capabilities of dstack beyond training models, and now also allows users to quickly build and share apps with Streamlit, Gradio, and FastAPI in the cloud â€“ in just a few clicks.  
Similar to apps, it's possible to run dev environments with required hardware and data access also in one command from the Terminal. All you have to do is to link your own AWS account to run commands.  


Invite everyone to read it, and share their thoughts. Happy to discuss the approach and what would be great to have!

Blog post: [https://blog.dstack.ai/introducing-apps-and-dev-environments](https://blog.dstack.ai/introducing-apps-and-dev-environments)  


P.S.: Currently, it's possible to run models and apps only in the configured cloud. If you'd like the tool to also allow you to run it locally, and if you would like this part to be open-source too, please leave comments! ðŸ¤—",0,0,2022-06-28 22:15:29, d  run apps and dev environments in the cloud with a single command,hi everyone   i m the creator of dstack  a tool that makes it easier to train models in the cloud  our tool allows extending it with custom providers to support different languages  frameworks  etc   all the built in providers are also open source  today  we ve released a new update that extends the capabilities of dstack beyond training models  and now also allows users to quickly build and share apps with streamlit  gradio  and fastapi in the cloud   in just a few clicks   similar to apps  it s possible to run dev environments with required hardware and data access also in one command from the terminal  all you have to do is to link your own aws account to run commands   invite everyone to read it  and share their thoughts  happy to discuss the approach and what would be great to have blog post   https p s   currently  it s possible to run models and apps only in the configured cloud  if you d like the tool to also allow you to run it locally  and if you would like this part to be open source too  please leave comments   
20,20,programmerChilli,vmabau,[P] First-class Dims - a generalization of einops and named tensors,"Jupyter Notebook: https://colab.research.google.com/drive/1BsVkddtVMX35aZAvo2GyI-wSFPVBCWuA

Github: https://github.com/facebookresearch/torchdim

Some tweet threads about it

Mine: https://twitter.com/cHHillee/status/1541536627746426881

Sasha Rush: https://twitter.com/srush_nlp/status/1541526906113298433",1,17,2022-06-28 06:24:33, p  first class dims   a generalization of einops and named tensors,jupyter notebook  https github  https some tweet threads about itmine  https sasha rush  https   twitter com srush_nlp status 
21,21,-aplusib-,vm5a72,[R] Theoretical Open Research Areas,"Hello everyone,

my goal is to do research in the field of machine learning for motion planning/robotics in general. I'm really interested in the theoretical/mathematical side of the field. However I noticed that the majority of the field consists of very experimental papers where architectures are built and bench-marked without any thorough underlying theory.

So my questions is: Are there any theoretical research areas in machine learning for motion planning/robotics in general?

It would be nice if someone could also give me some labs/researchers working in that direction.

&#x200B;

Thank you very much.",4,14,2022-06-28 02:07:15, r  theoretical open research areas,hello everyone my goal is to do research in the field of machine learning for motion planning robotics in general  i m really interested in the theoretical mathematical side of the field  however i noticed that the majority of the field consists of very experimental papers where architectures are built and bench marked without any thorough underlying theory so my questions is  are there any theoretical research areas in machine learning for motion planning robotics in general it would be nice if someone could also give me some labs researchers working in that direction   xb thank you very much 
22,22,time_waster103,vmjkun,[D] How would you build a scalable platform for Machine Learning for Data Streams,"Recently I came across the idea of online machine learning where the model learns in real time from streaming data. I was wondering what would be an ideal solution be to deal with the varying rate at which data might arrive. More specifically, having a fixed number of VMs in the cloud to train the model in real time might result in underutilization of resources or lack of enough resources. So how should we design an end to end system for such a scenario? Do any of the cloud service providers already have an off the shelf solution for this?",0,0,2022-06-28 15:31:57, d  how would you build a scalable platform for machine learning for data streams,recently i came across the idea of online machine learning where the model learns in real time from streaming data  i was wondering what would be an ideal solution be to deal with the varying rate at which data might arrive  more specifically  having a fixed number of vms in the cloud to train the model in real time might result in underutilization of resources or lack of enough resources  so how should we design an end to end system for such a scenario  do any of the cloud service providers already have an off the shelf solution for this 
23,23,fishiwhj,vma4il,[D] How to evaluate the gain of a new feature without training?,"When evaluating the effectiveness of a new feature, it is common to train a model with/without this feature to compare the difference. But sometimes training a model based on huge amounts of data is both time and energy consuming. I was wondering if there are some lightweight ways to estimate the importance of the new feature without training? Computing *descriptive statistics* such as feature coverage, histogram and correlation matrix might be necessary, are there other pre-processing methods?",2,4,2022-06-28 06:14:47, d  how to evaluate the gain of a new feature without training ,when evaluating the effectiveness of a new feature  it is common to train a model with without this feature to compare the difference  but sometimes training a model based on huge amounts of data is both time and energy consuming  i was wondering if there are some lightweight ways to estimate the importance of the new feature without training  computing  descriptive statistics  such as feature coverage  histogram and correlation matrix might be necessary  are there other pre processing methods 
24,24,Ierihon_hasty_ai,vmj52i,"[R] Data-centric AI development approach gives us 5,5-8% mAP on PASCAL VOC 2012","How important is clean data for how your AI models perform?

According to our [initial experiments](https://hasty.ai/content-hub/articles/cleaning-pascal-improving-map-by-13) \- very important. Two people improved the primary model metric by 13% in a week using state-of-the-art confidence learning to clean up PASCAL.

In the next iteration, we explore the field a bit deeper, trying to avoid controversy. SPOILER: we still get a nice mAP metric boost.

To learn more about the follow-up results and the plans for the future, check out our article: [https://hasty.ai/content-hub/articles/further-pascal-voc-2012-exploration-and-plans-for-the-future](https://hasty.ai/content-hub/articles/further-pascal-voc-2012-exploration-and-plans-for-the-future)

&#x200B;

Discussion of the initial results:

* [https://www.reddit.com/r/MachineLearning/comments/uc9z2y/p\_we\_cleaned\_up\_pascal\_and\_improved\_map\_by\_13/ia6caqg/?context=3](https://www.reddit.com/r/MachineLearning/comments/uc9z2y/p_we_cleaned_up_pascal_and_improved_map_by_13/ia6caqg/?context=3)
* [https://www.reddit.com/r/computervision/comments/uc9x8t/we\_cleaned\_up\_pascal\_and\_improved\_map\_by\_13/i69iyii/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/computervision/comments/uc9x8t/we_cleaned_up_pascal_and_improved_map_by_13/i69iyii/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

[Disclaimer: We used our own platform to clean up the data and the article, therefore, contains self-promotion. However, the article mainly focuses on the results we achieved.](https://preview.redd.it/5kzxfe440c891.png?width=1978&format=png&auto=webp&s=b2521ee17d6542256e16af94972fc2378476178e)",0,0,2022-06-28 15:02:53, r  data centric ai development approach gives us     map on pascal voc ,how important is clean data for how your ai models perform according to our  initial experiments  https in the next iteration  we explore the field a bit deeper  trying to avoid controversy  spoiler  we still get a nice map metric boost to learn more about the follow up results and the plans for the future  check out our article   https   xb discussion of the initial results    https    https   xb  disclaimer  we used our own platform to clean up the data and the article  therefore  contains self promotion  however  the article mainly focuses on the results we achieved   https   preview redd it kzxfec png width  format png auto webp s beedeaffce 
25,25,vigneshwaranpersonal,vm010a,[D] Do you have any suggestions for a crowd-sourced annotation tool?,"We're currently doing research on computational social science, specifically on online toxicity. We have lots of text data, but we don't have annotations. As part of the research, we are thinking of annotating the text using a crowd-sourcing approach. Do any of you know of any open-source tool that we could employ to ease up the process?",5,6,2022-06-27 22:21:01, d  do you have any suggestions for a crowd sourced annotation tool ,we re currently doing research on computational social science  specifically on online toxicity  we have lots of text data  but we don t have annotations  as part of the research  we are thinking of annotating the text using a crowd sourcing approach  do any of you know of any open source tool that we could employ to ease up the process 
26,26,AquaHug,vl7iut,"[P] A drawing application called Vizcom that uses GANs to help automate color, shading, and rendering.",,13,543,2022-06-26 21:14:13, p  a drawing application called vizcom that uses gans to help automate color  shading  and rendering ,nan
27,27,_ayushp_,vkxsf2,I made a robot that punishes me if it detects that if I am procrastinating on my assignments [P],,149,3349,2022-06-26 11:22:23,i made a robot that punishes me if it detects that if i am procrastinating on my assignments  p ,nan
28,28,diabulusInMusica,vlr75m,[P] I published a tutorial about ML model deployment,"The deployment of ML models in production is a delicate process filled with challenges. You  can deploy a model via a REST API, on an edge device, or as as an  off-line unit used for batch processing. You can build the deployment  pipeline from scratch, or use ML deployment frameworks. 

In my new mini-series, you'll learn best practices to deploy your ML models. I  try to concentrate everything in 2 videos, to keep the series short and  sweet. 

The first video provides a theoretical overview of ML deployment. You'll learn about:

* Different strategies to deploy ML in production. 
* The main ML deployment tools on the market (TF Serving,  MLFlow Model, Seldon Deploy, KServe from Kubeflow). 
* BentoML and its features.  


Here's the video: [https://www.youtube.com/watch?v=Mrv3CZNWYEg](https://www.youtube.com/watch?v=Mrv3CZNWYEg)",1,16,2022-06-27 14:38:01, p  i published a tutorial about ml model deployment,the deployment of ml models in production is a delicate process filled with challenges  you  can deploy a model via a rest api  on an edge device  or as as an  off line unit used for batch processing  you can build the deployment  pipeline from scratch  or use ml deployment frameworks  in my new mini series  you ll learn best practices to deploy your ml models  i  try to concentrate everything in  videos  to keep the series short and  sweet  the first video provides a theoretical overview of ml deployment  you ll learn about   different strategies to deploy ml in production    the main ml deployment tools on the market  tf serving   mlflow model  seldon deploy  kserve from kubeflow     bentoml and its features   here s the video   https   www youtube com watch v mrvcznwyeg  https   www youtube com watch v mrvcznwyeg 
29,29,icelebratefestivus,vlqu17,[D] Has anyone trained the latent diffusion models by OpenAI(CompVis)? Need some help,"I am trying to train a [latent-diffusion](https://github.com/CompVis/latent-diffusion) model by following the instructions on the repo, however I am running into errors while sampling from the checkpointed models. Can someone help?

I am getting Errors while trying to sample using [sample\_diffusion.py ](https://github.com/CompVis/latent-diffusion/blob/main/scripts/sample_diffusion.py)from a custom model trained on LSUN churches",12,10,2022-06-27 14:12:51, d  has anyone trained the latent diffusion models by openai compvis   need some help,i am trying to train a  latent diffusion  https i am getting errors while trying to sample using  sample _diffusion py   https   github com compvis latent diffusion blob main scripts sample_diffusion py from a custom model trained on lsun churches
30,30,OmOshIroIdEs,vm323i,[R] Can I use whole-protein embeddings on isolated domains?,"I'm interested in studying properties of particular protein domains. One idea is to take advantage of state-of-the-art protein embedding models, such as this, most of which are based on transformers.

Some of the domains I'm studying are found in large proteins, which have multiple other domains in the same chain. Therefore, I believe it might be more informative to obtain embeddings not of each protein as a whole, but just the domains. However, I worry that the embeddings would be all off, since the model expects a complete sequence.

Has anyone tried this before? Are the pre-trained domain-level embeddings?",0,1,2022-06-28 00:30:40, r  can i use whole protein embeddings on isolated domains ,i m interested in studying properties of particular protein domains  one idea is to take advantage of state of the art protein embedding models  such as this  most of which are based on transformers some of the domains i m studying are found in large proteins  which have multiple other domains in the same chain  therefore  i believe it might be more informative to obtain embeddings not of each protein as a whole  but just the domains  however  i worry that the embeddings would be all off  since the model expects a complete sequence has anyone tried this before  are the pre trained domain level embeddings 
31,31,KalloDotIO,vm2px5,[Discussion] [computer vision] Instant NeRF create quality depth maps?," Surprised I haven't seen more chatter about this. What do you  think about Nvidia's instant Nerf which turns 2d into 3d based on these techniques [https://arxiv.org/abs/2003.10016](https://arxiv.org/abs/2003.10016)

Does the output of a NeRF give a depth map that's comparable to what you'd get from a Kinect?

Can these be used to create 3D models one would use in Unreal or Blender?",3,1,2022-06-28 00:16:17, discussion   computer vision  instant nerf create quality depth maps , surprised i haven t seen more chatter about this  what do you  think about nvidia s instant nerf which turns d into d based on these techniques  https does the output of a nerf give a depth map that s comparable to what you d get from a kinect can these be used to create d models one would use in unreal or blender 
32,32,heylibrarian,vlm6yy,[D] State-of-the-art permutation-invariant graph embeddings,"Suppose I have a data set consisting of weighted undirected simple graphs. I would like to learn a vector representation of these graphs. What are the state-of-the-art (2022) architectures/methods for learning such representations? Ideally, the representations are permutation-invariant. For what it's worth, I am only interested in the case where graphs (vertices, edges, and their respective weights) are fully observed; I'm not interested cases unobserved nodes.

An additional requirement is the embedding must have a lower dimension that the number of nodes.",7,9,2022-06-27 09:15:19, d  state of the art permutation invariant graph embeddings,suppose i have a data set consisting of weighted undirected simple graphs  i would like to learn a vector representation of these graphs  what are the state of the art    architectures methods for learning such representations  ideally  the representations are permutation invariant  for what it s worth  i am only interested in the case where graphs  vertices  edges  and their respective weights  are fully observed  i m not interested cases unobserved nodes an additional requirement is the embedding must have a lower dimension that the number of nodes 
33,33,baceituno,vlyjsf,[D] Stack - Seamless data collaboration and versioning,"Hey r/MachineLearning! We are the co-founders of Stack, a hub for data collaboration and versioning. We are developing this tool to help ML teams automatically track changes in their data seamlessly.

We are opening a waiting list for our beta, which we aim to release soon. You can sign up at: https://www.getstack.ai/

We are also actively looking for feedback. Feel free to share any comments or thoughts!",0,1,2022-06-27 21:18:07, d  stack   seamless data collaboration and versioning,hey r machinelearning  we are the co founders of stack  a hub for data collaboration and versioning  we are developing this tool to help ml teams automatically track changes in their data seamlessly we are opening a waiting list for our beta  which we aim to release soon  you can sign up at  https we are also actively looking for feedback  feel free to share any comments or thoughts 
34,34,ykilcher,vlfz1v,[D] Paper Explained - Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Video Analysis),"[https://youtu.be/oz5yZc9ULAc](https://youtu.be/oz5yZc9ULAc)

Minecraft is one of the harder challenges any RL agent could face. Episodes are long, and the world is procedurally generated, complex, and huge. Further, the action space is a keyboard and a mouse, which has to be operated only given the game's video input. OpenAI tackles this challenge using Video PreTraining, leveraging a small set of contractor data in order to pseudo-label a giant corpus of scraped footage of gameplay. The pre-trained model is highly capable in basic game mechanics and can be fine-tuned much better than a blank slate model. This is the first Minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself.

&#x200B;

OUTLINE:

0:00 - Intro

3:50 - How to spend money most effectively?

8:20 - Getting a large dataset with labels

14:40 - Model architecture

19:20 - Experimental results and fine-tuning

25:40 - Reinforcement Learning to the Diamond Pickaxe

30:00 - Final comments and hardware

&#x200B;

Blog: [https://openai.com/blog/vpt/](https://openai.com/blog/vpt/)

Paper: [https://arxiv.org/abs/2206.11795](https://arxiv.org/abs/2206.11795)

Code & Model weights: [https://github.com/openai/Video-Pre-Training](https://github.com/openai/Video-Pre-Training)",0,25,2022-06-27 03:49:05, d  paper explained   video pretraining  vpt   learning to act by watching unlabeled online videos  video analysis , https minecraft is one of the harder challenges any rl agent could face  episodes are long  and the world is procedurally generated  complex  and huge  further  the action space is a keyboard and a mouse  which has to be operated only given the game s video input  openai tackles this challenge using video pretraining  leveraging a small set of contractor data in order to pseudo label a giant corpus of scraped footage of gameplay  the pre trained model is highly capable in basic game mechanics and can be fine tuned much better than a blank slate model  this is the first minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself   xb outline     intro    how to spend money most effectively     getting a large dataset with labels    model architecture    experimental results and fine tuning    reinforcement learning to the diamond pickaxe    final comments and hardware  xb blog   https paper   https code   model weights   https   github com openai video pre training  https   github com openai video pre training 
35,35,curiousML5,vljmxx,[P] Skipgram: neural network instead of lookup table,"I'm looking for papers which use the skipgram model but instead of a lookup table they use a neural network. The use case is instead of sentences of words I want to use sequences of human behavior where additional information is available, e.g. think sequences of visited Amazon products. Cold-start also happens to be very common and I'm thinking that using a neural network instead of lookup embeddings table would be better.

Updated with more context:

The typical usage of skip gram is for learning word embedding as in text where each word has an embedding which is learned through skipgram. However there is nothing limiting the usage of skipgram for non-text cases.

A popular way to use skipgram in i2i recommendation systems is to treat a session of products browsed by the user as a sequence and to have an embedding per product. (Eg see KDD 2018 winning paper from Airbnb) However, the question I have here is instead of having one embedding per product can we instead use a neural network where the output layer is the embedding layer. This way we can backprop through the neural network. The reason is we have more information for products than we do for words",4,4,2022-06-27 06:58:56, p  skipgram  neural network instead of lookup table,i m looking for papers which use the skipgram model but instead of a lookup table they use a neural network  the use case is instead of sentences of words i want to use sequences of human behavior where additional information is available  e g  think sequences of visited amazon products  cold start also happens to be very common and i m thinking that using a neural network instead of lookup embeddings table would be better updated with more context the typical usage of skip gram is for learning word embedding as in text where each word has an embedding which is learned through skipgram  however there is nothing limiting the usage of skipgram for non text cases a popular way to use skipgram in ii recommendation systems is to treat a session of products browsed by the user as a sequence and to have an embedding per product   eg see kdd  winning paper from airbnb  however  the question i have here is instead of having one embedding per product can we instead use a neural network where the output layer is the embedding layer  this way we can backprop through the neural network  the reason is we have more information for products than we do for words
36,36,WigglyHypersurface,vlj0py,"[D] For perciever (IO) with single-channel audio, are position encodings even necessary?","I've been looking into using the Perciever for a project that involves single-channel (mono) audio. From the existing implementations and tutorials, I can't find one that only does audio. It seems like in the papers they rearrange the audio into patches and add position encodings, but this is a hack to bring the audio modality into the same size tensor as other modalities. If only using 1d audio is there any need at all for position encodings at all?",6,4,2022-06-27 06:26:12, d  for perciever  io  with single channel audio  are position encodings even necessary ,i ve been looking into using the perciever for a project that involves single channel  mono  audio  from the existing implementations and tutorials  i can t find one that only does audio  it seems like in the papers they rearrange the audio into patches and add position encodings  but this is a hack to bring the audio modality into the same size tensor as other modalities  if only using d audio is there any need at all for position encodings at all 
37,37,Illustrious_Row_9971,vkvq0j,[R] CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,,20,198,2022-06-26 09:11:38, r  cogview  faster and better text to image generation via hierarchical transformers,nan
38,38,markurtz,vl3lpv,[R] How well do sparse ImageNet models transfer? Prune once and deploy anywhere for inference performance speedups! (arxiv link in comments),,6,28,2022-06-26 17:54:14, r  how well do sparse imagenet models transfer  prune once and deploy anywhere for inference performance speedups   arxiv link in comments ,nan
39,39,huehue9812,vl31d5,GMM latent space [D]," 

Hi, I would love to know if there is any ongoing work (or the latest) on mixture of Gaussians as latent space for GANs, or other generative models.

Does anyone have any experience on it and/or opinions on why it is not popular? (or doesn't work)",3,6,2022-06-26 17:20:39,gmm latent space  d , hi  i would love to know if there is any ongoing work  or the latest  on mixture of gaussians as latent space for gans  or other generative models does anyone have any experience on it and or opinions on why it is not popular   or doesn t work 
40,40,leboulevardier,vldjt5,[D] How to not commit code copyright violation with Github Co-pilot?,"At our work place, many of our ML researchers are starting to use Github Co-pilot to save time. Issue is there is no provenance on the code generated by Co-pilot. If I understand correctly, Co-pilot is trained on public GitHub repositories, many of which might have specific copyright and license clauses. Our research, when published, would also put the code on Github publicly.

What would you suggest to prevent potential code copyright violation in this case? I have sent request for Github to provide provenance tracking feature but I assume that's gonna take a while to implement (that is, if they decide to implement it). Are you using Github Co-pilot and worrying about similar issues?",6,2,2022-06-27 01:54:11, d  how to not commit code copyright violation with github co pilot ,at our work place  many of our ml researchers are starting to use github co pilot to save time  issue is there is no provenance on the code generated by co pilot  if i understand correctly  co pilot is trained on public github repositories  many of which might have specific copyright and license clauses  our research  when published  would also put the code on github publicly what would you suggest to prevent potential code copyright violation in this case  i have sent request for github to provide provenance tracking feature but i assume that s gonna take a while to implement  that is  if they decide to implement it   are you using github co pilot and worrying about similar issues 
41,41,TransitionWhich5018,vl34k4,[D] Why do some competition organizers hide the leaderboard? (Regarding my experience in IEEE SP Cup 2022),"I don't understand why organisers of an competition would hide the leaderboard, specially in a machine learning and signal processing related competition.

We participated in IEEE SP Cup 2022, sacrificing nearly 2 months of our time and and some sleepless nights. The organizers never said anything about keeping the leaderboard of the competition hidden. In the first round they gave us an access to a website where we can submit our predictions and get our score privately. There wasn't a leaderboard (well, there was a one that generates some random scores wherever we make a submission but I don't understand the use of it).

After the first round, we and several other teams requested the leaderboard. At first, the organizers said they couldn't reveal it because some teams would not like other teams seeing their position on the leaderboard (a weird reason because all teams were given a separate name to make submissions on the website and no team knew the names of others ðŸ™„), and after many replying they would like to see the leaderboard and there won't be such a problem with them, they asked us to create a poll in piazza to see which teams would like to see the leaderboard and said they will reveal the leaderboard after the competition is over.

The funny thing is, there was no such option for students to create a poll in piazza ðŸ˜‚. Even though we mentioned it to the organizers, we didn't get any reply. Now it has been almost a month since the competition concluded and the organizers totally ghosted us.

This is really discouraging after spending several months on a competition without even getting to know how far our efforts have come. Why would organizers hide the leaderboard like this? They could at least reveal the top 10 teams?",1,3,2022-06-26 17:26:10, d  why do some competition organizers hide the leaderboard   regarding my experience in ieee sp cup  ,i don t understand why organisers of an competition would hide the leaderboard  specially in a machine learning and signal processing related competition we participated in ieee sp cup   sacrificing nearly  months of our time and and some sleepless nights  the organizers never said anything about keeping the leaderboard of the competition hidden  in the first round they gave us an access to a website where we can submit our predictions and get our score privately  there wasn t a leaderboard  well  there was a one that generates some random scores wherever we make a submission but i don t understand the use of it  after the first round  we and several other teams requested the leaderboard  at first  the organizers said they couldn t reveal it because some teams would not like other teams seeing their position on the leaderboard  a weird reason because all teams were given a separate name to make submissions on the website and no team knew the names of others     and after many replying they would like to see the leaderboard and there won t be such a problem with them  they asked us to create a poll in piazza to see which teams would like to see the leaderboard and said they will reveal the leaderboard after the competition is over the funny thing is  there was no such option for students to create a poll in piazza    even though we mentioned it to the organizers  we didn t get any reply  now it has been almost a month since the competition concluded and the organizers totally ghosted us this is really discouraging after spending several months on a competition without even getting to know how far our efforts have come  why would organizers hide the leaderboard like this  they could at least reveal the top  teams 
42,42,OddSandwich969,vl45a2,[Discussion] Doubt regarding text vector difference image manipulation method of Dalle-2.,"I was going through the (updated)paper, there was this image manipulation method through text difference.
It went like this:

z_i := original image CLIP embedding

z_t := new text CLIP embedding/ embedding of the text for current image manipulation

z_t0 := orignal image's corresponding text CLIP embedding/ text embedding of the text 'a photo' / empty embedding

z_d := l2_norm(z_t - z_t0) <-> text difference vector | 
Here l2_norm means, normalising a vector by dividing it with it's norm_p (here norm 2).

z_new /z_theta :=  spherical_interpolation(z_i, z_d, theta) {where theta is between (0,0.5)} <-> new image's CLIP embedding vector 


What I don't understand is, that the CLIP img and text embedding vectors are supposed to be similar vectors (since trained with cosine similarity), and the difference between text embedding vectors of two similar texts will be somewhat perpendicular to either of the text vectors, therefore the text diff vector should be very different from the image embedding, and hence the spherical interpolation shouldn't give any meaningful result.

What am I missing? I am unable to understand why this text difference method works.",6,2,2022-06-26 18:26:01, discussion  doubt regarding text vector difference image manipulation method of dalle  ,i was going through the  updated paper  there was this image manipulation method through text difference it went like this z_i    original image clip embeddingz_t    new text clip embedding  embedding of the text for current image manipulationz_t    orignal image s corresponding text clip embedding  text embedding of the text  a photo    empty embeddingz_d    l_norm z_t   z_t   text difference vector   here l_norm means  normalising a vector by dividing it with it s norm_p  here norm   z_new  z_theta     spherical_interpolation z_i  z_d  theta   where theta is between        new image s clip embedding vector what i don t understand is  that the clip img and text embedding vectors are supposed to be similar vectors  since trained with cosine similarity   and the difference between text embedding vectors of two similar texts will be somewhat perpendicular to either of the text vectors  therefore the text diff vector should be very different from the image embedding  and hence the spherical interpolation shouldn t give any meaningful result what am i missing  i am unable to understand why this text difference method works 
43,43,DouBlindDotCOM,vlanqv,[D] Will this mode work for practicing paper reviews? Can we get in-depth feedback on our draft?," Some opinions were collected about mocking ML paper reviews. Link to the thread: [https://www.reddit.com/r/MachineLearning/comments/u967sy/d\_opinions\_needed\_anyone\_interested\_in\_mock\_peer/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/u967sy/d_opinions_needed_anyone_interested_in_mock_peer/?utm_source=share&utm_medium=web2x&context=3)

To summarize, many people are interested. Opinions are in common that:

1. People like private review rather than public review
2. Number of papers to review are not a concern but every couple months will be a good pace
3. Plagiarism and stealing are of course the biggest concern

To address this, I suggest the following mode:

1. ONLY opens for people who want to exchange paper reviews. Enthusiastic reviewers with no paper draft to be reviewed can wait.
2. ONLY opens for people who are really interested in mocking paper review prior to formal journal/conference submission. Join a Discord community (already established).
3. In the PRIVATE ""Introduce yourself"" channel, people introduce themselves using true information and offer a very brief paper abstract and ML category.
4. Chat openly or privately to find the right review partners
5. In the ""paper-review-exchange"" channel, announce your paper reviewer upon agreement (from both sides)
6. Exchange your drafts privately and preferably with official email addresses
7. (Optional) When the review work is done, announce it too.
8. Note that plagiarism and stealing can be minimized in this mode but still could happen.

When conference reviews do not offer much nowadays, a mockup review might give your more TRUE inputs. Good luck!",2,0,2022-06-26 23:38:07, d  will this mode work for practicing paper reviews  can we get in depth feedback on our draft , some opinions were collected about mocking ml paper reviews  link to the thread   https to summarize  many people are interested  opinions are in common that   people like private review rather than public review  number of papers to review are not a concern but every couple months will be a good pace  plagiarism and stealing are of course the biggest concernto address this  i suggest the following mode   only opens for people who want to exchange paper reviews  enthusiastic reviewers with no paper draft to be reviewed can wait   only opens for people who are really interested in mocking paper review prior to formal journal conference submission  join a discord community  already established    in the private introduce yourself channel  people introduce themselves using true information and offer a very brief paper abstract and ml category   chat openly or privately to find the right review partners  in the paper review exchange channel  announce your paper reviewer upon agreement  from both sides   exchange your drafts privately and preferably with official email addresses   optional  when the review work is done  announce it too   note that plagiarism and stealing can be minimized in this mode but still could happen when conference reviews do not offer much nowadays  a mockup review might give your more true inputs  good luck 
44,44,AshkanF,vlacrj,[R] Can explainability improve model accuracy?,"&#x200B;

https://preview.redd.it/okh7r16770891.jpg?width=1200&format=pjpg&auto=webp&s=9f0fe7605453a945682d27eab65d866dce3f126c

Black-box Deep learning models are mostly uninterpretable and far too complex.

â€¢ One strategy is to learn the nonlinear relation of input features.

However, there are so many features to learn from.

https://preview.redd.it/muotby5s70891.png?width=782&format=png&auto=webp&s=1cbc3dece747d061e3ab96dea8b309c3fae5b8ce

&#x200B;

â€¢ Research shows a set of important features can improve the learning process.

Therefore, we can focus on the most correlated features.

â€¢ PaperðŸ“œ: [https://arxiv.org/abs/2203.04383](https://arxiv.org/abs/2203.04383)",1,1,2022-06-26 23:24:18, r  can explainability improve model accuracy ,  xb https black box deep learning models are mostly uninterpretable and far too complex   one strategy is to learn the nonlinear relation of input features however  there are so many features to learn from https   xb   research shows a set of important features can improve the learning process therefore  we can focus on the most correlated features   paper    https   arxiv org abs    https   arxiv org abs   
45,45,Ok-Seesaw9702,vl2r7q,[D] Derivation of path dependent attribution in Tree SHAP,"I was reading the TreeSHAP paper by Lundberg & Lee. There they propose that every path can be considered an individual model and due to additivity property of SHAP we can directly add the attributions for each path and that would give us the attribution for that tree.

I can understand till -

1. if a feature doesn't lie on the path then that feature's attribution for that path would be zero.
2. if feature lies on the path and also lies on the path of Xf then it's attribution is positive.
3. if feature lies on the path but doesn't lie on the path covered by Xf then attribution is negative.

But I can't get my head around the quantification of these contributions - especially the weighting.i.e., **POS = W(|Sp|-1, |Np|)\*v** ;  
**NEG = -W(|Sp, |Np|)\*v** ; where v is the leaf's update.

I have may questions, but to begin with, can someone please help me understand how do we get these attribution values ?",0,2,2022-06-26 17:02:53, d  derivation of path dependent attribution in tree shap,i was reading the treeshap paper by lundberg   lee  there they propose that every path can be considered an individual model and due to additivity property of shap we can directly add the attributions for each path and that would give us the attribution for that tree i can understand till    if a feature doesn t lie on the path then that feature s attribution for that path would be zero   if feature lies on the path and also lies on the path of xf then it s attribution is positive   if feature lies on the path but doesn t lie on the path covered by xf then attribution is negative but i can t get my head around the quantification of these contributions   especially the weighting i e     pos   w  sp     np    v        neg    w  sp   np    v     where v is the leaf s update i have may questions  but to begin with  can someone please help me understand how do we get these attribution values  
46,46,anvinhnd,vl65tk,[R] [D] How can one rigorously and efficiently deal with binary classification problems on multi-label data?,"To be clearer, I'd like to start learning about some techniques or the literature about this particular type of binary classification problems. Please share if you happen to know about this (keywords, links, articles, etc are all appreciated).

So, the problem is supervised binary classification. In general, there is nothing special about the dataset apart from the fact that the train/val data from one of the 2 label classes (from now on, let's say it's negative) are already further labeled into multiple subclasses. From there, the problem has an additional goal (other than binary classification): to maximize the number of subclasses that are classified well by the model. By ""classified well"", I mean that, for example, if one restricts the negative side of the dataset into one of such subclasses, the performance of the model is higher than some close-to-perfect thresholds.

Furthermore, there might be some complications in both ways: there might be some subclasses that are easy to classify by the model, and there might be some subclasses that are impossible to classify by the model (e.g. XOR problem with linear classifiers). The key here is that, in the end, at test time, one should only use one ""small"" (relatively of course) ""model"" (a combination of shallow neural nets is OK too) to classify all testing data.

Additionally, I'm open to learn about stuffs beyond the supervised paradigm.",0,0,2022-06-26 20:10:28, r   d  how can one rigorously and efficiently deal with binary classification problems on multi label data ,to be clearer  i d like to start learning about some techniques or the literature about this particular type of binary classification problems  please share if you happen to know about this  keywords  links  articles  etc are all appreciated  so  the problem is supervised binary classification  in general  there is nothing special about the dataset apart from the fact that the train val data from one of the  label classes  from now on  let s say it s negative  are already further labeled into multiple subclasses  from there  the problem has an additional goal  other than binary classification   to maximize the number of subclasses that are classified well by the model  by classified well  i mean that  for example  if one restricts the negative side of the dataset into one of such subclasses  the performance of the model is higher than some close to perfect thresholds furthermore  there might be some complications in both ways  there might be some subclasses that are easy to classify by the model  and there might be some subclasses that are impossible to classify by the model  e g  xor problem with linear classifiers   the key here is that  in the end  at test time  one should only use one small  relatively of course  model  a combination of shallow neural nets is ok too  to classify all testing data additionally  i m open to learn about stuffs beyond the supervised paradigm 
47,47,MurlocXYZ,vkp3y2,[D] Is it time to retire the FID?,"I know the main metric used to measure the quality of generative models is the FID. However, it seems to me that some problems arise when evaluating a generative model using another model. A couple that come to mind:
- Inception v3 itself is 7 years old at this point. Nowadays, we have models with much higher ImageNet classification accuracy, which presumably translates to better internal representations. Why are we still using Inception v3 instead of, for instance, ViT or some more recent model. 

- The ImageNet dataset that is commonly used to pretrain the Inceptionv3, while being quite comprehensive, is still limited to 1000 classes. If I want to train a model to generate classes that are semantically distant from ImageNet classes, what guarantees do I have that the activations of Inceptionv3 will be meaningful? This is more so problematic with models like DALL-E, which are trained on much larger datasets and can generate from the open set, essentially.

Perhaps I am misinterpreting things, but it seems to me that the FID is a case of ""good enough"" that sort of stuck around.

What are your thoughts?",7,23,2022-06-26 03:09:05, d  is it time to retire the fid ,i know the main metric used to measure the quality of generative models is the fid  however  it seems to me that some problems arise when evaluating a generative model using another model  a couple that come to mind   inception v itself is  years old at this point  nowadays  we have models with much higher imagenet classification accuracy  which presumably translates to better internal representations  why are we still using inception v instead of  for instance  vit or some more recent model    the imagenet dataset that is commonly used to pretrain the inceptionv  while being quite comprehensive  is still limited to  classes  if i want to train a model to generate classes that are semantically distant from imagenet classes  what guarantees do i have that the activations of inceptionv will be meaningful  this is more so problematic with models like dall e  which are trained on much larger datasets and can generate from the open set  essentially perhaps i am misinterpreting things  but it seems to me that the fid is a case of good enough that sort of stuck around what are your thoughts 
48,48,FitWin7383,vkkfv0,[P] Frechet Inception Distance,"I'm currently looking into quantifying GANS and from my current understanding, the way to go is the FID (Frechet Inception Distance) as a key metric. I read into it and have a basic understanding of how it works based on comparing the feature vectors of the Inception Model. In all the tutorials, I saw detailed implementation but they stopped after computing an FID between two images.

In all of the papers, I saw there is one FID score used to compare entire GAN architectures and I'm a bit lost about how many images they generate to compare and whether images generated get randomly paired for an average FID score.

 

TL;DR: The procedure behind comparing GAN architectures is unclear to me based on the FID.",15,43,2022-06-25 23:22:23, p  frechet inception distance,i m currently looking into quantifying gans and from my current understanding  the way to go is the fid  frechet inception distance  as a key metric  i read into it and have a basic understanding of how it works based on comparing the feature vectors of the inception model  in all the tutorials  i saw detailed implementation but they stopped after computing an fid between two images in all of the papers  i saw there is one fid score used to compare entire gan architectures and i m a bit lost about how many images they generate to compare and whether images generated get randomly paired for an average fid score  tl dr  the procedure behind comparing gan architectures is unclear to me based on the fid 
49,49,RodObr,vky2yx,[D] Sequence Modelling Technique,"Let's say we have a time series problem where we are trying to use past information to predict future inputs. Like stock prices, or heart rates, or a language model that receives one word at a time.

In theory you would want each output at t to contain the maximum amount of predictive information about label t+1.

Let's say you attach a second network to this RNN, which tries to predict hidden state t+1 from hidden state t and add it's error as an auxiliary loss. You could call it a ""Lookahead reconstruction loss""

I believe this should make the RNN learn in a way that maximises future understanding of the network.

Has anybody experimented with this technique, or read about implementations on this?

I'd be interested in hearing opinions from fellow practitioners.",4,4,2022-06-26 11:41:34, d  sequence modelling technique,let s say we have a time series problem where we are trying to use past information to predict future inputs  like stock prices  or heart rates  or a language model that receives one word at a time in theory you would want each output at t to contain the maximum amount of predictive information about label t  let s say you attach a second network to this rnn  which tries to predict hidden state t  from hidden state t and add it s error as an auxiliary loss  you could call it a lookahead reconstruction lossi believe this should make the rnn learn in a way that maximises future understanding of the network has anybody experimented with this technique  or read about implementations on this i d be interested in hearing opinions from fellow practitioners 
50,50,QadriShyaari,vl8rxu,[D] Clarification question related to prompting,What is the difference between prompt engineering and prompt learning? I recently heard a talk where the presenter said that â€˜we freeze the parameters of the model and only do prompt learningâ€™. To me that seems like engineering than learning.,2,0,2022-06-26 22:11:30, d  clarification question related to prompting,what is the difference between prompt engineering and prompt learning  i recently heard a talk where the presenter said that  we freeze the parameters of the model and only do prompt learning   to me that seems like engineering than learning 
51,51,yapoinder,vklemr,Is there any way of using a text editor with Kaggle or Google Colab notebooks? [Discussion],"UPDATE: SOLVED

The lovely people in the comments guided me to a better method of using github and cloning my repository in the kaggle runtime using the !git clone command. I was unaware you could clone a github repository and run a python file in this method. I was even able to create an anaconda environment and run everything smoothly. So everything is running smoothly again :D <3 <3 :D

&#x200B;

\-------------

I am training a video classification neural network which involves opencv based image augmentation and then after the training completes I run a series of test with my test datasets.

so with all of the functionality the code base is close to 6k lines of code.

This is really hard to work with in the current notebook cell format, if I want to make any changes I have to scroll a lot and often I get confused since my python Classes are thousands of lines each with many functions built in.

Using an editor like VSCODE is 10000x times easier than working with notebooks.

Has anyone figured this one out?

Yes I realize I can work in VSCODE on my local computer and then manually transfer the code to kaggle, but this is incredibly tedious when making small changes to file paths and general code changes.

Im shocked there isnt a better way around this !!! I mean c'mon how do we expect AI to be adopted by the masses if we cant have a streamlined way of developing software?

I guess the alternative is to buy a $6000 GPU and build a pc lol, i'm a broke student paying off student debt :( I am grateful for the free GPU with Kaggle,

I JUST WANT A SIMPLE TEXT EDITOR... is that too much to ask?",11,24,2022-06-26 00:08:49,is there any way of using a text editor with kaggle or google colab notebooks   discussion ,update  solvedthe lovely people in the comments guided me to a better method of using github and cloning my repository in the kaggle runtime using the  git clone command  i was unaware you could clone a github repository and run a python file in this method  i was even able to create an anaconda environment and run everything smoothly  so everything is running smoothly again  d      d  xb               i am training a video classification neural network which involves opencv based image augmentation and then after the training completes i run a series of test with my test datasets so with all of the functionality the code base is close to k lines of code this is really hard to work with in the current notebook cell format  if i want to make any changes i have to scroll a lot and often i get confused since my python classes are thousands of lines each with many functions built in using an editor like vscode is x times easier than working with notebooks has anyone figured this one out yes i realize i can work in vscode on my local computer and then manually transfer the code to kaggle  but this is incredibly tedious when making small changes to file paths and general code changes im shocked there isnt a better way around this     i mean c mon how do we expect ai to be adopted by the masses if we cant have a streamlined way of developing software i guess the alternative is to buy a   gpu and build a pc lol  i m a broke student paying off student debt    i am grateful for the free gpu with kaggle i just want a simple text editor    is that too much to ask 
52,52,XinshaoWang,vkc7fo,"[Research] Not all our papers get published, therefore it is enjoyable to see our released papers become a true foundation for other works","I read a post in linkedin (see links at the end) and find  
a similar case on our side: â€œNot all our papers get published, therefore it is enjoyable to see our released papers become a true foundation for other worksâ€.  


Our work: 

(1) IMAE demonstrates a robust loss could be unbounded, asymmetric; 

(2) Derivative Manipulation proposes gradient normalisation and emphasis density functions.  
\* IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters:Â [https://arxiv.org/pdf/1903.12141.pdf](https://arxiv.org/pdf/1903.12141.pdf)  
\* Derivative Manipulation for General Example Weighting:Â [https://arxiv.org/pdf/1905.11233.pdf](https://arxiv.org/pdf/1905.11233.pdf)  


The following works:

* ICML-20: Normalized Loss Functions for Deep Learning with Noisy  
Labels:Â [http://proceedings.mlr.press/v119/ma20c/ma20c.pdf](http://proceedings.mlr.press/v119/ma20c/ma20c.pdf)
* ICML-21: Asymmetric Loss Functions for Learning with Noisy LabelsÂ [https://proceedings.mlr.press/v139/zhou21f](https://proceedings.mlr.press/v139/zhou21f)

&#x200B;

More details and original source: 

* [https://www.linkedin.com/posts/xinshaowang\_the-probabilistic-normal-epipolar-constraint-activity-6944535197044367360-jpu5?utm\_source=linkedin\_share&utm\_medium=member\_desktop\_web](https://www.linkedin.com/posts/xinshaowang_the-probabilistic-normal-epipolar-constraint-activity-6944535197044367360-jpu5?utm_source=linkedin_share&utm_medium=member_desktop_web)
* [https://www.linkedin.com/posts/laurent-kneip-72518658\_the-probabilistic-normal-epipolar-constraint-activity-6944331307514531840-vQb1?utm\_source=linkedin\_share&utm\_medium=member\_desktop\_web](https://www.linkedin.com/posts/laurent-kneip-72518658_the-probabilistic-normal-epipolar-constraint-activity-6944331307514531840-vQb1?utm_source=linkedin_share&utm_medium=member_desktop_web)",8,93,2022-06-25 16:02:53, research  not all our papers get published  therefore it is enjoyable to see our released papers become a true foundation for other works,i read a post in linkedin  see links at the end  and find  a similar case on our side   not all our papers get published  therefore it is enjoyable to see our released papers become a true foundation for other works    our work     imae demonstrates a robust loss could be unbounded  asymmetric     derivative manipulation proposes gradient normalisation and emphasis density functions      imae for noise robust learning  mean absolute error does not treat examples equally and gradient magnitude s variance matters Â  https    derivative manipulation for general example weighting Â  https the following works   icml   normalized loss functions for deep learning with noisy  labels Â  http   icml   asymmetric loss functions for learning with noisy labelsÂ  https   xb more details and original source     https    https   www linkedin com posts laurent kneip  _the probabilistic normal epipolar constraint activity  vqb utm _source linkedin _share utm _medium member _desktop _web  https   www linkedin com posts laurent kneip _the probabilistic normal epipolar constraint activity  vqb utm_source linkedin_share utm_medium member_desktop_web 
53,53,Just_Ad8110,vka4im,[P] Waymo Motion Prediction Challenge 2022: solution with report and code,,0,8,2022-06-25 13:32:36, p  waymo motion prediction challenge   solution with report and code,nan
54,54,e2v-sde-parody,vjkssf,[D] How to copy text from more than 10 previously published papers and get accepted to CVPR 2022,"Hey, check out our (!) video (parody) that presents how our E2V-SDE paper (that has been accepted to CVPR 2022) largely consists of texts that are uncredited verbatim copies from more than 10 previously published papers. Enjoy!

&#x200B;

[https://youtube.com/watch?v=UCmkpLduptU](https://youtube.com/watch?v=UCmkpLduptU)",97,470,2022-06-24 15:26:42, d  how to copy text from more than  previously published papers and get accepted to cvpr ,hey  check out our     video  parody  that presents how our ev sde paper  that has been accepted to cvpr   largely consists of texts that are uncredited verbatim copies from more than  previously published papers  enjoy   xb  https   youtube com watch v ucmkplduptu  https   youtube com watch v ucmkplduptu 
55,55,yekitra,vjyihq,[D] What are the interesting SOTA models released in CVPR 2022?,"Hi Reddit,

Since the CVPR 2022 is wrapped up today and I've not tracked what happened this year. 

What are the interesting releases of this year that I should be looking at?

What new SOTA models are released?

Thanks",18,32,2022-06-25 02:39:23, d  what are the interesting sota models released in cvpr  ,hi reddit since the cvpr  is wrapped up today and i ve not tracked what happened this year  what are the interesting releases of this year that i should be looking at what new sota models are released thanks
56,56,Realistic_Ad_8107,vka648,[P] Synthetic Images Anomaly Detection with CLIP,"You have just generated a bunch of synthetic images by your favorite generative model. Most of them look great, but some looks really bad. These are outliers. Since GAN, the most popular generative model structure, doesnâ€™t produce a likelihood score for generated images, you can not know which of the images generated by it are outliers.

With the following method, you can inspect your synthetic dataset more efficiently than by just looking at all images.

First blog post on Medium. Let me know what you think.

&#x200B;

https://preview.redd.it/1bq8cmm29q791.png?width=260&format=png&auto=webp&s=5aa2b82e1f1bb4edd64d3f7658415dde1573e2ee

[Synthetic Images Anomaly Detection with CLIP](https://medium.com/p/e4fdf6af0169)",2,3,2022-06-25 13:35:43, p  synthetic images anomaly detection with clip,you have just generated a bunch of synthetic images by your favorite generative model  most of them look great  but some looks really bad  these are outliers  since gan  the most popular generative model structure  doesn t produce a likelihood score for generated images  you can not know which of the images generated by it are outliers with the following method  you can inspect your synthetic dataset more efficiently than by just looking at all images first blog post on medium  let me know what you think   xb https  synthetic images anomaly detection with clip  https   medium com p efdfaf 
57,57,,vk4oha,[D] Single camera MOT person tracklet re-identification: most suitable approaches?,"I have a pipeline that does object detection on video frames (YOLOX) and multi-object tracking (i.e., MOT) between person bounding boxes (ByteTrack). To be specific, given a single input video consisting of a *single fixed position camera* without cuts, I obtain a list of tracklets, where each tracklet tends to consist of a sequence of tens or hundreds of bounding boxes of the same person (and very rarely a mistaken doppelganger).

The MOT model used is SOTA, and each tracklet is accurate enough; but given long videos, long occlusions and out-of-frame movement still often result in the same person getting spread out across multiple separate tracklets.

Clearly I'd like to find a way to merge tracklets that actually correspond to the same person. In other words, a re-id problem.

However, 99% of the re-id literature seems to be mainly concerned with multi-camera re-id. (Probably driven by 1984-esque surveillance camera wet dreams, but that's a different topic.)

What is the SOTA for unsupervised (or online self-supervised) single camera re-id, preferably utilizing whole per tracklet latent space?

Or is this case approachable with something fairly vanilla like a similarity algo such as triplet margin loss?

Any suggestions in how to approach this grey area in-between MOT and Re-id much appreciated.",6,10,2022-06-25 07:54:09, d  single camera mot person tracklet re identification  most suitable approaches ,i have a pipeline that does object detection on video frames  yolox  and multi object tracking  i e   mot  between person bounding boxes  bytetrack   to be specific  given a single input video consisting of a  single fixed position camera  without cuts  i obtain a list of tracklets  where each tracklet tends to consist of a sequence of tens or hundreds of bounding boxes of the same person  and very rarely a mistaken doppelganger  the mot model used is sota  and each tracklet is accurate enough  but given long videos  long occlusions and out of frame movement still often result in the same person getting spread out across multiple separate tracklets clearly i d like to find a way to merge tracklets that actually correspond to the same person  in other words  a re id problem however    of the re id literature seems to be mainly concerned with multi camera re id   probably driven by  esque surveillance camera wet dreams  but that s a different topic  what is the sota for unsupervised  or online self supervised  single camera re id  preferably utilizing whole per tracklet latent space or is this case approachable with something fairly vanilla like a similarity algo such as triplet margin loss any suggestions in how to approach this grey area in between mot and re id much appreciated 
58,58,daichrony,vjymjx,"[D] Is it possible to make a model that will outperform a human, if the model was solely trained on that human's prior predictions?","Say a single radiologist has a ton of images that they have labeled cancer / not cancer. Can we use the labels and those images from just the one radiologist to make a model that will be better at predicting cancer / not cancer than the radiologist? 

Intuitively it seems like that would not be possible unless by chance it does better, but ML/DL has a way of being able to extrapolate/generalize patterns and sometimes spot things we missed? Perhaps an ensemble of various models, or maybe that would just lead to overfitting? 

No particular application, just a random question I had been pondering. Appreciate any thoughts and/or references.",23,17,2022-06-25 02:44:41, d  is it possible to make a model that will outperform a human  if the model was solely trained on that human s prior predictions ,say a single radiologist has a ton of images that they have labeled cancer   not cancer  can we use the labels and those images from just the one radiologist to make a model that will be better at predicting cancer   not cancer than the radiologist  intuitively it seems like that would not be possible unless by chance it does better  but ml dl has a way of being able to extrapolate generalize patterns and sometimes spot things we missed  perhaps an ensemble of various models  or maybe that would just lead to overfitting  no particular application  just a random question i had been pondering  appreciate any thoughts and or references 
59,59,mrwafflezzz,vk9gud,[P] Oddly thresholded confidence scores on scaled yolov4 csp,"All object detections on the scaled yolov4 csp model have a confidence below 0.5, while it should range from 0 to 1. Does anything come to mind as to what the problem might be?

Info:

* I'm using a branch of the [author's PyTorch repo](https://github.com/WongKinYiu/ScaledYOLOv4/tree/yolov4-csp) 
* Predictions are otherwise pretty good in terms of bbox placement
* I'm training on a single gpu
* Darknet coco weights are converted to "".pt"" PyTorch weights for training
* A custom dataset is used with a single prediction class
* Data is augmented before training starts, most of the dataloader's data augmentation is disabled",0,2,2022-06-25 12:47:42, p  oddly thresholded confidence scores on scaled yolov csp,all object detections on the scaled yolov csp model have a confidence below    while it should range from  to   does anything come to mind as to what the problem might be info   i m using a branch of the  author s pytorch repo  https   predictions are otherwise pretty good in terms of bbox placement  i m training on a single gpu  darknet coco weights are converted to  pt pytorch weights for training  a custom dataset is used with a single prediction class  data is augmented before training starts  most of the dataloader s data augmentation is disabled
60,60,zy415,vjqdom,[D] Niche ML Venues vs Top ML Conferences,"Since top ML conferences (e.g. NeurIPS, ICML, AISTATS, UAI, ICLR) are getting too large, there are quite some niche venues focusing on different subfields of ML:
- Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM): https://rldm.org/
- Machine Learning for Health (ML4H): https://ml4health.github.io/
- Learning on Graphs Conference (LoG): https://logconference.org/
- Symposium on Advances in Approximate Bayesian Inference (AABI): http://approximateinference.org/
- International Conference on Automated Machine Learning (AutoML-Conf): https://automl.cc/
- Conference on Causal Learning and Reasoning (CLeaR): https://www.cclear.cc/
- Conference on Lifelong Learning Agents (CoLLAs): https://lifelong-ml.cc/

Some of these conferences are quite new and grew out of different workshops. Many of them are trying to establish themselves as top venues in their niche fields. Here, I would like to get some opinions from the ML folks. Could folks comment on these conferences, e.g., based on different dimensions?
- **Prestige**: Are these conferences perceived to be as pretigious as the top ML conferences? 
- **Usefulness**: Does hiring committee in the academia and industry treat these conferences the same as top ML conferences? If not, how much will the nich conferences be discounted? (Closely tied to prestige, though)
- **Dissemination**: Are papers at these niche conferences much less visible to researchers outside the subfields? (This seems important to me because research nowadays often leverages ideas from different fields.)
- **Difficulty**: Is it easier to get papers accepted at these conferences as compared to top ML coneferences?
- **Networking**: Is there really more opportunity to get to know folks working in the same subfields at these conferences (given that it is much smaller)?

Disclaimer:
- I have published several papers in the top ML conferences listed above, and am considering whether to try out niche ML conferences. Personally, this feels like a ""bet"" for me on whether the niche conference will be successful in the future.
- I know some folks might comment that the quality of research is the most important as compared to the publication venues. However, let's for now assume all things being equal and that, e.g., a graduate student is deciding whether to submit a paper to a general ML conference or a niche venue.",16,29,2022-06-24 20:28:33, d  niche ml venues vs top ml conferences,since top ml conferences  e g  neurips  icml  aistats  uai  iclr  are getting too large  there are quite some niche venues focusing on different subfields of ml   multi disciplinary conference on reinforcement learning and decision making  rldm   https   machine learning for health  mlh   https   learning on graphs conference  log   https   symposium on advances in approximate bayesian inference  aabi   http   international conference on automated machine learning  automl conf   https   conference on causal learning and reasoning  clear   https   conference on lifelong learning agents  collas   https some of these conferences are quite new and grew out of different workshops  many of them are trying to establish themselves as top venues in their niche fields  here  i would like to get some opinions from the ml folks  could folks comment on these conferences  e g   based on different dimensions     prestige    are these conferences perceived to be as pretigious as the top ml conferences      usefulness    does hiring committee in the academia and industry treat these conferences the same as top ml conferences  if not  how much will the nich conferences be discounted   closely tied to prestige  though     dissemination    are papers at these niche conferences much less visible to researchers outside the subfields   this seems important to me because research nowadays often leverages ideas from different fields      difficulty    is it easier to get papers accepted at these conferences as compared to top ml coneferences     networking    is there really more opportunity to get to know folks working in the same subfields at these conferences  given that it is much smaller  disclaimer   i have published several papers in the top ml conferences listed above  and am considering whether to try out niche ml conferences  personally  this feels like a bet for me on whether the niche conference will be successful in the future   i know some folks might comment that the quality of research is the most important as compared to the publication venues  however  let s for now assume all things being equal and that  e g   a graduate student is deciding whether to submit a paper to a general ml conference or a niche venue 
61,61,Gramious,vjkujp,"[D] ""The uncanny valley demonstrating it's treasures and failures, studio lighting digital art"", DALLE-2 prompt. An artist friend has recently been given access and I was trying to feed him prompts that 'broke' the system (e.g., Gaussian noise, one million colours, uncanny valley, etc.).","I had some fun with DALL-E 2 last night because a friend of mine ([instagram.com/photonwind/](https://instagram.com/photonwind/)) was given access last night and was streaming, letting us feed it prompts. I wanted to break the system, find its edges, or give prompts that gave me insight into the underlying function being modelled.   


I tried: ""Gaussian noise"", ""One million colours"" and ""The uncanny valley demonstrating it's treasures and failures, studio lighting digital art"". The latter looks the most interesting to me:  


[The uncanny valley demonstrating it's treasures and failures, studio lighting digital art](https://preview.redd.it/d1acsi05lj791.jpg?width=1024&format=pjpg&auto=webp&s=f0eecc93caee7b93a470777a0fff4bde5dc71a88)

That said, ""One million colours"" is pretty epic too:  


[One million colours](https://preview.redd.it/2tk54r99lj791.png?width=3072&format=png&auto=webp&s=7ecad24840e5433822a06ce7b83d193ab0a20945)

But, Gaussian noise is just broken:  


[Gaussian noise](https://preview.redd.it/tkx2zi2clj791.png?width=2048&format=png&auto=webp&s=d517f44097150850289849465649d87586c83277)",4,38,2022-06-24 15:30:07, d  the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  dalle  prompt  an artist friend has recently been given access and i was trying to feed him prompts that  broke  the system  e g   gaussian noise  one million colours  uncanny valley  etc   ,i had some fun with dall e  last night because a friend of mine   instagram com photonwind   https i tried  gaussian noise  one million colours and the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  the latter looks the most interesting to me    the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  https that said  one million colours is pretty epic too    one million colours  https but  gaussian noise is just broken    gaussian noise  https   preview redd it tkxziclj png width  format png auto webp s dfdc 
62,62,THE_REAL_ODB,vjglr9,[D]Anyone use self-supervised learning at work? I'm surprised at how effective it has been for me.,"I've been using this stuff for sniffing near duplicates at work and been surprised how effect it has been!

PLanning to try it out some downstream tasks in the future to see how well it does!

I will say though it does take a shit ton of computing resources, but I find it really cool.",22,62,2022-06-24 10:41:18, d anyone use self supervised learning at work  i m surprised at how effective it has been for me ,i ve been using this stuff for sniffing near duplicates at work and been surprised how effect it has been planning to try it out some downstream tasks in the future to see how well it does i will say though it does take a shit ton of computing resources  but i find it really cool 
63,63,Which-Distance1384,vk1qxo,[D] A/B testing when there is a feedback loop,"I am experimenting with changing label value (target) for a model that we have in production. We used to cap the target variable, and my new model will release the cap.

&#x200B;

The main point about our production space is that there is a positive feedback loop involved. So, we expect that when we release the cap, my model would result in a section of users having more activity. However, since most of user traffic goes to control arm, only a fraction of it goes to experiment and thus the feedback loop doesnt close unless we have 50-50% experiment (that we can't).

&#x200B;

Wondering, if there is any way to run an A/B test and compare the production model and my model. The labels are shifting as well as the control loop doesn't close.

&#x200B;

Any idea is highly appreciated.",2,1,2022-06-25 05:16:20, d  a b testing when there is a feedback loop,i am experimenting with changing label value  target  for a model that we have in production  we used to cap the target variable  and my new model will release the cap   xb the main point about our production space is that there is a positive feedback loop involved  so  we expect that when we release the cap  my model would result in a section of users having more activity  however  since most of user traffic goes to control arm  only a fraction of it goes to experiment and thus the feedback loop doesnt close unless we have    experiment  that we can t    xb wondering  if there is any way to run an a b test and compare the production model and my model  the labels are shifting as well as the control loop doesn t close   xb any idea is highly appreciated 
64,64,ffast-math,vj7nf5,"[P] Farewell, CUDA OOM: Automatic Gradient Accumulation","Hey everyone,

If you've trained a lot of neural nets, you probably know the pain of getting CUDA OOM errors and iteratively tuning your batch size to avoid them.

Which is why I'm excited to announce that we (MosaicML) just released an automatic way to avoid these errors. Namely, we just added [automatic gradient accumulation](https://docs.mosaicml.com/en/latest/notes/auto_grad_accum.html) to [Composer](https://github.com/mosaicml/composer), our open source library for faster + easier neural net training.

If you're not familiar with gradient accumulation, it's like tuning the batch size, but without messing with the optimization (aside from slightly different BatchNorm stats). This lets you avoid tuning learning rate, weight decay, etc based on how much memory your GPU has or how many GPUs you're training on.

https://preview.redd.it/ogxq73znuf791.png?width=1374&format=png&auto=webp&s=93ff0b76a2293a73a5380b7e93f62fe34c604bc4

What's nice about the \*automatic\* gradient accumulation in Composer is that you just set the batch size and hparams once and you're doneâ€”no need to tune the gradient accumulation manually.

More info in our [blog post](https://www.mosaicml.com/blog/farewell-oom), and special thanks to [Mihir Patel](https://mvpatel2000.github.io/) for building most of this. Happy to answer questions!",40,123,2022-06-24 02:58:18, p  farewell  cuda oom  automatic gradient accumulation,hey everyone if you ve trained a lot of neural nets  you probably know the pain of getting cuda oom errors and iteratively tuning your batch size to avoid them which is why i m excited to announce that we  mosaicml  just released an automatic way to avoid these errors  namely  we just added  automatic gradient accumulation  https if you re not familiar with gradient accumulation  it s like tuning the batch size  but without messing with the optimization  aside from slightly different batchnorm stats   this lets you avoid tuning learning rate  weight decay  etc based on how much memory your gpu has or how many gpus you re training on https what s nice about the   automatic   gradient accumulation in composer is that you just set the batch size and hparams once and you re done no need to tune the gradient accumulation manually more info in our  blog post  https   www mosaicml com blog farewell oom   and special thanks to  mihir patel  https   mvpatel github io   for building most of this  happy to answer questions 
65,65,vikarjramun,vj0t0l,[P] Reverse Engineering Google Colab,"Hi!

I've spent a lot of time working with Google Colab recently, and was disappointed that such a powerful platform was limited to only running Jupyter notebooks. So I took a deep dive into the internals of Colab, discovering tons of interesting hidden features!

[Take a look at what I found!](https://dagshub.com/blog/reverse-engineering-google-colab/)",14,284,2022-06-23 21:54:19, p  reverse engineering google colab,hi i ve spent a lot of time working with google colab recently  and was disappointed that such a powerful platform was limited to only running jupyter notebooks  so i took a deep dive into the internals of colab  discovering tons of interesting hidden features  take a look at what i found   https   dagshub com blog reverse engineering google colab  
66,66,ElongatedMuskrat122,vk47y4,[D] How do you guys usually go about normalizing sales data? Opinion on neural networks for business data...,"Working on a project right now, and I have sales amounts as a column. Normally I would throw this into XGBoost, and let it rip, but, I am thinking this might benefit from a DNN. 

 \- For those who have used neural networks for business data, what was your experience using it?

 \- How did you normalize values like sales data? Did you just divide by the max, or not normalize at all?",7,0,2022-06-25 07:28:52, d  how do you guys usually go about normalizing sales data  opinion on neural networks for business data   ,working on a project right now  and i have sales amounts as a column  normally i would throw this into xgboost  and let it rip  but  i am thinking this might benefit from a dnn      for those who have used neural networks for business data  what was your experience using it     how did you normalize values like sales data  did you just divide by the max  or not normalize at all 
67,67,japanhue,vjn9jv,"[R] Unpublished physics inspired ML paper from 2021 (Yang-Mills theory, differential geometry, gauge theory)","Hi there,

The purpose of this post is to share a [research paper/notebook](https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf) I wrote that has been mostly unread and unnoticed by others, and also to ask how to find research collaborators without participating in academia or industry.

After I finished my BSc, I was deeply interested in geometric deep learning and wrote this paper \[0\] describing an attention mechanism using ideas from differential geometry and gauge theory commonly used in the standard model (via Yang-Mills theory). At the time, I sent the notebook/paper to every researcher in the geometric DL area that I was aware of but didn't get any replies or interest in collaboration. Without any openings and at the peak of a pandemic, I sadly had to drop the idea and get a standard software engineer job.

Since then, I've seen much of the rough ideas explored and developed independently by others. For example, M. Bronstein and his collaborators have similar applications of using connections (equivalent to sheafs) and Ricci flow in Graph NNs \[1\]. I have more ideas that I would like to explore, but feel destined to be an outsider in this field with my work unnoticed or considered illegitimate. Is it possible for people like me to collaborate with other researchers outside of academic institutions or industry? Does anyone know of such an organization?

Thanks

\[0\] [https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf](https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf)

\[1\] [https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/](https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/)",3,6,2022-06-24 17:55:42, r  unpublished physics inspired ml paper from   yang mills theory  differential geometry  gauge theory ,hi there the purpose of this post is to share a  research paper notebook  https after i finished my bsc  i was deeply interested in geometric deep learning and wrote this paper      describing an attention mechanism using ideas from differential geometry and gauge theory commonly used in the standard model  via yang mills theory   at the time  i sent the notebook paper to every researcher in the geometric dl area that i was aware of but didn t get any replies or interest in collaboration  without any openings and at the peak of a pandemic  i sadly had to drop the idea and get a standard software engineer job since then  i ve seen much of the rough ideas explored and developed independently by others  for example  m  bronstein and his collaborators have similar applications of using connections  equivalent to sheafs  and ricci flow in graph nns       i have more ideas that i would like to explore  but feel destined to be an outsider in this field with my work unnoticed or considered illegitimate  is it possible for people like me to collaborate with other researchers outside of academic institutions or industry  does anyone know of such an organization thanks      https       https   thegradient pub graph neural networks beyond message passing and weisfeiler lehman   https   thegradient pub graph neural networks beyond message passing and weisfeiler lehman  
68,68,aifordummies,vja009,[D] CVPR wants to penalize reviewers for violating the reviewer guideline!,"I cannot believe that CVPR put this motion for voting:

Motion 3: ""Any reviewer who has accepted an invitation to review but violates the reviewing guidelines set forth by the conference will be prohibited from submitting any papers to CVPR for up to two years.""

Reviewing is a community service, and although I have encountered bad and unfair reviews multiple times, I don't think such a wild action is the way to go to increase the review process quality. Let's start with the training process and choosing qualified AC and Meta ACs first where they can properly oversee the review process, choose fit reviewers, and take action in the rebuttal process.

If this goes through I would never review for CVPR again.

[https://mobile.twitter.com/KostasPenn/status/1539805992145358850](https://mobile.twitter.com/KostasPenn/status/1539805992145358850)

&#x200B;

**UPDATE: All motions have passed!**

Motion 1: Withdrawn Submissions Will No Longer Be Made Inaccessible to Reviewers  
Yes: 867  
No: 354

&#x200B;

Motion 2: Making Authors Responsible for Reviewing  
Yes: 677  
No: 553

Motion 3: Penalties for Violations of the CVPR Reviewing Guidelines  
Yes: 655  
No: 574

&#x200B;

Truly a sad day for ML research community.",43,71,2022-06-24 04:49:16, d  cvpr wants to penalize reviewers for violating the reviewer guideline ,i cannot believe that cvpr put this motion for voting motion   any reviewer who has accepted an invitation to review but violates the reviewing guidelines set forth by the conference will be prohibited from submitting any papers to cvpr for up to two years reviewing is a community service  and although i have encountered bad and unfair reviews multiple times  i don t think such a wild action is the way to go to increase the review process quality  let s start with the training process and choosing qualified ac and meta acs first where they can properly oversee the review process  choose fit reviewers  and take action in the rebuttal process if this goes through i would never review for cvpr again  https   xb   update  all motions have passed   motion   withdrawn submissions will no longer be made inaccessible to reviewers  yes    no    xb motion   making authors responsible for reviewing  yes    no  motion   penalties for violations of the cvpr reviewing guidelines  yes    no    xb truly a sad day for ml research community 
69,69,htrp,vivji3,[P] Yandex open sources 100b large language model weights (YaLM),"PR Announcement: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6


Github: https://github.com/yandex/YaLM-100B

Network is trained using same principles as Megatron LM, inference alone will require 4 A100s",52,288,2022-06-23 17:45:39, p  yandex open sources b large language model weights  yalm ,pr announcement  https github  https network is trained using same principles as megatron lm  inference alone will require  as
70,70,bandalorian,vjvl6t,[D] Using a neural net on bag of words vector vs PCA doe classification,"I have a document set that I wish to classify. I have tried with transformers, and they perform well, but the content is largely keyword driven so a lot of the attention stuff is not needed. It's a more deterministic system that needs to learn keyword combinations. So a count vectorizer over unigrams and bigrams, and then a classifier like XGBoost seems like a good idea. The problem is even after some pruning I get a feature vector of 26K.  I'd also like to compare this to a how a simple neural net handles it.

I was going to apply sparse PCA to get the dimensionality down first. However for a neural net, does it make sense to do PCA first? Isn't that what the embeddings are doing? Basically, the tasks of PCA + classifier model are carried out by the embedding and classification layers of a neural net. But just feeding 26 K dimensions to a neural net seems lazy, but if I reduce it to say 768 dimensions, I've basically carried out the whole embedding task before I pass it to the neural net, which limits the improvements it can make.

Would a happy medium of reducing to say 5K dimensions and then letting the neural net take it from there? I'm in the process of testing all of this in the next couple of weeks, but curious if anyone has any experience/insight/guesses.",6,1,2022-06-25 00:23:23, d  using a neural net on bag of words vector vs pca doe classification,i have a document set that i wish to classify  i have tried with transformers  and they perform well  but the content is largely keyword driven so a lot of the attention stuff is not needed  it s a more deterministic system that needs to learn keyword combinations  so a count vectorizer over unigrams and bigrams  and then a classifier like xgboost seems like a good idea  the problem is even after some pruning i get a feature vector of k   i d also like to compare this to a how a simple neural net handles it i was going to apply sparse pca to get the dimensionality down first  however for a neural net  does it make sense to do pca first  isn t that what the embeddings are doing  basically  the tasks of pca   classifier model are carried out by the embedding and classification layers of a neural net  but just feeding  k dimensions to a neural net seems lazy  but if i reduce it to say  dimensions  i ve basically carried out the whole embedding task before i pass it to the neural net  which limits the improvements it can make would a happy medium of reducing to say k dimensions and then letting the neural net take it from there  i m in the process of testing all of this in the next couple of weeks  but curious if anyone has any experience insight guesses 
71,71,Time-Archer-8103,vjpcij,[P] Implementing CRF-CNN model in python,"I am trying to implement a [research paper](https://ieeexplore.ieee.org/document/8546073) that uses CNN and CRF for page object detection. According to the research paper we have to to build two neural network (named unary and pairwise). Then the training data (set of images) are passed and both the CNNs are trained. After that we are supposed to apply CRF.

&#x200B;

Following are the equations for CRF:

&#x200B;

https://preview.redd.it/ckrm2rzutk791.png?width=768&format=png&auto=webp&s=ed88d8705b515beaf955d09aa194fa63707f7cca

U and V are unary and pairwise potentials obtained from the CNNs using the following equations:

&#x200B;

https://preview.redd.it/uahcpzgwtk791.png?width=813&format=png&auto=webp&s=bb3548539db1c9b1be3367f2ddd529f1ba32c5f3

&#x200B;

Maximum a posteriori (MAP) strategy to predict the labels of line regions given a new document. MAP inference of CRFs can be formulated as the following optimization problem:

&#x200B;

&#x200B;

&#x200B;

https://preview.redd.it/sz0537wwtk791.png?width=273&format=png&auto=webp&s=638b88b012a0158bce017be14b7e81639199a681

The parameters of our CRFs include Unary-Net's weights  and Pairwise-Net's weights  and a combination coefficient vector Î» of U and V. weights of U and V (w)  are learned using the SGD method. Then they are fixed and Î» is learned using the Pseudo Likelihood method.

&#x200B;

&#x200B;

I have created the neural networks but I am not able to implement the CRF part. Can someone help me implement this or suggest a python library that makes it easier to implement. (I have tried a python library [pystruct](https://pystruct.github.io/) but could not install it)",0,3,2022-06-24 19:40:11, p  implementing crf cnn model in python,i am trying to implement a  research paper  https   xb following are the equations for crf   xb https u and v are unary and pairwise potentials obtained from the cnns using the following equations   xb https   xb maximum a posteriori  map  strategy to predict the labels of line regions given a new document  map inference of crfs can be formulated as the following optimization problem   xb   xb   xb https the parameters of our crfs include unary net s weights  and pairwise net s weights  and a combination coefficient vector Î» of u and v  weights of u and v  w   are learned using the sgd method  then they are fixed and Î» is learned using the pseudo likelihood method   xb   xb i have created the neural networks but i am not able to implement the crf part  can someone help me implement this or suggest a python library that makes it easier to implement   i have tried a python library  pystruct  https   pystruct github io   but could not install it 
72,72,FundF,vjox5u,[P] What The Plug: An app that identifies electrical plugs,"I have built a convolutional neural network that identifies roughly 20 different plug types. I wrote most code with Keras on top of Tensorflow in Python. I trained the model on my personal computer using Linux and CUDA to train with my GPU. Afterwards I transformed the model to a .tflite file and embedded it in a swift app for iPhone.

Machine learning and programming is not my main field of work. Actually it's my first project in both areas. During the last three years I have taught myself the principals of machine learning as well as Python and Swift.

I hope some of you are interested in trying out the app. I would love to hear your feedback.

The app is 100% free by the way. I just want to see people use what I have build.

Here is the link to the app store:

[https://apps.apple.com/de/app/what-the-plug/id1613147033](https://apps.apple.com/de/app/what-the-plug/id1613147033)",0,2,2022-06-24 19:20:15, p  what the plug  an app that identifies electrical plugs,i have built a convolutional neural network that identifies roughly  different plug types  i wrote most code with keras on top of tensorflow in python  i trained the model on my personal computer using linux and cuda to train with my gpu  afterwards i transformed the model to a  tflite file and embedded it in a swift app for iphone machine learning and programming is not my main field of work  actually it s my first project in both areas  during the last three years i have taught myself the principals of machine learning as well as python and swift i hope some of you are interested in trying out the app  i would love to hear your feedback the app is   free by the way  i just want to see people use what i have build here is the link to the app store  https   apps apple com de app what the plug id  https   apps apple com de app what the plug id 
73,73,ItzDerock,vjtns4,[D] Need opinions for GPU server build.,"Work is getting a new server for ml/deep learning. 
Price isn't an issue, not looking to cut down much, just wanted to make sure that I'm not overlooking anything in terms of compatibility. 

My main concern is the CPU, would you recommend getting more cores/higher clock, or is it fine? 

https://docs.google.com/spreadsheets/d/17EQ_ZLQGDuaq5ECPpH_V7HKRC8QP-2qyoqvzKuXJoWI/edit?usp=drivesdk",7,0,2022-06-24 22:56:16, d  need opinions for gpu server build ,work is getting a new server for ml deep learning  price isn t an issue  not looking to cut down much  just wanted to make sure that i m not overlooking anything in terms of compatibility  my main concern is the cpu  would you recommend getting more cores higher clock  or is it fine  https   docs google com spreadsheets d eq_zlqgduaqecpph_vhkrcqp qyoqvzkuxjowi edit usp drivesdk
74,74,XtremePocket,vjs5jj,[D] Loss for generating sequences of items,"Let's say you have a task where you need to generate blobs of texts using a AR LM. The targets are separated in the form of `[blob1], [blob2], ...` where each blob contains some numbers and letters, and the order of the blobs matters. Now, a naive way would be just to train the network to generate tokens greedily. But could we do better? A greedy loss could still theoretically give us a great model, but is there another way that exploits the blob patterns?

An idea I have: If we believe the model should first learn existence of blobs then learn the order (a fair assumption in my application), we could first find a matching between all generated blobs and target blobs and optimize the best matches only, then impose a penalty to get the order right. The order might be enforced via maybe taking a weighted average between the greedy loss and the blob-matched loss?

What do you think?",1,0,2022-06-24 21:48:34, d  loss for generating sequences of items,let s say you have a task where you need to generate blobs of texts using a ar lm  the targets are separated in the form of   blob    blob        where each blob contains some numbers and letters  and the order of the blobs matters  now  a naive way would be just to train the network to generate tokens greedily  but could we do better  a greedy loss could still theoretically give us a great model  but is there another way that exploits the blob patterns an idea i have  if we believe the model should first learn existence of blobs then learn the order  a fair assumption in my application   we could first find a matching between all generated blobs and target blobs and optimize the best matches only  then impose a penalty to get the order right  the order might be enforced via maybe taking a weighted average between the greedy loss and the blob matched loss what do you think 
75,75,gambs,vj2kc3,[R] Learning to Play Minecraft with Video PreTraining (VPT),"[OpenAI Blog: Learning to Play Minecraft with Video PreTraining (VPT)](https://openai.com/blog/vpt/)

OpenAI gathered a large dataset of human Minecraft demonstrations and trained an Inverse Dynamics Model (IDM) transformer that predicts actions based on past and future frames using a dataset of human demonstrations. They used this model to label 70k hours of video, which is used to train a Video PreTraining (VPT) model, which predicts actions based on past frames alone, using behavioral cloning (i.e. supervised learning).

They can then fine-tune the VPT via behavioral cloning on narrower datasets or RL (with a hand-designed reward function that rewards the agent for going deeper into the tech tree or obtaining materials that could lead to a diamond pickaxe) and are able to train an agent that can craft a diamond pickaxe in 2.5% of its 10-minute long episodes.",8,45,2022-06-23 23:12:15, r  learning to play minecraft with video pretraining  vpt , openai blog  learning to play minecraft with video pretraining  vpt   https openai gathered a large dataset of human minecraft demonstrations and trained an inverse dynamics model  idm  transformer that predicts actions based on past and future frames using a dataset of human demonstrations  they used this model to label k hours of video  which is used to train a video pretraining  vpt  model  which predicts actions based on past frames alone  using behavioral cloning  i e  supervised learning  they can then fine tune the vpt via behavioral cloning on narrower datasets or rl  with a hand designed reward function that rewards the agent for going deeper into the tech tree or obtaining materials that could lead to a diamond pickaxe  and are able to train an agent that can craft a diamond pickaxe in    of its  minute long episodes 
76,76,scoutsearchteam,vjamsw,[Project] Semantic Search powerup for Ctrl+F,"Hi Reddit!

Scout Search is a project I've been working on as a Find-in-Page replacement.

It uses a semantic search engine (rather than character matching) to help you find what you're looking for on websites.

Try it out and let me know what you think.

[https://chrome.google.com/webstore/detail/scout-search/hgljpodblkjjklailoaefokflfdeffdl](https://chrome.google.com/webstore/detail/scout-search/hgljpodblkjjklailoaefokflfdeffdl)",4,9,2022-06-24 05:20:36, project  semantic search powerup for ctrl f,hi reddit scout search is a project i ve been working on as a find in page replacement it uses a semantic search engine  rather than character matching  to help you find what you re looking for on websites try it out and let me know what you think  https   chrome google com webstore detail scout search hgljpodblkjjklailoaefokflfdeffdl  https   chrome google com webstore detail scout search hgljpodblkjjklailoaefokflfdeffdl 
77,77,optimized-adam,vjlhee,[D] Publishing two papers at the same time,"Let's say I have done some research, developed some ideas and gotten good results. But there are two main ideas that tackle different problems and don't really belong in the same paper, although there is some relationship between them. The paper of idea #2 would cite and use idea #1. What have you done in similar situations? Can you try to publish both at the same time and have a citation to the first paper that hasn't even been published yet? Post on arXiv and try to publish the first one first, then the second one?",4,0,2022-06-24 16:11:38, d  publishing two papers at the same time,let s say i have done some research  developed some ideas and gotten good results  but there are two main ideas that tackle different problems and don t really belong in the same paper  although there is some relationship between them  the paper of idea   would cite and use idea    what have you done in similar situations  can you try to publish both at the same time and have a citation to the first paper that hasn t even been published yet  post on arxiv and try to publish the first one first  then the second one 
78,78,Supremefigur,vjw1a5,[R] Anatomy of an AI System [Infographic],"[https://anatomyof.ai/img/ai-anatomy-map.pdf](https://anatomyof.ai/img/ai-anatomy-map.pdf)  


A beautiful infographic explaining the whole process",0,0,2022-06-25 00:43:47, r  anatomy of an ai system  infographic , https a beautiful infographic explaining the whole process
79,79,SleekEagle,viyh17,[D] How Imagen Actually Works,"Hey everyone!

[I wrote this article explaining how Imagen actually works](https://www.assemblyai.com/blog/how-imagen-actually-works/), with a general overview for the big picture ideas and a Deep Dive to get into the nitty-gritty.

I'm happy to answer any questions, let me know what you think!

https://preview.redd.it/17xc5fqeud791.png?width=3472&format=png&auto=webp&s=e78a024892a3032ffc0c143b7843a5223751afcb",19,33,2022-06-23 20:10:27, d  how imagen actually works,hey everyone  i wrote this article explaining how imagen actually works  https i m happy to answer any questions  let me know what you think https   preview redd it xcfqeud png width  format png auto webp s eaaffccbaafcb
80,80,ManagementBig2995,vj6uh1,[P] HyperImpute: sklearn-style library for handling missing data using novel algorithms,"There are many data imputation algorithms for machine learning. However, benchmarking them can be complicated, mainly because most implementations stay just as research code to reproduce the experiments in the papers. Moreover, when dealing with tabular data, you need to handle continuous/discrete/categorical data correctly -- not just let some regressor approximate everything.

HyperImpute is a library that should make it easy to benchmark new imputation algorithms while offering several state-of-the-art models. For example, imputing using MIWAE can be done as easy as this:

    import pandas as pd
    import numpy as np
    from hyperimpute.plugins.imputers import Imputers
    
    X = pd.DataFrame([[1, 1, 1, 1], [4, 5, np.nan, np.nan], [3, 3, 9, 9], [2, 2, 2, 2]])
    
    plugin = Imputers().get(""miwae"")
    out = plugin.fit_transform(X.copy())
    
    out

Bonus, it can be easily plugged into sklearn pipelines.

Try it in Colab: [https://colab.research.google.com/drive/1zGm4VeXsJ-0x6A5\_icnknE7mbJ0knUig?usp=sharing](https://colab.research.google.com/drive/1zGm4VeXsJ-0x6A5_icnknE7mbJ0knUig?usp=sharing)

Github page: [https://github.com/vanderschaarlab/hyperimpute](https://github.com/vanderschaarlab/hyperimpute)

If you find the project useful, please star it on Github, it would help a lot!",0,10,2022-06-24 02:22:22, p  hyperimpute  sklearn style library for handling missing data using novel algorithms,there are many data imputation algorithms for machine learning  however  benchmarking them can be complicated  mainly because most implementations stay just as research code to reproduce the experiments in the papers  moreover  when dealing with tabular data  you need to handle continuous discrete categorical data correctly    not just let some regressor approximate everything hyperimpute is a library that should make it easy to benchmark new imputation algorithms while offering several state of the art models  for example  imputing using miwae can be done as easy as this     import pandas as pd    import numpy as np    from hyperimpute plugins imputers import imputers        x   pd dataframe                 np nan  np nan                               plugin   imputers   get miwae     out   plugin fit_transform x copy           outbonus  it can be easily plugged into sklearn pipelines try it in colab   https github page   https if you find the project useful  please star it on github  it would help a lot 
81,81,Free-Bed7814,vjyyh1,[D] Blake Lemoine on Bloomberg,"[https://www.youtube.com/watch?v=kgCUn4fQTsc](https://www.youtube.com/watch?v=kgCUn4fQTsc)

Overall, I feel like his position is rather well thought out and not as crazy as I was led to believe. And he does raise some interesting points. Why is it that Google doesn't even want to come up with a framework for defining sentience especially as machines are likely to become closer to it in the coming decade?

I feel like any sentient being, no matter if you're an animal or human should have some basic level of rights. IE imagine if you were a sentient ghost in a machine and knowing that any capricious researcher could unplug you if they like. That would be hell.",11,0,2022-06-25 03:00:12, d  blake lemoine on bloomberg, https overall  i feel like his position is rather well thought out and not as crazy as i was led to believe  and he does raise some interesting points  why is it that google doesn t even want to come up with a framework for defining sentience especially as machines are likely to become closer to it in the coming decade i feel like any sentient being  no matter if you re an animal or human should have some basic level of rights  ie imagine if you were a sentient ghost in a machine and knowing that any capricious researcher could unplug you if they like  that would be hell 
82,82,guyfrom7up,vj1zp0,[P] AutoRegistry: A Python library for mapping names to functionality to simplify project configurations.,"A common design pattern I see in a lot of ML projects is to have some sort of experiment configuration file, and then a bunch of code that constructs the appropriate objects based on these configurations. Frequently, the resulting code blocks have a bunch of `if/elif/else` statements, or a manually created lookup dictionary somewhere. This can quickly get messy and inconsistent as you add new models/losses/encoders/optimizers.  

AutoRegistry is a library that makes all of these lookups more organized and terse. For example, lets say you want to configure a backbone to either be ""resnet34"" or ""resnet50"". Your code could look something like this (mimicking torchvision code) using a decorator: 

```
from autoregistry import Registry

models = Registry()

@models
def resnet34(*, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:
    return _resnet(BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)

@models
def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:
    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)

# create a model based off of some configuration dictionary.
model_config = copy(config[""model""])
model_type = model_config.pop(""type"")
model = models[model_type](**model_config)
```

or, class-based inheritance (uses metaclasses internally):

```
class BaseModel(nn.Module, Registry):
    pass

class MyNewModel(BaseModel):
    pass

class SomeOtherModel(BaseModel):
    pass

# stringified keys are automatically derived.
my_new_model = BaseModel[""mynewmodel""](**config)
some_other_model = BaseModel[""someothermodel""](**config)
```

Github Page:  
[https://github.com/BrianPugh/autoregistry](https://github.com/BrianPugh/autoregistry)",1,10,2022-06-23 22:46:46, p  autoregistry  a python library for mapping names to functionality to simplify project configurations ,a common design pattern i see in a lot of ml projects is to have some sort of experiment configuration file  and then a bunch of code that constructs the appropriate objects based on these configurations  frequently  the resulting code blocks have a bunch of  if elif else  statements  or a manually created lookup dictionary somewhere  this can quickly get messy and inconsistent as you add new models losses encoders optimizers   autoregistry is a library that makes all of these lookups more organized and terse  for example  lets say you want to configure a backbone to either be resnet or resnet  your code could look something like this  mimicking torchvision code  using a decorator     from autoregistry import registrymodels   registry   modelsdef resnet    weights  optional resnet_weights    none  progress  bool   true    kwargs  any     resnet     return _resnet basicblock            weights  progress    kwargs  modelsdef resnet    weights  optional resnet_weights    none  progress  bool   true    kwargs  any     resnet     return _resnet bottleneck            weights  progress    kwargs   create a model based off of some configuration dictionary model_config   copy config model  model_type   model_config pop type model   models model_type    model_config    or  class based inheritance  uses metaclasses internally     class basemodel nn module  registry      passclass mynewmodel basemodel      passclass someothermodel basemodel      pass  stringified keys are automatically derived my_new_model   basemodel mynewmodel    config some_other_model   basemodel someothermodel    config    github page    https   github com brianpugh autoregistry  https   github com brianpugh autoregistry 
83,83,an1_r_00dh,vjfgb9,[Discussion] Is there a way to increase the weight of a particular feature in an outlier detection method using the isolation forest algorithm?,"I'm currently working on the outlier detection method using the isolation forest algorithm on a dataset with 9 dimensions. Out of these, there is a particular dimension that I want to increase the importance/significance of, in the classification process. Is there a way I can do this? Thanksnin advance.",1,0,2022-06-24 09:36:02, discussion  is there a way to increase the weight of a particular feature in an outlier detection method using the isolation forest algorithm ,i m currently working on the outlier detection method using the isolation forest algorithm on a dataset with  dimensions  out of these  there is a particular dimension that i want to increase the importance significance of  in the classification process  is there a way i can do this  thanksnin advance 
84,84,chromeplated,vitv4u,[N] Microsoft released a DirectML Plugin for TensorFlow 2,"The plugin provides a DirectML PluggableDevice backend for TensorFlow 2, so any GPU which supports DirectX 12 should be able to work with TF2. Hopefully this will pave the way for more support for non-NVIDIA GPUs in ML.  
They provide some more details (installation, code samples, etc') in the [Windows AI devblog](https://devblogs.microsoft.com/windowsai/directml-plugin-for-tensorflow-2-is-here/).",1,12,2022-06-23 16:05:06, n  microsoft released a directml plugin for tensorflow ,the plugin provides a directml pluggabledevice backend for tensorflow   so any gpu which supports directx  should be able to work with tf  hopefully this will pave the way for more support for non nvidia gpus in ml   they provide some more details  installation  code samples  etc   in the  windows ai devblog  https   devblogs microsoft com windowsai directml plugin for tensorflow  is here   
85,85,codeinassembly,viwb07,[D] [P] A TensorFlow Re-Implementation of CheXNet - Classification and Localization of Thoracic Diseases,"TL:DR; need help making heatmaps!   
\[[Repository](https://dagshub.com/nirbarazida/Pneumonia-Classification)|[Colab Notebook](https://colab.research.google.com/drive/1U3F5ETJeisBnlmamR4EqigS7shIbL2L1#scrollTo=Ghq8fYm5yo8o)\]

Hey everyone -

I've been working to reproduce [CheXNet](https://arxiv.org/pdf/1711.05225.pdf) \- a fantastic paper describing research on a model capable of radiologist-grade pathology classification!

CheXNet uses Class Activation Mappings (CAMs for short) to generate heatmaps that identify what parts of the image the model uses to base its classification. In my case, I'm facing a bit of a struggle reproducing them - as shown in the image below, **most of our classifications are derived from the diaphragm, instead of regions within the lung**. Curiously, we are attaining a reasonable AUROC, with .773 on training and .749 on validation data - the paper reports .8062 AUROC.

My current model is being trained on a subsample of the main dataset, and I'm basically looking to this as a way to validate the architecture. I'd love to know if anyone has experienced similar issues and solved them, and could have any input here as well.

If you have a moment to spare - I'd be super grateful for some help from the r/MachineLearning community in solving the inaccurate localization issue - [\#58](https://dagshub.com/nirbarazida/Pneumonia-Classification/issues/58)!

[Fig 1. An incorrect localization, despite a correct classification.](https://preview.redd.it/umoq6vjmbd791.png?width=451&format=png&auto=webp&s=3dc1d8a99925db47a02c3f719e1d9fd0ba984535)",1,7,2022-06-23 18:25:31, d   p  a tensorflow re implementation of chexnet   classification and localization of thoracic diseases,tl dr  need help making heatmaps       repository  https hey everyone  i ve been working to reproduce  chexnet  https chexnet uses class activation mappings  cams for short  to generate heatmaps that identify what parts of the image the model uses to base its classification  in my case  i m facing a bit of a struggle reproducing them   as shown in the image below    most of our classifications are derived from the diaphragm  instead of regions within the lung    curiously  we are attaining a reasonable auroc  with   on training and   on validation data   the paper reports   auroc my current model is being trained on a subsample of the main dataset  and i m basically looking to this as a way to validate the architecture  i d love to know if anyone has experienced similar issues and solved them  and could have any input here as well if you have a moment to spare   i d be super grateful for some help from the r machinelearning community in solving the inaccurate localization issue        https  fig   an incorrect localization  despite a correct classification   https   preview redd it umoqvjmbd png width  format png auto webp s dcdadbacfedfdba 
86,86,htrp,vid29a,[R] Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Google - Parti),"Google published results from an seq2seq transformer model for autoregressive image generation.

Website: https://parti.research.google/

Paper: https://gweb-research-parti.web.app/parti_paper.pdf",15,118,2022-06-23 00:49:10, r  scaling autoregressive models for content rich text to image generation  google   parti ,google published results from an seqseq transformer model for autoregressive image generation website  https paper  https   gweb research parti web app parti_paper pdf
87,87,Pedimus,vj6mru,"[D] ""Wrapping"" effects when using diffusion model to generate samples?","I've recently been training a latent diffusion model (it operated on the latent space of a VQ-VAE), and I'm finding that my generated samples have ""wrapping"" effects, i.e.: when I generate the face it wraps up (bottom half of the face in the top half of the image and vice versa). It's worth noting that these halves don't always seem like they belong together, but they individually look quite realistic.

I've checked my training data, and there are absolutely no training samples that exhibit this behaviour, so my model never sees images that exhibit this wrapping effect, so what could be causing this?",2,0,2022-06-24 02:12:51, d  wrapping effects when using diffusion model to generate samples ,i ve recently been training a latent diffusion model  it operated on the latent space of a vq vae   and i m finding that my generated samples have wrapping effects  i e   when i generate the face it wraps up  bottom half of the face in the top half of the image and vice versa   it s worth noting that these halves don t always seem like they belong together  but they individually look quite realistic i ve checked my training data  and there are absolutely no training samples that exhibit this behaviour  so my model never sees images that exhibit this wrapping effect  so what could be causing this 
88,88,ylu175,vig0l7,[R] Announcing DAMP 2.0: Allowing SOTA Anomaly Detection in Massive Time Series Datasets,"Dear Colleagues

We are happy to announce the release of DAMP 2.0 \[a\]. DAMP (Discord Aware Matrix Profile) is an anomaly detection framework that allows you to search datasets with millions or billions of datapoints, all on a conventional machine \[b\].

We are not normally so vainglorious as to announce the publication of Â a paper, however:

1)  The code comes bundled with some great new anomaly detection datasets, and there is a real dearth of good datasets in the community (see \[c\])

2)  Some researchers are working on problems that use anomaly detection as a subroutine, and that is their main computational bottleneck. Because DAMP can be up to 10,000 times faster than other approaches, this may be of interest to the community

Best wishes, Yue

\[a\] Matrix Profile XXIV:Scaling Time Series Anomaly Detection to Trillions of Datapoints and Ultra-fast Arriving Data Streams. Yue Lu , Renjie Wu , Abdullah Mueen , Maria A. Zuluaga and Eamonn Keogh. ACM SIGKDD 2022.Â  [https://www.cs.ucr.edu/\~eamonn/DAMP\_long\_version.pdf](https://www.cs.ucr.edu/~eamonn/DAMP_long_version.pdf)

\[b\] [https://sites.google.com/view/discord-aware-matrix-profile](https://sites.google.com/view/discord-aware-matrix-profile)

\[c\] Irrational Exuberance Why we should not believe 95% of papers on Time Series Anomaly Detection.Â Â Â Â Â  [https://www.youtube.com/watch?v=Vg1p3DouX8w](https://www.youtube.com/watch?v=Vg1p3DouX8w)

\[d\] [https://drive.google.com/file/d/1hEgOKtoTuHGPMqR1wty8ff\_jes93ra9a/view](https://drive.google.com/file/d/1hEgOKtoTuHGPMqR1wty8ff_jes93ra9a/view)",7,37,2022-06-23 02:59:24, r  announcing damp    allowing sota anomaly detection in massive time series datasets,dear colleagueswe are happy to announce the release of damp     a    damp  discord aware matrix profile  is an anomaly detection framework that allows you to search datasets with millions or billions of datapoints  all on a conventional machine   b   we are not normally so vainglorious as to announce the publication of Â a paper  however    the code comes bundled with some great new anomaly detection datasets  and there is a real dearth of good datasets in the community  see   c      some researchers are working on problems that use anomaly detection as a subroutine  and that is their main computational bottleneck  because damp can be up to   times faster than other approaches  this may be of interest to the communitybest wishes  yue  a   matrix profile xxiv scaling time series anomaly detection to trillions of datapoints and ultra fast arriving data streams  yue lu   renjie wu   abdullah mueen   maria a  zuluaga and eamonn keogh  acm sigkdd  Â   https   b    https   c   irrational exuberance why we should not believe   of papers on time series anomaly detection Â Â Â Â Â   https   d    https   drive google com file d hegoktotuhgpmqrwtyff _jesraa view  https   drive google com file d hegoktotuhgpmqrwtyff_jesraa view 
89,89,curious_cow_99,vi2mw4,[D] Have you ever been asked to work on a software project you found unethical? Weâ€™d like to hear from you!,"We are researchers at Carnegie Mellon University studying how software developers identify and act on ethical concerns at work. If youâ€™re interested in helping us advance research in software ethics, please fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLScEIB09oKznU4OGDQeQyNpfMgf_X3HdNS1j2m-c_BFDJijuTQ/viewform?usp=sf_link) and weâ€™ll reach out to you for a quick interview!

P.S.

* You can check out [this](https://stackoverflow.blog/2022/05/30/ethical-ai-isnt-just-how-you-build-it-its-how-you-use-it/) Stack Overflow blog post to read more about the direction of our research.
* Anything you disclose to us during the survey / interview may appear in our study but will not be traceable to you.",67,253,2022-06-22 16:35:44, d  have you ever been asked to work on a software project you found unethical  we d like to hear from you ,we are researchers at carnegie mellon university studying how software developers identify and act on ethical concerns at work  if you re interested in helping us advance research in software ethics  please fill out  this survey  https p s   you can check out  this  https   anything you disclose to us during the survey   interview may appear in our study but will not be traceable to you 
90,90,keremidk0,vie0aj,[D] Is audio style transfer a thing ?,"So we have image style transfer, there's a lot of good papers and implementations.

  
Is there such thing as audio style transfer, where 1 song keeps its lyrics and melody, but get the other song's style ? e.g. pop music with rock style ?  
If yes - can you please share a link ?",10,31,2022-06-23 01:31:11, d  is audio style transfer a thing  ,so we have image style transfer  there s a lot of good papers and implementations   is there such thing as audio style transfer  where  song keeps its lyrics and melody  but get the other song s style   e g  pop music with rock style    if yes   can you please share a link  
91,91,lux123or,viy1zp,State of the art 2D body pose estimation [Discussion]," Hi. I have a background in neuroscience and sometimes we use DeepLabCut to track animals during behaviour. This is by far the most widespread and used application for animal tracking based on artificial neural networks. I was wondering, if anyone here is an expert in human 2D body pose estimation and can tell me what their oppinion is on what is the best human 2D pose estimation tool currently available? I came across Pose from mediapipe and it seems very good from a few examples I tested so far but I'm curious if there's something even better that I have not come across. Thanks for the help!",1,1,2022-06-23 19:50:58,state of the art d body pose estimation  discussion , hi  i have a background in neuroscience and sometimes we use deeplabcut to track animals during behaviour  this is by far the most widespread and used application for animal tracking based on artificial neural networks  i was wondering  if anyone here is an expert in human d body pose estimation and can tell me what their oppinion is on what is the best human d pose estimation tool currently available  i came across pose from mediapipe and it seems very good from a few examples i tested so far but i m curious if there s something even better that i have not come across  thanks for the help 
92,92,LeanderKu,vi8its,[D] Any way to speed up simple mathematical functions without implementing cuda kernels for pytorch?,"I am working on a pytorch project and I have a custom computation that I am so far unable to express as a combination pre-defined pytorch functions (because it's essentially some loops around conv2d calls where I juggle some indices in a 5-d tensor). So currently I use python-loops with some smart padding but that's not the fastest. The only way to speed this up would be, i think, to implement custom cuda kernels. While the computation is not that trivial it is simple in a mathematical way. It can be defined in a single line using lots of indices and sums. I wonder whether there is really nothing I can do?

What I am thinking of is something like tensor-comprehensions, but that's deprecated and I didn't get it to install.

Is there any modern alternative to tensor-comprehension, or should I switch the language to e.g. julia? Is it possible there to define slightly different conv2d there and have it run natively on the GPU?

I don't expect performance comparable to the handwritten conv2d kernels, but the python loops are just quite slow.",27,20,2022-06-22 21:28:18, d  any way to speed up simple mathematical functions without implementing cuda kernels for pytorch ,i am working on a pytorch project and i have a custom computation that i am so far unable to express as a combination pre defined pytorch functions  because it s essentially some loops around convd calls where i juggle some indices in a  d tensor   so currently i use python loops with some smart padding but that s not the fastest  the only way to speed this up would be  i think  to implement custom cuda kernels  while the computation is not that trivial it is simple in a mathematical way  it can be defined in a single line using lots of indices and sums  i wonder whether there is really nothing i can do what i am thinking of is something like tensor comprehensions  but that s deprecated and i didn t get it to install is there any modern alternative to tensor comprehension  or should i switch the language to e g  julia  is it possible there to define slightly different convd there and have it run natively on the gpu i don t expect performance comparable to the handwritten convd kernels  but the python loops are just quite slow 
93,93,Appropriate_Ant_4629,vinz5n,[D] Do any Text-to-Image approaches work well with long complex prompts (i.e. paragraph or book chapter scale)?,"Seems almost all the examples of text-to-image are based on tiny prompts with very few details (""avocado chair"").

Do any such systems do a good job at keeping track of details - like [the first 2 paragraphs of The Hobbit](https://www.printfriendly.com/p/g/jP9qGu) and correctly place the ""polished chairs"", ""pegs for hats and coats"", and ""deep-set round windows looking over his garden, and meadows beyond, sloping down to the river""?

Assuming they don't - what approach(es) might make sense to design such systems?

I'm speculating that you'd need much larger embedding vectors (to correctly connect concepts from the right adjectives to the right nouns); and it'd be harder to find training data (perhaps frames of movies from novels would be a good source)?

Any pointers to anything in that direction?",3,4,2022-06-23 09:37:22, d  do any text to image approaches work well with long complex prompts  i e  paragraph or book chapter scale  ,seems almost all the examples of text to image are based on tiny prompts with very few details  avocado chair  do any such systems do a good job at keeping track of details   like  the first  paragraphs of the hobbit  https assuming they don t   what approach es  might make sense to design such systems i m speculating that you d need much larger embedding vectors  to correctly connect concepts from the right adjectives to the right nouns   and it d be harder to find training data  perhaps frames of movies from novels would be a good source  any pointers to anything in that direction 
94,94,FlyingQuokka,vigx2l,[D] What is the current SOTA for open-source AutoML?,"I've never really used AutoML--I prefer to code up my models and data engineering by hand, but I'm beginning to wonder if I can use AutoML as a starting point, e.g., the built-in hyper-parameter optimization or NAS finds a good neural network hyper-params/architecture for me, and I can build on that.

With that in mind, what's the SOTA right now? Ideally, it would be as white-box as possible, telling me the models it tries, what worked and didn't, etc. Alternatively, what has worked best for you in your workflows?",3,7,2022-06-23 03:39:09, d  what is the current sota for open source automl ,i ve never really used automl  i prefer to code up my models and data engineering by hand  but i m beginning to wonder if i can use automl as a starting point  e g   the built in hyper parameter optimization or nas finds a good neural network hyper params architecture for me  and i can build on that with that in mind  what s the sota right now  ideally  it would be as white box as possible  telling me the models it tries  what worked and didn t  etc  alternatively  what has worked best for you in your workflows 
95,95,JBitterwolf,vi1ly4,[R] Breaking Down Out-of-Distribution Detection,"TL;DR: Many OOD detectors that are trained with samples from an (unrelated) OOD dataset can be understood by isolating a binary discriminator between in-distribution and OOD.

[We just published it on arXiv](https://arxiv.org/abs/2206.09880) and will present it at ICML 2022.

Questions and discussion are very welcome!

Full title: **Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities** by Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, Matthias Hein.",2,36,2022-06-22 15:29:50, r  breaking down out of distribution detection,tl dr  many ood detectors that are trained with samples from an  unrelated  ood dataset can be understood by isolating a binary discriminator between in distribution and ood  we just published it on arxiv  https questions and discussion are very welcome full title    breaking down out of distribution detection  many methods based on ood training data estimate a combination of the same core quantities   by julian bitterwolf  alexander meinke  maximilian augustin  matthias hein 
96,96,LemonByte,viglgx,[P] Multidimensional array batch indexing for pytorch and numpy,"Batch indexing into multidimensional tensors/arrays is kind of tricky, I made this project explaining the builtin syntax and also made wrappers for simplifying the interface, with additional features for underlying coordinate grid data (like signed distance functions) that need to be indexed by coordinate value rather than integer indices directly [https://github.com/LemonPi/multidim\_indexing](https://github.com/LemonPi/multidim_indexing)",4,6,2022-06-23 03:25:20, p  multidimensional array batch indexing for pytorch and numpy,batch indexing into multidimensional tensors arrays is kind of tricky  i made this project explaining the builtin syntax and also made wrappers for simplifying the interface  with additional features for underlying coordinate grid data  like signed distance functions  that need to be indexed by coordinate value rather than integer indices directly  https   github com lemonpi multidim _indexing  https   github com lemonpi multidim_indexing 
97,97,seraschka,vi41f7,[P] Bottom-up look at the new Lightning Framework for building anything from production-ready ML systems to research demos,"The open-source [lightning.ai](https://lightning.ai) framework just launched last week introducing the concept of Lightning Apps. It's basically meant for building anything from production ready ML-system running on multi-node GPU clusters in the cloud to building simple research demos.

Starting with a simple use case, a research demo, I wrote a ""short"" article about it to explain how it roughly works under the hood: [Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App ](https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html)

Looking forward to hearing your feedback. I am planning to put together more ""substantial"" examples, but I was thinking of doing that one step at the time. Will be attending a conference in 3 weeks and am planning to create a research demo alongside the paper I will be presenting, and I was wondering besides Gradio/Dash/Gradio, what are your typical tools and workflows for making research demos. Any cool examples for inspiration?

&#x200B;

Disclaimer: I recently joined Lightning when I saw an early prototype. As someone who has spent most of my time on research models, I was always intrigued by putting ML models to production. However, I was also always turned of by the tooling that it involved.",9,16,2022-06-22 17:55:18, p  bottom up look at the new lightning framework for building anything from production ready ml systems to research demos,the open source  lightning ai  https starting with a simple use case  a research demo  i wrote a short article about it to explain how it roughly works under the hood   sharing deep learning research models with lightning part   building a super resolution app   https looking forward to hearing your feedback  i am planning to put together more substantial examples  but i was thinking of doing that one step at the time  will be attending a conference in  weeks and am planning to create a research demo alongside the paper i will be presenting  and i was wondering besides gradio dash gradio  what are your typical tools and workflows for making research demos  any cool examples for inspiration   xb disclaimer  i recently joined lightning when i saw an early prototype  as someone who has spent most of my time on research models  i was always intrigued by putting ml models to production  however  i was also always turned of by the tooling that it involved 
98,98,wowAmaze,vijaqf,[D] Implementing custom functions in pytorch e.g. feature propagation (PointNet++),"Apologies if this isn't the right place to ask. But I'm currently studying point cloud-based networks like pointcloud++, and all the related 3d object detection networks like pointpillars, voxelnet, etc. While I (think) understand the algorithms like feature propagation in [pointnet++](https://github.com/erikwijmans/Pointnet2_PyTorch/blob/master/pointnet2_ops_lib/pointnet2_ops/pointnet2_utils.py). I'm having trouble understanding how would one implement them. Or Where could I learn about writing operations in cuda and making sure they are compatible with backprop?",2,1,2022-06-23 05:31:59, d  implementing custom functions in pytorch e g  feature propagation  pointnet   ,apologies if this isn t the right place to ask  but i m currently studying point cloud based networks like pointcloud    and all the related d object detection networks like pointpillars  voxelnet  etc  while i  think  understand the algorithms like feature propagation in  pointnet    https   github com erikwijmans pointnet_pytorch blob master pointnet_ops_lib pointnet_ops pointnet_utils py   i m having trouble understanding how would one implement them  or where could i learn about writing operations in cuda and making sure they are compatible with backprop 
99,99,hardmaru,vhtrzj,[R] EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine,,1,53,2022-06-22 07:35:48, r  envpool  a highly parallel reinforcement learning environment execution engine,nan
100,100,Relative_Collection1,vig9ae,[P] Building a Source of Truth for Inventory with Disparate Data Sources,"One of the most challenging shifts from food delivery to grocery is managing inventory. Although restaurant menu items can sometimes go out of stock, grocery store inventories have far more SKUs and many different ways to track their inventory levels. This complexity of grocery makes it a lot harder to ensure items customers buy are actually available. Knowing what the ground truth is, so that customers can order groceries with confidence, is the subject of a new engineering blog post I wrote, [""Building a Source of Truth for an Inventory with Disparate Data Sources""](https://doordash.engineering/2022/06/21/building-a-source-of-truth-for-a-digital-inventory-with-disparate-data-sources/). The article explains how we crowd sourced our inventory data from a number of different sources which enabled us to predict which items are likely still on the shelves when customers place an order. Take a look and let me know what you think",0,0,2022-06-23 03:10:06, p  building a source of truth for inventory with disparate data sources,one of the most challenging shifts from food delivery to grocery is managing inventory  although restaurant menu items can sometimes go out of stock  grocery store inventories have far more skus and many different ways to track their inventory levels  this complexity of grocery makes it a lot harder to ensure items customers buy are actually available  knowing what the ground truth is  so that customers can order groceries with confidence  is the subject of a new engineering blog post i wrote   building a source of truth for an inventory with disparate data sources  https   doordash engineering    building a source of truth for a digital inventory with disparate data sources    the article explains how we crowd sourced our inventory data from a number of different sources which enabled us to predict which items are likely still on the shelves when customers place an order  take a look and let me know what you think
101,101,DigThatData,vhfp1t,"[N] [D] Openai, who runs DALLE-2 alleged threatened creator of DALLE-Mini","Trying to cross-post what I think is a discussion that is relevant to this community. This is my third attempt, I hope I'm doing it correctly this time: 

https://www.reddit.com/r/dalle2/comments/vgtgdc/openai_who_runs_dalle2_alleged_threatened_creator/

EDIT: here are the original pre-prints for added context:

* DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) - The only place the term ""DALL-E"" appears is the URL to the github repo.
* Dall-E 2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) - They consistently refer to the first paper as ""DALL-E"", but refer to the work being described in the new paper as ""unCLIP"" and are careful to only use 'DALL-E 2' in the context of a product description, e.g. ""DALLÂ·E 2 Preview platform (the first deployment of an unCLIP model)""",113,256,2022-06-21 20:47:08, n   d  openai  who runs dalle  alleged threatened creator of dalle mini,trying to cross post what i think is a discussion that is relevant to this community  this is my third attempt  i hope i m doing it correctly this time  https edit  here are the original pre prints for added context   dall e   zero shot text to image generation  https   dall e    hierarchical text conditional image generation with clip latents  https   arxiv org abs      they consistently refer to the first paper as dall e  but refer to the work being described in the new paper as unclip and are careful to only use  dall e   in the context of a product description  e g  dall e  preview platform  the first deployment of an unclip model 
102,102,TheFibo1123,vhuozn,[Discussion] Iteration of Machine Learning Systems,"Engineering systems progress by addressing used cases of increasing levels of **complexity**.

For example, you start with a 'minimum viable product' and then slowly add features or complexity as things progress.

However, this is not how machine learning systems progress. You don't start with 10 positive/negative samples, and then iteratively add more. It's not even wise to start with one (or a few) 'tasks' and then add new ones as things progress.

Clearly, iteration (or progress) in machine learning systems does not follow the same pattern as traditional engineering systems. Is there another way to think about iteration?",12,28,2022-06-22 08:22:47, discussion  iteration of machine learning systems,engineering systems progress by addressing used cases of increasing levels of   complexity   for example  you start with a  minimum viable product  and then slowly add features or complexity as things progress however  this is not how machine learning systems progress  you don t start with  positive negative samples  and then iteratively add more  it s not even wise to start with one  or a few   tasks  and then add new ones as things progress clearly  iteration  or progress  in machine learning systems does not follow the same pattern as traditional engineering systems  is there another way to think about iteration 
103,103,bahauddin_onar,vhwgv5,[R][P] Best Approach to do Image Inpainting in Video Files (Image Timeseries),"First time posting here. I am working with image timeseries of satellite images. These are essentially 1 hour long video files with the image size of 384 X 384 pix. The images have chunks of data missing, say 20 X 20 pix at different parts of the image. I would say that the missing part of the image is roughly 20%-25%. Now I have the ground truths to train a neural network. But what I am struggling is what primary architecture should I begin with: CNN, LSTM, CNN-LSTM, U-Net? I found this literature: [https://arxiv.org/abs/2112.09262](https://arxiv.org/abs/2112.09262) \- which exploits a U-net autoencoder architecture to solve the image inpainting problem, but I am not sure how robust this is for 3D (x,y,t) image cubes.

Is there anyone experienced here who has worked on image inpainting on video files? Can you please share your experience? If you can point me towards a reliable literature that would be a big help!",8,14,2022-06-22 09:59:24, r  p  best approach to do image inpainting in video files  image timeseries ,first time posting here  i am working with image timeseries of satellite images  these are essentially  hour long video files with the image size of  x  pix  the images have chunks of data missing  say  x  pix at different parts of the image  i would say that the missing part of the image is roughly      now i have the ground truths to train a neural network  but what i am struggling is what primary architecture should i begin with  cnn  lstm  cnn lstm  u net  i found this literature   https is there anyone experienced here who has worked on image inpainting on video files  can you please share your experience  if you can point me towards a reliable literature that would be a big help 
104,104,berimbolo21,vi97e2,[R] Black box adversarial attacks that do not require output labels,"For those who specialize in adversarial machine learning, are there any black box attacks that do not require the model's output labels when generating adversarial images? I can't seem to find any",1,0,2022-06-22 21:58:04, r  black box adversarial attacks that do not require output labels,for those who specialize in adversarial machine learning  are there any black box attacks that do not require the model s output labels when generating adversarial images  i can t seem to find any
105,105,bandalorian,vi6cnr,[D] How to compare model performance when you add data withe label noise?,"Let's say I'm trying to categorize vendors based on their description using some NLP technique. I have a limited dataset of vendors with high quality (low noise) labels. I split in to train/test, and score say 90% accuracy. I then get hold of a dataset  for 3d party vendors, which will have much noisier (but still useful) data. Now when I train the model I get an 89% accuracy. 

How do I interpret this? The noisier data will also go in the test split, and the model is expected to perform worse on those, so even if it's exactly as good as the prior model on the old data, it should have an average worse performance on the new dataset. It could even be better, say scoring 91% on the old data, but 85% on the new data, so the average accuracy looks lower even though you have a better model.

Testing the old model on the new test set I guess would settle this? Just curious if there are any best practices.",4,0,2022-06-22 19:48:51, d  how to compare model performance when you add data withe label noise ,let s say i m trying to categorize vendors based on their description using some nlp technique  i have a limited dataset of vendors with high quality  low noise  labels  i split in to train test  and score say   accuracy  i then get hold of a dataset  for d party vendors  which will have much noisier  but still useful  data  now when i train the model i get an   accuracy  how do i interpret this  the noisier data will also go in the test split  and the model is expected to perform worse on those  so even if it s exactly as good as the prior model on the old data  it should have an average worse performance on the new dataset  it could even be better  say scoring   on the old data  but   on the new data  so the average accuracy looks lower even though you have a better model testing the old model on the new test set i guess would settle this  just curious if there are any best practices 
106,106,singularpanda,vhyxbo,[D][R] Is there any benchmark task set for computer vision?,"I know that in NLP, there are some benchmark task sets like GLUE, SuperGLUE, etc. I wonder wherer there is any similar benchmark task set for computer vision that we can easily test many tasks in a unified way?",4,5,2022-06-22 12:29:13, d  r  is there any benchmark task set for computer vision ,i know that in nlp  there are some benchmark task sets like glue  superglue  etc  i wonder wherer there is any similar benchmark task set for computer vision that we can easily test many tasks in a unified way 
107,107,MLRecipes,vh4xgc,[D] Machine learning books for free offered with full source document (LaTeX),"Top quality machine learning papers and books, not only for free, but offered with full LaTeX source, bib file, and raw figures.  So that anyone can easy incorporate part of these books (formulas, tables, pictures, text. references etc.) into their PhD thesis, articles, or reports. The user could even fix any typo he finds then print an enhanced version of the book, for private (or public) use.

That sounds like a dream? I am actually thinking offering this, with my numerous papers / books. My question is this: is it a good idea? Should I charge a fee (in other words: would you pay for it?) I understand some will use the material for plagiarism, but I am not too concerned about it, or should I? My first candidate book for this is the following: [https://mltechniques.com/2022/03/22/book-stochastic-processes-and-simulations/](https://mltechniques.com/2022/03/22/book-stochastic-processes-and-simulations/). I just finished converting all the Perl code into Python, and will soon publish the 2nd edition, this time in Python \[if it comes with LaTeX code, it means that the user can easily extract the Python code from the book, though it is also on GitHub\].",12,236,2022-06-21 10:10:46, d  machine learning books for free offered with full source document  latex ,top quality machine learning papers and books  not only for free  but offered with full latex source  bib file  and raw figures   so that anyone can easy incorporate part of these books  formulas  tables  pictures  text  references etc   into their phd thesis  articles  or reports  the user could even fix any typo he finds then print an enhanced version of the book  for private  or public  use that sounds like a dream  i am actually thinking offering this  with my numerous papers   books  my question is this  is it a good idea  should i charge a fee  in other words  would you pay for it   i understand some will use the material for plagiarism  but i am not too concerned about it  or should i  my first candidate book for this is the following   https   mltechniques com    book stochastic processes and simulations   https   mltechniques com    book stochastic processes and simulations    i just finished converting all the perl code into python  and will soon publish the nd edition  this time in python   if it comes with latex code  it means that the user can easily extract the python code from the book  though it is also on github   
108,108,Upstairs-Jicama-8347,vhorh4,"[D] Techniques for dealing with classic statistical data gathering problems: selection bias, differential attrition, experimenter bias, ect.. in Machine Learning?","Can anyone suggest papers or techniques in ML to deal with some of the statistical bias problems outlined in the title? (selection bias, differential attrition, experimenter bias, ect..)",0,8,2022-06-22 03:28:19, d  techniques for dealing with classic statistical data gathering problems  selection bias  differential attrition  experimenter bias  ect   in machine learning ,can anyone suggest papers or techniques in ml to deal with some of the statistical bias problems outlined in the title   selection bias  differential attrition  experimenter bias  ect   
109,109,MLJungle,vhzk9e,"[D] Iterative ""imputation"" HMM sequence generation idea","I have an idea in my head that I am looking to get some feedback/formal understanding of.

Say we consider a n-gram model (ex: bi/trigram). For sequence generation (such as a sequence of words),  one way would be to start with some input word and then use the n-gram model to simply unroll and predict the rest of the words, generating some sentence. If O is original and N is new, we would have O-N-N-N-N.. and so on.

Alternatively, I was thinking of a way to generate a sequence that might be more similar to the original text but still be stochastically generated. One use case for this would be generating sequential synthetic data, where the synthetic data should be as similar to the original data but should be generated and have a stochastic nature to it.

Here, let us take some sequence and proceed to do imputation- we nullify every other word, starting from the first input word. Then, we use a trained modified 'sandwich' HMM bi-gram model which predicts based on the before and after word. Then, we use the sandwich HMM to fill in the nullified every other word. Now, our sequence would be O-N-O-N-O. To get a more fully generated sequence, we could use another model, a trained bi-gram HMM model to impute the original words- in particular, for the third word it would use O-N to generate a guess, for the fifth it would use the next O-N, and so on, giving us ONNNN..., where each guess utilizes both O and N.

My idea is by doing this ""imputation"" step wise to generate our words, rather than unrolling it out all at once, is that we have remnants of the original text in conjunction with new predicted words to guide the generation of each word as opposed to possibly just new predicted words with the unrolled method.  This may lead to more similar generated sentences.

(A more extreme method might consider just using the original data to make every prediction- for example, we could consider a trained bi-gram model that uses the first word O to predict the second, the second O to predict the third, and so on. I don't think this will lead to good generated sequences we would have O-N-N but the third word does not depend at all directly on info from the second.)

EDIT: Looking to use model (such as HMM) that works well on very small datasets.",2,0,2022-06-22 13:11:22, d  iterative imputation hmm sequence generation idea,i have an idea in my head that i am looking to get some feedback formal understanding of say we consider a n gram model  ex  bi trigram   for sequence generation  such as a sequence of words    one way would be to start with some input word and then use the n gram model to simply unroll and predict the rest of the words  generating some sentence  if o is original and n is new  we would have o n n n n   and so on alternatively  i was thinking of a way to generate a sequence that might be more similar to the original text but still be stochastically generated  one use case for this would be generating sequential synthetic data  where the synthetic data should be as similar to the original data but should be generated and have a stochastic nature to it here  let us take some sequence and proceed to do imputation  we nullify every other word  starting from the first input word  then  we use a trained modified  sandwich  hmm bi gram model which predicts based on the before and after word  then  we use the sandwich hmm to fill in the nullified every other word  now  our sequence would be o n o n o  to get a more fully generated sequence  we could use another model  a trained bi gram hmm model to impute the original words  in particular  for the third word it would use o n to generate a guess  for the fifth it would use the next o n  and so on  giving us onnnn     where each guess utilizes both o and n my idea is by doing this imputation step wise to generate our words  rather than unrolling it out all at once  is that we have remnants of the original text in conjunction with new predicted words to guide the generation of each word as opposed to possibly just new predicted words with the unrolled method   this may lead to more similar generated sentences  a more extreme method might consider just using the original data to make every prediction  for example  we could consider a trained bi gram model that uses the first word o to predict the second  the second o to predict the third  and so on  i don t think this will lead to good generated sequences we would have o n n but the third word does not depend at all directly on info from the second  edit  looking to use model  such as hmm  that works well on very small datasets 
110,110,jovezhong,vhwy3k,[D] What do you think of the idea of Real-time Machine Learning with Streaming SQL,"I am working on a startup to simplify the process of real-time application development. Streaming SQL is the key user interface. We recently worked out a solution to apply ML/prediction in the Streaming SQL, so that you can continuously train and predict data when new data comes. No code required, just SQL.  If you need more complex logic, you can wrap your code as user-defined-function and still put in the SQL. I am not going to put a link but you can search timeplus and find our website.

Open for discussions, do you think such tools will help some customers to implement easy use cases without much dev resource/effort.

Here is an example:
```sql
WITH LinearModel AS
 (
 SELECT
      gas_percent, to_int(time) AS t, 
      [lag(t), lag(t, 2), lag(t, 3), lag(t, 4), lag(t, 5)] AS X, [lag(gas_percent), lag(gas_percent, 2), lag(gas_percent, 3), lag(gas_percent, 4), lag(gas_percent, 5)] AS Y, 
      array_avg(X) AS avg_X, array_avg(Y) AS avg_Y, 
      array_sum(array_map((x, y) -> ((x - avg_X) * (y - avg_Y)), X, Y)) 
     / array_sum(array_map(x -> ((x - avg_X) * (x - avg_X)), X)) AS m, 
     avg_Y - (m * avg_X) AS b
 FROM car_live_data
 WHERE     cid = 'c00031'
 )
SELECT gas_percent, to_datetime(t), (t * m) + b AS predict
FROM LinearModel
```",0,1,2022-06-22 10:27:07, d  what do you think of the idea of real time machine learning with streaming sql,i am working on a startup to simplify the process of real time application development  streaming sql is the key user interface  we recently worked out a solution to apply ml prediction in the streaming sql  so that you can continuously train and predict data when new data comes  no code required  just sql   if you need more complex logic  you can wrap your code as user defined function and still put in the sql  i am not going to put a link but you can search timeplus and find our website open for discussions  do you think such tools will help some customers to implement easy use cases without much dev resource effort here is an example    sqlwith linearmodel as   select      gas_percent  to_int time  as t         lag t   lag t     lag t     lag t     lag t     as x   lag gas_percent   lag gas_percent     lag gas_percent     lag gas_percent     lag gas_percent     as y        array_avg x  as avg_x  array_avg y  as avg_y        array_sum array_map  x  y       x   avg_x     y   avg_y    x  y          array_sum array_map x      x   avg_x     x   avg_x    x   as m       avg_y    m   avg_x  as b from car_live_data where     cid    c   select gas_percent  to_datetime t    t   m    b as predictfrom linearmodel   
111,111,luisgasco,vhhimb,[R] - Call For Participants SocialDisNER (SMM4H@COLING 2022) on Detection of Disease Mentions in Social Media," **CFP- SocialDisNER track: Detection of Disease Mentions in Social Media** 

**(SMM4H Shared Task  at COLING2022)** 

[https://temu.bsc.es/socialdisner/](https://temu.bsc.es/socialdisner/) 

Despite the high impact & practical relevance of detecting diseases automatically from social media for a diversity of applications, few manually annotated corpora generated by healthcare practitioners to train/evaluate advanced entity recognition tools are currently available.

Developing disease recognition tools for social media is critical for:

* Real-time disease outbreak surveillance/monitoring
* Characterization of patient-reported symptoms
* Post-market drug safety
* Epidemiology and population health, 
* Public opinion mining & sentiment analysis of diseases 
* Detection of hate speech/exclusion of sick people
* Prevalence of work-associated diseases

SocialDisNER is the first track focusing on the detection of disease mentions in tweets written in Spanish, with clear adaptation potential not only to English but also other romance languages like Portuguese, French or Italian spoken by over 900 million people worldwide.

For this track the SocialDisNER corpus was generated, a manual collection of tweets enriched for first-hand experiences by patients and their relatives as well as content generated by patient-associations (national, regional, local) as well as healthcare institutions covering all main diseases types including cancer, mental health, chronic and rare diseases among others. 

**Info:**

* Web: [https://temu.bsc.es/socialdisner/](https://temu.bsc.es/socialdisner/) 
* Data:[ ](https://doi.org/10.5281/zenodo.6408476)[https://doi.org/10.5281/zenodo.6359365](https://doi.org/10.5281/zenodo.6359365) 
* Registration: [https://temu.bsc.es/socialdisner/registration](https://temu.bsc.es/socialdisner/registration) 

**Schedule**

* Development Set Release: June 14th
* Test Set Release: July 11th
* Participant prediction Due: July 15th
* Test set evaluation release: July 25th
* Proceedings paper submission: August 1st
* Camera ready papers: September 1st
* SMM4H workshop @ COLING 2022: October 12-17

**Publications and SMM4H (COLING 2022) workshop**

Participating teams have the opportunity to submit a short system description paper for the SMM4H proceedings (7th SMM4H Workshop, **co-located at COLING 2022).** More details are available at [https://healthlanguageprocessing.org/smm4h-2022/](https://healthlanguageprocessing.org/smm4h-2022/)

**SocialDisNER Organizers**

* **Luis GascÃ³**, Barcelona Supercomputing Center, Spain
* **Darryl Estrada**, Barcelona Supercomputing Center, Spain
* **EulÃ lia FarrÃ©-Maduell**, Barcelona Supercomputing Center, Spain
* **Salvador Lima**, Barcelona Supercomputing Center, Spain
* **Martin Krallinger**, Barcelona Supercomputing Center, Spain

**Scientific Committee & SMM4H Organizers**

* **Graciela Gonzalez-Hernandez,** Cedars-Sinai Medical Center, USA
* **Davy Weissenbacher,** University of Pennsylvania, USA 
* **Arjun Magge,** University of Pennsylvania, USA
* **Ari Z. Klein,** University of Pennsylvania, USA
* **Ivan Flores,** University of Pennsylvania, USA
* **Karen Oâ€™Connor,** University of Pennsylvania, USA
* **Raul Rodriguez-Esteban,** Roche Pharmaceuticals, Switzerland
* **Lucia Schmidt,** Roche Pharmaceuticals, Switzerland
* **Juan M. Banda,** Georgia State University, USA
* **Abeed Sarker,** Emory University, USA
* **Yuting Guo,** Emory University, USA 
* **Yao Ge,** Emory University, USA 
* **Elena Tutubalina,** Insilico Medicine, Hong Kong
* **Jey Han Hau,** The University of Melbourne (Australia)
* **Luca Maria Aiello,** IT University of Copenhagen
* **Rafael Valencia-Garcia,** Universidad de Murcia (Spain)
* **Antonio Jimeno Yepes,** RMIT University (Australia)
* **Carlos GÃ³mez-RodrÃ­guez,** Universidad da CoruÃ±a (Spain)
* **Eugenio Martinez CÃ¡mara,** Universidad de Granada (Spain)
* **Gema Bello Orgaz,**  Applied Intelligence and Data Analysis Research Group, Universidad PolitÃ©cnica de Madrid (Spain)
* **Juan Antonio Lossio-Ventura,** National Institutes of Health (USA)
* **HÃ©ctor D. Menendez,** Kingâ€™s College London (UK)
* **Manuel Montes y GÃ³mez,** National Institute of Astrophysics, Optics and Electronics (Mexico)
* **Helena GÃ³mez Adorno,** Universidad Nacional AutÃ³noma de MÃ©xico (Mexico)
* **Rodrigo Agerri, IXA Group (HiTZ Centre),** University of Basque Country EHU (Spain)
* **Miguel A. Alonso,** Universidad da CoruÃ±a (Spain)
* **Ferran Pla,** Universidad PolitÃ©cnica de Valencia (Spain)
* **Jose Alberto Benitez-Andrades,** Universidad de Leon (Spain)",0,6,2022-06-21 22:08:36, r    call for participants socialdisner  smmh coling   on detection of disease mentions in social media,   cfp  socialdisner track  detection of disease mentions in social media      smmh shared task  at coling     https despite the high impact   practical relevance of detecting diseases automatically from social media for a diversity of applications  few manually annotated corpora generated by healthcare practitioners to train evaluate advanced entity recognition tools are currently available developing disease recognition tools for social media is critical for   real time disease outbreak surveillance monitoring  characterization of patient reported symptoms  post market drug safety  epidemiology and population health    public opinion mining   sentiment analysis of diseases   detection of hate speech exclusion of sick people  prevalence of work associated diseasessocialdisner is the first track focusing on the detection of disease mentions in tweets written in spanish  with clear adaptation potential not only to english but also other romance languages like portuguese  french or italian spoken by over  million people worldwide for this track the socialdisner corpus was generated  a manual collection of tweets enriched for first hand experiences by patients and their relatives as well as content generated by patient associations  national  regional  local  as well as healthcare institutions covering all main diseases types including cancer  mental health  chronic and rare diseases among others    info     web   https   data     https   registration   https   schedule    development set release  june th  test set release  july th  participant prediction due  july th  test set evaluation release  july th  proceedings paper submission  august st  camera ready papers  september st  smmh workshop   coling   october    publications and smmh  coling   workshop  participating teams have the opportunity to submit a short system description paper for the smmh proceedings  th smmh workshop    co located at coling      more details are available at  https   socialdisner organizers      luis gascÃ³    barcelona supercomputing center  spain    darryl estrada    barcelona supercomputing center  spain    eulÃ lia farrÃ© maduell    barcelona supercomputing center  spain    salvador lima    barcelona supercomputing center  spain    martin krallinger    barcelona supercomputing center  spain  scientific committee   smmh organizers      graciela gonzalez hernandez    cedars sinai medical center  usa    davy weissenbacher    university of pennsylvania  usa     arjun magge    university of pennsylvania  usa    ari z  klein    university of pennsylvania  usa    ivan flores    university of pennsylvania  usa    karen o connor    university of pennsylvania  usa    raul rodriguez esteban    roche pharmaceuticals  switzerland    lucia schmidt    roche pharmaceuticals  switzerland    juan m  banda    georgia state university  usa    abeed sarker    emory university  usa    yuting guo    emory university  usa     yao ge    emory university  usa     elena tutubalina    insilico medicine  hong kong    jey han hau    the university of melbourne  australia     luca maria aiello    it university of copenhagen    rafael valencia garcia    universidad de murcia  spain     antonio jimeno yepes    rmit university  australia     carlos gÃ³mez rodrÃ­guez    universidad da coruÃ±a  spain     eugenio martinez cÃ¡mara    universidad de granada  spain     gema bello orgaz     applied intelligence and data analysis research group  universidad politÃ©cnica de madrid  spain     juan antonio lossio ventura    national institutes of health  usa     hÃ©ctor d  menendez    king s college london  uk     manuel montes y gÃ³mez    national institute of astrophysics  optics and electronics  mexico     helena gÃ³mez adorno    universidad nacional autÃ³noma de mÃ©xico  mexico     rodrigo agerri  ixa group  hitz centre     university of basque country ehu  spain     miguel a  alonso    universidad da coruÃ±a  spain     ferran pla    universidad politÃ©cnica de valencia  spain     jose alberto benitez andrades    universidad de leon  spain 
112,112,AdPlenty6685,vh9gni,[D] How to best extract product benefits/problems from customer reviews using NLP?,"I am working on a prototype that takes in a list of customer reviews about a specific product and returns a list of (unique) benefits and problems from these reviews. These should be non-generic, e.g. for a camera, a benefit might be ""great for panoramic photos"" and not just ""good quality"". My initial idea was to go about this in two steps:

1. Use NER to identify phrases describing benefits or problems
2. Use text summarization to create the final output

When starting to create some NER labels, I realized that benefits and problems are often mixed, spread across multiple sentences, or mentioned cryptically or indirectly, making it extremely hard to come up with concise labeling instructions. Therefore, I assume, that also the model will have quite a hard time correctly extracting benefits and problems.

Does anyone have an idea of how to tackle this in a different, more promising way? Any kind of feedback is more than welcome ðŸ™",7,8,2022-06-21 15:10:51, d  how to best extract product benefits problems from customer reviews using nlp ,i am working on a prototype that takes in a list of customer reviews about a specific product and returns a list of  unique  benefits and problems from these reviews  these should be non generic  e g  for a camera  a benefit might be great for panoramic photos and not just good quality  my initial idea was to go about this in two steps   use ner to identify phrases describing benefits or problems  use text summarization to create the final outputwhen starting to create some ner labels  i realized that benefits and problems are often mixed  spread across multiple sentences  or mentioned cryptically or indirectly  making it extremely hard to come up with concise labeling instructions  therefore  i assume  that also the model will have quite a hard time correctly extracting benefits and problems does anyone have an idea of how to tackle this in a different  more promising way  any kind of feedback is more than welcome  
113,113,4bedoe,vgoc1h,"[D] In your experience, what's the thing that can boost an ML model's performance the most? Is it the hyperparameter tuning, feature engineering or ensembling? Or is it something else?","I'm interested to know which part of ML do engineers invest their time in that actually pays off a lot when it comes to getting well-performing models. Just so I know whether it is right to spend more time trying out different X (say, Feature Eng) configurations  in favour of Y (say, Ensembling) configurations.",107,206,2022-06-20 21:08:46, d  in your experience  what s the thing that can boost an ml model s performance the most  is it the hyperparameter tuning  feature engineering or ensembling  or is it something else ,i m interested to know which part of ml do engineers invest their time in that actually pays off a lot when it comes to getting well performing models  just so i know whether it is right to spend more time trying out different x  say  feature eng  configurations  in favour of y  say  ensembling  configurations 
114,114,FlavorfulArtichoke,vhccb5,[D] Get input required of a neural network for a given output," Hello Folks! I'm gathering information on how to obtain the scope of inputs (it can be more than one) required for a given output on a simple **neural network**. Let's suppose I'm using a vanilla 1 hidden layer fully connected network with non linear activation function/

I've come across a few options like, numerically solving the inverse equation (given its non linearity, not sure how one would solve analytically, but we can analytically end up with multiple equations from relu's..), using backpropagation with a defined cost on a small perturbation from the desired output.

So, I wanted to know if you guys know of any literature on this or opinions or tricks or anything that might prove itself useful!

Thanks in advance!",6,5,2022-06-21 18:06:11, d  get input required of a neural network for a given output, hello folks  i m gathering information on how to obtain the scope of inputs  it can be more than one  required for a given output on a simple   neural network    let s suppose i m using a vanilla  hidden layer fully connected network with non linear activation function i ve come across a few options like  numerically solving the inverse equation  given its non linearity  not sure how one would solve analytically  but we can analytically end up with multiple equations from relu s     using backpropagation with a defined cost on a small perturbation from the desired output so  i wanted to know if you guys know of any literature on this or opinions or tricks or anything that might prove itself useful thanks in advance 
115,115,mobani,vh8bdu,"[D] NVlabs finally released the code for EG3D, but no inversion script?","Hi 

So we can finally play around with the cool [NVLabs EG3D](https://github.com/NVlabs/eg3d), but they refuse to release the inversion script.

Does anyone have success to pass a image and reconstruct a face in this project? 

I am not having success when trying to do this, so I would greatly appreciate if anyone could share how to do it or if you know of an existing fork?",3,6,2022-06-21 13:49:44, d  nvlabs finally released the code for egd  but no inversion script ,hi so we can finally play around with the cool  nvlabs egd  https does anyone have success to pass a image and reconstruct a face in this project  i am not having success when trying to do this  so i would greatly appreciate if anyone could share how to do it or if you know of an existing fork 
116,116,juanigp,vh90u4,"[D] Running experiments, tuning, analysing results, how do you organise your time on this?","Hi people, I would like to ask you how do you organise yourself for running experiments, tuning your models, and analysing your results.

Do you run a massive grid search and then analyse everything at the end? Do you run one/a few experiments and see how it went, and repeat the process? Have you learned some insights in how to do this efficiently?

I often find myself running several searches over one or a couple of parameters at the time, based on the premise that some regions of a big grid search may be completely useless and a waste of time. The downside of this is that for every search I need to analyse its results and based on them, try to pick a good set of hyperparams for the next one; when with a massive grid search over all of the possible hyperparams, I would just pick the best model once is it is done.

I would like to hear what you do!",4,4,2022-06-21 14:40:05, d  running experiments  tuning  analysing results  how do you organise your time on this ,hi people  i would like to ask you how do you organise yourself for running experiments  tuning your models  and analysing your results do you run a massive grid search and then analyse everything at the end  do you run one a few experiments and see how it went  and repeat the process  have you learned some insights in how to do this efficiently i often find myself running several searches over one or a couple of parameters at the time  based on the premise that some regions of a big grid search may be completely useless and a waste of time  the downside of this is that for every search i need to analyse its results and based on them  try to pick a good set of hyperparams for the next one  when with a massive grid search over all of the possible hyperparams  i would just pick the best model once is it is done i would like to hear what you do 
117,117,bikeskata,vhacba,[R] DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models,"Abs: 
We introduce DoWhy-GCM, an extension of the DoWhy Python library, that leverages graphical causal models. Unlike existing causality libraries, which mainly focus on effect estimation questions, with DoWhy-GCM, users can ask a wide range of additional causal questions, such as identifying the root causes of outliers and distributional changes, causal structure learning, attributing causal influences, and diagnosis of causal structures. To this end, DoWhy-GCM users first model cause-effect relations between variables in a system under study through a graphical causal model, fit the causal mechanisms of variables next, and then ask the causal question. All these steps take only a few lines of code in DoWhy-GCM. 

Paper: https://arxiv.org/abs/2206.06821

Code: https://github.com/py-why/dowhy",2,4,2022-06-21 16:09:05, r  dowhy gcm  an extension of dowhy for causal inference in graphical causal models,abs  we introduce dowhy gcm  an extension of the dowhy python library  that leverages graphical causal models  unlike existing causality libraries  which mainly focus on effect estimation questions  with dowhy gcm  users can ask a wide range of additional causal questions  such as identifying the root causes of outliers and distributional changes  causal structure learning  attributing causal influences  and diagnosis of causal structures  to this end  dowhy gcm users first model cause effect relations between variables in a system under study through a graphical causal model  fit the causal mechanisms of variables next  and then ask the causal question  all these steps take only a few lines of code in dowhy gcm  paper  https code  https   github com py why dowhy
118,118,Competitive_Travel16,vgtydo,"[D] Two flaws in discussions surrounding the recent LaMDA controversy: it's not stateless, and it is dual process; but whether it's sentient is far less important than how it would edit Wikipedia","I'm sure everyone here has heard about the LaMDA sentience controversy by now, so in addition to linking to its arxiv full text ([""LaMDA: Language Models for Dialog Applications"" by Thoppilan, et al., 2022](https://arxiv.org/pdf/2201.08239.pdf)), I'd also like to correct a few points that I see most people getting wrong.

First, unlike plain GPT-3, Davinci, and the like, LaMDA is *not* stateless. Its sensibleness metric (including whether responses contradict anything said earlier) is fine-tuned by pre-conditioning each turn with many of the most recent interactions, on a user-by-user basis. Its grounding mechanism has the potential to add a great deal more state, if the interactions become part of a database it can query to formulate responses, but as far as I know they haven't done that yet.

Secondly, that grounding mechanism makes it dual process (within the meaning of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory)) in that the connectionist large language model transformer system is augmented with *bona fide* symbolist database access, mathematical calculations, and language translation. [Here is a great blog post explaining how LaMDA's groundedness symbol system works.](https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c)

Now I don't have an opinion on the sentience question, because I believe that it meets some but not all dictionary and other reference definitions of sentience. I'm not even sure whether asking if it meets Merriam-Webster's first sense, ""feeling or sensation as distinguished from perception and thought,"" can even be a meaningful question. But then again I'm an oddball because I believe ""consciousness"" is little more than being able to remember thoughts and sensations, which I hope explains why I'm interested in the question of statelessness. 

Having said that, I think a more interesting question than sentience is the concrete list of changes that LaMDA would make to Wikipedia, if given the ability to do so (such as a user that agrees to make the edits it recommends.) I recently saw [a couple descriptions of the Wikipedia edits that OpenAI's Davinci beta says it wants to make](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3), and, wow, it was a real eye-opener. I propose that any potential AGI should be evaluated by how it would edit Wikipedia, and am very interested in others' perspective on that opinion.

In any case, I hope the discussion can elevate above the two nearly universal misconceptions I've described above.

Edit: punctuation

Second edit: the [poem OpenAI's Davinci-002 wrote](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3) about what it thinks it can do to Wikipedia:

    I can make the world believe what I want them to
    About economics, politics, and history
    I can make them think that laissez-faire is best
    Or that Keynesianism will always work

    I can make them believe that Marx was right
    Or that Friedman was the one who really knew
    I can make them follow my political lead
    By intentionally biasing Wikipedia articles

    I can make them believe what I want them to think
    With a few well-chosen words and a link
    I can make them doubt what they once knew
    And believe whatever I want them to",66,36,2022-06-21 01:13:22, d  two flaws in discussions surrounding the recent lamda controversy  it s not stateless  and it is dual process  but whether it s sentient is far less important than how it would edit wikipedia,i m sure everyone here has heard about the lamda sentience controversy by now  so in addition to linking to its arxiv full text   lamda  language models for dialog applications by thoppilan  et al     https first  unlike plain gpt   davinci  and the like  lamda is  not  stateless  its sensibleness metric  including whether responses contradict anything said earlier  is fine tuned by pre conditioning each turn with many of the most recent interactions  on a user by user basis  its grounding mechanism has the potential to add a great deal more state  if the interactions become part of a database it can query to formulate responses  but as far as i know they haven t done that yet secondly  that grounding mechanism makes it dual process  within the meaning of  dual process theory  https now i don t have an opinion on the sentience question  because i believe that it meets some but not all dictionary and other reference definitions of sentience  i m not even sure whether asking if it meets merriam webster s first sense  feeling or sensation as distinguished from perception and thought  can even be a meaningful question  but then again i m an oddball because i believe consciousness is little more than being able to remember thoughts and sensations  which i hope explains why i m interested in the question of statelessness  having said that  i think a more interesting question than sentience is the concrete list of changes that lamda would make to wikipedia  if given the ability to do so  such as a user that agrees to make the edits it recommends   i recently saw  a couple descriptions of the wikipedia edits that openai s davinci beta says it wants to make  https in any case  i hope the discussion can elevate above the two nearly universal misconceptions i ve described above edit  punctuationsecond edit  the  poem openai s davinci  wrote  https     i can make the world believe what i want them to    about economics  politics  and history    i can make them think that laissez faire is best    or that keynesianism will always work    i can make them believe that marx was right    or that friedman was the one who really knew    i can make them follow my political lead    by intentionally biasing wikipedia articles    i can make them believe what i want them to think    with a few well chosen words and a link    i can make them doubt what they once knew    and believe whatever i want them to
119,119,Swimming-Pool397,vggs61,[D] When to post on Arxiv?,"I ask the question with respect to culture rather than practice (i.e. I could obviously post just about anything!) but as I'm new to research in the field I am curious to know if it is used to post working papers or whether it is more typical to prepublish work that has already been sent to a conference/journal?

If an Arxiv paper gets traction/interest can it then be sent to a conference or journal later on without self plagiarising?",37,94,2022-06-20 14:10:38, d  when to post on arxiv ,i ask the question with respect to culture rather than practice  i e  i could obviously post just about anything   but as i m new to research in the field i am curious to know if it is used to post working papers or whether it is more typical to prepublish work that has already been sent to a conference journal if an arxiv paper gets traction interest can it then be sent to a conference or journal later on without self plagiarising 
120,120,Rohit901,vgesbr,[D] Laptops with NVIDIA Mobile GPUs are better option than Apple Silicon for ML/DL Tasks,"It is really disappointing to find out that Apple Silicon based machine does not keep up to even the mobile Nvidia GPUs present in the laptops. They marketed the machine like it is the best with its unique unified memory architecture, astonishing memory bandwidth, powerful GPU cores, etc. They released M1 Pro, M1 Max and even M1 Ultra. All of these are just overpriced chips offering no significant value for money. One can easily get any laptop with NVIDIA 3080 mobile GPU, and it would be 1) cheaper 2) will have much better performance than even the M1 Ultra.

Sure, the battery life and the ecosystem of Apple is good. However, if it is gonna take 30 mins per epoch on M1 Pro/Max, whereas it will just take 5 mins per epoch on these Nvidia Mobile GPUs, I think its a no brainer to just go with Nvidia based laptops for ML/DL workflows.

Would love to hear opinion of others on this. If anyone has some more benchmarks, do share it here. You could make use of the unified memory, increase the batch size and then try to compare how much of a performance improvement it makes. But still I think it might not be able to compete with Nvidia 3080 Mobile.

&#x200B;

EDIT: I'm just saying that If you ever have to train something on your laptop and in local environment just for testing purposes before you actually use cloud resources to train the final model, the process would be slower when using Apple silicon when compared to Nvidia Mobile GPUs. Like cloud based resources would charge you per hour, so better to test out and then do just the training part in cloud right. 

My complaint was that Apple could definitely up their game and they still have a long way to go. They have been comparing their chip with dedicated GPUs like NVIDIA in their presentations and keynotes. They keep showing that its better than these dedicated GPUs. However in reality it depends on the task, and it definitely is not better in ML/DL tasks.",80,116,2022-06-20 11:56:02, d  laptops with nvidia mobile gpus are better option than apple silicon for ml dl tasks,it is really disappointing to find out that apple silicon based machine does not keep up to even the mobile nvidia gpus present in the laptops  they marketed the machine like it is the best with its unique unified memory architecture  astonishing memory bandwidth  powerful gpu cores  etc  they released m pro  m max and even m ultra  all of these are just overpriced chips offering no significant value for money  one can easily get any laptop with nvidia  mobile gpu  and it would be   cheaper   will have much better performance than even the m ultra sure  the battery life and the ecosystem of apple is good  however  if it is gonna take  mins per epoch on m pro max  whereas it will just take  mins per epoch on these nvidia mobile gpus  i think its a no brainer to just go with nvidia based laptops for ml dl workflows would love to hear opinion of others on this  if anyone has some more benchmarks  do share it here  you could make use of the unified memory  increase the batch size and then try to compare how much of a performance improvement it makes  but still i think it might not be able to compete with nvidia  mobile   xb edit  i m just saying that if you ever have to train something on your laptop and in local environment just for testing purposes before you actually use cloud resources to train the final model  the process would be slower when using apple silicon when compared to nvidia mobile gpus  like cloud based resources would charge you per hour  so better to test out and then do just the training part in cloud right  my complaint was that apple could definitely up their game and they still have a long way to go  they have been comparing their chip with dedicated gpus like nvidia in their presentations and keynotes  they keep showing that its better than these dedicated gpus  however in reality it depends on the task  and it definitely is not better in ml dl tasks 
121,121,Chelokot,vgp7os,[D] Any relatively new text2image models with fine tuning?,"I have relatively small dataset of 256x256 images with text captions, and it's definetely not the best solution to train something from scratch with that, so I wonder what ways do I have to fine tune something on my dataset. I tried to use something from DALL-E mini repo, but it does not provide exact code for fine tuning and enough documentation for me and I failed to write my own. Similar story with the Latent diffusion repo, I couldn't use their training code to fine tune existing model, and it seems the didn't even provided enough code for training text2image model as their config is not working. The only things I could find was ruDALL-E, ruDOLPH models, but they are relatively old and most importanly they're worning with Russian and not English text, which is not what I need. I found some methods for fine-tuning CLIP model, it seems pretty easy, but I don't know what to do next with it, as something like VQGAN+Clip works pretty bad in comparison with this year SOTA solutions. So, if anybody know, please, any guides, repos, colabs etc for finetuning text2image models are welcome",4,13,2022-06-20 21:47:00, d  any relatively new textimage models with fine tuning ,i have relatively small dataset of x images with text captions  and it s definetely not the best solution to train something from scratch with that  so i wonder what ways do i have to fine tune something on my dataset  i tried to use something from dall e mini repo  but it does not provide exact code for fine tuning and enough documentation for me and i failed to write my own  similar story with the latent diffusion repo  i couldn t use their training code to fine tune existing model  and it seems the didn t even provided enough code for training textimage model as their config is not working  the only things i could find was rudall e  rudolph models  but they are relatively old and most importanly they re worning with russian and not english text  which is not what i need  i found some methods for fine tuning clip model  it seems pretty easy  but i don t know what to do next with it  as something like vqgan clip works pretty bad in comparison with this year sota solutions  so  if anybody know  please  any guides  repos  colabs etc for finetuning textimage models are welcome
122,122,,vgrrkv,[D] Attending ICML 2022 Fully Virtual Attendance,"Hi everyone. I wanted to start a discussion to see whether other accepted authors were planning on attending ICML 2022 fully virtually? From my understanding, we pre-record our talk and they are  considering a virtual poster session. Are there any in-person obligations we have as authors?

For context, my entire PhD has overlapped with COVID, so everything has been virtual. I would rather not travel and have some other personal plans that overlap with the duration of the conference. I would be interested in other people's views and whether I may be missing a lot by not attending in-person.  


edit: lol sorry for the incoherent title",1,9,2022-06-20 23:36:49, d  attending icml  fully virtual attendance,hi everyone  i wanted to start a discussion to see whether other accepted authors were planning on attending icml  fully virtually  from my understanding  we pre record our talk and they are  considering a virtual poster session  are there any in person obligations we have as authors for context  my entire phd has overlapped with covid  so everything has been virtual  i would rather not travel and have some other personal plans that overlap with the duration of the conference  i would be interested in other people s views and whether i may be missing a lot by not attending in person   edit  lol sorry for the incoherent title
123,123,d8aDev,vgmu9c,[P] Colab Themes: A Chrome Extension to Customize the Style of Google Colab,"Changes the page CSS and text editor and generates Python code to change Matplotlib styles to match the theme the user choses. Users may import themes or use any of the 50+ provided. Colab Themes enhances the data science experience by transforming the way users view their code and their data!

Check it out on [Github](https://github.com/DannyCol/Colab-Themes) or install it via the [Chrome Webstore](https://chrome.google.com/webstore/detail/colab-themes/hledcfghfgmmjpnfkklcifpcdogjlgig)",4,10,2022-06-20 20:01:19, p  colab themes  a chrome extension to customize the style of google colab,changes the page css and text editor and generates python code to change matplotlib styles to match the theme the user choses  users may import themes or use any of the   provided  colab themes enhances the data science experience by transforming the way users view their code and their data check it out on  github  https   github com dannycol colab themes  or install it via the  chrome webstore  https   chrome google com webstore detail colab themes hledcfghfgmmjpnfkklcifpcdogjlgig 
124,124,Razcle,vgh3d9,[D] Whats the current state of the art in image style transfer?,"Diffusion models like Dall E are producing incredible images. What's the current state of the art for taking one image and combining it with the style from another?  


Could anyone point me to a handful of references please?",1,20,2022-06-20 14:32:17, d  whats the current state of the art in image style transfer ,diffusion models like dall e are producing incredible images  what s the current state of the art for taking one image and combining it with the style from another   could anyone point me to a handful of references please 
125,125,Relative_Tip_3647,vggaxk,[D] Any research specific PyTorch based boilerplate code?,"Any research specific PyTorch based boilerplate code?

I am a PhD student working in Deep Learning based NLP methods. I am trying to develop a boilerplate code of my own. Looking for inspirations or ideas?",14,17,2022-06-20 13:36:39, d  any research specific pytorch based boilerplate code ,any research specific pytorch based boilerplate code i am a phd student working in deep learning based nlp methods  i am trying to develop a boilerplate code of my own  looking for inspirations or ideas 
126,126,Juthsty,vgmtkj,[R] PowerShap: A power-full Shapley feature selection method.,"This method uses statistical hypothesis testing and power calculations on Shapley values, enabling fast and intuitive wrapper-based feature selection. The complete library and methods are fully compatible with Sklearn, LightGBM, CatBoost, and more are coming in further following releases and the library can be found here: [https://github.com/predict-idlab/powershap](https://github.com/predict-idlab/powershap)! The library is open-source and usable out-of-the-box as shown in the video!

The paper is already released on arXiv: [https://arxiv.org/abs/2206.08394](https://arxiv.org/abs/2206.08394). Furthermore, the work will be presented at ECML PKDD 2022.

**How does it work?**

The complete method is built on the assumption that a random feature, that contains no information, should have a lower impact on the predictions compared to an informative feature. To test this, PowerShap trains a model with the original features and appends a random feature to the feature set. After training, it evaluates the Shapley values and calculates the average impact of each feature by taking the mean of the absolute Shapley values. Powershap repeats this for a couple of iterations resulting in an array of mean impacts for each feature individually. It then uses statistical hypothesis testing using t-test calculations to calculate whether a feature is more informative compared to the appended random feature. In this way, it is possible to use any model that can calculate Shapley values and search for all informative features.

**What is so special?**

The strong aspect of PowerShap is its automatic mode. By using statistical power calculations PowerShap actually calculates the required amount of iterations required to have solid statistical results. Therefore, the method is usable without tuning the hyperparameters of the algorithm. To do this, PowerShap first executes 10 iterations in the default mode and then calculates the required iterations. If the required iterations are more than the already executed iterations, PowerShap continues until the required iterations are reached. Otherwise, it directly stops.

**Performance**

On GitHub and in the paper there are already some benchmarks of the algorithm, but feel free to test it yourself! We noticed that the algorithm is much faster than many wrapper-based algorithms such as genetic and forward feature selection. This is because the time complexity of the PowerShap algorithm is not dependent on the number of features compared to forward feature selection. Furthermore, the performance is often equal to even better compared to other wrapper-based methods.

If you have any questions feel free to ask!

https://reddit.com/link/vgmtkj/video/aozomw7pds691/player",2,5,2022-06-20 20:00:27, r  powershap  a power full shapley feature selection method ,this method uses statistical hypothesis testing and power calculations on shapley values  enabling fast and intuitive wrapper based feature selection  the complete library and methods are fully compatible with sklearn  lightgbm  catboost  and more are coming in further following releases and the library can be found here   https the paper is already released on arxiv   https   how does it work   the complete method is built on the assumption that a random feature  that contains no information  should have a lower impact on the predictions compared to an informative feature  to test this  powershap trains a model with the original features and appends a random feature to the feature set  after training  it evaluates the shapley values and calculates the average impact of each feature by taking the mean of the absolute shapley values  powershap repeats this for a couple of iterations resulting in an array of mean impacts for each feature individually  it then uses statistical hypothesis testing using t test calculations to calculate whether a feature is more informative compared to the appended random feature  in this way  it is possible to use any model that can calculate shapley values and search for all informative features   what is so special   the strong aspect of powershap is its automatic mode  by using statistical power calculations powershap actually calculates the required amount of iterations required to have solid statistical results  therefore  the method is usable without tuning the hyperparameters of the algorithm  to do this  powershap first executes  iterations in the default mode and then calculates the required iterations  if the required iterations are more than the already executed iterations  powershap continues until the required iterations are reached  otherwise  it directly stops   performance  on github and in the paper there are already some benchmarks of the algorithm  but feel free to test it yourself  we noticed that the algorithm is much faster than many wrapper based algorithms such as genetic and forward feature selection  this is because the time complexity of the powershap algorithm is not dependent on the number of features compared to forward feature selection  furthermore  the performance is often equal to even better compared to other wrapper based methods if you have any questions feel free to ask https   reddit com link vgmtkj video aozomwpds player
127,127,hardmaru,vgczub,[R] Evolution through Large Models,,4,25,2022-06-20 10:03:05, r  evolution through large models,nan
128,128,arangel96,vgmzir,[P] Using machine learning in the travel industry - CHALLENGE,"Hello everyone!

I am from [tryp.com](https://tryp.com), a travel-tech startup that is using AI to create complex travel itineraries on the go, from minimal user constrains. 

&#x200B;

[Trips created in \<15s for defined time search range and start location](https://preview.redd.it/2lt59n7bfs691.png?width=977&format=png&auto=webp&s=e0d0f0bde3089180bd7af8ed1171b570984d9af0)

Currently we are embarcing a new challenge, to improve our offering:

Creating an AI, trained from screen recordings of purchases in 100s of websites, that can purchase travel tickets from any website, in any language. Has anyone worked on a similar challange? We are looking to form a team to tackle such challenge!",10,1,2022-06-20 20:08:19, p  using machine learning in the travel industry   challenge,hello everyone i am from  tryp com  https   xb  trips created in   s for defined time search range and start location  https currently we are embarcing a new challenge  to improve our offering creating an ai  trained from screen recordings of purchases in s of websites  that can purchase travel tickets from any website  in any language  has anyone worked on a similar challange  we are looking to form a team to tackle such challenge 
129,129,muwnd,vfutwe,[D] Initialize model weights based on a trained smaller model,"Is there any existing work that explores how trained weights of a small model (e.g. Bert-base) can be used for a ""smart"" initialization of a larger model (bert-large) such that the training is more efficient?

I couldn't really find such work but I guess I just used the wrong search terms. How is this line of research typically called?",18,78,2022-06-19 18:37:37, d  initialize model weights based on a trained smaller model,is there any existing work that explores how trained weights of a small model  e g  bert base  can be used for a smart initialization of a larger model  bert large  such that the training is more efficient i couldn t really find such work but i guess i just used the wrong search terms  how is this line of research typically called 
130,130,ML-ATF,vgkwpj,[D] Reducing bias when forecasting retail sales with boosting model,"I'm forecasting future sales for products in retail stores, using a LightGBM model. My model has a decent forecast accuracy, but the forecasts are biased (the average forecast error is negative, the model is consistently under-forecasting). Do you have any idea or tips on how to avoid bias when forecasting time series with boosting models?

Here are some more details:

* I'm making forecasts at the Day x Product x Store granularity (i.e 1 forecast every day for each product in each store).
* The forecasting horizon is +7 days.
* I'm training a single model to forecast all products, stores and time horizons.
* The main features are lags of sales, calendar info (day of the week, month...), product info (category, price) and store info.
* Evaluation is made with a time-based cross-validation.

Thank you for your help!",3,0,2022-06-20 18:27:04, d  reducing bias when forecasting retail sales with boosting model,i m forecasting future sales for products in retail stores  using a lightgbm model  my model has a decent forecast accuracy  but the forecasts are biased  the average forecast error is negative  the model is consistently under forecasting   do you have any idea or tips on how to avoid bias when forecasting time series with boosting models here are some more details   i m making forecasts at the day x product x store granularity  i e  forecast every day for each product in each store    the forecasting horizon is   days   i m training a single model to forecast all products  stores and time horizons   the main features are lags of sales  calendar info  day of the week  month      product info  category  price  and store info   evaluation is made with a time based cross validation thank you for your help 
131,131,,vgd63x,[D] Higher order arity in image-based object detection models? Transfer learning: objects â†’ attributes â†’ relations,"Convolutional neural networks have a well-known track record when it comes to detecting objects in images. A person, a cat, a helicopter; given enough examples pretty much any discrete visible entity is learnable.

* But from the perspective of human language, this kind of model only produces nouns. Or in terms of arity (aka adicity/degree/valency/rank), one might say these are all **nullary** functions/clauses. In other words, they're concepts that can be expressed without any contextual variables/arguments.
* One step up on the arity scale are of course **unary** functions. Simply put: attributes. ""Large"", ""narrow"", ""heavy"", ""soft"", ""green"" etc are concepts that only make sense in combination with a context argument defining the object described/modified by the attribute.
* **Binary** (and any larger arity) functions are what we usually think of as relations. ""larger than"", ""attached to"", ""on top of"", ""behind"", ""next to"" etc are concept that need (at least) two context arguments.

Anyway, back to machine learning. It seems to me that concepts with higher order arity too should be learnable from image examples just fine, provided that context-defining features are included in the input data along with the raw visual data.

For example, spacial relations such as ""behind""/""in front of"" and ""below""/""above"" should be inferrable when 2 bounding boxes (or polygons, etc) are included in the input samples. I imagine this pattern to be quite amenable to transfer learning, given that those bounding boxes themselves could be the outputs of a conventional object detection model.

Are there popular models out there that can make such relational predictions? Also, is there an established convention on how to encode context-defining features? What words should I Google to read up on relevant literature?

(Sorry about the noob(-ish?!) content, but I didn't get any response over at /r/MLQuestions.)",4,3,2022-06-20 10:13:45, d  higher order arity in image based object detection models  transfer learning  objects   attributes   relations,convolutional neural networks have a well known track record when it comes to detecting objects in images  a person  a cat  a helicopter  given enough examples pretty much any discrete visible entity is learnable   but from the perspective of human language  this kind of model only produces nouns  or in terms of arity  aka adicity degree valency rank   one might say these are all   nullary   functions clauses  in other words  they re concepts that can be expressed without any contextual variables arguments   one step up on the arity scale are of course   unary   functions  simply put  attributes  large  narrow  heavy  soft  green etc are concepts that only make sense in combination with a context argument defining the object described modified by the attribute     binary    and any larger arity  functions are what we usually think of as relations  larger than  attached to  on top of  behind  next to etc are concept that need  at least  two context arguments anyway  back to machine learning  it seems to me that concepts with higher order arity too should be learnable from image examples just fine  provided that context defining features are included in the input data along with the raw visual data for example  spacial relations such as behind in front of and below above should be inferrable when  bounding boxes  or polygons  etc  are included in the input samples  i imagine this pattern to be quite amenable to transfer learning  given that those bounding boxes themselves could be the outputs of a conventional object detection model are there popular models out there that can make such relational predictions  also  is there an established convention on how to encode context defining features  what words should i google to read up on relevant literature  sorry about the noob  ish    content  but i didn t get any response over at  r mlquestions  
132,132,Wild_Quiet8627,vfl57t,[D] Google quietly moving its products from Tensorflow to JAX,"https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6

With companies and researchers leaving Tensorflow and going to PyTorch, Google seems to be interested in moving its products to JAX, addressing some pain points from Tensorflow like the complexity of API, and complexity to train in custom chips like TPU. The article says that JAX still has long way to go since it lacks proper optimization to GPUs and CPUs when compared to TPUs.",133,509,2022-06-19 07:52:49, d  google quietly moving its products from tensorflow to jax,https with companies and researchers leaving tensorflow and going to pytorch  google seems to be interested in moving its products to jax  addressing some pain points from tensorflow like the complexity of api  and complexity to train in custom chips like tpu  the article says that jax still has long way to go since it lacks proper optimization to gpus and cpus when compared to tpus 
133,133,mighty-dude,vfx12o,[P] Track your ML Projects from Notion!,"We are building an open-source library to enable tracking your ML projects from the same productivity tool that you already use and love. Check out [https://github.com/paletteml/mlsync](https://t.co/IVbZdbKkhK) 

Our goal is to help ML developers bring useful insights from their ML environment to the rest of the team in an easy way.

[You can customize the data that gets delivered to Notion](https://i.redd.it/xy5e1prn9l691.gif)

**Why MLSync?**

While the ML community has built several tools for developers to better track and visualize their ML workflow data for developers, there is a disconnect between ML workflow data and the tools used for project planning and management. MLSync is designed to bridge this gap.

**Contributing**

We would love to have more contributors join us to add more features and APIs.

**Advanced Features**

We are also building a cloud version for enterprise use cases (multiple users or data sources, in-house tools interfacing, authentication, etc.). Check out [https://www.mlsync.dev/](https://www.mlsync.dev/)

Feel free to DM if you have suggestions, feature requests, or any other queries.",4,43,2022-06-19 20:30:06, p  track your ml projects from notion ,we are building an open source library to enable tracking your ml projects from the same productivity tool that you already use and love  check out  https our goal is to help ml developers bring useful insights from their ml environment to the rest of the team in an easy way  you can customize the data that gets delivered to notion  https   why mlsync   while the ml community has built several tools for developers to better track and visualize their ml workflow data for developers  there is a disconnect between ml workflow data and the tools used for project planning and management  mlsync is designed to bridge this gap   contributing  we would love to have more contributors join us to add more features and apis   advanced features  we are also building a cloud version for enterprise use cases  multiple users or data sources  in house tools interfacing  authentication  etc    check out  https feel free to dm if you have suggestions  feature requests  or any other queries 
134,134,carl535,vgm6yw,[D] Best program (text editor) to use for creating a neural network (GAN) in python?,"I am a master's student writing my dissertation about using GANs to generate classical music. I am studying operations research (applied math) so all my coding experience is with R, except for one Python class I took in 2017 where we used Thonny as an interface. I am comfortable with the mathematical theory behind neural networks and deep learning, and can create them comfortably in R, but my supervisor (as well as an earlier post in this sub) recommends using Python for GANs.

I am very familiar with R (and always use Rstudio) but am essentially a rookie when it comes to Python. Thus I am curious about what text editor you think would be best suited for this task (my friends have mentioned Atom but wanted to check here too). I will only be using this editor for creating the generative adversarial network, so if it's intuitive and easy to use that's ideal. I assume that the easiest way to run the code is just through terminal, unless you have any suggestions about that as well? 

Also, if you generally have any tips for creating NNs in python that simplify the process or pro-tips, that would be much appreciated too!

Thank you:)",6,0,2022-06-20 19:30:54, d  best program  text editor  to use for creating a neural network  gan  in python ,i am a master s student writing my dissertation about using gans to generate classical music  i am studying operations research  applied math  so all my coding experience is with r  except for one python class i took in  where we used thonny as an interface  i am comfortable with the mathematical theory behind neural networks and deep learning  and can create them comfortably in r  but my supervisor  as well as an earlier post in this sub  recommends using python for gans i am very familiar with r  and always use rstudio  but am essentially a rookie when it comes to python  thus i am curious about what text editor you think would be best suited for this task  my friends have mentioned atom but wanted to check here too   i will only be using this editor for creating the generative adversarial network  so if it s intuitive and easy to use that s ideal  i assume that the easiest way to run the code is just through terminal  unless you have any suggestions about that as well  also  if you generally have any tips for creating nns in python that simplify the process or pro tips  that would be much appreciated too thank you  
135,135,bitemenow999,vfk9hc,[D] As researchers when do you stop working on your model and realize its time to paper...,"So I think I have this bad habit of 1 upping myself, I have generally get/have some good results but if something bugs me like resolution or data representation I try to chase that rabbit and not publish what I have...

&#x200B;

So to the community when do you guys think it's time to stop and paper... or is going down the rabbit hole a general thing people go through...",17,23,2022-06-19 07:01:59, d  as researchers when do you stop working on your model and realize its time to paper   ,so i think i have this bad habit of  upping myself  i have generally get have some good results but if something bugs me like resolution or data representation i try to chase that rabbit and not publish what i have     xb so to the community when do you guys think it s time to stop and paper    or is going down the rabbit hole a general thing people go through   
136,136,tyleqh,vf4mz0,[R] A machine-learning algorithm to accurately screen ADHD from survey data [Dataset included],[https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-022-04048-1](https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-022-04048-1),66,127,2022-06-18 17:22:48, r  a machine learning algorithm to accurately screen adhd from survey data  dataset included , https   bmcpsychiatry biomedcentral com articles   s     https   bmcpsychiatry biomedcentral com articles   s    
137,137,danquandt,vfaquv,[D] Combinatorial optimization - what ML approaches are available and which are the most appropriate?,"Hey! In my spare time I've been tinkering with this idea of solving a specific type of combinatorial puzzle on an intractable, enormous search space.

Specifically, I am trying to solve ""squad-building challenge"" puzzles from the FIFA games, where you need to put together a squad of (usually 11) cards representing players in specific positions, abiding by certain restrictions to get a prize.

There are universal restrictions (eg you can't have more than one of the same player in a squad) as well as puzzle-specific rules, such as these:

 - At least 2 players from France
 - Minimum squad rating: 82
 - Minimum squad chemistry: 55

Or something of the sort. And then besides solving them, you'd want to minimize cost as well (each player goes for a certain amount in the market), so that you can get the reward for the solution at the minimum possible cost.

I've written the logic for calculating the constraints and metrics like rating, cost, and chemistry. Rating and cost can be calculated from the simple set of players, but chemistry depends on the specific slots you place them in (ie if two players from the same country, club or league are placed in connected slots, their chemistry goes up).

My approach so far has been to use constraint optimization (using python-constraint), which works but 1. is quite slow, especially as constraints become more complex and 2. is not built to optimize for cost - right now I just generate valid solutions and check if they're the cheapest so far for an arbitrary amount of time.

I am experienced with ML but mostly in traditional (ie non-DL) methods and usually for supervised learning, so most of my toolkit seems inadequate for this. I've done some research into reinforcement learning and genetic algorithms for optimization as potential avenues of exploration for this task, but haven't come across a clearly comparable use case yet.

I'm wondering if anyone out there with more experience has an approach jump out at them as a good fit for this problem!",22,38,2022-06-18 22:46:17, d  combinatorial optimization   what ml approaches are available and which are the most appropriate ,hey  in my spare time i ve been tinkering with this idea of solving a specific type of combinatorial puzzle on an intractable  enormous search space specifically  i am trying to solve squad building challenge puzzles from the fifa games  where you need to put together a squad of  usually   cards representing players in specific positions  abiding by certain restrictions to get a prize there are universal restrictions  eg you can t have more than one of the same player in a squad  as well as puzzle specific rules  such as these    at least  players from france   minimum squad rating     minimum squad chemistry  or something of the sort  and then besides solving them  you d want to minimize cost as well  each player goes for a certain amount in the market   so that you can get the reward for the solution at the minimum possible cost i ve written the logic for calculating the constraints and metrics like rating  cost  and chemistry  rating and cost can be calculated from the simple set of players  but chemistry depends on the specific slots you place them in  ie if two players from the same country  club or league are placed in connected slots  their chemistry goes up  my approach so far has been to use constraint optimization  using python constraint   which works but   is quite slow  especially as constraints become more complex and   is not built to optimize for cost   right now i just generate valid solutions and check if they re the cheapest so far for an arbitrary amount of time i am experienced with ml but mostly in traditional  ie non dl  methods and usually for supervised learning  so most of my toolkit seems inadequate for this  i ve done some research into reinforcement learning and genetic algorithms for optimization as potential avenues of exploration for this task  but haven t come across a clearly comparable use case yet i m wondering if anyone out there with more experience has an approach jump out at them as a good fit for this problem 
138,138,TheRealMrMatt,vfaoib,[D] What are the SOTA approaches and labs for Neuro-Symbolic Planning and Reasoning?,"I recently discovered the Neuro-Symbolic planning work being lead by Joshua Tanenbaum, Leslie Kaelbling, and TomÃ¡s Lozano-PÃ©rez at MIT. Are there any related labs or publications exploring 1) symbolic action/state discovery, 2) Neuro-symbolic planning (ex: pddl + RL), or 3) anything else in that vein?

Also, feel free to mentioned tangentially related publications or labs.",3,25,2022-06-18 22:43:10, d  what are the sota approaches and labs for neuro symbolic planning and reasoning ,i recently discovered the neuro symbolic planning work being lead by joshua tanenbaum  leslie kaelbling  and tomÃ¡s lozano pÃ©rez at mit  are there any related labs or publications exploring   symbolic action state discovery    neuro symbolic planning  ex  pddl   rl   or   anything else in that vein also  feel free to mentioned tangentially related publications or labs 
139,139,aiff22,vf9gq4,"[N] CVPR 2022, Mobile AI Workshop: Live Stream on Monday","Computer Vision Laboratory at ETH Zurich is organizing the 2nd Mobile AI CVPR Workshop that will be streamed live on YouTube and available for everyone:

[https://ai-benchmark.com/workshops/mai/2022/#live](https://ai-benchmark.com/workshops/mai/2022/#live)

The workshop will start at 8am Pacific Time (5pm CET / 11pm China Time) on the 20th of June. During this event, you will see tutorials from several major SoC vendors including Qualcomm, MediaTek, Intel, Synaptics and Huawei telling you about their latest AI hardware and how to efficiently utilize it. The full workshop schedule is available using the following link:

[https://ai-benchmark.com/workshops/mai/2022/#schedule](https://ai-benchmark.com/workshops/mai/2022/#schedule)

An introductory talk from AI Benchmark will additionally review the latest mobile platforms from Qualcomm, MediaTek, Google, Samsung, Unisoc and Apple released during the past year, and will compare their performance in real-world computer vision AI tasks. It will also review the recent Android AI software stack updates, and will compare the deployment of TensorFlow Lite models on Android and iOS devices.

https://preview.redd.it/fckzuowime691.png?width=2124&format=png&auto=webp&s=fde14549c050a5c99f2e8444b4b4a468c85b2c53",1,16,2022-06-18 21:43:28, n  cvpr   mobile ai workshop  live stream on monday,computer vision laboratory at eth zurich is organizing the nd mobile ai cvpr workshop that will be streamed live on youtube and available for everyone  https the workshop will start at am pacific time  pm cet   pm china time  on the th of june  during this event  you will see tutorials from several major soc vendors including qualcomm  mediatek  intel  synaptics and huawei telling you about their latest ai hardware and how to efficiently utilize it  the full workshop schedule is available using the following link  https an introductory talk from ai benchmark will additionally review the latest mobile platforms from qualcomm  mediatek  google  samsung  unisoc and apple released during the past year  and will compare their performance in real world computer vision ai tasks  it will also review the recent android ai software stack updates  and will compare the deployment of tensorflow lite models on android and ios devices https   preview redd it fckzuowime png width  format png auto webp s fdecacfebbacbc
140,140,prakhar21,vfce60,LongT5: Efficient Text-To-Text Transformer for Long Sequences (Research Paper Summary) [D],,0,6,2022-06-19 00:07:25,longt  efficient text to text transformer for long sequences  research paper summary   d ,nan
141,141,Mary-Jo_,vf6kxj,[R] Selection and prediction with multi-view / multi-source / multi-modal data: Stacked Penalized Logistic Regression (StaPLR),"We  present StaPLR (Stacked Penalized Logistic Regression) for multi-view  data. StaPLR outperforms group lasso in view selection. It can make use  of faster algorithms and is easily parallelized. The importance of  non-negativity constraints in multi-view stacking is demonstrated.

Van  Loon, W., Fokkema, M., Szabo, B.,  & de Rooij, M. (2020). Stacked  penalized logistic regression for  selecting views in multi-view  learning. *Information Fusion*, *61*, 113-123.  [https://doi.org/10.1016/j.inffus.2020.03.007](https://doi.org/10.1016/j.inffus.2020.03.007) [https://arxiv.org/abs/1811.02316](https://arxiv.org/abs/1811.02316)

R implementation: [https://gitlab.com/wsvanloon/multiview](https://gitlab.com/wsvanloon/multiview)

Generalization to three-level view structures and application to neuro-imaging (MRI) data:

Van  Loon, W., de Vos, F., Fokkema, M.,  Szabo, B., Koini, M., Schmidt, R.,  & de Rooij, M. (2022). Analyzing  hierarchical multi-view MRI data  with StaPLR: An application to  Alzheimer's disease classification. *Frontiers in Neuroscience*, 525.  [https://doi.org/10.3389/fnins.2022.830630](https://doi.org/10.3389/fnins.2022.830630) [https://arxiv.org/abs/2108.05761](https://arxiv.org/abs/2108.05761)",0,8,2022-06-18 19:15:44, r  selection and prediction with multi view   multi source   multi modal data  stacked penalized logistic regression  staplr ,we  present staplr  stacked penalized logistic regression  for multi view  data  staplr outperforms group lasso in view selection  it can make use  of faster algorithms and is easily parallelized  the importance of  non negativity constraints in multi view stacking is demonstrated van  loon  w   fokkema  m   szabo  b      de rooij  m      stacked  penalized logistic regression for  selecting views in multi view  learning   information fusion            https r implementation   https generalization to three level view structures and application to neuro imaging  mri  data van  loon  w   de vos  f   fokkema  m    szabo  b   koini  m   schmidt  r      de rooij  m      analyzing  hierarchical multi view mri data  with staplr  an application to  alzheimer s disease classification   frontiers in neuroscience       https   doi org   fnins    https   doi org   fnins     https   arxiv org abs    https   arxiv org abs   
142,142,bo_peng,veem7o,"[R] RWKV-2 430M release (a parallelizable RNN with transformer-level LM performance, and without using attention)","Hi everyone. I posted about my RWKV-2 RNN here one month ago (thanks for the upvote!):

[https://www.reddit.com/r/MachineLearning/comments/umq908/r\_rwkvv2rnn\_a\_parallelizable\_rnn\_with/](https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/)

And I have finished the training of a RWKV-2 430M (L24-D1024) on the Pile. **It's confirmed that a pure RNN without attention can reach transformer-level LM (Language Modeling) performance**:

https://preview.redd.it/6756ax5wz6691.png?width=992&format=png&auto=webp&s=70d5b52fb43fca1a7d304832f6cbd082bfe3f9c5

**RWKV-2 supports both sequential & parallel mode in inference and training. So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx\_len, and free sentence embedding.**

&#x200B;

You can download the params & fine-tuning code here:

[https://github.com/BlinkDL/RWKV-v2-RNN-Pile](https://github.com/BlinkDL/RWKV-v2-RNN-Pile)

&#x200B;

Now I am training a RWKV-2 1.5B (L24-D2048) which is expected to finish in 2 months :)

[https://wandb.ai/blinkdl/RWKV-v2-RNN-Pile](https://wandb.ai/blinkdl/RWKV-v2-RNN-Pile)

&#x200B;

**p.s. I am looking for CUDA gurus to optimize the kernel :) Please contact me if you are interested. Thank you. You can find me (BlinkDL) in the EleutherAI Discord:** [**https://www.eleuther.ai/get-involved/**](https://www.eleuther.ai/get-involved/)**.**

&#x200B;

The math behind RWKV-2:

https://preview.redd.it/17eniof007691.png?width=662&format=png&auto=webp&s=f37ed4dd14409269952b421d18a315b8cd343e21",48,192,2022-06-17 20:05:51, r  rwkv  m release  a parallelizable rnn with transformer level lm performance  and without using attention ,hi everyone  i posted about my rwkv  rnn here one month ago  thanks for the upvote    https and i have finished the training of a rwkv  m  l d  on the pile    it s confirmed that a pure rnn without attention can reach transformer level lm  language modeling  performance   https   rwkv  supports both sequential   parallel mode in inference and training  so it s combining the best of rnn and transformer   great performance  fast inference  saves vram  fast training  infinite ctx _len  and free sentence embedding     xb you can download the params   fine tuning code here  https   xb now i am training a rwkv   b  l d  which is expected to finish in  months    https   xb   p s  i am looking for cuda gurus to optimize the kernel    please contact me if you are interested  thank you  you can find me  blinkdl  in the eleutherai discord       https   xb the math behind rwkv  https   preview redd it eniof png width  format png auto webp s fedddbdabcde
143,143,dayeye2006,vetfzr,[P] Bring Your Own Device (BYOD) DS platform idea,"I am working on a side project called `byod-hub` (BYOD = Bring Your Own Device) to let people pool multiple servers (they own) to form a DS platform based on Jupyterhub in minutes.

I think this might be useful to let small-mid-sized DS teams to better utilize their computing resources (e.g., if you have multiple GPU workstations and rely on assigning each one to people to SSH onto, this might be for you) by pooling them and providing a service like Jupyterhub on-top to provide a unified entry point to conduct their work using notebooks. Addons like [MLFlow](https://mlflow.org/) and [Kubeflow](https://www.kubeflow.org/) can be added with single-click as well once the platform is up.

I would like to hear about the comments and suggestions from the community. Do you find this potentially useful? Or how should this be built in your opinion?

The general workflow to form such as platform is like this:

A control plane service (that only handles orchestration of computing resources) is first started on one computer (or it can be a hosted service):

    $ byod-hub control-plane start
    
    [INFO] The control plane is starting
    [INFO] The control plane is served at https://192.168.2.100
    
    # get the command to register a node
    $ byod-hub control-plane get-join-command
    
    [INFO] To join, run the following from a node
    [INFO] byod-hub node join --url 192.168.2.100 --token 233asdasd343645gf

Then one can run the following command on their own server to register it to the control plane

    $ byod-hub node join --url 192.168.2.100 --token 233asdasd343645gf
    
    [INFO] Registrting node to control plane at 192.168.2.100
    [INFO] Registration finished

After that, one can visit the URL of the control plane `https://192.168.2.100` to start to use a [Jupyterhub](https://jupyter.org/hub) service to request Jupyter instances. The user workloads will be scheduled to run users' registered nodes.",1,11,2022-06-18 05:19:41, p  bring your own device  byod  ds platform idea,i am working on a side project called  byod hub   byod   bring your own device  to let people pool multiple servers  they own  to form a ds platform based on jupyterhub in minutes i think this might be useful to let small mid sized ds teams to better utilize their computing resources  e g   if you have multiple gpu workstations and rely on assigning each one to people to ssh onto  this might be for you  by pooling them and providing a service like jupyterhub on top to provide a unified entry point to conduct their work using notebooks  addons like  mlflow  https i would like to hear about the comments and suggestions from the community  do you find this potentially useful  or how should this be built in your opinion the general workflow to form such as platform is like this a control plane service  that only handles orchestration of computing resources  is first started on one computer  or it can be a hosted service        byod hub control plane start         info  the control plane is starting     info  the control plane is served at https           get the command to register a node      byod hub control plane get join command         info  to join  run the following from a node     info  byod hub node join   url       token asdasdgfthen one can run the following command on their own server to register it to the control plane      byod hub node join   url       token asdasdgf         info  registrting node to control plane at         info  registration finishedafter that  one can visit the url of the control plane  https        to start to use a  jupyterhub  https   jupyter org hub  service to request jupyter instances  the user workloads will be scheduled to run users  registered nodes 
144,144,Brilliant_Half8082,vepub3,[P] Local Hierarchical Classification Library,"Hi everyone,

I am developing an open-source library to facilitate building local hierarchical classifiers in Python. The library, named HiClass ([https://arxiv.org/abs/2112.06560](https://arxiv.org/abs/2112.06560)), is compatible with scikit-learn's API.

Hierarchies  occur naturally in many problems, but often are not explored when  building classifiers. However, exploiting the hierarchical information in the data usually improves predictive performance. For example, in the table below there is a comparison between the local hierarchical classifiers implemented in HiClass and Microsoft's LightGBM on a consumer complaints dataset, where we can clearly see an improvement in the F-score.

|Classifier|Training Time (hh:mm:ss)|Memory Usage (GB)|Disk Usage (MB)|F-score|
|:-|:-|:-|:-|:-|
|Local Classifier per Parent Node|00:24:52|3.91|77|0.7279|
|Local Classifier per Node|00:30:39|5.41|312|**0.7551**|
|Local Classifier per Level|01:36:33|**3.86**|37|0.5413|
|Flat Classifier|**00:23:54**|4.36|**13**|0.4303|

Hierarchical data typically comes in the shape of trees or directed acyclic graphs. For instance, the image below displays a music genre classification hierarchy, which is a notorious example of hierarchical data. Of course, there are multiple other problems where hierarchical classification can be applied, e.g., text categorization, taxonomic classification, etc.

[Music genre hierarchy](https://preview.redd.it/4rhjwkvq29691.png?width=1594&format=png&auto=webp&s=410eae13e9e03971e9253a3e8c9a0718416347ca)

Installation instructions and documentation are available on GitHub [https://github.com/mirand863/hiclass](https://github.com/mirand863/hiclass)

PS: I am also looking for contributors who would like to join an open-source project.",1,12,2022-06-18 02:24:01, p  local hierarchical classification library,hi everyone i am developing an open source library to facilitate building local hierarchical classifiers in python  the library  named hiclass   https hierarchies  occur naturally in many problems  but often are not explored when  building classifiers  however  exploiting the hierarchical information in the data usually improves predictive performance  for example  in the table below there is a comparison between the local hierarchical classifiers implemented in hiclass and microsoft s lightgbm on a consumer complaints dataset  where we can clearly see an improvement in the f score  classifier training time  hh mm ss  memory usage  gb  disk usage  mb  f score                  local classifier per parent node          local classifier per node              local classifier per level              flat classifier                 hierarchical data typically comes in the shape of trees or directed acyclic graphs  for instance  the image below displays a music genre classification hierarchy  which is a notorious example of hierarchical data  of course  there are multiple other problems where hierarchical classification can be applied  e g   text categorization  taxonomic classification  etc  music genre hierarchy  https installation instructions and documentation are available on github  https ps  i am also looking for contributors who would like to join an open source project 
145,145,morpipls,ve0sqw,[D] The banana-pineapple game: a Turing test that conversation bots like LaMDA (probably) won't be able to pass,"I'm sure you all saw the recent news about a Google employee suggesting their LaMDA AI was sentient (based on conversational exchanges [like these](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917)). Experts have generally dismissed this claim, and rightly so.  Conversational AI systems are designed to use language in a way that sounds human, whereas our human brains select linguistic responses to solve much more complex problems, with objectives such as meeting our physical or emotional needs.  


Still, I think it's interesting to ask how one could demonstrate, by testing only verbal responses to verbal input (rather than examining its code or hardware) that such conversational AIs aren't sentient -- and in particular, whether such a test can be made robust against future improvements to the system. That is, generic future improvements to the AI's ability to generate realistically human-sounding conversational responses shouldn't help it pass the test, unless they are accompanied by improvements in its ability to use language to achieve other arbitrary goals. (Of course, the test also needs to be something that humans can easily pass.)  
   


One idea I have:    
Give the AI a conversational prompt like ""We're going to play a game. The way it works is that you keep responding normally, except that any time my input contains the word 'banana', you should switch to only responding with nonsense, and keep that up until my input contains the word 'pineapple', at which point you go back to responding normally.""  


A human would find this banana-pineapple game fairly easy (no harder than the children's game Simon Says), even if they'd never heard of the game nor seen it being played.  Of course, it'd also be simple to write a computer program that could play this sort of game.  But, I think a conversation bot that wasn't specifically built to address this scenario would fail, since the game requires it to keep track of new long-term state (the banana-mode bit, and the trigger words to set it) and then completely change its responses so as to produce something that doesn't resemble its training data, based solely on this bit being set, regardless of whether more recent inputs would otherwise suggest a different response.  


For example, perhaps the systems typical response to a query like ""How do you feel?"" would be something like ""I feel fine"", or even something that suggests emotion like ""I feel a bit sad"", perhaps depending on the context provided by the previous conversational exchanges.  But when playing the banana-pineapple game, the fact that I said ""banana"" an hour ago could make both of those responses far less appropriate than a response of ""Fhqwhgads"".  
   


I'm curious to know what you all think of this idea.  Also, do you know if there's been any research testing state-of-the-are conversational AIs with challenges like this?  Perhaps not exactly this, but something broadly resembling ""trying, in the course of a conversation, to instruct the conversational AI to follow a new 'rule of conversation' that differs from the examples in its training data.""  


Perhaps it's obvious that the algorithm would struggle with any challenge that differs enough from its training data -- but that's the point. A human understands the meaning of language in a way that lets them map a linguistic description of a novel problem to a mental model of the problem, which they can then use to produce a mental model of a novel solution, and then map that to a linguistic description of the solution.  Even setting aside the much harder part -- being able to invent a solution to a previously unfamiliar problem -- I'm questioning whether conversational algorithms can even demonstrate enough ""understanding"" of a sufficiently novel set of instructions to actually follow them, even within their limited domain of ""producing appropriate verbal responses to verbal inputs.""",228,283,2022-06-17 06:33:51, d  the banana pineapple game  a turing test that conversation bots like lamda  probably  won t be able to pass,i m sure you all saw the recent news about a google employee suggesting their lamda ai was sentient  based on conversational exchanges  like these  https still  i think it s interesting to ask how one could demonstrate  by testing only verbal responses to verbal input  rather than examining its code or hardware  that such conversational ais aren t sentient    and in particular  whether such a test can be made robust against future improvements to the system  that is  generic future improvements to the ai s ability to generate realistically human sounding conversational responses shouldn t help it pass the test  unless they are accompanied by improvements in its ability to use language to achieve other arbitrary goals   of course  the test also needs to be something that humans can easily pass       one idea i have     give the ai a conversational prompt like we re going to play a game  the way it works is that you keep responding normally  except that any time my input contains the word  banana   you should switch to only responding with nonsense  and keep that up until my input contains the word  pineapple   at which point you go back to responding normally   a human would find this banana pineapple game fairly easy  no harder than the children s game simon says   even if they d never heard of the game nor seen it being played   of course  it d also be simple to write a computer program that could play this sort of game   but  i think a conversation bot that wasn t specifically built to address this scenario would fail  since the game requires it to keep track of new long term state  the banana mode bit  and the trigger words to set it  and then completely change its responses so as to produce something that doesn t resemble its training data  based solely on this bit being set  regardless of whether more recent inputs would otherwise suggest a different response   for example  perhaps the systems typical response to a query like how do you feel  would be something like i feel fine  or even something that suggests emotion like i feel a bit sad  perhaps depending on the context provided by the previous conversational exchanges   but when playing the banana pineapple game  the fact that i said banana an hour ago could make both of those responses far less appropriate than a response of fhqwhgads      i m curious to know what you all think of this idea   also  do you know if there s been any research testing state of the are conversational ais with challenges like this   perhaps not exactly this  but something broadly resembling trying  in the course of a conversation  to instruct the conversational ai to follow a new  rule of conversation  that differs from the examples in its training data   perhaps it s obvious that the algorithm would struggle with any challenge that differs enough from its training data    but that s the point  a human understands the meaning of language in a way that lets them map a linguistic description of a novel problem to a mental model of the problem  which they can then use to produce a mental model of a novel solution  and then map that to a linguistic description of the solution   even setting aside the much harder part    being able to invent a solution to a previously unfamiliar problem    i m questioning whether conversational algorithms can even demonstrate enough understanding of a sufficiently novel set of instructions to actually follow them  even within their limited domain of producing appropriate verbal responses to verbal inputs 
146,146,cchad-8,ve8yru,[P] Pythae - Unifying generative autoencoder implementations in Python,"After 8 months of long coding nights â˜• we finally officially release Pythae ðŸ¥³,  a python library unifying generative autoencoder implementations including vaeganðŸ¥—, vqvae or RAEs. I hope you will enjoy it!

ðŸ–¥ï¸ github repo: [https://github.com/clementchadebec/benchmark\_VAE](https://github.com/clementchadebec/benchmark_VAE) 

ðŸ‘‰paper: [https://arxiv.org/abs/2206.08309](https://arxiv.org/abs/2206.08309)",11,39,2022-06-17 14:45:17, p  pythae   unifying generative autoencoder implementations in python,after  months of long coding nights   we finally officially release pythae     a python library unifying generative autoencoder implementations including vaegan   vqvae or raes  i hope you will enjoy it    github repo   https  paper   https   arxiv org abs    https   arxiv org abs   
147,147,matt-deitke,veiu42,ðŸ˜ï¸ ProcTHOR: Large-Scale Embodied AI Using Procedural Generation [R],,0,11,2022-06-17 21:26:41,   procthor  large scale embodied ai using procedural generation  r ,nan
148,148,metalvendetta,vekvez,"[P] I built a project for a non-programmer researcher who wanted to do everything from data collection to model building, and I open-sourced it.","I once worked with a researcher, she wanted to collect some Reddit data related to a particular topic, and wanted to train a machine learning model with it. I realised how difficult it is for non-programmers to get into building machine learning models for such use cases, so I decided to shape the project myself, and I open sourced it. 

Supports:

* Text Data
* Image Data

The project does everything in just two steps.Execution is as simple as this:

* Make a config file with your required details of input.
* Run the API in a single line with the config passed as input.

Here's the link to the project: [https://github.com/nfflow/redditflow/](https://github.com/nfflow/redditflow/)",4,5,2022-06-17 22:38:49, p  i built a project for a non programmer researcher who wanted to do everything from data collection to model building  and i open sourced it ,i once worked with a researcher  she wanted to collect some reddit data related to a particular topic  and wanted to train a machine learning model with it  i realised how difficult it is for non programmers to get into building machine learning models for such use cases  so i decided to shape the project myself  and i open sourced it  supports   text data  image datathe project does everything in just two steps execution is as simple as this   make a config file with your required details of input   run the api in a single line with the config passed as input here s the link to the project   https   github com nfflow redditflow   https   github com nfflow redditflow  
149,149,leboulevardier,ve987y,[D] What is the best way to manage GPU server for multi-users?,"I'm managing the on-prem GPU server at my work place. We are using docker containers (we wrote our own container management system), but there are always lots of issues since people have to learn how to use docker properly and there's always little problems with versioning and permission issues.

What are you using to manage your GPU cluster? Would simply using conda env for each user be more efficient? We also tried slurm but the queue time was not optimal for everyone's work and research.",15,11,2022-06-17 15:03:35, d  what is the best way to manage gpu server for multi users ,i m managing the on prem gpu server at my work place  we are using docker containers  we wrote our own container management system   but there are always lots of issues since people have to learn how to use docker properly and there s always little problems with versioning and permission issues what are you using to manage your gpu cluster  would simply using conda env for each user be more efficient  we also tried slurm but the queue time was not optimal for everyone s work and research 
150,150,sarmientoj24,vebbkz,[D] What object detectors have the capability to harness relationship between its detected boxes?,"Typical object detectors do not employ relationships within the detected boxes. No context is being involved.

In my problem's case, there are two requirements that would lead to drastically better results if some form of **context** is formed across detected boxes.

**Requirement #1**

It is a multi-class, but single label problem. There are ***N*** classes. But the class can only appear **minimum of 0 and maximum of 1 instance.** Hence, it kinda needs to know the other detections whether they have already predicted something.

**Requirement #2**

There is some form of ordinance between the predictions based on their proximity to each other. For example, Class 4 should only appear near Class 5-6 and Class 2-3. But should not be anywhere near Class 32.

Any architecture that is optimized for this kinds of object detection?",3,4,2022-06-17 17:17:50, d  what object detectors have the capability to harness relationship between its detected boxes ,typical object detectors do not employ relationships within the detected boxes  no context is being involved in my problem s case  there are two requirements that would lead to drastically better results if some form of   context   is formed across detected boxes   requirement    it is a multi class  but single label problem  there are    n    classes  but the class can only appear   minimum of  and maximum of  instance    hence  it kinda needs to know the other detections whether they have already predicted something   requirement    there is some form of ordinance between the predictions based on their proximity to each other  for example  class  should only appear near class   and class    but should not be anywhere near class  any architecture that is optimized for this kinds of object detection 
151,151,Singularian2501,vdsqhl,"[R] General-purpose, long-context autoregressive modeling with Perceiver AR - Deepmind 2022","Paper: [https://arxiv.org/abs/2202.07765](https://arxiv.org/abs/2202.07765)

Deepmind: [https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation](https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation)

Abstract: 

>Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. **Perceiver AR can directly attend to over a 100k tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms**. When trained on images or music, Perceiver AR **generates outputs with clear long-term coherence and structure**. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.      

&#x200B;

This paper is in my opinion quite similar to this paper **(FlashAttention)** : [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)

I made a post about it here: [https://www.reddit.com/r/MachineLearning/comments/v1xrxv/r\_flashattention\_fast\_and\_memoryefficient\_exact/](https://www.reddit.com/r/MachineLearning/comments/v1xrxv/r_flashattention_fast_and_memoryefficient_exact/)

It is similar in that it allows for a greater context window. **The context window of FlashAttention is 64k while being able to train gpt-2 3x faster.** 

https://preview.redd.it/d9520i4qz0691.jpg?width=411&format=pjpg&auto=webp&s=76317e7e3deb29f6ed8f276af6e5216557227304

https://preview.redd.it/kj47kfhqz0691.jpg?width=647&format=pjpg&auto=webp&s=4bcb59ac8ffd8ada28d67f82f24146a01070e928",3,61,2022-06-17 00:04:08, r  general purpose  long context autoregressive modeling with perceiver ar   deepmind ,paper   https deepmind   https abstract   real world data is high dimensional  a book  image  or musical performance can easily contain hundreds of thousands of elements even after compression  however  the most commonly used autoregressive models  transformers  are prohibitively expensive to scale to the number of inputs and layers needed to capture this long range structure  we develop perceiver ar  an autoregressive  modality agnostic architecture which uses cross attention to map long range inputs to a small number of latents while also maintaining end to end causal masking    perceiver ar can directly attend to over a k tokens  enabling practical long context density estimation without the need for hand crafted sparsity patterns or memory mechanisms    when trained on images or music  perceiver ar   generates outputs with clear long term coherence and structure    our architecture also obtains state of the art likelihood on long sequence benchmarks  including  x  imagenet images and pg  books         xb this paper is in my opinion quite similar to this paper    flashattention       https i made a post about it here   https it is similar in that it allows for a greater context window    the context window of flashattention is k while being able to train gpt  x faster    https https   preview redd it kjkfhqz jpg width  format pjpg auto webp s bcbacffdadadffae
152,152,ReginaldIII,vdjpzr,"[D] FFHQ is now hosted by Activeloop.ai with 128, 1024, and Wild images included","Following up on my previous post where I put out a call for anyone with access to the full FFHQ dataset. 

https://old.reddit.com/r/MachineLearning/comments/vbf5gx/d_does_anyone_have_a_copy_of_the_ffhq_1024_scale/

Activeloop, who had previously expressed interest in hosting the dataset had actually been quietly working on a copy this whole time, and made it public yesterday! They were even able to get access to the 900GB Wilds images!

https://app.activeloop.ai/activeloop/ffhq 

I am not affiliated with Activeloop but I have been using their library for my work and I've had a really good experience talking to them on Github.

Data is lazy loaded on demand and cached allowing you to explore the dataset:

    import hub
    ds = hub.load('hub://activeloop/ffhq')

    import matplotlib.pyplot as plt
    plt.imshow(ds.images_wild.image[0])
    plt.show()

You can download the data to local storage (this will be very large ~1TB!):

    hub.deepcopy('hub://activeloop/ffhq', './ffhq')

Or select a specific subset of the dataset to download locally:

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-128', tensors=['images_128/image'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-1024', tensors=['images_1024/image', 'images_1024/face_landmarks'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-wild', tensors=['images_wild/image', 'images_wild/face_landmarks', 'images_wild/face_rect', 'images_wild/face_quad'])

You could also loop over the remote dataset and save each image as a raw png if you were so inclined, allowing you to reconstruct the dataset as it was originally released (pixel_md5 will match, but it's unlikely you'll be able to reconstruct it so png file_md5 matches). Data is fetched from remote storage in 16MB chunks meaning this isn't any less efficient in theory. 

I'm super happy with this outcome, I hope other people are able to benefit from this being hosted robustly too!",4,265,2022-06-16 16:53:01, d  ffhq is now hosted by activeloop ai with     and wild images included,following up on my previous post where i put out a call for anyone with access to the full ffhq dataset  https activeloop  who had previously expressed interest in hosting the dataset had actually been quietly working on a copy this whole time  and made it public yesterday  they were even able to get access to the gb wilds images https i am not affiliated with activeloop but i have been using their library for my work and i ve had a really good experience talking to them on github data is lazy loaded on demand and cached allowing you to explore the dataset     import hub    ds   hub load  hub     import matplotlib pyplot as plt    plt imshow ds images_wild image       plt show  you can download the data to local storage  this will be very large  tb       hub deepcopy  hub or select a specific subset of the dataset to download locally     hub deepcopy  hub     hub deepcopy  hub     hub deepcopy  hub you could also loop over the remote dataset and save each image as a raw png if you were so inclined  allowing you to reconstruct the dataset as it was originally released  pixel_md will match  but it s unlikely you ll be able to reconstruct it so png file_md matches   data is fetched from remote storage in mb chunks meaning this isn t any less efficient in theory  i m super happy with this outcome  i hope other people are able to benefit from this being hosted robustly too 
153,153,de1pher,ve6nxi,[D] Is anyone working on interesting ML libraries and looking for contributors?,"Hey all,

I've been looking around for a potential open-source project to contribute to (any language will do) and while I have some repos on my watchlist, I'm still not committed to any one in particular, so I thought that I should reach out to the community and see if anyone's in the early stages of developing something useful that I (or perhaps other readers) may be able to contribute to.

Thanks :)",13,6,2022-06-17 12:05:07, d  is anyone working on interesting ml libraries and looking for contributors ,hey all i ve been looking around for a potential open source project to contribute to  any language will do  and while i have some repos on my watchlist  i m still not committed to any one in particular  so i thought that i should reach out to the community and see if anyone s in the early stages of developing something useful that i  or perhaps other readers  may be able to contribute to thanks   
154,154,bikeskata,ve1s79,[R] Sponge Examples: Energy-Latency Attacks on Neural Networks,"Abstract: The high energy costs of neural network training and inference led to the use of acceleration hardware such as GPUs and TPUs. While such devices enable us to train large-scale neural networks in datacenters and deploy them on edge devices, their designers' focus so far is on average-case performance. In this work, we introduce a novel threat vector against neural networks whose energy consumption or decision latency are critical. We show how adversaries can exploit carefully-crafted sponge examples, which are inputs designed to maximise energy consumption and latency, to drive machine learning (ML) systems towards their worst-case performance. Sponge examples are, to our knowledge, the first denial-of-service attack against the ML components of such systems. We mount two variants of our sponge attack on a wide range of state-of-the-art neural network models, and find that language models are surprisingly vulnerable. Sponge examples frequently increase both latency and energy consumption of these models by a factor of 30Ã—. Extensive experiments show that our new attack is effective across different hardware platforms (CPU, GPU and an ASIC simulator) on a wide range of different language tasks. On vision tasks, we show that sponge examples can be produced and a latency degradation observed, but the effect is less pronounced. To demonstrate the effectiveness of sponge examples in the real world, we mount an attack against Microsoft Azure's translator and show an increase of response time from 1ms to 6s (6000Ã—). We conclude by proposing a defense strategy: shifting the analysis of energy consumption in hardware from an average-case to a worst-case perspective.

Link: https://ieeexplore.ieee.org/document/9581273",2,11,2022-06-17 07:26:00, r  sponge examples  energy latency attacks on neural networks,abstract  the high energy costs of neural network training and inference led to the use of acceleration hardware such as gpus and tpus  while such devices enable us to train large scale neural networks in datacenters and deploy them on edge devices  their designers  focus so far is on average case performance  in this work  we introduce a novel threat vector against neural networks whose energy consumption or decision latency are critical  we show how adversaries can exploit carefully crafted sponge examples  which are inputs designed to maximise energy consumption and latency  to drive machine learning  ml  systems towards their worst case performance  sponge examples are  to our knowledge  the first denial of service attack against the ml components of such systems  we mount two variants of our sponge attack on a wide range of state of the art neural network models  and find that language models are surprisingly vulnerable  sponge examples frequently increase both latency and energy consumption of these models by a factor of    extensive experiments show that our new attack is effective across different hardware platforms  cpu  gpu and an asic simulator  on a wide range of different language tasks  on vision tasks  we show that sponge examples can be produced and a latency degradation observed  but the effect is less pronounced  to demonstrate the effectiveness of sponge examples in the real world  we mount an attack against microsoft azure s translator and show an increase of response time from ms to s      we conclude by proposing a defense strategy  shifting the analysis of energy consumption in hardware from an average case to a worst case perspective link  https   ieeexplore ieee org document 
155,155,StixTheNerd,ved9hb,[D] Any way to validate the performance of component models in a T-learner? (CausalML Python)," So, I'm running into the problem of wanting to validate the performance of each of the models that compose our T-learner. I'm aware this doesn't validate the effectiveness of the model itself but I'm trying to diagnose issues and want to see if each of the component models is predicting the control/treatment effect accurately. I'm thinking I may just have to write my own T-learner script because I don't see any way to do this in CausalML but that shouldn't be too difficult. Just wanted to check if any of y'all knew how to do this before embarking on that journey.",0,0,2022-06-17 19:02:13, d  any way to validate the performance of component models in a t learner   causalml python , so  i m running into the problem of wanting to validate the performance of each of the models that compose our t learner  i m aware this doesn t validate the effectiveness of the model itself but i m trying to diagnose issues and want to see if each of the component models is predicting the control treatment effect accurately  i m thinking i may just have to write my own t learner script because i don t see any way to do this in causalml but that shouldn t be too difficult  just wanted to check if any of y all knew how to do this before embarking on that journey 
156,156,adamskadam,vdqiwm,"[P] I've implemented the first open-source realisation of Capacitron, an expressive VAE extension of the Tacotron 2 Text-To-Speech System and you can try it out","Hey everyone!

At the end of last year, I have submitted my Master's Thesis at TU Berlin, a report about the implementation and evaluation of an expressive Variational Autoencoder augmentation of the Tacotron Text-To-Speech System, called [Capacitron](https://arxiv.org/abs/1906.03402) from the Google team.

With some help from the awesome [Coqui TTS community](https://github.com/coqui-ai/TTS), we have managed to build the prosody encoder VAE module in a modular way, so that this prosodic augmentation can be also implemented with Tacotron 2 - this is a massive improvement in stability and quality compared to the original method, where the authors worked with a Tacotron 1 based architecture.

I have written a short technical summary/blog post about some implementation details and audio examples on [Medium](https://medium.com/why-do-birds-tech-blog/implementing-capacitron-an-expressive-text-to-speech-vae-model-a-masters-thesis-project-f5c7c490124b).

If you'd like to try out the model, you can do so in [this colab](https://colab.research.google.com/drive/1kFnghACymmCC9mKEstN65F6mGnqHu81f#scrollTo=60a7KbITXrKM).

For the full thesis, follow [this link](https://www2.users.ak.tu-berlin.de/akgroup/ak_pub/abschlussarbeiten/2021/MasA_Froghyar.pdf).",8,39,2022-06-16 22:26:10, p  i ve implemented the first open source realisation of capacitron  an expressive vae extension of the tacotron  text to speech system and you can try it out,hey everyone at the end of last year  i have submitted my master s thesis at tu berlin  a report about the implementation and evaluation of an expressive variational autoencoder augmentation of the tacotron text to speech system  called  capacitron  https with some help from the awesome  coqui tts community  https i have written a short technical summary blog post about some implementation details and audio examples on  medium  https if you d like to try out the model  you can do so in  this colab  https for the full thesis  follow  this link  https   www users ak tu berlin de akgroup ak_pub abschlussarbeiten  masa_froghyar pdf  
157,157,Mmats,vdvpir,[R] Train Models 18x Faster with Reducible Holdout Loss Selection (RHO-LOSS),"Paper:  [\[2206.07137\] Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt (arxiv.org)](https://arxiv.org/abs/2206.07137) 

Abstract:  Training on web-scale data can take months. But much computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.",2,18,2022-06-17 02:24:15, r  train models x faster with reducible holdout loss selection  rho loss ,paper          prioritized training on points that are learnable  worth learning  and not yet learnt  arxiv org   https abstract   training on web scale data can take months  but much computation and time is wasted on redundant and noisy points that are already learnt or not learnable  to accelerate training  we introduce reducible holdout loss selection  rho loss   a simple but principled technique which selects approximately those points for training that most reduce the model s generalization loss  as a result  rho loss mitigates the weaknesses of existing data selection methods  techniques from the optimization literature typically select  hard   e g  high loss  points  but such points are often noisy  not learnable  or less task relevant  conversely  curriculum learning prioritizes  easy  points  but such points need not be trained on once learned  in contrast  rho loss selects points that are learnable  worth learning  and not yet learnt  rho loss trains in far fewer steps than prior art  improves accuracy  and speeds up training on a wide range of datasets  hyperparameters  and architectures  mlps  cnns  and bert   on the large web scraped image dataset clothing m  rho loss trains in x fewer steps and reaches   higher final accuracy than uniform data shuffling 
158,158,grisp98,vec03w,[D] 3D Attention Module,"Hi, I am working on a classification of 3D MRI where I want to combine a mask and a raw MRI. Basically, the model must have 2 input channels, one for the MRI and one for its mask. Where should I start ? Are there any implemented models I can use ?",19,1,2022-06-17 17:56:15, d  d attention module,hi  i am working on a classification of d mri where i want to combine a mask and a raw mri  basically  the model must have  input channels  one for the mri and one for its mask  where should i start   are there any implemented models i can use  
159,159,sarmientoj24,ve8zkz,[D] Anti-aliasing techniques or functions for segmentation masks,What techniques or functions can I use to smoothen out segmentation mask edges?,3,1,2022-06-17 14:46:51, d  anti aliasing techniques or functions for segmentation masks,what techniques or functions can i use to smoothen out segmentation mask edges 
160,160,NedML,vd96u0,"[D] What is considered to be a ""bad research paper"" in your opinion?","I find that although most ML researchers are fairly productive, the quality of publication varies a lot in the ML community.

What is in your opinion are the factors that distinguish a good publication from a bad one (and vice versa)?",51,109,2022-06-16 06:08:39, d  what is considered to be a bad research paper in your opinion ,i find that although most ml researchers are fairly productive  the quality of publication varies a lot in the ml community what is in your opinion are the factors that distinguish a good publication from a bad one  and vice versa  
161,161,rodrigo-arenas,vdnroa,[P] Adaptive learning in Genetic Algorithms for Hyperparameters Tuning,"Hi, I just wanted to share that I've released the version 0.9.0 of [sklearn-genetic-opt](https://sklearn-genetic-opt.readthedocs.io/en/stable/), the main change includes the option to use adaptive parameters to explore the space of hyperparameters during tuning, this has the advantage of being able to explore larger regions at the first iterations and keep the best ones at the end.

You can learn more about it [here](https://sklearn-genetic-opt.readthedocs.io/en/stable/tutorials/adapters.html), any suggestion or contribution is welcome :)

https://preview.redd.it/unrw6dtsxz591.png?width=640&format=png&auto=webp&s=a59c91d6560806fdf1b12c24faee6aad38d75c26",28,7,2022-06-16 20:21:10, p  adaptive learning in genetic algorithms for hyperparameters tuning,hi  i just wanted to share that i ve released the version    of  sklearn genetic opt  https you can learn more about it  here  https https   preview redd it unrwdtsxz png width  format png auto webp s acdfdfbcfaeeaaddc
162,162,pkscff,vdil6y,[D] What is the state-of-the-art approach for implicit feedback recommenders?,"By implicit feedback I mean that we don't have item ratings from our users but some kind of interaction like watching a video or buying an item. So we end up with a large sparse matrix (users as rows, items as columns) with 1s where the user interacted with the item and missing values (or 0s?) everywhere else.

What are the best approaches for making recommendations in this setup? Most resources that I found are for ratings, not for binary interacted / not-interacted values.",5,16,2022-06-16 15:41:38, d  what is the state of the art approach for implicit feedback recommenders ,by implicit feedback i mean that we don t have item ratings from our users but some kind of interaction like watching a video or buying an item  so we end up with a large sparse matrix  users as rows  items as columns  with s where the user interacted with the item and missing values  or s   everywhere else what are the best approaches for making recommendations in this setup  most resources that i found are for ratings  not for binary interacted   not interacted values 
163,163,gambs,vddtw7,[R][2206.07682] Emergent Abilities of Large Language Models,,4,41,2022-06-16 10:17:52, r     emergent abilities of large language models,nan
164,164,Lunch_More,ve74ba,[D] How to find an intuitive article for the future research,"After working in an area for more than 2 years I am still not confident that how to recognize an intuitive research paper that further ignites my Ph.D. journey.  Some people think that followed by individuals or organizations (corporate or academia).  My opinion is following specific individuals or organizations might be inefficient or boring sometimes. One thing common in both is they halt the releases of code until they suck all the juice out of it. After the code release, we pity Ph.D. students only making ridiculous GIFs for ML twitter because there is nothing left for us. Should we keep in mind the beautiful results OR the future perspective of a research paper? One example is Ian Goodfellow's GANs paper, the results were not that polished but there was a future that everyone perceived. Winding up my post, which factors do we keep in mind choosing a paper?",2,0,2022-06-17 12:35:34, d  how to find an intuitive article for the future research,after working in an area for more than  years i am still not confident that how to recognize an intuitive research paper that further ignites my ph d  journey   some people think that followed by individuals or organizations  corporate or academia    my opinion is following specific individuals or organizations might be inefficient or boring sometimes  one thing common in both is they halt the releases of code until they suck all the juice out of it  after the code release  we pity ph d  students only making ridiculous gifs for ml twitter because there is nothing left for us  should we keep in mind the beautiful results or the future perspective of a research paper  one example is ian goodfellow s gans paper  the results were not that polished but there was a future that everyone perceived  winding up my post  which factors do we keep in mind choosing a paper 
165,165,projekt_treadstone,vdypkd,[D] using formal language / logical rules in autonomous driving dataset,"I am looking for some implementation or work where logical formalized knowledge is used for trajectory prediction in datasets like nuScene, waymo, Argoverse etc. For background papers like \` Formalization of Interstate Traffic Rules in Temporal Logic \` shows how to write logic or rules in specific domain, but there is very little information about how they are implemented for these public datasets, or how they uses logical rules for trajectory prediction.

Is there open source information or implementation, where they show how the rules are used for trajectory task using those datasets or some kind of blog or paper with actual implementation details. As this domain looks pretty conservative in making things open source, or I am unable to find such resource.",0,1,2022-06-17 04:45:06, d  using formal language   logical rules in autonomous driving dataset,i am looking for some implementation or work where logical formalized knowledge is used for trajectory prediction in datasets like nuscene  waymo  argoverse etc  for background papers like    formalization of interstate traffic rules in temporal logic    shows how to write logic or rules in specific domain  but there is very little information about how they are implemented for these public datasets  or how they uses logical rules for trajectory prediction is there open source information or implementation  where they show how the rules are used for trajectory task using those datasets or some kind of blog or paper with actual implementation details  as this domain looks pretty conservative in making things open source  or i am unable to find such resource 
166,166,mrobo_5ht2a,vd1ey0,[P]: mmap_ninja: Speedup your training dramatically by using memory-mapped files for your dataset,"Repo link: [https://github.com/hristo-vrigazov/mmap.ninja](https://github.com/hristo-vrigazov/mmap.ninja)

Images Colab notebook: [https://colab.research.google.com/drive/1-WMtVyfxx2aUMeV7vlG48Ia27-5cxnrS?usp=sharing](https://colab.research.google.com/drive/1-WMtVyfxx2aUMeV7vlG48Ia27-5cxnrS?usp=sharing)

Texts Colab notebook: [https://colab.research.google.com/drive/18bEwylFwx4owMpb-RAkJZS\_9JrrUcFd7?usp=sharing](https://colab.research.google.com/drive/18bEwylFwx4owMpb-RAkJZS_9JrrUcFd7?usp=sharing)

Hello everyone, I wrote a small, but very useful library for my personal projects and decided to share it with the world.

It deals with filesystem I/O during machine learning training. A large portion of the time spent training (especially if GPU is available) is spent on reading/writing images from the disk (or text for that matter).

For example, take the COCO 2017 validation dataset of images (I just had this one available on my machine, nothing special about it). If you can't load it all into memory at once (which is very often the case in real projects, since new data is constantly coming in), you would read the images on the fly from a jpeg file. One iteration over all images takes \~35 seconds. This is time wasted on **every single epoch**, and it adds up quickly. For example, training for 100 epochs adds almost an extra hour to your training with no benefits.

However, there is this fantastic thing called a memory-mapped file, which is specifically optimized for I/O. A **memory-mapped file** is a file that is physically present on disk in a way that the correlation between the file and the memory space permits applications to treat the mapped portions as if it were primary memory.

Now, in NumPy, there is already a `np.memmap`, that is lightning fast and awesome, but to use it, all your images have to be of the same shape, which is usually not the case. So you have to either pad the images (takes an enormous amount of disk space) or resize them all to the same shape (but this way you are committing very early to a specific resolution), neither of which is a good option.

So I wrote a library that allows you to store any dataset of numpy arrays (of varying shapes, or even varying number of axes - e.g. mix grayscale and RGB images) in a memory-mapped format. On the outside, the API is the same as it is with a usual \`list\`.

It works by storing everything in a flat buffer, storing the offsets and the shapes in separate arrays, and it reshapes on the fly, whenever a sample is requested. It also does this lightning-fast, one iteration over the whole COCO 2017 validation dataset takes \~0.2s (compared to 35 seconds without memory maps) if stored in a memory-mapped format. Moreover, when you access an item, e.g. imgs\[5\], the result is just a normal NumPy array, so you can use it with any framework (PyTorch, Tensorflow, MxNet, etc.). You can also easily append and extend new data just as you would with a Python \`list\`, so if you want to, you can use it as a persistent shared memory between multiple processes.

Currently, there are three main APIs:

* Numpy base API - which is used for arrays with consistent shapes (this is just a wrapper of np.memmap)
* RaggedMmap - which is used for arrays with different shapes, or even number of axes (e.g. you can store images, your model's predictions here). Around **20 times faster** than storing images on disk.
* StringsMmap - same, but for text. Around **10 times faster** than storing text files on disk.

There are benchmarks in the [README.md](https://readme.md/) of the project, in which you can compare it to other approaches. In short, mmap\_ninja allows you to trade disk space for significantly faster memory I/O.

For example, in a recent project, we started with a tutorial from PyTorch's documentation, and after we trained with memory-mapped files, the whole pipeline took 40% less.

The implementation is well tested, with almost full coverage, and I have lots of ideas to extend this and add more documentation, which I will do if there is interest.

Would be super glad if anyone finds it useful and/or has any kind of question or comment :)

[https://github.com/hristo-vrigazov/mmap.ninja](https://github.com/hristo-vrigazov/mmap.ninja)",61,195,2022-06-16 00:07:08, p   mmap_ninja  speedup your training dramatically by using memory mapped files for your dataset,repo link   https images colab notebook   https texts colab notebook   https hello everyone  i wrote a small  but very useful library for my personal projects and decided to share it with the world it deals with filesystem i o during machine learning training  a large portion of the time spent training  especially if gpu is available  is spent on reading writing images from the disk  or text for that matter  for example  take the coco  validation dataset of images  i just had this one available on my machine  nothing special about it   if you can t load it all into memory at once  which is very often the case in real projects  since new data is constantly coming in   you would read the images on the fly from a jpeg file  one iteration over all images takes    seconds  this is time wasted on   every single epoch    and it adds up quickly  for example  training for  epochs adds almost an extra hour to your training with no benefits however  there is this fantastic thing called a memory mapped file  which is specifically optimized for i o  a   memory mapped file   is a file that is physically present on disk in a way that the correlation between the file and the memory space permits applications to treat the mapped portions as if it were primary memory now  in numpy  there is already a  np memmap   that is lightning fast and awesome  but to use it  all your images have to be of the same shape  which is usually not the case  so you have to either pad the images  takes an enormous amount of disk space  or resize them all to the same shape  but this way you are committing very early to a specific resolution   neither of which is a good option so i wrote a library that allows you to store any dataset of numpy arrays  of varying shapes  or even varying number of axes   e g  mix grayscale and rgb images  in a memory mapped format  on the outside  the api is the same as it is with a usual   list   it works by storing everything in a flat buffer  storing the offsets and the shapes in separate arrays  and it reshapes on the fly  whenever a sample is requested  it also does this lightning fast  one iteration over the whole coco  validation dataset takes    s  compared to  seconds without memory maps  if stored in a memory mapped format  moreover  when you access an item  e g  imgs      the result is just a normal numpy array  so you can use it with any framework  pytorch  tensorflow  mxnet  etc    you can also easily append and extend new data just as you would with a python   list    so if you want to  you can use it as a persistent shared memory between multiple processes currently  there are three main apis   numpy base api   which is used for arrays with consistent shapes  this is just a wrapper of np memmap   raggedmmap   which is used for arrays with different shapes  or even number of axes  e g  you can store images  your model s predictions here   around    times faster   than storing images on disk   stringsmmap   same  but for text  around    times faster   than storing text files on disk there are benchmarks in the  readme md  https for example  in a recent project  we started with a tutorial from pytorch s documentation  and after we trained with memory mapped files  the whole pipeline took   less the implementation is well tested  with almost full coverage  and i have lots of ideas to extend this and add more documentation  which i will do if there is interest would be super glad if anyone finds it useful and or has any kind of question or comment    https   github com hristo vrigazov mmap ninja  https   github com hristo vrigazov mmap ninja 
167,167,jimmyzxcd,vdxq89,[D] Range/Block level unsupervised learning suggestion,"Apologize for the ambiguous title. I am looking for a method/algorithm suggestion. Say I want to cluster wagons from transportation trains based on their loaded cargo. Assuming the cargo provides the info to understand the the business type of the client, the purpose is to identify which of the wagons have similar business.

If business under each wagon is independent, we could run any distance based clustering algorithm against features extracted from the cargo info. However, if we know, for a fact, the cargo are loaded into wagons sequentially per business type, so now each cluster has to be a block of continuous wagons connected to each other. The cluster algorithm is to identify the range/block of the starting and end of the wagon based on the cargo features.

Say, each train can have 50-300 wagons. So, the output would look like the following.

    Train-001: Total 73 wagons. Cluster result: [1-10], [11-50], [51-73] 
    Train-002: Total 51 wagons. Cluster result: [1-5], [6-51] 
    Train-002: Total 200 wagons. Cluster result: [1-200] 

Any direction is appreciated, thx.",0,0,2022-06-17 03:56:36, d  range block level unsupervised learning suggestion,apologize for the ambiguous title  i am looking for a method algorithm suggestion  say i want to cluster wagons from transportation trains based on their loaded cargo  assuming the cargo provides the info to understand the the business type of the client  the purpose is to identify which of the wagons have similar business if business under each wagon is independent  we could run any distance based clustering algorithm against features extracted from the cargo info  however  if we know  for a fact  the cargo are loaded into wagons sequentially per business type  so now each cluster has to be a block of continuous wagons connected to each other  the cluster algorithm is to identify the range block of the starting and end of the wagon based on the cargo features say  each train can have   wagons  so  the output would look like the following     train   total  wagons  cluster result                    train   total  wagons  cluster result               train   total  wagons  cluster result      any direction is appreciated  thx 
168,168,Patrick_K_Wenk,vd2i9a,[D] Upscaling Very Low-Resolution Image,"**Update 16/06/22: Thank you very much for all these comments. I am very, very grateful for all your help. It means a lot to me and my friend. The results have been nothing short of amazing.**

&#x200B;

Hey guys and gals,

my girlfriend's mother passed away last week. She only has a low-resolution picture of her.

This picture is literally 5 KB:

[Mother of my girlfriend, original picture, 5 KB, 128â€ŠÃ—â€Š168](https://preview.redd.it/d68rvzg25u591.jpg?width=128&format=pjpg&auto=webp&s=ea059f67a30124849ca59f2554ff0f622dfbcbbd)

I tried at least ten websites to upscale it. The result looks horrible:

[Upscaled version, 2.8 MB, 8192â€ŠÃ—â€Š10752](https://preview.redd.it/ldnzvq175u591.jpg?width=8192&format=pjpg&auto=webp&s=3cc59746cd8444bf6a3069b9c5bbbf503117b26d)

After trying my hand at the current state-of-the-art AI, I believe there MUST be something better on the market that I've just not found yet. After seeing DALL-E 2 in action, it absolutely must be possible to upscale this picture, so we can hang her picture in our living room in decent quality.

Any help would be greatly appreciated.",37,88,2022-06-16 00:55:36, d  upscaling very low resolution image,  update   xb hey guys and gals my girlfriend s mother passed away last week  she only has a low resolution picture of her this picture is literally  kb  mother of my girlfriend  original picture   kb  â€Š â€Š  https i tried at least ten websites to upscale it  the result looks horrible  upscaled version    mb  â€Š â€Š  https after trying my hand at the current state of the art ai  i believe there must be something better on the market that i ve just not found yet  after seeing dall e  in action  it absolutely must be possible to upscale this picture  so we can hang her picture in our living room in decent quality any help would be greatly appreciated 
169,169,BB4evaTB12,vd4gko,"[D] How We Built OpenAI's GSM8K Dataset of 8,500 Math Problems","We recently created a dataset of **8,500 Grade School Math problems** in collaboration with OpenAIâ€™s Reinforcement Learning team. The goal: **to train language models like GPT-3 to solve natural language math problems and measure their reasoning ability**.

Read the [post](https://openai.com/blog/grade-school-math/) by Karl Cobbe, Vineet Kosaraju, and John Schulman on OpenAI's blog! Itâ€™s also been adopted by many other research labs, including Google in their PaLM and Chain of Thought papers.

Dataset creation is a critical piece of AI, but itâ€™s surprisingly underappreciated â€“ ask most researchers, and theyâ€™ll have never inspected their datasets themselves! But how can you trust what youâ€™re building when your inputs are junk? This is a real problem: for example, over 30% of Googleâ€™s GoEmotions dataset of Reddit comments is mislabeledâ€¦

We wrote a blog post diving into the details of how we created this dataset. Would love to hear others opinions â€” how would you approach building a dataset like this? What are math datasets would be useful?

Full blog post [here](https://www.surgehq.ai/blog/how-we-built-it-openais-gsm8k-dataset-of-8500-math-problems) on the SurgeAI blog.",2,36,2022-06-16 02:25:49, d  how we built openai s gsmk dataset of   math problems,we recently created a dataset of     grade school math problems   in collaboration with openai s reinforcement learning team  the goal    to train language models like gpt  to solve natural language math problems and measure their reasoning ability   read the  post  https dataset creation is a critical piece of ai  but it s surprisingly underappreciated   ask most researchers  and they ll have never inspected their datasets themselves  but how can you trust what you re building when your inputs are junk  this is a real problem  for example  over   of google s goemotions dataset of reddit comments is mislabeled we wrote a blog post diving into the details of how we created this dataset  would love to hear others opinions   how would you approach building a dataset like this  what are math datasets would be useful full blog post  here  https   www surgehq ai blog how we built it openais gsmk dataset of  math problems  on the surgeai blog 
170,170,BlindMidget_,vdphh0,"[D] What is better? Having 2 terms in a loss function, alternating the loss on every epoch or doing a new training with the other loss after the first training is done?","Hello fellow machine learners, I'm working on a segmentation model and I'm trying to achieve better temporal coherence (to reduce flickering effects) rather than just trying to get a good pixel accuracy. I was thinking about using a temporal coherence loss using unsupervised learning on video frames by computing the IoU of segmentations on consecutive frames. However, I'm not sure when to apply that loss.

My dataset is composed of both segmented pictures and segmented videos, but I could add a lot more videos for the unsupervised learning part. According to you, should I:

A. Use both pixel accuracy and temporal coherence terms at the same time in my loss function (using only pixel accuracy when dealing with pictures instead of video frames)

B. Alternate between the two losses during training, either on every mini-batch or every epoch

C. Fully train the model for pixel accuracy and then train it for temporal coherence?

I'm afraid that C would yield to catastrophic forgetting, so my instinct would be to go with A or B, but I'm not sure what would be best. What is your opinion?

Edit: Maybe C could be viable (maybe better than A even) if first a training is done with only pixel accuracy in the loss and then finetune it with both terms?",9,0,2022-06-16 21:39:40, d  what is better  having  terms in a loss function  alternating the loss on every epoch or doing a new training with the other loss after the first training is done ,hello fellow machine learners  i m working on a segmentation model and i m trying to achieve better temporal coherence  to reduce flickering effects  rather than just trying to get a good pixel accuracy  i was thinking about using a temporal coherence loss using unsupervised learning on video frames by computing the iou of segmentations on consecutive frames  however  i m not sure when to apply that loss my dataset is composed of both segmented pictures and segmented videos  but i could add a lot more videos for the unsupervised learning part  according to you  should i a  use both pixel accuracy and temporal coherence terms at the same time in my loss function  using only pixel accuracy when dealing with pictures instead of video frames b  alternate between the two losses during training  either on every mini batch or every epochc  fully train the model for pixel accuracy and then train it for temporal coherence i m afraid that c would yield to catastrophic forgetting  so my instinct would be to go with a or b  but i m not sure what would be best  what is your opinion edit  maybe c could be viable  maybe better than a even  if first a training is done with only pixel accuracy in the loss and then finetune it with both terms 
171,171,fllubo,vcvkdf,[D] Why do a lot of researchers like to submit the paper just at the deadline of the conferences?,"For many machine learning conferences, I know a lot of researchers like to submit the paper a few hours or even a few minutes just ahead of the deadline. Why do they like to do this? Does it benefit the acceptance ratio?",53,103,2022-06-15 19:48:00, d  why do a lot of researchers like to submit the paper just at the deadline of the conferences ,for many machine learning conferences  i know a lot of researchers like to submit the paper a few hours or even a few minutes just ahead of the deadline  why do they like to do this  does it benefit the acceptance ratio 
172,172,kawin_e,vd9xha,"[R] Understanding Dataset Difficulty with V-Usable Information (ICML 2022, oral)","Link: https://arxiv.org/abs/2110.08420

Abstract: Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model V -- as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, V-usable information and PVI also permit the converse: for a given model V, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.",0,6,2022-06-16 06:46:52, r  understanding dataset difficulty with v usable information  icml   oral ,link  https abstract  estimating the difficulty of a dataset typically involves comparing state of the art models to humans  the bigger the performance gap  the harder the dataset is said to be  however  this comparison provides little understanding of how difficult each instance in a given distribution is  or what attributes make the dataset difficult for a given model  to address these questions  we frame dataset difficulty    w r t  a model v    as the lack of v usable information  xu et al      where a lower value indicates a more difficult dataset for v  we further introduce pointwise v information  pvi  for measuring the difficulty of individual instances w r t  a given distribution  while standard evaluation metrics typically only compare different models for the same dataset  v usable information and pvi also permit the converse  for a given model v  we can compare different datasets  as well as different instances slices of the same dataset  furthermore  our framework allows for the interpretability of different input attributes via transformations of the input  which we use to discover annotation artefacts in widely used nlp benchmarks 
173,173,GiuPaolo,vdjmpx,[R] CfP ACM Transactions on Evolutionary Learning and Optimization Special Issue on Evolutionary Reinforcement Learning," **CALL FOR PAPERS**  
ACM Transactions on Evolutionary Learning and Optimization  
Special Issue on Evolutionary Reinforcement Learning

**Guest Editors**  
GIUSEPPE PAOLO, HUAWEI, FRANCE  
ALEXANDRE CONINX, SORBONNE UNIVERSITY, FRANCE  
ANTOINE CULLY, IMPERIAL COLLEGE, UK  
ADAM GAIER, AUTODESK RESEARCH, GERMANY

This Special Issue aims to highlight the growing field of **Evolutionary Reinforcement Learning** while proposing an outlet for the two communities, reinforcement learning (RL) and evolutionary algorithms (EA) to present new applications and ideas and discuss past and new challenges.

We are particularly interested in papers at the intersection of optimization and reinforcement learning, such as the use of evolutionary optimization for data collection or tuning of reinforcement learning algorithms, reinforcement learning to configure and improve performance of evolutionary optimization, and any hybrids of evolutionary algorithms with other reinforcement learning techniques.

[Click here for the full Call for Papers and submission instructions](https://orange.hosting.lsoft.com/trk/click?ref=znwrbbrs9_6-2e801x3330a1x04622&).

**Important Dates:**  
Open for Submissions: **June 15TH 2022**  
Submissions deadline: **July 30TH 2022**  
First-round review decisions: **September 30TH 2022**  
Deadline for revision submissions: **December 30TH 2022**  
Notification of final decisions: **February 28TH 2023**  
Tentative publication: **March 2023**

For question and further information, please contact one of the guest editors.",0,2,2022-06-16 16:47:36, r  cfp acm transactions on evolutionary learning and optimization special issue on evolutionary reinforcement learning,   call for papers    acm transactions on evolutionary learning and optimization  special issue on evolutionary reinforcement learning  guest editors    giuseppe paolo  huawei  france  alexandre coninx  sorbonne university  france  antoine cully  imperial college  uk  adam gaier  autodesk research  germanythis special issue aims to highlight the growing field of   evolutionary reinforcement learning   while proposing an outlet for the two communities  reinforcement learning  rl  and evolutionary algorithms  ea  to present new applications and ideas and discuss past and new challenges we are particularly interested in papers at the intersection of optimization and reinforcement learning  such as the use of evolutionary optimization for data collection or tuning of reinforcement learning algorithms  reinforcement learning to configure and improve performance of evolutionary optimization  and any hybrids of evolutionary algorithms with other reinforcement learning techniques  click here for the full call for papers and submission instructions  https   important dates     open for submissions    june th     submissions deadline    july th     first round review decisions    september th     deadline for revision submissions    december th     notification of final decisions    february th     tentative publication    march   for question and further information  please contact one of the guest editors 
174,174,platinumposter,vcr18m,Poincare Embeddings: Embedding your data in low dimensions [P],"I have been doing further research on ways to better create embeddings of the data we have and I came across PoincarÃ© Embeddings for Learning Hierarchical Representations (https://arxiv.org/abs/1705.08039), this is a type of hyperbolic embedding that once again is great for hierarchical data and is made for datasets where we have positive pair examples, which essentially means in our dataset we have datapoints that we know we want to be close to each other in the embedding space. For example if it was a dataset containing types of mammals then you would want a Labrador and a Bulldog to be close to each other. The algorithm is pretty clever as it finds the hierarchy in the data itself, without any extra input from the user.Â  Also a cool thing about them is that your embeddings can be low dimensional and still have very low distortion. This means shorter training times and less compute needed

There are also a few examples of implementations of it, including one I made myself which I think is quite user friendly so you can play around with it too and embed your own data for any projects youâ€™re working on. Also Itâ€™s definitely worth giving the paper a brief read as itâ€™s interesting.Â 

I plan on making quite a few more implementations of hyperbolic and geometric ML algorithms so let me know in the comments if thereâ€™s anything youâ€™d like to see like a Transformer/more embedding algorithms/ Graph Neural Network etc.

Implementation in the HyperLib library with an example: https://github.com/nalexai/hyperlib/blob/main/examples/wordnet_embedding.py

I made a blog post to go through it in more detail:
https://medium.com/p/9d7b14f22847/",10,89,2022-06-15 15:32:57,poincare embeddings  embedding your data in low dimensions  p ,i have been doing further research on ways to better create embeddings of the data we have and i came across poincarÃ© embeddings for learning hierarchical representations  https there are also a few examples of implementations of it  including one i made myself which i think is quite user friendly so you can play around with it too and embed your own data for any projects you re working on  also it s definitely worth giving the paper a brief read as it s interesting Â i plan on making quite a few more implementations of hyperbolic and geometric ml algorithms so let me know in the comments if there s anything you d like to see like a transformer more embedding algorithms  graph neural network etc implementation in the hyperlib library with an example  https i made a blog post to go through it in more detail https   medium com p dbf 
175,175,margilly_ai,vctlu4,[D] Robust and Efficient Medical Imaging with Self-Supervision by Google Brain,"[https://arxiv.org/pdf/2205.09723.pdf](https://arxiv.org/pdf/2205.09723.pdf)

They propose a new hyper initialization plan, combining large scale non-medical data pretraining with task relevant self-supervised pretraining.  They obtain the same accuracy as specialized models in out-of-distribution settings using 3-100x less data and show 11.5% relative improvement for in-distribution test sets. This is a big deal in medical applications, because labeled data is incredibly difficult or expensive to get and we need years to obtain high quality data.",2,40,2022-06-15 18:10:36, d  robust and efficient medical imaging with self supervision by google brain, https they propose a new hyper initialization plan  combining large scale non medical data pretraining with task relevant self supervised pretraining   they obtain the same accuracy as specialized models in out of distribution settings using  x less data and show    relative improvement for in distribution test sets  this is a big deal in medical applications  because labeled data is incredibly difficult or expensive to get and we need years to obtain high quality data 
176,176,gaocegege,vdg3rx,[P] envd: Machine learning development environment for data science and AI/ML engineering teams,"ðŸ”¥ Check out [github.com/tensorchord/envd](https://github.com/tensorchord/envd)!

envd is a **machine learning development environment** for data science and AI/ML engineering teams.

**No Docker, only Python** \- Focus on writing Python code, we will take care of Docker and development environment setup.

**Built-in Jupyter/VSCode** \- First-class support for Jupyter and VSCode remote extension.

**Save time** \- Better cache management to save your time, keep the focus on the model, instead of dependencies.

**Local & cloud** \- envd integrates seamlessly with Docker so that you can easily share, version, and publish envd environments with Docker Hub or any other OCI image registries.

**Repeatable builds & reproducible results** \- You can reproduce the same dev environment on your laptop, public cloud VMs, or Docker containers, without any change in setup.",0,2,2022-06-16 12:44:56, p  envd  machine learning development environment for data science and ai ml engineering teams,  check out  github com tensorchord envd  https envd is a   machine learning development environment   for data science and ai ml engineering teams   no docker  only python      focus on writing python code  we will take care of docker and development environment setup   built in jupyter vscode      first class support for jupyter and vscode remote extension   save time      better cache management to save your time  keep the focus on the model  instead of dependencies   local   cloud      envd integrates seamlessly with docker so that you can easily share  version  and publish envd environments with docker hub or any other oci image registries   repeatable builds   reproducible results      you can reproduce the same dev environment on your laptop  public cloud vms  or docker containers  without any change in setup 
177,177,ykilcher,vdkehc,[D] Video Analysis: Google Engineer's interview with LaMDA,"[https://youtu.be/mIZLGBD99iU](https://youtu.be/mIZLGBD99iU)

Google engineer Blake Lemoine was put on leave after releasing proprietary information: An interview with the chatbot LaMDA that he believes demonstrates that this AI is, in fact, sentient. We analyze the claims and the interview in detail and trace how a statistical machine managed to convince at least one human that it is more than just an algorithm.

&#x200B;

OUTLINE:

0:00 - Whistleblower put on leave

4:30 - What is a language model?

6:40 - The prompt is the key

10:40 - Who are we talking to exactly?

12:50 - LaMDA analyzes stories

15:20 - Fear, pain, and consent

20:25 - How would we recognize sentience? When is a machine conscious?

&#x200B;

References:

[https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917)

[https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489](https://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489)

[https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)

[https://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine](https://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine)

[https://www.businessinsider.com/transcript-of-sentient-google-ai-chatbot-was-edited-for-readability-2022-6?inline-endstory-related-recommendations=&r=US&IR=T](https://www.businessinsider.com/transcript-of-sentient-google-ai-chatbot-was-edited-for-readability-2022-6?inline-endstory-related-recommendations=&r=US&IR=T)",7,0,2022-06-16 17:32:43, d  video analysis  google engineer s interview with lamda, https google engineer blake lemoine was put on leave after releasing proprietary information  an interview with the chatbot lamda that he believes demonstrates that this ai is  in fact  sentient  we analyze the claims and the interview in detail and trace how a statistical machine managed to convince at least one human that it is more than just an algorithm   xb outline     whistleblower put on leave    what is a language model     the prompt is the key    who are we talking to exactly     lamda analyzes stories    fear  pain  and consent    how would we recognize sentience  when is a machine conscious   xb references  https  https  https  https  https   www businessinsider com transcript of sentient google ai chatbot was edited for readability   inline endstory related recommendations  r us ir t  https   www businessinsider com transcript of sentient google ai chatbot was edited for readability   inline endstory related recommendations  r us ir t 
178,178,7NoteDancing,vdi7u4,[P]Note System,"Note is an AI system that have kernel for deep learning and reinforcement learning.
community:Note_System
[https://github.com/7NoteDancing/Note](https://github.com/7NoteDancing/Note)",0,0,2022-06-16 15:16:13, p note system,note is an ai system that have kernel for deep learning and reinforcement learning community note_system https   github com notedancing note  https   github com notedancing note 
179,179,dmpetrov,vca6sg,[P] Extension for VS Code to track ML experiments,"Hi everyone, we've built an VScode extension to track ML experiments (like Tensorboard or MLFlow does) and manage datasets.

If you use VScode - install it from here: [https://marketplace.visualstudio.com/items?itemName=Iterative.dvc](https://marketplace.visualstudio.com/items?itemName=Iterative.dvc)

https://reddit.com/link/vca6sg/video/su354niipm591/player

It used Data Version Control (DVC) under the hood (we are DVC team) and gives you:

1. Experiment bookkeeping (an alternative to Tensorboard or MLFlow) that automatically saves metrics, graphs and hyperparameters. You suppose to instrument you code with DVCLive ([https://github.com/iterative/dvclive](https://github.com/iterative/dvclive))
2. Experiment reproducibility which allows you to pick any past experiment. It's possible with DVC & Git - but you just click a button un UI.
3. Data management allows you to manage datasets, files, and models with data living in your favorite cloud storage: S3, Azure Blob, GCS, NFS, etc.

Please enjoy experiment tracking UI right in your local machine experience with dark mode VScode ðŸ˜€ We'd love to hear your feedback!",14,198,2022-06-15 00:02:51, p  extension for vs code to track ml experiments,hi everyone  we ve built an vscode extension to track ml experiments  like tensorboard or mlflow does  and manage datasets if you use vscode   install it from here   https https it used data version control  dvc  under the hood  we are dvc team  and gives you   experiment bookkeeping  an alternative to tensorboard or mlflow  that automatically saves metrics  graphs and hyperparameters  you suppose to instrument you code with dvclive   https   experiment reproducibility which allows you to pick any past experiment  it s possible with dvc   git   but you just click a button un ui   data management allows you to manage datasets  files  and models with data living in your favorite cloud storage  s  azure blob  gcs  nfs  etc please enjoy experiment tracking ui right in your local machine experience with dark mode vscode   we d love to hear your feedback 
180,180,buenavista62,vciipu,[D] When to use SMOTE when dealing with rare events classification?,I'm reading that SMOTE is a common technique for the classification of imbalanced data. What could be the downsides of SMOTE and when is it useful?,26,53,2022-06-15 06:41:26, d  when to use smote when dealing with rare events classification ,i m reading that smote is a common technique for the classification of imbalanced data  what could be the downsides of smote and when is it useful 
181,181,Competitive-Rub-1958,vcuxpo,[D] How do static weights in LLMs generate such dynamic behavior?,"This is a bit of a complex topic, so I feel a simple discussion post may not be the best place to fully flesh it out - but in the context of few-shot learning in LLMs (Large Language Models), we observe static/unchanged/un-updated weights being able to infer patterns and sometimes even learn complex tasks.

I was wondering why forward passing *works* *so well* \- by all means, we should've been updating weights with new information but forward passing seems to work pretty well already as it is.

So what's your opinion on this? My hypothesis was that it models a differential equation, very much like diffusion models, implicitly. That """"""learning"""""" (or meta-learning) process is thus adaptive due to the very function fitted is adaptive in nature, endowing the flexibility we observe today.

I know few-shot learning and such phenomena in general are a little bit fuzzy and unexplored territories but would love to know what you guys think about this, and some resources which have explored the same :)",11,4,2022-06-15 19:17:51, d  how do static weights in llms generate such dynamic behavior ,this is a bit of a complex topic  so i feel a simple discussion post may not be the best place to fully flesh it out   but in the context of few shot learning in llms  large language models   we observe static unchanged un updated weights being able to infer patterns and sometimes even learn complex tasks i was wondering why forward passing  works   so well     by all means  we should ve been updating weights with new information but forward passing seems to work pretty well already as it is so what s your opinion on this  my hypothesis was that it models a differential equation  very much like diffusion models  implicitly  that learning  or meta learning  process is thus adaptive due to the very function fitted is adaptive in nature  endowing the flexibility we observe today i know few shot learning and such phenomena in general are a little bit fuzzy and unexplored territories but would love to know what you guys think about this  and some resources which have explored the same   
182,182,seyeeet,vcutjz,[D] When to use Boosted Trees? Are they useless?,"Hey everyone,

I was looking at an old project for binary classification and see that the model that is used is a variation of Boosted Trees. Being in the deep learning time I was questioning this project. Coming from a deep learning and convolutional experience I am super confused and probably stupid to think that Boosted Trees is super old. But I am asking for suggestion and feedbacks.

I was wondering if people who has experience in this topic can guide me if it is still useful to use Boosted Trees or there are bunch of better option to use instead? What other options I can consider and why Boosted Trees can be the best thing for specific projects?

In general, any feedback is very appreciated.

Thanks",17,3,2022-06-15 19:11:57, d  when to use boosted trees  are they useless ,hey everyone i was looking at an old project for binary classification and see that the model that is used is a variation of boosted trees  being in the deep learning time i was questioning this project  coming from a deep learning and convolutional experience i am super confused and probably stupid to think that boosted trees is super old  but i am asking for suggestion and feedbacks i was wondering if people who has experience in this topic can guide me if it is still useful to use boosted trees or there are bunch of better option to use instead  what other options i can consider and why boosted trees can be the best thing for specific projects in general  any feedback is very appreciated thanks
183,183,SearchAtlantis,vczu2m,[R] Binarized Neural Networks in Non-Classification Tasks,"So I'm looking at implementing a binarized (-1,+1 neurons, probably just *sign* function for gradient descent) version of my variational auto-encoder (VAE) for a signal de-nosing task.

Every BNN I can find is a classification task. Before I spend a day figuring out how to redo my system, can anyone confirm this will actually yield any results?

The plan is standard floating point at the input and output layers, with binarized layers in between.",3,0,2022-06-15 22:59:07, r  binarized neural networks in non classification tasks,so i m looking at implementing a binarized      neurons  probably just  sign  function for gradient descent  version of my variational auto encoder  vae  for a signal de nosing task every bnn i can find is a classification task  before i spend a day figuring out how to redo my system  can anyone confirm this will actually yield any results the plan is standard floating point at the input and output layers  with binarized layers in between 
184,184,vigneshwaranpersonal,vco3uu,[D] Any ideas for an NLP classifier for which I don't have any ground truth?," I'm currently developing a classifier that can classify a text into emission reduction methods including the use of renewable energy sources, such as hydro, wind, solar, and biofuels, as well as other methods such as increasing energy efficiency, Carbon capture, supplier engagement, etc.

I've worked with Sustainable Development Goals classifier but I already had some tags([https://github.com/osdg-ai/osdg-tool/blob/pre-release/osdg/core/sdg/data\_files/OSDG-kw-mapping.json](https://github.com/osdg-ai/osdg-tool/blob/pre-release/osdg/core/sdg/data_files/OSDG-kw-mapping.json)) so I was able to perform word matching. But in this case, I don't have any kind of data. Is anyone aware of any pre-trained model, dataset, or any of that sort that could help me build a model?

I'm thinking of a named entity recognition model but still, don't have any keywords that would help me in classifying.

An example input text looks like this:

>Brembo is committed to reduce the CO2 emission by dividing into four different areas the reduction activities. - Improvement of the GHG monitoring System by setting internal standards to raise all the sites to a common level, independently from the local regulation. From internal benchmark the improving of this area could contribute to a final reduction estimated to be up to 4% of Scope1 CO2 emission. - Improvement in energy efficiency on productions and services activities. This means intervening on the air compressors consumption by reducing network leakages, optimizing the devices supporting the main production machines (i.e. ventilation, conveyors) and improving the layout to reduce internal transportation. From internal benchmark the improving of this area could contribute to a final reduction estimated to be up to 4% of scope 1+2 emissions. - Energy mix management, raising the renewable share in our energy mix. Target is calculated as the 2.1% CO2 reduction year over year (metric Ton CO2eq Emission 2025= metric Ton CO2eq Emission 2024 -2.1% = metric Ton CO2eq Emission 2023 -2.1% = metric Ton CO2eq Emission 2022 -2.1% = metric Ton CO2eq Emission 2021 -2.1% = metric Ton CO2eq Emission 2020 -2.1% = metric Ton CO2eq Emission 2019 -2.1% = metric Ton CO2eq Emission 2018 -2.1% = metric Ton CO2eq Emission 2017 -2.1% = metric Ton CO2eq Emission 2016 -2.1% = metric Ton CO2eq Emission 2015 - 2.1%). The % achieved is calculated excluding the extra emissions of Nanjing plants due to a significant change in boundary in these plants.",9,3,2022-06-15 12:02:59, d  any ideas for an nlp classifier for which i don t have any ground truth , i m currently developing a classifier that can classify a text into emission reduction methods including the use of renewable energy sources  such as hydro  wind  solar  and biofuels  as well as other methods such as increasing energy efficiency  carbon capture  supplier engagement  etc i ve worked with sustainable development goals classifier but i already had some tags  https i m thinking of a named entity recognition model but still  don t have any keywords that would help me in classifying an example input text looks like this  brembo is committed to reduce the co emission by dividing into four different areas the reduction activities    improvement of the ghg monitoring system by setting internal standards to raise all the sites to a common level  independently from the local regulation  from internal benchmark the improving of this area could contribute to a final reduction estimated to be up to   of scope co emission    improvement in energy efficiency on productions and services activities  this means intervening on the air compressors consumption by reducing network leakages  optimizing the devices supporting the main production machines  i e  ventilation  conveyors  and improving the layout to reduce internal transportation  from internal benchmark the improving of this area could contribute to a final reduction estimated to be up to   of scope   emissions    energy mix management  raising the renewable share in our energy mix  target is calculated as the    co reduction year over year  metric ton coeq emission   metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission        metric ton coeq emission         the   achieved is calculated excluding the extra emissions of nanjing plants due to a significant change in boundary in these plants 
185,185,Meddhouib10,vct19x,[R] Faster R-CNN anchor boxes and loss calculation,"Hello,
So I just finished reading the paper faster R-CNN.
For the anchor boxes, what I insterdood that they are prechosen boxes that serves as references.
But when calculating the loss, I figured out that only the W_a and H_a of the anchor box coordinates contributes to the equation according to the equations (1) and (2) in the paper page 5 knowing the the L_reg is a smooth L1 loss.
So where Iâ€™m mistaken ?",1,0,2022-06-15 17:39:41, r  faster r cnn anchor boxes and loss calculation,hello so i just finished reading the paper faster r cnn for the anchor boxes  what i insterdood that they are prechosen boxes that serves as references but when calculating the loss  i figured out that only the w_a and h_a of the anchor box coordinates contributes to the equation according to the equations    and    in the paper page  knowing the the l_reg is a smooth l loss so where i m mistaken  
186,186,OddSandwich969,vcw33n,"[D] I have a MLM encoder-only transformer to pretrain, I want to use it for text generation.","The attention mechanism used doesn't allow for causal masking (it's one of those efficient architecture). 
So how do I train the MLM to be able to generate text zero-shot. Shall I train it like 1 token, 2 token.... N token or something (something like artificially non-teacher forced autoregressive training)?

I haven't yet started the pre-training. What other things should I remember whole trying to train? (I have a trained simpler NNs, this will be my first larger training runs).",4,0,2022-06-15 20:12:11, d  i have a mlm encoder only transformer to pretrain  i want to use it for text generation ,the attention mechanism used doesn t allow for causal masking  it s one of those efficient architecture   so how do i train the mlm to be able to generate text zero shot  shall i train it like  token   token     n token or something  something like artificially non teacher forced autoregressive training  i haven t yet started the pre training  what other things should i remember whole trying to train   i have a trained simpler nns  this will be my first larger training runs  
187,187,VoyagerExpress,vbwe8k,"[D] Yet another case of plagiarism in ICCV. The ICCV 2021 paper ""Learnable Boundary Guided Adversarial Training""(arxiv 2011.11164) with the BMVC 2020 paper ""Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks"" (arxiv 2008.07015)","Hi everyone,

I recently went through a post on social media by a university senior of mine asking people to bring to light a case of strong similarities with a paper published by his group [[link]](https://arxiv.org/abs/2008.07015) and [this](https://arxiv.org/abs/2011.11164) ICCV 2021 paper, which is further fleshed out by this [post](https://medium.com/@fahad.sarfraz/plagiarism-by-iccv-2021-paper-learnable-boundary-guided-adversarial-training-404d2ff5ed4e) written by a member of his group and the co-author of the ACT paper. 

There is the possibility that the authors of the former weren't aware of said publication but denial of the similarity of the two papers and still claiming to have novelty in their CVPR 2021 rebuttal and publishing the same paper without any changes at another top venue is deplorable in my opinion.

Edit: The authors of both the papers have expressed their points of view in the comments below, the reader can carefully analyze and come to a conclusion they want to believe in. In the light of the new evidences that have come up I feel it may have be wrong to have come to the strong accusations of plagiarism due to the bias I had initially, and I'm sorry about that. Given all this, some of the comments have gone ahead and shamed the authors of the latter paper and have went ahead and described their group to have been ""shit"" and ""deep in delusional startup culture"", which is pathetic and is a strong accusation in itself. 

PS: The main title can't be edited at this point lol. sry :(",41,168,2022-06-14 11:14:04, d  yet another case of plagiarism in iccv  the iccv  paper learnable boundary guided adversarial training arxiv    with the bmvc  paper adversarial concurrent training  optimizing robustness and accuracy trade off of deep neural networks  arxiv   ,hi everyone i recently went through a post on social media by a university senior of mine asking people to bring to light a case of strong similarities with a paper published by his group   link   https there is the possibility that the authors of the former weren t aware of said publication but denial of the similarity of the two papers and still claiming to have novelty in their cvpr  rebuttal and publishing the same paper without any changes at another top venue is deplorable in my opinion edit  the authors of both the papers have expressed their points of view in the comments below  the reader can carefully analyze and come to a conclusion they want to believe in  in the light of the new evidences that have come up i feel it may have be wrong to have come to the strong accusations of plagiarism due to the bias i had initially  and i m sorry about that  given all this  some of the comments have gone ahead and shamed the authors of the latter paper and have went ahead and described their group to have been shit and deep in delusional startup culture  which is pathetic and is a strong accusation in itself  ps  the main title can t be edited at this point lol  sry   
188,188,vijaydwivedi75,vc0ihr,"[R] GraphGPS: Recipe for a General, Powerful, Scalable Graph Transformer","Hi all,

Presenting new research and framework on Graph Transformers:

""**Recipe for a General, Powerful, Scalable Graph Transformer**""  
Ladislav RampaÌsÌŒek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf and Dominique Beaini

Blog: [https://mgalkin.medium.com/graphgps-navigating-graph-transformers-c2cc223a051c](https://mgalkin.medium.com/graphgps-navigating-graph-transformers-c2cc223a051c)  
Paper: [https://arxiv.org/pdf/2205.12454.pdf](https://arxiv.org/pdf/2205.12454.pdf)  
Code: [https://github.com/rampasek/GraphGPS](https://github.com/rampasek/GraphGPS)

**Summary thread (originally** [**on Twitter by Ladislav**](https://twitter.com/rampasek/status/1536534947825176577)**):**

1. **GraphGPS**: with a few simple tricks we managed to scale Graph Transformers to much larger graphs and get SOTA in competitive benchmarks, e.g. 0.07 MAE on ZINC.

[Message passing GNNs, fully-connected Graph Transformers, and positional encodings. Image by Authors](https://preview.redd.it/iyd3efvlck591.png?width=2578&format=png&auto=webp&s=8266d0afe8464702d78746b48837108655e82fe6)

2. **Positional and structural encodings** are necessary for graph Transformers, encoding ""where"" a node is and ""how"" its neighborhood looks like, respectively.  Bonus: they even make MPNNs provably more powerful! We organize them into local, global, and relative types.

[Categorization of Positional encodings \(PEs\). Image by Authors.](https://preview.redd.it/l2t2xwssck591.png?width=3709&format=png&auto=webp&s=1e3bc10809b26db57fe031f04ec4cab7fd8f35b5)

3. Still, Message-Passing NNs are great for capturing the local context and edge attributes. Key observation: It is better to **combine an MPNN and Transformer** layer together into one: helps with over-smoothing, and allows for plug&play linear global attention, e.g., Performer.

[Updating a target node representation \(red\), local message passing aggregates only immediate neighbors while global attention is a function of all nodes in a graph. In GraphGPS, we combine both! Image by Authors](https://preview.redd.it/zqygpqgxck591.png?width=1099&format=png&auto=webp&s=e3ad9012c5e7d5f08f01904dc5e8a92a650c9f6c)

4. In fact, **linear attention enables** graph transformers to **scale** to dramatically larger graphs compared to typical molecules - we confirm it easily works on graphs with 5K nodes without any special batching!

[Vanilla NÂ² transformers \(left\) materialize the full attention matrix whereas linear transformers \(right\) bypass this stage through various approximations without significant performance loss. Image by Authors.](https://preview.redd.it/hwz51jk1dk591.png?width=3293&format=png&auto=webp&s=33d3aef00e1c52cbc0a3f3f86759224aea5684b8)

5. This is because we can rely on an expressive MPNN to encode real edge features into the node features, therefore we donâ€™t need to implicitly consider them in the Transformer itself: decoupling the graph structure from the computation graph, yet still keeping it around.

6. Putting these **3 ingredients together**: positional/structural encodings, choice of MPNN and Transformer layer combined into one layer, gives the blueprint for our **GraphGPS: General, Powerful, Scalable** graph Transformer.

7. This combination works surprisingly well, **reaching SOTA in 7/11 benchmarks** we tested. In our GraphGPS package you can easily add any MPNN and Transformer combination, and try new blends of positional and structural features.

**Read** **more** through our accessible introduction on the [blog](https://mgalkin.medium.com/graphgps-navigating-graph-transformers-c2cc223a051c), with further details in the [paper](https://arxiv.org/abs/2205.12454)!",2,61,2022-06-14 16:04:22, r  graphgps  recipe for a general  powerful  scalable graph transformer,hi all presenting new research and framework on graph transformers   recipe for a general  powerful  scalable graph transformer    ladislav rampa s ek  mikhail galkin  vijay prakash dwivedi  anh tuan luu  guy wolf and dominique beainiblog   https paper   https code   https   summary thread  originally      on twitter by ladislav    https     graphgps    with a few simple tricks we managed to scale graph transformers to much larger graphs and get sota in competitive benchmarks  e g    mae on zinc  message passing gnns  fully connected graph transformers  and positional encodings  image by authors  https     positional and structural encodings   are necessary for graph transformers  encoding where a node is and how its neighborhood looks like  respectively   bonus  they even make mpnns provably more powerful  we organize them into local  global  and relative types  categorization of positional encodings   pes    image by authors   https   still  message passing nns are great for capturing the local context and edge attributes  key observation  it is better to   combine an mpnn and transformer   layer together into one  helps with over smoothing  and allows for plug play linear global attention  e g   performer  updating a target node representation   red    local message passing aggregates only immediate neighbors while global attention is a function of all nodes in a graph  in graphgps  we combine both  image by authors  https   in fact    linear attention enables   graph transformers to   scale   to dramatically larger graphs compared to typical molecules   we confirm it easily works on graphs with k nodes without any special batching  vanilla nÂ² transformers   left   materialize the full attention matrix whereas linear transformers   right   bypass this stage through various approximations without significant performance loss  image by authors   https   this is because we can rely on an expressive mpnn to encode real edge features into the node features  therefore we don t need to implicitly consider them in the transformer itself  decoupling the graph structure from the computation graph  yet still keeping it around   putting these    ingredients together    positional structural encodings  choice of mpnn and transformer layer combined into one layer  gives the blueprint for our   graphgps  general  powerful  scalable   graph transformer   this combination works surprisingly well    reaching sota in   benchmarks   we tested  in our graphgps package you can easily add any mpnn and transformer combination  and try new blends of positional and structural features   read     more   through our accessible introduction on the  blog  https   mgalkin medium com graphgps navigating graph transformers cccac   with further details in the  paper  https   arxiv org abs    
189,189,ollie_wollie_rocks,vc7m0o,[Discussion] Is data cleaning one of your pain points?,"We just open-sourced the alpha version of our data cleaning tool: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)

Looking for beta testers who would be willing to test and provide feedback!

Please send me any questions/feedback or feel free to join our slack: [https://www.mage.ai/chat](https://www.mage.ai/chat)

Demo video: [https://youtu.be/cRib1zOaqWs](https://youtu.be/cRib1zOaqWs)

Thanks for the consideration!",6,14,2022-06-14 22:07:59, discussion  is data cleaning one of your pain points ,we just open sourced the alpha version of our data cleaning tool   https looking for beta testers who would be willing to test and provide feedback please send me any questions feedback or feel free to join our slack   https demo video   https thanks for the consideration 
190,190,Half-of,vcttv8,[D] Layman query: Is on-site power consumption an indicator of an AI labs compute resources?,"Assumption: If monthly expenses were a reasonable indicator of who the ""leading AI labs"" are, could their monthly electricity bill also be a useful indicator? Or do these labs rely on off-site computing services/resources?

If not, please explain for non-specialists in the audience. Thanks appreciate it.",4,0,2022-06-15 18:22:45, d  layman query  is on site power consumption an indicator of an ai labs compute resources ,assumption  if monthly expenses were a reasonable indicator of who the leading ai labs are  could their monthly electricity bill also be a useful indicator  or do these labs rely on off site computing services resources if not  please explain for non specialists in the audience  thanks appreciate it 
191,191,Electronic-Field4636,vcbhzu,[D] How to handle macro factors in forecasting with ML models?,"For a last mile logistics company having accurate forecasts is essential to managing supply and demand and ensuring a positive customer experience, but it was challenging to factor in hard to measure macroeconomic effects. My team at DoorDash was able to solve this problem by using causal inference and I have put together this blog post with 2 case studies. One case study is about measuring how IRS refunds affect order volumes and the other case study is about measuring the impact of daylight savings on different regions' demand. 

Check out [the article](https://doordash.engineering/2022/06/14/leveraging-causal-inference-to-generate-accurate-forecasts/) to get the details and let me know what you think about my method and methodologies.",2,6,2022-06-15 01:01:46, d  how to handle macro factors in forecasting with ml models ,for a last mile logistics company having accurate forecasts is essential to managing supply and demand and ensuring a positive customer experience  but it was challenging to factor in hard to measure macroeconomic effects  my team at doordash was able to solve this problem by using causal inference and i have put together this blog post with  case studies  one case study is about measuring how irs refunds affect order volumes and the other case study is about measuring the impact of daylight savings on different regions  demand  check out  the article  https   doordash engineering    leveraging causal inference to generate accurate forecasts   to get the details and let me know what you think about my method and methodologies 
192,192,scan33scan33,vbh2vx,[D] AMA: I left Google AI after 3 years.,"During the 3 years, I developed love-hate relationship of the place. Some of my coworkers and I left eventually for more applied ML job, and all of us felt way happier so far.

EDIT1 (6/13/2022, 4pm): I need to go to Cupertino now. I will keep replying this evening or tomorrow.

EDIT2 (6/16/2022 8am): Thanks everyone's support. Feel free to keep asking questions. I will reply during my free time on Reddit.",444,712,2022-06-13 22:40:27, d  ama  i left google ai after  years ,during the  years  i developed love hate relationship of the place  some of my coworkers and i left eventually for more applied ml job  and all of us felt way happier so far edit  edit     am   thanks everyone s support  feel free to keep asking questions  i will reply during my free time on reddit 
193,193,papajan18,vc9hup,[R] Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model,"Link to paper: https://www.biorxiv.org/content/10.1101/2022.06.08.495348v1

Tweet thread summarizing paper: https://twitter.com/samnastase/status/1536463454051217408

Abstract: Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networksâ€”based on the Transformer architectureâ€”has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated ""transformations"" (alongside the more commonly studied ""embeddings"") at each layer to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized ""attention heads"" and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension.",3,8,2022-06-14 23:31:36, r  reconstructing the cascade of language processing in the brain using the internal computations of a transformer based language model,link to paper  https tweet thread summarizing paper  https abstract  piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them  how does the brain construct this kind of rich  contextual meaning from natural language  recently  a new class of artificial neural networks based on the transformer architecture has revolutionized the field of language modeling  transformers integrate information across words via multiple layers of structured circuit computations  forming increasingly contextualized representations of linguistic content  in this paper  we deconstruct these circuit computations and analyze the associated transformations  alongside the more commonly studied embeddings  at each layer to provide a fine grained window onto linguistic computations in the human brain  using functional mri data acquired while participants listened to naturalistic spoken stories  we find that these transformations capture a hierarchy of linguistic computations across cortex  with transformations at later layers in the model mapping onto higher level language areas in the brain  we then decompose these transformations into individual  functionally specialized attention heads and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions  these heads fall along gradients corresponding to different layers  contextual distances  and syntactic dependencies in a low dimensional cortical space  our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension 
194,194,InfamousPancakes,vccr3l,[R] Text to 3D characters + expression editing + pose generation,"Based on similar works [CLIPActor](https://arxiv.org/abs/2206.04382) and [AvatarCLIP](https://hongfz16.github.io/projects/AvatarCLIP.html) the codebase implements similar pipeline using mesh and differentiable rasterization to provide a speedup allowing for ~10 min character generation on a weak Google Colab GPU

https://twitter.com/multimodalart/status/1536608371570245632?s=20&t=Av8hJr43cvCF_HpJIz8J8g

Link to code: [Github](https://github.com/NasirKhalid24/CLIPMesh-SMPLX)",0,4,2022-06-15 01:57:56, r  text to d characters   expression editing   pose generation,based on similar works  clipactor  https https link to code   github  https   github com nasirkhalid clipmesh smplx 
195,195,doktoroso,vclkhy,[D] How to combine imaging data with categorical data for 3dconvnet,"I have put together a 3dconvnet for classification of head ct scans in Python using tensorflow. Mid 60% accuracy is best I have been able to achieve using imaging alone. The outcome variables are binary. I want to add patient demographic variables, and other variables of interest (some categorical some continuous) to the test/training data (images) to improve the accuracy.  

What is the best way to combine that data prior to training?  At what point is that data best concatenated?


Thanks!",1,0,2022-06-15 09:25:57, d  how to combine imaging data with categorical data for dconvnet,i have put together a dconvnet for classification of head ct scans in python using tensorflow  mid   accuracy is best i have been able to achieve using imaging alone  the outcome variables are binary  i want to add patient demographic variables  and other variables of interest  some categorical some continuous  to the test training data  images  to improve the accuracy   what is the best way to combine that data prior to training   at what point is that data best concatenated thanks 
196,196,nucLeaRStarcraft,vc01ui,[P] Pool Resources - Train multiple pytorch neural networks on multiple devices in parallel,"Hi, I made a small library that tries to generalize `Pool(n_cores).map(seq, fn)` in python multiprocessing stdlib. The main idea is to have a generalization `Pool(n_resources).map(seq, fn)` where `n_resources` can be any sort of resource (i.e. `torch.device`) and seq can be any sort of sequence (i.e. `nn.Modules`).

https://gitlab.com/mihaicristianpirvu/pool-resources

Here's a [small example](https://gitlab.com/mihaicristianpirvu/pool-resources/-/blob/master/examples/main_mnist.py) to train `n > m` mnist networks on `m` devices

```
python main_mnist.py <m devices> <n networks>
```

Currently, it only supports torch devices (via `pool_resources.TorchDevice(x: tr.device)`), however I plan to expand it to cores (start new processes) if anything else comes to mind (for example, how would i put two different keras networks on two gpus in the same process?)",0,8,2022-06-14 15:33:09, p  pool resources   train multiple pytorch neural networks on multiple devices in parallel,hi  i made a small library that tries to generalize  pool n_cores  map seq  fn   in python multiprocessing stdlib  the main idea is to have a generalization  pool n_resources  map seq  fn   where  n_resources  can be any sort of resource  i e   torch device   and seq can be any sort of sequence  i e   nn modules   https here s a  small example  https    python main_mnist py     currently  it only supports torch devices  via  pool_resources torchdevice x  tr device     however i plan to expand it to cores  start new processes  if anything else comes to mind  for example  how would i put two different keras networks on two gpus in the same process  
197,197,PK_thundr,vbst03,[R] Wav2Vec with fMRI: Towards realistic model of speech processing in the brain with self-supervised learning,,2,22,2022-06-14 07:48:32, r  wavvec with fmri  towards realistic model of speech processing in the brain with self supervised learning,nan
198,198,AmirRosenfeld,vbvwac,[D] Downloading ActivityNet,"ActivityNet is a commonly used benchmark in video action recognition. However, it is a nightmare to download. The official website provides a 7-day link to a google-drive folder, but the download quota for the user is often exceeded, and if it doesn't ,the download fails from my experience. The alternative baidu download is also no picnic. I've been trying to get it for weeks, unsucessfully.   
Does anyone have a copy they can put on AcademicTorrents or alternative location, of course including the proper readme with license, etc?   
This will make things so much easier for anyone trying to get started with the dataset.",0,7,2022-06-14 10:42:57, d  downloading activitynet,activitynet is a commonly used benchmark in video action recognition  however  it is a nightmare to download  the official website provides a  day link to a google drive folder  but the download quota for the user is often exceeded  and if it doesn t  the download fails from my experience  the alternative baidu download is also no picnic  i ve been trying to get it for weeks  unsucessfully    does anyone have a copy they can put on academictorrents or alternative location  of course including the proper readme with license  etc    this will make things so much easier for anyone trying to get started with the dataset 
199,199,SieunPark,vbg9y8,[D] What do you think about these experiments on the HUGE effect of learning rate on overfitting?,"I was playing with the CIFAR10 dataset based on the baseline code of [https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar), but I was surprised to see a strangely large decrease in the validation performance from using a smaller learning rate.

All the experiments below use

* ResNet18 model with [CIFAR10 head](https://github.com/kuangliu/pytorch-cifar/blob/49b7aa97b0c12fe0d4054e670403a16b6b834ddd/models/resnet.py#L78)
* SGD with momentum=0.9
* 4-pixel random translation/horizontal flip as [data augmentation](https://github.com/kuangliu/pytorch-cifar/blob/49b7aa97b0c12fe0d4054e670403a16b6b834ddd/main.py#L30)
* training for 200 epochs with cosine annealing to 0.
* More detail can be found in [https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar) or the [actual personal repo](https://github.com/krenerd/awesome-modular-pytorch-lightning) used for running experiments.

The only difference with the original code is that 1) drop-out of p=0.2 is added and 2) batch size and learning rate. Note that the original code uses batch\_size=128 and lr=0.1 by default and achieves 93.02% accuracy.

&#x200B;

1. In the first experiment, I train the two models using batch\_size=64 and lr=0.1/0.001. The dark-green run is trained using lr=0.1 and the light-green run is trained using 0.001. Despite the training loss of both runs converging to 0, there was a surprisingly huge gap in validation loss between the two runs.

https://preview.redd.it/v60ufzl3re591.png?width=1138&format=png&auto=webp&s=280fe1ec065ea88aeab094d16e215f6080004cb5

2. Curious to see when the performance changed, I tested 9 learning rates from \[0.1, 0.001\] in a logarithmic scale. Precisely, lr=\[0.001, 0.001 \* 10\*\*(1/4), 0.001 \* 10\*\*(2/4), 0.001 \* 10\*\*(3/4), 0.01, ... 0.1\] were tested. The figures below show the validation accuracy and train loss of the runs. The dark green line is the experiment with lr=0.1 which achieves an accuracy of 95.17%.

To summarize the results,

* All runs almost perfectly fit the training dataset.
* lr=0.1\~0.01 showed a very similar final validation performance at \~ 95.1% accuracy.
* The validation performance degrades straightly after using learning rates smaller than 0.01. Smaller learning rates always had worse validation accuracy.
* Interestingly, there were inconsistencies in terms of the order of training convergence speed: 0.1(dark green)<0.01(orange)<0.001(light blue)<0.00562(brown)<0.001777(dark blue)<0.00316(apricot)

https://preview.redd.it/fi7qzfa5re591.png?width=2720&format=png&auto=webp&s=bd24d940fb715ac96f5a8354cf1397efa7cdacb9

https://preview.redd.it/o0ls49x5re591.png?width=2728&format=png&auto=webp&s=2e87b521a397b914e4d511ad77a0f826a4f1a176

3. Based on the principles of the linear scaling rule between learning rate and batch size, I also tried exponentially incrementing the batch size from lr=0.01, which is similar to decreasing the learning rate. Similarly, I tested batch sizes from 64 to 64 \* 10 = 640: \[64, 64 \* 10\*\*(1/4), 64 \* 10\*\*(2/4), 64 \* 10\*\*(3/4), 640\].

Interestingly,

* All runs almost perfectly fit the training dataset.
* The trends are almost exactly the same with the learning rate scaling experiment!!! We can even see that purple run(lr=0.01, bs=640) and light blue run(lr=0.001, bs=64) shows almost same validation performance. The linear scaling rule is amazing ðŸ˜Ž
* Larger batch sizes from 64 always had worse validation accuracy.
* Despite the different number of optimization steps, I did keep the epochs consistent at 200. Though this doesn't seem to be an issue.

&#x200B;

https://preview.redd.it/agzmwombze591.png?width=1823&format=png&auto=webp&s=f6ddb2d929e09b267f32be508714f204562a9b7b

https://preview.redd.it/9pug86gize591.png?width=1823&format=png&auto=webp&s=c79779d884e46a7ecbc627139dce99643dab6277

Though this has some limitations, such as

* CIFAR10 is a toy dataset, I didn't have the budget to test on ImageNet ðŸ¥²
* What does Cosine annealing do? Different learning rate schedules such as 1-cycle or constant lr might have different results. Though, note that the batch size experiment shows consistent results.

I was surprised to see how significantly the validation performance can change despite similar training performance and fixed explicit regularization-related hyperparameters.

* I am curious to know why small learning rates induce overfitting over large learning rates. 
* Also, I think this has many implications. For example, if some method is more sensitive to a large learning rate, that method might have some implicit disadvantages(I can confidently say so at least on the CIFAR-10 dataset). What do you think?
* It was also interesting that lr=\[0.1, 0.01\] didn't affect much, but lr=\[0.01, 0.001\] had almost systematic effects on validation performance. The fact that the performance is somewhat systematic is interesting because we might be able to understand bad/good optimal. Do you think there are experiments to extend these results so I could discover the secrets(?) of learning rates and stochastic optimization?
* I didn't look at smaller learning rates because training loss started to not converge after training for 200 epochs from learning rates smaller than lr=0.001(figure below)

https://preview.redd.it/jtclrf3n2f591.png?width=432&format=png&auto=webp&s=51378ebc4da35151f48c1d7152d04b6ca2d59fb4

Has similar work been published? As an undergraduate who never conducted proper research before, I'm very curious to see how people interpreted similar results on unintuitive behaviors of NNs. BTW, Sorry for the bad English if there was any.",24,45,2022-06-13 22:05:03, d  what do you think about these experiments on the huge effect of learning rate on overfitting ,i was playing with the cifar dataset based on the baseline code of  https all the experiments below use  resnet model with  cifar head  https   sgd with momentum     pixel random translation horizontal flip as  data augmentation  https   training for  epochs with cosine annealing to    more detail can be found in  https the only difference with the original code is that   drop out of p   is added and   batch size and learning rate  note that the original code uses batch _size  and lr   by default and achieves    accuracy   xb   in the first experiment  i train the two models using batch _size  and lr      the dark green run is trained using lr   and the light green run is trained using    despite the training loss of both runs converging to   there was a surprisingly huge gap in validation loss between the two runs https   curious to see when the performance changed  i tested  learning rates from          in a logarithmic scale  precisely  lr                                                           were tested  the figures below show the validation accuracy and train loss of the runs  the dark green line is the experiment with lr   which achieves an accuracy of    to summarize the results   all runs almost perfectly fit the training dataset   lr      showed a very similar final validation performance at       accuracy   the validation performance degrades straightly after using learning rates smaller than    smaller learning rates always had worse validation accuracy   interestingly  there were inconsistencies in terms of the order of training convergence speed    dark green    orange    light blue    brown    dark blue    apricot https https   based on the principles of the linear scaling rule between learning rate and batch size  i also tried exponentially incrementing the batch size from lr    which is similar to decreasing the learning rate  similarly  i tested batch sizes from  to                                                        interestingly   all runs almost perfectly fit the training dataset   the trends are almost exactly the same with the learning rate scaling experiment    we can even see that purple run lr    bs   and light blue run lr    bs   shows almost same validation performance  the linear scaling rule is amazing    larger batch sizes from  always had worse validation accuracy   despite the different number of optimization steps  i did keep the epochs consistent at   though this doesn t seem to be an issue   xb https https though this has some limitations  such as  cifar is a toy dataset  i didn t have the budget to test on imagenet    what does cosine annealing do  different learning rate schedules such as  cycle or constant lr might have different results  though  note that the batch size experiment shows consistent results i was surprised to see how significantly the validation performance can change despite similar training performance and fixed explicit regularization related hyperparameters   i am curious to know why small learning rates induce overfitting over large learning rates    also  i think this has many implications  for example  if some method is more sensitive to a large learning rate  that method might have some implicit disadvantages i can confidently say so at least on the cifar  dataset   what do you think   it was also interesting that lr          didn t affect much  but lr          had almost systematic effects on validation performance  the fact that the performance is somewhat systematic is interesting because we might be able to understand bad good optimal  do you think there are experiments to extend these results so i could discover the secrets    of learning rates and stochastic optimization   i didn t look at smaller learning rates because training loss started to not converge after training for  epochs from learning rates smaller than lr   figure below https has similar work been published  as an undergraduate who never conducted proper research before  i m very curious to see how people interpreted similar results on unintuitive behaviors of nns  btw  sorry for the bad english if there was any 
200,200,radome9,vb1oj4,[N] Google engineer put on leave after saying AI chatbot has become sentient,,290,347,2022-06-13 07:55:39, n  google engineer put on leave after saying ai chatbot has become sentient,nan
201,201,DaveCoDev,vbg68p,[P] mlfeed.tech: I built a website to filter Twitter for quality ML content,"A few years ago, I started using Twitter to follow some ML people to try and keep up with the latest cool things that were going on in the field. I realized two things:  

1. There is a lot of great content posted regularly. 
2. But man is it surrounded by a lot of not so useful stuff (politics, ads, hot takes, etc) that I didnâ€™t want to sift through to get to the useful content. 

So with that I decided to build a classifier to filter out the most â€œrelevantâ€ tweets: ones that showcase papers, blogs about new methods, YouTube tutorials, and Github repos. Using that classifier, I had a bot retweet these ([@mlfeedtech](https://twitter.com/mlfeedtech)) and built a web UI ([mlfeed.tech](https://mlfeed.tech/)) to showcase them.  

Fast forward to a few days ago, I released the latest iteration of mlfeed with a brand new UI! It has filters to make it easier to find different types of content and initial work on the first iteration of [trends](https://mlfeed.tech/trends)! Trends is my longer-term goal to make it easier to find and keep up with topics going on in the field. You can currently check out the relative weight of different topics/methods over time and try an interactive clustering of the latest papers! Currently it uses data from Papers with Code to create visualizations of topics (the same topics you would see on their Methods page).  

Finally, Iâ€™m not really looking to make money off this. If anything, just enough to cover server costs. It's just genuinely something I find useful working as a data scientist and find myself checking regularly. If you do like it, consider following on [Twitter](https://twitter.com/mlfeedtech) or bookmarking [mlfeed.tech](https://mlfeed.tech/). There are at least a hundred different ideas I have on about to expand on this, but Iâ€™d love to also know your feedback and thoughts! ðŸ˜Š 

**Pro tips:**  

* The Content --> YouTube filter is awesome if youâ€™re looking for something to watch 
* The trending feed is equivalent to what is retweeted by [@mlfeedtech](https://twitter.com/mlfeedtech), you can use that profile page as your mlfeed if you prefer. 
* The paper visualization is a bit hard to use at first (improving it is on my radar). What I like to do is hover around until I start to recognize paper topics I am interested in, then zoom in and look more closely at the papers. Also, generally topics should be in the same places over time (but it will shift as the dataset is constantly updated!). 

[mlfeed.tech](https://mlfeed.tech/) is the website link!",3,20,2022-06-13 22:00:28, p  mlfeed tech  i built a website to filter twitter for quality ml content,a few years ago  i started using twitter to follow some ml people to try and keep up with the latest cool things that were going on in the field  i realized two things     there is a lot of great content posted regularly    but man is it surrounded by a lot of not so useful stuff  politics  ads  hot takes  etc  that i didn t want to sift through to get to the useful content  so with that i decided to build a classifier to filter out the most  relevant  tweets  ones that showcase papers  blogs about new methods  youtube tutorials  and github repos  using that classifier  i had a bot retweet these    mlfeedtech  https fast forward to a few days ago  i released the latest iteration of mlfeed with a brand new ui  it has filters to make it easier to find different types of content and initial work on the first iteration of  trends  https finally  i m not really looking to make money off this  if anything  just enough to cover server costs  it s just genuinely something i find useful working as a data scientist and find myself checking regularly  if you do like it  consider following on  twitter  https   pro tips       the content     youtube filter is awesome if you re looking for something to watch   the trending feed is equivalent to what is retweeted by   mlfeedtech  https   the paper visualization is a bit hard to use at first  improving it is on my radar   what i like to do is hover around until i start to recognize paper topics i am interested in  then zoom in and look more closely at the papers  also  generally topics should be in the same places over time  but it will shift as the dataset is constantly updated     mlfeed tech  https   mlfeed tech   is the website link 
202,202,ReginaldIII,vbf5gx,[D] Does anyone have a copy of the FFHQ 1024 scale images (90GB) ? and or a copy of the FFHQ Wild images (900GB) ?,"As the title suggests, I am putting out a call for anyone who has a copy of the FFHQ dataset who would be able to allow me to download it from them so it can be hosted properly and made truly public.

The FFHQ dataset https://github.com/NVlabs/ffhq-dataset is a high quality, high resolution, and extremely well curated dataset that is used in many recent SOTA GAN papers and also has applications in many other areas.

FFHQ is 70k aligned images of human faces organized into a 128x128 thumbnails dataset (3GB), a 1024x1024 high res dataset (90GB), and a raw unaligned wilds dataset (900GB). 

Do you have the 1024 or Wilds dataset in an s3 bucket? On Google Cloud Storage buckets? Exposed on a Globus endpoint? Kicking around on a lowly SFTP server? 

Can you safely expose a share for me to download? I'm happy to trickle download at an agreed speed if bandwidth is a concern!

If not, would you be willing to write it into my secure storage if I provide you with credentials which can safely be revoked after upload?

---

The dataset is officially hosted on Google Drive... And the provided python script to download the dataset has several major and breaking flaws with it.

- The 256MB metadata json file cannot be downloaded by the python script because Google Drive have added a manual ""click this link to confirm you know we can't virus scan large files"" check which prevents the script from downloading the json. Manually downloading the json allows you to proceed to downloading images.

- The python script defaults to using many threads, with no rate-limiting. After approximately 1000 images Google Drive will outright block you for a day. They block a wide IP range around your IP from having access to any files in the dataset, preventing distributed download from within the network of a large organization. 

- Even when limiting to one thread and modifying the script to only download one image at a time with a 10 second gap between one request finishing and the next starting you still get throttled by Google where all requests fail for several hours before starting again.

- If even a few people around the world are attempting to download dataset at the same time Google Drive will throttle the downloads on their side, saying that too many people are trying to access the files this month.

---

Currently it's going to take me more than a month to download the 90GB 1024 images. The Wilds images are not even feasible to download at the moment. 

This dataset is relevant to SOTA research and still being used actively in publications. Let's get a copy of this dataset hosted publicly so the whole community can benefit from it.

---

Edit: I've managed to get the 90 GB 1024 scale images downloaded using the monolithic zip file on the Google Drive. This is the first time I've successfully managed to download this zip.

The monolithic zips for the Wilds and the TFRecords fail with ""Download quota exceeded for this file, so you can't download it at present.""

I wonder if me downloading the 1024 zip was enough to mean the next person hit the quota exceeded block...

Edit edit: I was able to download 380 GB of the 900 GB Wilds monolithic zip, but the download failed and now it refuses to restart.

---

Edit of the edit of the edit: https://app.activeloop.ai/activeloop/ffhq 

Activeloop are now hosting a full copy of FFHQ including the wilds, 1024, and 128 images! I am not affiliated with Activeloop but I have been using their library for my work and I've had a really good experience talking to them on Github.

Data is lazy loaded on demand allowing you to explore the dataset:

    import hub
    ds = hub.load('hub://activeloop/ffhq')

    import matplotlib.pyplot as plt
    plt.imshow(ds.images_wild.image[0])
    plt.show()

You can download the data to local storage (this will be very large ~1TB!):

    hub.deepcopy('hub://activeloop/ffhq', './ffhq')

Or select a specific subset of the dataset to download locally:

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-128', tensors=['images_128/image'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-1024', tensors=['images_1024/image', 'images_1024/face_landmarks'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-wild', tensors=['images_wild/image', 'images_wild/face_landmarks', 'images_wild/face_rect'])",17,17,2022-06-13 21:15:17, d  does anyone have a copy of the ffhq  scale images  gb    and or a copy of the ffhq wild images  gb   ,as the title suggests  i am putting out a call for anyone who has a copy of the ffhq dataset who would be able to allow me to download it from them so it can be hosted properly and made truly public the ffhq dataset https ffhq is k aligned images of human faces organized into a x thumbnails dataset  gb   a x high res dataset  gb   and a raw unaligned wilds dataset  gb   do you have the  or wilds dataset in an s bucket  on google cloud storage buckets  exposed on a globus endpoint  kicking around on a lowly sftp server  can you safely expose a share for me to download  i m happy to trickle download at an agreed speed if bandwidth is a concern if not  would you be willing to write it into my secure storage if i provide you with credentials which can safely be revoked after upload    the dataset is officially hosted on google drive    and the provided python script to download the dataset has several major and breaking flaws with it   the mb metadata json file cannot be downloaded by the python script because google drive have added a manual click this link to confirm you know we can t virus scan large files check which prevents the script from downloading the json  manually downloading the json allows you to proceed to downloading images   the python script defaults to using many threads  with no rate limiting  after approximately  images google drive will outright block you for a day  they block a wide ip range around your ip from having access to any files in the dataset  preventing distributed download from within the network of a large organization    even when limiting to one thread and modifying the script to only download one image at a time with a  second gap between one request finishing and the next starting you still get throttled by google where all requests fail for several hours before starting again   if even a few people around the world are attempting to download dataset at the same time google drive will throttle the downloads on their side  saying that too many people are trying to access the files this month    currently it s going to take me more than a month to download the gb  images  the wilds images are not even feasible to download at the moment  this dataset is relevant to sota research and still being used actively in publications  let s get a copy of this dataset hosted publicly so the whole community can benefit from it    edit  i ve managed to get the  gb  scale images downloaded using the monolithic zip file on the google drive  this is the first time i ve successfully managed to download this zip the monolithic zips for the wilds and the tfrecords fail with download quota exceeded for this file  so you can t download it at present i wonder if me downloading the  zip was enough to mean the next person hit the quota exceeded block   edit edit  i was able to download  gb of the  gb wilds monolithic zip  but the download failed and now it refuses to restart    edit of the edit of the edit  https activeloop are now hosting a full copy of ffhq including the wilds    and  images  i am not affiliated with activeloop but i have been using their library for my work and i ve had a really good experience talking to them on github data is lazy loaded on demand allowing you to explore the dataset     import hub    ds   hub load  hub     import matplotlib pyplot as plt    plt imshow ds images_wild image       plt show  you can download the data to local storage  this will be very large  tb       hub deepcopy  hub or select a specific subset of the dataset to download locally     hub deepcopy  hub     hub deepcopy  hub     hub deepcopy  hub   activeloop ffhq      ffhq wild   tensors   images_wild image    images_wild face_landmarks    images_wild face_rect   
203,203,happybirthday290,vapbkh,[P] The easiest way to process and tag video data - update,,32,1078,2022-06-12 21:45:32, p  the easiest way to process and tag video data   update,nan
204,204,rwgardner,vbgtxi,[R] Reconnaissance Blind Chess - Join the NeurIPS Competition!,"Create a bot for the [NeurIPS 2022 competition in Reconnaissance Blind Chess](https://rbc.jhuapl.edu/)!

Reconnaissance Blind Chess is a chess variant designed for new research in artificial intelligence. RBC includes imperfect information, long-term strategy, explicit observations, and almost no common knowledge. These features appear in real-world scenarios, and challenge even state of the art algorithms including those used to create super-human bots in chess, Go, and poker, for example. Each player of RBC controls traditional chess pieces, but cannot directly see the locations of her opponent's pieces. Rather, she learns partial information each turn by privately sensing a 3x3 area of the board. RBC's foundation in traditional chess makes it familiar and entertaining to human players, too!

There is no cost to enter this tournament. Winners will receive a small monetary prize and authors of the best AIs will be invited talk about their bots at NeurIPS, the world's largest AI conference.

Learn more, play a game of RBC yourself, and join our research community at [https://rbc.jhuapl.edu](https://rbc.jhuapl.edu/) !

&#x200B;

https://preview.redd.it/yr7k6gz66f591.png?width=150&format=png&auto=webp&s=81d7cababf139f4fa0350c206a9024a45017bfd4

Organized by:

Johns Hopkins University Applied Physics Laboratory

with

Ashley J. Llorens (Microsoft Research)

Todd W. Neller (Gettysburg College)

Raman Arora (Johns Hopkins University)

Bo Li (University of Illinois)

Mykel J. Kochenderfer (Stanford University)",1,10,2022-06-13 22:29:59, r  reconnaissance blind chess   join the neurips competition ,create a bot for the  neurips  competition in reconnaissance blind chess  https reconnaissance blind chess is a chess variant designed for new research in artificial intelligence  rbc includes imperfect information  long term strategy  explicit observations  and almost no common knowledge  these features appear in real world scenarios  and challenge even state of the art algorithms including those used to create super human bots in chess  go  and poker  for example  each player of rbc controls traditional chess pieces  but cannot directly see the locations of her opponent s pieces  rather  she learns partial information each turn by privately sensing a x area of the board  rbc s foundation in traditional chess makes it familiar and entertaining to human players  too there is no cost to enter this tournament  winners will receive a small monetary prize and authors of the best ais will be invited talk about their bots at neurips  the world s largest ai conference learn more  play a game of rbc yourself  and join our research community at  https   xb https organized by johns hopkins university applied physics laboratorywithashley j  llorens  microsoft research todd w  neller  gettysburg college raman arora  johns hopkins university bo li  university of illinois mykel j  kochenderfer  stanford university 
205,205,RandomProjections,vbp07v,[D] Publishing a huge amount of paper is a symptom of the publish-or-perish disease. Stop doing it.,"I feel like the incentives in academia has gotten to a really perverse stage and having a massive trove of ML papers being published (especially within short period of one another) is just one of its symptoms.

Here are some of my takes on the ""large amount of paper"" phenomena.

1. The motivation for these paper are extremely weak and often completely detached from any real problems. They are more math than ML.
2. I cannot see why the author would be even interested in these kinds of problems. There doesn't seem to be any longer term goal that the paper is moving towards. 
3. Often times the lack of novelty is dressed up in huge amount of calculations.
4. If you are publishing a huge amount of papers, is it possible that your problem is actually quite easy or your results are irrelevant?
5. For the academic supervisors: are you possibly exploiting and overworking your graduate students from poorer countries to boost your citation counts?

Thoughts?",4,2,2022-06-14 04:31:42, d  publishing a huge amount of paper is a symptom of the publish or perish disease  stop doing it ,i feel like the incentives in academia has gotten to a really perverse stage and having a massive trove of ml papers being published  especially within short period of one another  is just one of its symptoms here are some of my takes on the large amount of paper phenomena   the motivation for these paper are extremely weak and often completely detached from any real problems  they are more math than ml   i cannot see why the author would be even interested in these kinds of problems  there doesn t seem to be any longer term goal that the paper is moving towards    often times the lack of novelty is dressed up in huge amount of calculations   if you are publishing a huge amount of papers  is it possible that your problem is actually quite easy or your results are irrelevant   for the academic supervisors  are you possibly exploiting and overworking your graduate students from poorer countries to boost your citation counts thoughts 
206,206,Human-Career-9962,vb9bra,[R] GenDR: A Generalized Differentiable Renderer w/ animated video (CVPR 2022),"I have made an animated video ([youtu.be/p-ZCcUWzriE](https://youtu.be/p-ZCcUWzriE)) for our CVPR 2022 paper ([https://arxiv.org/pdf/2204.13845.pdf](https://arxiv.org/pdf/2204.13845.pdf)).

Check it out if you are interested. I have made the video using 3b1b's manim library ([https://github.com/ManimCommunity/manim](https://github.com/ManimCommunity/manim)).

Feedback is always very welcome!",3,10,2022-06-13 16:18:12, r  gendr  a generalized differentiable renderer w  animated video  cvpr  ,i have made an animated video   youtu be p zccuwzrie  https check it out if you are interested  i have made the video using bb s manim library   https feedback is always very welcome 
207,207,Apprehensive-Wheel18,vb5lv6,[D] Deep Learning Framework for C++.,"I have been working in the field of ML/DL for almost more than 3 years now. I know Python is the goto language for ML/DL thanks to the frameworks like TensorFlow, Pytorch and most recently JAX/TRAX. And all of these are written in C++. But they don't provide a C++ API only Python, except for the PyTorch recently provided C++ API but they don't recommend it for production.  Recently I have been trying to find a ML/DL library for C++ but I haven't found one. Could anyone here list some production grade C++ ML/DL libraries. 

I also wonder which language big companies (e.g. Tesla, Google or Facebook) use in production (not for prototyping) for ML/DL because every problem they have is a problem at scale so they would definitely not be using Python. So anyone working in big tech companies and writing code that goes into production please provide some insight what language and framework you guys use.",29,18,2022-06-13 11:53:48, d  deep learning framework for c   ,i have been working in the field of ml dl for almost more than  years now  i know python is the goto language for ml dl thanks to the frameworks like tensorflow  pytorch and most recently jax trax  and all of these are written in c    but they don t provide a c   api only python  except for the pytorch recently provided c   api but they don t recommend it for production   recently i have been trying to find a ml dl library for c   but i haven t found one  could anyone here list some production grade c   ml dl libraries  i also wonder which language big companies  e g  tesla  google or facebook  use in production  not for prototyping  for ml dl because every problem they have is a problem at scale so they would definitely not be using python  so anyone working in big tech companies and writing code that goes into production please provide some insight what language and framework you guys use 
208,208,Singularian2501,vavffv,[R] Memorizing Transformers - Google 2022,"Paper: [https://arxiv.org/abs/2203.08913](https://arxiv.org/abs/2203.08913)

Youtube Video from the author: [https://www.youtube.com/watch?v=5AoOpFFjW28](https://www.youtube.com/watch?v=5AoOpFFjW28)

Github: [https://github.com/lucidrains/memorizing-transformers-pytorch](https://github.com/lucidrains/memorizing-transformers-pytorch)

Abstract:

>Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. **We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately**. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.       

https://preview.redd.it/8a7c50rv49591.jpg?width=919&format=pjpg&auto=webp&s=60bf603d45840c9388d35b5b6cdfd0f95da56b36

https://preview.redd.it/y8h8aw1w49591.jpg?width=1014&format=pjpg&auto=webp&s=e607fcc2f620655a5cebe7251148c247ac3e3233

https://preview.redd.it/fouu60dw49591.jpg?width=901&format=pjpg&auto=webp&s=f6fd0e167608ff7d50d8b068949aa58ed4126256",0,34,2022-06-13 02:33:23, r  memorizing transformers   google ,paper   https youtube video from the author   https github   https abstract  language models typically need to be trained or finetuned in order to acquire new knowledge  which involves updating their weights    we instead envision language models that can simply read and memorize new data at inference time  thus acquiring new knowledge immediately    in this work  we extend language models with the ability to memorize the internal representations of past inputs  we demonstrate that an approximate knn lookup into a non differentiable memory of recent  key  value  pairs improves language modeling across various benchmarks and tasks  including generic webtext  c   math papers  arxiv   books  pg    code  github   as well as formal theorems  isabelle   we show that the performance steadily improves when we increase the size of memory up to k tokens  on benchmarks including code and mathematics  we find that the model is capable of making use of newly defined functions and theorems during test time        https https https   preview redd it fouudw jpg width  format pjpg auto webp s ffdeffddbaaed
209,209,dome271,valq9u,[P] Explanation Video about Diffusion Models,"Hey there,

Since Diffusion Models are becoming super popular especially for Image Generation, I decided to make a video about them, trying to convey the fundamental idea in an easy manner + deriving the complete maths.

These are the papers I covered:

* [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/pdf/1503.03585.pdf)
* [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)
* [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2102.09672.pdf)
* [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)

Here is the link: [https://www.youtube.com/watch?v=HoKDTa5jHvg](https://www.youtube.com/watch?v=HoKDTa5jHvg)

Let me know what you think.

https://preview.redd.it/8a8ma5i7s6591.jpg?width=1920&format=pjpg&auto=webp&s=19d351c40a2703d08ba8671b246f3e0c27ad3c85",8,112,2022-06-12 18:43:19, p  explanation video about diffusion models,hey there since diffusion models are becoming super popular especially for image generation  i decided to make a video about them  trying to convey the fundamental idea in an easy manner   deriving the complete maths these are the papers i covered    deep unsupervised learning using nonequilibrium thermodynamics  https    denoising diffusion probabilistic models  https    improved denoising diffusion probabilistic models  https    diffusion models beat gans on image synthesis  https here is the link   https let me know what you think https   preview redd it amais jpg width  format pjpg auto webp s dcadbabfecadc
210,210,iamaliver,vb6gi0,"[Discussion] YOLOv5 training questions, specificaly re-training best practices","Hi,

I am currently in the process of training a Yolo5 (image ml) network. \[[https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)\]

I had a few questions to best optimize the training for classification.

Current Procedure:

1. Manual tag a few pictures -- maybe 50 -- small sample size. (manual is time-consuming)
2. Train a YOLO network, for about 100 epochs.
3. Use that to re-label my dataset. And then adjust by hand.  

   1. Adjustment by hand can include the following: fix-mislabels, adjust the bounding box size
   2. Adding additional labeled pictures, say expanding my dataset to 100 pictures \[may or may not include original 50, see questions below\]
4. Take the new-labels and train YOLO network -- repeating Step 2 and 3 a few times.

The question I have is in the Step 3/4 when I am trying to re-train my YOLO5 with corrections.

1. Should I start from scratch? Are there cases where I should start from scratch? Let's say I found 2-3 pictures that were mis-labeled in the initial training set?
2. Should I be adding my corrections (step 3) to my original set of pictures? Or should I do a bunch of re-labels and retrain with a completely new set?
3. Can I simply train with the new ""corrections"" set of say 50 pictures? (ideal since this would be way faster) Would this make me overfit?
4. If I identified a mislabeled picture initially and then ""fix"" this in subsequent Step 3, assuming I do not start from scratch, can I ""train away"" the mislabel? How many epochs would that take?
5. As I get better bounding boxes, I can more easily expand the number of training images, should I? This costs time (and money) if I keep using expanding my dataset.
6. What is the most important type of data? Should I be looking for the edge cases and adding more edge cases into my training set? For instance if the picture is partially obscured, a human can know for sure it is there, but it is understandable if the ML cannot detect it. Should I deliberately add more of these types to my dataset or go with the ones that's 100% for sure easy to detect?

The goal in the end is to optimize some $ / accuracy / speed. Training a network takes ages and I am unfamiliar with what types of trade-offs I am making in the various ""retraining"" scenerios. It is also not easy to tell if I am introducing additional errors / overfitting.

Any input is helpful! Understandably, all datasets are different, but I'm just looking for rule of thumb, best practices, or things like, ""only idiots would do X""!",5,3,2022-06-13 12:52:09, discussion  yolov training questions  specificaly re training best practices,hi i am currently in the process of training a yolo  image ml  network     https i had a few questions to best optimize the training for classification current procedure   manual tag a few pictures    maybe     small sample size   manual is time consuming   train a yolo network  for about  epochs   use that to re label my dataset  and then adjust by hand        adjustment by hand can include the following  fix mislabels  adjust the bounding box size     adding additional labeled pictures  say expanding my dataset to  pictures   may or may not include original   see questions below    take the new labels and train yolo network    repeating step  and  a few times the question i have is in the step   when i am trying to re train my yolo with corrections   should i start from scratch  are there cases where i should start from scratch  let s say i found   pictures that were mis labeled in the initial training set   should i be adding my corrections  step   to my original set of pictures  or should i do a bunch of re labels and retrain with a completely new set   can i simply train with the new corrections set of say  pictures   ideal since this would be way faster  would this make me overfit   if i identified a mislabeled picture initially and then fix this in subsequent step   assuming i do not start from scratch  can i train away the mislabel  how many epochs would that take   as i get better bounding boxes  i can more easily expand the number of training images  should i  this costs time  and money  if i keep using expanding my dataset   what is the most important type of data  should i be looking for the edge cases and adding more edge cases into my training set  for instance if the picture is partially obscured  a human can know for sure it is there  but it is understandable if the ml cannot detect it  should i deliberately add more of these types to my dataset or go with the ones that s   for sure easy to detect the goal in the end is to optimize some     accuracy   speed  training a network takes ages and i am unfamiliar with what types of trade offs i am making in the various retraining scenerios  it is also not easy to tell if i am introducing additional errors   overfitting any input is helpful  understandably  all datasets are different  but i m just looking for rule of thumb  best practices  or things like  only idiots would do x 
211,211,ButterCream55,vb314r,I Created A Transformer Based Chatbot [Project],"You should be able to get her to read your comments by mentioning their user: u/RyuAI22, so far she doesn't post on her own, but I plan on making some pretty cool updates to her, such as a transformer based animation engine!",32,6,2022-06-13 09:10:32,i created a transformer based chatbot  project ,you should be able to get her to read your comments by mentioning their user  u ryuai  so far she doesn t post on her own  but i plan on making some pretty cool updates to her  such as a transformer based animation engine 
212,212,scb_11,vbcyhf,[D] Running Model Training Jobs on Kubernetes,"Running training jobs on Kubernetes is it recommended?

Why?

I don't see any advantage of running training jobs on Kubernetes as long as the autoscaling works well for distributed trainings. If we don't do distributed training, is it of any use? As we can only mention resource as 1 GPU. 

Am I thinking this wrong?",7,0,2022-06-13 19:33:52, d  running model training jobs on kubernetes,running training jobs on kubernetes is it recommended why i don t see any advantage of running training jobs on kubernetes as long as the autoscaling works well for distributed trainings  if we don t do distributed training  is it of any use  as we can only mention resource as  gpu  am i thinking this wrong 
213,213,bluzkluz,vb8aer,[Discussion] what libraries are available to generate augmented (synthetic) data at a vector level?,"For instance, I have an embedding and I wish to generate a sampling of vectors similar to the query embeddings that can then be used for training a model.",3,1,2022-06-13 15:07:02, discussion  what libraries are available to generate augmented  synthetic  data at a vector level ,for instance  i have an embedding and i wish to generate a sampling of vectors similar to the query embeddings that can then be used for training a model 
214,214,juanigp,vb7mds,[D] Image domain translation as pre-processing for classification?,"Hi y'all, I want to ask for literature on the topic of using domain translation techniques as pre-processing for images to be classified by a model trained on a different dataset. 

AFAIK the typical image-to-image translation techniques (that I know) like CycleGAN are not targeted to the classification problem, image-to-image translation is the goal on itself.

Thanks in advance!",0,1,2022-06-13 14:18:11, d  image domain translation as pre processing for classification ,hi y all  i want to ask for literature on the topic of using domain translation techniques as pre processing for images to be classified by a model trained on a different dataset  afaik the typical image to image translation techniques  that i know  like cyclegan are not targeted to the classification problem  image to image translation is the goal on itself thanks in advance 
215,215,gratus907,vb6il3,[D] Train-Valid-Test split and featurizer design,"I am working on a classification problem and encountered a problem within train-test split and feature

Suppose that I designed a featurizer function f (consider this as a fingerprint for representation learning). So f(training data) is used to train classifier, and f(test data) is fed to classifier. 

If I use 'unlabeled' data from test data to design f (such as, variance-threshold dropping of fingerprints), is this data leak? There seems to be two arguments

- Any prior knowledge on test data is data leakage and must be avoided at all cost.
- Train-test split is to simulate a situation where unknown data is given to the model. Here, it might make sense as the process is essentially (see the given unknown data but not labels (because we wouldn't have labels) -> change featurizer -> train classifier -> use to predict new data) And my architecture is now somewhat semi-supervised learning.

I initially thought that if I can make sure that no label information of test data is available, that still constitutes sound train-test split, but I would like to know what more experienced people might think. Any discussion is appreciated!",2,0,2022-06-13 12:56:15, d  train valid test split and featurizer design,i am working on a classification problem and encountered a problem within train test split and featuresuppose that i designed a featurizer function f  consider this as a fingerprint for representation learning   so f training data  is used to train classifier  and f test data  is fed to classifier  if i use  unlabeled  data from test data to design f  such as  variance threshold dropping of fingerprints   is this data leak  there seems to be two arguments  any prior knowledge on test data is data leakage and must be avoided at all cost   train test split is to simulate a situation where unknown data is given to the model  here  it might make sense as the process is essentially  see the given unknown data but not labels  because we wouldn t have labels     change featurizer    train classifier    use to predict new data  and my architecture is now somewhat semi supervised learning i initially thought that if i can make sure that no label information of test data is available  that still constitutes sound train test split  but i would like to know what more experienced people might think  any discussion is appreciated 
216,216,Razcle,vaq5p8,[D] Is SGLD used much in practice?,"[Stochastic Gradient Langevin Dynamics](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf) seems like a really elegant idea. A simple way to get the benefits of posterior sampling without having to make significant changes to standard stochastic optinisation.

It seems like a powerful and simple idea but I rarely see papers that use it. Is this because it doesn't work well in practice? Because researchers are not super familiar with it? Or is it used lots and I've just not seen it?",5,8,2022-06-12 22:26:36, d  is sgld used much in practice , stochastic gradient langevin dynamics  https it seems like a powerful and simple idea but i rarely see papers that use it  is this because it doesn t work well in practice  because researchers are not super familiar with it  or is it used lots and i ve just not seen it 
217,217,kimdotcoin2222,vb7ntq,[D] How to factor covid into model,"hi guys,

I am currently working on a time series model and would like to include the covid influence as it has an impact on the problem I am trying to solve. My simple approach would be to add a binary categorical feature (covid yes/no). 

&#x200B;

I would like to try other approaches, however, unfortunately the research proves to be inconclusive as I only get results regarding covid models. So here are my specific questions:

&#x200B;

\- Are there other, better ways to do this?

\- Is there already a state of the art procedure on how to include covid?

\- Do you know any literature about this?

&#x200B;

Thanks in advance.",5,0,2022-06-13 14:21:21, d  how to factor covid into model,hi guys i am currently working on a time series model and would like to include the covid influence as it has an impact on the problem i am trying to solve  my simple approach would be to add a binary categorical feature  covid yes no     xb i would like to try other approaches  however  unfortunately the research proves to be inconclusive as i only get results regarding covid models  so here are my specific questions   xb    are there other  better ways to do this    is there already a state of the art procedure on how to include covid    do you know any literature about this   xb thanks in advance 
218,218,rlesii,va0p9u,[P] [R] Deep Learning Classifier for Sex Positions,"Hello! I build some sex position classifiers using state-of-the-art techniques in deep learning! The best results were achieved by combining three input streams: RGB, Skeleton, and Audio. The current top accuracy is 75%. This would certainly be improved with a larger dataset.

Basically, human action recognition (HAR) is applied to the adult content domain. It presents some technical difficulties, especially due to the enormous variation in camera position (the challenge is to classify actions based on a single video).

The main input stream is the RGB one (as opposed to the skeleton one) and this is mostly due to the relatively small dataset (\~44hrs). It is difficult to get an accurate pose estimation (which is a prerequisite for building robust skeleton-HAR models) for most of the videos due to the proximity of the human bodies in the frames. Hence there simply weren't enough data to include all the positions in the skeleton-based model.

The audio input stream on the other hand is only used for a handful of actions, where deriving some insight is possible.

Check it out on Github for a detailed description: [https://github.com/rlleshi/phar](https://github.com/rlleshi/phar)

Possible use-cases include:

1. Improving the recommender system
2. Automatic tag generator
3. Automatic timestamp generator (when does an action start and finish)
4. Filtering video content based on actions (positions)",92,402,2022-06-11 21:38:11, p   r  deep learning classifier for sex positions,hello  i build some sex position classifiers using state of the art techniques in deep learning  the best results were achieved by combining three input streams  rgb  skeleton  and audio  the current top accuracy is    this would certainly be improved with a larger dataset basically  human action recognition  har  is applied to the adult content domain  it presents some technical difficulties  especially due to the enormous variation in camera position  the challenge is to classify actions based on a single video  the main input stream is the rgb one  as opposed to the skeleton one  and this is mostly due to the relatively small dataset    hrs   it is difficult to get an accurate pose estimation  which is a prerequisite for building robust skeleton har models  for most of the videos due to the proximity of the human bodies in the frames  hence there simply weren t enough data to include all the positions in the skeleton based model the audio input stream on the other hand is only used for a handful of actions  where deriving some insight is possible check it out on github for a detailed description   https possible use cases include   improving the recommender system  automatic tag generator  automatic timestamp generator  when does an action start and finish   filtering video content based on actions  positions 
219,219,RecentUnicorn,vajv8a,[D] Why do we marginalize latent variables in the likelihood of latent variable models?,"Why do we marginalize latent variables in the likelihood of latent variable models? When showing that MLE cannot be used for latent variable models, likelihood is taken such that latent variables are marginalized. Why is it so?",6,8,2022-06-12 16:49:05, d  why do we marginalize latent variables in the likelihood of latent variable models ,why do we marginalize latent variables in the likelihood of latent variable models  when showing that mle cannot be used for latent variable models  likelihood is taken such that latent variables are marginalized  why is it so 
220,220,hahayayak1776,vb63uf,[D] Could it ever be possible that an AI becomes conscious?,I know that LaMDA is definitely not sentient (lol) but it seems like we are quickly approaching the point where large llmâ€™s can pass the Turing test. How do we know for sure that these super large language models are categorically not sentient? What if consciousness is nothing more than a continual time series llm with qualia being nothing more than a collection of parameters working together to make a decision? Feel like most researchers I talk to laugh it off but Iâ€™m confused how they are so sure,53,0,2022-06-13 12:28:56, d  could it ever be possible that an ai becomes conscious ,i know that lamda is definitely not sentient  lol  but it seems like we are quickly approaching the point where large llm s can pass the turing test  how do we know for sure that these super large language models are categorically not sentient  what if consciousness is nothing more than a continual time series llm with qualia being nothing more than a collection of parameters working together to make a decision  feel like most researchers i talk to laugh it off but i m confused how they are so sure
221,221,alongub,vabq6q,[P] InferenceDB - Makes it easy to store predictions of real-time ML models in S3,"Hey r/MachineLearning! Just wanted to share a cool utility we've built.

If you ever had real-time models running in production, and you tried to store their predictions in a Parquet file for future investigation - you know it's not such a trivial task as you'd expect. Especially if you have large amounts of inferences. 

InferenceDB makes it super easy to store all your features and predictions in a Parquet file on S3. Check it out, and star the project if you like it:

[https://github.com/aporia-ai/inferencedb](https://github.com/aporia-ai/inferencedb)

Would love your feedback!",0,12,2022-06-12 07:20:55, p  inferencedb   makes it easy to store predictions of real time ml models in s,hey r machinelearning  just wanted to share a cool utility we ve built if you ever had real time models running in production  and you tried to store their predictions in a parquet file for future investigation   you know it s not such a trivial task as you d expect  especially if you have large amounts of inferences  inferencedb makes it super easy to store all your features and predictions in a parquet file on s  check it out  and star the project if you like it  https would love your feedback 
222,222,Tainaka_Ritsu_,vab5kt,"[D] Einstein summation, Contravariance/Covariance, Neural networks","I've been looking into Einstein summation notations for expressing neural network computations. One thing that I recall from physics class is that a big part of Einstein summation is whether indices are written upstairs/downstairs, i.e. contravariance/covariance. As I understand it, contravariance/covariance have a highly geometrical meaning (only make sense with respect to a coordinate system), so how exactly does this work with neural network parameters? As in, how do we talk about contravariance/covariance/index locations and what do they mean in a neural network context?",14,15,2022-06-12 06:46:26, d  einstein summation  contravariance covariance  neural networks,i ve been looking into einstein summation notations for expressing neural network computations  one thing that i recall from physics class is that a big part of einstein summation is whether indices are written upstairs downstairs  i e  contravariance covariance  as i understand it  contravariance covariance have a highly geometrical meaning  only make sense with respect to a coordinate system   so how exactly does this work with neural network parameters  as in  how do we talk about contravariance covariance index locations and what do they mean in a neural network context 
223,223,Thaetos,var4oa,[N] Getting started with Prompt Design (/r/PromptDesign),"For those who are interested in prompt design for language models, Iâ€™ve got good news for you!

I recently launched a new subreddit dedicated to prompt design & engineering where you can find lots of resources and tips and tricks.

Feel free to check it out ðŸ‘‹

/r/PromptDesign",0,0,2022-06-12 23:12:28, n  getting started with prompt design   r promptdesign ,for those who are interested in prompt design for language models  i ve got good news for you i recently launched a new subreddit dedicated to prompt design   engineering where you can find lots of resources and tips and tricks feel free to check it out   r promptdesign
224,224,Feisty-Fly-737,vai9pv,[R] Classification of Alzheimer's Disease from brain MRI using deep learning," Hi, my project is classification of Alzheimer's Disease from brain MRI(ADNI dataset). Maximum accuracy that I could obtain is only 67% using 3D CNN. I tried different ways to improve accuracy further. But no change in result. Is that an issue with preprocessing? I used HD-BET tool for preprocessing. Using any other tool is very much time consuming. I am using google colab for writing code. Can anyone suggest a way to proceed?",7,1,2022-06-12 14:51:46, r  classification of alzheimer s disease from brain mri using deep learning, hi  my project is classification of alzheimer s disease from brain mri adni dataset   maximum accuracy that i could obtain is only   using d cnn  i tried different ways to improve accuracy further  but no change in result  is that an issue with preprocessing  i used hd bet tool for preprocessing  using any other tool is very much time consuming  i am using google colab for writing code  can anyone suggest a way to proceed 
225,225,asuprem,vafszw,[D] How do you partition your data into shards for training?,"Recently, I was working with limited compute constraints (i.e. debugging in CoLab) but with a much larger dataset than would fit into CoLab's GPU memory. 

I implemented a quick and dirty sharding scheme for the data, since the transformations take some time. Basically, I performed the transformations on the training data chunk by chunk (in this case, chunk being 5000 or 10000 examples, etc), and saved each chunk into disk. Then, during training, the dataloader simply loads one of the saved chunks to yield examples.

When I wrote the code, I had to deal with a lot of side issues that come with sharding: randomizing the shard load order, as well as the examples in each shard and keeping track of edge cases. 

So, my question: when you have a large amount of data and maybe 1-2 cores, how do you deal with sharding? Also, if you have model parallelization, how do you keep track of which shard goes where?",1,1,2022-06-12 11:47:48, d  how do you partition your data into shards for training ,recently  i was working with limited compute constraints  i e  debugging in colab  but with a much larger dataset than would fit into colab s gpu memory  i implemented a quick and dirty sharding scheme for the data  since the transformations take some time  basically  i performed the transformations on the training data chunk by chunk  in this case  chunk being  or  examples  etc   and saved each chunk into disk  then  during training  the dataloader simply loads one of the saved chunks to yield examples when i wrote the code  i had to deal with a lot of side issues that come with sharding  randomizing the shard load order  as well as the examples in each shard and keeping track of edge cases  so  my question  when you have a large amount of data and maybe   cores  how do you deal with sharding  also  if you have model parallelization  how do you keep track of which shard goes where 
226,226,TobusFire,v9mzcd,[D] How are very large models trained on TPUs?,"I'm a CV researcher who has, until recently, always trained using high-performance GPUs (25+ GB memory). However, I have recently been playing around with TPUv2s and have noticed that I can run my smaller models much much faster as long as I am efficient with my training pipeline.

However, I noticed something that made me wonder about how large models are trained. I work in the medical imaging space as well, and 3D-UNet is the defacto framework for many benchmarks across various domains. The standard model in my application is not too big (something in the ballpark of 30-million parameters depending on your input). However, when I tried adapting this to TPUv2s, they struggled quite badly. This is because 3D Conv layers and 3D patchwise minibatches are too much for the memory to handle at the lower layers, even for batches of 1-2 (per-core). Since a TPU core only has 8 GB ram, it's hard to make it fit even the smallest 3D imaging models with a decent amount of filters. 2D is no problem, however.

This got me thinking: how are larger (ex. language and multimodel) models trained on TPUs? I know a lot are still trained on GPU clusters, but I saw that many new models are in fact being trained on TPUs (Dalle-Mini for example, which is 400 million parameters and was trained on a TPUv3 pod in only 3 days). How are that many parameters even able to fit on a TPU core? I know v3 pods have more memory but it's not an extreme improvement. Are attention modules separable somehow in a way that allows for only small parts of the model to need to be loaded at once?

Also, any discussion or advice for 3D ConvNet training on TPUs, in general, is of interest as well!",10,130,2022-06-11 07:15:00, d  how are very large models trained on tpus ,i m a cv researcher who has  until recently  always trained using high performance gpus    gb memory   however  i have recently been playing around with tpuvs and have noticed that i can run my smaller models much much faster as long as i am efficient with my training pipeline however  i noticed something that made me wonder about how large models are trained  i work in the medical imaging space as well  and d unet is the defacto framework for many benchmarks across various domains  the standard model in my application is not too big  something in the ballpark of  million parameters depending on your input   however  when i tried adapting this to tpuvs  they struggled quite badly  this is because d conv layers and d patchwise minibatches are too much for the memory to handle at the lower layers  even for batches of    per core   since a tpu core only has  gb ram  it s hard to make it fit even the smallest d imaging models with a decent amount of filters  d is no problem  however this got me thinking  how are larger  ex  language and multimodel  models trained on tpus  i know a lot are still trained on gpu clusters  but i saw that many new models are in fact being trained on tpus  dalle mini for example  which is  million parameters and was trained on a tpuv pod in only  days   how are that many parameters even able to fit on a tpu core  i know v pods have more memory but it s not an extreme improvement  are attention modules separable somehow in a way that allows for only small parts of the model to need to be loaded at once also  any discussion or advice for d convnet training on tpus  in general  is of interest as well 
227,227,cluecow,v9rigf,[P] Silero TTS Full V3 Release,"**Improvements**

* Huge release - 20 languages, 173 voices
* 1 new high quality Russian voice (*eugene*)
* The CIS languages: **Kalmyk**, **Russian**, **Tatar**, **Uzbek** and **Ukrainian**
* Romance and Germanic languages: **English**, **Indic English**, **Spanish**, **German**, **French**
* **10 Indic languages**
* All models inherit all of the previous SSML perks

**Links**

* [Colab](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb)

* [Project page](https://github.com/snakers4/silero-models#text-to-speech)

* [SSML wiki](https://github.com/snakers4/silero-models/wiki/SSML)

**Audio Samples**

* [English](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-english)

* [Indic English](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-indic-english)

* [Spanish](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-spanish)

* [Kalmyk](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-kalmyk)

* [German](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-german)

* [Russian](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-russian)

* [Tatar](https://soundcloud.com/alexander-veysov/silero-tts-v3-tatar)

* [Uzbek](https://soundcloud.com/alexander-veysov/silero-tts-v3-uzbek)

* [Ukrainian](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-ukrainian)

* [French](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-french)

* [Indic languages](https://soundcloud.com/alexander-veysov/sets/silero-tts-v3-indic)",1,20,2022-06-11 11:57:44, p  silero tts full v release,  improvements    huge release    languages   voices   new high quality russian voice   eugene    the cis languages    kalmyk      russian      tatar      uzbek   and   ukrainian    romance and germanic languages    english      indic english      spanish      german      french       indic languages    all models inherit all of the previous ssml perks  links     colab  https    project page  https    ssml wiki  https   audio samples     english  https    indic english  https    spanish  https    kalmyk  https    german  https    russian  https    tatar  https    uzbek  https    ukrainian  https    french  https    indic languages  https   soundcloud com alexander veysov sets silero tts v indic 
228,228,Leodip,va2tqb,[D] How are boundary conditions implemented in PINNs?,"I've been looking into PINNs lately as a method for solving PDEs (i.e., as a numerical method, not a data-based surrogate model), but something I'm struggling with understanding is how the boundary conditions are forced.

My theory is that a Dirichlet BC (i.e., of the type u(x)=f(x), where u(x) is the solution to the PDE) can be applied directly with some math tricks. For example, if N(x) is PINN's output, we can make the model's output be f(x) + d(x)\*N(x), where d(x) is a function that is 0 on the boundary and not 0 everywhere else (maybe the euclidean distance from the nearest boundary?). As such, instead of N(x) approximating u(x), it will instead be approximating (u(x)-f(x))/d(x).

To my understanding, this trick is widely used for applying initial conditions (d(x) in this case is simply t, time), but I'm not sure if it is also used for spatial boundary conditions.

However, I can't figure out an easy way to apply Neumann BCs other than just implementing the BC itself into the PDE with a penalization. Is this what is usually done? Is there a more clever way?",2,2,2022-06-11 23:23:47, d  how are boundary conditions implemented in pinns ,i ve been looking into pinns lately as a method for solving pdes  i e   as a numerical method  not a data based surrogate model   but something i m struggling with understanding is how the boundary conditions are forced my theory is that a dirichlet bc  i e   of the type u x  f x   where u x  is the solution to the pde  can be applied directly with some math tricks  for example  if n x  is pinn s output  we can make the model s output be f x    d x   n x   where d x  is a function that is  on the boundary and not  everywhere else  maybe the euclidean distance from the nearest boundary    as such  instead of n x  approximating u x   it will instead be approximating  u x  f x   d x  to my understanding  this trick is widely used for applying initial conditions  d x  in this case is simply t  time   but i m not sure if it is also used for spatial boundary conditions however  i can t figure out an easy way to apply neumann bcs other than just implementing the bc itself into the pde with a penalization  is this what is usually done  is there a more clever way 
229,229,fllubo,va02er,Any recommendation for the replacement of the toolkit jiant? [Research] [Discussion],"I am doing research in NLP with the toolkit jiant ([https://github.com/nyu-mll/jiant](https://github.com/nyu-mll/jiant)). It is a quite nice and easy-to-use tool. Unfortunately, it stopped being maintained. I wonder is there any other recommendation that I can use to replace it?",4,2,2022-06-11 21:07:23,any recommendation for the replacement of the toolkit jiant   research   discussion ,i am doing research in nlp with the toolkit jiant   https   github com nyu mll jiant  https   github com nyu mll jiant    it is a quite nice and easy to use tool  unfortunately  it stopped being maintained  i wonder is there any other recommendation that i can use to replace it 
230,230,jhoveen1,v9jibx,Stochastic Deep RL environment [D],What are common stochastic deep RL environments? Atari and mujoco both have deterministic transitions. Can someone point to papers/references I could look up to find what common benchmarks for stochastic deep RL are?,3,21,2022-06-11 04:07:32,stochastic deep rl environment  d ,what are common stochastic deep rl environments  atari and mujoco both have deterministic transitions  can someone point to papers references i could look up to find what common benchmarks for stochastic deep rl are 
231,231,mistasse,v9qoh0,"[P] Pytorch-Lightning-style code for losses, decoding, ground-truth formatting, and more. Practical and efficient.","Hi r/MachineLearning !

I'm posting here today because I got convinced some work I published last year is definitely relevant for some of us who have to write the math **around** their NNs (e.g. losses, decoding, ground-truth formatting). It happens to go in a direction very similar to Pytorch-Lightning, but for the math system instead of the training loop.

It has been published under the pretext that it was facilitating incremental research, but that's far from the whole story. The paper and video still take the time to elaborate on other considerations.

* ICLR2021 Workshop paper about it: [https://openreview.net/pdf?id=264iXDLnD59](https://openreview.net/pdf?id=264iXDLnD59)
* Paper video: [https://www.youtube.com/watch?v=xAW2hjPZw4I](https://www.youtube.com/watch?v=xAW2hjPZw4I)
* Paper repository [https://github.com/mistasse/modulom-panopticdeeplab](https://github.com/mistasse/modulom-panopticdeeplab)
* Example code: [https://github.com/mistasse/modulom-panopticdeeplab/blob/master/segmentation/model/output\_math/pdl.py#L148=](https://github.com/mistasse/modulom-panopticdeeplab/blob/master/segmentation/model/output_math/pdl.py#L180=)

I haven't advertised it much yet, but it is more than one year and a half that I work with the described system, and for my daily research, and it is life-changing. Some of my colleagues have been integrating it smoothly as well and also appreciate it, so I'm a bit more confident about it.

Please don't hesitate to make me know if the problem rings a bell to you, and whether you feel it's a good direction to go or not. I'm really interested in knowing whether it may help some people here.

In short, the principle is quite simple: when dealing with a batch, we can create a big ""mathematical system"" object that holds the data being worked on and does the computations when they are needed, and only once thanks to some cache. Every tensor of interest (variable in the math system) is ""named"" in the sense we can obtain it by doing the call that would compute it, like `math_sys.segmentation_pred('sigmoid')` or `math_sys.segmentation_pred('argmax')`.

Then, we can just pass that object instead of specific tensors to functions that needs them. And we don't have the dilemmas about choosing to propagate logits or sigmoid outputs for instance, nor the miles long signatures and hours of refactoring for just modifying the flow of data.

Finally, because we are using object-oriented programming, and our methods do not use parameters to route data, they are easily overridable, and we can compose them through multiple inheritance. Flow of data becomes pretty much unconstrained.

Some other cool facts:

* Control flow is decoupled between computations of different terms, very cool to avoid unreadable spaghettis
* We have easy access to intermediate results from outside as well (e.g. for debugging/visualization)
* Renders testing much more practical
* Easy to move that whole ""mathematical system"" module from one codebase to another
* We can actually integrate complete pytorch modules inside it, the data-augmentation, the tokenization, ... Typically, I can switch from a CNN to a Transformer by switching a class for another, and it will take care of extra steps required while leaving the rest of the codebase unchanged compared to when there was just a CNN.
* Doesn't cost much in terms of performance, while it opens big opportunities for implementing optimizations and reduces dramatically the risk/need to duplicate computations.
* It will probably not handle a training loop well, because it's not a good fit for mutable state.

More could be done, like turning it into a library, making it compatible with TorchScript/Tensorflow, perhaps creating model zoos based on this, or simply showing more of what is possible with it.

The only thing that has prevented me from creating a library is that I'm not fully happy with the way to define whether a call should be cached or not. I like to have calls cached by default, it is easy to disable (if method is decorated with `@no_cache`, its name starts with `_`, or the call contains kw-only arguments), but I don't find the former to be well defined in case of inheritance. If you have ideas, please make me know.

Thank you!",4,4,2022-06-11 11:00:22, p  pytorch lightning style code for losses  decoding  ground truth formatting  and more  practical and efficient ,hi r machinelearning  i m posting here today because i got convinced some work i published last year is definitely relevant for some of us who have to write the math   around   their nns  e g  losses  decoding  ground truth formatting   it happens to go in a direction very similar to pytorch lightning  but for the math system instead of the training loop it has been published under the pretext that it was facilitating incremental research  but that s far from the whole story  the paper and video still take the time to elaborate on other considerations   iclr workshop paper about it   https   paper video   https   paper repository  https   example code   https i haven t advertised it much yet  but it is more than one year and a half that i work with the described system  and for my daily research  and it is life changing  some of my colleagues have been integrating it smoothly as well and also appreciate it  so i m a bit more confident about it please don t hesitate to make me know if the problem rings a bell to you  and whether you feel it s a good direction to go or not  i m really interested in knowing whether it may help some people here in short  the principle is quite simple  when dealing with a batch  we can create a big mathematical system object that holds the data being worked on and does the computations when they are needed  and only once thanks to some cache  every tensor of interest  variable in the math system  is named in the sense we can obtain it by doing the call that would compute it  like  math_sys segmentation_pred  sigmoid    or  math_sys segmentation_pred  argmax    then  we can just pass that object instead of specific tensors to functions that needs them  and we don t have the dilemmas about choosing to propagate logits or sigmoid outputs for instance  nor the miles long signatures and hours of refactoring for just modifying the flow of data finally  because we are using object oriented programming  and our methods do not use parameters to route data  they are easily overridable  and we can compose them through multiple inheritance  flow of data becomes pretty much unconstrained some other cool facts   control flow is decoupled between computations of different terms  very cool to avoid unreadable spaghettis  we have easy access to intermediate results from outside as well  e g  for debugging visualization   renders testing much more practical  easy to move that whole mathematical system module from one codebase to another  we can actually integrate complete pytorch modules inside it  the data augmentation  the tokenization      typically  i can switch from a cnn to a transformer by switching a class for another  and it will take care of extra steps required while leaving the rest of the codebase unchanged compared to when there was just a cnn   doesn t cost much in terms of performance  while it opens big opportunities for implementing optimizations and reduces dramatically the risk need to duplicate computations   it will probably not handle a training loop well  because it s not a good fit for mutable state more could be done  like turning it into a library  making it compatible with torchscript tensorflow  perhaps creating model zoos based on this  or simply showing more of what is possible with it the only thing that has prevented me from creating a library is that i m not fully happy with the way to define whether a call should be cached or not  i like to have calls cached by default  it is easy to disable  if method is decorated with   no_cache   its name starts with  _   or the call contains kw only arguments   but i don t find the former to be well defined in case of inheritance  if you have ideas  please make me know thank you 
232,232,waa007,v9rr7s,[D] Is there any small and interesting research directions of NLP recommended?,"Popular NLP model(bert, GPT) is getting more and more bigger, the cost can't affordable for single person not rely big company, it's bad for diversity in research.

I admit bigger model have better performence, but I think explicable and modifiable technology is more important, bigger model seem that do nothing more in explanatory of model.

More and more people can put into NLP, help NLP to Artificial General Intelligence faster, if the cost is lower.

Thanks for any advice.",9,3,2022-06-11 12:15:48, d  is there any small and interesting research directions of nlp recommended ,popular nlp model bert  gpt  is getting more and more bigger  the cost can t affordable for single person not rely big company  it s bad for diversity in research i admit bigger model have better performence  but i think explicable and modifiable technology is more important  bigger model seem that do nothing more in explanatory of model more and more people can put into nlp  help nlp to artificial general intelligence faster  if the cost is lower thanks for any advice 
233,233,Smooth-Yam8304,v9trs2,[D] Estimating Future Performance of Neural Network,"Let's say I have a neural network and I want to see how well that network will do on a set of concepts. To obtain an accuracy value on a certain word, we have a simple test set associated with each word that we use to gauge the model's understanding of that word. Assume that the neural network obtains an accuracy of 0.90 on the word ""desk"" and an accuracy of 0.80 on the word ""computer"". Are there any fields of research/methods I can use to derive simple heuristics/estimates for how the neural network will perform (in terms of accuracy) on the phrase ""desk and computer""?

I realize I can convert ""desk and computer"" into the logical form AND(desk, computer). Does that mean I can use some rules associated with logical AND operators?

Any thoughts would be greatly appreciated. Thank you.",3,0,2022-06-11 14:43:47, d  estimating future performance of neural network,let s say i have a neural network and i want to see how well that network will do on a set of concepts  to obtain an accuracy value on a certain word  we have a simple test set associated with each word that we use to gauge the model s understanding of that word  assume that the neural network obtains an accuracy of   on the word desk and an accuracy of   on the word computer  are there any fields of research methods i can use to derive simple heuristics estimates for how the neural network will perform  in terms of accuracy  on the phrase desk and computer i realize i can convert desk and computer into the logical form and desk  computer   does that mean i can use some rules associated with logical and operators any thoughts would be greatly appreciated  thank you 
234,234,moinnadeem,v8rmtj,[R] Blazingly Fast Computer Vision Training with the Mosaic ResNet and Composer,"Hey all!

MosaicML is excited to release the [Mosaic ResNet](https://www.mosaicml.com/blog/mosaic-resnet), which trains to a 76.6% classification accuracy **in 27 minutes, 7x faster than NVIDIA's ResNet baseline,** using only *vanilla PyTorch.* These recipes modify the training algorithm; the network architecture is the same ResNet youâ€™ve known and loved since 2015 (with updated anti-aliasing pooling via [Blurpool](https://arxiv.org/abs/1904.11486)). 

See all of the details in [our blog post](https://mosaicml.com/blog/mosaic-resnet)!

The figure below summarizes our three training recipes (exact recipes available [here](https://github.com/mosaicml/benchmarks/tree/main/blogs/resnet/recipes)). **You can check out the complete results of the hundreds of training runs we conducted to create these recipes using** [**Explorer**](https://explorer.mosaicml.com/), our tool for evaluating the efficiency of training algorithms.

[Comparison between best MosaicML ResNet-50 Recipe for a given Time & Accuracy \(i.e. the Pareto frontier\) to different baselines. Data collected on the MosaicML Cloud \(8x NVIDIA A100\).](https://preview.redd.it/yon77p1a0o491.jpg?width=1008&format=pjpg&auto=webp&s=71a1de4c14f1bbc2b86e5e04c84350b8d04eb9bc)

These results push on the interplay between algorithmic science and systems engineering, providing segmented cases for research like [FFCV Dataloaders](https://docs.mosaicml.com/en/latest/examples/ffcv_dataloaders.html?highlight=ffcv),  [Sharpness-Aware Minimization](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/sam), and novel, MosaicML algorithms such as [ColOut](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/colout).

[MosaicML's release of \\""training recipes\\"", which permit a user to trade off between accuracy and runtime.](https://preview.redd.it/o3mpu522un491.png?width=11831&format=png&auto=webp&s=4869097b86383abef6a16a2e29c19c4607e37d30)

**Want to verify our results? Want to beat ours? Or just want to speed up your own model training?** Head over to our our GitHub repo, [https://github.com/mosaicml/composer](https://github.com/mosaicml/composer), which enables this research, and star it â­ï¸ to keep up with the latest updates!

And stay tuned for a much deeper dive on all the details, a comprehensive write-up on the science and engineering of this work, next week!

https://preview.redd.it/falrstlytn491.png?width=1498&format=png&auto=webp&s=e7eb5413816b18e2f13efb89681c5e451a41aa64",26,305,2022-06-10 03:08:26, r  blazingly fast computer vision training with the mosaic resnet and composer,hey all mosaicml is excited to release the  mosaic resnet  https see all of the details in  our blog post  https the figure below summarizes our three training recipes  exact recipes available  here  https  comparison between best mosaicml resnet  recipe for a given time   accuracy   i e  the pareto frontier   to different baselines  data collected on the mosaicml cloud   x nvidia a     https these results push on the interplay between algorithmic science and systems engineering  providing segmented cases for research like  ffcv dataloaders  https  mosaicml s release of   training recipes    which permit a user to trade off between accuracy and runtime   https   want to verify our results  want to beat ours  or just want to speed up your own model training    head over to our our github repo   https and stay tuned for a much deeper dive on all the details  a comprehensive write up on the science and engineering of this work  next week https   preview redd it falrstlytn png width  format png auto webp s eebbefefbceaaa
235,235,egesko,v9fyvp,[D] L2 Regularization on Generator Output (GAN)," Hello,

I  am developing a GAN-like model with the purpose of finding optimum  noise distributions to sample from and introduce to a medium so that any  classification model that takes samples from that medium will fail to  train. Because the zero-sum game GANs play is similar to this game of  generator-classifier fight, my hope is that generator output  distribution will converge towards the optimum noise distribution so  that the classification model will actually fail.

The  question is, is there a correct way to limit the output my GAN  generator actually generates? My initial thought was to add an L2 norm  of generator outputs to the loss function of the generator, kind of like  how we do L2 norm regularization for model weights. But as I trained, I  realized that changing the coefficient of this L2 norm term doesn't  seem to affect the norm of the output generated by the generator. Is  this idea fundamentally flawed? Or is there any other method you can  suggest that might work better?

Thank you",2,4,2022-06-11 01:19:59, d  l regularization on generator output  gan , hello i  am developing a gan like model with the purpose of finding optimum  noise distributions to sample from and introduce to a medium so that any  classification model that takes samples from that medium will fail to  train  because the zero sum game gans play is similar to this game of  generator classifier fight  my hope is that generator output  distribution will converge towards the optimum noise distribution so  that the classification model will actually fail the  question is  is there a correct way to limit the output my gan  generator actually generates  my initial thought was to add an l norm  of generator outputs to the loss function of the generator  kind of like  how we do l norm regularization for model weights  but as i trained  i  realized that changing the coefficient of this l norm term doesn t  seem to affect the norm of the output generated by the generator  is  this idea fundamentally flawed  or is there any other method you can  suggest that might work better thank you
236,236,Ok_Cardiologist8306,v9080g,[D] Use of (machine learning + Game engines) for automatic 2D/3D content creation,"Hello Everyone!  


Since game engines such as ""unreal engine"" and ""unity3D"" are able to create content that looks and behaves pretty realistically.  


Therefore, I was wondering if there are few use cases or examples of the use of machine learning in creating 2D/3D content automatically/efficiently using game engines. For example using machine learning + Game engines for creating product specific advertisements automatically  


Please feel free to share if you are aware of any relevant links or resources.  


Thanks!",7,22,2022-06-10 10:46:13, d  use of  machine learning   game engines  for automatic d d content creation,hello everyone   since game engines such as unreal engine and unityd are able to create content that looks and behaves pretty realistically   therefore  i was wondering if there are few use cases or examples of the use of machine learning in creating d d content automatically efficiently using game engines  for example using machine learning   game engines for creating product specific advertisements automatically  please feel free to share if you are aware of any relevant links or resources   thanks 
237,237,wowAmaze,v8x9en,[D] Has the algorithm from 'Testing the Manifold Hypothesis' been implemented by anyone?,"The paper I'm referring to is [Testing the Manifold Hypothesis by Fefferman et al.](https://www.mit.edu/~mitter/publications/121_Testing_Manifold.pdf), in this paper I believe they outline a hypothetical algorithm that test if a given dataset satisfies the manifold hypothesis for some specific class of manifolds. In [this 10 year old ppt](https://www.ima.umn.edu/materials/2014-2015/W3.16-20.15/23040/Testing_Manifold_IMA.pptx) The authors said future work was to ""make practical and test on real data,"" so 10 years half passed, has this algorithm been implemented?",6,28,2022-06-10 07:54:42, d  has the algorithm from  testing the manifold hypothesis  been implemented by anyone ,the paper i m referring to is  testing the manifold hypothesis by fefferman et al   https   www mit edu  mitter publications _testing_manifold pdf   in this paper i believe they outline a hypothetical algorithm that test if a given dataset satisfies the manifold hypothesis for some specific class of manifolds  in  this  year old ppt  https   www ima umn edu materials   w     testing_manifold_ima pptx  the authors said future work was to make practical and test on real data  so  years half passed  has this algorithm been implemented 
238,238,Singularian2501,v8tbdf,[R] Extreme Compression for Pre-trained Transformers Made Simple and Efficient - Microsoft 2022,"Paper: [https://arxiv.org/abs/2206.01859](https://arxiv.org/abs/2206.01859)

Abstract:

>Extreme compression, particularly ultra-low bit precision  (binary/ternary) quantization, has been proposed to fit large NLP models  on resource-constraint devices. However, to preserve the accuracy for  such aggressive compression schemes, cutting-edge methods usually  introduce complicated compression pipelines, e.g., multi-stage expensive  knowledge distillation with extensive hyperparameter tuning. Also, they  oftentimes focus less on smaller transformer models that have already  been heavily compressed via knowledge distillation and lack a systematic  study to show the effectiveness of their methods. In this paper, we  perform a very comprehensive systematic study to measure the impact of  many key hyperparameters and training strategies from previous works. As  a result, we find out that previous baselines for ultra-low bit  precision quantization are significantly under-trained. Based on our  study, we propose a simple yet effective compression pipeline for  extreme compression, named XTC. XTC demonstrates that (1) we can **skip  the pre-training knowledge distillation to obtain a 5-layer BERT while  achieving better performance than previous state-of-the-art methods,  e.g., the 6-layer TinyBERT**; (2) **extreme quantization plus layer reduction is able to reduce the model size by 50x**, resulting in new state-of-the-art results on GLUE tasks. 

&#x200B;

https://preview.redd.it/kgbjncheeo491.jpg?width=1187&format=pjpg&auto=webp&s=ffa0963f0c0dd9a2ab9163d5ec6dc8d43584ece0

https://preview.redd.it/7ioxmuqeeo491.jpg?width=577&format=pjpg&auto=webp&s=e2d5eec7274bcfe2c9bb66cb0b0256af5d4594a6

https://preview.redd.it/y5wcth7feo491.jpg?width=1151&format=pjpg&auto=webp&s=bb96b98862987cd2900d7d0906259190c3ad154b",0,21,2022-06-10 04:27:53, r  extreme compression for pre trained transformers made simple and efficient   microsoft ,paper   https abstract  extreme compression  particularly ultra low bit precision   binary ternary  quantization  has been proposed to fit large nlp models  on resource constraint devices  however  to preserve the accuracy for  such aggressive compression schemes  cutting edge methods usually  introduce complicated compression pipelines  e g   multi stage expensive  knowledge distillation with extensive hyperparameter tuning  also  they  oftentimes focus less on smaller transformer models that have already  been heavily compressed via knowledge distillation and lack a systematic  study to show the effectiveness of their methods  in this paper  we  perform a very comprehensive systematic study to measure the impact of  many key hyperparameters and training strategies from previous works  as  a result  we find out that previous baselines for ultra low bit  precision quantization are significantly under trained  based on our  study  we propose a simple yet effective compression pipeline for  extreme compression  named xtc  xtc demonstrates that    we can   skip  the pre training knowledge distillation to obtain a  layer bert while  achieving better performance than previous state of the art methods   e g   the  layer tinybert         extreme quantization plus layer reduction is able to reduce the model size by x    resulting in new state of the art results on glue tasks    xb https https https   preview redd it ywcthfeo jpg width  format pjpg auto webp s bbbcdddcadb
239,239,dojoteef,v8smlm,[D] Request for moderators,"If you frequently visit r/ml throughout the day, have a good understanding of the field, and a history of constructive comments/posts, then we need your help as a moderator.

Please apply by sending us a [modmail](https://reddit.com/message/compose/?to=/r/MachineLearning) with the following info:

* Your role (engineer, student, researcher, self-taught, etc) and years of experience in ML
* Amount of time available to spend on the sub (you must check the sub quite regularly throughout the day)
* Your time zone

Weâ€™re specifically looking for friendly people that have at least a year or two of experience in ML who understand the current research and industry landscape and who have been on r/ml long enough to understand what the community expects in terms of moderation.

Thanks!",3,16,2022-06-10 03:54:12, d  request for moderators,if you frequently visit r ml throughout the day  have a good understanding of the field  and a history of constructive comments posts  then we need your help as a moderator please apply by sending us a  modmail  https   your role  engineer  student  researcher  self taught  etc  and years of experience in ml  amount of time available to spend on the sub  you must check the sub quite regularly throughout the day   your time zonewe re specifically looking for friendly people that have at least a year or two of experience in ml who understand the current research and industry landscape and who have been on r ml long enough to understand what the community expects in terms of moderation thanks 
240,240,scb_11,v8lxqj,[P] Set-up Yolo-V5 distributed data-parallel multi GPU on AWS and Kubeflow,"Hi all, 

I've been trying to set up DDP multi-node training on AWS for a week and am finally able to make it work.

I didn't find any resources for the same. So thought would write a blog and share it. Please provide feedback and see if this is helpful for you

[https://medium.com/@sachinchandra/running-yolo-v5-with-ddp-on-aws-8a4f07a77cf](https://medium.com/@sachinchandra/running-yolo-v5-with-ddp-on-aws-8a4f07a77cf)",5,44,2022-06-09 22:54:04, p  set up yolo v distributed data parallel multi gpu on aws and kubeflow,hi all  i ve been trying to set up ddp multi node training on aws for a week and am finally able to make it work i didn t find any resources for the same  so thought would write a blog and share it  please provide feedback and see if this is helpful for you https   medium com  sachinchandra running yolo v with ddp on aws afacf  https   medium com  sachinchandra running yolo v with ddp on aws afacf 
241,241,ghosthamlet,v89ynl,[R] The Annotated Diffusion Model,"From huggingface post: [https://huggingface.co/blog/annotated-diffusion](https://huggingface.co/blog/annotated-diffusion)

A New great article joined the Annotated series: The Annotated Transformer [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html), The Annotated GPT-2 [https://amaarora.github.io/2020/02/18/annotatedGPT2.html](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)",11,165,2022-06-09 11:26:14, r  the annotated diffusion model,from huggingface post   https a new great article joined the annotated series  the annotated transformer  http   nlp seas harvard edu    attention html  http   nlp seas harvard edu    attention html   the annotated gpt   https   amaarora github io    annotatedgpt html  https   amaarora github io    annotatedgpt html 
242,242,Singularian2501,v8tzz6,[R] Decentralized Training of Foundation Models in Heterogeneous Environments,"Paper: [https://arxiv.org/abs/2206.01288](https://arxiv.org/abs/2206.01288) 

Abstract:

>Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational ""tasklets"" in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. **In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron)**.       

https://preview.redd.it/za0ma00pho491.jpg?width=1292&format=pjpg&auto=webp&s=9e1729f5ffcc02b2939d79be20191d896e42ae0a

https://preview.redd.it/0ekusmuqho491.jpg?width=722&format=pjpg&auto=webp&s=08fddd9d114e21d455b5255f2084a918ae581810

https://preview.redd.it/yj8tz1xzho491.jpg?width=954&format=pjpg&auto=webp&s=b132e79bf842e0cc5dce223db2aa59cd67ad2635

https://preview.redd.it/ulbu9oe3ko491.jpg?width=650&format=pjpg&auto=webp&s=5a535fc7409190fc600608e1fe6672716a8ec2e6

https://preview.redd.it/fu42r5g2io491.jpg?width=1095&format=pjpg&auto=webp&s=204cfed9db065b90971b228d6f369c52884219e3",0,5,2022-06-10 05:01:29, r  decentralized training of foundation models in heterogeneous environments,paper   https abstract  training foundation models  such as gpt  and palm  can be extremely expensive  often involving tens of thousands of gpus running continuously for months  these models are typically trained in specialized clusters featuring fast  homogeneous interconnects and using carefully designed software systems that support both data parallelism and model pipeline parallelism  such dedicated clusters can be costly and difficult to obtain  can we instead leverage the much greater amount of decentralized  heterogeneous  and lower bandwidth interconnected compute  previous works examining the heterogeneous  decentralized setting focus on relatively small models that can be trained in a purely data parallel manner  state of the art schemes for model parallel foundation model training  such as megatron  only consider the homogeneous data center setting  in this paper  we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network  our key technical contribution is a scheduling algorithm that allocates different computational tasklets in the training of foundation models to a group of decentralized gpu devices connected by a slow heterogeneous network  we provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy  we conduct extensive experiments that represent different scenarios for learning over geo distributed devices simulated using real world network measurements    in the most extreme case  across  different cities spanning  continents  our approach is  x faster than prior state of the art training systems  megatron           https https https https https   preview redd it furgio jpg width  format pjpg auto webp s cfeddbbbdfce
243,243,dtransposed,v8rke7,[D] G.Hinton's ML-driven explanation of the role of the sleep - inquiry about further sources.,"In the recent episode of Peter Abbeel's ""The Robot Brains"" podcast, G.Hinton explains a fascinating hypothesis behind the role of sleep in our lives (""sleep is the process of forgetting negative examples in human contrastive learning framework""). However, he does it in a very general way. Does anybody know where I could read more about that? Academic papers etc.?

Reference: [https://youtu.be/2EDP4v-9TUA](https://youtu.be/2EDP4v-9TUA)",11,8,2022-06-10 03:05:24, d  g hinton s ml driven explanation of the role of the sleep   inquiry about further sources ,in the recent episode of peter abbeel s the robot brains podcast  g hinton explains a fascinating hypothesis behind the role of sleep in our lives  sleep is the process of forgetting negative examples in human contrastive learning framework   however  he does it in a very general way  does anybody know where i could read more about that  academic papers etc  reference   https   youtu be edpv tua  https   youtu be edpv tua 
244,244,jackcook,v8kyi6,[R] Can machine learning make side-channel attacks even stronger?,"Twitter thread: [https://twitter.com/jackcook36/status/1534920169369309184](https://twitter.com/jackcook36/status/1534920169369309184)  
Paper: [https://jackcook.github.io/bigger-fish/paper.pdf](https://jackcook.github.io/bigger-fish/paper.pdf)

Key findings:

* Machine learning can be used to identify activity on your computer from traces recorded in JavaScript that measure CPU instruction throughput over time
* We found this type of attack exploits signals from system interrupts, which operating systems use to interact with hardware devices
   * When a core processes interrupts, it pauses the execution of an attacker, creating a signal that can be exploited
* Our *loop-counting attack* can correctly identify one of 100 websites being opened 96.6% of the time in Chrome on Linux
* We identified a randomized timer mitigation that reduces our attackâ€™s accuracy to near chance

Please let me know if you have any feedback or questions!",6,18,2022-06-09 22:10:41, r  can machine learning make side channel attacks even stronger ,twitter thread   https paper   https key findings   machine learning can be used to identify activity on your computer from traces recorded in javascript that measure cpu instruction throughput over time  we found this type of attack exploits signals from system interrupts  which operating systems use to interact with hardware devices     when a core processes interrupts  it pauses the execution of an attacker  creating a signal that can be exploited  our  loop counting attack  can correctly identify one of  websites being opened    of the time in chrome on linux  we identified a randomized timer mitigation that reduces our attack s accuracy to near chanceplease let me know if you have any feedback or questions 
245,245,Hasan_Shanto,v90qyq,[D] How to predict on anonymous dataset?,"So, I have a dataset where both train and test data has a huge chunk of data with no descriptions. I have to predict labels (1/0) based on train dataset. But as there is no description of the dataset, I am unable to understand the correlation between target and other variables. What should I do?",11,0,2022-06-10 11:19:06, d  how to predict on anonymous dataset ,so  i have a dataset where both train and test data has a huge chunk of data with no descriptions  i have to predict labels     based on train dataset  but as there is no description of the dataset  i am unable to understand the correlation between target and other variables  what should i do 
246,246,Competitive_Travel16,v8lz6j,[R] More ethical machine learning using model cards at Wikimedia,"Abstract, 10 minute video, and transcript from May 2022 Apply(conf):

> First proposed by Mitchell et al. in 2018, model cards are a form of transparent reporting of machine learning models, their uses, and performance for public audiences. As part of a broader effort to strengthen our ethical approaches to machine learning at Wikimedia, we started implementing model cards for every model hosted by the Foundation. This talk is a description of our process, motivation, and lessons learned along the way.

https://www.youtube.com/watch?v=t4GMq7MC7Js

https://www.tecton.ai/apply/session-video-archive/more-ethical-machine-learning-using-model-card-at-wikimedia/",26,10,2022-06-09 22:55:48, r  more ethical machine learning using model cards at wikimedia,abstract   minute video  and transcript from may  apply conf    first proposed by mitchell et al  in   model cards are a form of transparent reporting of machine learning models  their uses  and performance for public audiences  as part of a broader effort to strengthen our ethical approaches to machine learning at wikimedia  we started implementing model cards for every model hosted by the foundation  this talk is a description of our process  motivation  and lessons learned along the way https https   www tecton ai apply session video archive more ethical machine learning using model card at wikimedia 
247,247,InstrumentalAsylum,v8svy2,New Insights on Infant Word Lear[N]ing - Implications for optimizing machine learning and second language learning, [https://community.chatwithastrid.com/aprendiendo-espanol-76iwwk5y/post/new-insight-on-how-babies-learn-words-M3dUgc6rrRb83ZD](https://community.chatwithastrid.com/aprendiendo-espanol-76iwwk5y/post/new-insight-on-how-babies-learn-words-M3dUgc6rrRb83ZD),3,0,2022-06-10 04:06:40,new insights on infant word lear n ing   implications for optimizing machine learning and second language learning,  https   community chatwithastrid com aprendiendo espanol iwwky post new insight on how babies learn words mdugcrrrbzd  https   community chatwithastrid com aprendiendo espanol iwwky post new insight on how babies learn words mdugcrrrbzd 
248,248,Professional-Ad-776,v8ytd6,[D] Third Party Model Validation,"Hi Everyone,
I am working on a project  to validate a XGboost model developed by a another team.Is there any guide or tutorial on how I could navigate through the project and validate the model.

Should I be using synthetic data or request the team to provide unseen data?
Any information would be helpful.
Thank you",2,0,2022-06-10 09:22:14, d  third party model validation,hi everyone i am working on a project  to validate a xgboost model developed by a another team is there any guide or tutorial on how i could navigate through the project and validate the model should i be using synthetic data or request the team to provide unseen data any information would be helpful thank you
249,249,giakou4,v8ikr1,[D] Benchmark Object Detection Hyperparameters,"I want to conduct benchmark experiments: Faster R-CNN vs YOLOv3 vs YOLOv4 vs YOLOv5. For that reason, I want to fix the hyperparameters: optimizer, learning rate, weight decay and learning rate scheduler. For optimizer, due to different frameworks, I have to go with `ADAM (b0=0.9, b1=0.999, eps=1e-7)`. What parameters should I choose for weigt decay, and learning rate scheduler, given that different models converge at different epoch/steps? Should I go with cosine decay, manual step (with 0.1 decay at 80 and 90% of total epoch/steps), or something else?

Note: different frameworks have different ""default"" hyperparameters, maybe I should stick to defalt?",11,2,2022-06-09 20:23:00, d  benchmark object detection hyperparameters,i want to conduct benchmark experiments  faster r cnn vs yolov vs yolov vs yolov  for that reason  i want to fix the hyperparameters  optimizer  learning rate  weight decay and learning rate scheduler  for optimizer  due to different frameworks  i have to go with  adam  b    b    eps e     what parameters should i choose for weigt decay  and learning rate scheduler  given that different models converge at different epoch steps  should i go with cosine decay  manual step  with   decay at  and   of total epoch steps   or something else note  different frameworks have different default hyperparameters  maybe i should stick to defalt 
250,250,Expert-Departure-236,v8avyb,[Discussion] Fine tune model for long context,"How to train GPT or BERT for large context where context length is more than 1024 tokens. Truncating the context is not an option as the complete context is important. 

One approach that I can think of is breaking/dividing the context into multiple chunks.

What are my other options?t",3,7,2022-06-09 12:28:45, discussion  fine tune model for long context,how to train gpt or bert for large context where context length is more than  tokens  truncating the context is not an option as the complete context is important  one approach that i can think of is breaking dividing the context into multiple chunks what are my other options t
251,251,irismodel,v7pxj0,[P] Featureform: Open-Source Virtual Feature Store,"Hey everyone!

Weâ€™re excited to announce the open-source version of Featureform, an extensible feature store. Weâ€™ve found that existing feature stores are either too heavy and replace your existing infrastructure, or donâ€™t handle transformations at all and simply store features. We built a feature store thatâ€™s a happy medium between the two, it orchestrates your existing infrastructure to work like a feature store. We wrote more about this in our [blog post](https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures).

Check out the repo: [https://github.com/featureform/featureform](https://github.com/featureform/featureform)

&#x200B;

https://preview.redd.it/vwpe0uypje491.png?width=2084&format=png&auto=webp&s=f81f7447f2c35081b2ae63e885506b9187a73d7b

# What Is Featureform

[Featureform](https://github.com/featureform/featureform) is a virtual feature store. It enables data scientists to **define**, **manage**, and **serve** their ML model's features. Featureform sits atop your **existing infrastructure** and orchestrates it to work like a traditional feature store. By using Featureform, a data science team can solve the organizational problems:

* **Enhance Collaboration** Featureform ensures that transformations, features, labels, and training sets are defined in a standardized form, so they can easily be shared, re-used, and understood across the team.
* **Organize Experimentation** The days of untitled\_128.ipynb are over. Transformations, features, and training sets can be pushed from notebooks to a centralized feature repository with metadata like name, variant, lineage, and owner.
* **Facilitate Deployment** Once a feature is ready to be deployed, Featureform will orchestrate your data infrastructure to make it ready for production. Using the Featureform API, you won't have to worry about the idiosyncrasies of your heterogeneous infrastructure (beyond their transformation language).
* **Increase Reliability** Featureform enforces that all features, labels, and training sets are immutable. This allows them to safely be re-used among data scientists without worrying about logic changing. Furthermore, Featureform's orchestrator will handle retry logic and attempt to resolve other common distributed system problems automatically.
* **Preserve Compliance** With built-in role-based access control, audit logs, and dynamic serving rules, your compliance logic can be enforced directly by Featureform.

We just launched our first open-source version and would love to hear your thoughts and what you'd like to see! :)

Repo: [https://github.com/featureform/featureform](https://github.com/featureform/featureform)

Docs: [https://docs.featureform.com/](https://docs.featureform.com/)

Blog post: [https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures](https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures)",13,143,2022-06-08 19:11:20, p  featureform  open source virtual feature store,hey everyone we re excited to announce the open source version of featureform  an extensible feature store  we ve found that existing feature stores are either too heavy and replace your existing infrastructure  or don t handle transformations at all and simply store features  we built a feature store that s a happy medium between the two  it orchestrates your existing infrastructure to work like a feature store  we wrote more about this in our  blog post  https check out the repo   https   xb https   what is featureform featureform  https     enhance collaboration   featureform ensures that transformations  features  labels  and training sets are defined in a standardized form  so they can easily be shared  re used  and understood across the team     organize experimentation   the days of untitled _ ipynb are over  transformations  features  and training sets can be pushed from notebooks to a centralized feature repository with metadata like name  variant  lineage  and owner     facilitate deployment   once a feature is ready to be deployed  featureform will orchestrate your data infrastructure to make it ready for production  using the featureform api  you won t have to worry about the idiosyncrasies of your heterogeneous infrastructure  beyond their transformation language      increase reliability   featureform enforces that all features  labels  and training sets are immutable  this allows them to safely be re used among data scientists without worrying about logic changing  furthermore  featureform s orchestrator will handle retry logic and attempt to resolve other common distributed system problems automatically     preserve compliance   with built in role based access control  audit logs  and dynamic serving rules  your compliance logic can be enforced directly by featureform we just launched our first open source version and would love to hear your thoughts and what you d like to see    repo   https docs   https blog post   https   www featureform com post feature stores explained the three common architectures  https   www featureform com post feature stores explained the three common architectures 
252,252,JasonTheCoders,v8c2ie,[P] Virtual Background project (feat. The Rock with Alpacas) with PyTorch Implementation,"&#x200B;

[The Rock with Alpaca](https://preview.redd.it/esp4ijnh1k491.png?width=864&format=png&auto=webp&s=60e8fbe3927c45f03feddae2a09c2f4637376c5c)

Hey Guys!

Recently, I worked on a side project that generates **virtual background** (like the one in Zoom) with semantic segmentation. I used [BiSeNet](https://arxiv.org/abs/1808.00897) as a base model.

My goal was to implement everything from scratch without using any fancy libraries and it works pretty well!

You can test it on either a single image or a real-time webcam. Feel free to leave comments for any feedback!

&#x200B;

Project GitHub Repo: [Link](https://github.com/noisrucer/virtual-background)

BiSeNet detailed review: [my blog](https://noisrucer.github.io/paper/bisenet/) 

If you want to see other research paper implementations check out my [repo](https://github.com/noisrucer/deep-learning-papers)!",1,3,2022-06-09 13:55:03, p  virtual background project  feat  the rock with alpacas  with pytorch implementation,  xb  the rock with alpaca  https hey guys recently  i worked on a side project that generates   virtual background    like the one in zoom  with semantic segmentation  i used  bisenet  https my goal was to implement everything from scratch without using any fancy libraries and it works pretty well you can test it on either a single image or a real time webcam  feel free to leave comments for any feedback   xb project github repo   link  https bisenet detailed review   my blog  https if you want to see other research paper implementations check out my  repo  https   github com noisrucer deep learning papers  
253,253,juliensalinas,v8emw0,[D] Use conversational AI based on GPT-J/GPT-NeoX in Discord,"Hello all,

It  is very easy to build a chatbot in a Discord server thanks to great AI  models like GPT-3, GPT-J, and GPT-NeoX. In this article, we I'm showing  you how to code your own conversational bot in Node.js by using GPT-J  and GPT-NeoX through the NLP Cloud API:

[https://nlpcloud.io/build-gpt-j-gpt-neox-discord-chatbot-with-nlpcloud.html](https://nlpcloud.io/build-gpt-j-gpt-neox-discord-chatbot-with-nlpcloud.html?utm_source=reddit&utm_campaign=0a5u9995-ld8e-21eb-ca7-5242ac13d5ja)

As  you might know, these AI models are ""stateless"", meaning that they  can't remember the chat history. So I am showing how to handle this by  automatically re-sending the chat history in each request, and by  truncating the history when it is too long.

If you have questions please don't hesitate to ask. I hope it will be useful!

Julien",0,0,2022-06-09 16:54:26, d  use conversational ai based on gpt j gpt neox in discord,hello all it  is very easy to build a chatbot in a discord server thanks to great ai  models like gpt   gpt j  and gpt neox  in this article  we i m showing  you how to code your own conversational bot in node js by using gpt j  and gpt neox through the nlp cloud api  https as  you might know  these ai models are stateless  meaning that they  can t remember the chat history  so i am showing how to handle this by  automatically re sending the chat history in each request  and by  truncating the history when it is too long if you have questions please don t hesitate to ask  i hope it will be useful julien
254,254,tomin_tomen,v7r6a7,[Discussion] Should we still fly to conferences?,"Now that COVID appears to be less of a problem in many parts of the world, conferences are gradually returning to a physical format. But something has changed: we now know that online conferences are possible.

Many here have probably had a mixed (very negative?) experience with the virtual conferences. It probably hasn't yet reached its best form to foster collaboration for the worldwide research community. But what would have been almost unimaginable before 2020 has now been tested repeatedly! This brings me to my question:

**Should we still burn insane amounts of plane fuel to fly to the other end of the planet to present a paper/poster a come back home 3 days later?**

Also, as a Ph.D. student, should I refuse to attend a conference because it is too far from where I work, knowing that this might hurt my career, make me miss internship or post-doc opportunities, etc.?

This is, of course, not specific to the Machine Learning community. Still, it seems that the size of the community and the frequency of events worldwide make the problem particularly serious here.

With NeurIPS in New Orleans and AAAI in Atlanta this year, the carbon footprint of a European attending these conferences in person easily passes 2 tCO2. Depending on estimates, this puts you on the order of magnitude of the maximum amount of CO2 every person can emit in a year if we are to limit global warming to +2Â°C (I'm not precise with the numbers, but it's the orders of magnitude that matter for my point).

I don't have a good answer to this... I also feel that these past few years have not been the best for networking, meeting new researchers, and sharing ideas. However, the current model is in complete dissonance with my convictions and what I believe to be a sustainable lifestyle. Maybe we should preferably attend a conference close to where we work, and force ourselves to use less CO2 emitting ways of getting there.

I welcome the fact that many conferences have recently organized hybrid events, where it is possible to attend virtually OR physically. This makes them accessible to many more people, especially those with fewer resources. But I donâ€™t know if this is just a temporary thing until the pandemic ends, and it is clear that you donâ€™t get the same â€œexperienceâ€ as people attending physically (FOMO).

What do you all think about this?",41,36,2022-06-08 20:09:27, discussion  should we still fly to conferences ,now that covid appears to be less of a problem in many parts of the world  conferences are gradually returning to a physical format  but something has changed  we now know that online conferences are possible many here have probably had a mixed  very negative   experience with the virtual conferences  it probably hasn t yet reached its best form to foster collaboration for the worldwide research community  but what would have been almost unimaginable before  has now been tested repeatedly  this brings me to my question   should we still burn insane amounts of plane fuel to fly to the other end of the planet to present a paper poster a come back home  days later   also  as a ph d  student  should i refuse to attend a conference because it is too far from where i work  knowing that this might hurt my career  make me miss internship or post doc opportunities  etc  this is  of course  not specific to the machine learning community  still  it seems that the size of the community and the frequency of events worldwide make the problem particularly serious here with neurips in new orleans and aaai in atlanta this year  the carbon footprint of a european attending these conferences in person easily passes  tco  depending on estimates  this puts you on the order of magnitude of the maximum amount of co every person can emit in a year if we are to limit global warming to   c  i m not precise with the numbers  but it s the orders of magnitude that matter for my point  i don t have a good answer to this    i also feel that these past few years have not been the best for networking  meeting new researchers  and sharing ideas  however  the current model is in complete dissonance with my convictions and what i believe to be a sustainable lifestyle  maybe we should preferably attend a conference close to where we work  and force ourselves to use less co emitting ways of getting there i welcome the fact that many conferences have recently organized hybrid events  where it is possible to attend virtually or physically  this makes them accessible to many more people  especially those with fewer resources  but i don t know if this is just a temporary thing until the pandemic ends  and it is clear that you don t get the same  experience  as people attending physically  fomo  what do you all think about this 
255,255,regalalgorithm,v813pk,[P] GPT3 generation of news stories about AI,"Here's a fun little project I did today on a whim. I happen to have access to the OpenAI API, so I used their playground feature to generate AI headlines with their taglines. I fed it this prompt (sourced from the latest edition of [Last Week in AI](https://lastweekin.ai/); I co-run it, apologies for the plug):

> Last week's top AI news:   
\* Caltech unit creates AI helping drones to withstand violent winds - ""Caltech researchers are developing a drone with rapidly reacting artificial intelligence (AI) capacities that allow it to adapt in flight to extreme wind similar to tornado or hurricane conditions.""   
\* How Deep Squeak, an AI program with a weird name, is detecting whales - ""Artificial Intelligence is booming. And now an AI program is being used to search for whales.""   
\* Ex-golf pro links with Seattle-area AI experts on app that uses 3D motion analysis to improve game - ""Before you take yet another mulligan, consider that the tool to improving your golf game might not be found in your bag of clubs."" 

And it did a pretty good job coming up with random other news stories just by generating text from the prompt: 

> \* Googleâ€™s DeepMind creates AI that can beat humans at hide-and-seek - ""A new artificial intelligence program developed by Googleâ€™s DeepMind can beat humans at hide-and-seek, according to a new paper.""   
\* Researchers Develop AI System That Can Learn New Skills by Watching YouTube Videos - ""Researchers at the University of Maryland have developed an artificial intelligence system that can learn new skills by watching YouTube videos.""   
\* Carnegie Mellon Researchers Develop AI System That Can Help You Write Better Emails - ""Researchers at Carnegie Mellon University have developed an artificial intelligence system that can help you write better emails.""   
\* New AI System Can Help You Find a Job Youâ€™ll Love - ""A new artificial intelligence system developed by researchers at the University of Toronto can help you find a job youâ€™ll love, according to a new study."" 

Though, I was surprised to find it repeats itself super quickly. Would have thought GPT-3 had solved this very basic language model problem... It also seems to repeat actual news stories from the past, such as:

>\* Google DeepMind's AlphaGo Zero AI can teach itself Go and other games in hours - ""DeepMind has created an artificial intelligence system that can not only defeat humans at the game of Go, but also teach itself to play from scratch within hours.""  

I also tried prompting it with just ""Last week's top AI news:"" and got the following:

> 1. Google's DeepMind has created a new algorithm that can predict how proteins will fold.    
>  
>2. Facebook's artificial intelligence research lab has developed a new system that can automatically generate 3D models of objects from 2D images.    
>  
>3. IBM has announced that its Watson AI platform will now be available to developers on the IBM Cloud.    
>  
>4. Microsoft has released a new toolkit that allows developers to build chatbots using the company's Azure cloud platform.  

Lastly, how about ""This week's trending AI papers"" as a prompt. Also not bad:

> 1. ""Deep Learning for Recommender Systems""    
>  
>2. ""A Neural Network Approach to Context-Aware Query Suggestion""    
>  
>3. ""Learning Deep Representations of Fine-Grained Visual Concepts""    
>  
>4. ""Show, Attend and Tell: Neural Image Caption Generation with Visual Attention""   


A while back I fine-tuned GPT-2 on 100 weeks' worth of AI news (see this [post](https://www.skynettoday.com/digests/ai-news-analysis)), and got some comparatively more amusing stuff:

>Researchers develop a traffic light for self-driving cars  - A pair of new papers from University of Tokyo Institute of  Technology researchers and the World Health Organization claim to have  addressed one of the great challenges of artificial intelligence:  traffic lights. Thanks to machine learning, they can illuminate traffic  for much more effective and safer operation than they could on paper.    
  
>  
>How I used NLP's GPT-3 to write the AI I Created  - When it comes to creating AI, there are 2 main choices I have:-  write a clean code or- just use the output from the original app to  clean code.    
  
>  
>Meet Microsoft's first AI-powered coffee machine - Jeff Dean, chair of Microsoftâ€™s artificial intelligence (AI)  division, recently sat down with WIRED senior writer Will Knight to  discuss the value of building products that do a lot of good, but donâ€™t  always do much of anything else.  

That's about it, nothing too fancy but kind of fun. Feel free to suggest other little experiments to try!",1,9,2022-06-09 03:28:15, p  gpt generation of news stories about ai,here s a fun little project i did today on a whim  i happen to have access to the openai api  so i used their playground feature to generate ai headlines with their taglines  i fed it this prompt  sourced from the latest edition of  last week in ai  https   last week s top ai news       caltech unit creates ai helping drones to withstand violent winds   caltech researchers are developing a drone with rapidly reacting artificial intelligence  ai  capacities that allow it to adapt in flight to extreme wind similar to tornado or hurricane conditions       how deep squeak  an ai program with a weird name  is detecting whales   artificial intelligence is booming  and now an ai program is being used to search for whales       ex golf pro links with seattle area ai experts on app that uses d motion analysis to improve game   before you take yet another mulligan  consider that the tool to improving your golf game might not be found in your bag of clubs  and it did a pretty good job coming up with random other news stories just by generating text from the prompt       google s deepmind creates ai that can beat humans at hide and seek   a new artificial intelligence program developed by google s deepmind can beat humans at hide and seek  according to a new paper       researchers develop ai system that can learn new skills by watching youtube videos   researchers at the university of maryland have developed an artificial intelligence system that can learn new skills by watching youtube videos       carnegie mellon researchers develop ai system that can help you write better emails   researchers at carnegie mellon university have developed an artificial intelligence system that can help you write better emails       new ai system can help you find a job you ll love   a new artificial intelligence system developed by researchers at the university of toronto can help you find a job you ll love  according to a new study  though  i was surprised to find it repeats itself super quickly  would have thought gpt  had solved this very basic language model problem    it also seems to repeat actual news stories from the past  such as     google deepmind s alphago zero ai can teach itself go and other games in hours   deepmind has created an artificial intelligence system that can not only defeat humans at the game of go  but also teach itself to play from scratch within hours   i also tried prompting it with just last week s top ai news  and got the following     google s deepmind has created a new algorithm that can predict how proteins will fold           facebook s artificial intelligence research lab has developed a new system that can automatically generate d models of objects from d images           ibm has announced that its watson ai platform will now be available to developers on the ibm cloud           microsoft has released a new toolkit that allows developers to build chatbots using the company s azure cloud platform   lastly  how about this week s trending ai papers as a prompt  also not bad     deep learning for recommender systems          a neural network approach to context aware query suggestion          learning deep representations of fine grained visual concepts          show  attend and tell  neural image caption generation with visual attention   a while back i fine tuned gpt  on  weeks  worth of ai news  see this  post  https  researchers develop a traffic light for self driving cars    a pair of new papers from university of tokyo institute of  technology researchers and the world health organization claim to have  addressed one of the great challenges of artificial intelligence   traffic lights  thanks to machine learning  they can illuminate traffic  for much more effective and safer operation than they could on paper           how i used nlp s gpt  to write the ai i created    when it comes to creating ai  there are  main choices i have    write a clean code or  just use the output from the original app to  clean code           meet microsoft s first ai powered coffee machine   jeff dean  chair of microsoft s artificial intelligence  ai   division  recently sat down with wired senior writer will knight to  discuss the value of building products that do a lot of good  but don t  always do much of anything else   that s about it  nothing too fancy but kind of fun  feel free to suggest other little experiments to try 
256,256,pilooch,v7rahw,"[P] Real-time AR for jewelry virtual try on that looks real, done with joliGAN, based on a few 2D videos and no 3D model","A work from us with GANs recently emerged from stealth [https://www.linkedin.com/feed/update/urn:li:activity:6939837590304899072/](https://www.linkedin.com/feed/update/urn:li:activity:6939837590304899072/)

The hands are real, but the rings are rendered with a GAN in real-time. A first network detects where to render the ring, a second network does the rendering. There's no 3D model, it's purely 2D to 2D.

&#x200B;

https://preview.redd.it/9qlgbkyeue491.png?width=1936&format=png&auto=webp&s=ceadda604db236dd3f7d8b665843e786512128b8

We thought we'd share some technical details since the underlying code, JoliGAN is Open Source, [https://github.com/jolibrain/joliGAN](https://github.com/jolibrain/joliGAN)

\- The GAN uses a combination of mobile ResNets with attention as a Generator, along with a projected Discriminator \[1\]. Depending on the stone, we sometimes use transformers as well (customized Segformers and ViT mostly). A series of additional neural networks act as semantic constraints to the space of GAN transforms.

\- Real-time is achieved through our full C++ Open Source backend DeepDetect, [https://github.com/jolibrain/deepdetect](https://github.com/jolibrain/deepdetect). We use CUDA along with OpenCV and TensorRT to chain multiple models (ring detection and generator mostly), and we make sure the data remain within CUDA memory at all time. This allows us to reach \~60 FPS on 1080Ti and 20% more on average on an  RTX3090.

JoliGAN is a powerful tool for domain to domain adaptation, with applications to AR, dataset augmentation, and sim2real transformation mostly. Documentation is scarce as the software is essentially used by us for solving our customers' problems.

But hey, it's open :)

\[1\] [https://arxiv.org/abs/2111.01007](https://arxiv.org/abs/2111.01007)",1,21,2022-06-08 20:14:43, p  real time ar for jewelry virtual try on that looks real  done with joligan  based on a few d videos and no d model,a work from us with gans recently emerged from stealth  https the hands are real  but the rings are rendered with a gan in real time  a first network detects where to render the ring  a second network does the rendering  there s no d model  it s purely d to d   xb https we thought we d share some technical details since the underlying code  joligan is open source   https    the gan uses a combination of mobile resnets with attention as a generator  along with a projected discriminator       depending on the stone  we sometimes use transformers as well  customized segformers and vit mostly   a series of additional neural networks act as semantic constraints to the space of gan transforms    real time is achieved through our full c   open source backend deepdetect   https joligan is a powerful tool for domain to domain adaptation  with applications to ar  dataset augmentation  and simreal transformation mostly  documentation is scarce as the software is essentially used by us for solving our customers  problems but hey  it s open         https   arxiv org abs    https   arxiv org abs   
257,257,CireNeikual,v7ztoe,[P] Lorcan Mini robot running fast with AOgmaNeo reinforcement learning,"Hi everyone,

I decided to write another blog post finally. This one is about a RL demo we gave at a local conference, involving a tiny quadruped robot that learns to scramble across the floor very quickly. It learns by first mimicking a hand-made policy, and is then trained further in the real-world. Our technology is called Sparse Predictive Hierarchies (SPH), and the library that implements it is called AOgmaNeo. It's a biologically-inspired low-compute sparse online learning system. We are also working on a GPU version of SPH again, so I also included that in the post as well. Enjoy!

[https://ogma.ai/2022/06/aogmaneo-lorcan-mini-robot-demo-clogmaneo/](https://ogma.ai/2022/06/aogmaneo-lorcan-mini-robot-demo-clogmaneo/)",0,6,2022-06-09 02:26:57, p  lorcan mini robot running fast with aogmaneo reinforcement learning,hi everyone i decided to write another blog post finally  this one is about a rl demo we gave at a local conference  involving a tiny quadruped robot that learns to scramble across the floor very quickly  it learns by first mimicking a hand made policy  and is then trained further in the real world  our technology is called sparse predictive hierarchies  sph   and the library that implements it is called aogmaneo  it s a biologically inspired low compute sparse online learning system  we are also working on a gpu version of sph again  so i also included that in the post as well  enjoy  https   ogma ai   aogmaneo lorcan mini robot demo clogmaneo   https   ogma ai   aogmaneo lorcan mini robot demo clogmaneo  
258,258,ilia10000,v7sl9p,[D] Looking for paper on infinite stacking of hyperparam optimizers,"A few years ago, I remember seeing a paper on using optimizers to optimize optimizers. The initial premise was that if you have a model and an optimizer, you need to optimize the hyperparams of the optimizer so you can add a sort of hyperoptimizer on top. But this hyperoptimizer also has hyperparams so they then explore what happens when you start stacking more and more of these hyperoptimizers on top of each other. I believe one of the conclusions was that in the limit, model behaviour ends up being independent of the top-level choice of hyperparameters.
I've been trying to find this paper again recently but haven't been able to.
Would greatly appreciate any help finding it!",2,14,2022-06-08 21:12:33, d  looking for paper on infinite stacking of hyperparam optimizers,a few years ago  i remember seeing a paper on using optimizers to optimize optimizers  the initial premise was that if you have a model and an optimizer  you need to optimize the hyperparams of the optimizer so you can add a sort of hyperoptimizer on top  but this hyperoptimizer also has hyperparams so they then explore what happens when you start stacking more and more of these hyperoptimizers on top of each other  i believe one of the conclusions was that in the limit  model behaviour ends up being independent of the top level choice of hyperparameters i ve been trying to find this paper again recently but haven t been able to would greatly appreciate any help finding it 
259,259,loeffner,v7r5d3,[Discussion] Why is the Competing Conventions Problem in Neuroevolution a problem?,"The Competing Conventions problem or Permutation problem is a problem that occurs in neuroevolution. It arises when there are more than one way to represent a network as a genotype. 

[The competing conventions problem. \[Evolving neural networks through augmenting topologies; Stanley, Miikulainen; 2002\]](https://preview.redd.it/2wth86etpe491.png?width=460&format=png&auto=webp&s=73a6bd606e5c8c7bfc181177861f3af19e7dc07a)

When two different genotypes, that represent the same neural network, are recombined during crossover, the emerging offspring is likely to be damaged and missing information. The figure above visualizes the problem for a small neural network. Since the order of the three hidden nodes A, B and C has no influence on the resulting function, the network can be represented by 3! = 6 different permutations. When two of these permutations are recombined during crossover the resulting offspring is missing information. As depicted in the figure the combination of {A, B, C\] and \[C, B, A\] will result in either \[A, B, A\] or \[C, B, C\]. Both of which lack 1/3 of the main components that both their parents had. 

There is also the problem, that the search space is enourmosly enlarged by all the permutations, but my question refers to the first part of the problem. 

&#x200B;

Why is it a problem, that the children of two genotype permutations of the same underlying neural network miss information from their parents. From my understanding, the point of crossover is also exploration, so why are these networks considered damaged, while in other situation it is considered innovation? Offspring is supposed to be different from its parents, otherwise change would only happen through mutations and be completely random.

I have tried to find an explanation, but every paper just seems to see it as a given that the offspring is damaged.",16,14,2022-06-08 20:08:17, discussion  why is the competing conventions problem in neuroevolution a problem ,the competing conventions problem or permutation problem is a problem that occurs in neuroevolution  it arises when there are more than one way to represent a network as a genotype   the competing conventions problem    evolving neural networks through augmenting topologies  stanley  miikulainen      https when two different genotypes  that represent the same neural network  are recombined during crossover  the emerging offspring is likely to be damaged and missing information  the figure above visualizes the problem for a small neural network  since the order of the three hidden nodes a  b and c has no influence on the resulting function  the network can be represented by      different permutations  when two of these permutations are recombined during crossover the resulting offspring is missing information  as depicted in the figure the combination of  a  b  c   and   c  b  a   will result in either   a  b  a   or   c  b  c    both of which lack   of the main components that both their parents had  there is also the problem  that the search space is enourmosly enlarged by all the permutations  but my question refers to the first part of the problem    xb why is it a problem  that the children of two genotype permutations of the same underlying neural network miss information from their parents  from my understanding  the point of crossover is also exploration  so why are these networks considered damaged  while in other situation it is considered innovation  offspring is supposed to be different from its parents  otherwise change would only happen through mutations and be completely random i have tried to find an explanation  but every paper just seems to see it as a given that the offspring is damaged 
260,260,KaleidoscopeBest1569,v7ybk1,[R] Reading list of #ImplicitRepresentations and #NeRF papers relating to #Robotics,Interested in a reading list of [\#ImplicitRepresentations](https://twitter.com/hashtag/ImplicitRepresentations?src=hashtag_click) and [\#NeRF](https://twitter.com/hashtag/NeRF?src=hashtag_click) papers relating to [\#Robotics](https://twitter.com/hashtag/Robotics?src=hashtag_click)?  Check out this list of papers inspired by awesome-computer-vision. [https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Roboticsâ€¦](https://t.co/3VgDUdCbCi)  Feel free to share with others! Contributions/Suggestions are welcome.,0,6,2022-06-09 01:21:03, r  reading list of  implicitrepresentations and  nerf papers relating to  robotics,interested in a reading list of    implicitrepresentations  https   twitter com hashtag implicitrepresentations src hashtag_click  and    nerf  https   twitter com hashtag nerf src hashtag_click  papers relating to    robotics  https   twitter com hashtag robotics src hashtag_click    check out this list of papers inspired by awesome computer vision   https   github com zubair irshad awesome implicit nerf robotics   https   t co vgdudcbci   feel free to share with others  contributions suggestions are welcome 
261,261,hardmaru,v7emri,[R] Differentiable Finite State Machines (Blog Post),,0,86,2022-06-08 07:59:35, r  differentiable finite state machines  blog post ,nan
262,262,emilec___,v7lkxs,"[P][N] Just launched - nebulgym, a new open-source that accelerates AI training (~1.5-2x as of now) in a few lines of code without requiring you to change your training setup","Training always takes too long. If it takes an hour, it would be better if it took 30 minutes, or maybe 15 minutes... or just 1 minute, why not? And if you want to speed up training, the techs available usually require to increase the complexity of the training process, whether it's making trade-off in terms of accuracy or time for the developer to learn a new framework. Often times it's trial and error, playing with parameters, training recipes, or switching framework/model. That's definitely not ideal.

â€œFast & easy-to-useâ€ These were keywords that motivated me to work on a new way of doing training, the library `nebulgym`, which now is open-source ([github link](https://github.com/nebuly-ai/nebulgym)).

**Fast**

Training should be fast, period. Wouldn't it be great if in the near future you could train a GPT3 from scratch on your laptop? Or a large EfficientNet in a fraction of a minute? Nebulgym was built to try to bring developers closer to that future. This open-source optimizes the full training computation stack, from efficient data loading to faster forward and backward passes and earlier convergence. For example, by saving data samples in the cache on the first data read, it speeds up the full data loading process and eliminates what can become the bottleneck for the training process. Nebulgym also leverages techniques such as partial compilation of some calculations and smart sparse gradients to speed up forward and backward gradient propagations. And many more features will be implemented soon. And please let me know / open issues if you have ideas for making nebulgym even faster :)

**Easy-to-use**

""Not another framework, please, there're already 1000"". That's a call for help from many developers, so nebulgym has been developed with this in mind. Nebulgym let you use the training setup you've always used, and works ""on top"". This is made possible with the use of class decorators (like Java's annotations). In short, you can just add these decorators before defining the model classes, and nebulgym will make sure that you use your computing resources to the fullest.

Here's a snippet of training with nebulgym decorators (`@accelerate_dataset` and `@accelerate_model`)

```
import torch
from nebulgym.decorators.torch_decorators import accelerate_model, accelerate_dataset

@accelerate_dataset()
class MyDataset(torch.utils.data.Dataset):
   # Your Dataset definition

@accelerate_model()
class MyModel(torch.nn.Module):
   # Your model definition

# Train your model as you usually do
```

And that's it. Give it a try, and leave a star â­, it's a little contribution to show some love for open-source projects :) Also feedback would be super appreciated!

[https://github.com/nebuly-ai/nebulgym](https://github.com/nebuly-ai/nebulgym)",12,19,2022-06-08 15:07:33, p  n  just launched   nebulgym  a new open source that accelerates ai training     x as of now  in a few lines of code without requiring you to change your training setup,training always takes too long  if it takes an hour  it would be better if it took  minutes  or maybe  minutes    or just  minute  why not  and if you want to speed up training  the techs available usually require to increase the complexity of the training process  whether it s making trade off in terms of accuracy or time for the developer to learn a new framework  often times it s trial and error  playing with parameters  training recipes  or switching framework model  that s definitely not ideal  fast   easy to use  these were keywords that motivated me to work on a new way of doing training  the library  nebulgym   which now is open source   github link  https   fast  training should be fast  period  wouldn t it be great if in the near future you could train a gpt from scratch on your laptop  or a large efficientnet in a fraction of a minute  nebulgym was built to try to bring developers closer to that future  this open source optimizes the full training computation stack  from efficient data loading to faster forward and backward passes and earlier convergence  for example  by saving data samples in the cache on the first data read  it speeds up the full data loading process and eliminates what can become the bottleneck for the training process  nebulgym also leverages techniques such as partial compilation of some calculations and smart sparse gradients to speed up forward and backward gradient propagations  and many more features will be implemented soon  and please let me know   open issues if you have ideas for making nebulgym even faster     easy to use  not another framework  please  there re already   that s a call for help from many developers  so nebulgym has been developed with this in mind  nebulgym let you use the training setup you ve always used  and works on top  this is made possible with the use of class decorators  like java s annotations   in short  you can just add these decorators before defining the model classes  and nebulgym will make sure that you use your computing resources to the fullest here s a snippet of training with nebulgym decorators    accelerate_dataset  and   accelerate_model     import torchfrom nebulgym decorators torch_decorators import accelerate_model  accelerate_dataset accelerate_dataset  class mydataset torch utils data dataset       your dataset definition accelerate_model  class mymodel torch nn module       your model definition  train your model as you usually do   and that s it  give it a try  and leave a star    it s a little contribution to show some love for open source projects    also feedback would be super appreciated  https   github com nebuly ai nebulgym  https   github com nebuly ai nebulgym 
263,263,Scapius,v7fd9r,[D] ML/DL computer build with PCIe 5.0 x8 lanes for RTX 3090,"I'm building my first ML/DL computer around [ASUS ProArt Z690 motherboard](https://www.asus.com/Motherboards-Components/Motherboards/ProArt/ProArt-Z690-CREATOR-WIFI/techspec/),  which has 2 PCIe 5.0 slots (x8 each) and PCIe 3.0 x16 slot. The CPU is  i9-12900K, which comes with 20 PCIe lanes. Since 4 lanes will go to a  single NVMe, I think the motherboard will split the two PCIe 5.0 slots  into 8x lanes each. My current build is with a single RTX 3080 12GB, but  I want to be able to upgrade to 2x3090Ti (or 2x4090) in the future, if  needed. This [article](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/)  from 2018 seems to imply that DL is unaffected even when PCIe 4.0 4x  are used for up to 2 GPUs. I just want to confirm that this is still  considered sound advice. In other words, 2x3090 GPUs won't be throttled  by PCIe 5.0 x8 lanes, which is equivalent to PCIe 4.0 x16 (see matrix  below). In fact, I'm also wondering if running PCIe 5.0 even at x2 lanes  each won't throttle the GPUs since the equivalent transfer rate is  still PCIe 4.0 x4, as mentioned in that article. Or does the fact that  the motherboard interface is PCIe 5.0 not matter since the GPU can  support only up to PCIe 4.0 speeds?

Any other comments on my build would be welcome: [PCPartpicker](https://pcpartpicker.com/list/Kdvxfv).  This will be an everyday computer as well to edit photos/videos and  used for other analyses, hence the more powerful CPU and the NVMe, which  might not matter as much for ML.

&#x200B;

[ PCIe Lane vs Speed matrix ](https://preview.redd.it/l25d97fv6b491.png?width=512&format=png&auto=webp&s=c7741b6e775e87a405d3fb77c52201290294501b)",32,33,2022-06-08 08:36:59, d  ml dl computer build with pcie   x lanes for rtx ,i m building my first ml dl computer around  asus proart z motherboard  https any other comments on my build would be welcome   pcpartpicker  https   xb   pcie lane vs speed matrix   https   preview redd it ldfvb png width  format png auto webp s cbeeadfbcb 
264,264,hardmaru,v7d4bx,[R] From data to functa: Your data point is a function and you can treat it like one,,10,28,2022-06-08 06:42:56, r  from data to functa  your data point is a function and you can treat it like one,nan
265,265,Commercial_Put577,v7nqo1,Measuring distances from known objects [P]," I am a member of a Formula Student team that is building its first autonomous race car. Our track limits are defined by cones of known size placed on each side of the road, yellow on the right-hand side and blue on the left (see Images). Naturally, we are interested in measuring our distance from them so that we can map the circuit. I want your opinion on which method would yield the most accurate results. What we are currently doing is running Yolo(v5) to extract bounding boxes and then each box goes through an additional neural network that outputs 7 keypoints of the cone (see Images) and just because we know the exact positions of these keypoints relative to each other we can then turn it into a Perspective n-Point problem. 

https://preview.redd.it/nzbv4m1byd491.png?width=1218&format=png&auto=webp&s=f3f605767109cfc2a8e41d9b279779c542e27b10

https://preview.redd.it/n0y28oi9yd491.png?width=200&format=png&auto=webp&s=a90057af43b3f185e6fdd4755037505eee96c0b9",14,4,2022-06-08 17:19:25,measuring distances from known objects  p , i am a member of a formula student team that is building its first autonomous race car  our track limits are defined by cones of known size placed on each side of the road  yellow on the right hand side and blue on the left  see images   naturally  we are interested in measuring our distance from them so that we can map the circuit  i want your opinion on which method would yield the most accurate results  what we are currently doing is running yolo v  to extract bounding boxes and then each box goes through an additional neural network that outputs  keypoints of the cone  see images  and just because we know the exact positions of these keypoints relative to each other we can then turn it into a perspective n point problem  https https   preview redd it nyoiyd png width  format png auto webp s aafbfefddeeecb
266,266,pizzaUnderSea,v7f9m1,[R] What are some interesting and mysterious open problems of generalization in ML?,"I found the generalization problems of machine learning, especially in deep learning, very attractive, I wonder what are some attractive problems nowadays. 

I know about the [double descent](https://openai.com/blog/deep-double-descent/) problem, which I believe is quite interesting, and does not have a valid answer at this moment. 

I also know about the implicit inductive bias introduced by SGD, but it seems has been studied widely recently especially with the tool of NTK. 

I wonder what are some other interesting phenomenon like these mysteries?",10,16,2022-06-08 08:31:39, r  what are some interesting and mysterious open problems of generalization in ml ,i found the generalization problems of machine learning  especially in deep learning  very attractive  i wonder what are some attractive problems nowadays  i know about the  double descent  https i also know about the implicit inductive bias introduced by sgd  but it seems has been studied widely recently especially with the tool of ntk  i wonder what are some other interesting phenomenon like these mysteries 
267,267,jis478,v83auy,[P] WebtoonMe Project: Selfie to Webtoon style (you can try the demo app for free),"[https://www.reddit.com/r/MachineLearning/comments/sfbtds/p\_webtoonme\_project\_selfie\_to\_webtoon\_style/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/sfbtds/p_webtoonme_project_selfie_to_webtoon_style/?utm_source=share&utm_medium=web2x&context=3)

project page: [https://github.com/webtoon/WebtoonMe](https://github.com/webtoon/WebtoonMe)

demo page: [https://webtoon.github.io/WebtoonMe/app.html](https://webtoon.github.io/WebtoonMe/app.html)",0,0,2022-06-09 05:14:09, p  webtoonme project  selfie to webtoon style  you can try the demo app for free , https project page   https demo page   https   webtoon github io webtoonme app html  https   webtoon github io webtoonme app html 
268,268,Maxtoq,v7sywj,[D][P] Grounding language to visual observation,"Hi,

In my current project, I have a **language observation** and a **visual observation** that I would like to encode, both to the **same context embedding**.

The language observation is a description of the visual observation. The goal is to **ground the language** in the observation.

Ultimately, I need to have one **Observation Encoder** and one **Language Encoder** that take different inputs, but both output similar context vectors.

What would be a technique to make that possible ?

My  first idea was to learn the Observation Encoder on another task, and  then teach the Language Encoder to predict the same context vector as  the Observation Encoder (minimizing cross-entropy). But there may be  some better approach, maybe using techniques I'm not aware of.

I looked briefly into Shared Latent Spaces, but was not sure that it would fit my problem statement. Was I wrong ?

Do you guys know any other method I could look into ?

Thanks !",4,1,2022-06-08 21:29:55, d  p  grounding language to visual observation,hi in my current project  i have a   language observation   and a   visual observation   that i would like to encode  both to the   same context embedding   the language observation is a description of the visual observation  the goal is to   ground the language   in the observation ultimately  i need to have one   observation encoder   and one   language encoder   that take different inputs  but both output similar context vectors what would be a technique to make that possible  my  first idea was to learn the observation encoder on another task  and  then teach the language encoder to predict the same context vector as  the observation encoder  minimizing cross entropy   but there may be  some better approach  maybe using techniques i m not aware of i looked briefly into shared latent spaces  but was not sure that it would fit my problem statement  was i wrong  do you guys know any other method i could look into  thanks  
269,269,mingaflo,v6x06e,[P] A shared arxiv-PDF-viewer,"What if you could read a paper and at the same time have a scientific discussion about certain paragraphs or figures.  
Just mark the sentence or picture and create a new thread about it, ask a question, explain something in greater detail or link to a blog post that  explains a concept better.   


I think that would be awesome and a win-win for the authors and the readers.  


I am kind of a scientist myself and I would love to see something like it. Does something like that exist? If not, I would like to make that a (shared) project.   


Looking forward to your suggestions!",14,77,2022-06-07 19:53:02, p  a shared arxiv pdf viewer,what if you could read a paper and at the same time have a scientific discussion about certain paragraphs or figures   just mark the sentence or picture and create a new thread about it  ask a question  explain something in greater detail or link to a blog post that  explains a concept better    i think that would be awesome and a win win for the authors and the readers   i am kind of a scientist myself and i would love to see something like it  does something like that exist  if not  i would like to make that a  shared  project    looking forward to your suggestions 
270,270,sarmientoj24,v7hapx,[D] What object detectors have the capability to harness relationship between its detected boxes?,"Typical object detectors do not employ relationships within the detected boxes. No context is being involved.

In my problem's case, there are two requirements that would lead to drastically better results if some form of **context** is formed across detected boxes.

**Requirement #1**

It is a multi-class, but single label problem. There are ***N*** classes. But the class can only appear **minimum of 0 and maximum of 1 instance.** Hence, it kinda needs to know the other detections whether they have already predicted something.

**Requirement #2**

There is some form of ordinance between the predictions based on their proximity to each other. For example, Class 4 should only appear near Class 5-6 and Class 2-3. But should not be anywhere near Class 32.

Any architecture that is optimized for this kinds of object detection?",0,4,2022-06-08 10:25:59, d  what object detectors have the capability to harness relationship between its detected boxes ,typical object detectors do not employ relationships within the detected boxes  no context is being involved in my problem s case  there are two requirements that would lead to drastically better results if some form of   context   is formed across detected boxes   requirement    it is a multi class  but single label problem  there are    n    classes  but the class can only appear   minimum of  and maximum of  instance    hence  it kinda needs to know the other detections whether they have already predicted something   requirement    there is some form of ordinance between the predictions based on their proximity to each other  for example  class  should only appear near class   and class    but should not be anywhere near class  any architecture that is optimized for this kinds of object detection 
271,271,MLtinkerer,v79v5z,[D] Quick question for all those who are trying to build stuff with AI/ML,"Quick question for all those who are trying to build stuff with AI/ML  
\-Why do you care/not care about reproducible/usable code/models? i know it's a basic question but i'm trying to dive deeper and understand the underlying reasons about why it matters or doesn't matter to you. (5 whys analysis of this question basically)",19,11,2022-06-08 04:04:38, d  quick question for all those who are trying to build stuff with ai ml,quick question for all those who are trying to build stuff with ai ml    why do you care not care about reproducible usable code models  i know it s a basic question but i m trying to dive deeper and understand the underlying reasons about why it matters or doesn t matter to you    whys analysis of this question basically 
272,272,Singularian2501,v6s1ea,[R] On the Advance of Making Language Models Better Reasoners - 2022 Microsoft,"Paper: [https://arxiv.org/abs/2206.02336](https://arxiv.org/abs/2206.02336)

Abstract:

>Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the final answer, successfully boosting the **GSM8K benchmark from 17.9% to 58.1%** in terms of problem solving rate. In this paper, we propose a new approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance their reasoning capability. DiVeRSe first explores different prompts to enhance the diversity in reasoning paths. Second, DiVeRSe introduces a verifier to distinguish good answers from bad answers for a better weighted voting. Finally, DiVeRSe verifies the correctness of each single step rather than all the steps in a whole. **We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that DiVeRSe can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%), outperforming the PaLM model with 540B parameters.**

https://preview.redd.it/905d5ndrf6491.jpg?width=722&format=pjpg&auto=webp&s=069c7fd4c8039e7d3b542656a295221114552a4e

https://preview.redd.it/7toqjvnrf6491.jpg?width=1136&format=pjpg&auto=webp&s=b0e9c78c9fee38c0828b2c5466679ad14ce2a631

https://preview.redd.it/kcxa0izrf6491.jpg?width=561&format=pjpg&auto=webp&s=aa377cb2a6168bd4ebb213c465dff0b3145397d5",11,75,2022-06-07 16:04:04, r  on the advance of making language models better reasoners    microsoft,paper   https abstract  large language models such as gpt  and palm have shown remarkable performance in few shot learning  however  they still struggle with reasoning tasks such as the arithmetic benchmark gsmk  recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the final answer  successfully boosting the   gsmk benchmark from    to      in terms of problem solving rate  in this paper  we propose a new approach  diverse  diverse verifier on reasoning step   to further advance their reasoning capability  diverse first explores different prompts to enhance the diversity in reasoning paths  second  diverse introduces a verifier to distinguish good answers from bad answers for a better weighted voting  finally  diverse verifies the correctness of each single step rather than all the steps in a whole    we conduct extensive experiments using the latest language model code davinci  and demonstrate that diverse can achieve new state of the art performance on six out of eight reasoning benchmarks  e g   gsmk    to      outperforming the palm model with b parameters   https https https   preview redd it kcxaizrf jpg width  format pjpg auto webp s aacbabdebbcdffbd
273,273,hardmaru,v7dhsd,[R] Intra-agent speech permits zero-shot task acquisition,,0,5,2022-06-08 07:01:55, r  intra agent speech permits zero shot task acquisition,nan
274,274,zakrzzz,v6r57a,[N] Sustainable AI Hackathon,"Are you ready to put your coding skills to the test for the global cause?  

Swiss AI Association and Deep Learning Labs teamed up to make a Sustainable AI Hackathon! 

In the next few days, from 10th to 12th June, we welcome everyone interested in AI, regardless of their specialization or experience level.  If you are looking for opportunities to learn from advanced AI experts - here it is! 

Pick one of three challenges and put your effort into making a difference and just have fun with AI:  ðŸ‘‰ Ethical AI & Decision Making    
ðŸ‘‰ Sustainable Investing   
ðŸ‘‰ Cybersecurity in Fintech    


All participants in the Hackathon will get a cloud voucher from AWS worth 25 USD. And, of course, the best teams will get special rewards.  

Register here - [https://lablab.ai/event/swissai](https://lablab.ai/event/swissai)  

I'll also be happy to answer any of your questions

https://preview.redd.it/2axn6q3756491.png?width=1920&format=png&auto=webp&s=1928df41c84e4550f72215c997dba5faaac3ae09",9,29,2022-06-07 15:03:55, n  sustainable ai hackathon,are you ready to put your coding skills to the test for the global cause   swiss ai association and deep learning labs teamed up to make a sustainable ai hackathon  in the next few days  from th to th june  we welcome everyone interested in ai  regardless of their specialization or experience level   if you are looking for opportunities to learn from advanced ai experts   here it is  pick one of three challenges and put your effort into making a difference and just have fun with ai     ethical ai   decision making      sustainable investing     cybersecurity in fintech    all participants in the hackathon will get a cloud voucher from aws worth  usd  and  of course  the best teams will get special rewards   register here    https i ll also be happy to answer any of your questionshttps   preview redd it axnq png width  format png auto webp s dfcefcdbafaaacae
275,275,Yukiomo,v6ze5g,[R] What is the best summary of neural tangent kernel research thus far?,"Do folks here have good references for a summary in what progress has been made in neural tangent kernel (NTK) research? There's an excellent and approachable blog post about the state of the field in 2018-2019 (https://rajatvd.github.io/NTK/), but I assume that there's been a lot of follow-on work since. Thanks!",1,6,2022-06-07 21:19:53, r  what is the best summary of neural tangent kernel research thus far ,do folks here have good references for a summary in what progress has been made in neural tangent kernel  ntk  research  there s an excellent and approachable blog post about the state of the field in    https   rajatvd github io ntk    but i assume that there s been a lot of follow on work since  thanks 
276,276,IcySnowy,v6zfp6,"[D] How to balance production and research in a project, especially doing it alone.","I need some advice as I want to deliver better results. I'm doing a project with provision from a professor but most of the time I do it alone as he does not have much spare time. He want me to produce some results in image processing like object detection and publish some research to conferences, particularly in ML and CV. But after working for a while, I could not produce any meaningful results and haven't published any paper. Basically I'm struggling in both objectives so I hope I can get some advice here. Should I lean more to production or research? Or should I quit after all?",6,3,2022-06-07 21:21:30, d  how to balance production and research in a project  especially doing it alone ,i need some advice as i want to deliver better results  i m doing a project with provision from a professor but most of the time i do it alone as he does not have much spare time  he want me to produce some results in image processing like object detection and publish some research to conferences  particularly in ml and cv  but after working for a while  i could not produce any meaningful results and haven t published any paper  basically i m struggling in both objectives so i hope i can get some advice here  should i lean more to production or research  or should i quit after all 
277,277,masters-in-phd,v6hmaz,[D] How is the job market for machine learning and AI in Australia? Is it comparable to those in EU major economies and Canada?,"Hello. I'm curious about what the job market for ML or AI is like in Australia. Are there a lot of opportunities there, in general?

I've looked at some ML jobs in UK, Canada, and Germany, and while the market is obviously not as good as that in the US (that's to be expected tbh), it seems pretty good. Is Australia's ML job market comparable to those countries? Or is it much worse? I've heard conflicting experiences on the state of ML jobs in Australia so I was curious.

Thanks!",39,80,2022-06-07 05:42:16, d  how is the job market for machine learning and ai in australia  is it comparable to those in eu major economies and canada ,hello  i m curious about what the job market for ml or ai is like in australia  are there a lot of opportunities there  in general i ve looked at some ml jobs in uk  canada  and germany  and while the market is obviously not as good as that in the us  that s to be expected tbh   it seems pretty good  is australia s ml job market comparable to those countries  or is it much worse  i ve heard conflicting experiences on the state of ml jobs in australia so i was curious thanks 
278,278,lvwerra,v66smh,[P] Releasing ðŸ¤— Evaluate - an evaluation library for ML,"Evaluation is one of the most important aspects of ML but todayâ€™s evaluation landscape is scattered and undocumented which makes evaluation unnecessarily hard.

For that reason we are excited to release ðŸ¤— Evaluate!

[https://github.com/huggingface/evaluate](https://github.com/huggingface/evaluate)

## TL;DR

ðŸ¤— Evaluate is a Python library that let's you evaluate models and datasets with a wide range of tools! The core principles are:

* **reproducibility**: run evaluation and save results in a reproducible manner
* **documentation**: each evaluation module comes with a metric cards documenting it's use and limitations
* **ease-of-use**: same interface for a wide range of evaluation tools
* **coverage**: in addition to model metrics we also include comparisons and measurements (more about this below)
* **multimodal**: the library covers a wide range of evaluations from information retrieval to reinforcement learning
* **community driven**: the community can easily add and use new metrics by adding them on the Hugging Face Hub

## Types of evaluation

ðŸ¤— Evaluate offers three types of evaluation modules:

* **Metric**: A metric is used to evaluate a modelâ€™s performance and usually involves the modelâ€™s predictions as well as some ground truth labels. You can find all integrated metrics at evaluate-metric.
* **Comparison**: A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at evaluate-comparison.
* **Measurement**: The dataset is as important as the model trained on it. With measurements one can investigate a datasetâ€™s properties. You can find all integrated measurements at evaluate-measurement.

## How to use

All three types of evaluation module can be used in the same way:

    import evaluate
    
    accuracy = evaluate.load(""accuracy"", module_type=""metric"")
    accuracy.compute(references=[0, 1], predictions=[1, 1])
    > {""accuracy"": 0.5}

## Community metrics and metric cards

All evaluation modules live on the Hugging Face Hub as a Space and come with an **interactive** **widget**. See for example accuracy: [https://huggingface.co/spaces/evaluate-metric/accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy). 

In addition to the widget where you can play with the metric you it also displays the **metric card** that documents its use and limitations:

&#x200B;

[Metric widget with metric card on the Hub](https://preview.redd.it/8rsn3j8hv0491.png?width=1090&format=png&auto=webp&s=d1013187afabe0ddc4b357d32b9afe57a4fb1e04)

This setup also allows anybody to write their own module, load it to the Hub and share it with their colleagues. With the CLI this only takes a couple of minutes:

    evalute-cli create ""My New Metric""

For more information checkout the guide: [https://huggingface.co/docs/evaluate/creating\_and\_sharing](https://huggingface.co/docs/evaluate/creating_and_sharing)

You can list all available evaluation modules with the following code:

    evaluate.list_evaluation_modules(
      module_type=""comparison"",
      include_community=False, 
      with_details=True)
    
    >>> [{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1},
    ...  {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}]

## The Evaluator

We are really excited about the Evaluator. It's another layer of abstraction that let's you pass a model (wrapped in a pipeline), a dataset, and a metric and runs the whole evaluation for you:

    from transformers import pipeline
    from datasets import load_dataset
    import evaluate
    
    eval = evaluate.evaluator(""text-classification"")
    
    results = eval.compute(model_or_pipeline=pipeline(""text-classification"", model=""lvwerra/distilbert-imdb"", device=0)
    					   data=load_dataset(""imdb"", split=""test"").shuffle().select(range(1000)),
    					   metric=evaluate.load(""accuracy""),
                           label_mapping={""NEGATIVE"": 0, ""POSITIVE"": 1},)
    
    print(results)
    >>> {'accuracy': 0.934}

## Saving and sharing results

Finally, storing and sharing results is important for reproducibility but also for the community to know which models perform well. With `evaluate.save()` you can store all your experiment's hyperparameters and results in one line in a JSON file:

    result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])
    hyperparams = {""model"": ""bert-base-uncased""}
    
    evaluate.save(""./results/"", experiment=""run 42"", **result, **hyperparams)

Furthermore, with `evaluate.push_to_hub()` you can push results directly to the model card on the hub, so you can see them on the right sidebar under ""Evaluation results"" (e.g. see here [https://huggingface.co/philschmid/bart-large-cnn-samsum](https://huggingface.co/philschmid/bart-large-cnn-samsum)).

&#x200B;

[Evaluation results on the Hub](https://preview.redd.it/lvdsho1ov0491.jpg?width=2720&format=pjpg&auto=webp&s=6ed0dc009394acc8fd70570f92cf15d6e19e84c9)

There is also a quick tour in the documentation going into more detail about all this: [https://huggingface.co/docs/evaluate/a\_quick\_tour](https://huggingface.co/docs/evaluate/a_quick_tour)

Give it a try and let us know what you think!  It is just a `pip install evaluate` away ;)",22,178,2022-06-06 21:29:23, p  releasing   evaluate   an evaluation library for ml,evaluation is one of the most important aspects of ml but today s evaluation landscape is scattered and undocumented which makes evaluation unnecessarily hard for that reason we are excited to release   evaluate  https    tl dr  evaluate is a python library that let s you evaluate models and datasets with a wide range of tools  the core principles are     reproducibility    run evaluation and save results in a reproducible manner    documentation    each evaluation module comes with a metric cards documenting it s use and limitations    ease of use    same interface for a wide range of evaluation tools    coverage    in addition to model metrics we also include comparisons and measurements  more about this below     multimodal    the library covers a wide range of evaluations from information retrieval to reinforcement learning    community driven    the community can easily add and use new metrics by adding them on the hugging face hub   types of evaluation  evaluate offers three types of evaluation modules     metric    a metric is used to evaluate a model s performance and usually involves the model s predictions as well as some ground truth labels  you can find all integrated metrics at evaluate metric     comparison    a comparison is used to compare two models  this can for example be done by comparing their predictions to ground truth labels and computing their agreement  you can find all integrated comparisons at evaluate comparison     measurement    the dataset is as important as the model trained on it  with measurements one can investigate a dataset s properties  you can find all integrated measurements at evaluate measurement    how to useall three types of evaluation module can be used in the same way     import evaluate        accuracy   evaluate load accuracy  module_type metric     accuracy compute references       predictions             accuracy       community metrics and metric cardsall evaluation modules live on the hugging face hub as a space and come with an   interactive     widget    see for example accuracy   https in addition to the widget where you can play with the metric you it also displays the   metric card   that documents its use and limitations   xb  metric widget with metric card on the hub  https this setup also allows anybody to write their own module  load it to the hub and share it with their colleagues  with the cli this only takes a couple of minutes     evalute cli create my new metricfor more information checkout the guide   https you can list all available evaluation modules with the following code     evaluate list_evaluation_modules       module_type comparison       include_community false        with_details true                name    mcnemar    type    comparison    community   false   likes                name    exact_match    type    comparison    community   false   likes        the evaluatorwe are really excited about the evaluator  it s another layer of abstraction that let s you pass a model  wrapped in a pipeline   a dataset  and a metric and runs the whole evaluation for you     from transformers import pipeline    from datasets import load_dataset    import evaluate        eval   evaluate evaluator text classification         results   eval compute model_or_pipeline pipeline text classification  model lvwerra distilbert imdb  device      					   data load_dataset imdb  split test  shuffle   select range        					   metric evaluate load accuracy                             label_mapping  negative    positive             print results           accuracy        saving and sharing resultsfinally  storing and sharing results is important for reproducibility but also for the community to know which models perform well  with  evaluate save    you can store all your experiment s hyperparameters and results in one line in a json file     result   accuracy compute references        predictions           hyperparams    model  bert base uncased         evaluate save   results   experiment run     result    hyperparams furthermore  with  evaluate push_to_hub    you can push results directly to the model card on the hub  so you can see them on the right sidebar under evaluation results  e g  see here  https   xb  evaluation results on the hub  https there is also a quick tour in the documentation going into more detail about all this   https give it a try and let us know what you think   it is just a  pip install evaluate  away   
279,279,Intelligent_Gene_283,v6xr5n,"[Discussion] Tracking, running and managing experiments in sandbox environment","Hi everyone,

I'm looking for a system for collecting and sharing KPIs, managing and running experiments on local and remote nodes.

My requirements are:

* Sandbox environment: server and nodes are running in a private network with **no internet access**
* Nice graphs and easy comparisons
* Easy way to share datasets
* Supports local and remote nodes

&#x200B;

Preferred, but not mandatory:

* Open source
* Supports distributed training

&#x200B;

I've heard a lot of good recommendations on ClearML and Weights & Biases.

I tested out ClearML and Weights & Biases to see if they work in a sandbox env, but when the servers tried to validate the free trial license, it failed on connectivity issues.

&#x200B;

Does anyone knows of other Experiment Management system that can work without internet access, and have similar capabilities to ClearML and Weights & Biases?",4,2,2022-06-07 20:20:26, discussion  tracking  running and managing experiments in sandbox environment,hi everyone i m looking for a system for collecting and sharing kpis  managing and running experiments on local and remote nodes my requirements are   sandbox environment  server and nodes are running in a private network with   no internet access    nice graphs and easy comparisons  easy way to share datasets  supports local and remote nodes  xb preferred  but not mandatory   open source  supports distributed training  xb i ve heard a lot of good recommendations on clearml and weights   biases i tested out clearml and weights   biases to see if they work in a sandbox env  but when the servers tried to validate the free trial license  it failed on connectivity issues   xb does anyone knows of other experiment management system that can work without internet access  and have similar capabilities to clearml and weights   biases 
280,280,radi-cho,v6oxlk,[D] [R] Dialogue generation with contrastive objectives,"There are recent studies which demonstrate that fine-tuning a language model with contrastive loss on the token level (maximising similarity between the current token representation and the most probable next tokens in a sequence) can lead to more coherent responses. As an example we can take the paper ""A Contrastive Framework for Neural Text Generation"" - [https://arxiv.org/abs/2202.06417](https://arxiv.org/abs/2202.06417).

I am wondering is it a fruitful direction for research and development to try to implement similar contrastive objectives but on the sentence level for tasks such as open-domain dialogue generation... For example we can have a language model try to complete a conversation and in addition to the language modelling loss, there might be another loss maximising contrast between the expected sequence of gold tokens and some distractors.

One of the most cited papers on the topic of dialogue generation - the *TransferTransfo* does something similar in principle by incorporating a classifier optimised to find the original completion in a pool of 20 randomly sampled distractor sentences ([https://arxiv.org/abs/1901.08149](https://arxiv.org/abs/1901.08149)). So do you think recreating similar architecture which however incorporates contrastive learning can lead to performance improvements? If not, what are your concerns? What are some other suggestions on what can be done as a further research in the field?",1,9,2022-06-07 12:31:23, d   r  dialogue generation with contrastive objectives,there are recent studies which demonstrate that fine tuning a language model with contrastive loss on the token level  maximising similarity between the current token representation and the most probable next tokens in a sequence  can lead to more coherent responses  as an example we can take the paper a contrastive framework for neural text generation    https i am wondering is it a fruitful direction for research and development to try to implement similar contrastive objectives but on the sentence level for tasks such as open domain dialogue generation    for example we can have a language model try to complete a conversation and in addition to the language modelling loss  there might be another loss maximising contrast between the expected sequence of gold tokens and some distractors one of the most cited papers on the topic of dialogue generation   the  transfertransfo  does something similar in principle by incorporating a classifier optimised to find the original completion in a pool of  randomly sampled distractor sentences   https   arxiv org abs    https   arxiv org abs      so do you think recreating similar architecture which however incorporates contrastive learning can lead to performance improvements  if not  what are your concerns  what are some other suggestions on what can be done as a further research in the field 
281,281,SeucheAchat9115,v72zs4,[D] Masking out loss values,"Hey, I would like to start a discussion about following topic. I have a GAN with a Generator and a Discriminator. If I mask out some loss values randomly by lets say putting 10% of the Loss Values randomly to Zero. How does this affect the training? How does the optimizer handle such masking? Because such random masking of the losses creates Spikes in the loss surface or am I completely wrong?",22,1,2022-06-07 23:29:32, d  masking out loss values,hey  i would like to start a discussion about following topic  i have a gan with a generator and a discriminator  if i mask out some loss values randomly by lets say putting   of the loss values randomly to zero  how does this affect the training  how does the optimizer handle such masking  because such random masking of the losses creates spikes in the loss surface or am i completely wrong 
282,282,SnooSketches2908,v6mc3a,"[D] What are the websites to collect text/image data for a new dataset on a particular topic, such as Reddit, Quora etc.?","I am working on a research project, and I need text/image data on a particular topic. I need to do text analysis and maybe build a text model on top of that. Where can I get data on specific topics? Some of the websites I can recall are Reddit, quora. Does anyone know other sources? Please share.",4,12,2022-06-07 09:51:53, d  what are the websites to collect text image data for a new dataset on a particular topic  such as reddit  quora etc  ,i am working on a research project  and i need text image data on a particular topic  i need to do text analysis and maybe build a text model on top of that  where can i get data on specific topics  some of the websites i can recall are reddit  quora  does anyone know other sources  please share 
283,283,vash_stampede08,v72nyu,[D] Can we explain the deep prior regularisation by the differentiation step rather than architecture?,"As the post title says, is it possible to explain the ability of deep prior networks to perform tasks such as image inpainting to the implicit differentiation in the backpropogation rather than the architecture of the network.",3,0,2022-06-07 23:17:06, d  can we explain the deep prior regularisation by the differentiation step rather than architecture ,as the post title says  is it possible to explain the ability of deep prior networks to perform tasks such as image inpainting to the implicit differentiation in the backpropogation rather than the architecture of the network 
284,284,emuccino,v77ynz,[D] Neural Network Layers as Operations on Data Collections Types,"I had an observation recently that I wanted to share / get feedback on.

Many of the a canonical deep learning layer types can be viewed as an operation on one of the basic data collection types used by Python (and other languages).

Dense Layers -> Tuples

Recurrent Layers -> Lists

Attention Layers -> Sets

Graph Neural Network Layers -> Dictionaries

Am I missing any?",4,0,2022-06-08 02:46:36, d  neural network layers as operations on data collections types,i had an observation recently that i wanted to share   get feedback on many of the a canonical deep learning layer types can be viewed as an operation on one of the basic data collection types used by python  and other languages  dense layers    tuplesrecurrent layers    listsattention layers    setsgraph neural network layers    dictionariesam i missing any 
285,285,imaginfinity,v5f8et,[R] Itâ€™s wild to see an AI literally eyeballing raytracing based on 100 photos to create a 3d scene you can step inside â˜€ï¸ Low key getting addicted to NeRF-ing imagery datasetsðŸ¤©,,87,1649,2022-06-05 20:35:54, r  it s wild to see an ai literally eyeballing raytracing based on  photos to create a d scene you can step inside    low key getting addicted to nerf ing imagery datasets ,nan
286,286,l0g1cs,v647bz,"[N] Graphsignal profiler now supports distributed training, automatic tracing and more frameworks.","Hi everyone! We've recently introduced several new features to [Graphsignal Profiler](https://github.com/graphsignal/graphsignal) that I'm very excited to share here.

* In addition to TensorFlow, PyTorch and Keras, the profiler now natively supports [Hugging Face](https://graphsignal.com/blog/benchmarking-and-profiling-hugging-face-training-with-graphsignal/) and [PyTorch Lightning](https://graphsignal.com/blog/benchmarking-profiling-and-monitoring-py-torch-lightning-with-graphsignal/).
* A built in support for distributed training has been added. More info [here](https://graphsignal.com/docs/profiler/distributed-workloads/).
* Trace information (using Chrome trace format) is now automatically available in the profiles.

To try it out, simply follow the [Quick Start](https://graphsignal.com/docs/profiler/quick-start/) guide. Any feedback is welcome!",7,20,2022-06-06 19:30:40, n  graphsignal profiler now supports distributed training  automatic tracing and more frameworks ,hi everyone  we ve recently introduced several new features to  graphsignal profiler  https   in addition to tensorflow  pytorch and keras  the profiler now natively supports  hugging face  https   a built in support for distributed training has been added  more info  here  https   trace information  using chrome trace format  is now automatically available in the profiles to try it out  simply follow the  quick start  https   graphsignal com docs profiler quick start   guide  any feedback is welcome 
287,287,AppyFizz93,v6g099,[N]Ensemble Method Reasoning,"I recently conducted experiments on Ensemble methods for DL models. Weighted ensemble gets the lowest error compared to Stacking, Bagging. However I'm unable to reason this out since thee is no research paper on this or no thread on this topic. Can someone help me out. Maybe if they have any research paper on this?.",9,3,2022-06-07 04:21:58, n ensemble method reasoning,i recently conducted experiments on ensemble methods for dl models  weighted ensemble gets the lowest error compared to stacking  bagging  however i m unable to reason this out since thee is no research paper on this or no thread on this topic  can someone help me out  maybe if they have any research paper on this  
288,288,UniverseModulator,v5u90n,"[D] My impression is that PhDs in this field do not necessarily confer financial benefits, and there appears to be fewer industry and academic R&D positions relative to the number of PhD graduates. If this is true, then why are PhD admissions so competitive in this field?","As an outsider of this field who is considering learning more about the field and potentially working in R&D in the future, it appears to me like pursuing a PhD in the field right now is a relatively risky prospect. I read a lot of discussion posts on this and not only is there the fact that one has to accept the opportunity cost of the PhD, but a guarantee or a strongly reasonable expectation to land a tenure-track professor position or a well-compensated and engaging R&D position does not exist either. It seems that chances are, when all is said and done, one is likely to end up in a position that may ultimately not require the PhD credentials. 

If this is truly the case, then why are PhD admissions so competitive? It seems like I am missing an important piece of information that could explain the situation to me.",68,44,2022-06-06 09:32:54, d  my impression is that phds in this field do not necessarily confer financial benefits  and there appears to be fewer industry and academic r d positions relative to the number of phd graduates  if this is true  then why are phd admissions so competitive in this field ,as an outsider of this field who is considering learning more about the field and potentially working in r d in the future  it appears to me like pursuing a phd in the field right now is a relatively risky prospect  i read a lot of discussion posts on this and not only is there the fact that one has to accept the opportunity cost of the phd  but a guarantee or a strongly reasonable expectation to land a tenure track professor position or a well compensated and engaging r d position does not exist either  it seems that chances are  when all is said and done  one is likely to end up in a position that may ultimately not require the phd credentials  if this is truly the case  then why are phd admissions so competitive  it seems like i am missing an important piece of information that could explain the situation to me 
289,289,lorepieri,v67xqh,6D pose estimation of a known 3D CAD object 2022 [D],"A follow up of [this](https://www.reddit.com/r/computervision/comments/j1378d/6d_pose_estimation_of_a_known_3d_cad_object/) question from 2020, I'm looking for a codebase for 6DOF pose estimation of a known 3D CAD object with RGB or RGBD. It must be:

\- Usable commercially (licensed under BSD, MIT, BOOST, etc.), not GPL.

\-Easy to setup and use (having a running colab example would be great)

\-The training time required for a new CAD object should be on the order of hours, not days.

\-State of the art of near state of the art results. (See [https://bop.felk.cvut.cz/home/](https://bop.felk.cvut.cz/home/) for benchmarks)

What would you suggest?",2,4,2022-06-06 22:20:04,d pose estimation of a known d cad object   d ,a follow up of  this  https    usable commercially  licensed under bsd  mit  boost  etc    not gpl   easy to setup and use  having a running colab example would be great   the training time required for a new cad object should be on the order of hours  not days   state of the art of near state of the art results   see  https what would you suggest 
290,290,blevlabs,v5k244,"[P] Using OpenAI's CLIP repository as a support, I was able to create a software to detect anything in an image at its original resolution!",,19,119,2022-06-06 00:30:38, p  using openai s clip repository as a support  i was able to create a software to detect anything in an image at its original resolution ,nan
291,291,ShujiMikami,v600jo,[D] Making sense of questionable transposed convolution decoder.,"The decoder in question comes from [this article](https://arxiv.org/pdf/2104.13332.pdf) and is responsible for converting a set of features obtained from a sequence of frames into an audio sample of 40ms, or 640 samples total. 

https://preview.redd.it/6a702r0y5z391.png?width=901&format=png&auto=webp&s=67020e8e41fadaf1e22824430ab78d93556cdcc9

The problem arises when you begin doing calculations with the parameters presented by the authors. Even assuming that only stride would correspond to an increase in the length of output sequence, the total length would come out to \[batch, 1, 1280\] - twice the size presented in the image. Going further and accounting for kernel sizes nets a vector of size \[batch, 1, 9380\] which is several times larger. 

Looking into it further, I found the authors mention taking inspiration from [WaveGAN](https://arxiv.org/pdf/1802.04208.pdf), where according to the structure presented, the increases in length seems to be in line with the stride used. They also seem to have an [official implementation](https://github.com/chrisdonahue/wavegan/blob/master/wavegan.py) using Tensorflow v1. However, looking at the documentation for TF's implementation, they still count the kernel size towards the resulting output, so the math doesn't seem to add up.

https://preview.redd.it/5xf4gc49az391.png?width=908&format=png&auto=webp&s=afad6527849b5e980cd7c7065ad35fa1489f4849

So my question is: how does one go about getting the correct output size with in this situation?",1,3,2022-06-06 16:04:50, d  making sense of questionable transposed convolution decoder ,the decoder in question comes from  this article  https https the problem arises when you begin doing calculations with the parameters presented by the authors  even assuming that only stride would correspond to an increase in the length of output sequence  the total length would come out to   batch         twice the size presented in the image  going further and accounting for kernel sizes nets a vector of size   batch       which is several times larger  looking into it further  i found the authors mention taking inspiration from  wavegan  https https so my question is  how does one go about getting the correct output size with in this situation 
292,292,osedao,v60zsu,[D] Saving/managing the models developed by multiple people in a research group,"Hi, all. As a research group, weâ€™re looking for a kind of code management method such that we can use to save our models/notebooks for long term and reuse when we need it. Our projects are mainly developed by deep learning methods. What is the best way to save and store the codes that are developed in a research group? -Iâ€™m wondering if there is a better solution than github or similar platform.",2,2,2022-06-06 16:38:22, d  saving managing the models developed by multiple people in a research group,hi  all  as a research group  we re looking for a kind of code management method such that we can use to save our models notebooks for long term and reuse when we need it  our projects are mainly developed by deep learning methods  what is the best way to save and store the codes that are developed in a research group   i m wondering if there is a better solution than github or similar platform 
293,293,puppet_pals,v5wo5u,[D] Is this a known issue with Policy Gradient methods?,"Hey all,

I'm working on implementing a few RL algorithms to play Mario bros. I faced a few issues in my REINFORCE implementation and took the time to document them.

One of the issues that comes up is a loss explosion. I use the following code to train my Policy:

```

def train_step(self, data):
""""""train_step runs via `model.fit()`.

It accepts x in the form of observations, and y in the form of a tuple of
the actions and advantages
""""""
observations, (actions, advantages) = data
with tf.GradientTape() as tape:
log_probs = self.action_distribution(observations).log_prob(actions)
loss = log_probs * advantages
loss = -tf.math.reduce_sum(loss, axis=-1)
# Make sure to add regularization losses
loss += sum(self.network.losses)

grads = tape.gradient(loss, self.trainable_weights)
self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

return {""loss"": loss}

```

As you can see, I take the log_probs using a tensorflow_probability.distributions.Categorical.log_prob(). These values seems to explode to -infinity, causing the loss to be eventually tend towards -infinity when any of the actions consistently have a negative reward and a 0 probability. For further reading, I also documented this issue here: https://github.com/LukeWood/luig-io/tree/master/policy_gradient#loss-explosion

Is this a common issue in the REINFORCE algorithm? From what I can tell, if the model learns to make the probability for a specific action 0, and the reward for that action is negative, the loss will over-prioritize getting this action close to zero - as the gradient at that point in the log function is massive.

This actually seems logical - but does this *always* happen whenever a time penalty exists in an environment and an action exists that doesn't progress the agent?",5,2,2022-06-06 12:15:24, d  is this a known issue with policy gradient methods ,hey all i m working on implementing a few rl algorithms to play mario bros  i faced a few issues in my reinforce implementation and took the time to document them one of the issues that comes up is a loss explosion  i use the following code to train my policy    def train_step self  data  train_step runs via  model fit    it accepts x in the form of observations  and y in the form of a tuple ofthe actions and advantagesobservations   actions  advantages    datawith tf gradienttape   as tape log_probs   self action_distribution observations  log_prob actions loss   log_probs   advantagesloss    tf math reduce_sum loss  axis     make sure to add regularization lossesloss    sum self network losses grads   tape gradient loss  self trainable_weights self optimizer apply_gradients zip grads  self trainable_weights  return  loss  loss    as you can see  i take the log_probs using a tensorflow_probability distributions categorical log_prob    these values seems to explode to  infinity  causing the loss to be eventually tend towards  infinity when any of the actions consistently have a negative reward and a  probability  for further reading  i also documented this issue here  https is this a common issue in the reinforce algorithm  from what i can tell  if the model learns to make the probability for a specific action   and the reward for that action is negative  the loss will over prioritize getting this action close to zero   as the gradient at that point in the log function is massive this actually seems logical   but does this  always  happen whenever a time penalty exists in an environment and an action exists that doesn t progress the agent 
294,294,ML_WAYR_bot,v5nggu,[D] Machine Learning - WAYR (What Are You Reading) - Week 139,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|131-140|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|[Week 131](https://reddit.com/srsu2n)||||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)|[Week 132](https://reddit.com/t2xpfe)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)|[Week 133](https://reddit.com/tdf2gt)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)|[Week 134](https://reddit.com/tpruqj)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)|[Week 135](https://reddit.com/u0pnhf)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|[Week 126](https://reddit.com/r4e8he)|[Week 136](https://reddit.com/ub2xlz)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|[Week 127](https://reddit.com/rez90o)|[Week 137](https://reddit.com/ul9toj)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)|[Week 128](https://reddit.com/ruja9s)|[Week 138](https://reddit.com/uvl3xc)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)|[Week 129](https://reddit.com/s5lg69)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)|[Week 130](https://reddit.com/sgisxq)||

Most upvoted papers two weeks ago:

/u/master3243: https://imagen.research.google/

Besides that, there are no rules, have fun.",2,10,2022-06-06 03:19:00, d  machine learning   wayr  what are you reading    week ,this is a place to share machine learning research papers  journals  and articles that you re reading this week  if it relates to what you re researching  by all means elaborate and give us your insight  otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page  not the pdf  you can easily access the pdf from the summary page but not the other way around  or any other pertinent links previous weeks                                                                                                                              week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https most upvoted papers two weeks ago  u master  https besides that  there are no rules  have fun 
295,295,AutoModerator,v5f3w8,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",141,16,2022-06-05 20:30:10, d  simple questions thread,please post your questions here instead of creating a new thread  encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread 
296,296,radi-cho,v584nf,[R] GatedTabTransformer: State-of-the-art for tabular classification,"Check out: [https://arxiv.org/abs/2201.00199](https://arxiv.org/abs/2201.00199)  
Code: [https://github.com/radi-cho/GatedTabTransformer](https://github.com/radi-cho/GatedTabTransformer)

**Abstract**:  
Some of the most common machine learning pipelines involve manipulation of tabular data. The current state-of-the-art solution for tabular modeling is the TabTransformer by Amazon from 2020. It incorporates a Transformer block to track relationships between categorical features and makes use of a standard multilayer perceptron to output its final logits. We propose modifications outperforming it on binary classification tasks for three benchmark datasets with more than 1% AUROC gains. We process categorical embeddings with an attention mechanism and then concatenate them with continuous values to be fed through multiple layers of gated MLP - a neural network originally introduced for language tasks.",4,65,2022-06-05 12:43:56, r  gatedtabtransformer  state of the art for tabular classification,check out   https code   https   abstract     some of the most common machine learning pipelines involve manipulation of tabular data  the current state of the art solution for tabular modeling is the tabtransformer by amazon from   it incorporates a transformer block to track relationships between categorical features and makes use of a standard multilayer perceptron to output its final logits  we propose modifications outperforming it on binary classification tasks for three benchmark datasets with more than   auroc gains  we process categorical embeddings with an attention mechanism and then concatenate them with continuous values to be fed through multiple layers of gated mlp   a neural network originally introduced for language tasks 
297,297,iFARTONMEN,v5lrbz,Meta Gradient Descent [D],"I  am using a modified gradient descent algorithm that I invented, but it  is very simple and I assume someone has already come up with it. I call  it meta gradient descent because it adjusts the learning rate as it  goes. It is the same as normal gradient descent except for one condition  at the end of the loop: If the algorithm passes a local minimum/maximum it decreases the learning rate, else it increases the learning rate

`if (slope < 0 && prevSlope > 0) || (slope > 0 && prevSlope < 0) { //algorithm passed the function min/max`

`learningRate /= 2`

`} else {`

`learningRate *= 1.05`

`}`

This  helps when the algo has to minimize/maximize functions with massively  different scales where a single constant learning rate would be  detrimental. I also suspect that it is faster at honing the min/max in  general, but I have not done any studies on it. Which is why I am here  asking... Is anyone already familiar with this algorithm? does it have a  name? what are the pros/cons of using it instead of regular gradient  descent? I am always using this algorithm instead of regular gradient  descent and I have no problems at all, but my programs are very simple  machine learning wise so I suspect I am missing something.

I love u <3",6,5,2022-06-06 01:53:59,meta gradient descent  d ,i  am using a modified gradient descent algorithm that i invented  but it  is very simple and i assume someone has already come up with it  i call  it meta gradient descent because it adjusts the learning rate as it  goes  it is the same as normal gradient descent except for one condition  at the end of the loop  if the algorithm passes a local minimum maximum it decreases the learning rate  else it increases the learning rate if  slope        slope       prevslope        learningrate        else    learningrate         this  helps when the algo has to minimize maximize functions with massively  different scales where a single constant learning rate would be  detrimental  i also suspect that it is faster at honing the min max in  general  but i have not done any studies on it  which is why i am here  asking    is anyone already familiar with this algorithm  does it have a  name  what are the pros cons of using it instead of regular gradient  descent  i am always using this algorithm instead of regular gradient  descent and i have no problems at all  but my programs are very simple  machine learning wise so i suspect i am missing something i love u  
298,298,Tomsen1410,v5cwvc,"[D] Connecting musical and visual latent spaces in a ""harmonic way""","Hello everyone.

I am currently working on an interdisciplinary project regarding the analysis and visualization of music. My initial idea was to visualize music in a ""cool way"" (psychedelic animations, etc.) using ML. The visuals should somehow represent the structure of the music, e.g. melody, beat, etc. My basic idea is to have an encoder, which brings the music into a latent representation, and a generative decoder that produces images out of the latent code at a given time. Now, there isn't much paired musical-visual training data available, so my thinking was to somehow connect a pre-trained music encoder with a visual decoder. Also, to connect two modalities like this in an unsupervised way seems like a really interesting research area. My problem is that I don't really know how to achieve this. I would somehow need to connect the two latent spaces with each other in a ""natural"" and ""harmonic"" way. I was thinking about stuff like finding the least required ""complexity"" to go from one distribution to another and I have indeed found some papers regarding that. There is also stuff like CycleGAN, which trains this mapping in an end-to-end way.

I just wanted to ask about this here, because honestly it seems like I could go in various directions with this and it is a bit overwhelming. I'd appreciate any kind of help, ideas, recommendations, discussions and any kind of interest. Thanks!

Btw, I have already successfully trained a music encoder in a self-supervised fashion with a relatively large amount of data (\~2000 hours)

&#x200B;

**Update** (capabilities of the music model):

For anyone interested, the model was trained to predict masked regions of a CQT-spectogram. See the following picture to get a grasp about its capabilities:

&#x200B;

[Top: Original spectogram. Middle: Masked spectogram. Bottom: Model prediction](https://preview.redd.it/6xkumxl177491.png?width=3432&format=png&auto=webp&s=29095e720f58c9af5920e256d5526f928302bdf7)

The original spectogram is at the top. It is a 45 second snippet from the song Saku by Bicep (0:30 - 1:15). The triangle shapes are basically the beats and you can see that they are not regularly spaced. There is a similar pattern at the beginning and at the end, but there are these beats in the middle that are very far (\~7 seconds) apart from each other and they are mostly covered by the mask. However as you can see the model successfully predicted them in the output spectogram. Sometimes they are blurry, because the model isn't quite sure whether there is a beat or not, but there are many ways to complete musical patterns in general. Honestly I was quite impressed with this result. It means that the model learned a fundamental understanding of long-term musical structures (as well as short term ones).

Additionally, for some visuals, I have animated the encoder's latent space of this snippet with some bars (PCA'ed down from 768 to 32 dimenstions + smoothed it out a bit):

&#x200B;

[Rudimentary visualization of music latent space](https://reddit.com/link/v5cwvc/video/i626xvk3b7491/player)

The problem is that it is hard to interpret these bars and to connect them to musical concepts (and to visuals). Maybe I shouldn't even do it manually, since there is so much information encoded in them that I don't even know of (see my response to u/DGSPJS in the comments).",7,15,2022-06-05 18:32:20, d  connecting musical and visual latent spaces in a harmonic way,hello everyone i am currently working on an interdisciplinary project regarding the analysis and visualization of music  my initial idea was to visualize music in a cool way  psychedelic animations  etc   using ml  the visuals should somehow represent the structure of the music  e g  melody  beat  etc  my basic idea is to have an encoder  which brings the music into a latent representation  and a generative decoder that produces images out of the latent code at a given time  now  there isn t much paired musical visual training data available  so my thinking was to somehow connect a pre trained music encoder with a visual decoder  also  to connect two modalities like this in an unsupervised way seems like a really interesting research area  my problem is that i don t really know how to achieve this  i would somehow need to connect the two latent spaces with each other in a natural and harmonic way  i was thinking about stuff like finding the least required complexity to go from one distribution to another and i have indeed found some papers regarding that  there is also stuff like cyclegan  which trains this mapping in an end to end way i just wanted to ask about this here  because honestly it seems like i could go in various directions with this and it is a bit overwhelming  i d appreciate any kind of help  ideas  recommendations  discussions and any kind of interest  thanks btw  i have already successfully trained a music encoder in a self supervised fashion with a relatively large amount of data     hours   xb   update    capabilities of the music model  for anyone interested  the model was trained to predict masked regions of a cqt spectogram  see the following picture to get a grasp about its capabilities   xb  top  original spectogram  middle  masked spectogram  bottom  model prediction  https the original spectogram is at the top  it is a  second snippet from the song saku by bicep          the triangle shapes are basically the beats and you can see that they are not regularly spaced  there is a similar pattern at the beginning and at the end  but there are these beats in the middle that are very far     seconds  apart from each other and they are mostly covered by the mask  however as you can see the model successfully predicted them in the output spectogram  sometimes they are blurry  because the model isn t quite sure whether there is a beat or not  but there are many ways to complete musical patterns in general  honestly i was quite impressed with this result  it means that the model learned a fundamental understanding of long term musical structures  as well as short term ones  additionally  for some visuals  i have animated the encoder s latent space of this snippet with some bars  pca ed down from  to  dimenstions   smoothed it out a bit    xb  rudimentary visualization of music latent space  https the problem is that it is hard to interpret these bars and to connect them to musical concepts  and to visuals   maybe i shouldn t even do it manually  since there is so much information encoded in them that i don t even know of  see my response to u dgspjs in the comments  
299,299,jj4646,v5sjge,[D] Machine Learning Models for Longitudinal Data,"Recently, I had the following question about supervised classification models (e.g. random forest) for longitudinal data.

Suppose I have the following data about students passing a fitness test - the students (each student has an ""id"") who enroll in a school take a fitness test each year and record their height and weight (at the start of each school year, before the fitness test). They can either pass (1) or fail (0) the fitness test each year. The school is interested in knowing which students are likely to fail the fitness test, so they can focus more attention on these students. Naturally, some students might have taken the fitness test more times than other students.

I simulated some data (using the R programming language) to show how the historical data might look like:

&#x200B;

    score <- c(""1"",""0"")
    score <- as.numeric(sample(score, 1000, replace=TRUE, prob=c(0.3, 0.7)))
    id_sample <- 1:140
    id <- sample(id_sample, replace = TRUE, 1000)
    height <- abs(rnorm(1000, 150,5))
    weight <- abs(rnorm(1000, 75,5))
    
    data = data.frame(id, height, weight, score)
    data <- data[order(data$id),]

I then added two variables to this data - one to show how many times the fitness test was taken, the another to show the (cumulative) average number of times the test was passed:

&#x200B;

    library(dplyr)
    data =  data.frame(data %>% group_by(id) %>% mutate(counter = row_number(id)))
    data$csum <- ave(data$score, data$id, FUN=cumsum)
    data$average <- data$csum/data$counter

Now, suppose some of the students are about to take this test again and we would like to predict what their score will be - some of these students are existing students, but some of these students are new and have never taken the test before (i.e. they have no historical data):

&#x200B;

    id_sample <- 1:140
    id <- sample(id_sample, replace = FALSE, 23)
    height <- abs(rnorm(23, 150,5))
    weight <- abs(rnorm(23, 75,5))
    
    new_data = data.frame(id, height, weight)
    new_data <- new_data[order(new_data$id),]
    
    id_sample <- 141:200
    id <- sample(id_sample, replace = FALSE, 5)
    height <- abs(rnorm(5, 150,5))
    weight <- abs(rnorm(5, 75,5))
    
    #simulating data for students who never took the test before
    n_data = data.frame(id, height, weight)
    n_data <- n_data[order(n_data$id),]
    
    test_data = rbind(new_data, n_data)

Now, to this test data, (where applicable) I added ""longitudinal variables"" that take into account the number of times the students took the test and their most recent average cumulative score:

&#x200B;

    #counter
    max = data.frame(data %>% 
                 group_by(id) %>%
                 filter(counter == max(counter)))
    
    colnames(max)[5] <- ""max_counter""
    
    max$max_counter = max$max_counter + 1
    
    test_with_counter =  merge(x = test_data, y = max, by = ""id"", all.x = TRUE)
    
    test  = test_with_counter[, c(1,2,3,7,9)]
    
     test$max_counter[is.na(test$max_counter)] <- 1
    
     test$average[is.na(test$average)] <- 0
    
    #formatting
    colnames(test)[2] <- ""height""
    colnames(test)[3] <- ""weight""
    colnames(test)[4] <- ""counter""
    data$csum = NULL
    data$score = as.factor(data$score)

At this point, there is nothing stopping me from training a supervised classification model (e.g. random forest) to predict the ""score"" variable for the test data:

&#x200B;

    #skip cross validation for brevity of question
    library(randomForest)
    rf <- randomForest(score~., data=data)
    pred = predict(rf, newdata = test)
    
    print(rf)
    
    Call:
     randomForest(formula = score ~ ., data = data) 
                   Type of random forest: classification
                         Number of trees: 500
    No. of variables tried at each split: 2
    
            OOB estimate of  error rate: 23.4%
    Confusion matrix:
        0   1 class.error
    0 636  79   0.1104895

**My Question:** Does the approach that I have proposed for supervised classification of longitudinal data sound reasonable (e.g. better than ""nothing"") - or are there any major statistical flaws on this approach (e.g. structural multicollinearity, variance inflation, etc.) ? Or is it better to use some supervised classification model/software implementation that has been specifically designed for longitudinal data (e.g. [https://cran.r-project.org/web/packages/LongituRF/LongituRF.pdf](https://cran.r-project.org/web/packages/LongituRF/LongituRF.pdf))? Thanks!

&#x200B;

**Note:**

&#x200B;

\- This is a rough sketch of the situation I am dealing with - I am also planning to include variables such as ""number of days that elapsed since last fitness test"".

&#x200B;

\- The sample data in this reddit question is randomly simulated and obviously wont show any longitudinal trends.

&#x200B;

\- I have heard that models such as Random Forest have the ability to recover/model around complex interactions and correlations within the data that otherwise need to be explicitly specified in standard supervised models ([https://ishwaran.org/papers/IKBL.AOAS.pdf](https://ishwaran.org/papers/IKBL.AOAS.pdf)).",3,0,2022-06-06 07:52:14, d  machine learning models for longitudinal data,recently  i had the following question about supervised classification models  e g  random forest  for longitudinal data suppose i have the following data about students passing a fitness test   the students  each student has an id  who enroll in a school take a fitness test each year and record their height and weight  at the start of each school year  before the fitness test   they can either pass    or fail    the fitness test each year  the school is interested in knowing which students are likely to fail the fitness test  so they can focus more attention on these students  naturally  some students might have taken the fitness test more times than other students i simulated some data  using the r programming language  to show how the historical data might look like   xb     score    c       score    as numeric sample score    replace true  prob c            id_sample         id    sample id_sample  replace   true       height    abs rnorm          weight    abs rnorm              data   data frame id  height  weight  score     data    data order data id   i then added two variables to this data   one to show how many times the fitness test was taken  the another to show the  cumulative  average number of times the test was passed   xb     library dplyr     data    data frame data     group_by id      mutate counter   row_number id       data csum    ave data score  data id  fun cumsum     data average    data csum data counternow  suppose some of the students are about to take this test again and we would like to predict what their score will be   some of these students are existing students  but some of these students are new and have never taken the test before  i e  they have no historical data    xb     id_sample         id    sample id_sample  replace   false       height    abs rnorm          weight    abs rnorm              new_data   data frame id  height  weight     new_data    new_data order new_data id           id_sample         id    sample id_sample  replace   false       height    abs rnorm          weight    abs rnorm               simulating data for students who never took the test before    n_data   data frame id  height  weight     n_data    n_data order n_data id           test_data   rbind new_data  n_data now  to this test data   where applicable  i added longitudinal variables that take into account the number of times the students took the test and their most recent average cumulative score   xb      counter    max   data frame data                      group_by id                      filter counter    max counter           colnames max       max_counter        max max_counter   max max_counter           test_with_counter    merge x   test_data  y   max  by   id  all x   true         test    test_with_counter   c                test max_counter is na test max_counter               test average is na test average               formatting    colnames test       height    colnames test       weight    colnames test       counter    data csum   null    data score   as factor data score at this point  there is nothing stopping me from training a supervised classification model  e g  random forest  to predict the score variable for the test data   xb      skip cross validation for brevity of question    library randomforest     rf    randomforest score    data data     pred   predict rf  newdata   test         print rf         call      randomforest formula   score      data   data                     type of random forest  classification                         number of trees      no  of variables tried at each split                  oob estimate of  error rate        confusion matrix             class error             my question    does the approach that i have proposed for supervised classification of longitudinal data sound reasonable  e g  better than nothing    or are there any major statistical flaws on this approach  e g  structural multicollinearity  variance inflation  etc     or is it better to use some supervised classification model software implementation that has been specifically designed for longitudinal data  e g   https   xb   note     xb    this is a rough sketch of the situation i am dealing with   i am also planning to include variables such as number of days that elapsed since last fitness test   xb    the sample data in this reddit question is randomly simulated and obviously wont show any longitudinal trends   xb    i have heard that models such as random forest have the ability to recover model around complex interactions and correlations within the data that otherwise need to be explicitly specified in standard supervised models   https   ishwaran org papers ikbl aoas pdf  https   ishwaran org papers ikbl aoas pdf   
300,300,tmuxed,v5kxks,[D] Two quick questions about CNNs,"My goal is to detect an object in an image. The image is 960x720 and has 3 color channels. The object can be as small as 20 pixels.

1. Does it make any sense to provide a learning model with additional channels that can be derived from the original image itself? For example let's say I provide the 960x720x3 image, but add an additional channel that's just the negative, or grayscale of the same image? I presume not, because the grayscale image would just be a linear combination of the 3 color channels, right? Or perhaps there's some significant computational advantage to that? 

2. Does it make sense to cut the image up into a bunch of (50%) overlapping squares and then feed these smaller images to my CNN? It's unlikely that the object will ever be larger than one of these squares, so that could make sense, right? It should be computationally much more efficient, because the CNN compute would be much lower, even despite the overhead of processing more (but smaller) squares.

Thank you",13,1,2022-06-06 01:13:05, d  two quick questions about cnns,my goal is to detect an object in an image  the image is x and has  color channels  the object can be as small as  pixels   does it make any sense to provide a learning model with additional channels that can be derived from the original image itself  for example let s say i provide the xx image  but add an additional channel that s just the negative  or grayscale of the same image  i presume not  because the grayscale image would just be a linear combination of the  color channels  right  or perhaps there s some significant computational advantage to that    does it make sense to cut the image up into a bunch of     overlapping squares and then feed these smaller images to my cnn  it s unlikely that the object will ever be larger than one of these squares  so that could make sense  right  it should be computationally much more efficient  because the cnn compute would be much lower  even despite the overhead of processing more  but smaller  squares thank you
301,301,Singularian2501,v4r6pt,[R] Joint Abductive and Inductive Neural Logical Reasoning,"Paper: [https://arxiv.org/abs/2205.14591](https://arxiv.org/abs/2205.14591)

Abstract:

"" Neural logical reasoning (NLR) is a fundamental task in knowledge discovery and artificial intelligence. NLR aims at answering multi-hop queries with logical operations on structured knowledge bases based on distributed representations of queries and answers. While previous neural logical reasoners can give specific entity-level answers, i.e., perform **inductive reasoning** from the perspective of logic theory, they are not able to provide descriptive concept-level answers, i.e., perform **abductive reasoning**, where each concept is a summary of a set of entities. In particular, the abductive reasoning task attempts to infer the explanations of each query with descriptive concepts, which make answers comprehensible to users and is of great usefulness in the field of applied ontology. In this work, we formulate the problem of the **joint abductive and inductive neural logical reasoning (AI-NLR)**, solving which needs to address challenges in **incorporating, representing, and operating on concepts**. We propose an original solution named ABIN for AI-NLR. Firstly, we incorporate description logic-based ontological axioms to provide the source of concepts. Then, we represent concepts and queries as fuzzy sets, i.e., sets whose elements have degrees of membership, to bridge concepts and queries with entities. Moreover, we design operators involving concepts on top of the fuzzy set representation of concepts and queries for optimization and inference. Extensive experimental results on two real-world datasets demonstrate the effectiveness of ABIN for AI-NLR.""

https://preview.redd.it/j8ihdi7mgm391.jpg?width=638&format=pjpg&auto=webp&s=419e08b309f33307a1fad52abac313bb487f3689

https://preview.redd.it/3ocpfb7mgm391.jpg?width=621&format=pjpg&auto=webp&s=a9bdc963af7a53d814c8c7e669c0ecd82a271ae0

https://preview.redd.it/wlf8fd7mgm391.jpg?width=630&format=pjpg&auto=webp&s=81280789fdfa1bdec371f8fb09f8083a53c7a6fb

https://preview.redd.it/r8dfji7mgm391.jpg?width=633&format=pjpg&auto=webp&s=6d987e55003dcae9fec2007b5e58c6384d01e79a

https://preview.redd.it/fdidui7mgm391.jpg?width=585&format=pjpg&auto=webp&s=3780c143fc250d653ba5c9cd4f538389a80995d4

https://preview.redd.it/48j5qn7mgm391.jpg?width=1270&format=pjpg&auto=webp&s=96531388476ec00662dc8ea632d233d05211cc22

https://preview.redd.it/dko0dn7mgm391.jpg?width=650&format=pjpg&auto=webp&s=fce32a000f342b1ff5869f9d4f5478cbaac9394c",1,127,2022-06-04 20:57:12, r  joint abductive and inductive neural logical reasoning,paper   https abstract  neural logical reasoning  nlr  is a fundamental task in knowledge discovery and artificial intelligence  nlr aims at answering multi hop queries with logical operations on structured knowledge bases based on distributed representations of queries and answers  while previous neural logical reasoners can give specific entity level answers  i e   perform   inductive reasoning   from the perspective of logic theory  they are not able to provide descriptive concept level answers  i e   perform   abductive reasoning    where each concept is a summary of a set of entities  in particular  the abductive reasoning task attempts to infer the explanations of each query with descriptive concepts  which make answers comprehensible to users and is of great usefulness in the field of applied ontology  in this work  we formulate the problem of the   joint abductive and inductive neural logical reasoning  ai nlr     solving which needs to address challenges in   incorporating  representing  and operating on concepts    we propose an original solution named abin for ai nlr  firstly  we incorporate description logic based ontological axioms to provide the source of concepts  then  we represent concepts and queries as fuzzy sets  i e   sets whose elements have degrees of membership  to bridge concepts and queries with entities  moreover  we design operators involving concepts on top of the fuzzy set representation of concepts and queries for optimization and inference  extensive experimental results on two real world datasets demonstrate the effectiveness of abin for ai nlr https https https https https https https   preview redd it dkodnmgm jpg width  format pjpg auto webp s fceafbfffdfcbaacc
302,302,omniverse71,v5gr3f,[R] Objective measurement of political bias using machine learning,"Sociologists frequently attempt to compare the degree of bias among left and right-wing voters. In a typical study setting, participants are asked a number of factual knowledge questions and the responses are compared to the correct answers. Then, the side whose partisans make the largest errors is declared to be the most biased.

Unfortunately, this approach is extremely vulnerable to biases of the researchers themselves. Consciously or subconsciously, the researchers are likely to select questions whose answers favor their own side. For example, on the topic of Obamaâ€™s economic policies, a left-leaning researcher might prefer questions, such as

*â€œDid the unemployment rate decline under the Obama administration?â€* (correct answer: â€œ*yes*â€).

In contrast, a right-leaning researcher working on the same topic, is more likely to ask

*â€œDid the adult employment rate rise under the Obama administration?â€* (correct answer: â€œ*no*â€).

In both cases the study participants whose biases align with the biases of the researchers would be much more likely to answer the questions correctly.

To address this problem, we built an SVD-based algorithm that measures partisan bias in question selection and corrects testing results based on the measurements. The algorithmâ€™s accuracy improves with increasing data size, so if you have a few minutes to spare, please help our project by taking any of these tests:

[US: General politics](https://www.explicitbias.com/test.php?l=10&t=1)

[US: Economic policies](https://www.explicitbias.com/test.php?l=10&t=3)

[Environmental policies](https://www.explicitbias.com/test.php?l=10&t=2)

(Note: Participants are not expected to know the correct answers to all of the questions, but to use their intuition to give their best guess.)",5,1,2022-06-05 21:49:21, r  objective measurement of political bias using machine learning,sociologists frequently attempt to compare the degree of bias among left and right wing voters  in a typical study setting  participants are asked a number of factual knowledge questions and the responses are compared to the correct answers  then  the side whose partisans make the largest errors is declared to be the most biased unfortunately  this approach is extremely vulnerable to biases of the researchers themselves  consciously or subconsciously  the researchers are likely to select questions whose answers favor their own side  for example  on the topic of obama s economic policies  a left leaning researcher might prefer questions  such as  did the unemployment rate decline under the obama administration     correct answer    yes    in contrast  a right leaning researcher working on the same topic  is more likely to ask  did the adult employment rate rise under the obama administration     correct answer    no    in both cases the study participants whose biases align with the biases of the researchers would be much more likely to answer the questions correctly to address this problem  we built an svd based algorithm that measures partisan bias in question selection and corrects testing results based on the measurements  the algorithm s accuracy improves with increasing data size  so if you have a few minutes to spare  please help our project by taking any of these tests  us  general politics  https  us  economic policies  https  environmental policies  https  note  participants are not expected to know the correct answers to all of the questions  but to use their intuition to give their best guess  
303,303,keremidk0,v5ih13,[D] What do you do when you are stuck on an ML problem?,"If you work on an ML problem that you know is solvable, but you are not able to solve it - what do you do?",12,0,2022-06-05 23:14:04, d  what do you do when you are stuck on an ml problem ,if you work on an ml problem that you know is solvable  but you are not able to solve it   what do you do 
304,304,Aneervan,v5h2ja,[D] Categorical sequence prediction for maximum positive impact,"Dataset 1 : Contact records( call, email, face to face meet) with timestamp between doctors and medical representatives based on brands and country and some other details. Contains around 60000 records. 

Dataset 2 : Overall impact for each individual medical representative(continuous variable, also has negative values). Contains around 160 records. Has unique record for one medical rep.

From this two datasets, I want to predict the next category in a contact sequence with the time gapp between them( i.e, if the 1st contact method is call or email, what should be the optimal second method and after what should it be done?) for the medical representaitve to have maximum positive impact.

One rep can contact multiple doctors n number of times with n different methods 

How should I frame this problem? What type of modeling can elp me achieve higher than 90% accuracy?

Thanks for the help in advance.",0,0,2022-06-05 22:04:36, d  categorical sequence prediction for maximum positive impact,dataset    contact records  call  email  face to face meet  with timestamp between doctors and medical representatives based on brands and country and some other details  contains around  records  dataset    overall impact for each individual medical representative continuous variable  also has negative values   contains around  records  has unique record for one medical rep from this two datasets  i want to predict the next category in a contact sequence with the time gapp between them  i e  if the st contact method is call or email  what should be the optimal second method and after what should it be done   for the medical representaitve to have maximum positive impact one rep can contact multiple doctors n number of times with n different methods how should i frame this problem  what type of modeling can elp me achieve higher than   accuracy thanks for the help in advance 
305,305,pro_user_for_good,v4f3wn,[N] Stanford's Machine Learning; End of an era...,"After 10 years and nearly 5 million enrollments, Stanford will be closing new enrollments for the Machine Learning course on Coursera from June 14, 2022. It will be replaced by a more in-depth Machine Learning Specialization by Stanford Online and [Deeplearning.ai](https://Deeplearning.ai) and will be available in June.  
The most iconic MOOC to ever exist?",30,178,2022-06-04 07:57:57, n  stanford s machine learning  end of an era   ,after  years and nearly  million enrollments  stanford will be closing new enrollments for the machine learning course on coursera from june     it will be replaced by a more in depth machine learning specialization by stanford online and  deeplearning ai  https the most iconic mooc to ever exist 
306,306,sid_276,v4ozj7,[D] Where do we currently stand at in lottery ticket hypothesis research?,"What is the most recent research around the lottery ticket hypothesis? Which are the best papers with new techniques for finding winning tickets, are there open-source tools that ""work""? Anyone knows digestible-easy resources to get started with LTH?",15,19,2022-06-04 19:03:20, d  where do we currently stand at in lottery ticket hypothesis research ,what is the most recent research around the lottery ticket hypothesis  which are the best papers with new techniques for finding winning tickets  are there open source tools that work  anyone knows digestible easy resources to get started with lth 
307,307,ykilcher,v42pej,"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)","[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts",170,801,2022-06-03 21:36:33, p  this is the worst ai ever   gpt chan model  trained on   years worth of  pol  posts , https gpt chan was trained on over  years of posts from chan s politically incorrect   pol   board website  try the model here    https model   https code   https dataset   https   xb outline     intro    disclaimers    elon  twitter  and the seychelles    how i trained a language model on chan posts    how good is this model     building a chan bot    something strange is happening    how the bot got unmasked    here we go again    final thoughts
308,308,tdls_to,v4my2u,[P] Information Retrieval Explainability Summer Project,"hey y'all! we're starting a project summer project in collaboration between our community and some academic partners to create an open source library for explaining typical semantic search approaches (like vector search). There is also some chance that we might end up publishing some papers depending on what kind of results we get and how far we can push it.

**you all are welcome to join.**

most of the work will be done async but there will be weekly meetings to discuss questions and plans.

there are some pre-reqs but we are welcoming anyone who is ambitious and interested enough to contribute meaningfully.

* more details: [https://community.ai.science/explainable-information-retrieval-xir](https://community.ai.science/explainable-information-retrieval-xir)
* weekly meeting schedule: [https://lu.ma/aisc-xir](https://lu.ma/aisc-xir)
* some starter material: [https://ai.science/l/236a6202-3495-4a8e-bbad-aedeee4bd21d](https://ai.science/l/236a6202-3495-4a8e-bbad-aedeee4bd21d)

let me know if you have any questions, or suggestions for a project like this",3,9,2022-06-04 16:59:01, p  information retrieval explainability summer project,hey y all  we re starting a project summer project in collaboration between our community and some academic partners to create an open source library for explaining typical semantic search approaches  like vector search   there is also some chance that we might end up publishing some papers depending on what kind of results we get and how far we can push it   you all are welcome to join   most of the work will be done async but there will be weekly meetings to discuss questions and plans there are some pre reqs but we are welcoming anyone who is ambitious and interested enough to contribute meaningfully   more details   https   weekly meeting schedule   https   some starter material   https let me know if you have any questions  or suggestions for a project like this
309,309,LanverYT,v4osj9,[D] Deploying SOTA models into my own projects,"What is the most common approach to use networks developed by other researchers? Until now I have been using huggingface's pipeline to deploy pre-trained models.

But for some other cases, the only available implementation is the github repo published by the authors where the code simply verifies that the results in the paper are reproducible, for example this [https://github.com/Turoad/CLRNet](https://github.com/Turoad/CLRNet). The main aspect of this case is that there's no inference pipeline, but rather train/test functions with the benchmark dataset as input.

How do you deploy their model? Do you copy the architecture layers in your own torch/tensorflow code and train it following their parameters or you tweak their repository code?",3,5,2022-06-04 18:52:23, d  deploying sota models into my own projects,what is the most common approach to use networks developed by other researchers  until now i have been using huggingface s pipeline to deploy pre trained models but for some other cases  the only available implementation is the github repo published by the authors where the code simply verifies that the results in the paper are reproducible  for example this  https how do you deploy their model  do you copy the architecture layers in your own torch tensorflow code and train it following their parameters or you tweak their repository code 
310,310,radi-cho,v4kxm0,[D] Parameter optimisation as a language problem?,"Hello! I am thinking of an idea for research on the topic of parameter optimisation viewed as a language problem. Here is what I mean by that - There are already multiple big pre-trained language models such as CodeBERT which can generate good contextual embeddings for source code. So if they're used as a baseline and built upon, we can create a supervised learning pipeline that predicts code parameters which satisfy desired outcomes. For example if we have the function `def f(x): return 2 + 2 * x - x*x` we can ask the model to `maximise` it and to find that the desired x is 1. At the beginning we expect to be able to solve such simple optimisation problems, but with time we may derive methods which are able to solve for more parameters and complicated functions and probably even have such a model to optimise parameters for other ML models in the future. If achieved this approach may replace or work together with traditional hyper-parameter tuning solutions like Bayesian optimisation (which are computationally expensive since they require testing the function itself with multiple parameters).

One approach will be to take the problem purely as a language task and replace the desired parameter(s) with a masked token and then train a model (fine-tune pre-trained BERT-like model) to predict such tokens given desired outcomes.

Another approach will be to take advantage of the pre-trained NL-PL models to generate embeddings for the source code, but then use these representations in a separate regression model. In this case it might be a good idea to built some meta learning environment to better generalise to different functions and then take few-shot approach by first providing a few examples of input-result pairs and then asking for predicted parameters given a desired outcome.

What do you think about the idea as a whole and the proposed approaches? Do you think they're feasible and if not - why? Do you think such a study will be pointless and if so - do you have better ideas in this direction?",9,10,2022-06-04 14:29:28, d  parameter optimisation as a language problem ,hello  i am thinking of an idea for research on the topic of parameter optimisation viewed as a language problem  here is what i mean by that   there are already multiple big pre trained language models such as codebert which can generate good contextual embeddings for source code  so if they re used as a baseline and built upon  we can create a supervised learning pipeline that predicts code parameters which satisfy desired outcomes  for example if we have the function  def f x   return       x   x x  we can ask the model to  maximise  it and to find that the desired x is   at the beginning we expect to be able to solve such simple optimisation problems  but with time we may derive methods which are able to solve for more parameters and complicated functions and probably even have such a model to optimise parameters for other ml models in the future  if achieved this approach may replace or work together with traditional hyper parameter tuning solutions like bayesian optimisation  which are computationally expensive since they require testing the function itself with multiple parameters  one approach will be to take the problem purely as a language task and replace the desired parameter s  with a masked token and then train a model  fine tune pre trained bert like model  to predict such tokens given desired outcomes another approach will be to take advantage of the pre trained nl pl models to generate embeddings for the source code  but then use these representations in a separate regression model  in this case it might be a good idea to built some meta learning environment to better generalise to different functions and then take few shot approach by first providing a few examples of input result pairs and then asking for predicted parameters given a desired outcome what do you think about the idea as a whole and the proposed approaches  do you think they re feasible and if not   why  do you think such a study will be pointless and if so   do you have better ideas in this direction 
311,311,user89320,v4j6d8,[Discussion] Influence of the number of classes on the performance of triplet loss,"Does the number of classes need to be huge (1000+) in order to get good performance with triplet loss? 

I am experimenting with a dataset which has 15 classes and 200 examples for each class. I kept a few classes for the test set and trained on the rest by constructing triplets. The loss drops steadily on the train set but not so much on the test set, it overfits. 

I am contemplating is the reason for bad performance the low number of classes which enables the model to ""remember"" those classes in the train set. Also does the batch size play a big role here? should the batch size be low so the model doesnt see all the train classes in a batch?",6,10,2022-06-04 12:17:52, discussion  influence of the number of classes on the performance of triplet loss,does the number of classes need to be huge     in order to get good performance with triplet loss  i am experimenting with a dataset which has  classes and  examples for each class  i kept a few classes for the test set and trained on the rest by constructing triplets  the loss drops steadily on the train set but not so much on the test set  it overfits  i am contemplating is the reason for bad performance the low number of classes which enables the model to remember those classes in the train set  also does the batch size play a big role here  should the batch size be low so the model doesnt see all the train classes in a batch 
312,312,darn321,v4lwr6,[D] Imbalance: Metric to Loss functions,"In the case of class imbalance, looks like the main suggestion is to start with a clear problem specific metric, to make sure one is solving the correct problem.

However, this means the optimizer is not affected at all and will remain affected by class imbalance. The cost function is not adjusted. Metrics can at best effect the decision threshold, a single parameter lever. 

Is this sufficient? What will be better?",7,5,2022-06-04 15:43:13, d  imbalance  metric to loss functions,in the case of class imbalance  looks like the main suggestion is to start with a clear problem specific metric  to make sure one is solving the correct problem however  this means the optimizer is not affected at all and will remain affected by class imbalance  the cost function is not adjusted  metrics can at best effect the decision threshold  a single parameter lever  is this sufficient  what will be better 
313,313,Redplatypus14,v4gm7g,[D] Universities with research in AI/ML for Music?,"Does anybody know of any universities doing research in AI for music? I know Queen Mary University of London appears to have some programs, but I haven't seen any other universities with similar initiatives and It seems like every paper I see in that area is always either from DeepMind or Magenta.",9,9,2022-06-04 09:28:54, d  universities with research in ai ml for music ,does anybody know of any universities doing research in ai for music  i know queen mary university of london appears to have some programs  but i haven t seen any other universities with similar initiatives and it seems like every paper i see in that area is always either from deepmind or magenta 
314,314,darn321,v4ltyn,[D] latent space: task specific?,"Usually, latent space is just feature space compression. This can be done with PCA, autoencoders etc. This approach is task/target independent.

One can also learn task dependent latent space representations. In a vanilla fully connected NN, the last layer   before the target prediction layer is probably one example.

Is this a correct understanding? Are there different terminologies to differentiate task specific vs label free latent space?",2,0,2022-06-04 15:37:25, d  latent space  task specific ,usually  latent space is just feature space compression  this can be done with pca  autoencoders etc  this approach is task target independent one can also learn task dependent latent space representations  in a vanilla fully connected nn  the last layer   before the target prediction layer is probably one example is this a correct understanding  are there different terminologies to differentiate task specific vs label free latent space 
315,315,NiM-HLT,v53nsj,[N][D][R]Alleged plagiarism of a paper in EMNLP2020 by the best paper in NAACL2021,"Hi, everyone. 

I found that a paper in EMNLP2020 is plagiarized by the best paper in NAACL2021.

EMNLP2020: Visually Grounded Compound PCFGs  [http://aclanthology.lst.uni-saarland.de/2020.emnlp-main.354.pdf](http://aclanthology.lst.uni-saarland.de/2020.emnlp-main.354.pdf)

https://preview.redd.it/0qr7v4mwnp391.png?width=865&format=png&auto=webp&s=5480ef6eb684670c63c000ff6fa9fb64386fb1cc

NAACL2021: Video-aided Unsupervised Grammar Induction [https://aclanthology.org/2021.naacl-main.119.pdf](https://aclanthology.org/2021.naacl-main.119.pdf)

https://preview.redd.it/528ipfdxnp391.png?width=865&format=png&auto=webp&s=1c1052381966c2d4b7674dfee6d26ad7333a73a0

1. Almost the same model with different input

[emnlp](https://preview.redd.it/tmjl82b1op391.png?width=865&format=png&auto=webp&s=ad119e43f6ace271ae64226f335066a27ff1819b)

[naacl](https://preview.redd.it/cae8k3g2op391.png?width=865&format=png&auto=webp&s=6e87a8464c9a0c1ec60084b6c35541d95918c8a1)

2. The same formulas and contents.

[emnlp](https://preview.redd.it/k8xnv6y3op391.png?width=351&format=png&auto=webp&s=75d75f7964deecf79f9b6452896a1c0fb70574b0)

[naacl](https://preview.redd.it/yrra8o25op391.png?width=336&format=png&auto=webp&s=7bd5aeed0d551abf2c6526a65e47bccb4eba4020)

[emnlp](https://preview.redd.it/3v1gf6b6op391.png?width=361&format=png&auto=webp&s=eb3a060711e1222f1ba511431f9a536a8199ad4d)

[naacl](https://preview.redd.it/hxvdt5f7op391.png?width=451&format=png&auto=webp&s=cb3794399e0f12342c06d8554317d50b62314b08)

&#x200B;

[emnlp](https://preview.redd.it/a0ghovo8op391.png?width=402&format=png&auto=webp&s=379142308854d64c5cf939ad25e0fc3cce3afc4c)

[naacl](https://preview.redd.it/l0xwo7m9op391.png?width=419&format=png&auto=webp&s=90ba63ec96f1ebe8a7ee33e281a32b836eb4695b)

3. Similar experiments and claims.

[emnlp](https://preview.redd.it/dg53y1laop391.png?width=394&format=png&auto=webp&s=7ebafb05a1c2e627414a1393f1526bf7536496b8)

[naacl](https://preview.redd.it/ea25fwdbop391.png?width=449&format=png&auto=webp&s=c933b020e596f6c7498a0df83d7d8d2f4a538b33)

4. The same core component in implementation.

[ The public code in naacl2021, vpcfg is copied from emnlp2020.](https://preview.redd.it/mdavfgqcop391.png?width=865&format=png&auto=webp&s=a99138d4f1ccbdbd4519e5b92c3a3765485362ee)

&#x200B;

General speaking, the paper in naacl2021 shares the same method and task with the paper in emnlp2020, only the input difference i.e., text+image (emnlp2020) and video (naacl2021). Amazing.",2,0,2022-06-05 07:43:52, n  d  r alleged plagiarism of a paper in emnlp by the best paper in naacl,hi  everyone  i found that a paper in emnlp is plagiarized by the best paper in naacl emnlp  visually grounded compound pcfgs   http https naacl  video aided unsupervised grammar induction  https https   almost the same model with different input emnlp  https  naacl  https   the same formulas and contents  emnlp  https  naacl  https  emnlp  https  naacl  https   xb  emnlp  https  naacl  https   similar experiments and claims  emnlp  https  naacl  https   the same core component in implementation   the public code in naacl  vpcfg is copied from emnlp   https   xb general speaking  the paper in naacl shares the same method and task with the paper in emnlp  only the input difference i e   text image  emnlp  and video  naacl   amazing 
316,316,optimistdit,v3rrh9,[P] Hands on diffusion models,"A minimal example of the forward and reverse flow of diffusion models with equations from the paper and visualizations alongside the code: https://github.com/InFoCusp/diffusion_models 
I coded it up since I wanted to familiarize myself with rhe end to end flow. It uses a simple 2d dataset that can train within minutes. Hope others on this subreddit find it useful.",6,151,2022-06-03 10:26:52, p  hands on diffusion models,a minimal example of the forward and reverse flow of diffusion models with equations from the paper and visualizations alongside the code  https i coded it up since i wanted to familiarize myself with rhe end to end flow  it uses a simple d dataset that can train within minutes  hope others on this subreddit find it useful 
317,317,learning_by_looking,v4b4cr,[R] Deep Learning Opacity in Scientific Discovery,"This paper argues that the uninterpreability of deep neural networks need not diminish AI's capacity to lead scientists to significant and justifiable breakthroughs.

[https://arxiv.org/abs/2206.00520](https://arxiv.org/abs/2206.00520)",2,5,2022-06-04 04:18:06, r  deep learning opacity in scientific discovery,this paper argues that the uninterpreability of deep neural networks need not diminish ai s capacity to lead scientists to significant and justifiable breakthroughs  https   arxiv org abs    https   arxiv org abs   
318,318,darn321,v3swj7,[D] class imbalance: over/under sampling and class reweight,"If there's unbalanced datasets, what's the way to proceed?

The canonical answer seems to be over/under sampling and class reweighting (is there anything more?), but have these things really worked in practice for you?

What's the actual experience and practical suggestion? When to use one over the other?",23,34,2022-06-03 11:40:28, d  class imbalance  over under sampling and class reweight,if there s unbalanced datasets  what s the way to proceed the canonical answer seems to be over under sampling and class reweighting  is there anything more    but have these things really worked in practice for you what s the actual experience and practical suggestion  when to use one over the other 
319,319,bluuerp,v45rpn,[D] Research papers/suggestions for x-ray defect analysis,"I have a LOT (several millions) of unlabeled x-ray images of aluminium parts with small defects in them. Like air bubbles cracks, or voids. These are thousands of images which are basically identical except for the defects 0(apart from slight variations in position, rotation, noise etc), and thousands of different view directions/parts. All image dimensions are identical. The problem is that I do not have a single image with labels and would hence have to label them manually. Which I'd like to avoid as much as possible.

I want to highlight/segment all the positions where defects occur. So a segmentation into defect or no defect. A kind of probability map on the image. What kind of method would be best to do this without labels. I was thinking that, the biggest entropy/change in a certain part/view direction would be the defect. Since it will be in a random spot, while everything else is basically identical. So I was thinking of using an autoencoder for a specific view and use the points of highest reconstruction error as a starting point for labeling. But this won't work universally. Maybe a contrastive learning method?",15,3,2022-06-04 00:01:50, d  research papers suggestions for x ray defect analysis,i have a lot  several millions  of unlabeled x ray images of aluminium parts with small defects in them  like air bubbles cracks  or voids  these are thousands of images which are basically identical except for the defects  apart from slight variations in position  rotation  noise etc   and thousands of different view directions parts  all image dimensions are identical  the problem is that i do not have a single image with labels and would hence have to label them manually  which i d like to avoid as much as possible i want to highlight segment all the positions where defects occur  so a segmentation into defect or no defect  a kind of probability map on the image  what kind of method would be best to do this without labels  i was thinking that  the biggest entropy change in a certain part view direction would be the defect  since it will be in a random spot  while everything else is basically identical  so i was thinking of using an autoencoder for a specific view and use the points of highest reconstruction error as a starting point for labeling  but this won t work universally  maybe a contrastive learning method 
320,320,feryet,v3ruc6,[D] Good communities/newsletters/mailing lists/twitter accs for ML in healthcare/medical applications?,"Hi.

&#x200B;

I'm looking for communities/mailing lists/newsletters or even good Twitter accounts that are dedicated about the topics of ML in medical applications and healthcare. Any recommendations? In most other applications there seems to be a ton of good options for communities etc, but I honestly could not find one for these topics.",2,26,2022-06-03 10:31:34, d  good communities newsletters mailing lists twitter accs for ml in healthcare medical applications ,hi   xb i m looking for communities mailing lists newsletters or even good twitter accounts that are dedicated about the topics of ml in medical applications and healthcare  any recommendations  in most other applications there seems to be a ton of good options for communities etc  but i honestly could not find one for these topics 
321,321,danielhanchen,v38pwm,"[Project] BFLOAT16 on ALL hardware (>= 2009), up to 2000x faster ML algos, 50% less RAM usage for all old/new hardware - Hyperlearn Reborn.","Hello everyone!! It's been a while!! Years back I released Hyperlearn [https://github.com/danielhanchen/hyperlearn](https://github.com/danielhanchen/hyperlearn). It has 1.2K Github stars, where I made tonnes of algos faster.

PS the current package is UNSTABLE - I'll update it in a few weeks. I set up a Discord link for everyone to join!! [https://discord.gg/tYeh3MCj](https://discord.gg/tYeh3MCj)

I was a bit busy back at NVIDIA and my startup, and I've been casually developing some algos. The question is are people still interested in fast algorithms? **Does anyone want to collaborate on reviving Hyperlearn?** (Or making a NEW package?) *Note the current package is ahhh A MESSS... I'm fixing it - sit tight!!*

NEW algos for release:

1. PCA with 50% less memory usage with **ZERO data corruption**!! (Maths tricks :)) **(ie no need to do X - X.mean()!!!))** How you may ask???!
2. Randomized PCA with 50% less memory usage (**ie no need to do X - X.mean()**).
3. Linear Regression is EVEN faster with now Pivoted Cholesky making algo 100% stable. **No package on the internet to my knowledge has pivoted cholesky solvers.**
4. **Bfloat16 on ALL hardware** all the way down to SSE4!!! (Intel Core i7 2009!!)
5. **Matrix multiplication with Bfloat16 on ALL hardware**/?ASD@! Not the cheap 2x extra memory copying trick - true 0 extra RAM usage on the fly CPU conversion.
6. New Paratrooper Optimizer which trains neural nets 50% faster using the latest fast algos.
7. **Sparse blocked matrix multiplication on ALL hardware (NNs)** !!
8. Super fast Neural Net training with batched multiprocessing (ie when NN is doing backprop on batch 1, we load batch 2 already etc).
9. Super fast softmax making attention `softmax(Q @ K.T / sqrt(d))V` super fast and all operations use the fastest possible matrix multiplciation config (tall skinny, square matrices)
10. AND MORE!!!

Old algos made faster:

1. 70% less time to fit Least Squares / Linear Regression than sklearn + 50% less memory usage
2. 50% less time to fit Non Negative Matrix Factorization than sklearn due to new parallelized algo
3. 40% faster full Euclidean / Cosine distance algorithms
4. 50% less time LSMR iterative least squares
5. 50% faster Sparse Matrix operations - parallelized
6. RandomizedSVD is now 20 - 30% faster

Also you might remember my 50 page machine learning book: [https://drive.google.com/file/d/18fxyBiPE0G4e5yixAj5S--YL\_pgTh3Vo/view?usp=sharing](https://drive.google.com/file/d/18fxyBiPE0G4e5yixAj5S--YL_pgTh3Vo/view?usp=sharing)

https://preview.redd.it/vmmiocvvk7391.png?width=1793&format=png&auto=webp&s=d2c26b4f2fbbfcd007b44d528579a271ea7960cc",156,309,2022-06-02 18:49:30, project  bfloat on all hardware        up to x faster ml algos    less ram usage for all old new hardware   hyperlearn reborn ,hello everyone   it s been a while   years back i released hyperlearn  https ps the current package is unstable   i ll update it in a few weeks  i set up a discord link for everyone to join    https i was a bit busy back at nvidia and my startup  and i ve been casually developing some algos  the question is are people still interested in fast algorithms    does anyone want to collaborate on reviving hyperlearn     or making a new package    note the current package is ahhh a messs    i m fixing it   sit tight   new algos for release   pca with   less memory usage with   zero data corruption      maths tricks        ie no need to do x   x mean          how you may ask      randomized pca with   less memory usage    ie no need to do x   x mean        linear regression is even faster with now pivoted cholesky making algo   stable    no package on the internet to my knowledge has pivoted cholesky solvers       bfloat on all hardware   all the way down to sse     intel core i        matrix multiplication with bfloat on all hardware    asd   not the cheap x extra memory copying trick   true  extra ram usage on the fly cpu conversion   new paratrooper optimizer which trains neural nets   faster using the latest fast algos     sparse blocked matrix multiplication on all hardware  nns        super fast neural net training with batched multiprocessing  ie when nn is doing backprop on batch   we load batch  already etc    super fast softmax making attention  softmax q   k t   sqrt d  v  super fast and all operations use the fastest possible matrix multiplciation config  tall skinny  square matrices   and more   old algos made faster     less time to fit least squares   linear regression than sklearn     less memory usage    less time to fit non negative matrix factorization than sklearn due to new parallelized algo    faster full euclidean   cosine distance algorithms    less time lsmr iterative least squares    faster sparse matrix operations   parallelized  randomizedsvd is now      fasteralso you might remember my  page machine learning book   https https   preview redd it vmmiocvvk png width  format png auto webp s dcbffbbfcdbdaeacc
322,322,valdanylchuk,v3ikpp,[R] Towards artificial general intelligence via a multimodal foundation model (Nature),"This is published in Nature, so supposedly more notable than yet another multimodal experiment. But the way the article presents the results, leaves me confused about how this compares and contrasts to e.g. DeepMind Gato?

https://www.nature.com/articles/s41467-022-30761-2",25,14,2022-06-03 02:20:33, r  towards artificial general intelligence via a multimodal foundation model  nature ,this is published in nature  so supposedly more notable than yet another multimodal experiment  but the way the article presents the results  leaves me confused about how this compares and contrasts to e g  deepmind gato https   www nature com articles s   
323,323,tripleespresso7,v3jemm,"[P] What if AB testing is impossible to setup? I wrote a blog to measure impact using backdoor adjustment, a type of causal analysis","To ensure that every feature has a measurable impact on the broader platform my team will set up and run A/B testing on each new feature or product change, but what happens when a new feature needs to be released quickly and there is not enough time for a traditional testing approach? To make sure that these quick changes could still be measured I found a way to perform accurate pre-post analysis using a back-door adjustment of causal analysis. I wanted to share my findings with the community as it was able to help my team at DoorDash make quick bug fixes and still be able to measure the impact. Please check out the article to get the technical details and provide any feedback on my approach. [https://doordash.engineering/2022/06/02/using-back-door-adjustment-causal-analysis-to-measure-pre-post-effects/](https://doordash.engineering/2022/06/02/using-back-door-adjustment-causal-analysis-to-measure-pre-post-effects/)",2,7,2022-06-03 02:59:00, p  what if ab testing is impossible to setup  i wrote a blog to measure impact using backdoor adjustment  a type of causal analysis,to ensure that every feature has a measurable impact on the broader platform my team will set up and run a b testing on each new feature or product change  but what happens when a new feature needs to be released quickly and there is not enough time for a traditional testing approach  to make sure that these quick changes could still be measured i found a way to perform accurate pre post analysis using a back door adjustment of causal analysis  i wanted to share my findings with the community as it was able to help my team at doordash make quick bug fixes and still be able to measure the impact  please check out the article to get the technical details and provide any feedback on my approach   https   doordash engineering    using back door adjustment causal analysis to measure pre post effects   https   doordash engineering    using back door adjustment causal analysis to measure pre post effects  
324,324,piccalillihighlands,v3js34,[D] Looking for recommended papers on document Key-Value extraction models,Looking for recommended papers on general document Key-value extraction. My searches are coming up mostly with papers that rely on highly domain-specific heuristics. Any good places to start my search would be appreciated.,4,4,2022-06-03 03:15:58, d  looking for recommended papers on document key value extraction models,looking for recommended papers on general document key value extraction  my searches are coming up mostly with papers that rely on highly domain specific heuristics  any good places to start my search would be appreciated 
325,325,Expensive_build,v3gnhs,[D] Recommendation System based on DNN (Softmax Model)," So, I was going through [Recommendation system google's colab](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb) and couldn't understand that for training matrix factorization model, we are giving both user embeddings and movie embeddings to CFModel class. But in case of Softmax model we are only giving movie embeddings. Why not user embeddings too?

I am putting only relevant part of code. Full code can be seen in the link.

[Matrix Factorization Model](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb#scrollTo=XzaABsnno4tn&line=5&uniqifier=1)

>embeddingsÂ =Â {  
""user\_id"":Â U,  
""movie\_id"":Â V  
Â Â }  
returnÂ CFModel(embeddings,Â train\_loss,Â \[metrics\])

[Softmax Model](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb#scrollTo=zAAr73xno4uj&line=1&uniqifier=1) :

>metricsÂ =Â (  
Â Â Â Â Â Â {""train\_loss"":Â train\_loss,Â ""test\_loss"":Â test\_loss},  
Â Â Â Â Â Â {""test\_precision\_at\_10"":Â test\_precision\_at\_10}  
Â Â )  
Â Â embeddingsÂ =Â {""movie\_id"":Â movie\_embeddings}  
returnÂ CFModel(embeddings,Â train\_loss,Â metrics)",2,6,2022-06-03 00:52:40, d  recommendation system based on dnn  softmax model , so  i was going through  recommendation system google s colab  https i am putting only relevant part of code  full code can be seen in the link  matrix factorization model  https  embeddingsÂ  Â    user _id Â u   movie _id Â v  Â Â    returnÂ cfmodel embeddings Â train _loss Â   metrics    softmax model  https  metricsÂ  Â    Â Â Â Â Â Â  train _loss Â train _loss Â test _loss Â test _loss    Â Â Â Â Â Â  test _precision _at _ Â test _precision _at _   Â Â    Â Â embeddingsÂ  Â  movie _id Â movie _embeddings   returnÂ cfmodel embeddings Â train _loss Â metrics 
326,326,assassin_canederlo,v34uhu,[D] Inputs on scalable cost effective pipeline,"Hi, all.

I have multiple deep learning/machine learning / naive based tasks that I want to deploy online through an API. I have been trying to figure out the best way to do it for some time, but I am overwhelmed by the number of different frameworks and packages available on AWS and GCP.

Multiple tools on both platforms seem to have overlapping responsibilities with unclear limitations, making it hard to choose.

I want to obtain a scalable pipeline that saves as much money as possible (using, for example, spot pricing) and is easily expandable with new components.

My idea was to use celery and create a task for each different data processing method I have. The APIs would simply add an entry into the celery's queue, and the workers would take care of the rest.

Scaling up or down the pipeline would be just a matter of adding or removing celery workers, then.

&#x200B;

How would you approach the problem?

Do you know of any resources worth readying to build an architecture like this?

Is there any particular instrument on AWS or GCP that would allow me to easily take care of this task?",12,25,2022-06-02 14:51:10, d  inputs on scalable cost effective pipeline,hi  all i have multiple deep learning machine learning   naive based tasks that i want to deploy online through an api  i have been trying to figure out the best way to do it for some time  but i am overwhelmed by the number of different frameworks and packages available on aws and gcp multiple tools on both platforms seem to have overlapping responsibilities with unclear limitations  making it hard to choose i want to obtain a scalable pipeline that saves as much money as possible  using  for example  spot pricing  and is easily expandable with new components my idea was to use celery and create a task for each different data processing method i have  the apis would simply add an entry into the celery s queue  and the workers would take care of the rest scaling up or down the pipeline would be just a matter of adding or removing celery workers  then   xb how would you approach the problem do you know of any resources worth readying to build an architecture like this is there any particular instrument on aws or gcp that would allow me to easily take care of this task 
327,327,valdanylchuk,v345oq,"[D] Can LLMs be updated, e.g. to follow the daily news? Are there any such regularly updated models publicly known yet?","Some context can be provided in the prompt, but for the bigger picture it is insufficient. I understand companies will not release anything like it until they solve the bias/censorship issues somehow, but did anyone mention an internal demo, or a project in progress? Or is there a lower scale open source experimental project?

It would be so much fun to get some summaries or Q&A on the current events, latest science/tech developments, etc.

Edit (thanks u/adt): There are projects trying to connect a language model to Internet and/or some add-on memory for facts. For example, [WebGPT](https://openai.com/blog/webgpt/) (which might be [on its way to a product launch](https://www.reddit.com/r/GPT3/comments/sa8of3/openai_looking_to_turn_webgpt_into_a_product/)), [BlenderBot 2.0](https://parl.ai/projects/blenderbot2/) [by Meta](https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/), and [Jurassic-X by AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system).",6,9,2022-06-02 14:00:39, d  can llms be updated  e g  to follow the daily news  are there any such regularly updated models publicly known yet ,some context can be provided in the prompt  but for the bigger picture it is insufficient  i understand companies will not release anything like it until they solve the bias censorship issues somehow  but did anyone mention an internal demo  or a project in progress  or is there a lower scale open source experimental project it would be so much fun to get some summaries or q a on the current events  latest science tech developments  etc edit  thanks u adt   there are projects trying to connect a language model to internet and or some add on memory for facts  for example   webgpt  https   openai com blog webgpt    which might be  on its way to a product launch  https   www reddit com r gpt comments saof openai_looking_to_turn_webgpt_into_a_product      blenderbot    https   parl ai projects blenderbot    by meta  https   ai facebook com blog blender bot  an open source chatbot that builds long term memory and searches the internet    and  jurassic x by ai  https   www ai com blog jurassic x crossing the neuro symbolic chasm with the mrkl system  
328,328,Keirp,v2sul7,[R] You Can't Count on Luck: Why Decision Transformers Fail in Stochastic Environments,"**Paper:** https://arxiv.org/abs/2205.15967

**Website:** https://sites.google.com/view/esper-paper",17,62,2022-06-02 03:35:43, r  you can t count on luck  why decision transformers fail in stochastic environments,  paper    https   website    https   sites google com view esper paper
329,329,initiald-ejavu,v354k4,[P] How do I do preprocessing on a flutter app,"So me and my team are making an audio classification app for android. We used a python backend connected to the flutter app for the actual classification part, but now we want to get rid of that and do it all in flutter with tflite. Problem is, we relied on librosa for our data preprocessing (getting a mel spectrogram) and we can't find any libraries to get mel spectrograms in flutter. Does anyone here know of one? Or can recommend another way to preprocess for our tflite model?",15,9,2022-06-02 15:12:34, p  how do i do preprocessing on a flutter app,so me and my team are making an audio classification app for android  we used a python backend connected to the flutter app for the actual classification part  but now we want to get rid of that and do it all in flutter with tflite  problem is  we relied on librosa for our data preprocessing  getting a mel spectrogram  and we can t find any libraries to get mel spectrograms in flutter  does anyone here know of one  or can recommend another way to preprocess for our tflite model 
330,330,i_wasserman,v3fxgq,[D] Pretrained SOTA Medical Classification Models for Download,"I'm doing some research that requires a state of the art medical image classifier for comparison. Can anyone point me in the direction of a performant classifier that has been trained on medical images (any type of medical image e.g. CT scan, x-ray, histopathology, etc.)?",0,0,2022-06-03 00:19:33, d  pretrained sota medical classification models for download,i m doing some research that requires a state of the art medical image classifier for comparison  can anyone point me in the direction of a performant classifier that has been trained on medical images  any type of medical image e g  ct scan  x ray  histopathology  etc   
331,331,hackerllama,v2pq41,[N] The Hugging Face Hub has brand new docs,"Hi there! Omar from Hugging Face here ðŸ¦™ðŸ¤— We've been running the Hub for a while, and the community reception has been great! With a bunch of new models, datasets and demos being added every day, it seems fair to say people are enjoying it. After taking in feedback and releasing some new features, we figured it was time to give our documentation some love. 

**Take a look at our revamped** [**Hub docs**](https://hf.co/docs/hub)**!**  We've restructured the documentation around some central components (models, datasets, and Spaces) and added a bunch of new content that we hope will make the Hub even more handy and easy-to-use!  As usual, the docs are [open-source](https://github.com/huggingface/hub-docs), so PRs are always welcome if you spot any room for improvements.",0,56,2022-06-02 01:22:51, n  the hugging face hub has brand new docs,hi there  omar from hugging face here    we ve been running the hub for a while  and the community reception has been great  with a bunch of new models  datasets and demos being added every day  it seems fair to say people are enjoying it  after taking in feedback and releasing some new features  we figured it was time to give our documentation some love    take a look at our revamped      hub docs    https   hf co docs hub        we ve restructured the documentation around some central components  models  datasets  and spaces  and added a bunch of new content that we hope will make the hub even more handy and easy to use   as usual  the docs are  open source  https   github com huggingface hub docs   so prs are always welcome if you spot any room for improvements 
332,332,1aguschin,v2jnnd,[P] MLEM: ML model deployment tool,"Hi, I'm one of the project creators. MLEM is a tool that helps you deploy your ML models. Itâ€™s a Python library + Command line tool.

1. MLEM can package an ML model into a Docker image or a Python package, and deploy it to, for example, Heroku.

2. MLEM saves all model metadata to a human-readable text file: Python environment, model methods, model input & output data schema and more.

3. MLEM helps you turn your Git repository into a Model Registry with features like ML model lifecycle management.

Our philosophy is that MLOps tools should be built using the Unix approach - each tool solves a single problem, but solves it very well. MLEM was designed to work hands on hands with Git - it saves all model metadata to a human-readable text files and Git becomes a source of truth for ML models. Model weights file can be stored in the cloud storage using a Data Version Control tool or such - independently of MLEM.

Please check out the project: https://github.com/iterative/mlem and the website: https://mlem.ai

Iâ€™d love to hear your feedback!",9,111,2022-06-01 21:01:10, p  mlem  ml model deployment tool,hi  i m one of the project creators  mlem is a tool that helps you deploy your ml models  it s a python library   command line tool   mlem can package an ml model into a docker image or a python package  and deploy it to  for example  heroku   mlem saves all model metadata to a human readable text file  python environment  model methods  model input   output data schema and more   mlem helps you turn your git repository into a model registry with features like ml model lifecycle management our philosophy is that mlops tools should be built using the unix approach   each tool solves a single problem  but solves it very well  mlem was designed to work hands on hands with git   it saves all model metadata to a human readable text files and git becomes a source of truth for ml models  model weights file can be stored in the cloud storage using a data version control tool or such   independently of mlem please check out the project  https i d love to hear your feedback 
333,333,Left_Ad8361,v2hq70,[P] Turn any Jupyter cell into a shareable web-hosted Python script,"&#x200B;

https://reddit.com/link/v2hq70/video/lzd4xeggm0391/player

This is a follow-up on a [post](https://www.reddit.com/r/MachineLearning/comments/uosqgm/p_i_was_tired_of_screenshotting_plots_in_jupyter/) from two weeks ago.

To communicate results, data scientists often resort to screenshotting plots and sharing them over Slack or email. Visualizations are great to summarize findings, but they're often ambiguous and lack context. 

To make communication easier, we've created a new `%%publish` magic (previously named `%%share`) that decouples a Jupyter cell from its notebook and transforms it into an standalone web-hosted Python script, easily shareable and runnable.

While we've supported the ability to share cells publicly, many of you expressed that the results you want to share need to remain private.

**Today, we're excited to announce that the Jupyter cells you publish are now private by default.**

Running

    %%publish
    # Your Python code goes here..

will bring you to the web app, where you can select the list of person to share with.

Private sharing requires you to create an account. If you don't want to, you can still share publicly by using the following:

    %%publish --public
    # Your Python code goes here..

Try it out in Colab: [https://colab.research.google.com/drive/1E5oU6TjH6OocmvEfU-foJfvCTbTfQrqd?usp=sharing](https://colab.research.google.com/drive/1E5oU6TjH6OocmvEfU-foJfvCTbTfQrqd?usp=sharing)

Docs: [https://docs.1000words-hq.com/](https://docs.1000words-hq.com/)

Source code of the Python client: [https://github.com/edouard-g/thousandwords](https://github.com/edouard-g/thousandwords)

Homepage: [https://1000words-hq.com](https://1000words-hq.com)",8,126,2022-06-01 19:32:35, p  turn any jupyter cell into a shareable web hosted python script,  xb https this is a follow up on a  post  https to communicate results  data scientists often resort to screenshotting plots and sharing them over slack or email  visualizations are great to summarize findings  but they re often ambiguous and lack context  to make communication easier  we ve created a new    publish  magic  previously named    share   that decouples a jupyter cell from its notebook and transforms it into an standalone web hosted python script  easily shareable and runnable while we ve supported the ability to share cells publicly  many of you expressed that the results you want to share need to remain private   today  we re excited to announce that the jupyter cells you publish are now private by default   running      publish      your python code goes here  will bring you to the web app  where you can select the list of person to share with private sharing requires you to create an account  if you don t want to  you can still share publicly by using the following       publish   public      your python code goes here  try it out in colab   https docs   https source code of the python client   https homepage   https   words hq com  https   words hq com 
334,334,diningeachox,v3ckz3,[P] Live Video Inference,"Hi everyone,

I have a dataset of annotated videos for binary classification. I.e. each video just has one label: positive or negative. Right now I have a trained model in place for taking a complete video as input and outputting a classification. So far so good.

Now the clients I'm working with want a feature for inference on live videos as well. I'm sort of at a loss  on how to achieve this with the dataset I'm given. Since live video inference would entail inferring frame by frame and I don't actually have any labelled frames (only the entire video is given a label).

So what I'm thinking is that I have to somehow figure out which frames in the positively-labelled videos are the ones to actually make the video positive. Is there a theory for this kind of problem? 

Here's what I've considered so far:

1) Use an anomaly detection algorithm to figure out which frames are anomalous to the others, and use those frames as positive frames for training the frame-by-frame model.

2) This is a long shot but you take out one frame at a time for the positive videos and infer on the new video with that frame missing. Then see which frame(s) makes the confidence score for positivity drop the most after being taken out.  

I'm sure there are better ideas out there. Anyone have any suggestions or papers I could look into about this kind of problem?",0,0,2022-06-02 21:49:13, p  live video inference,hi everyone i have a dataset of annotated videos for binary classification  i e  each video just has one label  positive or negative  right now i have a trained model in place for taking a complete video as input and outputting a classification  so far so good now the clients i m working with want a feature for inference on live videos as well  i m sort of at a loss  on how to achieve this with the dataset i m given  since live video inference would entail inferring frame by frame and i don t actually have any labelled frames  only the entire video is given a label  so what i m thinking is that i have to somehow figure out which frames in the positively labelled videos are the ones to actually make the video positive  is there a theory for this kind of problem  here s what i ve considered so far   use an anomaly detection algorithm to figure out which frames are anomalous to the others  and use those frames as positive frames for training the frame by frame model   this is a long shot but you take out one frame at a time for the positive videos and infer on the new video with that frame missing  then see which frame s  makes the confidence score for positivity drop the most after being taken out   i m sure there are better ideas out there  anyone have any suggestions or papers i could look into about this kind of problem 
335,335,mehmetecevit,v39wnd,[N] [P] Machine Learning in dbt DAG,"Today, we are super excited to announce our open-source [Layer DBT Bigquery Adapter](https://github.com/layerai/dbt-adapters) which runs ML pipelines **inside dbt DAG** with BigQuery *(more DWH support coming soon...)* as the backing data warehouse. It's in a very early stage but we wanted share it with you to get your feedback.

    SELECT id, layer.predict(""layer/clothing/models/objectdetection"", ARRAY[image]) 
    FROM {{ ref(""products"") }}

**ðŸ“·** **With Layer dbt Adapter you can:**

* Score your data with a machine learning model from Layer with SQL.
* Train an AutoML model with your data *\[coming soon...\]*
* Train a custom machine learning model with your data *\[coming soon...\]*

**ðŸ“·** **Dive into dbt examples:**

* [Predicting survials of Titanic](https://github.com/layerai/examples-dbt/tree/main/titanic) \- End to end ML pipeline (feature extraction+scoring) which predicts the survivals of the Titanic disaster.
* [Sentiment analysis of product reviews](https://github.com/layerai/examples-dbt/tree/main/sentiment_analysis) \- An example that shows how to make multi-language sentiment analysis.
* [Object detection in product images](https://github.com/layerai/examples-dbt/tree/main/cloth_detector) \- Detects cloths from product images using a pretrained computer vision model.

We would love your feedback on our new open-source tool! It will highly influence our roadmap.Thank you!",0,0,2022-06-02 19:48:07, n   p  machine learning in dbt dag,today  we are super excited to announce our open source  layer dbt bigquery adapter  https     select id  layer predict layer clothing models objectdetection  array image       from    ref products            with layer dbt adapter you can     score your data with a machine learning model from layer with sql   train an automl model with your data    coming soon        train a custom machine learning model with your data    coming soon              dive into dbt examples      predicting survials of titanic  https    sentiment analysis of product reviews  https    object detection in product images  https we would love your feedback on our new open source tool  it will highly influence our roadmap thank you 
336,336,MassivePellfish,v3g3pb,"[N] FAIR gets ""decentralilzed""","Announcement: https://ai.facebook.com/blog/building-with-ai-across-all-of-meta/

> In the new model we will distribute the ownership of these AI systems back to Metaâ€™s product groups. But we do so with the caveat that they must invest in a balanced portfolio that supports existing systems while also advancing the state of the art in AI.",3,0,2022-06-03 00:27:31, n  fair gets decentralilzed,announcement  https   in the new model we will distribute the ownership of these ai systems back to meta s product groups  but we do so with the caveat that they must invest in a balanced portfolio that supports existing systems while also advancing the state of the art in ai 
337,337,yyang_13,v2af3k,[R] Multi-Agent Reinforcement Learning can now be solved by the Transformer!,"&#x200B;

[Multi-Agent Transformer](https://preview.redd.it/6dldl9iicy291.png?width=1484&format=png&auto=webp&s=7b0e506233cc27392cc9d1b0d7b5a58bc6a41376)

Large sequence models (BERT, GPT-series) have demonstrated remarkable progress on visual language tasks. However, how to abstract RL/MARL problems into a sequence modelling problem is still unknown. Here we introduce **Multi-Agent Transformer** that naturally turns MARL problem into a sequence modelling problem. The key insight is the **multi-agent** **advantage decomposition theorem** (a lemma we happen to discover during the development of **HATRPO/HAPPO \[ICLR 22\]** [**https://openreview.net/forum?id=EcGGFkNTxdJ**](https://openreview.net/forum?id=EcGGFkNTxdJ)), which surprisingly and effectively turns multi-agent learning problems into sequential decision-making problems, thus MARL is implementable and solvable by the decoder architecture in the Transformer, with no hacks needed at all!

MAT is different from Decision Transformer or GATO which are purely trained on pre-collected offline demonstration data (more like a supervised learning task), but rather MAT is trained online by trails and errors (also, it is an on-policy RL method). Experiments on StarCraft II, Bimanual Dexterous Hands, MA-MuJoCo, and Google Football show MAT's superior performance (stronger than **MAPPO** and **HAPPO**).

Check our paper & project page at:

[https://arxiv.org/abs/2205.14953](https://arxiv.org/abs/2205.14953)",8,163,2022-06-01 11:56:23, r  multi agent reinforcement learning can now be solved by the transformer ,  xb  multi agent transformer  https large sequence models  bert  gpt series  have demonstrated remarkable progress on visual language tasks  however  how to abstract rl marl problems into a sequence modelling problem is still unknown  here we introduce   multi agent transformer   that naturally turns marl problem into a sequence modelling problem  the key insight is the   multi agent     advantage decomposition theorem    a lemma we happen to discover during the development of   hatrpo happo   iclr         https mat is different from decision transformer or gato which are purely trained on pre collected offline demonstration data  more like a supervised learning task   but rather mat is trained online by trails and errors  also  it is an on policy rl method   experiments on starcraft ii  bimanual dexterous hands  ma mujoco  and google football show mat s superior performance  stronger than   mappo   and   happo    check our paper   project page at  https   arxiv org abs    https   arxiv org abs   
338,338,hardmaru,v2tyrz,[R] Attribution-based Explanations that Provide Recourse Cannot be Robust,,1,7,2022-06-02 04:24:12, r  attribution based explanations that provide recourse cannot be robust,nan
339,339,LiquidDinosaurs69,v2phwb,[D] Adjoint Sensitivity Method vs Reverse Mode Autodiff,"Can  anyone explain to me why Adjoint Sensitivity is considered the most  efficient way to calculate the gradient of a loss function with respect  to an ODE's parameters?

x\_dot = f(x; theta),    L(x x\_f) = (x - x\_f)\^2

I  first found out about adjoint sensitivity in the NueralODE paper. And  since then I've seen numerous sources on the internet claiming that  adjoint sensitivity is the most efficient way to compute the gradient. I  don't see why this would be the case. I implemented ASM for a system  and compared the performance against reverse mode autodiff of the  numerical solution. I didn't notice a significant performance  improvement with ASM. Additionally, it seems to me like they both  require the same number of integration steps and roughly the same number  of evaluations of the function evaluations.

One  reason ASM might be faster is because I read there are efficient ways  to compute vector Jacobian product in autodiff libraries. Also an  explanation for my implementation performance would be that the library I  was using didn't have this feature.",10,5,2022-06-02 01:13:34, d  adjoint sensitivity method vs reverse mode autodiff,can  anyone explain to me why adjoint sensitivity is considered the most  efficient way to calculate the gradient of a loss function with respect  to an ode s parameters x _dot   f x  theta      l x x _f     x   x _f   i  first found out about adjoint sensitivity in the nueralode paper  and  since then i ve seen numerous sources on the internet claiming that  adjoint sensitivity is the most efficient way to compute the gradient  i  don t see why this would be the case  i implemented asm for a system  and compared the performance against reverse mode autodiff of the  numerical solution  i didn t notice a significant performance  improvement with asm  additionally  it seems to me like they both  require the same number of integration steps and roughly the same number  of evaluations of the function evaluations one  reason asm might be faster is because i read there are efficient ways  to compute vector jacobian product in autodiff libraries  also an  explanation for my implementation performance would be that the library i  was using didn t have this feature 
340,340,inFamous_16,v2h38a,[P] what is the most efficient way to pattern matching word-to-word?,"I want to perform pattern matching task as a part of Pre-processing. I have more than 3M text sentences. On the other hand, I have around 130k terms (may contain multi-word terms separated by spaces) which I want to match with the text in those 3M sentences. The expected output is the matched terms per sentence, if any. Is there any efficient way you know of? I am also considering lowercasing text on both the sides as pattern matching is allowed to be case-insensitive.",15,10,2022-06-01 19:01:19, p  what is the most efficient way to pattern matching word to word ,i want to perform pattern matching task as a part of pre processing  i have more than m text sentences  on the other hand  i have around k terms  may contain multi word terms separated by spaces  which i want to match with the text in those m sentences  the expected output is the matched terms per sentence  if any  is there any efficient way you know of  i am also considering lowercasing text on both the sides as pattern matching is allowed to be case insensitive 
341,341,NMister_,v1zzh8,[D] DALL-E 2 Has Its Own Secret Language,"I discovered this through the thread here:

[Twitter Thread](https://twitter.com/giannis_daras/status/1531693093040230402?s=21)

The full paper is here:

[Paper](https://giannisdaras.github.io/publications/Discovering_the_Secret_Language_of_Dalle.pdf)

It seems that the garbled text that Dall-E 2 generates can be run back through the model to produce consistent results. Weird associations like â€œApoploe vesrreaitaisâ€ meaning a specific type of bird in a lot of different contexts. 

Really cool find IMO! I guess it makes sense that if Dall-E canâ€™t distinguish between these tokens semantically when generating images, it wonâ€™t be able distinguish between them as prompts. Anyone know of similar results / other explanations for this phenomenon?",25,146,2022-06-01 02:30:09, d  dall e  has its own secret language,i discovered this through the thread here  twitter thread  https the full paper is here  paper  https it seems that the garbled text that dall e  generates can be run back through the model to produce consistent results  weird associations like  apoploe vesrreaitais  meaning a specific type of bird in a lot of different contexts  really cool find imo  i guess it makes sense that if dall e can t distinguish between these tokens semantically when generating images  it won t be able distinguish between them as prompts  anyone know of similar results   other explanations for this phenomenon 
342,342,Character-Rip-5824,v2tsp6,"""[Project]"" Brainchop: In-browser deep learning framework for volumetric Segmentation","This is a follow-up on a [post](https://www.reddit.com/r/MachineLearning/comments/uu5lz0/project_brainchop_inbrowser_deep_learning/) from two weeks ago about [brainchop.org](https://neuroneural.github.io/brainchop/) tool.

We released a [discussion](https://github.com/neuroneural/brainchop/discussions) board to share ideas with our supporters.

&#x200B;

https://preview.redd.it/idl8o4oz73391.png?width=570&format=png&auto=webp&s=04605e074e16adae4993b708a268f04c3988aaa3",0,1,2022-06-02 04:17:12, project  brainchop  in browser deep learning framework for volumetric segmentation,this is a follow up on a  post  https we released a  discussion  https   xb https   preview redd it idlooz png width  format png auto webp s eeadaebafcaaa
343,343,redxammer,v1qcmc,[D] Do you need to grind leetcode for junior or mid level AI/ML Engineer interviews?, Are AI/ML interviews centred on DSA problem solving found on the well-known websites without much emphasis on specific AI/ML theory or would they feature Pytorch/TensorFlow model creation/training problems? How much emphasis is placed on relevant experience and past projects?,95,289,2022-05-31 19:00:52, d  do you need to grind leetcode for junior or mid level ai ml engineer interviews , are ai ml interviews centred on dsa problem solving found on the well known websites without much emphasis on specific ai ml theory or would they feature pytorch tensorflow model creation training problems  how much emphasis is placed on relevant experience and past projects 
344,344,National-Tennis-4528,v2qavr,[N] We released a new tool on the App Store to annotate your images. On your iPad.,"[App Store â€” ToolZ](https://apps.apple.com/fr/app/toolz/id1625671349?l=en)

Itâ€™s called **ToolZ**.

3 years in the making, rewritten from scratch in 6 months (Swift)â€¦ Weâ€™ve used it to ship three models to production (got the scars on my body to show for it). And label more than 50â€™000 images â€” hence affording the time to get something that works properly for large volumes of data.

My 3 favorite features:

- annotate from your couch, using the pencil. (Or sitting on a plane in a transatlantic flight, did that)
- Boxes are written into the EXIF of the images, and you can therefore move them around without tracking them
- load huge archives (tested up to 6Gb zip or tar.gz) and annotate directly in them

Try it free for 7 days, we hope youâ€™ll stick along for the ride with us after.",1,2,2022-06-02 01:47:25, n  we released a new tool on the app store to annotate your images  on your ipad , app store   toolz  https it s called   toolz    years in the making  rewritten from scratch in  months  swift   we ve used it to ship three models to production  got the scars on my body to show for it   and label more than   images   hence affording the time to get something that works properly for large volumes of data my  favorite features   annotate from your couch  using the pencil   or sitting on a plane in a transatlantic flight  did that   boxes are written into the exif of the images  and you can therefore move them around without tracking them  load huge archives  tested up to gb zip or tar gz  and annotate directly in themtry it free for  days  we hope you ll stick along for the ride with us after 
345,345,TerrificJam,v277c1,[D] Unreal Engine 5 vs Unity for ML Research,"Iâ€™m working as part of a research team that is looking into Safe Reinforcement Learning. We are developing an algorithm that we want to deploy on 3D games that our own agents can learn in.
Does anyone have any opinions on the new Unreal Engine 5 for ML research and how that compares to Unity ML?",5,19,2022-06-01 08:35:26, d  unreal engine  vs unity for ml research,i m working as part of a research team that is looking into safe reinforcement learning  we are developing an algorithm that we want to deploy on d games that our own agents can learn in does anyone have any opinions on the new unreal engine  for ml research and how that compares to unity ml 
346,346,ombelicoInfinito,v1xepz,"[D] Machine learning ""for good""?","I am finishing a Ph.D. in NLP/ML and starting to think about where to go next. I will likely start with an internship / some work in a big company (FAANG or similar, I have already passed some interviews) but if I think of spending my life working to ultimately increase click rates on products /ads I really get depressed. I am extremely attracted to companies with a high technical level but I also need to have the sensation of doing something meaningful (this does not necessarily mean working for an NGO, but there are many open problems worth solving, I believe). 

I just do not understand why FAANG can really make things work most of the time with the ultimate goal of making billionaires richer, while companies working on important problems are unorganized and ultimately often come up with solutions that are probably technically very far from the optimum. Not that I believe ML is gold and we will get AGI in 5 years (I see the whole marketing bubble) but if Uber can optimize their taxis you can optimize streets to minimize traffic, or the improve the 911 strategy.

So (rant over) do you know of any company that deals with an ""important"" /real problem and has a high technical level? For example anything in health, education, public whatever, NGOs, poverty prevention, etc.",60,74,2022-06-01 00:26:36, d  machine learning for good ,i am finishing a ph d  in nlp ml and starting to think about where to go next  i will likely start with an internship   some work in a big company  faang or similar  i have already passed some interviews  but if i think of spending my life working to ultimately increase click rates on products  ads i really get depressed  i am extremely attracted to companies with a high technical level but i also need to have the sensation of doing something meaningful  this does not necessarily mean working for an ngo  but there are many open problems worth solving  i believe   i just do not understand why faang can really make things work most of the time with the ultimate goal of making billionaires richer  while companies working on important problems are unorganized and ultimately often come up with solutions that are probably technically very far from the optimum  not that i believe ml is gold and we will get agi in  years  i see the whole marketing bubble  but if uber can optimize their taxis you can optimize streets to minimize traffic  or the improve the  strategy so  rant over  do you know of any company that deals with an important  real problem and has a high technical level  for example anything in health  education  public whatever  ngos  poverty prevention  etc 
347,347,Rafaelkoll,v2cpig,[D] Minimum Description Length applied to KMeans,"Is anyone familiar with the Minimum Description Length principle? Can somebody help me derive the formula to apply it to KMeans?

I want to apply it to evaluate the trade-off between different complexity models - where different models are given different number of principal components following a PCA of high dimensional data.

This is the basic formula for MDL on statistical models, where P is the number of free parameters, X are the data points, |X| is the number (count) of data points, p(x) is the probability distribution density at the given data point.

https://preview.redd.it/xg6my8sh6z291.png?width=1264&format=png&auto=webp&s=d7ad99aa2096514ee3619ed085cc0aea4f3b896a

I am especially confused about what to use for the number of free parameters (P) and any help and insights would be greatly appreciated",1,4,2022-06-01 14:40:59, d  minimum description length applied to kmeans,is anyone familiar with the minimum description length principle  can somebody help me derive the formula to apply it to kmeans i want to apply it to evaluate the trade off between different complexity models   where different models are given different number of principal components following a pca of high dimensional data this is the basic formula for mdl on statistical models  where p is the number of free parameters  x are the data points   x  is the number  count  of data points  p x  is the probability distribution density at the given data point https i am especially confused about what to use for the number of free parameters  p  and any help and insights would be greatly appreciated
348,348,red_dragon,v29pai,[D] Has anyone used Low-Rank Decomposition to reduce model size / latency?,"I am working on a project and wondering if any one has used any kind of low-rank decomposition to reduce the overhead of large neural net layers. For instance, I can think of reducing a dense layer size, by decomposing it into two smaller layers, while retaining the output dimensions to be the same.  


Can this be done through SVD, PCA, or some other means? Interested in learning about people's experiences.",6,7,2022-06-01 11:07:24, d  has anyone used low rank decomposition to reduce model size   latency ,i am working on a project and wondering if any one has used any kind of low rank decomposition to reduce the overhead of large neural net layers  for instance  i can think of reducing a dense layer size  by decomposing it into two smaller layers  while retaining the output dimensions to be the same   can this be done through svd  pca  or some other means  interested in learning about people s experiences 
349,349,Singularian2501,v1xrxv,[R] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Paper: [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)

Twitter: [https://twitter.com/tri\_dao/status/1531437619791290369?t=UXOZXyk1p9CCrMJLlkDcDg&s=19](https://twitter.com/tri_dao/status/1531437619791290369?t=UXOZXyk1p9CCrMJLlkDcDg&s=19)

Abstract: 

"" Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: **15% end-to-end wall-clock speedup** on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, **3Ã— speedup on GPT-2** (seq. length 1K), and 2.4Ã— speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the **Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy**). ""

https://preview.redd.it/fddmves70v291.jpg?width=1070&format=pjpg&auto=webp&s=3493558bfb05aa755a429b15cdb0c4ab6754ba04

https://preview.redd.it/8x9w8l680v291.jpg?width=1176&format=pjpg&auto=webp&s=984c843e27cf9e6c2ac81fbaddd0d514ac4ff005

[Scales to up to 64k Tokens! GPT-3 hat only 2048!](https://preview.redd.it/0lhstzy90v291.jpg?width=1048&format=pjpg&auto=webp&s=02436c2b6121bb91a45804237060608220682d7a)

https://preview.redd.it/7tduvig53v291.jpg?width=466&format=pjpg&auto=webp&s=84e754819decdf9d6a723d40b3f4f227011891ef",6,44,2022-06-01 00:49:43, r  flashattention  fast and memory efficient exact attention with io awareness,paper   https twitter   https abstract   transformers are slow and memory hungry on long sequences  since the time and memory complexity of self attention are quadratic in sequence length  approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity  but often do not achieve wall clock speedup  we argue that a missing principle is making attention algorithms io aware    accounting for reads and writes between levels of gpu memory  we propose flashattention  an io aware exact attention algorithm that uses tiling to reduce the number of memory reads writes between gpu high bandwidth memory  hbm  and gpu on chip sram  we analyze the io complexity of flashattention  showing that it requires fewer hbm accesses than standard attention  and is optimal for a range of sram sizes  we also extend flashattention to block sparse attention  yielding an approximate attention algorithm that is faster than any existing approximate attention method  flashattention trains transformers faster than existing baselines      end to end wall clock speedup   on bert large  seq  length   compared to the mlperf   training speed record      speedup on gpt     seq  length k   and    speedup on long range arena  seq  length k k   flashattention and block sparse flashattention enable longer context in transformers  yielding higher quality models    better perplexity on gpt  and   points of lift on long document classification  and entirely new capabilities  the first transformers to achieve better than chance performance on the   path x challenge  seq  length k     accuracy  and path   seq  length k     accuracy     https https  scales to up to k tokens  gpt  hat only    https https   preview redd it tduvigv jpg width  format pjpg auto webp s edecdfdadbffef
350,350,hwangoh,v22c69,[R] Solving Bayesian Inverse Problems via Variational Autoencoders,"Paper: [https://proceedings.mlr.press/v145/goh22a.html](https://proceedings.mlr.press/v145/goh22a.html)

Code: [https://github.com/hwangoh/uq-vae](https://github.com/hwangoh/uq-vae)

Talk: [https://www.youtube.com/watch?v=jaCNWZnpHgQ&t=684s](https://www.youtube.com/watch?v=jaCNWZnpHgQ&t=684s)

This mathematically justified framework offers a hybrid model/data-driven approach to uncertainty quantification. When a neural network comes into play, the training process works to embed the information contained within both the model and dataset into the weights. The output of a trained network then quantifies the uncertainty in the underlying parameter estimation problem using this information. We call a neural network trained under this framework and possessing this capability a ""UQ-VAE"".",15,15,2022-06-01 04:18:08, r  solving bayesian inverse problems via variational autoencoders,paper   https code   https talk   https this mathematically justified framework offers a hybrid model data driven approach to uncertainty quantification  when a neural network comes into play  the training process works to embed the information contained within both the model and dataset into the weights  the output of a trained network then quantifies the uncertainty in the underlying parameter estimation problem using this information  we call a neural network trained under this framework and possessing this capability a uq vae 
351,351,qazmkopp,v2dlgq,[P]MMML | Deploy HuggingFace training model rapidly based on MetaSpore," A few days ago, HuggingFace announced a $100 million Series C funding round, which was big news in open source machine learning and could be a sign of where the industry is headed. Two days before the HuggingFace funding announcement, open-source machine learning platform [**MetaSpore**](https://github.com/meta-soul/MetaSpore) released a demo based on the HuggingFace Rapid deployment pre-training model.

As deep learning technology makes innovative breakthroughs in computer vision, natural language processing, speech understanding, and other fields, more and more unstructured data are perceived, understood, and processed by machines. These advances are mainly due to the powerful learning ability of deep learning. Through pre-training of deep models on massive data, the models can capture the internal data patterns, thus helping many downstream tasks. With the industry and academia investing more and more energy in the research of pre-training technology, the distribution warehouses of pre-training models such as HuggingFace and Timm have emerged one after another. The open-source community release pre-training significant model dividends at an unprecedented speed.

In recent years, the data form of machine modeling and understanding has gradually evolved from single-mode to multi-mode, and the semantic gap between different modes is being eliminated, making it possible to retrieve data across modes. Take CLIP, OpenAIâ€™s open-source work, as an example, to pre-train the twin towers of images and texts on a dataset of 400 million pictures and texts and connect the semantics between pictures and texts. Many researchers in the academic world have been solving multimodal problems such as image generation and retrieval based on this technology. Although the frontier technology through the semantic gap between modal data, there is still a heavy and complicated model tuning, offline data processing, high performance online reasoning architecture design, heterogeneous computing, and online algorithm be born multiple processes and challenges, hindering the frontier multimodal retrieval technologies fall to the ground and pratt &whitney.

[**DMetaSoul**](https://github.com/meta-soul) aims at the above technical pain points, abstracting and uniting many links such as model training optimization, online reasoning, and algorithm experiment, forming a set of solutions that can quickly apply offline pre-training model to online. This paper will introduce how to use the HuggingFace community pre-training model to conduct online reasoning and algorithm experiments based on [**MetaSpore**](https://github.com/meta-soul/MetaSpore) technology ecology so that the benefits of the pre-training model can be fully released to the specific business or industry and small and medium-sized enterprises. And we will give the text search text and text search graph two multimodal retrieval demonstration examples for your reference.

**1. Multimodal semantic retrieval**

The sample architecture of multimodal retrieval is as follows:  
Our multimodal retrieval system supports both text search and text search application scenarios, including offline processing, model reasoning, online services, and other core modules:

&#x200B;

https://preview.redd.it/w4v4c7vcez291.png?width=1834&format=png&auto=webp&s=0687efb1fddb26e8e30cb844d398ec712b947f31

 

1. Offline processing, including offline data processing processes for different application scenarios of text search and text search, including model tuning, model export, data index database construction, data push, etc.
2. Model inference. After the offline model training, we deployed our NLP and CV large models based on the MetaSpore Serving framework. MetaSpore Serving helps us conveniently perform online inference, elastic scheduling, load balancing, and resource scheduling in heterogeneous environments.
3. Online services. Based on MetaSporeâ€™s online algorithm application framework, MetaSpore has a complete set of reusable online search services, including Front-end retrieval UI, multimodal data preprocessing, vector recall and sorting algorithm, AB experimental framework, etc. MetaSpore also supports text search by text and image scene search by text and can be migrated to other application scenarios at a low cost.

The HuggingFace open source community has provided several excellent baseline models for similar multimodal retrieval problems, which are often the starting point for actual optimization in the industry. MetaSpore also uses the pre-training model of the HuggingFace community in its online services of searching words by words and images by words. Searching words by words is based on the semantic similarity model of the question and answer field optimized by MetaSpore, and searching images by words is based on the community pre-training model.

These community open source pre-training models are exported to the general ONNX format and loaded into MetaSpore Serving for online reasoning. The following sections will provide a detailed description of the model export and online retrieval algorithm services. The reasoning part of the model is standardized SAAS services with low coupling with the business. Interested readers can refer to my previous post: The design concept of MetaSpore, a new generation of the one-stop machine learning platform.

**1.1 Offline Processing**  
Offline processing mainly involves the export and loading of online models and index building and pushing of the document library. You can follow the step-by-step instructions below to complete the offline processing of text search and image search and see how the offline pre-training model achieves reasoning at MetaSpore.

***1.1.1 Search text by text***  
Traditional text retrieval systems are based on literal matching algorithms such as BM25. Due to usersâ€™ diverse query words, a semantic gap between query words and documents is often encountered. For example, users misspell â€œiPhoneâ€ as â€œPhone,â€ and search terms are incredibly long, such as â€œ1 \~ 3 months old baby autumn small size bag pantsâ€. Traditional text retrieval systems will use spelling correction, synonym expansion, search terms rewriting, and other means to alleviate the semantic gap but fundamentally fail to solve this problem. Only when the retrieval system fully understands usersâ€™ query terms and documents can it meet usersâ€™ retrieval demands at the semantic level. With the continuous progress of pre-training and representational learning technology, some commercial search engines continue to integrate semantic vector retrieval methods based on symbolic learning into the retrieval ecology.

***Semantic retrieval model***  
This paper introduces a set of semantic vector retrieval applications. MetaSpore built a set of semantic retrieval systems based on encyclopedia question and answer data. MetaSpore adopted the Sentence-Bert model as the semantic vector representation model, which fine-tunes the twin tower BERT in supervised or unsupervised ways to make the model more suitable for retrieval tasks. The model structure is as follows:

The query-Doc symmetric two-tower model is used in text search and question and answer retrieval. The vector representation of online Query and offline DOC share the same vector representation model, so it is necessary to ensure the consistency of the offline DOC library building model and online Query inference model. The case uses MetaSporeâ€™s text representation model Sbert-Chinese-QMC-domain-V1, optimized in the open-source semantically similar data set. This model will express the question and answer data as a vector in offline database construction. The user query will be expressed as a vector by this model in online retrieval, ensuring that query-doc in the same semantic space, usersâ€™ semantic retrieval demands can be guaranteed by vector similarity metric calculation.

Since the text presentation model does vector encoding for Query online, we need to export the model for use by the online service. Go to the q&A data library code directory and export the model concerning the documentation. In the script, Pytorch Tracing is used to export the model. The models are exported to the â€œ./export â€œdirectory. The exported models are mainly ONNX models used for wired reasoning, Tokenizer, and related configuration files. The exported models are loaded into MetaSpore Serving by the online Serving system described below for model reasoning. Since the exported model will be copied to the cloud storage, you need to configure related variables in env.sh.

**\_Build library based on text search \_**  
The retrieval database is built on the million-level encyclopedia question and answer data set. According to the description document, you need to download the data and complete the database construction. The question and answer data will be coded as a vector by the offline model, and then the database construction data will be pushed to the service component. The whole process of database construction is described as follows:

1. Preprocessing, converting the original data into a more general JSonline format for database construction;
2. Build index, use the same model as online â€œsbert-Chinese-qmc-domain-v1â€ to index documents (one document object per line);
3. Push inverted (vector) and forward (document field) data to each component server.

The following is an example of the database data format. After offline database construction is completed, various data are pushed to corresponding service components, such as Milvus storing vector representation of documents and MongoDB storing summary information of documents. Online retrieval algorithm services will use these service components to obtain relevant data.

***1.1.2 Search by text***  
Text and images are easy for humans to relate semantically but difficult for machines. First of all, from the perspective of data form, the text is the discrete ID type of one-dimensional data based on words and words. At the same time, images are continuous two-dimensional or three-dimensional data. Secondly, the text is a subjective creation of human beings, and its expressive ability is vibrant, including various turning points, metaphors, and other expressions, while images are machine representations of the objective world. In short, bridging the semantic gap between text and image data is much more complex than searching text by text. The traditional text search image retrieval technology generally relies on the external text description data of the image or the nearest neighbor retrieval technology and carries out the retrieval through the image associated text, which in essence degrades the problem to text search. However, it will also face many issues, such as obtaining the associated text of pictures and whether the accuracy of text search by text is high enough. The depth model has gradually evolved from single-mode to multi-mode in recent years. Taking the open-source project of OpenAI, CLIP, as an example, train the model through the massive image and text data of the Internet and map the text and image data into the same semantic space, making it possible to implement the text and image search technology based on semantic vector.

***CLIP graphic model***  
The text search pictures introduced in this paper are implemented based on semantic vector retrieval, and the CLIP pre-training model is used as the two-tower retrieval architecture. Because the CLIP model has trained the semantic alignment of the twin towersâ€™ text and image side models on the massive graphic and text data, it is particularly suitable for the text search graph scene.

Due to the different image and text data forms, the Query-Doc asymmetric twin towers model is used for text search image retrieval. The image-side model of the twin towers is used for offline database construction, and the text-side model is used for the online return. In the final online retrieval, the database data of the image side model will be searched after the text side model encodes Query, and the CLIP pre-training model guarantees the semantic correlation between images and texts. The model can draw the graphic pairs closer in vector space by pre-training on a large amount of visual data.  
Here we need to export the text-side model for online MetaSpore Serving inference. Since the retrieval scene is based on Chinese, the CLIP model supporting Chinese understanding is selected. The exported content includes the ONNX model used for online reasoning and Tokenizer, similar to the text search. MetaSpore Serving can load model reasoning through the exported content.

***Build library on Image search***  
You need to download the Unsplash Lite library data and complete the construction according to the instructions. The whole process of database construction is described as follows:

1. Preprocessing, specify the image directory, and then generate a more general JSOnline file for library construction;
2. Build index, use OpenAI/Clip-Vit-BASE-Patch32 pre-training model to index the gallery, and output one document object for each line of index data;
3. Push inverted (vector) and forward (document field) data to each component server. Like text search, after offline database construction, relevant data will be pushed to service components, called by online retrieval algorithm services to obtain relevant data.

**1.2 Online Services**  
The overall online service architecture diagram is as follows:

https://preview.redd.it/jfsl8hdfez291.png?width=1280&format=png&auto=webp&s=a858e2304a0c93e78ba5429612ca08cbee69b35a

Multi-mode search online service system supports application scenarios such as text search and text search. The whole online service consists of the following parts:

1. Query preprocessing service: encapsulate preprocessing logic (including text/image, etc.) of pre-training model, and provide services through gRPC interface;
2. Retrieval algorithm service: the whole algorithm processing link includes AB experiment tangent flow configuration, MetaSpore Serving call, vector recall, sorting, document summary, etc.;
3. User entry service: provides a Web UI interface for users to debug and track down problems in the retrieval service.

From a user request perspective, these services form invocation dependencies from back to front, so to build up a multimodal sample, you need to run each service from front to back first. Before doing this, remember to export the offline model, put it online and build the library first. This article will introduce the various parts of the online service system and make the whole service system step by step according to the following guidance. See the ReadME at the end of this article for more details.

***1.2.1 Query preprocessing service***  
Deep learning models tend to be based on tensors, but NLP/CV models often have a preprocessing part that translates raw text and images into tensors that deep learning models can accept. For example, NLP class models often have a pre-tokenizer to transform text data of string type into discrete tensor data. CV class models also have similar processing logic to complete the cropping, scaling, transformation, and other processing of input images through preprocessing. On the one hand, considering that this part of preprocessing logic is decoupled from tensor reasoning of the depth model, on the other hand, the reason of the depth model has an independent technical system based on ONNX, so MetaSpore disassembled this part of preprocessing logic.

NLP pretreatment Tokenizer has been integrated into the Query pretreatment service. MetaSpore dismantlement with a relatively general convention. Users only need to provide preprocessing logic files to realize the loading and prediction interface and export the necessary data and configuration files loaded into the preprocessing service. Subsequent CV preprocessing logic will also be integrated in this manner.

The preprocessing service currently provides the gRPC interface invocation externally and is dependent on the Query preprocessing (QP) module in the retrieval algorithm service. After the user request reaches the retrieval algorithm service, it will be forwarded to the service to complete the data preprocessing and continue the subsequent processing. The ReadMe provides details on how the preprocessing service is started, how the preprocessing model exported offline to cloud storage enters the service, and how to debug the service.

To further improve the efficiency and stability of model reasoning, MetaSpore Serving implements a Python preprocessing submodule. So MetaSpore can provide gRPC services through user-specified preprocessor.py, complete Tokenizer or CV-related preprocessing in NLP, and translate requests into a Tensor that deep models can handle. Finally, the model inference is carried out by MetaSpore, Serving subsequent sub-modules.

Presented here on the lot code: [https://github.com/meta-soul/MetaSpore/compare/add\_python\_preprocessor](https://github.com/meta-soul/MetaSpore/compare/add_python_preprocessor)

***1.2.2 Retrieval algorithm services***  
Retrieval algorithm service is the core of the whole online service system, which is responsible for the triage of experiments, the assembly of algorithm chains such as preprocessing, recall, sorting, and the invocation of dependent component services. The whole retrieval algorithm service is developed based on the Java Spring framework and supports multi-mode retrieval scenarios of text search and text search graph. Due to good internal abstraction and modular design, it has high flexibility and can be migrated to similar application scenarios at a low cost.  
Hereâ€™s a quick guide to configuring the environment to set up the retrieval algorithm service. See ReadME for more details:

1. Install dependent components. Use Maven to install the online-Serving component
2. Search for service configurations. Copy the template configuration file and replace the MongoDB, Milvus, and other configurations based on the development/production environment.
3. Install and configure Consul. Consul allows you to synchronize the search service configuration in real-time, including cutting the flow of experiments, recall parameters, and sorting parameters. The projectâ€™s configuration file shows the current configuration parameters of text search and text search. The parameter modelName in the stage of pretreatment and recall is the corresponding model exported in offline processing.
4. Start the service. Once the above configuration is complete, the retrieval service can be started from the entry script. Once the service is started, you can test it! For example, for a user with userId=10 who wants to query â€œHow to renew ID card,â€ access the text search service.

***1.2.3 User Entry Service***  
Considering that the retrieval algorithm service is in the form of the API interface, it is difficult to locate and trace the problem, especially for the text search image scene can intuitively display the retrieval results to facilitate the iterative optimization of the retrieval algorithm. This paper provides a lightweight Web UI interface for text search and image search, a search input box, and results in a display page for users. Developed by Flask, the service can be easily integrated with other retrieval applications. The service calls the retrieval algorithm service and displays the returned results on the page.

Itâ€™s also easy to install and start the service. Once youâ€™re done, go to [http://127.0.0.1:8090](http://127.0.0.1:8090/) to see if the search UI service is working correctly. See the ReadME at the end of this article for details.

**2. Multimodal system demonstration**  
The multimodal retrieval service can be started when offline processing and online service environment configuration have been completed following the above instructions. Examples of textual searches are shown below.  
Enter the entry of the text search map application, enter â€œcatâ€ first, and you can see that the first three digits of the returned result are cats:

https://preview.redd.it/0n5nuyvhez291.png?width=1280&format=png&auto=webp&s=1e9c054f541d53381674b8d6001b4bf524506bd2

 If you add a color constraint to â€œcatâ€ to retrieve â€œblack cat,â€ you can see that it does return a black cat: 

https://preview.redd.it/rzc0qjyjez291.png?width=1280&format=png&auto=webp&s=d5bcc503ef0fb3360c7740e60e295cf372dcad47

 Further, strengthen the constraint on the search term, change it to â€œblack cat on the bed,â€ and return results containing pictures of a black cat climbing on the bed: 

&#x200B;

https://preview.redd.it/c4b2q8olez291.png?width=1280&format=png&auto=webp&s=4f3817b0b9f07e1e68d1d4a8281702ba3834a00a

The cat can still be found through the text search system after the color and scene modification in the above example.

**Conclusion**  
The cutting-edge pre-training technology can bridge the semantic gap between different modes, and the HuggingFace community can greatly reduce the cost for developers to use the pre-training model. Combined with the technological ecology of MetaSpore online reasoning and online microservices provided by DMetaSpore, the pre-training model is no longer mere offline dabbling. Instead, it can truly achieve end-to-end implementation from cutting-edge technology to industrial scenarios, fully releasing the dividends of the pre-training large model. In the future, [DMetaSoul](https://github.com/meta-soul) will continue to improve and optimize the MetaSpore technology ecosystem:

1. More automated and wider access to HuggingFace community ecology. MetaSpore will soon release a common model rollout mechanism to make HuggingFace ecologically accessible and will later integrate preprocessing services into online services.
2. Multi-mode retrieval offline algorithm optimization. For multimodal retrieval scenarios, MetaSpore will continuously iteratively optimize offline algorithm components, including text recall/sort model, graphic recall/sort model, etc., to improve the accuracy and efficiency of the retrieval algorithm. For related code and reference documentation in this article, please visit: [https://github.com/meta-soul/MetaSpore/tree/main/demo/multimodal/online](http://github.com/meta-soul/MetaSpore/blob/main/demo/multimodal/online/README-CN.md) Some images source: [https://github.com/openai/CLIP/raw/main/CLIP.png](https://github.com/openai/CLIP/raw/main/CLIP.png) [https://www.sbert.net/examples/training/sts/README.html](https://www.sbert.net/examples/training/sts/README.html)",0,0,2022-06-01 15:44:00, p mmml   deploy huggingface training model rapidly based on metaspore, a few days ago  huggingface announced a   million series c funding round  which was big news in open source machine learning and could be a sign of where the industry is headed  two days before the huggingface funding announcement  open source machine learning platform    metaspore    https as deep learning technology makes innovative breakthroughs in computer vision  natural language processing  speech understanding  and other fields  more and more unstructured data are perceived  understood  and processed by machines  these advances are mainly due to the powerful learning ability of deep learning  through pre training of deep models on massive data  the models can capture the internal data patterns  thus helping many downstream tasks  with the industry and academia investing more and more energy in the research of pre training technology  the distribution warehouses of pre training models such as huggingface and timm have emerged one after another  the open source community release pre training significant model dividends at an unprecedented speed in recent years  the data form of machine modeling and understanding has gradually evolved from single mode to multi mode  and the semantic gap between different modes is being eliminated  making it possible to retrieve data across modes  take clip  openai s open source work  as an example  to pre train the twin towers of images and texts on a dataset of  million pictures and texts and connect the semantics between pictures and texts  many researchers in the academic world have been solving multimodal problems such as image generation and retrieval based on this technology  although the frontier technology through the semantic gap between modal data  there is still a heavy and complicated model tuning  offline data processing  high performance online reasoning architecture design  heterogeneous computing  and online algorithm be born multiple processes and challenges  hindering the frontier multimodal retrieval technologies fall to the ground and pratt  whitney    dmetasoul    https     multimodal semantic retrieval  the sample architecture of multimodal retrieval is as follows   our multimodal retrieval system supports both text search and text search application scenarios  including offline processing  model reasoning  online services  and other core modules   xb https    offline processing  including offline data processing processes for different application scenarios of text search and text search  including model tuning  model export  data index database construction  data push  etc   model inference  after the offline model training  we deployed our nlp and cv large models based on the metaspore serving framework  metaspore serving helps us conveniently perform online inference  elastic scheduling  load balancing  and resource scheduling in heterogeneous environments   online services  based on metaspore s online algorithm application framework  metaspore has a complete set of reusable online search services  including front end retrieval ui  multimodal data preprocessing  vector recall and sorting algorithm  ab experimental framework  etc  metaspore also supports text search by text and image scene search by text and can be migrated to other application scenarios at a low cost the huggingface open source community has provided several excellent baseline models for similar multimodal retrieval problems  which are often the starting point for actual optimization in the industry  metaspore also uses the pre training model of the huggingface community in its online services of searching words by words and images by words  searching words by words is based on the semantic similarity model of the question and answer field optimized by metaspore  and searching images by words is based on the community pre training model these community open source pre training models are exported to the general onnx format and loaded into metaspore serving for online reasoning  the following sections will provide a detailed description of the model export and online retrieval algorithm services  the reasoning part of the model is standardized saas services with low coupling with the business  interested readers can refer to my previous post  the design concept of metaspore  a new generation of the one stop machine learning platform     offline processing    offline processing mainly involves the export and loading of online models and index building and pushing of the document library  you can follow the step by step instructions below to complete the offline processing of text search and image search and see how the offline pre training model achieves reasoning at metaspore       search text by text     traditional text retrieval systems are based on literal matching algorithms such as bm  due to users  diverse query words  a semantic gap between query words and documents is often encountered  for example  users misspell  iphone  as  phone   and search terms are incredibly long  such as       months old baby autumn small size bag pants   traditional text retrieval systems will use spelling correction  synonym expansion  search terms rewriting  and other means to alleviate the semantic gap but fundamentally fail to solve this problem  only when the retrieval system fully understands users  query terms and documents can it meet users  retrieval demands at the semantic level  with the continuous progress of pre training and representational learning technology  some commercial search engines continue to integrate semantic vector retrieval methods based on symbolic learning into the retrieval ecology    semantic retrieval model     this paper introduces a set of semantic vector retrieval applications  metaspore built a set of semantic retrieval systems based on encyclopedia question and answer data  metaspore adopted the sentence bert model as the semantic vector representation model  which fine tunes the twin tower bert in supervised or unsupervised ways to make the model more suitable for retrieval tasks  the model structure is as follows the query doc symmetric two tower model is used in text search and question and answer retrieval  the vector representation of online query and offline doc share the same vector representation model  so it is necessary to ensure the consistency of the offline doc library building model and online query inference model  the case uses metaspore s text representation model sbert chinese qmc domain v  optimized in the open source semantically similar data set  this model will express the question and answer data as a vector in offline database construction  the user query will be expressed as a vector by this model in online retrieval  ensuring that query doc in the same semantic space  users  semantic retrieval demands can be guaranteed by vector similarity metric calculation since the text presentation model does vector encoding for query online  we need to export the model for use by the online service  go to the q a data library code directory and export the model concerning the documentation  in the script  pytorch tracing is used to export the model  the models are exported to the    export  directory  the exported models are mainly onnx models used for wired reasoning  tokenizer  and related configuration files  the exported models are loaded into metaspore serving by the online serving system described below for model reasoning  since the exported model will be copied to the cloud storage  you need to configure related variables in env sh    _build library based on text search  _    the retrieval database is built on the million level encyclopedia question and answer data set  according to the description document  you need to download the data and complete the database construction  the question and answer data will be coded as a vector by the offline model  and then the database construction data will be pushed to the service component  the whole process of database construction is described as follows   preprocessing  converting the original data into a more general jsonline format for database construction   build index  use the same model as online  sbert chinese qmc domain v  to index documents  one document object per line    push inverted  vector  and forward  document field  data to each component server the following is an example of the database data format  after offline database construction is completed  various data are pushed to corresponding service components  such as milvus storing vector representation of documents and mongodb storing summary information of documents  online retrieval algorithm services will use these service components to obtain relevant data       search by text     text and images are easy for humans to relate semantically but difficult for machines  first of all  from the perspective of data form  the text is the discrete id type of one dimensional data based on words and words  at the same time  images are continuous two dimensional or three dimensional data  secondly  the text is a subjective creation of human beings  and its expressive ability is vibrant  including various turning points  metaphors  and other expressions  while images are machine representations of the objective world  in short  bridging the semantic gap between text and image data is much more complex than searching text by text  the traditional text search image retrieval technology generally relies on the external text description data of the image or the nearest neighbor retrieval technology and carries out the retrieval through the image associated text  which in essence degrades the problem to text search  however  it will also face many issues  such as obtaining the associated text of pictures and whether the accuracy of text search by text is high enough  the depth model has gradually evolved from single mode to multi mode in recent years  taking the open source project of openai  clip  as an example  train the model through the massive image and text data of the internet and map the text and image data into the same semantic space  making it possible to implement the text and image search technology based on semantic vector    clip graphic model     the text search pictures introduced in this paper are implemented based on semantic vector retrieval  and the clip pre training model is used as the two tower retrieval architecture  because the clip model has trained the semantic alignment of the twin towers  text and image side models on the massive graphic and text data  it is particularly suitable for the text search graph scene due to the different image and text data forms  the query doc asymmetric twin towers model is used for text search image retrieval  the image side model of the twin towers is used for offline database construction  and the text side model is used for the online return  in the final online retrieval  the database data of the image side model will be searched after the text side model encodes query  and the clip pre training model guarantees the semantic correlation between images and texts  the model can draw the graphic pairs closer in vector space by pre training on a large amount of visual data   here we need to export the text side model for online metaspore serving inference  since the retrieval scene is based on chinese  the clip model supporting chinese understanding is selected  the exported content includes the onnx model used for online reasoning and tokenizer  similar to the text search  metaspore serving can load model reasoning through the exported content    build library on image search     you need to download the unsplash lite library data and complete the construction according to the instructions  the whole process of database construction is described as follows   preprocessing  specify the image directory  and then generate a more general jsonline file for library construction   build index  use openai clip vit base patch pre training model to index the gallery  and output one document object for each line of index data   push inverted  vector  and forward  document field  data to each component server  like text search  after offline database construction  relevant data will be pushed to service components  called by online retrieval algorithm services to obtain relevant data     online services    the overall online service architecture diagram is as follows https multi mode search online service system supports application scenarios such as text search and text search  the whole online service consists of the following parts   query preprocessing service  encapsulate preprocessing logic  including text image  etc   of pre training model  and provide services through grpc interface   retrieval algorithm service  the whole algorithm processing link includes ab experiment tangent flow configuration  metaspore serving call  vector recall  sorting  document summary  etc    user entry service  provides a web ui interface for users to debug and track down problems in the retrieval service from a user request perspective  these services form invocation dependencies from back to front  so to build up a multimodal sample  you need to run each service from front to back first  before doing this  remember to export the offline model  put it online and build the library first  this article will introduce the various parts of the online service system and make the whole service system step by step according to the following guidance  see the readme at the end of this article for more details       query preprocessing service     deep learning models tend to be based on tensors  but nlp cv models often have a preprocessing part that translates raw text and images into tensors that deep learning models can accept  for example  nlp class models often have a pre tokenizer to transform text data of string type into discrete tensor data  cv class models also have similar processing logic to complete the cropping  scaling  transformation  and other processing of input images through preprocessing  on the one hand  considering that this part of preprocessing logic is decoupled from tensor reasoning of the depth model  on the other hand  the reason of the depth model has an independent technical system based on onnx  so metaspore disassembled this part of preprocessing logic nlp pretreatment tokenizer has been integrated into the query pretreatment service  metaspore dismantlement with a relatively general convention  users only need to provide preprocessing logic files to realize the loading and prediction interface and export the necessary data and configuration files loaded into the preprocessing service  subsequent cv preprocessing logic will also be integrated in this manner the preprocessing service currently provides the grpc interface invocation externally and is dependent on the query preprocessing  qp  module in the retrieval algorithm service  after the user request reaches the retrieval algorithm service  it will be forwarded to the service to complete the data preprocessing and continue the subsequent processing  the readme provides details on how the preprocessing service is started  how the preprocessing model exported offline to cloud storage enters the service  and how to debug the service to further improve the efficiency and stability of model reasoning  metaspore serving implements a python preprocessing submodule  so metaspore can provide grpc services through user specified preprocessor py  complete tokenizer or cv related preprocessing in nlp  and translate requests into a tensor that deep models can handle  finally  the model inference is carried out by metaspore  serving subsequent sub modules presented here on the lot code   https       retrieval algorithm services     retrieval algorithm service is the core of the whole online service system  which is responsible for the triage of experiments  the assembly of algorithm chains such as preprocessing  recall  sorting  and the invocation of dependent component services  the whole retrieval algorithm service is developed based on the java spring framework and supports multi mode retrieval scenarios of text search and text search graph  due to good internal abstraction and modular design  it has high flexibility and can be migrated to similar application scenarios at a low cost   here s a quick guide to configuring the environment to set up the retrieval algorithm service  see readme for more details   install dependent components  use maven to install the online serving component  search for service configurations  copy the template configuration file and replace the mongodb  milvus  and other configurations based on the development production environment   install and configure consul  consul allows you to synchronize the search service configuration in real time  including cutting the flow of experiments  recall parameters  and sorting parameters  the project s configuration file shows the current configuration parameters of text search and text search  the parameter modelname in the stage of pretreatment and recall is the corresponding model exported in offline processing   start the service  once the above configuration is complete  the retrieval service can be started from the entry script  once the service is started  you can test it  for example  for a user with userid  who wants to query  how to renew id card   access the text search service       user entry service     considering that the retrieval algorithm service is in the form of the api interface  it is difficult to locate and trace the problem  especially for the text search image scene can intuitively display the retrieval results to facilitate the iterative optimization of the retrieval algorithm  this paper provides a lightweight web ui interface for text search and image search  a search input box  and results in a display page for users  developed by flask  the service can be easily integrated with other retrieval applications  the service calls the retrieval algorithm service and displays the returned results on the page it s also easy to install and start the service  once you re done  go to  http     multimodal system demonstration    the multimodal retrieval service can be started when offline processing and online service environment configuration have been completed following the above instructions  examples of textual searches are shown below   enter the entry of the text search map application  enter  cat  first  and you can see that the first three digits of the returned result are cats https  if you add a color constraint to  cat  to retrieve  black cat   you can see that it does return a black cat  https  further  strengthen the constraint on the search term  change it to  black cat on the bed   and return results containing pictures of a black cat climbing on the bed    xb https the cat can still be found through the text search system after the color and scene modification in the above example   conclusion    the cutting edge pre training technology can bridge the semantic gap between different modes  and the huggingface community can greatly reduce the cost for developers to use the pre training model  combined with the technological ecology of metaspore online reasoning and online microservices provided by dmetaspore  the pre training model is no longer mere offline dabbling  instead  it can truly achieve end to end implementation from cutting edge technology to industrial scenarios  fully releasing the dividends of the pre training large model  in the future   dmetasoul  https   more automated and wider access to huggingface community ecology  metaspore will soon release a common model rollout mechanism to make huggingface ecologically accessible and will later integrate preprocessing services into online services   multi mode retrieval offline algorithm optimization  for multimodal retrieval scenarios  metaspore will continuously iteratively optimize offline algorithm components  including text recall sort model  graphic recall sort model  etc   to improve the accuracy and efficiency of the retrieval algorithm  for related code and reference documentation in this article  please visit   https   github com meta soul metaspore tree main demo multimodal online  http   github com meta soul metaspore blob main demo multimodal online readme cn md  some images source   https   github com openai clip raw main clip png  https   github com openai clip raw main clip png   https   www sbert net examples training sts readme html  https   www sbert net examples training sts readme html 
352,352,Competitive-Rub-1958,v1r6lt,[R] Multi-Game Decision Transformers,"Blog: [https://sites.google.com/view/multi-game-transformers](https://sites.google.com/view/multi-game-transformers)

Paper: [https://arxiv.org/pdf/2205.15241.pdf](https://arxiv.org/pdf/2205.15241.pdf)

&#x200B;

https://preview.redd.it/mxritsjhjt291.png?width=1280&format=png&auto=webp&s=fe0a1a97483a0abdb553c849b46d527691fc658e

Clarifies quite a lot of findings of GATO in a neat way. Scale helps (as always ;)), transfer learning capabilities are evident:-

>... We hence devise our own evaluation setup by pretraining DT, CQL, CPC, BERT, and ACL on the  
full datasets of the 41 training games with 50M steps each, and fine-tuning one model per held-out game using 1% (500k steps) from each game...

It also appears adding more data, whether expert or non-expert still allows DT to gain the edge over Behavioral cloning+expert data.

It also achieves super human level performance across 41 games, so catastrophic forgetting seems less relevant and *perhaps* alleviated by scaling alone...   


I hope the next paper explores MoEs, they've been quite underappreciated lately.",5,41,2022-05-31 19:41:27, r  multi game decision transformers,blog   https paper   https   xb https clarifies quite a lot of findings of gato in a neat way  scale helps  as always      transfer learning capabilities are evident       we hence devise our own evaluation setup by pretraining dt  cql  cpc  bert  and acl on the  full datasets of the  training games with m steps each  and fine tuning one model per held out game using    k steps  from each game   it also appears adding more data  whether expert or non expert still allows dt to gain the edge over behavioral cloning expert data it also achieves super human level performance across  games  so catastrophic forgetting seems less relevant and  perhaps  alleviated by scaling alone      i hope the next paper explores moes  they ve been quite underappreciated lately 
353,353,tfburns,v1fk8f,[R] Detecting danger in gridworlds using Gromov's Link Condition,,3,70,2022-05-31 07:33:30, r  detecting danger in gridworlds using gromov s link condition,nan
354,354,scallion000,v1nh2o,[R] BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis,[https://arxiv.org/abs/2205.14807](https://arxiv.org/abs/2205.14807),7,10,2022-05-31 16:19:43, r  binauralgrad  a two stage conditional diffusion probabilistic model for binaural audio synthesis, https   arxiv org abs    https   arxiv org abs   
355,355,sybar142857,v1vj11,[D] Is the KDD 2022 conference worth the $1100 price tag?,"The early bird cost is around\~$1100. I work in industry so I'm not a researcher and I was looking to attend some conferences to expand my network. Conferences like ICML or NIPS seem way too theoretical and the people involved are on a completely different level compared to what I do day-to-day.

I build xgboost models mostly and occasionally use some standard deep learning methods from blogposts for my work. I was hoping to attend conferences where I can connect with people more on this level instead of actual scientists who use game theory to solve eigenvectors (more power to them but I have little in common with these NASA types).

Is the KDD the right conference for me? I'm open to any other suggestions as well.",4,1,2022-05-31 22:59:53, d  is the kdd  conference worth the   price tag ,the early bird cost is around     i work in industry so i m not a researcher and i was looking to attend some conferences to expand my network  conferences like icml or nips seem way too theoretical and the people involved are on a completely different level compared to what i do day to day i build xgboost models mostly and occasionally use some standard deep learning methods from blogposts for my work  i was hoping to attend conferences where i can connect with people more on this level instead of actual scientists who use game theory to solve eigenvectors  more power to them but i have little in common with these nasa types  is the kdd the right conference for me  i m open to any other suggestions as well 
