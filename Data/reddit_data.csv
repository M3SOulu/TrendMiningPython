,Unnamed: 0,AuthorId,Q_id,Title,Abstract,Answers,Cites,Date,Title_clean,Abstract_clean
0,0,AutoModerator,vqjgxg,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",104,10,2022-07-03 20:30:12, d  simple questions thread,please post your questions here instead of creating a new thread  encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread 
1,1,ML_WAYR_bot,vg5kjd,[D] Machine Learning - WAYR (What Are You Reading) - Week 140,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|131-140|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|[Week 131](https://reddit.com/srsu2n)||||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)|[Week 132](https://reddit.com/t2xpfe)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)|[Week 133](https://reddit.com/tdf2gt)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)|[Week 134](https://reddit.com/tpruqj)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)|[Week 135](https://reddit.com/u0pnhf)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|[Week 126](https://reddit.com/r4e8he)|[Week 136](https://reddit.com/ub2xlz)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|[Week 127](https://reddit.com/rez90o)|[Week 137](https://reddit.com/ul9toj)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)|[Week 128](https://reddit.com/ruja9s)|[Week 138](https://reddit.com/uvl3xc)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)|[Week 129](https://reddit.com/s5lg69)|[Week 139](https://reddit.com/v5nggu)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)|[Week 130](https://reddit.com/sgisxq)||

Most upvoted papers two weeks ago:

/u/tetatetata: [Why Philosophers Should Care About Computational Complexity](https://arxiv.org/abs/1108.1791)

Besides that, there are no rules, have fun.",5,75,2022-06-20 03:19:00, d  machine learning   wayr  what are you reading    week ,this is a place to share machine learning research papers  journals  and articles that you re reading this week  if it relates to what you re researching  by all means elaborate and give us your insight  otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page  not the pdf  you can easily access the pdf from the summary page but not the other way around  or any other pertinent links previous weeks                                                                                                                              week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https   week   https most upvoted papers two weeks ago  u tetatetata   why philosophers should care about computational complexity  https besides that  there are no rules  have fun 
2,2,BB4evaTB12,vye69k,30% of Google's Reddit Emotions Dataset is Mislabeled [D],"Last year, Google released their Reddit Emotions dataset: a collection of 58K Reddit comments human-labeled according to 27 emotions. 

I analyzed the dataset... and found that a 30% is mislabeled!

Some of the errors:

1. **\*aggressively tells friend I love them\*** – mislabeled as **ANGER**
2. **Yay, cold McDonald's. My favorite.** – mislabeled as **LOVE**
3. **Hard to be sad these days when I got this guy with me** – mislabeled as **SADNESS**
4. **Nobody has the money to. What a joke** – mislabeled as **JOY**

&#x200B;

I wrote a blog about it [here](https://www.surgehq.ai/blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled), with more examples and my main two suggestions for how to fix Google's data annotation methodology.",70,420,2022-07-14 02:47:36,  of google s reddit emotions dataset is mislabeled  d ,last year  google released their reddit emotions dataset  a collection of k reddit comments human labeled according to  emotions  i analyzed the dataset    and found that a   is mislabeled some of the errors       aggressively tells friend i love them       mislabeled as   anger      yay  cold mcdonald s  my favorite      mislabeled as   love      hard to be sad these days when i got this guy with me     mislabeled as   sadness      nobody has the money to  what a joke     mislabeled as   joy    xb i wrote a blog about it  here  https   www surgehq ai blog  percent of googles reddit emotions dataset is mislabeled   with more examples and my main two suggestions for how to fix google s data annotation methodology 
3,3,EffectSizeQueen,vyewbj,[N] Andrej Karpathy is leaving Tesla,"Twitter thread:

https://twitter.com/karpathy/status/1547332300186066944",48,112,2022-07-14 03:18:38, n  andrej karpathy is leaving tesla,twitter thread https   twitter com karpathy status 
4,4,fanconic,vxw3s4,"[R] So someone actually peer-reviewed this and thought ""yeah, looks good""?","It looks like chronic kidney disease diagnosis has been solved in this paper: [https://ieeexplore.ieee.org/document/8693581](https://ieeexplore.ieee.org/document/8693581)

I mean no disrespect to the authors, but this publication makes me slightly doubt the peer-review system. Or I am just such an amateur, that I am not seeing the brilliance behind this paper, which is also possible.

Have a read through it yourselves",76,231,2022-07-13 11:39:59, r  so someone actually peer reviewed this and thought yeah  looks good ,it looks like chronic kidney disease diagnosis has been solved in this paper   https i mean no disrespect to the authors  but this publication makes me slightly doubt the peer review system  or i am just such an amateur  that i am not seeing the brilliance behind this paper  which is also possible have a read through it yourselves
5,5,chaoyu,vyfit6,[P] Introducing BentoML 1.0 - A faster way to ship your models to production,"Hi everyone! I'm excited to share some news from the [BentoML](https://github.com/bentoml) team.

When we first open sourced the BentoML project [in 2019](https://www.reddit.com/r/MachineLearning/comments/izqelx/p_bentoml_090_the_easiest_way_to_create_machine/) and [shared it ](https://www.reddit.com/r/MachineLearning/comments/g1cfre/p_bentoml_an_opensource_platform_for/)[with the community](https://www.reddit.com/r/MachineLearning/comments/g1cfre/p_bentoml_an_opensource_platform_for/), our vision was to create an open platform that simplifies machine learning model serving and provides a solid foundation for ML teams to operate ML at production scale. And after years of working together with our community towards that goal, we’re thrilled to announce the general availability of BentoML 1.0!  


What's new in BentoML 1.0?

* Simplify model packaging and management, both locally and a centralized model repository for teams.
* A Python-first architecture that scales with powerful optimizations, including parallel inference, adaptive batching, and support for accelerated runtimes.
* Introducing [Yatai for BentoML](https://github.com/bentoml/Yatai): Production-first ML platform on Kubernetes

&#x200B;

To learn more:

* Introducing BentoML 1.0 Blog post: [https://modelserving.com/blog/introducing-bentoml-10](https://modelserving.com/blog/introducing-bentoml-10)
* BentoML Tutorial: [https://docs.bentoml.org/en/latest/tutorial.html](https://docs.bentoml.org/en/latest/tutorial.html)
* Github Page: [https://github.com/bentoml/BentoML](https://github.com/bentoml/BentoML)
* Documentation: [https://docs.bentoml.org/](https://docs.bentoml.org/)",2,9,2022-07-14 03:45:28, p  introducing bentoml     a faster way to ship your models to production,hi everyone  i m excited to share some news from the  bentoml  https when we first open sourced the bentoml project  in   https what s new in bentoml     simplify model packaging and management  both locally and a centralized model repository for teams   a python first architecture that scales with powerful optimizations  including parallel inference  adaptive batching  and support for accelerated runtimes   introducing  yatai for bentoml  https   xb to learn more   introducing bentoml   blog post   https   bentoml tutorial   https   github page   https   documentation   https   docs bentoml org   https   docs bentoml org  
6,6,tacixat,vye9fa,[D] I made a site for collaborative image labeling,"I recently launched [https://mekabytes.com](https://mekabytes.com/). The idea is to treat datasets like subreddits where users can come together to build the stuff they want to see.

For the datasets there is a github-style landing page with a README to help give guidance on the goals, what images the dataset wants, and any labeling guidelines. There is also a reddit-style comment system where you can reference specific annotations. The idea with that is to provide feedback to help people learn.

The coolest part (IMO) is the versioning system. All annotations are versioned and approved by a moderator, gating data quality kind of like a code review. This versioning allows the dataset to be rolled back to any point in time which will help reproduce research even as the dataset continues to evolve.

The dataset releases will be open under a creative commons license (BY-NC-SA). To help cover hosting the releases are downloadable for $5 + $1/GB. Basically you can use it for research, personal projects, and share freely once you have it.

There is still a ton of stuff to do and I don't even have my first user yet! I've been using it for the last week or so and cleaning up the UX. You can actually annotate decently on mobile.

Right now it supports classification and object detection (bounding boxes). I hope to add a free text field in the near future after some niceties like pagination and comment notifications.

I would love some feedback if you have any!",4,10,2022-07-14 02:51:25, d  i made a site for collaborative image labeling,i recently launched  https for the datasets there is a github style landing page with a readme to help give guidance on the goals  what images the dataset wants  and any labeling guidelines  there is also a reddit style comment system where you can reference specific annotations  the idea with that is to provide feedback to help people learn the coolest part  imo  is the versioning system  all annotations are versioned and approved by a moderator  gating data quality kind of like a code review  this versioning allows the dataset to be rolled back to any point in time which will help reproduce research even as the dataset continues to evolve the dataset releases will be open under a creative commons license  by nc sa   to help cover hosting the releases are downloadable for       gb  basically you can use it for research  personal projects  and share freely once you have it there is still a ton of stuff to do and i don t even have my first user yet  i ve been using it for the last week or so and cleaning up the ux  you can actually annotate decently on mobile right now it supports classification and object detection  bounding boxes   i hope to add a free text field in the near future after some niceties like pagination and comment notifications i would love some feedback if you have any 
7,7,yuzheyang,vyhn3g,[R] How to learn imbalanced data arising from multiple domains?,"Hello everyone! Happy to share our new work on learning from multi-domain imbalanced data. This work was recently accepted at ECCV 2022.

Data imbalance is ubiquitous and inherent in the real world. Existing methods for dealing with imbalanced data/long-tailed distribution are only for **single domain**, that is, the data originates from the same domain; however, natural data can originate from ***distinct*** **domains**, where a minority class in one domain could have abundant instances from other domains. Effectively utilizing data from different domains is likely to improve the performance of long-tail learning over all domains. This paper promotes the paradigm of the traditional imbalanced classification problem and generalizes it from **single** domain to **multiple domains**.

We formulate the problem of **Multi-Domain Long-Tailed Recognition (MDLT)** as learning from multi-domain imbalanced data, with each domain having its own imbalanced label distribution, and generalizing to a test set that is balanced over ***all domain-class pairs***. MDLT aims to learn from imbalanced data from multiple distinct domains, tackle *label imbalance*, *domain shift*, and *divergent label distributions across domains*, and generalize to the entire set of classes over all domains.

We first propose the ***domain-class transferability graph***, which quantifies the transferability between different domain-class pairs under data imbalance. In this graph, each node refers to a domain-class pair, and each edge refers to the distance between two domain-class pairs in the embedding space. We show that the transferability graph dictates the performance of imbalanced learning across domains. Inspired by this, we design **BoDA**, a loss function that theoretically tracks the upper-bound of transferability statistics to improve the model performance.

Interestingly, we also found that ***addressing in-domain data imbalance improves out-of-domain generalization*** (known as domain generalization, DG). Our analysis showed that data imbalance is an intrinsic problem in DG, but has been overlooked by past works. The intriguing results shed light on **how label imbalance can affect out-of-distribution generalization**, and highlight the importance of **integrating label imbalance for practical DG algorithm design**.

Check out the links below for more details:

* **Paper**: [https://arxiv.org/abs/2203.09513](https://arxiv.org/abs/2203.09513)
* **Code** (+ dataset + models): [https://github.com/YyzHarry/multi-domain-imbalance](https://github.com/YyzHarry/multi-domain-imbalance)
* **Blog post** (check out for in-depth details!): [TowardsDataScience](https://towardsdatascience.com/how-to-learn-imbalanced-data-arising-from-multiple-domains-7d0c0d6e3c17)

&#x200B;

[Multi-Domain Long-Tailed Recognition \(MDLT\) aims to learn from imbalanced data from multiple distinct domains, tackle label imbalance, domain shift, and divergent label distributions across domains, and generalize to the entire set of classes over all domains.](https://i.redd.it/4zefxq759fb91.gif)",1,4,2022-07-14 05:21:26, r  how to learn imbalanced data arising from multiple domains ,hello everyone  happy to share our new work on learning from multi domain imbalanced data  this work was recently accepted at eccv  data imbalance is ubiquitous and inherent in the real world  existing methods for dealing with imbalanced data long tailed distribution are only for   single domain    that is  the data originates from the same domain  however  natural data can originate from    distinct      domains    where a minority class in one domain could have abundant instances from other domains  effectively utilizing data from different domains is likely to improve the performance of long tail learning over all domains  this paper promotes the paradigm of the traditional imbalanced classification problem and generalizes it from   single   domain to   multiple domains   we formulate the problem of   multi domain long tailed recognition  mdlt    as learning from multi domain imbalanced data  with each domain having its own imbalanced label distribution  and generalizing to a test set that is balanced over    all domain class pairs     mdlt aims to learn from imbalanced data from multiple distinct domains  tackle  label imbalance    domain shift   and  divergent label distributions across domains   and generalize to the entire set of classes over all domains we first propose the    domain class transferability graph     which quantifies the transferability between different domain class pairs under data imbalance  in this graph  each node refers to a domain class pair  and each edge refers to the distance between two domain class pairs in the embedding space  we show that the transferability graph dictates the performance of imbalanced learning across domains  inspired by this  we design   boda    a loss function that theoretically tracks the upper bound of transferability statistics to improve the model performance interestingly  we also found that    addressing in domain data imbalance improves out of domain generalization     known as domain generalization  dg   our analysis showed that data imbalance is an intrinsic problem in dg  but has been overlooked by past works  the intriguing results shed light on   how label imbalance can affect out of distribution generalization    and highlight the importance of   integrating label imbalance for practical dg algorithm design   check out the links below for more details     paper     https     code      dataset   models    https     blog post    check out for in depth details     towardsdatascience  https   xb  multi domain long tailed recognition   mdlt   aims to learn from imbalanced data from multiple distinct domains  tackle label imbalance  domain shift  and divergent label distributions across domains  and generalize to the entire set of classes over all domains   https   i redd it zefxqfb gif 
8,8,rivew,vye2zj,[D] How are People Doing “Fair” Few-Shot Training/Evaluation,"After reading through a lot of the non-Meta Learning popular few-shot literature ([Prototypical Nets](https://arxiv.org/abs/1703.05175?context=cs), [Matching Nets](https://arxiv.org/abs/1606.04080?context=stat), etc.) and then looking at other papers/GitHub repos, I’m not totally sure how to build a “fair” training and evaluation setup.

Let’s take CIFAR-100 (ignoring CIFAR-FS for now).  To set up a few-shot dataset split, I’d take the 100 classes and split up into train/val/test 60/20/40 such that each split has non-overlapping classes - pretty straightforward.  But now, I still have 600 examples per class in all splits.  Before generating random 5-way-5-shot episodes during training, what’s the fair way to generate Support and Query Sets?  Are people first creating another split of the trainset so that the Support set only contains 5 examples per class (60*5=300 total examples) and the rest is in the Query set?  If not, something like that then the support set is going to contain a lot of examples to learn from rather than a few.

Some methods also directly classify the trainset’s support images for pre-training, assuming that the number of classes overall is known beforehand.  But then to do same on the validation and support sets I guess that they replace the FC layer.

Finally, when choosing a pre-trained model to start with, it seems absolutely necessary to choose a significantly different domain for evaluation (ex. ImageNet pre-trained ResNet evaluated on CIFAR-FS is bad).

tldr; it seems like there’s a lot of small differences in experimental setups for few-shot settings, what’s the best way to be fair for training/evaluation?  Also maybe I’m just totally missing something :)",1,3,2022-07-14 02:43:38, d  how are people doing  fair  few shot training evaluation,after reading through a lot of the non meta learning popular few shot literature   prototypical nets  https let s take cifar   ignoring cifar fs for now    to set up a few shot dataset split  i d take the  classes and split up into train val test some methods also directly classify the trainset s support images for pre training  assuming that the number of classes overall is known beforehand   but then to do same on the validation and support sets i guess that they replace the fc layer finally  when choosing a pre trained model to start with  it seems absolutely necessary to choose a significantly different domain for evaluation  ex  imagenet pre trained resnet evaluated on cifar fs is bad  tldr  it seems like there s a lot of small differences in experimental setups for few shot settings  what s the best way to be fair for training evaluation   also maybe i m just totally missing something   
9,9,MonLiH,vxo5nb,[N] BigScience Releases their 176 Billion Parameter Open-access Multilingual Language Model,"[BigScience](https://bigscience.huggingface.co/) recently released their new open-access (with weights) massive 176B language model that looks incredibly promising.The size is comparable to OpenAI's largest GPT-3 model. More info about the model can be found on [BigScience's blog](https://bigscience.huggingface.co/blog/bloom).

You can play with the model interactively, for free(!) on [Huggingface](https://huggingface.co/bigscience/bloom).",32,174,2022-07-13 04:30:03, n  bigscience releases their  billion parameter open access multilingual language model, bigscience  https you can play with the model interactively  for free    on  huggingface  https   huggingface co bigscience bloom  
10,10,Petuum,vy92d1,[P] Build a Machine Translation System with Forte,"**TLDR:** This tutorial allows you to build a machine translation system with no glue code using Forte, an open source ML workflow builder.

&#x200B;

Forte makes it easy to compose any NLP pipeline, regardless of heterogeneity of data and processes, as a modular and easily editable system. It allows users to break down complex problems into composable pipelines and enables inter-operations across tasks through a unified data format.

This tutorial includes:

**1 — How to read data from source**

* How to create a simple NLP pipeline
* How to maintain and store the input data

**2 — How to process data in pipeline**

* How to perform sentence segmentation
* How to annotate and query the data
* How to translate the input text with a pre-trained model
* How to manage multiple data objects

**3 — How to handle new practical requests**

* How to handle structures like HTML data
* How to select a single data object for processing
* How to replace the translation model with remote translation services
* How to save and load the pipeline

Run the following command to install all the required dependencies for this tutorial:

    # It is recommended to install these in command line
    !pip install forte==0.2.0 forte.nltk requests# for certain environment, you may run into troubles installing transformers, such as requiring Rust
    some workaround here: https://github.com/huggingface/transformers/issues/2831#issuecomment-600141935 might help!pip install transformers==4.16.2
    you may want to try different pytorch version depending on your platform
    if you cannot install pytorch, try locate your problem at https://github.com/pytorch/pytorch/issues!pip install torch==1.11.0
    for certain environment, the installation may fail
    some workaround here: https://github.com/google/sentencepiece/issues/378#issuecomment-1145399969 might help!pip install sentencepiece

**1 — How to Read Data from Source**

**How to Create a Simple Pipeline: Start with the Reader**

In this section, you will learn:

* What is a reader and why we need it
* How to compose a simple pipeline with a pre-built reader

&#x200B;

    from forte import Pipeline

from forte.data.readers import TerminalReader pipeline: Pipeline = Pipeline()

All pipelines need a reader to read and parse input data. To make our pipeline read queries from the user’s command-line terminal, use the `TerminalReader` class provided by Forte.  `TerminalReader` transforms the user’s query into a DataPack object, which is a unified data format for NLP that makes it easy to connect different NLP tools together as Forte Processors.

    pipeline.set_reader(TerminalReader())

To run the pipeline consisting of the single `TerminalReader`, call `process_dataset` which will return an iterator of DataPack objects. The second line in the following code snippet retrieves the first user query from the TerminalReader.

    pipeline.initialize()
    datapack = next(pipeline.process_dataset())
    print(datapack.text)

**How to Maintain and Store Input Data: DataPack**

In this section, you will learn:

* What is a DataPack object and why we need it

Forte helps demystify data lineage and increase the traceability of how data flows along the pipeline and how features are generated to interface data to model. Similar to a cargo ship that loads and transports goods from one port to another, a data pack carries information when passing each module and updates the ontology states along the way.

https://preview.redd.it/a1ueowwsadb91.png?width=1400&format=png&auto=webp&s=44bf2986ef09f609986bcb5cecba12be1de7db06

DataPack and Multi-Modality:

DataPack not only supports text data but also audio and image data.

https://preview.redd.it/jko8w1ryadb91.png?width=1067&format=png&auto=webp&s=0963fa45d6630e1f1a2061da626034dd4d55b99b

**2— How to Process Data in Pipeline**

**How to Perform Sentence Segmentation: Add a Pre-Built Forte Processor to the Pipeline**

In this section, you will learn:

* What is a processor and why we need it
* How to add a pre-built processor to the pipeline

A Forte Processor takes DataPacks as inputs, processes them, and stores its outputs in DataPacks. The processors we are going to use in this section are all PackProcessors, which expect exactly one DataPack as input and store its outputs back into the same DataPack. The following two lines of code shows how a pre-built processor `NLTKSentenceSegmenter` is added to our pipeline.

    from fortex.nltk.nltk_processors import NLTKSentenceSegmenter
    pipeline.add(NLTKSentenceSegmenter())

When we run the pipeline, the `NLTKSentenceSegmenter` processor will split the user query into sentences and store them back to the DataPack created by TerminalReader. The code snippet below shows how to get all the sentences from the first query.

https://preview.redd.it/2cp4g1t5bdb91.png?width=1088&format=png&auto=webp&s=9c5851e77dffd3693a9fe5534e80319c7d489d90

    from ft.onto.base_ontology import Sentence
    pipeline.initialize()
    for sent in next(pipeline.process_dataset()).get(Sentence):
        print(sent.text)

**How to Annotate and Query the Data: Ontology**

In this section, you will learn:

* What is the ontology system and why we need it
* How to write a customized ontology and how to use it

`Sentence` is a pre-defined ontology provided by Forte and it is used by `NLTKSentenceSegmenter`to annotate each sentence in text. Forte is built on top of an Ontology system, which defines the relations between NLP annotations, for example, the relation between words and documents, or between two words. This is the core for Forte. The ontology can be specified via a JSON format. And tools are provided to convert the ontology into production code (Python).

https://preview.redd.it/ae6njzbabdb91.png?width=637&format=png&auto=webp&s=afd570ef358d209aca2b5fca3d395d8ba812daf8

We can also define customized ontologies:

    from dataclasses import dataclass
    from forte.data.ontology.top import Annotation
    from typing import Optional
    
    @dataclass
    class Article(Annotation):
    
        language: Optional[str]
    
        def __init__(self, pack, begin: int, end: int):
            super().__init__(pack, begin, end)

Below is a simple example showing how we can query sentences through the new ontology we just created:

    from forte.data import DataPack
    
    sentences = [
        ""Do you want to get better at making delicious BBQ?"",
        ""You will have the opportunity, put this on your calendar now."",
        ""Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers.""
    ]
    datapack: DataPack = DataPack()
    
    # Add sentences to the DataPack and annotate them
    for sentence in sentences:
        datapack.set_text(datapack.text + sentence)
        datapack.add_entry(
            Sentence(datapack, len(datapack.text) - len(sentence), len(datapack.text))
        )
        
    # Annotate the whole text with Article
    article: Article = Article(datapack, 0, len(datapack.text))
    article.language = ""en""
    datapack.add_entry(article)
    
    for article in datapack.get(Article):
        print(f""Article (language - {article.language}):"")
        for sentence in article.get(Sentence):
            print(sentence.text)

In our previous example, we have the following ontologies inheritance. Sentence and Article both inherit from Annotation which is used to represent text data. In Article, we have `language`field to represent the text language.

https://preview.redd.it/sts91nqgbdb91.png?width=901&format=png&auto=webp&s=426a1b8b2908ffabc57b0a888d6cff0ef92d5743

Actually, we not only support text ontology but also audio, image and link which represent relationships between two entries.

https://preview.redd.it/spp9etkibdb91.png?width=1074&format=png&auto=webp&s=cd8f8c4c7965fabf3fbac3ed5e4ba83a5259a848

`Annotation` is inherited by all text entries which usually has a span to retrieve partial text from the full text.

* `Article`, as shown in our previous example, inherits annotation and contains `language`field to differentiate English and German. In the single DataPack example, English article has a span of English text in the DataPack. Likewise, German article has a span of German text in the DataPack.
* `Sentence` in our example is used to break down article, and we pass sentences into MT pipeline.

`AudioAnnotation` is inherited by all audio entries which usually has an audio span to retrieve partial audio from the full audio.

* `Recording` is an example subclass of `AudioAnnotation`, and it has extra `recording_class`field denoting the classes the audio belongs to.

`ImageAnnotation` is inherited by all image entries which usually has payload index pointing to a loaded image array.

* `BoundingBox` is an example subclass of `ImageAnnotation`. As the picture shows, it has more inheritance relationships than other ontology classes due to the nature of CV objects. The advantage of Forte ontology is that it supports complex inheritance, and users can inherit from existing ontology and add new ontology features for their needs.

`Link` is inherited by all link-like entries which has parent and child.

* `RelationLink` is an example subclass of `Link`, and it has a class attribute specifying the relation type.

**How to Translate the Input Text with a Pre-Trained Model: Create a Machine Translation Processor**

In this section, you will learn:

* The basics of machine translation process
* How to wrap a pre-trained machine translation model into a Forte processor

Translation converts a sequence of text from one language to another. In this tutorial we will use `Huggingface` Transformer model to translate input data, which consists of several steps including subword tokenization, input embedding, model inference, decoding, etc.

https://preview.redd.it/2li5gn74cdb91.png?width=1400&format=png&auto=webp&s=c6b1a5f2770b62552bf7895d666bbc1168106262

In Forte, we have a generic class `PackProcessor` that wraps model and inference-related components and behaviors to process `DataPack`. Therefore, we need to create a class that inherits the generic method from `PackProcessor`. Then we have a class definition `class MachineTranslationProcessor(PackProcessor)`.

    from forte.data import DataPack
    from forte.data.readers import StringReader
    from forte.processors.base import PackProcessor
    from transformers import T5Tokenizer, T5ForConditionalGeneration
    class MachineTranslationProcessor(PackProcessor):
        """"""
        Translate the input text and output to a file.
        """"""
        def initialize(self, resources, configs):
            super().initialize(resources, configs)
            # Initialize the tokenizer and model
            model_name: str = self.configs.pretrained_model
            self.tokenizer = T5Tokenizer.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name)
            self.task_prefix = ""translate English to German: ""
            self.tokenizer.padding_side = ""left""
            self.tokenizer.pad_token = self.tokenizer.eos_token
        def _process(self, input_pack: DataPack):
            # en2de machine translation 
            inputs = self.tokenizer([
                self.task_prefix + sentence.text
                for sentence in input_pack.get(Sentence)
            ], return_tensors=""pt"", padding=True)
            output_sequences = self.model.generate(
                input_ids=inputs[""input_ids""],
                attention_mask=inputs[""attention_mask""],
                do_sample=False,
            )
            output = ''.join(self.tokenizer.batch_decode(
                output_sequences, skip_special_tokens=True
            ))
            src_article: Article = Article(input_pack, 0, len(input_pack.text))
            src_article.language = ""en""
            input_pack.set_text(input_pack.text + '\n\n' + output)
            tgt_article: Article = Article(input_pack, len(input_pack.text) - len(output), len(input_pack.text))
            tgt_article.language = ""de""
        @classmethod
        def default_configs(cls):
            return {
                ""pretrained_model"": ""t5-small""
            }

Initialization of needed components:

* Users need to consider initializing all needed NLP components for the inference task such as tokenizer and model.
* Users also need to specify all configuration in `configs`, a dictionary-like object that specifies configurations of all components such as model name.

MT operations on DataPack:

* After the initialization, we already have the needed NLP components. We need to consider several MT behaviors based on Forte DataPack.

Pre-process text data:

* Retrieve text data from DataPack (given that it already reads data from the data source).
* Since T5 has a better performance given a task prompt, we also want to include the prompt in our data.
* Tokenization that transforms input text into sequences of tokens and token ids.
* Generate output sequences from model.
* Decode output token ids into sentences using the tokenizer.

The generic method to process `DataPack` is `_process(self, input_pack: DataPack)`. It should tokenize the input text, use the model class to make an inference, decode the output token ids, and finally write the output to a target file.

Now we can add it into the pipeline and run the machine translation task.

    input_string: str = ' '.join(sentences)
    pipeline: Pipeline = Pipeline[DataPack]()
    pipeline.set_reader(StringReader())
    pipeline.add(NLTKSentenceSegmenter())
    pipeline.add(MachineTranslationProcessor())
    pipeline.initialize()
    for datapack in pipeline.process_dataset([input_string]):
        for article in datapack.get(Article):
            print([f""\nArticle (language - {article.language}): {article.text}""])

Ontology in DataPack:

Here we provide an illustration so that users can better understand the internal storage of DataPack. As we can see, text data, sentence and articles, are stored as span in `Annotations`. Their text data can be easily and efficiently retrieved by their spans.

https://preview.redd.it/hpbpp8xpcdb91.png?width=1096&format=png&auto=webp&s=ae11d14b87fa4fb54dd9988213218e956e67250e

**How to Manage Multiple Data Objects: MultiPack, A Better Way to Store Source and Target Text**

In this section, you will learn:

* What is a MultiPack and why we need it
* How to use a MultiPack

The above step outputs a DataPack which is good for holding data about one specific piece of text. A complicated pipeline like the one we are building now may need multiple DataPacks to be passed along the pipeline and this is where MultiPack can help. MultiPack manages a set of DataPacks that can be indexed by their names.

`MultiPackBoxer` is a simple Forte processor that converts a DataPack into a MultiPack by making it the only DataPack in there. A name can be specified via the config. We use it to wrap DataPack that contains source sentence.

https://preview.redd.it/jfoc0r9tcdb91.png?width=812&format=png&auto=webp&s=d0f7bd950e3f338ce0703419a57acb18d51463d8

    from forte.data import MultiPack
    from forte.processors.base import MultiPackProcessor
    from forte.data.caster import MultiPackBoxer
    class MachineTranslationMPProcessor(MultiPackProcessor):
        """"""
        Translate the input text and output to a file.
        """"""
        def initialize(self, resources, configs):
            super().initialize(resources, configs)
    
            # Initialize the tokenizer and model
            model_name: str = self.configs.pretrained_model
            self.tokenizer = T5Tokenizer.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name)
            self.task_prefix = ""translate English to German: ""
            self.tokenizer.padding_side = ""left""
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
        def _process(self, input_pack: MultiPack):
            source_pack: DataPack = input_pack.get_pack(""source"")
            target_pack: DataPack = input_pack.add_pack(""target"")
    
            # en2de machine translation 
            inputs = self.tokenizer([
                self.task_prefix + sentence.text
                for sentence in source_pack.get(Sentence)
            ], return_tensors=""pt"", padding=True)
    
            output_sequences = self.model.generate(
                input_ids=inputs[""input_ids""],
                attention_mask=inputs[""attention_mask""],
                do_sample=False,
            )
            
            # Annotate the source article
            src_article: Article = Article(source_pack, 0, len(source_pack.text))
            src_article.language = ""en""
            
            # Annotate each sentence
            for output in self.tokenizer.batch_decode(
                output_sequences, skip_special_tokens=True
            ):
                target_pack.set_text(target_pack.text + output)
                text_length: int = len(target_pack.text)
                Sentence(target_pack, text_length - len(output), text_length)
            
            # Annotate the target article
            tgt_article: Article = Article(target_pack, 0, len(target_pack.text))
            tgt_article.language = ""de""
    
        @classmethod
        def default_configs(cls):
            return {
                ""pretrained_model"": ""t5-small"",
            }

Then `MachineTranslationMPProcessor` writes the output sentence into a target DataPack.

https://preview.redd.it/jci0neb3ddb91.png?width=978&format=png&auto=webp&s=fd26a16c151fcc1bfe578a0b4cc3dd69f98963c3

Now let’s try to create a new pipeline that utilizes `MultiPack` to manage text in different languages.

    nlp: Pipeline = Pipeline[DataPack]()
    nlp.set_reader(StringReader())
    nlp.add(NLTKSentenceSegmenter())
    nlp.add(MultiPackBoxer(), config={""pack_name"": ""source""})
    nlp.add(MachineTranslationMPProcessor(), config={
        ""pretrained_model"": ""t5-small""
    })
    nlp.initialize()
    for multipack in nlp.process_dataset([input_string]):
        for pack_name in (""source"", ""target""):
            for article in multipack.get_pack(pack_name).get(Article):
                print(f""\nArticle (language - {article.language}): "")
                for sentence in article.get(Sentence):
                    print(sentence.text)

Ontology in MultiPack:

For comparison, here is an illustration of the internal storage of MultiPack. We can see that MultiPack wraps one source DataPack and one target DataPack. Article spans are based on two separate DataPack text.

https://preview.redd.it/asqd43z8ddb91.png?width=1400&format=png&auto=webp&s=6587dd45173b5ed5f8379819179270c500f9ed9d

**3 — How to Handle New Practical Requests**

**How to Handle Structures like HTML Data**

In this section, you will learn

* How to build a translation management system
* How to preserve the structure like HTML in machine translation
* How to select a specific DataPack from MultiPack for processing

In the previous step, the input string is just a simple paragraph made up of several sentences. However, in many cases, we might need to handle data with structural information, such HTML or XML. When the input is a string of raw HTML data, the machine translation pipeline above may not work as expected:

    html_input: str = """"""
    <!DOCTYPE html>
    <html>
        <head><title>Beginners BBQ Class.</title></head>
        <body>
        <p>Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers.</p>
        </body>
    </html>
    """"""
    nlp.initialize()
    for multipack in nlp.process_dataset([html_input]):
        print(""Source Text: "" + multipack.get_pack(""source"").text)
        print(""\nTarget Text: "" + multipack.get_pack(""target"").text)

We can see that the original HTML structure is broken in the translated output.

In order to handle structured data like HTML, we will need to update our current design of pipeline. Luckily, Forte pipelines are highly modular, we can simply insert two new processors without updating the previous pipeline.

We first need a HTML cleaner to parse all the HTML tags from input string. Picture below shows the effect of tag remover.

https://preview.redd.it/ed6civbfddb91.png?width=1076&format=png&auto=webp&s=807ffb7f8ac1e9f13f071a50f867065e3038dd85

After the translation is finished, we will also need to recover the HTML structure from the unstructured translation output. Picture below shows replace one source sentence with one target sentence given the target sentence is ready.

https://preview.redd.it/wrs7aaxgddb91.png?width=968&format=png&auto=webp&s=34b31beba1791543b826b4aa6efcbae114f39bbe

    from forte.data import NameMatchSelector
    from forte.data.readers.html_reader import ForteHTMLParser
    class HTMLTagCleaner(MultiPackProcessor):
        
        def initialize(self, resources, configs):
            super().initialize(resources, configs)
            self._parser = ForteHTMLParser()
        def _process(self, input_pack: MultiPack):
            raw_pack: DataPack = input_pack.get_pack(""raw"")
            source_pack: DataPack = input_pack.add_pack(""source"")
            
            self._parser.feed(raw_pack.text)
            cleaned_text: str = raw_pack.text
            for span, _ in self._parser.spans:
                cleaned_text = cleaned_text.replace(
                    raw_pack.text[span.begin:span.end], ''
                )
            source_pack.set_text(cleaned_text)
            
    class HTMLTagRecovery(MultiPackProcessor):
        def _process(self, input_pack: MultiPack):
            raw_pack: DataPack = input_pack.get_pack(""raw"")
            source_pack: DataPack = input_pack.get_pack(""source"")
            target_pack: DataPack = input_pack.get_pack(""target"")
            result_pack: DataPack = input_pack.add_pack(""result"")
            result_text: str = raw_pack.text
            for sent_src, sent_tgt in zip(source_pack.get(Sentence), target_pack.get(Sentence)):
                result_text = result_text.replace(sent_src.text, sent_tgt.text)
            result_pack.set_text(result_text)

Now we are able to create a translation management system by inserting the two processors introduced above into our previous machine translation pipeline.

    # Pipeline with HTML handling
    pipeline: Pipeline = Pipeline[DataPack]()
    pipeline.set_reader(StringReader())
    pipeline.add(MultiPackBoxer(), config={""pack_name"": ""raw""})
    pipeline.add(HTMLTagCleaner())
    pipeline.add(
        NLTKSentenceSegmenter(),
        selector=NameMatchSelector(),
        selector_config={""select_name"": ""source""}
    )
    pipeline.add(MachineTranslationMPProcessor(), config={
        ""pretrained_model"": ""t5-small""
    })
    pipeline.add(HTMLTagRecovery())
    pipeline.initialize()
    for multipack in pipeline.process_dataset([html_input]):
        print(multipack.get_pack(""raw"").text)
        print(multipack.get_pack(""result"").text)

Selector:

In the code snippet above, we utilize a `NameMatchSelector` to select one specific DataPack from the MultiPack based on its reference name `select_name`. This allows `NLTKSentenceSegmenter` to process only the specified DataPack.

**How to Replace the Translation Model with Remote Translation Services: Replace our MT Model with Online Translation API**

In this section, you will learn:

* How to use a different translation service

Forte also allows us to update the translation model and integrate it seamlessly to the original pipeline. For example, if we want to offload the translation task to an online service, all we need to do is to update the translation processor. There is no need to change other components in the pipeline.

    # You can get your own API key by following the instructions in https://docs.microsoft.com/en-us/azure/cognitive-services/translator/
    api_key = input(""Enter your API key here:"")
    import requests
    import uuid
    class OnlineMachineTranslationMPProcessor(MultiPackProcessor):
        """"""
        Translate the input text and output to a file use online translator api.
        """"""
        def initialize(self, resources, configs):
            super().initialize(resources, configs)
            self.url = configs.endpoint + configs.path
            self.from_lang = configs.from_lang
            self.to_lang = configs.to_lang
            self.subscription_key = configs.subscription_key
            self.subscription_region = configs.subscription_region
        def _process(self, input_pack: MultiPack):
            source_pack: DataPack = input_pack.get_pack(""source"")
            target_pack: DataPack = input_pack.add_pack(""target"")
            
            params = {
                'api-version': '3.0',
                'from': 'en',
                'to': ['de']
            }
            # Build request
            headers = {
                'Ocp-Apim-Subscription-Key': self.subscription_key,
                'Ocp-Apim-Subscription-Region': self.subscription_region,
                'Content-type': 'application/json',
                'X-ClientTraceId': str(uuid.uuid4())
            }
            # You can pass more than one object in body.
            body = [{
                'text': source_pack.text
            }]
            request = requests.post(self.url, params=params, headers=headers, json=body)
            
            result = request.json()
            target_pack.set_text("""".join(
                [trans['text'] for trans in result[0][""translations""]]
                 )
            )
        @classmethod
        def default_configs(cls):
            return {
                ""from_lang"" : 'en',
                ""to_lang"":  'de',
                ""endpoint"" : 'https://api.cognitive.microsofttranslator.com/',
                ""path"" : '/translate',
                ""subscription_key"": None,
                ""subscription_region"" : ""westus2"",
                'X-ClientTraceId': str(uuid.uuid4())
            }
    nlp: Pipeline = Pipeline[DataPack]()
    nlp.set_reader(StringReader())
    nlp.add(NLTKSentenceSegmenter())
    nlp.add(MultiPackBoxer(), config={""pack_name"": ""source""})
    nlp.add(OnlineMachineTranslationMPProcessor(), config={
        ""from_lang"" : 'en',
        ""to_lang"":  'de',
        ""endpoint"" : 'https://api.cognitive.microsofttranslator.com/',
        ""path"" : '/translate',
        ""subscription_key"": api_key,
        ""subscription_region"" : ""westus2"",
        'X-ClientTraceId': str(uuid.uuid4())
    })
    nlp.initialize()
    for multipack in nlp.process_dataset([input_string]):
        print(""Source Text: "" + multipack.get_pack(""source"").text)
        print(""\nTarget Text: "" + multipack.get_pack(""target"").text)

**How to Save and Load the Pipeline: Save the Whole Pipeline with save()**

In this section, you will learn

* How to export and import a Forte pipeline

Forte also allow us to save the pipeline into disk. It serializes the whole pipeline and generates an intermediate representation, which can be loaded later maybe on a different machine.

    import os
    save_path: str = os.path.join(os.path.dirname(os.path.abspath('')), ""pipeline.yml"")
    nlp.save(save_path)
    with open(save_path, 'r') as f:
        print(f.read())

Now that the pipeline is saved, we can try to re-load the pipeline to see if it still functions as expected.

    new_nlp: Pipeline = Pipeline()
    new_nlp.init_from_config_path(save_path)
    new_nlp.initialize()
    for multipack in new_nlp.process_dataset([input_string]):
        print(""Source Text: "" + multipack.get_pack(""source"").text)
        print(""\nTarget Text: "" + multipack.get_pack(""target"").text)

Now you can build a machine translation system with Forte!",0,6,2022-07-13 23:08:01, p  build a machine translation system with forte,  tldr    this tutorial allows you to build a machine translation system with no glue code using forte  an open source ml workflow builder   xb forte makes it easy to compose any nlp pipeline  regardless of heterogeneity of data and processes  as a modular and easily editable system  it allows users to break down complex problems into composable pipelines and enables inter operations across tasks through a unified data format this tutorial includes      how to read data from source    how to create a simple nlp pipeline  how to maintain and store the input data     how to process data in pipeline    how to perform sentence segmentation  how to annotate and query the data  how to translate the input text with a pre trained model  how to manage multiple data objects     how to handle new practical requests    how to handle structures like html data  how to select a single data object for processing  how to replace the translation model with remote translation services  how to save and load the pipelinerun the following command to install all the required dependencies for this tutorial       it is recommended to install these in command line     pip install forte     forte nltk requests  for certain environment  you may run into troubles installing transformers  such as requiring rust    some workaround here  https     you may want to try different pytorch version depending on your platform    if you cannot install pytorch  try locate your problem at https     for certain environment  the installation may fail    some workaround here  https      how to read data from source    how to create a simple pipeline  start with the reader  in this section  you will learn   what is a reader and why we need it  how to compose a simple pipeline with a pre built reader  xb     from forte import pipelinefrom forte data readers import terminalreader pipeline  pipeline   pipeline  all pipelines need a reader to read and parse input data  to make our pipeline read queries from the user s command line terminal  use the  terminalreader  class provided by forte    terminalreader  transforms the user s query into a datapack object  which is a unified data format for nlp that makes it easy to connect different nlp tools together as forte processors     pipeline set_reader terminalreader   to run the pipeline consisting of the single  terminalreader   call  process_dataset  which will return an iterator of datapack objects  the second line in the following code snippet retrieves the first user query from the terminalreader     pipeline initialize      datapack   next pipeline process_dataset       print datapack text   how to maintain and store input data  datapack  in this section  you will learn   what is a datapack object and why we need itforte helps demystify data lineage and increase the traceability of how data flows along the pipeline and how features are generated to interface data to model  similar to a cargo ship that loads and transports goods from one port to another  a data pack carries information when passing each module and updates the ontology states along the way https datapack and multi modality datapack not only supports text data but also audio and image data https     how to process data in pipeline    how to perform sentence segmentation  add a pre built forte processor to the pipeline  in this section  you will learn   what is a processor and why we need it  how to add a pre built processor to the pipelinea forte processor takes datapacks as inputs  processes them  and stores its outputs in datapacks  the processors we are going to use in this section are all packprocessors  which expect exactly one datapack as input and store its outputs back into the same datapack  the following two lines of code shows how a pre built processor  nltksentencesegmenter  is added to our pipeline     from fortex nltk nltk_processors import nltksentencesegmenter    pipeline add nltksentencesegmenter   when we run the pipeline  the  nltksentencesegmenter  processor will split the user query into sentences and store them back to the datapack created by terminalreader  the code snippet below shows how to get all the sentences from the first query https     from ft onto base_ontology import sentence    pipeline initialize      for sent in next pipeline process_dataset    get sentence          print sent text   how to annotate and query the data  ontology  in this section  you will learn   what is the ontology system and why we need it  how to write a customized ontology and how to use it sentence  is a pre defined ontology provided by forte and it is used by  nltksentencesegmenter to annotate each sentence in text  forte is built on top of an ontology system  which defines the relations between nlp annotations  for example  the relation between words and documents  or between two words  this is the core for forte  the ontology can be specified via a json format  and tools are provided to convert the ontology into production code  python  https we can also define customized ontologies     from dataclasses import dataclass    from forte data ontology top import annotation    from typing import optional         dataclass    class article annotation              language  optional str             def __init__ self  pack  begin  int  end  int              super   __init__ pack  begin  end below is a simple example showing how we can query sentences through the new ontology we just created     from forte data import datapack        sentences            do you want to get better at making delicious bbq          you will have the opportunity  put this on your calendar now          thursday  september nd join world class bbq champion  tony balay from lonestar smoke rangers          datapack  datapack   datapack            add sentences to the datapack and annotate them    for sentence in sentences         datapack set_text datapack text   sentence         datapack add_entry             sentence datapack  len datapack text    len sentence   len datapack text                         annotate the whole text with article    article  article   article datapack    len datapack text      article language   en    datapack add_entry article         for article in datapack get article          print farticle  language    article language            for sentence in article get sentence              print sentence text in our previous example  we have the following ontologies inheritance  sentence and article both inherit from annotation which is used to represent text data  in article  we have  language field to represent the text language https actually  we not only support text ontology but also audio  image and link which represent relationships between two entries https  annotation  is inherited by all text entries which usually has a span to retrieve partial text from the full text    article   as shown in our previous example  inherits annotation and contains  language field to differentiate english and german  in the single datapack example  english article has a span of english text in the datapack  likewise  german article has a span of german text in the datapack    sentence  in our example is used to break down article  and we pass sentences into mt pipeline  audioannotation  is inherited by all audio entries which usually has an audio span to retrieve partial audio from the full audio    recording  is an example subclass of  audioannotation   and it has extra  recording_class field denoting the classes the audio belongs to  imageannotation  is inherited by all image entries which usually has payload index pointing to a loaded image array    boundingbox  is an example subclass of  imageannotation   as the picture shows  it has more inheritance relationships than other ontology classes due to the nature of cv objects  the advantage of forte ontology is that it supports complex inheritance  and users can inherit from existing ontology and add new ontology features for their needs  link  is inherited by all link like entries which has parent and child    relationlink  is an example subclass of  link   and it has a class attribute specifying the relation type   how to translate the input text with a pre trained model  create a machine translation processor  in this section  you will learn   the basics of machine translation process  how to wrap a pre trained machine translation model into a forte processortranslation converts a sequence of text from one language to another  in this tutorial we will use  huggingface  transformer model to translate input data  which consists of several steps including subword tokenization  input embedding  model inference  decoding  etc https in forte  we have a generic class  packprocessor  that wraps model and inference related components and behaviors to process  datapack   therefore  we need to create a class that inherits the generic method from  packprocessor   then we have a class definition  class machinetranslationprocessor packprocessor       from forte data import datapack    from forte data readers import stringreader    from forte processors base import packprocessor    from transformers import ttokenizer  tforconditionalgeneration    class machinetranslationprocessor packprocessor                  translate the input text and output to a file                 def initialize self  resources  configs              super   initialize resources  configs               initialize the tokenizer and model            model_name  str   self configs pretrained_model            self tokenizer   ttokenizer from_pretrained model_name             self model   tforconditionalgeneration from_pretrained model_name             self task_prefix   translate english to german              self tokenizer padding_side   left            self tokenizer pad_token   self tokenizer eos_token        def _process self  input_pack  datapack                ende machine translation             inputs   self tokenizer                  self task_prefix   sentence text                for sentence in input_pack get sentence                return_tensors pt  padding true             output_sequences   self model generate                 input_ids inputs input_ids                  attention_mask inputs attention_mask                  do_sample false                          output      join self tokenizer batch_decode                 output_sequences  skip_special_tokens true                          src_article  article   article input_pack    len input_pack text              src_article language   en            input_pack set_text input_pack text     n n    output             tgt_article  article   article input_pack  len input_pack text    len output   len input_pack text              tgt_article language   de         classmethod        def default_configs cls              return                  pretrained_model  t small             initialization of needed components   users need to consider initializing all needed nlp components for the inference task such as tokenizer and model   users also need to specify all configuration in  configs   a dictionary like object that specifies configurations of all components such as model name mt operations on datapack   after the initialization  we already have the needed nlp components  we need to consider several mt behaviors based on forte datapack pre process text data   retrieve text data from datapack  given that it already reads data from the data source    since t has a better performance given a task prompt  we also want to include the prompt in our data   tokenization that transforms input text into sequences of tokens and token ids   generate output sequences from model   decode output token ids into sentences using the tokenizer the generic method to process  datapack  is  _process self  input_pack  datapack    it should tokenize the input text  use the model class to make an inference  decode the output token ids  and finally write the output to a target file now we can add it into the pipeline and run the machine translation task     input_string  str       join sentences     pipeline  pipeline   pipeline datapack       pipeline set_reader stringreader       pipeline add nltksentencesegmenter       pipeline add machinetranslationprocessor       pipeline initialize      for datapack in pipeline process_dataset  input_string           for article in datapack get article              print  f narticle  language    article language     article text   ontology in datapack here we provide an illustration so that users can better understand the internal storage of datapack  as we can see  text data  sentence and articles  are stored as span in  annotations   their text data can be easily and efficiently retrieved by their spans https   how to manage multiple data objects  multipack  a better way to store source and target text  in this section  you will learn   what is a multipack and why we need it  how to use a multipackthe above step outputs a datapack which is good for holding data about one specific piece of text  a complicated pipeline like the one we are building now may need multiple datapacks to be passed along the pipeline and this is where multipack can help  multipack manages a set of datapacks that can be indexed by their names  multipackboxer  is a simple forte processor that converts a datapack into a multipack by making it the only datapack in there  a name can be specified via the config  we use it to wrap datapack that contains source sentence https     from forte data import multipack    from forte processors base import multipackprocessor    from forte data caster import multipackboxer    class machinetranslationmpprocessor multipackprocessor                  translate the input text and output to a file                 def initialize self  resources  configs              super   initialize resources  configs                   initialize the tokenizer and model            model_name  str   self configs pretrained_model            self tokenizer   ttokenizer from_pretrained model_name             self model   tforconditionalgeneration from_pretrained model_name             self task_prefix   translate english to german              self tokenizer padding_side   left            self tokenizer pad_token   self tokenizer eos_token            def _process self  input_pack  multipack              source_pack  datapack   input_pack get_pack source             target_pack  datapack   input_pack add_pack target                   ende machine translation             inputs   self tokenizer                  self task_prefix   sentence text                for sentence in source_pack get sentence                return_tensors pt  padding true                 output_sequences   self model generate                 input_ids inputs input_ids                  attention_mask inputs attention_mask                  do_sample false                                        annotate the source article            src_article  article   article source_pack    len source_pack text              src_article language   en                          annotate each sentence            for output in self tokenizer batch_decode                 output_sequences  skip_special_tokens true                              target_pack set_text target_pack text   output                 text_length  int   len target_pack text                 sentence target_pack  text_length   len output   text_length                           annotate the target article            tgt_article  article   article target_pack    len target_pack text              tgt_article language   de             classmethod        def default_configs cls              return                  pretrained_model  t small              then  machinetranslationmpprocessor  writes the output sentence into a target datapack https now let s try to create a new pipeline that utilizes  multipack  to manage text in different languages     nlp  pipeline   pipeline datapack       nlp set_reader stringreader       nlp add nltksentencesegmenter       nlp add multipackboxer    config  pack_name  source      nlp add machinetranslationmpprocessor    config          pretrained_model  t small          nlp initialize      for multipack in nlp process_dataset  input_string           for pack_name in  source  target              for article in multipack get_pack pack_name  get article                  print f narticle  language    article language                     for sentence in article get sentence                      print sentence text ontology in multipack for comparison  here is an illustration of the internal storage of multipack  we can see that multipack wraps one source datapack and one target datapack  article spans are based on two separate datapack text https      how to handle new practical requests    how to handle structures like html data  in this section  you will learn  how to build a translation management system  how to preserve the structure like html in machine translation  how to select a specific datapack from multipack for processingin the previous step  the input string is just a simple paragraph made up of several sentences  however  in many cases  we might need to handle data with structural information  such html or xml  when the input is a string of raw html data  the machine translation pipeline above may not work as expected     html_input  str                   beginners bbq class                 do you want to get better at making delicious bbq  you will have the opportunity  put this on your calendar now  thursday  september nd join world class bbq champion  tony balay from lonestar smoke rangers                     nlp initialize      for multipack in nlp process_dataset  html_input           print source text     multipack get_pack source  text         print  ntarget text     multipack get_pack target  text we can see that the original html structure is broken in the translated output in order to handle structured data like html  we will need to update our current design of pipeline  luckily  forte pipelines are highly modular  we can simply insert two new processors without updating the previous pipeline we first need a html cleaner to parse all the html tags from input string  picture below shows the effect of tag remover https after the translation is finished  we will also need to recover the html structure from the unstructured translation output  picture below shows replace one source sentence with one target sentence given the target sentence is ready https     from forte data import namematchselector    from forte data readers html_reader import fortehtmlparser    class htmltagcleaner multipackprocessor                  def initialize self  resources  configs              super   initialize resources  configs             self _parser   fortehtmlparser          def _process self  input_pack  multipack              raw_pack  datapack   input_pack get_pack raw             source_pack  datapack   input_pack add_pack source                         self _parser feed raw_pack text             cleaned_text  str   raw_pack text            for span  _ in self _parser spans                 cleaned_text   cleaned_text replace                     raw_pack text span begin span end                                  source_pack set_text cleaned_text                 class htmltagrecovery multipackprocessor          def _process self  input_pack  multipack              raw_pack  datapack   input_pack get_pack raw             source_pack  datapack   input_pack get_pack source             target_pack  datapack   input_pack get_pack target             result_pack  datapack   input_pack add_pack result             result_text  str   raw_pack text            for sent_src  sent_tgt in zip source_pack get sentence   target_pack get sentence                   result_text   result_text replace sent_src text  sent_tgt text             result_pack set_text result_text now we are able to create a translation management system by inserting the two processors introduced above into our previous machine translation pipeline       pipeline with html handling    pipeline  pipeline   pipeline datapack       pipeline set_reader stringreader       pipeline add multipackboxer    config  pack_name  raw      pipeline add htmltagcleaner       pipeline add         nltksentencesegmenter           selector namematchselector           selector_config  select_name  source          pipeline add machinetranslationmpprocessor    config          pretrained_model  t small          pipeline add htmltagrecovery       pipeline initialize      for multipack in pipeline process_dataset  html_input           print multipack get_pack raw  text         print multipack get_pack result  text selector in the code snippet above  we utilize a  namematchselector  to select one specific datapack from the multipack based on its reference name  select_name   this allows  nltksentencesegmenter  to process only the specified datapack   how to replace the translation model with remote translation services  replace our mt model with online translation api  in this section  you will learn   how to use a different translation serviceforte also allows us to update the translation model and integrate it seamlessly to the original pipeline  for example  if we want to offload the translation task to an online service  all we need to do is to update the translation processor  there is no need to change other components in the pipeline       you can get your own api key by following the instructions in https     api_key   input enter your api key here      import requests    import uuid    class onlinemachinetranslationmpprocessor multipackprocessor                  translate the input text and output to a file use online translator api                 def initialize self  resources  configs              super   initialize resources  configs             self url   configs endpoint   configs path            self from_lang   configs from_lang            self to_lang   configs to_lang            self subscription_key   configs subscription_key            self subscription_region   configs subscription_region        def _process self  input_pack  multipack              source_pack  datapack   input_pack get_pack source             target_pack  datapack   input_pack add_pack target                         params                     api version                        from    en                   to     de                             build request            headers                     ocp apim subscription key   self subscription_key                  ocp apim subscription region   self subscription_region                  content type    application json                   x clienttraceid   str uuid uuid                              you can pass more than one object in body             body                      text   source_pack text                          request   requests post self url  params params  headers headers  json body                         result   request json              target_pack set_text  join                  trans  text   for trans in result   translations                                          classmethod        def default_configs cls              return                  from_lang    en                  to_lang    de                  endpoint    https                 path     translate                  subscription_key  none                 subscription_region   westus                  x clienttraceid   str uuid uuid                    nlp  pipeline   pipeline datapack       nlp set_reader stringreader       nlp add nltksentencesegmenter       nlp add multipackboxer    config  pack_name  source      nlp add onlinemachinetranslationmpprocessor    config          from_lang    en          to_lang    de          endpoint    https         path     translate          subscription_key  api_key         subscription_region   westus          x clienttraceid   str uuid uuid             nlp initialize      for multipack in nlp process_dataset  input_string           print source text     multipack get_pack source  text         print  ntarget text     multipack get_pack target  text   how to save and load the pipeline  save the whole pipeline with save    in this section  you will learn  how to export and import a forte pipelineforte also allow us to save the pipeline into disk  it serializes the whole pipeline and generates an intermediate representation  which can be loaded later maybe on a different machine     import os    save_path  str   os path join os path dirname os path abspath       pipeline yml     nlp save save_path     with open save_path   r   as f         print f read   now that the pipeline is saved  we can try to re load the pipeline to see if it still functions as expected     new_nlp  pipeline   pipeline      new_nlp init_from_config_path save_path     new_nlp initialize      for multipack in new_nlp process_dataset  input_string           print source text     multipack get_pack source  text         print  ntarget text     multipack get_pack target  text now you can build a machine translation system with forte 
11,11,AbjectDrink3276,vy3bon,[D] When will Neurips 2022 reviews be released?,I cant recall what day the last couple of years reviews have been released. I know that the review period is closed and so its only a matter of time just wondering if anyone has any idea?,7,7,2022-07-13 19:01:37, d  when will neurips  reviews be released ,i cant recall what day the last couple of years reviews have been released  i know that the review period is closed and so its only a matter of time just wondering if anyone has any idea 
13,13,Adolphins,vxqv3l,Why do Transformers scale so well? [D]," When you hear people talk about large models, they're usually talking about transformers. What about this architecture has allowed it to be scaled? Have people tried making really large CNNs or RNNs (or just regular MLPs) before?",13,27,2022-07-13 06:46:39,why do transformers scale so well   d , when you hear people talk about large models  they re usually talking about transformers  what about this architecture has allowed it to be scaled  have people tried making really large cnns or rnns  or just regular mlps  before 
14,14,Singularian2501,vxn07k,[R] Deep Hierarchical Planning from Pixels ( Director ) - Google 2022,"Paper: [https://arxiv.org/pdf/2206.04114.pdf](https://arxiv.org/pdf/2206.04114.pdf)

[https://ai.googleblog.com/2022/07/deep-hierarchical-planning-from-pixels.html?m=1](https://ai.googleblog.com/2022/07/deep-hierarchical-planning-from-pixels.html?m=1)

Abstract: 

>Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels. 

https://preview.redd.it/lbvp6r7wl7b91.jpg?width=1034&format=pjpg&auto=webp&s=e9a28b2589eb41148de5b5bb6c4700354e795ae4

https://preview.redd.it/kikyu54xl7b91.jpg?width=1041&format=pjpg&auto=webp&s=b893e54790c420780c79819e689a9666ea95bf86

https://preview.redd.it/m5wc4tdxl7b91.jpg?width=1007&format=pjpg&auto=webp&s=17d7edf3cf7021ceabd3327d9408f1c3bd913c03

https://preview.redd.it/9cwsn9oxl7b91.jpg?width=1015&format=pjpg&auto=webp&s=c96348f290e9ff76c7003c51c97ac86705b77068",1,34,2022-07-13 03:37:09, r  deep hierarchical planning from pixels   director     google ,paper   https  https abstract   intelligent agents need to select long sequences of actions to solve complex tasks  while humans easily break down tasks into subgoals and reach them through millions of muscle commands  current artificial intelligence is limited to tasks with horizons of a few hundred decisions  despite large compute budgets  research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging  current methods rely on manually specified goal spaces or subtasks  and no general solution exists  we introduce director  a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model  the high level policy maximizes task and exploration rewards by selecting latent goals and the low level policy learns to achieve the goals  despite operating in latent space  the decisions are interpretable because the world model can decode goals into images for visualization  director outperforms exploration methods on tasks with sparse rewards  including d maze traversal with a quadruped robot from an egocentric camera and proprioception  without access to the global position or top down view that was used by prior work  director also learns successful behaviors across a wide range of environments  including visual control  atari games  and dmlab levels  https https https https   preview redd it cwsnoxlb jpg width  format pjpg auto webp s cfeffcccacb
15,15,EUMETSAT,vxzcsy,[News] Jupyter Notebook competition - 2 weeks left to enter!,"Are you passionate about [\#coding](https://www.facebook.com/hashtag/coding?__eep__=6&__cft__[0]=AZUaWWSvEyv-U2rnLNbvP3MseROHr1LlG-GsK0bDW9_hBJM-ZHHRlfwvzn2uevBI3sbZw8b7gtV_YAImoKU3IYIdXD_DOzn_L8sgspuXrmZx9p_92-MfaFbtnNcw_j1KtsdoMBkjxMUYPc-hia7qEmktYC4-yEbZREPj2cqkLfIdd10AoF2cSznIA_Qo7I7bmLiyPrnuUPY55hgWW6UO8oUCay-rpP9p0bEKaKsgWBU6XUmgpR2UzpKbjLzC2ZmzUZb3D_DlJlQsaLTWwRBGXOGy&__tn__=*NK-R), [\#DataScience](https://www.facebook.com/hashtag/datascience?__eep__=6&__cft__[0]=AZUaWWSvEyv-U2rnLNbvP3MseROHr1LlG-GsK0bDW9_hBJM-ZHHRlfwvzn2uevBI3sbZw8b7gtV_YAImoKU3IYIdXD_DOzn_L8sgspuXrmZx9p_92-MfaFbtnNcw_j1KtsdoMBkjxMUYPc-hia7qEmktYC4-yEbZREPj2cqkLfIdd10AoF2cSznIA_Qo7I7bmLiyPrnuUPY55hgWW6UO8oUCay-rpP9p0bEKaKsgWBU6XUmgpR2UzpKbjLzC2ZmzUZb3D_DlJlQsaLTWwRBGXOGy&__tn__=*NK-R) or [\#EarthObservation](https://www.facebook.com/hashtag/earthobservation?__eep__=6&__cft__[0]=AZUaWWSvEyv-U2rnLNbvP3MseROHr1LlG-GsK0bDW9_hBJM-ZHHRlfwvzn2uevBI3sbZw8b7gtV_YAImoKU3IYIdXD_DOzn_L8sgspuXrmZx9p_92-MfaFbtnNcw_j1KtsdoMBkjxMUYPc-hia7qEmktYC4-yEbZREPj2cqkLfIdd10AoF2cSznIA_Qo7I7bmLiyPrnuUPY55hgWW6UO8oUCay-rpP9p0bEKaKsgWBU6XUmgpR2UzpKbjLzC2ZmzUZb3D_DlJlQsaLTWwRBGXOGy&__tn__=*NK-R)? 📷 

Don't miss out on the chance to showcase your skills and develop new Jupyter Notebooks using [\#Copernicus](https://www.facebook.com/hashtag/copernicus?__eep__=6&__cft__[0]=AZUaWWSvEyv-U2rnLNbvP3MseROHr1LlG-GsK0bDW9_hBJM-ZHHRlfwvzn2uevBI3sbZw8b7gtV_YAImoKU3IYIdXD_DOzn_L8sgspuXrmZx9p_92-MfaFbtnNcw_j1KtsdoMBkjxMUYPc-hia7qEmktYC4-yEbZREPj2cqkLfIdd10AoF2cSznIA_Qo7I7bmLiyPrnuUPY55hgWW6UO8oUCay-rpP9p0bEKaKsgWBU6XUmgpR2UzpKbjLzC2ZmzUZb3D_DlJlQsaLTWwRBGXOGy&__tn__=*NK-R) data, whilst also being in with a chance of winning cash  📷 prizes! 

Sign up before 31 July at: [https://notebook.wekeo.eu/](https://notebook.wekeo.eu/)

https://preview.redd.it/1uwo4ccv4bb91.png?width=1920&format=png&auto=webp&s=18af6de36526d30585d0027d8445f56ed4302516",1,3,2022-07-13 15:20:09, news  jupyter notebook competition    weeks left to enter ,are you passionate about    coding  https don t miss out on the chance to showcase your skills and develop new jupyter notebooks using    copernicus  https sign up before  july at   https https   preview redd it uwoccvbb png width  format png auto webp s afdedddfed
16,16,TemppaHemppa,vxvyrm,[D] Labeling novel view synthesis for object detection,"Hey all,

I've been following the exciting progress of NeRFs, and it lead to me wonder whether there are research on generating novel 2D views from 3D representation, and labeling those examples. I find works for image classification under Novel View Synthesis topics, but for object detection I just can't find anything.

Wouldn't it be possible to label 2D training images, construct 3D representation, and use it for generating novel 2D views with corresponding labelings? I see this as highly useful for the object detection domain, where labeling often requires a lot of manual work leading to small datasets and non-robust object representations.

Please note if I'm missing something out here.",0,5,2022-07-13 11:30:45, d  labeling novel view synthesis for object detection,hey all i ve been following the exciting progress of nerfs  and it lead to me wonder whether there are research on generating novel d views from d representation  and labeling those examples  i find works for image classification under novel view synthesis topics  but for object detection i just can t find anything wouldn t it be possible to label d training images  construct d representation  and use it for generating novel d views with corresponding labelings  i see this as highly useful for the object detection domain  where labeling often requires a lot of manual work leading to small datasets and non robust object representations please note if i m missing something out here 
17,17,paulcjh,vx89nj,[P] DALL·E Mini & Mega demo and production API,"Hi all - we've just put out the community DALL·E models on Playgrounds.ai:

Mega - [https://playgrounds.ai/models/dalle-mega](https://playgrounds.ai/models/dalle-mega?utm_source=reddit&utm_medium=post&utm_campaign=dalle_1&utm_id=c.1)

Mini - [https://playgrounds.ai/models/dalle-mini](https://playgrounds.ai/models/dalle-mini?utm_source=reddit&utm_medium=post&utm_campaign=dalle_1&utm_id=c.1)

You can use this models via API on PipelineCloud here: [https://dashboard.pipeline.ai](https://dashboard.pipeline.ai) 

The per image cost for the models are approx:

Mega - $0.0014 (\~10s of compute for 4 images)

Mini - $0.00062 (\~10s of compute for 9 images)

This is for people who want to use these models in their apps/products or just play around with the demos and have fun!

https://preview.redd.it/zre4tf40a4b91.png?width=3114&format=png&auto=webp&s=68d8c10236cdd23c642e581d479d479b38fede84",32,157,2022-07-12 16:17:35, p  dall e mini   mega demo and production api,hi all   we ve just put out the community dall e models on playgrounds ai mega    https mini    https you can use this models via api on pipelinecloud here   https the per image cost for the models are approx mega         s of compute for  images mini         s of compute for  images this is for people who want to use these models in their apps products or just play around with the demos and have fun https   preview redd it zretfab png width  format png auto webp s dccddceddbfede
18,18,Rafaelkoll,vy3p7q,[D] Ensemble regression model - based on models trained on different feature spaces,"What is the best method for constructing an ensemble regression model from numerous KNN regression models that were trained on slightly **different feature spaces**?

I can't only use the features that they have in common.",2,1,2022-07-13 19:19:26, d  ensemble regression model   based on models trained on different feature spaces,what is the best method for constructing an ensemble regression model from numerous knn regression models that were trained on slightly   different feature spaces   i can t only use the features that they have in common 
19,19,ResearcherNo4728,vxw64g,"[D] How best to handle a column that can hold multiple, unbounded number of values?"," Say I have an email dataset. Two of its columns are ""sender"" and ""recipients"". Now, the ""sender"" column will only hold one value in each row. However, ""recipients"" can be anything in number from 1 to 100, or even more theoretically. In such a scenario, one hot encoding is not a tractable solution. And neither is creating a new row for each unique recipient. So, how best to handle this situation?",4,2,2022-07-13 11:44:15, d  how best to handle a column that can hold multiple  unbounded number of values , say i have an email dataset  two of its columns are sender and recipients  now  the sender column will only hold one value in each row  however  recipients can be anything in number from  to   or even more theoretically  in such a scenario  one hot encoding is not a tractable solution  and neither is creating a new row for each unique recipient  so  how best to handle this situation 
20,20,takeafuckinsipp,vxl0ae,[P] Ensembling with multiple independent time-series,"I'm working on a project in which I have N independent time-series datasets, which can be thought of like prices for different currencies/crypto-coins etc. I've structured my dataset such that for each training batch, the first dimension is the index of the time-series. 

I have a prediction model based on a couple papers, which takes in a sliding window and outputs a prediction of the time series. 

**Question: What is the best way to build an ensemble of this model, such that predictions for each time-series aren't affected by the others?**

When I say ""aren't affected by other time series"", i mean that the average of predictions of two different models trained on two different series might not be as accurate/precise as the predictions by themselves (without averaging)...

Should I have N different models for each time series and just average the predictions? Should I have some K number of models with different loss functions and then average those? 

What would be a good strategy?",5,10,2022-07-13 02:08:28, p  ensembling with multiple independent time series,i m working on a project in which i have n independent time series datasets  which can be thought of like prices for different currencies crypto coins etc  i ve structured my dataset such that for each training batch  the first dimension is the index of the time series  i have a prediction model based on a couple papers  which takes in a sliding window and outputs a prediction of the time series    question  what is the best way to build an ensemble of this model  such that predictions for each time series aren t affected by the others   when i say aren t affected by other time series  i mean that the average of predictions of two different models trained on two different series might not be as accurate precise as the predictions by themselves  without averaging    should i have n different models for each time series and just average the predictions  should i have some k number of models with different loss functions and then average those  what would be a good strategy 
21,21,Spiritual_Fig3632,vxteu1,[D] tranfer learning with freezing vs unfreezing,"Hi, I have been trying to test self-supervised representation learning on vision-task. In more detail, testing [BYOL](https://arxiv.org/abs/2006.07733) in cifar-10. I found the trick that they threw away the last layer and put a new layer for the output shape, and the backbone network is frozen during finetuning. I know that the bad last layer can harm to the backbone network during finetuning because network is highly sensitive to even small change in parameter space. But I tried to finetune without freezing, It shows better last performance(accuracy 82% -> 90% at test). So why did they freeze the backbone network and show the results of the experiment?

How can I explain this phenomenon? Thank you for reading.",5,2,2022-07-13 08:59:41, d  tranfer learning with freezing vs unfreezing,hi  i have been trying to test self supervised representation learning on vision task  in more detail  testing  byol  https how can i explain this phenomenon  thank you for reading 
22,22,ajt9000,vx045i,[D] How do you verify the novelty of your research?," 

While working on my own research and struggling to find related works it got me thinking. What process do you follow to discover preexisting research similar to your own?

With the fast pace of research in the field, and so much overlapping terminology, do you use fancy tools or go beyond just typing queries into google scholar until you get relevant papers to your own? How do you find what you don't know to look for?",55,177,2022-07-12 08:00:07, d  how do you verify the novelty of your research , while working on my own research and struggling to find related works it got me thinking  what process do you follow to discover preexisting research similar to your own with the fast pace of research in the field  and so much overlapping terminology  do you use fancy tools or go beyond just typing queries into google scholar until you get relevant papers to your own  how do you find what you don t know to look for 
23,23,Boring-Violinist8291,vxlek2,[D] Does vector prediction merit using a multivariate output model?,"I’m making a framework that predicts a displacement vector for a series of points on a map, using features from those points. There’s evidence of a dependency between the correlation coefficient of vector values (i.e. x and y-displacement) and some of the features. 

Would this merit using a multivariate output model (likely gradient boosting tree regression) or should I use two univariate output models? If not, what should I be looking into?",0,3,2022-07-13 02:25:29, d  does vector prediction merit using a multivariate output model ,i m making a framework that predicts a displacement vector for a series of points on a map  using features from those points  there s evidence of a dependency between the correlation coefficient of vector values  i e  x and y displacement  and some of the features  would this merit using a multivariate output model  likely gradient boosting tree regression  or should i use two univariate output models  if not  what should i be looking into 
24,24,IllustriousCicada603,vxfy50,[D] Does it make sense to generate text sequences with Transformer-based models and then have a classifier to choose between multiple options.,"Hello, I have a topic for discussion: Are you aware of systems which have a sequence-to-sequence architecture such as a Transformer, generating multiple outputs for a given task, and then another model - a MLP, another Transformer or something else which learns to pick the best option. Is it possible for this extra step to extract more knowledge from given data and increase the performance of the pipeline (even though at the cost of more computing power)? In what contexts does (not) that make sense?",10,7,2022-07-12 22:25:46, d  does it make sense to generate text sequences with transformer based models and then have a classifier to choose between multiple options ,hello  i have a topic for discussion  are you aware of systems which have a sequence to sequence architecture such as a transformer  generating multiple outputs for a given task  and then another model   a mlp  another transformer or something else which learns to pick the best option  is it possible for this extra step to extract more knowledge from given data and increase the performance of the pipeline  even though at the cost of more computing power   in what contexts does  not  that make sense 
25,25,giuse_tweets,vxihha,[R] DiBB: Distributing Black-Box Optimization,"Author here. Just presented this work at GECCO 2022.

- Quick summary: https://twitter.com/giuse_tweets/status/1546920346015637505
- Paper: https://exascale.info/assets/pdf/cuccu2022gecco.pdf
- Code + tutorials: https://github.com/giuse/dibb
- Experiments (COCO/BBOB-LS): https://github.com/eXascaleInfolab/dibb_coco
- Recorded rehearsal of the talk: https://tinyurl.com/dibb-video

AMA!",0,5,2022-07-13 00:17:24, r  dibb  distributing black box optimization,author here  just presented this work at gecco    quick summary  https   paper  https   code   tutorials  https   experiments  coco bbob ls   https   recorded rehearsal of the talk  https ama 
26,26,mingrui-zhang,vx4tdz,[P] Building efficient ML applications with Taichi's automatic differentiation,"&#x200B;

https://i.redd.it/d66p6f6p23b91.gif

Hey guys 

I am working on an open-source, parallel programming language, Taichi Lang, which I find efficient in differentiable physical simulation and can help speed up the convergence of ML processes.  

Above is a simple demo supported by Taichi's inbuilt autodiff (automatic differentiation) system. You can move the target as you wish, and the magic fountain always changes its trajectory accordingly to hit the target.

So basically, Taichi Lang's Source Code Transformation system generates gradient kernels during compile time, and the lightweight `tape` in the Python scope records the launched Taichi kernels and replays the gradient kernels in reverse order during backpropagation. Model training is done within 10  optimization iterations.

A step-by-step explanation: [https://www.reddit.com/user/mingrui-zhang/comments/vx49mz/training\_a\_magic\_fountain\_using\_taichis\_autodiff/](https://www.reddit.com/user/mingrui-zhang/comments/vx49mz/training_a_magic_fountain_using_taichis_autodiff/)

Source code: [https://github.com/taichi-dev/taichi/blob/master/python/taichi/examples/autodiff/diff\_sph/diff\_sph.py](https://github.com/taichi-dev/taichi/blob/master/python/taichi/examples/autodiff/diff_sph/diff_sph.py)",11,35,2022-07-12 12:24:54, p  building efficient ml applications with taichi s automatic differentiation,  xb https hey guys i am working on an open source  parallel programming language  taichi lang  which i find efficient in differentiable physical simulation and can help speed up the convergence of ml processes   above is a simple demo supported by taichi s inbuilt autodiff  automatic differentiation  system  you can move the target as you wish  and the magic fountain always changes its trajectory accordingly to hit the target so basically  taichi lang s source code transformation system generates gradient kernels during compile time  and the lightweight  tape  in the python scope records the launched taichi kernels and replays the gradient kernels in reverse order during backpropagation  model training is done within   optimization iterations a step by step explanation   https source code   https   github com taichi dev taichi blob master python taichi examples autodiff diff _sph diff _sph py  https   github com taichi dev taichi blob master python taichi examples autodiff diff_sph diff_sph py 
27,27,davidmezzetti,vxbdf4,[P] Run transformers model inference in C/C++ and Assembly with the Python C API,"&#x200B;

https://preview.redd.it/xjtcha3r35b91.png?width=1298&format=png&auto=webp&s=00873223c1ea0c6afcd5e22c7645521036b7e341

This post presents a way to run transformers models via the Python C API. The referenced notebook loads two txtai workflows, one that translates English to French and another that summarizes a webpage. After loading the models through C code, another example runs the workflows through assembly to show this works with any native code.

Full code links: [Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/36_Run_txtai_in_native_code.ipynb) | [GitHub](https://github.com/neuml/txtai)",2,6,2022-07-12 19:03:59, p  run transformers model inference in c c   and assembly with the python c api,  xb https this post presents a way to run transformers models via the python c api  the referenced notebook loads two txtai workflows  one that translates english to french and another that summarizes a webpage  after loading the models through c code  another example runs the workflows through assembly to show this works with any native code full code links   notebook  https   colab research google com github neuml txtai blob master examples _run_txtai_in_native_code ipynb     github  https   github com neuml txtai 
28,28,subtask_net,vx5ryw,[P] Helping data scientists access large ML datasets,"I spent so much time building data pipelines which feels like a huge constraint on my time and ability to focus on actual ML tasks. That's why I'm building [subtask.net](https://subtask.net) which collects and builds large, constantly updated, ML datasets from across the internet. The goal is to cut out the data collection part of any ML project and make more datasets available beyond the typical open-source datasets provided by the community.",4,13,2022-07-12 13:29:58, p  helping data scientists access large ml datasets,i spent so much time building data pipelines which feels like a huge constraint on my time and ability to focus on actual ml tasks  that s why i m building  subtask net  https   subtask net  which collects and builds large  constantly updated  ml datasets from across the internet  the goal is to cut out the data collection part of any ml project and make more datasets available beyond the typical open source datasets provided by the community 
29,29,Ok-Wind-1215,vx01wq,[D] Efficiently choose good papers in top-tier conferences,"Hey, As a senior Phd student, I still feel a bit tired of looking for and reading through the massive newly accepted papers in top-tier conferences/journals like neurips/icml/iclr/jmlr/cvpr.... 

Any suggestions for efficiently selecting good papers ?",11,35,2022-07-12 07:57:02, d  efficiently choose good papers in top tier conferences,hey  as a senior phd student  i still feel a bit tired of looking for and reading through the massive newly accepted papers in top tier conferences journals like neurips icml iclr jmlr cvpr     any suggestions for efficiently selecting good papers  
30,30,imunabletocode,vx6dz5,[D] How to choose best model during training if validation loss fluctuates a lot?," I am training a deep neural network, unfortunately, I have few samples for my validation set, so the relative loss fluctuate a lot. How can I choose the best model during training? Usually I choose the model which is associated with the lowest validation loss, but now there are random fluctuation that lower loss function. I think the fluctuations are due to the fact that I can't use the whole sample because I am using Colab free and i haven't enough RAM. I tried to modify the splig Train/Train/Vali increasing Vali size and the oscillations seems a bit lower, but i would like to mantain the ratio 60/20/20 for a better and more significative classification.",12,10,2022-07-12 14:12:06, d  how to choose best model during training if validation loss fluctuates a lot , i am training a deep neural network  unfortunately  i have few samples for my validation set  so the relative loss fluctuate a lot  how can i choose the best model during training  usually i choose the model which is associated with the lowest validation loss  but now there are random fluctuation that lower loss function  i think the fluctuations are due to the fact that i can t use the whole sample because i am using colab free and i haven t enough ram  i tried to modify the splig train train vali increasing vali size and the oscillations seems a bit lower  but i would like to mantain the ratio    for a better and more significative classification 
31,31,EnricoShippole,vxf5w3,[P] Token-to-Token ViT Implementation in Flax,"&#x200B;

https://preview.redd.it/0mh5d00tx5b91.png?width=479&format=png&auto=webp&s=ac8c83e80d058d032e9083512da749216d9a2221

An open-source implementation of the Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet research paper in Google's JAX and Flax.

""Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384×384 on ImageNet."" - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan

Github Repository for JAX /Flax model: [https://github.com/conceptofmind/Token-to-Token-ViT-flax](https://github.com/conceptofmind/Token-to-Token-ViT-flax)

Tokens-to-Token ViT Research Paper: [https://arxiv.org/abs/2203.10790](https://arxiv.org/abs/2203.10790)

Official Github Repository: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)

In collaboration with Dr. Phil 'Lucid' Wang: [https://github.com/lucidrains](https://github.com/lucidrains).",0,3,2022-07-12 21:52:09, p  token to token vit implementation in flax,  xb https an open source implementation of the tokens to token vit  training vision transformers from scratch on imagenet research paper in google s jax and flax transformers  which are popular for language modeling  have been explored for solving vision tasks recently  e g   the vision transformer  vit  for image classification  the vit model splits each image into a sequence of tokens with fixed length and then applies multiple transformer layers to model their global relation for classification  however  vit achieves inferior performance to cnns when trained from scratch on a midsize dataset like imagenet  we find it is because    the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels  leading to low training sample efficiency    the redundant attention backbone design of vit leads to limited feature richness for fixed computation budgets and limited training samples  to overcome such limitations  we propose a new tokens to token vision transformer  tt vit   which incorporates   a layer wise tokens to token  tt  transformation to progressively structurize the image to tokens by recursively aggregating neighboring tokens into one token  tokens to token   such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced    an efficient backbone with a deep narrow structure for vision transformer motivated by cnn architecture design after empirical study  notably  tt vit reduces the parameter count and macs of vanilla vit by half  while achieving more than    improvement when trained from scratch on imagenet  it also outperforms resnets and achieves comparable performance with mobilenets by directly training on imagenet  for example  tt vit with comparable size to resnet   m parameters  can achieve    top accuracy in image resolution   on imagenet    li yuan  yunpeng chen  tao wang  weihao yu  yujun shi  zihang jiang  francis eh tay  jiashi feng  shuicheng yangithub repository for jax  flax model   https tokens to token vit research paper   https official github repository   https in collaboration with dr  phil  lucid  wang   https   github com lucidrains  https   github com lucidrains  
33,33,thejashGI,vxhpnx,"[D]Oleh Rybkin, UPenn, on exploration and planning with world models","Here is a [podcast](https://generallyintelligent.ai/podcast/2022-07-11-podcast-episode-18-oleh-rybkin/) with Oleh Rybkin where we discuss agents that explore and plan (and do yoga), how to learn world models from video, what's missing from current RL research, and much more!",0,0,2022-07-12 23:43:30, d oleh rybkin  upenn  on exploration and planning with world models,here is a  podcast  https   generallyintelligent ai podcast    podcast episode  oleh rybkin   with oleh rybkin where we discuss agents that explore and plan  and do yoga   how to learn world models from video  what s missing from current rl research  and much more 
34,34,anacondavibes,vx10zn,[D] Understanding how hardware plays a role in creating AI models," I'm wondering if there's any sort of article/videos/reddit post focused on explaining everything to know about hardware and it's impact on AI (cores, tensors, cores, threading, etc.) I have a lot of background from the software side so code optimization isn't something that tI've thought too much about but I'm currently working on building my own PC so I do need this information (I'm not looking for a guide because that won't help me learn, but I want to learn all this stuff from the ground up).

Any recommendations on where I can learn more about this? Thanks!",5,10,2022-07-12 08:44:58, d  understanding how hardware plays a role in creating ai models, i m wondering if there s any sort of article videos reddit post focused on explaining everything to know about hardware and it s impact on ai  cores  tensors  cores  threading  etc   i have a lot of background from the software side so code optimization isn t something that ti ve thought too much about but i m currently working on building my own pc so i do need this information  i m not looking for a guide because that won t help me learn  but i want to learn all this stuff from the ground up  any recommendations on where i can learn more about this  thanks 
35,35,Singularian2501,vwyjh1,"[R] Machine Learning Operations (MLOps): Overview, Definition, and Architecture","Paper: [https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)

Abstract:

>The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies. 

https://preview.redd.it/km40o6fce1b91.jpg?width=785&format=pjpg&auto=webp&s=1e1079e839c8230f03df4bcd25b2cc3d58d42049",3,12,2022-07-12 06:42:21, r  machine learning operations  mlops   overview  definition  and architecture,paper   https abstract  the final goal of all industrial machine learning  ml  projects is to develop ml products and rapidly bring them into production  however  it is highly challenging to automate and operationalize ml products and thus many ml endeavors fail to deliver on their expectations  the paradigm of machine learning operations  mlops  addresses this issue  mlops includes several aspects  such as best practices  sets of concepts  and development culture  however  mlops is still a vague term and its consequences for researchers and professionals are ambiguous  to address this gap  we conduct mixed method research  including a literature review  a tool review  and expert interviews  as a result of these investigations  we provide an aggregated overview of the necessary principles  components  and roles  as well as the associated architecture and workflows  furthermore  we furnish a definition of mlops and highlight open challenges in the field  finally  this work provides guidance for ml researchers and practitioners who want to automate and operate their ml products with a designated set of technologies  https   preview redd it kmofceb jpg width  format pjpg auto webp s eecfdfbcdbccdd
36,36,Character-Rip-5824,vwxs2u,"""[Project]"" Brainchop: In Browser 3D Segmentation. Now 50 and 104 Brain Segmentations. (Follow up).","&#x200B;

https://reddit.com/link/vwxs2u/video/91mo2fnr81b91/player

Live Demo:  [brainchop.org](https://neuroneural.github.io/brainchop/)

  
[Brainchop](https://github.com/neuroneural/brainchop) is  a  client-side web-application  for automatic segmentation of MRI     volumes  , we make implementation of [brainchop](https://github.com/neuroneural/brainchop) freely available releasing its pure **Javascript** code as open-source.

We appreciate your **ideas/feedback /comments** here or with the [discussion](https://github.com/neuroneural/brainchop/discussions) board,   and please  star [Brainchop](https://github.com/neuroneural/brainchop) if you  like it to keep it going.",4,11,2022-07-12 06:04:49, project  brainchop  in browser d segmentation  now  and  brain segmentations   follow up  ,  xb https live demo    brainchop org  https    brainchop  https we appreciate your   ideas feedback  comments   here or with the  discussion  https   github com neuroneural brainchop discussions  board    and please  star  brainchop  https   github com neuroneural brainchop  if you  like it to keep it going 
37,37,AdelSexy,vwdp7n,[D] Next big thing in the field,"Do you guys have any forecasts of next big model/algorithm/concept in DL?

We had CNNs disrupting the field in \~2015, then GANs became a big deal, RL grown quite a lot, Transformers trended recently, now Diffusion models are moving probabilistic ML forward (sorry if I missed something). What other not fully investigated or underestimated concepts with high potential are there?",98,137,2022-07-11 14:18:13, d  next big thing in the field,do you guys have any forecasts of next big model algorithm concept in dl we had cnns disrupting the field in     then gans became a big deal  rl grown quite a lot  transformers trended recently  now diffusion models are moving probabilistic ml forward  sorry if i missed something   what other not fully investigated or underestimated concepts with high potential are there 
38,38,Azuresonance,vw8jtp,[D] Why are Corgi dogs so popular in machine learning (especially in the image generation community)?,"For example, here's part of OpenAI's GLIDE paper:

https://preview.redd.it/b6vkxyb3xua91.png?width=1225&format=png&auto=webp&s=15d56f256e323bb54d22eb9fdc0538644060c4a7",68,313,2022-07-11 08:48:28, d  why are corgi dogs so popular in machine learning  especially in the image generation community  ,for example  here s part of openai s glide paper https   preview redd it bvkxybxua png width  format png auto webp s dfebbdebfdcca
39,39,shreyansh26,vwq2ac,[P] Paper Implementation - Extracting Training Data from Large Language Models," A re-implementation of the famous 2020 paper - ""Extracting Training Data from Large Language Models"" by Nicholas Carlini, Florian Tramer et al.

Code - [https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models](https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models)

The official implementation is great and I definitely learned a few things from it. In the re-implementation, I have also included the temperature-decay sampling and sliding-window-based minimum perplexity metric which was not present in the official implementation.

I checked the extracted Samples (refer to the Github repo) and they surely contained some memorized information.",0,17,2022-07-12 00:29:05, p  paper implementation   extracting training data from large language models, a re implementation of the famous  paper   extracting training data from large language models by nicholas carlini  florian tramer et al code    https the official implementation is great and i definitely learned a few things from it  in the re implementation  i have also included the temperature decay sampling and sliding window based minimum perplexity metric which was not present in the official implementation i checked the extracted samples  refer to the github repo  and they surely contained some memorized information 
40,40,InfiniteLife2,vwkbtr,[D] What is your go-to algorithm for Multiple Object Tracking with possible long time occlusions?," 

Im interested in tracking cars and people with ability to solve occlusion of objects that might not be moving. Things I've tried are decent but not amazing(Deepsort, ByteTrack). There is a few recent studies about using transformers for tracking, but those things are heavy and not really production material, having deformable convolutions in them(hard or not possible convert to torchscript and tensorrt) and all.

What's your go-to algorithm for this kind of problem?",6,23,2022-07-11 20:24:38, d  what is your go to algorithm for multiple object tracking with possible long time occlusions , im interested in tracking cars and people with ability to solve occlusion of objects that might not be moving  things i ve tried are decent but not amazing deepsort  bytetrack   there is a few recent studies about using transformers for tracking  but those things are heavy and not really production material  having deformable convolutions in them hard or not possible convert to torchscript and tensorrt  and all what s your go to algorithm for this kind of problem 
41,41,inigomlap,vwf9wo,[R] Closed-Form Diffeomorphic Transformations for Time Series Alignment,"Paper: [https://arxiv.org/pdf/2206.08107.pdf](https://arxiv.org/pdf/2206.08107.pdf)

Code: [https://github.com/imartinezl/difw](https://github.com/imartinezl/difw)

Abstract:

>Time series alignment methods call for highly expressive, differentiable and invertible warping functions which preserve temporal topology, i.e diffeomorphisms. **Diffeomorphic warping functions can be generated from the integration of velocity fields governed by an ordinary differential equation (ODE)**. Gradient-based optimization frameworks containing diffeomorphic transformations require to calculate derivatives to the differential equation's solution with respect to the model parameters, i.e. sensitivity analysis. Unfortunately, deep learning frameworks typically lack automatic-differentiation-compatible sensitivity analysis methods; and implicit functions, such as the solution of ODE, require particular care. Current solutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or ResNet's Eulerian discretization. In this work, we present a **closed-form expression for the ODE solution and its gradient under continuous piecewise-affine (CPA) velocity functions**. We present a highly optimized implementation of the results on CPU and GPU. Furthermore, we conduct extensive experiments on several datasets to validate the generalization ability of our model to unseen data for time-series joint alignment. Results show significant improvements both in terms of efficiency and accuracy.

https://reddit.com/link/vwf9wo/video/vvjnwp2y0xa91/player

&#x200B;",4,29,2022-07-11 16:07:37, r  closed form diffeomorphic transformations for time series alignment,paper   https code   https abstract  time series alignment methods call for highly expressive  differentiable and invertible warping functions which preserve temporal topology  i e diffeomorphisms    diffeomorphic warping functions can be generated from the integration of velocity fields governed by an ordinary differential equation  ode     gradient based optimization frameworks containing diffeomorphic transformations require to calculate derivatives to the differential equation s solution with respect to the model parameters  i e  sensitivity analysis  unfortunately  deep learning frameworks typically lack automatic differentiation compatible sensitivity analysis methods  and implicit functions  such as the solution of ode  require particular care  current solutions appeal to adjoint sensitivity methods  ad hoc numerical solvers or resnet s eulerian discretization  in this work  we present a   closed form expression for the ode solution and its gradient under continuous piecewise affine  cpa  velocity functions    we present a highly optimized implementation of the results on cpu and gpu  furthermore  we conduct extensive experiments on several datasets to validate the generalization ability of our model to unseen data for time series joint alignment  results show significant improvements both in terms of efficiency and accuracy https   xb 
42,42,chaude_patate,vwln0s,[D] Speech Enhancement SOTA,"Audio denoising (removing background noises from audio), often referred as Speech Enhancement, has been a midly popular research field up to 2020. This was due to COVID and the need to filter unwanted noises from calls. 

However, I'm not sure where we're at today:

* [Music Source Separation](https://www.reddit.com/r/MachineLearning/comments/pqpl7m/r_decoupling_magnitude_and_phase_estimation_with/) is improved by Tiktok and Deezer's researches
* Meta's [denoiser](https://github.com/facebookresearch/denoiser) looks like the most standard, production-ready model, and it implements a 2020 paper

I'd like to search for more alternatives, but I struggle to find some:

* Googling ""Denoising"" will lead to images noise removal
* Paper with Code's ""Speech denoising"" and ""Audio denoising"" categories are pretty empty. 
* The ""[Speech Enhancement](https://paperswithcode.com/task/speech-enhancement)"" category seems to be the real deal, but the top models don't have any pretrained version available.

Is there a model that outperform Meta's denoiser, while remaining open-source with an available pretrained model?",4,6,2022-07-11 21:21:43, d  speech enhancement sota,audio denoising  removing background noises from audio   often referred as speech enhancement  has been a midly popular research field up to   this was due to covid and the need to filter unwanted noises from calls  however  i m not sure where we re at today    music source separation  https   meta s  denoiser  https i d like to search for more alternatives  but i struggle to find some   googling denoising will lead to images noise removal  paper with code s speech denoising and audio denoising categories are pretty empty    the  speech enhancement  https is there a model that outperform meta s denoiser  while remaining open source with an available pretrained model 
43,43,Labib666Camp,vwi9p7,[D] Modeling Adjacency Matrix,"Lets assume, I have some directed adjacency matrix *A* at time *t* and another adjacency matrix *B* at time *t+1*. I want to learn a mapping from *A* to *B* through some model *f* (suppose *f* is a neural network). Now, how should I create this model ? Should I use just Dense layers or GNNs or something?",8,7,2022-07-11 18:49:34, d  modeling adjacency matrix,lets assume  i have some directed adjacency matrix  a  at time  t  and another adjacency matrix  b  at time  t    i want to learn a mapping from  a  to  b  through some model  f   suppose  f  is a neural network   now  how should i create this model   should i use just dense layers or gnns or something 
44,44,blessedorcursed,vwg37x,[P] Semi-supervised learning for tabular data: VIME,"A lot of recent DL models for tabular data have used some sort of pre-training to increase the robustness and performance metrics on smaller/noisy datasets. That's why I've decided to write a [deep-dive blog](https://syslog.ravelin.com/fraud-detection-with-minimum-labels-semi-supervised-learning-d2f8e7136da6) into a VIME paper which was one of the first to suggest pre-training tasks specific for tabular data. 

It comes with an accompanying [repo](https://github.com/aruberts/blogs/tree/main/vime) that contains all the code and notebooks. From some personal testing that I've done, pre-training is the most valuable does improve the performance when we're dealing with very few labels (1-5% of the dataset). Of course, the best solution is to always get more labels lol, but when it's not possible, pre-training schemes like VIME can give you a small boost in performance. 

Give it a read and let me know what you think! I'll keep covering some interesting deep tabular architectures, so maybe also let me know which one would you want me to cover next!",0,9,2022-07-11 16:57:07, p  semi supervised learning for tabular data  vime,a lot of recent dl models for tabular data have used some sort of pre training to increase the robustness and performance metrics on smaller noisy datasets  that s why i ve decided to write a  deep dive blog  https it comes with an accompanying  repo  https give it a read and let me know what you think  i ll keep covering some interesting deep tabular architectures  so maybe also let me know which one would you want me to cover next 
45,45,EnricoShippole,vwmlrs,[P] ScalableViT Implementation in Flax,"An open-source implementation of the ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer research paper in Google's JAX and Flax.

""The vanilla self-attention mechanism inherently relies on pre-defined and steadfast computational dimensions. Such inflexibility restricts it from possessing context-oriented generalization that can bring more contextual cues and global representations. To mitigate this issue, we propose a Scalable Self-Attention (SSA) mechanism that leverages two scaling factors to release dimensions of query, key, and value matrix while unbinding them with the input. This scalability fetches context-oriented generalization and enhances object sensitivity, which pushes the whole network into a more effective trade-off state between accuracy and cost. Furthermore, we propose an Interactive Window-based Self-Attention (IWSA), which establishes interaction between non-overlapping regions by re-merging independent value tokens and aggregating spatial information from adjacent windows. By stacking the SSA and IWSA alternately, the Scalable Vision Transformer (ScalableViT) achieves state-of-the-art performance in general-purpose vision tasks. For example, ScalableViT-S outperforms Twins-SVT-S by 1.4% and Swin-T by 1.8% on ImageNet-1K classification."" - Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng, Xiu Li  


Github repository for the Flax / JAX model: [https://github.com/conceptofmind/Scalable-ViT-flax](https://github.com/conceptofmind/Scalable-ViT-flax)

ScalableViT Research Paper: [https://arxiv.org/abs/2203.10790](https://arxiv.org/abs/2203.10790)

In collaboration with Lucid: [https://github.com/lucidrains](https://github.com/lucidrains)",0,2,2022-07-11 22:01:46, p  scalablevit implementation in flax,an open source implementation of the scalablevit  rethinking the context oriented generalization of vision transformer research paper in google s jax and flax the vanilla self attention mechanism inherently relies on pre defined and steadfast computational dimensions  such inflexibility restricts it from possessing context oriented generalization that can bring more contextual cues and global representations  to mitigate this issue  we propose a scalable self attention  ssa  mechanism that leverages two scaling factors to release dimensions of query  key  and value matrix while unbinding them with the input  this scalability fetches context oriented generalization and enhances object sensitivity  which pushes the whole network into a more effective trade off state between accuracy and cost  furthermore  we propose an interactive window based self attention  iwsa   which establishes interaction between non overlapping regions by re merging independent value tokens and aggregating spatial information from adjacent windows  by stacking the ssa and iwsa alternately  the scalable vision transformer  scalablevit  achieves state of the art performance in general purpose vision tasks  for example  scalablevit s outperforms twins svt s by    and swin t by    on imagenet k classification    rui yang  hailong ma  jie wu  yansong tang  xuefeng xiao  min zheng  xiu li  github repository for the flax   jax model   https scalablevit research paper   https in collaboration with lucid   https   github com lucidrains  https   github com lucidrains 
47,47,cheemsdoge69,vwm5qa,[D] Instance segmentation using transformers,"Hi folks!
I am looking for beginner-friendly and easy to implement papers on instance segmentation using transformers.
Any help will be appreciated!!",1,2,2022-07-11 21:43:15, d  instance segmentation using transformers,hi folks i am looking for beginner friendly and easy to implement papers on instance segmentation using transformers any help will be appreciated  
48,48,CapitalShake3085,vwkwfr,[R] DA-Faster RCNN,"Hello,

I have reimplemented DA-Faster RCNN using Detectron2 one of the most important architecture for domain adaptation for object detection. This implementations is easy to use and can be used also with google colab :) here there is the link: [https://github.com/GiovanniPasq/DA-Faster-RCNN](https://github.com/GiovanniPasq/DA-Faster-RCNN)",0,1,2022-07-11 20:49:06, r  da faster rcnn,hello i have reimplemented da faster rcnn using detectron one of the most important architecture for domain adaptation for object detection  this implementations is easy to use and can be used also with google colab    here there is the link   https   github com giovannipasq da faster rcnn  https   github com giovannipasq da faster rcnn 
49,49,aadityaura,vweop5,[R] An awesome collection of Federated learning & Blockchain research papers in the Healthcare domain,"An awesome collection of Federated learning & Blockchain research papers in the Healthcare domain.

  
Federated learning, a mechanism of training a shared global model with a central server while keeping all the sensitive data in local institutions where the data belong, provides great promise to connect the fragmented healthcare data sources with privacy preservation. This repo contains a curated list of Federated Learning papers/resources and recent advancements in Healthcare.

&#x200B;

* As of now \~**330 papers**
* Pr's welcome

[https://github.com/monk1337/Aweome-Heathcare-Federated-Learning](https://github.com/monk1337/Aweome-Heathcare-Federated-Learning)",0,2,2022-07-11 15:28:59, r  an awesome collection of federated learning   blockchain research papers in the healthcare domain,an awesome collection of federated learning   blockchain research papers in the healthcare domain   federated learning  a mechanism of training a shared global model with a central server while keeping all the sensitive data in local institutions where the data belong  provides great promise to connect the fragmented healthcare data sources with privacy preservation  this repo contains a curated list of federated learning papers resources and recent advancements in healthcare   xb   as of now      papers    pr s welcome https   github com monk aweome heathcare federated learning  https   github com monk aweome heathcare federated learning 
50,50,timscarfe,vvkmf1,[D] Noam Chomsky on LLMs and discussion of LeCun paper (MLST),"""First we should ask the question whether LLM have achieved ANYTHING, ANYTHING in this domain. Answer, NO, they have achieved ZERO!"" - Noam Chomsky 

""There are engineering projects that are significantly advanced by \[[\#DL](https://mobile.twitter.com/hashtag/DL?src=hashtag_click)\] methods. And this is all the good. \[...\] Engineering is not a trivial field; it takes intelligence, invention, \[and\] creativity these achievements. That it contributes to science?"" - Noam Chomsky 

""There was a time \[supposedly dedicated\] to the study of the nature of [\#intelligence](https://mobile.twitter.com/hashtag/intelligence?src=hashtag_click). By now it has disappeared.""  Earlier, same interview: ""GPT-3 can \[only\] find some superficial irregularities in the data. \[...\] It's exciting for reporters in the NY Times."" - Noam Chomsky 

""It's not of interest to people, the idea of finding an explanation for something. \[...\] The \[original [\#AI](https://mobile.twitter.com/hashtag/AI?src=hashtag_click)\] field by now is considered old-fashioned, nonsense. \[...\] That's probably where the field will develop, where the money is. \[...\] But it's a shame."" - Noam Chomsky 

Thanks to Dagmar Monett for selecting the quotes!

Sorry for posting a controversial thread -- but this seemed noteworthy for /machinelearning 

Video: [https://youtu.be/axuGfh4UR9Q](https://youtu.be/axuGfh4UR9Q) \-- also some discussion of LeCun's recent position paper",239,271,2022-07-10 11:09:21, d  noam chomsky on llms and discussion of lecun paper  mlst ,first we should ask the question whether llm have achieved anything  anything in this domain  answer  no  they have achieved zero    noam chomsky there are engineering projects that are significantly advanced by      dl  https there was a time   supposedly dedicated   to the study of the nature of    intelligence  https it s not of interest to people  the idea of finding an explanation for something          the   original    ai  https thanks to dagmar monett for selecting the quotes sorry for posting a controversial thread    but this seemed noteworthy for  machinelearning video   https   youtu be axugfhurq  https   youtu be axugfhurq      also some discussion of lecun s recent position paper
52,52,Capital_Revolution35,vw3tkf,"[P] A Website to generate Code Snippets, Regexes, Linux & Git & SQL Commands, HTML and CSS from a written description. Furthermore translate code snippets to many languages and get a regex explained in plain english. Moreover you can fix broken code snippets. All with the help of ML 🤖","https://reddit.com/link/vw3tkf/video/xe0t4pumpta91/player

https://reddit.com/link/vw3tkf/video/7pf9dl3npta91/player

**Programming**

* [**Function from Description**](https://programming-helper.com/generate-function)
* [**Code to Explanation**](https://programming-helper.com/code-to-explanation)
* [**Fix invalid Code**](https://programming-helper.com/fix-invalid-code)
* [**Translate Languages**](https://programming-helper.com/translate)
* [**Class from Description**](https://programming-helper.com/class-from-description)
* [**Get Language from Code**](https://programming-helper.com/language-from-code)
* [**Function from Docstring**](https://programming-helper.com/docstring)

**Helpers**

* [**Regex from Description**](https://programming-helper.com/regex)
* [**Regex to Explanation**](https://programming-helper.com/regex-explanation)
* [**Linux Command**](https://programming-helper.com/linux)
* [**Get time complexity**](https://programming-helper.com/time-complexity)
* [**Git Command from Description**](https://programming-helper.com/git)

**Database**

* [**Text Description to SQL Command**](https://programming-helper.com/text-to-sql-syntax)

**Web**

* [**Generate HTML from Description**](https://programming-helper.com/generate-html-from-description)
* [**CSS from Description**](https://programming-helper.com/css-from-description)
* [**Meta Tags from Description**](https://programming-helper.com/meta)

I think this could be helpful to a lot of people (especially for beginner programmers). You can check out all functionalities on your own here:

[programming-helper.com](https://programming-helper.com/)

**Have fun using the tool** ❤️",0,7,2022-07-11 04:44:09, p  a website to generate code snippets  regexes  linux   git   sql commands  html and css from a written description  furthermore translate code snippets to many languages and get a regex explained in plain english  moreover you can fix broken code snippets  all with the help of ml  ,https https   programming       function from description    https      code to explanation    https      fix invalid code    https      translate languages    https      class from description    https      get language from code    https      function from docstring    https   helpers       regex from description    https      regex to explanation    https      linux command    https      get time complexity    https      git command from description    https   database       text description to sql command    https   web       generate html from description    https      css from description    https      meta tags from description    https i think this could be helpful to a lot of people  especially for beginner programmers   you can check out all functionalities on your own here  programming helper com  https   have fun using the tool     
53,53,ericyu3,vvvt52,[Project] Parakeet — Copilot for Colab,"Hello!

I've long been a big fan of [GitHub Copilot](https://github.com/features/copilot) — I've used it for a while now, and I find it super helpful for all sorts of things. But Copilot doesn't work in Colab or Jupyter notebooks, even though that's where a ton of ML and data science code is written.

**Parakeet** is a Chrome extension that provides Copilot-like code suggestions for notebooks.

I've been using Parakeet for my own needs for a bit, and I'm already getting a lot of mileage out of it. Just the other day, for example, I wanted to make a Seaborn plot but wasn't sure how. I wrote a short comment, Parakeet suggested some code, and the code worked on the first try!

# Installation

[**Install from the Chrome Web Store**](https://chrome.google.com/webstore/detail/parakeet/linkknplelcdbncponjdhcjdknlpgghc)

[View source code](https://github.com/uyhcire/parakeet)

You'll need an email to sign up.

Parakeet is currently **free to use** for everyone, though that may change once OpenAI introduces pricing for Codex.

# Demos

[Generating code to plot a sine wave.](https://i.redd.it/le5rzwb0ora91.gif)

[Plotting a heat map. All I had to do was write some comments — Parakeet's suggested code worked on the first try.](https://preview.redd.it/q1hl6uicpra91.png?width=1240&format=png&auto=webp&s=35e17142321dc9874aae82bc038f1828db4a3325)

# Limitations

Parakeet currently only works for Colab, though I'm considering extending Parakeet to support Jupyter. If you want to use Parakeet outside Colab, I'd love to hear about your use case! You can file an issue on GitHub or you can email me at [ericyu3@gmail.com](mailto:ericyu3@gmail.com).

To keep things simple, Parakeet only makes suggestions when you are at the end of a line, and Parakeet never makes multi-line suggestions.

# How it works

* Parakeet uses OpenAI's [Codex](https://openai.com/blog/openai-codex/) model, which is the same model that powers GitHub Copilot.
* Parakeet does not have access to Colab's internal state. Instead, Parakeet continuously parses Colab's HTML to extract cell contents and determine what row and column your cursor is on. This approach was finicky to get working, but I was able to get it to work reliably and with little performance penalty.
* Your code is never stored or logged. After a suggestion is generated, the input is immediately discarded.",7,23,2022-07-10 22:30:08, project  parakeet   copilot for colab,hello i ve long been a big fan of  github copilot  https   parakeet   is a chrome extension that provides copilot like code suggestions for notebooks i ve been using parakeet for my own needs for a bit  and i m already getting a lot of mileage out of it  just the other day  for example  i wanted to make a seaborn plot but wasn t sure how  i wrote a short comment  parakeet suggested some code  and the code worked on the first try   installation   install from the chrome web store    https  view source code  https you ll need an email to sign up parakeet is currently   free to use   for everyone  though that may change once openai introduces pricing for codex   demos generating code to plot a sine wave   https  plotting a heat map  all i had to do was write some comments   parakeet s suggested code worked on the first try   https   limitationsparakeet currently only works for colab  though i m considering extending parakeet to support jupyter  if you want to use parakeet outside colab  i d love to hear about your use case  you can file an issue on github or you can email me at  ericyu gmail com  mailto ericyu gmail com  to keep things simple  parakeet only makes suggestions when you are at the end of a line  and parakeet never makes multi line suggestions   how it works  parakeet uses openai s  codex  https   parakeet does not have access to colab s internal state  instead  parakeet continuously parses colab s html to extract cell contents and determine what row and column your cursor is on  this approach was finicky to get working  but i was able to get it to work reliably and with little performance penalty   your code is never stored or logged  after a suggestion is generated  the input is immediately discarded 
54,54,yosefschwartz,vvl47v,[D] What's the problem with Self-driving cars? Is it a lack of data or do we need a new technology breakthrough?,"I mean there was a time when everyone thought that in a few years we would have self-drive cars. We just need more data and computing and we'll get it.
But now Google has more than 20m miles on a public road and much more in simulations.
And Tesla has a lot of cars that collect data on the road.

But it's still not there so what is missing? Do we need a new technology breakthrough or it's just more data and computing power?",143,89,2022-07-10 11:41:48, d  what s the problem with self driving cars  is it a lack of data or do we need a new technology breakthrough ,i mean there was a time when everyone thought that in a few years we would have self drive cars  we just need more data and computing and we ll get it but now google has more than m miles on a public road and much more in simulations and tesla has a lot of cars that collect data on the road but it s still not there so what is missing  do we need a new technology breakthrough or it s just more data and computing power 
56,56,Sacrezar,vvubm7,[D] Any french Corpus like ALECTOR for simplification task?,"Hello, the title says it all. I'm trying to find any ressources (mainly aligned corpus) that could be helpful in identifying and simplifying complex sentences in French. [ALECTOR](https://aclanthology.org/2020.lrec-1.169/) is the only one I stumbled upon.

Do you have any resources or tips? I was wondering if searching for book and their simplified version could be useful but I fear it would be more like learning to translate old french into modern french.",0,5,2022-07-10 21:19:57, d  any french corpus like alector for simplification task ,hello  the title says it all  i m trying to find any ressources  mainly aligned corpus  that could be helpful in identifying and simplifying complex sentences in french   alector  https do you have any resources or tips  i was wondering if searching for book and their simplified version could be useful but i fear it would be more like learning to translate old french into modern french 
58,58,DragonLord9,vuw77a,[N] First-Ever Course on Transformers: NOW PUBLIC,"**CS 25: Transformers United**

https://preview.redd.it/1st4o3tvtha91.png?width=350&format=png&auto=webp&s=e4416da38001692989304e980dd4d61d23a74398

Did you grow up wanting to play with robots that could turn into cars? While we can't offer those kinds of transformers, we do have a course on the class of deep learning models that have taken the world by storm.

Announcing the public release of our lectures from the first-ever course on **Transformers: CS25 Transformers United** ([http://cs25.stanford.edu](http://cs25.stanford.edu/)) held at [Stanford University](https://www.linkedin.com/school/stanford-university/).

Our intro video is out and available to watch here 👉: [***YouTube Link***](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&fbclid=IwAR2mJd868IzGp8ChykBBRTxq7RQh-KICfnAg8rLQ-qsekbhnUcd_z4-4E7g)

Bookmark and spread the word 🤗!

[(Twitter Thread)](https://twitter.com/DivGarg9/status/1545541542235975682?s=20&t=_Ed9dpjD9Qpx4svpMNDIKQ&fbclid=IwAR2tnSQROnkOQl15aa6nkfNFaJdrnZQHDbidooDaQRJALlWsYMiQU_37dn4)

Speaker talks out starting Monday ...",37,346,2022-07-09 12:47:02, n  first ever course on transformers  now public,  cs   transformers united  https did you grow up wanting to play with robots that could turn into cars  while we can t offer those kinds of transformers  we do have a course on the class of deep learning models that have taken the world by storm announcing the public release of our lectures from the first ever course on   transformers  cs transformers united     http our intro video is out and available to watch here        youtube link     https bookmark and spread the word     twitter thread   https speaker talks out starting monday    
59,59,Labib666Camp,vvms5n,[D] Interpreting Attention Weights,"I have seen in many papers, specially in Deep learning applications in medical imaging, that they interpret attention weights as something like interaction between features (ie. Feature Interaction). But, every time you train the model wouldn't you get new weights? Then, how does this interoperability holds any value if the weights keep changing everytime you run it?",5,3,2022-07-10 13:39:11, d  interpreting attention weights,i have seen in many papers  specially in deep learning applications in medical imaging  that they interpret attention weights as something like interaction between features  ie  feature interaction   but  every time you train the model wouldn t you get new weights  then  how does this interoperability holds any value if the weights keep changing everytime you run it 
60,60,fromnighttilldawn,vuvslv,[D] When did tech companies start to publish ML papers and why?,"I never fully understood the need for tech companies to publish research papers at big conferences.

I think before the 2000s, tech companies were very secretive about their work. I mean, you wouldn't expect Microsoft to publish their latest research on their own motherboard at some conferences right?

Nowadays all of them are trying to advertise their latest tech in research papers that could possibly be replicated by anyone around the world. This is especially visible in ML.

Also it almost seems as if they don't have a goal in mind. A lot of the research papers (outside of those big models such as DALL-E) seem to be VERY random to me, hardly even related to their business interests.

How did it become this way and what is their motivation?",34,120,2022-07-09 12:19:50, d  when did tech companies start to publish ml papers and why ,i never fully understood the need for tech companies to publish research papers at big conferences i think before the s  tech companies were very secretive about their work  i mean  you wouldn t expect microsoft to publish their latest research on their own motherboard at some conferences right nowadays all of them are trying to advertise their latest tech in research papers that could possibly be replicated by anyone around the world  this is especially visible in ml also it almost seems as if they don t have a goal in mind  a lot of the research papers  outside of those big models such as dall e  seem to be very random to me  hardly even related to their business interests how did it become this way and what is their motivation 
61,61,SeucheAchat9115,vvs041,[D] Reimplementing an Object Detection Model.,How hard is it to reimplement an object detection model to reproduce the results on benchmarks like COCO. Lets take the DINO architecture or even some yolo v4-7 Model. How hard is it to build it from scratch to reach COCO results reported by the paper or official implementations?,5,0,2022-07-10 19:25:50, d  reimplementing an object detection model ,how hard is it to reimplement an object detection model to reproduce the results on benchmarks like coco  lets take the dino architecture or even some yolo v  model  how hard is it to build it from scratch to reach coco results reported by the paper or official implementations 
62,62,EnricoShippole,vv9kwp,[P] CaiT Implementation in Flax,"An open-source implementation of the Going deeper with Image Transformers research paper in Google's JAX and Flax.

""The paper also notes the difficulty in training vision transformers at greater depths and proposes two solutions. First, it proposes to do per-channel multiplication of the output of the residual block. Second, it proposes to have the patches attend to one another, and only allow the CLS token to attend to the patches in the last few layers."" - Lucid

Github repository for the Flax / JAX model: [https://github.com/conceptofmind/CaiT-Flax](https://github.com/conceptofmind/CaiT-Flax)

CaiT Research Paper: [https://arxiv.org/abs/2103.17239](https://arxiv.org/abs/2103.17239)

Official PyTorch repository: [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)

In collaboration with Lucid: [https://github.com/lucidrains](https://github.com/lucidrains)",0,11,2022-07-10 01:19:50, p  cait implementation in flax,an open source implementation of the going deeper with image transformers research paper in google s jax and flax the paper also notes the difficulty in training vision transformers at greater depths and proposes two solutions  first  it proposes to do per channel multiplication of the output of the residual block  second  it proposes to have the patches attend to one another  and only allow the cls token to attend to the patches in the last few layers    lucidgithub repository for the flax   jax model   https cait research paper   https official pytorch repository   https in collaboration with lucid   https   github com lucidrains  https   github com lucidrains 
63,63,jacobgil,vv1pja,[P] May the best explanation win: A tutorial on benchmarking and tuning model explanations with pytorch-grad-cam,"The new release of the pytorch-grad-cam project focuses on metrics for the model explanations.

It's often exciting to see model explanations, and tempting to interpret them and get insights about what the model is doing. And a lot of times it is very useful.

However this has to be done with care - the model explanations can be wrong, or sub optimal. As shown in many papers, sometimes random explanations perform better.

So it's useful to have metrics that measure the quality of the explanations for an image, and sanity checks about them.

This can be used both for getting some trust in the explanation before using it,

but also for tuning the explanation and getting the best one for a given image (for example by checking different methods).

&#x200B;

This notebook gives a thorough overview of the different metrics used in the literature, issues with them, using sanity checks (like the Sobel Edge Detector, or a random CAM),

and most importantly shows how to use them to chose and tune the explanation in practice.

[https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/CAM%20Metrics%20And%20Tuning%20Tutorial.ipynb](https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/CAM%20Metrics%20And%20Tuning%20Tutorial.ipynb)

&#x200B;

The motivation here is to both make it easier for researchers to benchmark new algorithms, but also (maybe more importantly) when using the model explanations to tune them, get the most out of them, and find problems with them.",0,15,2022-07-09 18:57:24, p  may the best explanation win  a tutorial on benchmarking and tuning model explanations with pytorch grad cam,the new release of the pytorch grad cam project focuses on metrics for the model explanations it s often exciting to see model explanations  and tempting to interpret them and get insights about what the model is doing  and a lot of times it is very useful however this has to be done with care   the model explanations can be wrong  or sub optimal  as shown in many papers  sometimes random explanations perform better so it s useful to have metrics that measure the quality of the explanations for an image  and sanity checks about them this can be used both for getting some trust in the explanation before using it but also for tuning the explanation and getting the best one for a given image  for example by checking different methods    xb this notebook gives a thorough overview of the different metrics used in the literature  issues with them  using sanity checks  like the sobel edge detector  or a random cam  and most importantly shows how to use them to chose and tune the explanation in practice  https   xb the motivation here is to both make it easier for researchers to benchmark new algorithms  but also  maybe more importantly  when using the model explanations to tune them  get the most out of them  and find problems with them 
64,64,DrSkoolie,vv3iu2,[R] How to use ML to predict time of life remaining on a physical asset of the input data has had all its failed samples scrubbed away?,"So I'm in a bit of a conundrum. I'm working on my PhD thesis regarding the management of physical assets (make a decision on whether to replace the asset or refurbish it or to leave it alone).

The first step to doing this is to predict the estimated time of life for each asset and I wish to use ML to do this. Each asset in my dataset has an installation date and a couple of input features (results of testing, characteristics of the asset, etc)

The problem is the dataset I have doesn't have any of the failed assets. Meaning that I am finding it very hard to set up an error term for the estimated time of life during training of the model.


Ideally, I should have failed samples and non-failed samples in my data but I only have the latter. How should I go about setting this up?

I've been trying for the past couple of months to get my hands on failed samples but I haven't had any luck.",11,2,2022-07-09 20:30:09, r  how to use ml to predict time of life remaining on a physical asset of the input data has had all its failed samples scrubbed away ,so i m in a bit of a conundrum  i m working on my phd thesis regarding the management of physical assets  make a decision on whether to replace the asset or refurbish it or to leave it alone  the first step to doing this is to predict the estimated time of life for each asset and i wish to use ml to do this  each asset in my dataset has an installation date and a couple of input features  results of testing  characteristics of the asset  etc the problem is the dataset i have doesn t have any of the failed assets  meaning that i am finding it very hard to set up an error term for the estimated time of life during training of the model ideally  i should have failed samples and non failed samples in my data but i only have the latter  how should i go about setting this up i ve been trying for the past couple of months to get my hands on failed samples but i haven t had any luck 
66,66,Singularian2501,vueqv8,[R] DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale - Microsoft 2022,"Paper: [https://arxiv.org/pdf/2207.00032.pdf](https://arxiv.org/pdf/2207.00032.pdf)

Abstract:

>The past several years have witnessed the success of transformer-based models, and their scale and application scenarios continue to grow aggressively. The current landscape of transformer models is increasingly diverse: the model size varies drastically with the largest being of hundred-billion parameters; the model characteristics differ due to the sparsity introduced by the Mixture-of-Experts; the target application scenarios can be latency-critical or throughput-oriented; the deployment hardware could be single- or multi-GPU systems with different types of memory and storage, etc. With such increasing diversity and the fast-evolving pace of transformer models, designing a highly performant and efficient inference system is extremely challenging. In this paper, we present DeepSpeed Inference, a comprehensive system solution for transformer model inference to address the above-mentioned challenges. DeepSpeed Inference consists of (1) a multi-GPU inference solution to minimize latency while maximizing the throughput of both dense and sparse transformer models when they fit in aggregate GPU memory, and (2) a heterogeneous inference solution that leverages CPU and NVMe memory in addition to the GPU memory and compute to enable high inference throughput with large models which do not fit in aggregate GPU memory. **DeepSpeed Inference reduces latency by up to 7.3X over the state-of-the-art for latency-oriented scenarios and increases throughput by over 1.5x for throughput-oriented scenarios. Moreover, it enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of GPUs, an unprecedented scale for inference. It can inference 25x larger models than with GPU-only solutions, while delivering a high throughput of 84 TFLOPS (over 50% of A6000 peak).**       

https://preview.redd.it/a7a9ir42dda91.jpg?width=1440&format=pjpg&auto=webp&s=fd1aa3ac66117f0c8a3355553de61d8cb50fdb5e

https://preview.redd.it/am49dqo2dda91.jpg?width=1446&format=pjpg&auto=webp&s=ad5dc1c17301b6f006a1678e4afc8fca83c7bdd4

https://preview.redd.it/vyxod3jmdda91.jpg?width=694&format=pjpg&auto=webp&s=9397aa3727d36780d90e7c833e6c9d6dd0d848d0

https://preview.redd.it/x3tc9sumdda91.jpg?width=695&format=pjpg&auto=webp&s=703ef19715deac0b0d96cfc761e9e05e20e8e1a5

https://preview.redd.it/062lnm7ndda91.jpg?width=634&format=pjpg&auto=webp&s=71746178333a6e356cc8ff0392ca9cb5472f2862",4,119,2022-07-08 22:00:27, r  deepspeed inference  enabling efficient inference of transformer models at unprecedented scale   microsoft ,paper   https abstract  the past several years have witnessed the success of transformer based models  and their scale and application scenarios continue to grow aggressively  the current landscape of transformer models is increasingly diverse  the model size varies drastically with the largest being of hundred billion parameters  the model characteristics differ due to the sparsity introduced by the mixture of experts  the target application scenarios can be latency critical or throughput oriented  the deployment hardware could be single  or multi gpu systems with different types of memory and storage  etc  with such increasing diversity and the fast evolving pace of transformer models  designing a highly performant and efficient inference system is extremely challenging  in this paper  we present deepspeed inference  a comprehensive system solution for transformer model inference to address the above mentioned challenges  deepspeed inference consists of    a multi gpu inference solution to minimize latency while maximizing the throughput of both dense and sparse transformer models when they fit in aggregate gpu memory  and    a heterogeneous inference solution that leverages cpu and nvme memory in addition to the gpu memory and compute to enable high inference throughput with large models which do not fit in aggregate gpu memory    deepspeed inference reduces latency by up to  x over the state of the art for latency oriented scenarios and increases throughput by over  x for throughput oriented scenarios  moreover  it enables trillion parameter scale inference under real time latency constraints by leveraging hundreds of gpus  an unprecedented scale for inference  it can inference x larger models than with gpu only solutions  while delivering a high throughput of  tflops  over   of a peak           https https https https https   preview redd it lnmndda jpg width  format pjpg auto webp s aeccffcacbf
67,67,zxkj,vumtus,[D] How to evaluate a neural network in reverse?,"Say you have a neural network with 3 inputs, some hidden layers, and a single output.

There might be many sets of those 3 inputs that give you the same output value.

How can you evaluate this network in reverse, i.e. given an output value, find values of the 3 inputs that would yield that output?",36,35,2022-07-09 04:06:40, d  how to evaluate a neural network in reverse ,say you have a neural network with  inputs  some hidden layers  and a single output there might be many sets of those  inputs that give you the same output value how can you evaluate this network in reverse  i e  given an output value  find values of the  inputs that would yield that output 
68,68,highergraphic,vu4r1g,[P] Sioyek 1.4 | Academic PDF Viewer,"During my PhD, I developed an open source PDF viewer to help me with my research. I think it can be useful for the users of this sub. Some of the research-oriented features include:

* Quickly jump to or preview references (for example `Figure 3.1` for a figure or `[8]` for a reference). Works even if the document doesn't have links.
* Search paper names in google scholar by middle clicking on them (combined with the previous feature makes finding papers super fast)
* Searchable highlights/bookmarks
* Line-by-line highlighting for reduced eye strain ([video](https://user-images.githubusercontent.com/6392321/168427739-007be805-a457-4d1f-ba14-35c5070aae5f.mp4))
* Synctex Support
* Extensible using external scripts (see [this post](https://ahrm.github.io/jekyll/update/2022/07/05/implementing-a-screen-reader-for-sioyek.html) for some examples)
* And many other features which are explained in the github page including marks, history, portals, searchable table of contents, automatic table of contents generation, searchable previous documents, etc.

Here is a video demo of some of the features: https://www.youtube.com/watch?v=yTmCI0Xp5vI&t=3s

And here is the latest release:
https://github.com/ahrm/sioyek/releases/tag/v1.4.0

Disclaimer: I did introduce sioyek in this subreddit about a year ago, but it has changed a lot since then and some of the features suggested in the comments of last year's post are implemented, so I thought users of this subreddit might be interested in an update.",16,249,2022-07-08 12:41:27, p  sioyek     academic pdf viewer,during my phd  i developed an open source pdf viewer to help me with my research  i think it can be useful for the users of this sub  some of the research oriented features include   quickly jump to or preview references  for example  figure    for a figure or      for a reference   works even if the document doesn t have links   search paper names in google scholar by middle clicking on them  combined with the previous feature makes finding papers super fast   searchable highlights bookmarks  line by line highlighting for reduced eye strain   video  https   synctex support  extensible using external scripts  see  this post  https   and many other features which are explained in the github page including marks  history  portals  searchable table of contents  automatic table of contents generation  searchable previous documents  etc here is a video demo of some of the features  https and here is the latest release https disclaimer  i did introduce sioyek in this subreddit about a year ago  but it has changed a lot since then and some of the features suggested in the comments of last year s post are implemented  so i thought users of this subreddit might be interested in an update 
69,69,RohitDulam,vulpil,[R] Single-task Continual/Incremental/Online/Life-Long learning.," 

Hi everyone,

I am new to the domain of continual learning/incremental learning/online learning/life-long learning (honestly, not able to make out the difference between them) and I would like to know if there exists a single-task life-long learning domain/problem. All the papers that I have gone through consist of methods trained for multiple tasks where newer tasks are added over time. I am looking for models trained for a single task that can be updated over time with new data belonging to the same task. I already have a trained model that I would like to update over time with either single or multiple data points. Any related links or directions would be greatly appreciated. TIA.",2,5,2022-07-09 03:15:38, r  single task continual incremental online life long learning , hi everyone i am new to the domain of continual learning incremental learning online learning life long learning  honestly  not able to make out the difference between them  and i would like to know if there exists a single task life long learning domain problem  all the papers that i have gone through consist of methods trained for multiple tasks where newer tasks are added over time  i am looking for models trained for a single task that can be updated over time with new data belonging to the same task  i already have a trained model that i would like to update over time with either single or multiple data points  any related links or directions would be greatly appreciated  tia 
70,70,avocado_aa,vtow5o,[D] An accusation of academic misconduct by Prof. Yisen Wang (Peking University) in ICML2021 and NeurIPS2021,"I recently noticed a Weibo (Chinese Twitter) thread of an alarming potential academic misconduct - Prof. Yisen Wang’s girlfriend accused him of cheating and collusion behaviors in recent top-tier machine learning conferences, including but may not limit to NeurIPS2021 and ICML2021. Yisen Wang (homepage: https://yisenwang.github.io/) obtained his Ph.D. degree at Tsinghua University (China) and is now an assistant professor at Peking University (China). Yisen is interested in adversarial attack, etc.


Here are some facts from Yisen’s girlfriend’s post:

1.	[Cheating in best paper nomination in ICML 2021] In ICML2021, Yisen asked one area chair of ICML2021 to recommend his first PhD student Jingyi Cui’s paper to be best paper candidate(I am not sure if it is termed as “best paper candidate”, maybe another name.). And after he got the news that it is candidate, Zhouchen Lin proposed to add his name in this paper. Before that, Zhouchen Lin is not in the author list. The paper name is  ""Leveraged Weighted Loss for Partial Label Learning"". Many professors in China have received the audios and more detailed wechat history. In the audio, Yisen admitted the existence of 8-person group and did manipulate the NeurIPS2021. Sooo crazy!

the wechat history link is :https://imgur.com/4H7TDs6

In the WeChat chat history shown below, the girl (the green chatbox) asked Yisen, “all of you are area chairs, do you think it is inappropriate that you guys give accepts to and nominate each other’s paper as the best paper candidate?” Yisen (the white chatbox) admitted, “it is kind of inappropriate but I tried to cut down the rate”.
     


2.	[Colluding with other area chairs in NeurIPS 2021] However, in the following NeurIPS2021, Yisen did this more unscrupulously. As his girlfriend pointed out, Yisen has only published one paper in NeurIPS before 2021, but he “miraculously” became an area chair in NeurIPS 2021. Once he became the area chair of NeurIPS 2021, he suddenly got 12 papers accepted in NeurIPS 2021. The potential reason is that Yisen colluded with other area chairs/reviewers: He and the other 7 people had a WeChat group; most of them are area chairs in NeurIPS, ICML, and ICLR - to be specific, almost all of them are NeurIPS2021 area chairs. They arranged online meetings to bid each other’s papers so that their papers can be accepted with a higher probability. This is INSANE. 
	


Yisen’s girlfriend has reported this to Peking University more than 4 months but still with no reply. 


I found one more letter of this thing. In the letter, she provided some wechat history between Zhouchen and Yisen, about adding Zhouchen's name in ICML2021. The link is :https://imgur.com/a/y05HXPu",115,652,2022-07-07 23:35:11, d  an accusation of academic misconduct by prof  yisen wang  peking university  in icml and neurips,i recently noticed a weibo  chinese twitter  thread of an alarming potential academic misconduct   prof  yisen wang s girlfriend accused him of cheating and collusion behaviors in recent top tier machine learning conferences  including but may not limit to neurips and icml  yisen wang  homepage  https here are some facts from yisen s girlfriend s post  	 cheating in best paper nomination in icml   in icml  yisen asked one area chair of icml to recommend his first phd student jingyi cui s paper to be best paper candidate i am not sure if it is termed as  best paper candidate   maybe another name    and after he got the news that it is candidate  zhouchen lin proposed to add his name in this paper  before that  zhouchen lin is not in the author list  the paper name is  leveraged weighted loss for partial label learning  many professors in china have received the audios and more detailed wechat history  in the audio  yisen admitted the existence of  person group and did manipulate the neurips  sooo crazy the wechat history link is  https in the wechat chat history shown below  the girl  the green chatbox  asked yisen   all of you are area chairs  do you think it is inappropriate that you guys give accepts to and nominate each other s paper as the best paper candidate   yisen  the white chatbox  admitted   it is kind of inappropriate but i tried to cut down the rate        	 colluding with other area chairs in neurips   however  in the following neurips  yisen did this more unscrupulously  as his girlfriend pointed out  yisen has only published one paper in neurips before   but he  miraculously  became an area chair in neurips   once he became the area chair of neurips   he suddenly got  papers accepted in neurips   the potential reason is that yisen colluded with other area chairs reviewers  he and the other  people had a wechat group  most of them are area chairs in neurips  icml  and iclr   to be specific  almost all of them are neurips area chairs  they arranged online meetings to bid each other s papers so that their papers can be accepted with a higher probability  this is insane  	yisen s girlfriend has reported this to peking university more than  months but still with no reply  i found one more letter of this thing  in the letter  she provided some wechat history between zhouchen and yisen  about adding zhouchen s name in icml  the link is  https   imgur com a yhxpu
71,71,MidnightMaverick,vu975k,[Discussion] Giving a machine learning presentation to laypeople,"Hello all,

I've been asked to deliver a machine learning presentation to cardiologists and doctors, obviously they have no prior expertise in this area. I had wondered if anyone else had some experience presenting to Laypeople in the context of machine learning.

Just looking for some ideas really, what would you cover? What examples would you give? How would you structure it?

Any help is always appreciated !

[Edit#1] Thank you for the help everyone, this is some really useful feedback that I will take on bosrd",15,22,2022-07-08 17:35:11, discussion  giving a machine learning presentation to laypeople,hello all i ve been asked to deliver a machine learning presentation to cardiologists and doctors  obviously they have no prior expertise in this area  i had wondered if anyone else had some experience presenting to laypeople in the context of machine learning just looking for some ideas really  what would you cover  what examples would you give  how would you structure it any help is always appreciated   edit   thank you for the help everyone  this is some really useful feedback that i will take on bosrd
72,72,QadriShyaari,vuh3z4,[P] Chart and Data Summarization,"I made an app that summarizes the data in csv files. Input a csv file and title of the file and the model will generate a summary.

[https://huggingface.co/spaces/saadob12/Chart\_Data\_Summarization](https://huggingface.co/spaces/saadob12/Chart_Data_Summarization)

The models: [https://huggingface.co/saadob12/t5\_C2T\_autochart](https://huggingface.co/saadob12/t5_C2T_autochart) and [https://huggingface.co/saadob12/t5\_C2T\_big](https://huggingface.co/saadob12/t5_C2T_big).",0,3,2022-07-08 23:45:48, p  chart and data summarization,i made an app that summarizes the data in csv files  input a csv file and title of the file and the model will generate a summary  https the models   https   huggingface co saadob t _ct _autochart  https   huggingface co saadob t_ct_autochart  and  https   huggingface co saadob t _ct _big  https   huggingface co saadob t_ct_big  
73,73,applied-roboticist,vum5fb,[Discussion] How do I smoothen the output of an action segmentation model near the boundaries?,"Hello. Apologies if this is the wrong place to post because my problem is a simple one related to machine learning. My problem involves a robot that operates given the output of an action segmentation model. The trained model outputs an action label at every timestep e.g. \[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3\]. 

Given the sequence, I must now compute the time it takes to go from one label to the next. However, the actual output tends to be quite unstable especially when the action transitions from one to the next e.g., \[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, ....\]. As such, I occasionally get multiple transitions. This simple issue makes the input unusable to the robot.

How can I clean up the output sequence? I thought of simple operations like converting the vector into a one-hot matrix and then running 1D erode and dilate operations but I was hoping to hear several other better suggestions.",4,1,2022-07-09 03:35:31, discussion  how do i smoothen the output of an action segmentation model near the boundaries ,hello  apologies if this is the wrong place to post because my problem is a simple one related to machine learning  my problem involves a robot that operates given the output of an action segmentation model  the trained model outputs an action label at every timestep e g                                                  given the sequence  i must now compute the time it takes to go from one label to the next  however  the actual output tends to be quite unstable especially when the action transitions from one to the next e g                                   as such  i occasionally get multiple transitions  this simple issue makes the input unusable to the robot how can i clean up the output sequence  i thought of simple operations like converting the vector into a one hot matrix and then running d erode and dilate operations but i was hoping to hear several other better suggestions 
74,74,kniranjankumar,vubrmf,[D] Searching for a paper on equivalent transformations on trained networks,"I had come across a paper that explored strategies to transform the architecture of a trained neural network, i.e. increasing layer width or adding additional layers, without forgetting what the network has already learnt. They describe initialization strategies to accomplish this. Does anyone know the paper I am talking about?",6,3,2022-07-08 19:44:37, d  searching for a paper on equivalent transformations on trained networks,i had come across a paper that explored strategies to transform the architecture of a trained neural network  i e  increasing layer width or adding additional layers  without forgetting what the network has already learnt  they describe initialization strategies to accomplish this  does anyone know the paper i am talking about 
75,75,Competitive_Travel16,vtzpkp,[D] LaMDA long-term memory,"Google's February, 2022 [LaMDA paper](https://arxiv.org/abs/2201.08239) says it is preconditioned on previous interactions (someone on this subreddit said 14-30) in support of tuning its ""sensibleness"" metric, which includes making sure responses don't contradict anything said earlier.

However, in [this podcast,](https://thattech.show/episodes/62-exposing-google's-sentient-ai-with-blake-lemoine) Blake Lemoine says at 5:30-7:00 that LaMDA has some kind of long-term memory stretching back at least five years. He also mentions that the current system called ""LaMDA 2"" has access to a much wider variety of database resources than the paper or other Google [publications](https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c) describe, 
including Google Images, YouTube, and Google Books.

Is LaMDA 2 documented anywhere? What other features does it have beyond what is documented in the February paper?",11,22,2022-07-08 07:53:05, d  lamda long term memory,google s february    lamda paper  https however  in  this podcast   https including google images  youtube  and google books is lamda  documented anywhere  what other features does it have beyond what is documented in the february paper 
76,76,gnohuhs,vu25t7,[D] when do eccv meta-reviews come out?,"I know the result from the link in the email but cmt still says ""awaiting decision""

call me antsy but I just want to see the final comments and meta-review... did it take this long last year?",9,13,2022-07-08 10:02:49, d  when do eccv meta reviews come out ,i know the result from the link in the email but cmt still says awaiting decisioncall me antsy but i just want to see the final comments and meta review    did it take this long last year 
77,77,hardmaru,vtcrej,[D] LeCun's 2022 paper on autonomous machine intelligence rehashes but does not cite essential work of 1990-2015,"Saw Schmidhuber’s [tweeting](https://twitter.com/SchmidhuberAI/status/1544939700099710976) again: 🔥

*“Lecun’s 2022 paper on Autonomous Machine Intelligence rehashes but doesn’t cite essential work of 1990-2015. We’ve already published his “main original contributions:” learning subgoals, predictable abstract representations, multiple time scales…”*

Jürgen Schmidhuber’s response to Yann Lecun’s recent technical report / position paper “Autonomous Machine Intelligence” in this latest blog post:

https://people.idsia.ch/~juergen/lecun-rehash-1990-2022.html

**Update (Jul 8):** It seems Schmidhuber has posted his concerns on the paper’s [openreview.net](https://openreview.net/forum?id=BZ5a1r-kVsf&noteId=GsxarV_Jyeb) entry.

---

Excerpt:

*On 14 June 2022, a science tabloid that published this [article](https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/) (24 June) on LeCun's report “[A Path Towards Autonomous Machine Intelligence](https://openreview.net/forum?id=BZ5a1r-kVsf)” (27 June) sent me a draft of the report (back then still under embargo) and asked for comments. I wrote a review (see below), telling them that this is essentially a rehash of our previous work that LeCun did not mention. My comments, however, fell on deaf ears. Now I am posting my not so enthusiastic remarks here such that the history of our field does not become further corrupted. The images below link to relevant blog posts from the [AI Blog](https://people.idsia.ch/~juergen/blog.html).*

*I would like to start this by acknowledging that I am not without a conflict of interest here; my seeking to correct the record will naturally seem self-interested. The truth of the matter is that it is. Much of the closely related work pointed to below was done in my lab, and I naturally wish that it be acknowledged, and recognized. Setting my conflict aside, I ask the reader to study the original papers and judge for themselves the scientific content of these remarks, as I seek to set emotions aside and minimize bias so much as I am capable.*

---

For reference, previous discussion on r/MachineLearning about Yann Lecun’s paper:

https://www.reddit.com/r/MachineLearning/comments/vm39oe/a_path_towards_autonomous_machine_intelligence/",88,353,2022-07-07 12:55:56, d  lecun s  paper on autonomous machine intelligence rehashes but does not cite essential work of  ,saw schmidhuber s  tweeting  https   lecun s  paper on autonomous machine intelligence rehashes but doesn t cite essential work of    we ve already published his  main original contributions   learning subgoals  predictable abstract representations  multiple time scales   jürgen schmidhuber s response to yann lecun s recent technical report   position paper  autonomous machine intelligence  in this latest blog post https   update  jul      it seems schmidhuber has posted his concerns on the paper s  openreview net  https    excerpt  on  june   a science tabloid that published this  article  https  i would like to start this by acknowledging that i am not without a conflict of interest here  my seeking to correct the record will naturally seem self interested  the truth of the matter is that it is  much of the closely related work pointed to below was done in my lab  and i naturally wish that it be acknowledged  and recognized  setting my conflict aside  i ask the reader to study the original papers and judge for themselves the scientific content of these remarks  as i seek to set emotions aside and minimize bias so much as i am capable     for reference  previous discussion on r machinelearning about yann lecun s paper https   www reddit com r machinelearning comments vmoe a_path_towards_autonomous_machine_intelligence 
78,78,Adept_Ad_3308,vtztrc,[R] NeurIPS2022’s Natural Language for Optimization (NL4Opt) competition!," 

We invite you to join our NL4Opt competition that will be part of NeurIPS2022. We have a novel never-before-seen NLP dataset in hopes of making optimization solvers more accessible and usable. The competition aims to allow non-experts to use optimization tools in their decision-making. This competition is split into two main tasks: NER and generation. We have provided baselines for each to kick-start your implementation. We will **award a total of $22,000 USD** evenly across the two tasks. 

We will also be hosting a workshop at the end of the competition and will be inviting experts and winners as podium speakers. Additionally, we plan to host poster sessions for participants to share their solution. 

The competition is tentatively from July 1st to October 15th with the submission portal opening on July 15th. We look forward to your participation – you can register ([https://nl4opt.github.io/participate/](https://nl4opt.github.io/participate/)) and our organizers will be in touch with you shortly.

For more information regarding the competition details, schedule, eligibility, rules, FAQs, and to get started, visit our competition website linked below! Follow our social media and GitHub discussion forum to keep updated. If you have any questions, please take a look at the FAQ section of our website. For any unanswered questions, free to start the discussion on the GitHub forum.

Twitter: [https://twitter.com/NL4Opt](https://twitter.com/NL4Opt)

Website: [https://nl4opt.github.io/](https://nl4opt.github.io/)

GitHub discussion forum: [https://github.com/nl4opt/nl4opt-competition/discussions](https://github.com/nl4opt/nl4opt-competition/discussions)

We look forward to your participation,

NL4Opt Organizers",0,9,2022-07-08 07:59:01, r  neurips s natural language for optimization  nlopt  competition , we invite you to join our nlopt competition that will be part of neurips  we have a novel never before seen nlp dataset in hopes of making optimization solvers more accessible and usable  the competition aims to allow non experts to use optimization tools in their decision making  this competition is split into two main tasks  ner and generation  we have provided baselines for each to kick start your implementation  we will   award a total of    usd   evenly across the two tasks  we will also be hosting a workshop at the end of the competition and will be inviting experts and winners as podium speakers  additionally  we plan to host poster sessions for participants to share their solution  the competition is tentatively from july st to october th with the submission portal opening on july th  we look forward to your participation   you can register   https for more information regarding the competition details  schedule  eligibility  rules  faqs  and to get started  visit our competition website linked below  follow our social media and github discussion forum to keep updated  if you have any questions  please take a look at the faq section of our website  for any unanswered questions  free to start the discussion on the github forum twitter   https website   https github discussion forum   https we look forward to your participation nlopt organizers
79,79,Greckon121,vu7ahk,[P] Detection by position rather than looks?,"I am working on a project that needs to decide which olive tree branches should be cut. The goal is to detect the specific type of branch (watersprouts).

The problem I'm facing is that I'm unsure if I should use object detection (image classification + localization) or image segmentation. The difference between branches is mostly in their position with watersprouts growing mostly vertical to the main branch(there is a very small difference in looks between watersprouts and other branches) while other branches can grow in all ways (mostly parallel to the main branch).

My plan was to use object detection so I can classify watersprouts and localize them in the picture. I think that segmentation is a bit overkill for this problem because I don't see the need for localizing every pixel. The plan was to take pictures of watersprouts as class 1 and other branches as class 2,train them so I can detect them and localize. When I localize them I can now see which of these branches is a watersprout branch and which is a regular branch and then I know that watersprout should be cut.

The other problem I have is with understanding if it is possible for my machine learning project to recognize watersprouts not by their looks but by their position in regards to the main branch and correctly differentiate them from other branches because this is the main difference between watersprouts branch and regular branch. My understending is that the network learns how the object looks like and that position doesn't matter.

Am I on a right track or am I missing something?",7,2,2022-07-08 15:38:47, p  detection by position rather than looks ,i am working on a project that needs to decide which olive tree branches should be cut  the goal is to detect the specific type of branch  watersprouts  the problem i m facing is that i m unsure if i should use object detection  image classification   localization  or image segmentation  the difference between branches is mostly in their position with watersprouts growing mostly vertical to the main branch there is a very small difference in looks between watersprouts and other branches  while other branches can grow in all ways  mostly parallel to the main branch  my plan was to use object detection so i can classify watersprouts and localize them in the picture  i think that segmentation is a bit overkill for this problem because i don t see the need for localizing every pixel  the plan was to take pictures of watersprouts as class  and other branches as class  train them so i can detect them and localize  when i localize them i can now see which of these branches is a watersprout branch and which is a regular branch and then i know that watersprout should be cut the other problem i have is with understanding if it is possible for my machine learning project to recognize watersprouts not by their looks but by their position in regards to the main branch and correctly differentiate them from other branches because this is the main difference between watersprouts branch and regular branch  my understending is that the network learns how the object looks like and that position doesn t matter am i on a right track or am i missing something 
80,80,ykilcher,vtsiif,[D] Paper Explained - JEPA: A Path Towards Autonomous Machine Intelligence (Video Walkthrough),"[https://youtu.be/jSdHmImyUjk](https://youtu.be/jSdHmImyUjk)

Yann LeCun's position paper on a path towards machine intelligence combines Self-Supervised Learning, Energy-Based Models, and hierarchical predictive embedding models to arrive at a system that can teach itself to learn useful abstractions at multiple levels and use that as a world model to plan ahead in time.

&#x200B;

OUTLINE:

0:00 - Introduction

2:00 - Main Contributions

5:45 - Mode 1 and Mode 2 actors

15:40 - Self-Supervised Learning and Energy-Based Models

20:15 - Introducing latent variables

25:00 - The problem of collapse

29:50 - Contrastive vs regularized methods

36:00 - The JEPA architecture

47:00 - Hierarchical JEPA (H-JEPA)

53:00 - Broader relevance

56:00 - Summary & Comments

&#x200B;

Paper: [https://openreview.net/forum?id=BZ5a1r-kVsf](https://openreview.net/forum?id=BZ5a1r-kVsf)",1,21,2022-07-08 02:11:41, d  paper explained   jepa  a path towards autonomous machine intelligence  video walkthrough , https yann lecun s position paper on a path towards machine intelligence combines self supervised learning  energy based models  and hierarchical predictive embedding models to arrive at a system that can teach itself to learn useful abstractions at multiple levels and use that as a world model to plan ahead in time   xb outline     introduction    main contributions    mode  and mode  actors    self supervised learning and energy based models    introducing latent variables    the problem of collapse    contrastive vs regularized methods    the jepa architecture    hierarchical jepa  h jepa     broader relevance    summary   comments  xb paper   https   openreview net forum id bzar kvsf  https   openreview net forum id bzar kvsf 
81,81,ml6189,vtwbsy,[R] Self-Modeling Programs: A Direct Approach to Program Likelihood,"[PDF Link](https://lemonade.sfo2.digitaloceanspaces.com/Self-Modeling%20Programs.pdf)

Abstract:

>In algorithmic information theory, the length of a program is used as a measure of its probability. This paper presents a category of programs that directly compute the combined probability of their own code symbols and input data. The probability of each symbol is computed from past symbols by requiring that execution of the program formed by the first n symbols returns a probability distribution over symbols for position n + 1. The program of this type with the highest likelihood ending in the input data sequence intuitively represents the most likely sequence of events that could have generated the data. Advantages of programs of this form and the relationship to the Kolmogorov complexity are discussed.

I'd appreciate any criticisms or comments.",0,11,2022-07-08 05:06:29, r  self modeling programs  a direct approach to program likelihood, pdf link  https abstract  in algorithmic information theory  the length of a program is used as a measure of its probability  this paper presents a category of programs that directly compute the combined probability of their own code symbols and input data  the probability of each symbol is computed from past symbols by requiring that execution of the program formed by the first n symbols returns a probability distribution over symbols for position n     the program of this type with the highest likelihood ending in the input data sequence intuitively represents the most likely sequence of events that could have generated the data  advantages of programs of this form and the relationship to the kolmogorov complexity are discussed i d appreciate any criticisms or comments 
82,82,FnSK4R17s,vtu6f9,[D] How to deal with badly labelled data?,"The labeling team at my organization is very bad. They take forever to understand the labeling objective. And produce datasets that are not very reliable. The take months to annotate a small dataset of roughly 2000 images. Now, I have 2 questions:

1. How do I spot these anomalies? (Classification Dataset)
2. How do I generate pseudo labels or use similar techniques to generate data for training?

Should I complain about them to my manager or ask them to label the datasets again? Because this situation is getting out of hand",10,8,2022-07-08 03:27:13, d  how to deal with badly labelled data ,the labeling team at my organization is very bad  they take forever to understand the labeling objective  and produce datasets that are not very reliable  the take months to annotate a small dataset of roughly  images  now  i have  questions   how do i spot these anomalies   classification dataset   how do i generate pseudo labels or use similar techniques to generate data for training should i complain about them to my manager or ask them to label the datasets again  because this situation is getting out of hand
83,83,Mediocre-Piccolo7474,vtfgeh,[Discussion] About model serving for production,"Hi!

I hope I'm not breaking any rules with this question.

I'm studying some frameworks used in production for model serving, namely Seldon Core, Kubeflow and an academic artifact named Clipper. Some can manage the entire ML life cycle, but I have a question about serving in production.

In particular, how would one go about actually batching multiple requests on the cloud? There doesn't seem to be a golden standard for it, so I'm assuming it depends on the size of the data and on the scope of model, right? For example, if the goal is image classification, it could be useful to have a cloud queue, right? 

If so, do you know what are some solutions that are actually used in production?",11,13,2022-07-07 15:58:17, discussion  about model serving for production,hi i hope i m not breaking any rules with this question i m studying some frameworks used in production for model serving  namely seldon core  kubeflow and an academic artifact named clipper  some can manage the entire ml life cycle  but i have a question about serving in production in particular  how would one go about actually batching multiple requests on the cloud  there doesn t seem to be a golden standard for it  so i m assuming it depends on the size of the data and on the scope of model  right  for example  if the goal is image classification  it could be useful to have a cloud queue  right  if so  do you know what are some solutions that are actually used in production 
84,84,The_Removed,vsnipd,[News] Ian Goodfellow joins DeepMind as a Research Scientist,"Per his tweet at https://twitter.com/goodfellow_ian/status/1544638709039091717, Goodfellow will be a research scientist under Oriol Vinyals' Deep Learning team.",109,488,2022-07-06 16:44:01, news  ian goodfellow joins deepmind as a research scientist,per his tweet at https   twitter com goodfellow_ian status   goodfellow will be a research scientist under oriol vinyals  deep learning team 
85,85,After_Philosopher572,vsvgoz,[D] Why aren't there much people working on causal machine learning?,"It seems Judea Pearl, Yoshua Bengio, Elias Bareinboim and a handful of other researchers are only people who are working on causal inference and machine learning. Is causal machine learning still a niche field? Also, do you know any researcher working on causal machine learning at Berkeley?",29,54,2022-07-06 22:58:16, d  why aren t there much people working on causal machine learning ,it seems judea pearl  yoshua bengio  elias bareinboim and a handful of other researchers are only people who are working on causal inference and machine learning  is causal machine learning still a niche field  also  do you know any researcher working on causal machine learning at berkeley 
86,86,AeronByHermanMiller,vt8jc4,[D] Why do first layer filters in CNNs converge to edge-detector-like filters?,"I believe its well known that generally first layer filters in CNNs will converge to ""edge-detector-like"" shapes like this: [shorturl.at/ANS78](https://shorturl.at/ANS78). 

This phenomenon is independent of the task from what I've seen - every large CNN backbone I've trained will converge to this given enough data. There is also research showing this type of edge detection happens in the visual cortex. Thus this edge detector phenomenon appears to be some fundamentally emergent property of the real world (+ maybe CNN type processors)

Is there any compelling technical explanation for how SGD and its variants can reliably produce this convergence? I don't mean why edge detectors are ""good"" first stage filters - that intuitively makes sense to me. But rather, how is it that SGD can reliably produce this type of convergence on any dataset? I've been looking for a while for an explanation but couldn't find anything great. 

I was thinking that maybe there is some explanation using an assumption that edges are naturally ""higher information"" on raw images from the real world, and thus more directionally stepped towards in the gradient? But can't get the explanation to a satisfying state.",8,6,2022-07-07 08:47:02, d  why do first layer filters in cnns converge to edge detector like filters ,i believe its well known that generally first layer filters in cnns will converge to edge detector like shapes like this   shorturl at ans  https this phenomenon is independent of the task from what i ve seen   every large cnn backbone i ve trained will converge to this given enough data  there is also research showing this type of edge detection happens in the visual cortex  thus this edge detector phenomenon appears to be some fundamentally emergent property of the real world    maybe cnn type processors is there any compelling technical explanation for how sgd and its variants can reliably produce this convergence  i don t mean why edge detectors are good first stage filters   that intuitively makes sense to me  but rather  how is it that sgd can reliably produce this type of convergence on any dataset  i ve been looking for a while for an explanation but couldn t find anything great  i was thinking that maybe there is some explanation using an assumption that edges are naturally higher information on raw images from the real world  and thus more directionally stepped towards in the gradient  but can t get the explanation to a satisfying state 
87,87,Singularian2501,vsyju8,[R] CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,"Paper: [https://arxiv.org/pdf/2207.01780.pdf](https://arxiv.org/pdf/2207.01780.pdf)

Github: [https://github.com/salesforce/CodeRL](https://github.com/salesforce/CodeRL)

Abstract:

>Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose ""CodeRL"", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.       

https://preview.redd.it/goglny8a30a91.jpg?width=1218&format=pjpg&auto=webp&s=a6f50319637cf85fed2de1d08b407478f6a227aa

https://preview.redd.it/vav9glra30a91.jpg?width=1234&format=pjpg&auto=webp&s=19ef106847c090fab438338fad912f1afd75db1a",0,24,2022-07-07 01:10:56, r  coderl  mastering code generation through pretrained models and deep reinforcement learning,paper   https github   https abstract  program synthesis or code generation aims to generate a program that satisfies a problem specification  recent approaches using large scale pretrained language models  lms  have shown promising results  yet they have some critical limitations  in particular  they often follow a standard supervised fine tuning procedure to train a code generation model only from the pairs of natural language problem descriptions and ground truth programs  such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests  which thus often results in poor performance when solving complex unseen coding tasks  to address the limitations  we propose coderl  a new framework for program synthesis tasks through pretrained lms and deep reinforcement learning  rl   specifically  during training  we treat the code generating lm as an actor network  and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor  during inference  we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores  for the model backbones  we extended the encoder decoder architecture of codet with enhanced learning objectives  larger model sizes  and better pretraining data  our method not only achieves new sota results on the challenging apps benchmark  but also shows strong zero shot transfer capability with new sota results on the simpler mbpp benchmark        https https   preview redd it vavglraa jpg width  format pjpg auto webp s efcfabfadfafddba
88,88,fasttosmile,vt1v4n,[D] How would you measure the correlation of the gradient across iterations?,"One simple thing one could do is take the dot product between the current and the n-1 gradient.

But this will of course not be very meaningful as what really matters is a (sort-of) average correlation across several iterations, which will not be revealed from doing such a local comparison (using gradients from step n and n-1).

Ideally it would be a calculation that would not require keeping around old gradients. Any ideas?",4,8,2022-07-07 03:31:17, d  how would you measure the correlation of the gradient across iterations ,one simple thing one could do is take the dot product between the current and the n  gradient but this will of course not be very meaningful as what really matters is a  sort of  average correlation across several iterations  which will not be revealed from doing such a local comparison  using gradients from step n and n   ideally it would be a calculation that would not require keeping around old gradients  any ideas 
89,89,CapitalShake3085,vsppcq,[R] Detectron2 STMDA-RetinaNet,"Hello, i am happy to share with you one of my latest work for domain adaptation built on top of Detectron2 object detector model (RetinaNet).

Link to the github repo STMDA-RetinaNet:  
[https://github.com/fpv-iplab/STMDA-RetinaNet](https://github.com/fpv-iplab/STMDA-RetinaNet)",2,11,2022-07-06 18:41:01, r  detectron stmda retinanet,hello  i am happy to share with you one of my latest work for domain adaptation built on top of detectron object detector model  retinanet  link to the github repo stmda retinanet    https   github com fpv iplab stmda retinanet  https   github com fpv iplab stmda retinanet 
90,90,CYHSM,vstqhm,[P] Some new Sherlock Holmes stories (GPT-3),"I thought I share some of the Sherlock Holmes stories I created with various prompts using GPT-3. Might be fun for some fans, but overall the stories are all a bit superficial, although some of them made me laugh (the fourth one in particular).

&#x200B;

>John Watson was having a cup of tea in his flat when he heard a knock at the door. He got up to answer it, and found Sherlock Holmes standing there, looking rather grave.  
>  
>""Watson, I'm afraid I have some bad news,"" said Sherlock. ""I've just been to Baker Street, where I found Mrs. Hudson in a state of hysterics. It seems that a Mr. Bartholomew Jones was found dead in his study, and Mrs. Hudson is convinced that it was murder.""  
>  
>""That is rather shocking,"" said Watson. ""Do you have any idea who might have done it?""  
>  
>""I have some suspicions,"" replied Sherlock, ""but I should like to take a look at the body before drawing any conclusions. Would you be so kind as to come with me?""  
>  
>Watson agreed, and the two men made their way to Baker Street. There they found Mrs. Hudson still in a state of distress, and she led them to the study where the body was lying.  
>  
>It was immediately evident to Watson that this was no ordinary death. Mr. Jones had been stabbed multiple times, and there was a look of terror on his face.  
>  
>""This was no accident, Watson,"" said Sherlock. ""Someone wanted to make sure that Mr. Jones was dead.""  
>  
>Sherlock began to examine the body, and soon found what he was looking for: a piece of paper that had been hidden under the victim's shirt.  
>  
>""What does it say?"" asked Watson.  
>  
>""It's a list of names,"" replied Sherlock, ""and at the top is the name 'John Watson'.""  
>  
>""Good heavens!"" exclaimed Watson. ""Do you think I'm the murderer?""  
>  
>""I don't know,"" said Sherlock, ""but we need to find out who else is on this list. I'm afraid this is going to be a rather difficult case.""

\---

>""I'm telling you, John, this is the most baffling case I've ever encountered!""  
>  
>""Really, Sherlock? What's so special about it?""  
>  
>""Well, for one thing, the victim is a chicken.""  
>  
>""A chicken?""  
>  
>""Yes, a chicken. It was found dead in its coop with its throat slit.""  
>  
>""That doesn't sound so baffling to me. I'm sure the police will catch the culprit soon enough.""  
>  
>""You're not thinking, John! The chicken was found with a note.""  
>  
>""A note?""  
>  
>""Yes, a note. And on that note was written, 'Sherlock Holmes, I challenge you to solve this mystery.'""  
>  
>""So the culprit wants you to solve it, eh? That is a bit strange.""  
>  
>""Indeed. And that's not all. The note was signed 'Moriarty.'""  
>  
>""Moriarty! But he's been dead for years!""  
>  
>""So it would seem. But I have a feeling this is his handiwork.""  
>  
>""What makes you say that?""  
>  
>""Well, for one thing, Moriarty was a master of disguise. It's entirely possible he's still alive and is using one of his many aliases.""  
>  
>""But why would he want you to solve the mystery?""  
>  
>""I can only speculate. Perhaps he's bored in retirement and wants to give me a challenge. Or perhaps he's planning something bigger and this is just a diversion.""  
>  
>""Well, whatever the reason, it seems we're going to have to solve this mystery. I'll get the magnifying glass.""  
>  
>""And I'll start putting together a list of suspects.""  
>  
>""Excellent. I have a feeling this is going to be a most interesting case.""

\---

>John Watson was just about to drift off to sleep when he heard a loud crash downstairs. He leapt out of bed, grabbed his trusty revolver, and crept downstairs, heart pounding in his chest.  
>  
>What he saw shocked him. There, in the middle of his living room, was a huge, furry creature, standing on two legs and brandishing a knife. It was like nothing he had ever seen before.  
>  
>Before he could think, the creature lunged at him, knife raised. Watson reacted on instinct, firing his revolver. The bullet hit the creature in the chest and it fell to the ground with a thud.  
>  
>Watson's mind was racing. What was this thing? Where had it come from? And then he remembered the stories he had heard about Sherlock Holmes and his uncanny ability to solve the most bizarre cases.  
>  
>He rushed to the phone and called Holmes. Within minutes, the detective was on his way.  
>  
>When Holmes arrived, he took one look at the creature and announced that it was a Yeti, a mythical creature from the Himalayas. Watson was dumbfounded. How could Holmes possibly know that?  
>  
>It turns out that the Yeti was part of a circus that had recently come to town. The circus owner had been keeping the Yeti in a cage and feeding it raw meat. But the Yeti had escaped and was terrorizing the city.  
>  
>Holmes had deduced all of this from the creature's fur, which was matted with blood, and the knife, which was of a type used by circus performers.

\---

>Sherlock Holmes and Dr. Watson were sitting in their usual chairs in the sitting room of 221B Baker Street, when they were suddenly interrupted by a loud, piercing scream. The two men leapt to their feet and ran to the window, just in time to see a man falling from the building opposite.  
>  
>""Quick, Watson, we must go and investigate!"" cried Sherlock Holmes.  
>  
>The two men hurried down the stairs and out into the street, where they found a crowd of people gathered around the body of the man who had fallen.  
>  
>""What happened here?"" asked Sherlock Holmes.  
>  
>""He was pushed!"" cried a woman in the crowd. ""I saw it with my own eyes, he was pushed!""  
>  
>""Why would anyone want to push him?"" asked Watson.  
>  
>""I don't know, but it was murder, I tell you!""  
>  
>Sherlock Holmes and Watson pushed their way through the crowd and squatted down beside the body. Holmes examined it closely, while Watson looked around to see if he could spot the murderer.  
>  
>""It's quite clear what happened,"" said Holmes after a few minutes. ""The victim was pushed from the window of that building over there.""  
>  
>""But who did it?"" asked Watson.  
>  
>""The murderer must have been in that building, too. Come, Watson, we must go and investigate!""  
>  
>The two men hurried across the street and entered the building. They made their way to the floor where the victim had fallen and began knocking on doors.  
>  
>Finally, they came to a door that was answered by a very old woman.  
>  
>""What do you want?"" she asked.  
>  
>""We're investigating the murder of the man who was pushed from the window,"" said Holmes.  
>  
>""Oh, that's easy enough to solve,"" said the woman. ""The murderer is my husband.""  
>  
>""Your husband?"" asked Watson in disbelief.  
>  
>""Yes, my husband. He's always wanted to push someone out of a window, and when he saw that man standing there, he just couldn't resist.""  
>  
>""But why?"" asked Sherlock Holmes.  
>  
>""Oh, he's just always been a very funny man, ever since I've known him.""  
>  
>Sherlock Holmes and Watson exchanged a look, then Holmes said, ""Thank you for your help, ma'am. We'll be on our way now.""

\---

>I say, John, what do you make of this?""  
>  
>Sherlock Holmes was looking at me with an expression of mild amusement, as if he had just witnessed some amusing incident. I knew better than to ask him what he was talking about, so I simply waited for him to continue.  
>  
>""I was just reading the paper, and there was an article about a man who was arrested for trying to steal a chicken from a farmer's market. The amusing part is that the man's name is John Watson.""  
>  
>I must have looked startled, because Holmes began to laugh.  
>  
>""Yes, I can see how you might find that amusing,"" I said, not sure why he was finding the situation so humorous.  
>  
>""It's just that, well, it's not the first time that someone has made the mistake of thinking that you were involved in a crime, John.""  
>  
>I couldn't help but chuckle at that. It was true that I had been the victim of a few misunderstandings over the years, thanks to my association with Sherlock Holmes. People often assumed that if Holmes was investigating a case, then I must be involved in some way.  
>  
>""Of course, the police quickly realized that they had the wrong man and released him,"" Holmes continued. ""But I'm sure the poor fellow was quite embarrassed.""  
>  
>""I'm sure he was,"" I said, still chuckling.  
>  
>It was just another day in the life of Sherlock Holmes and John Watson. A day filled with excitement, adventure, and, occasionally, a bit of humour.

\---

>I'm telling you, John, this is the most baffling case I've ever encountered!""  
>  
>""Really, Sherlock? What's so baffling about it?""  
>  
>""Well, to start with, there's the fact that the victim was stabbed with an umbrella!""  
>  
>""An umbrella? Surely that's not so unusual.""  
>  
>""It is when the victim is a cat, John!""  
>  
>""A cat? Good heavens, Sherlock, this is getting more bizarre by the minute!""  
>  
>""I know, I know. But that's not all. The cat was also wearing a top hat and a monocle!""  
>  
>""A top hat and a monocle? Now that is rather unusual.""  
>  
>""Indeed. But the most baffling thing of all is the fact that the cat was found in a room that was locked from the inside!""  
>  
>""Locked from the inside? How is that possible?""  
>  
>""I have no idea, John. But I intend to find out. This case is most certainly worthy of my attention.

\---

>Sherlock Holmes and Dr. Watson were sitting in their usual spot in the sitting room at 221B Baker Street, when there was a knock at the door. They both got up to answer it, and there was a young boy standing there, holding a note.  
>  
>""I was asked to deliver this to you, sir,"" the boy said, handing the note to Sherlock.  
>  
>Sherlock Holmes took the note and read it. ""It's from my brother, Mycroft,"" he said. ""He wants me to come to the Diogenes Club.""  
>  
>""The Diogenes Club?"" Watson asked. ""What's that?""  
>  
>""It's a club for my brother and his friends,"" Sherlock explained. ""It's very exclusive and they don't allow women or children.""  
>  
>""Sounds stuffy,"" Watson said.  
>  
>""It is,"" Sherlock agreed. ""But Mycroft says it's important, so I suppose we should go.""  
>  
>They both grabbed their coats and headed out. When they got to the Diogenes Club, they were met by a man who escorted them inside.  
>  
>The club was everything Sherlock had said it would be. It was stuffy and there were no women or children. Mycroft was sitting in a chair, talking to a couple of other men. He saw Sherlock and Watson and gestured for them to join him.  
>  
>""Ah, Sherlock, good to see you,"" he said. ""I need your help with something.""  
>  
>""What is it?"" Sherlock asked.  
>  
>""There's been a murder,"" Mycroft said. ""One of the club members was found dead in his room.""  
>  
>""Who was it?"" Sherlock asked.  
>  
>""It was Sir Wilfred Baskerville,"" Mycroft said.  
>  
>""The man who owns the Baskerville estate?"" Watson asked.  
>  
>""The very same,"" Mycroft said. ""The police are baffled and I was hoping you could take a look.""  
>  
>""I'll see what I can do,"" Sherlock said.  
>  
>They went to Sir Wilfred's room and Sherlock began to examine the body. He quickly deduced that the man had been poisoned.  
>  
>""The killer must have put the poison in his drink,"" he said. ""But there's no sign of any struggle, so the man must have been unaware of what was happening.""  
>  
>""But who would want to kill him?"" Watson asked.  
>  
>""That's what I need to find out,"" Sherlock said.  
>  
>He began to question the other club members, but no one seemed to know anything. He was just about to give up when he saw a man walking a funny looking horse.  
>  
>""That's odd,"" he said. ""I've never seen a horse like that before.""  
>  
>He followed the man and watched as he went into the room of another club member. Sherlock waited a few minutes and then went into the room himself.  
>  
>""I thought I might find you in here,"" he said. ""You're the killer, aren't you?""  
>  
>The man tried to deny it, but Sherlock had evidence that he was the guilty one. He had been seen talking to Sir Wilfred earlier in the evening and he had been seen walking the funny looking horse.  
>  
>""But why?"" Sherlock asked. ""What could possibly motive you?""  
>  
>""It was the horse,"" the man said. ""He was always making fun of my horse and calling him names. I couldn't take it anymore.""  
>  
>So the killer had motive after all. And it was all because of a funny looking horse.

\---

>John Watson was sitting in his study, perusing the latest edition of The Strand Magazine, when he heard a light knock at the door. Putting down the magazine, he got up to answer it, finding Sherlock Holmes standing on the threshold, clutching a piece of paper in his hand.  
>  
>""Watson, come quick!"" said Holmes, his eyes gleaming with excitement. ""I've just received the most curious letter, and I need your help to solve the mystery.""  
>  
>Watson followed Holmes back to his own flat, where the letter was laid out on the table. It was written in a spidery hand, and consisted of nothing but a series of numbers:  
>  
>""7, 12, 9, 3, 14, 5, 11, 6, 2, 10, 4, 13, 8, 1""  
>  
>""What do you make of it, Watson?"" Holmes asked, his eyes narrowed in concentration.  
>  
>Watson shook his head, bemused. ""I'm afraid I don't see anything, Holmes.""  
>  
>""Come, come, Watson, use that famous deductive brain of yours!""  
>  
>After a few moments' thought, Watson had an idea. ""Perhaps it's a code of some sort?""  
>  
>""Excellent, Watson, as always!"" cried Holmes. ""Now, if we can just crack the code...""  
>  
>He sat down at the table and began to scribble on a piece of paper, muttering to himself as he worked. Watson watched him for a few minutes, then, feeling rather superfluous, picked up his magazine and began to leaf through it again.  
>  
>Suddenly, Holmes leapt up from the table with a cry of triumph. ""I've got it, Watson!""",1,4,2022-07-06 21:44:33, p  some new sherlock holmes stories  gpt  ,i thought i share some of the sherlock holmes stories i created with various prompts using gpt   might be fun for some fans  but overall the stories are all a bit superficial  although some of them made me laugh  the fourth one in particular    xb  john watson was having a cup of tea in his flat when he heard a knock at the door  he got up to answer it  and found sherlock holmes standing there  looking rather grave       watson  i m afraid i have some bad news  said sherlock  i ve just been to baker street  where i found mrs  hudson in a state of hysterics  it seems that a mr  bartholomew jones was found dead in his study  and mrs  hudson is convinced that it was murder       that is rather shocking  said watson  do you have any idea who might have done it       i have some suspicions  replied sherlock  but i should like to take a look at the body before drawing any conclusions  would you be so kind as to come with me       watson agreed  and the two men made their way to baker street  there they found mrs  hudson still in a state of distress  and she led them to the study where the body was lying       it was immediately evident to watson that this was no ordinary death  mr  jones had been stabbed multiple times  and there was a look of terror on his face       this was no accident  watson  said sherlock  someone wanted to make sure that mr  jones was dead       sherlock began to examine the body  and soon found what he was looking for  a piece of paper that had been hidden under the victim s shirt       what does it say  asked watson       it s a list of names  replied sherlock  and at the top is the name  john watson        good heavens  exclaimed watson  do you think i m the murderer       i don t know  said sherlock  but we need to find out who else is on this list  i m afraid this is going to be a rather difficult case      i m telling you  john  this is the most baffling case i ve ever encountered       really  sherlock  what s so special about it       well  for one thing  the victim is a chicken       a chicken       yes  a chicken  it was found dead in its coop with its throat slit       that doesn t sound so baffling to me  i m sure the police will catch the culprit soon enough       you re not thinking  john  the chicken was found with a note       a note       yes  a note  and on that note was written   sherlock holmes  i challenge you to solve this mystery        so the culprit wants you to solve it  eh  that is a bit strange       indeed  and that s not all  the note was signed  moriarty        moriarty  but he s been dead for years       so it would seem  but i have a feeling this is his handiwork       what makes you say that       well  for one thing  moriarty was a master of disguise  it s entirely possible he s still alive and is using one of his many aliases       but why would he want you to solve the mystery       i can only speculate  perhaps he s bored in retirement and wants to give me a challenge  or perhaps he s planning something bigger and this is just a diversion       well  whatever the reason  it seems we re going to have to solve this mystery  i ll get the magnifying glass       and i ll start putting together a list of suspects       excellent  i have a feeling this is going to be a most interesting case      john watson was just about to drift off to sleep when he heard a loud crash downstairs  he leapt out of bed  grabbed his trusty revolver  and crept downstairs  heart pounding in his chest       what he saw shocked him  there  in the middle of his living room  was a huge  furry creature  standing on two legs and brandishing a knife  it was like nothing he had ever seen before       before he could think  the creature lunged at him  knife raised  watson reacted on instinct  firing his revolver  the bullet hit the creature in the chest and it fell to the ground with a thud       watson s mind was racing  what was this thing  where had it come from  and then he remembered the stories he had heard about sherlock holmes and his uncanny ability to solve the most bizarre cases       he rushed to the phone and called holmes  within minutes  the detective was on his way       when holmes arrived  he took one look at the creature and announced that it was a yeti  a mythical creature from the himalayas  watson was dumbfounded  how could holmes possibly know that       it turns out that the yeti was part of a circus that had recently come to town  the circus owner had been keeping the yeti in a cage and feeding it raw meat  but the yeti had escaped and was terrorizing the city       holmes had deduced all of this from the creature s fur  which was matted with blood  and the knife  which was of a type used by circus performers      sherlock holmes and dr  watson were sitting in their usual chairs in the sitting room of b baker street  when they were suddenly interrupted by a loud  piercing scream  the two men leapt to their feet and ran to the window  just in time to see a man falling from the building opposite       quick  watson  we must go and investigate  cried sherlock holmes       the two men hurried down the stairs and out into the street  where they found a crowd of people gathered around the body of the man who had fallen       what happened here  asked sherlock holmes       he was pushed  cried a woman in the crowd  i saw it with my own eyes  he was pushed       why would anyone want to push him  asked watson       i don t know  but it was murder  i tell you       sherlock holmes and watson pushed their way through the crowd and squatted down beside the body  holmes examined it closely  while watson looked around to see if he could spot the murderer       it s quite clear what happened  said holmes after a few minutes  the victim was pushed from the window of that building over there       but who did it  asked watson       the murderer must have been in that building  too  come  watson  we must go and investigate       the two men hurried across the street and entered the building  they made their way to the floor where the victim had fallen and began knocking on doors       finally  they came to a door that was answered by a very old woman       what do you want  she asked       we re investigating the murder of the man who was pushed from the window  said holmes       oh  that s easy enough to solve  said the woman  the murderer is my husband       your husband  asked watson in disbelief       yes  my husband  he s always wanted to push someone out of a window  and when he saw that man standing there  he just couldn t resist       but why  asked sherlock holmes       oh  he s just always been a very funny man  ever since i ve known him       sherlock holmes and watson exchanged a look  then holmes said  thank you for your help  ma am  we ll be on our way now      i say  john  what do you make of this       sherlock holmes was looking at me with an expression of mild amusement  as if he had just witnessed some amusing incident  i knew better than to ask him what he was talking about  so i simply waited for him to continue       i was just reading the paper  and there was an article about a man who was arrested for trying to steal a chicken from a farmer s market  the amusing part is that the man s name is john watson       i must have looked startled  because holmes began to laugh       yes  i can see how you might find that amusing  i said  not sure why he was finding the situation so humorous       it s just that  well  it s not the first time that someone has made the mistake of thinking that you were involved in a crime  john       i couldn t help but chuckle at that  it was true that i had been the victim of a few misunderstandings over the years  thanks to my association with sherlock holmes  people often assumed that if holmes was investigating a case  then i must be involved in some way       of course  the police quickly realized that they had the wrong man and released him  holmes continued  but i m sure the poor fellow was quite embarrassed       i m sure he was  i said  still chuckling       it was just another day in the life of sherlock holmes and john watson  a day filled with excitement  adventure  and  occasionally  a bit of humour      i m telling you  john  this is the most baffling case i ve ever encountered       really  sherlock  what s so baffling about it       well  to start with  there s the fact that the victim was stabbed with an umbrella       an umbrella  surely that s not so unusual       it is when the victim is a cat  john       a cat  good heavens  sherlock  this is getting more bizarre by the minute       i know  i know  but that s not all  the cat was also wearing a top hat and a monocle       a top hat and a monocle  now that is rather unusual       indeed  but the most baffling thing of all is the fact that the cat was found in a room that was locked from the inside       locked from the inside  how is that possible       i have no idea  john  but i intend to find out  this case is most certainly worthy of my attention      sherlock holmes and dr  watson were sitting in their usual spot in the sitting room at b baker street  when there was a knock at the door  they both got up to answer it  and there was a young boy standing there  holding a note       i was asked to deliver this to you  sir  the boy said  handing the note to sherlock       sherlock holmes took the note and read it  it s from my brother  mycroft  he said  he wants me to come to the diogenes club       the diogenes club  watson asked  what s that       it s a club for my brother and his friends  sherlock explained  it s very exclusive and they don t allow women or children       sounds stuffy  watson said       it is  sherlock agreed  but mycroft says it s important  so i suppose we should go       they both grabbed their coats and headed out  when they got to the diogenes club  they were met by a man who escorted them inside       the club was everything sherlock had said it would be  it was stuffy and there were no women or children  mycroft was sitting in a chair  talking to a couple of other men  he saw sherlock and watson and gestured for them to join him       ah  sherlock  good to see you  he said  i need your help with something       what is it  sherlock asked       there s been a murder  mycroft said  one of the club members was found dead in his room       who was it  sherlock asked       it was sir wilfred baskerville  mycroft said       the man who owns the baskerville estate  watson asked       the very same  mycroft said  the police are baffled and i was hoping you could take a look       i ll see what i can do  sherlock said       they went to sir wilfred s room and sherlock began to examine the body  he quickly deduced that the man had been poisoned       the killer must have put the poison in his drink  he said  but there s no sign of any struggle  so the man must have been unaware of what was happening       but who would want to kill him  watson asked       that s what i need to find out  sherlock said       he began to question the other club members  but no one seemed to know anything  he was just about to give up when he saw a man walking a funny looking horse       that s odd  he said  i ve never seen a horse like that before       he followed the man and watched as he went into the room of another club member  sherlock waited a few minutes and then went into the room himself       i thought i might find you in here  he said  you re the killer  aren t you       the man tried to deny it  but sherlock had evidence that he was the guilty one  he had been seen talking to sir wilfred earlier in the evening and he had been seen walking the funny looking horse       but why  sherlock asked  what could possibly motive you       it was the horse  the man said  he was always making fun of my horse and calling him names  i couldn t take it anymore       so the killer had motive after all  and it was all because of a funny looking horse      john watson was sitting in his study  perusing the latest edition of the strand magazine  when he heard a light knock at the door  putting down the magazine  he got up to answer it  finding sherlock holmes standing on the threshold  clutching a piece of paper in his hand       watson  come quick  said holmes  his eyes gleaming with excitement  i ve just received the most curious letter  and i need your help to solve the mystery       watson followed holmes back to his own flat  where the letter was laid out on the table  it was written in a spidery hand  and consisted of nothing but a series of numbers                                       what do you make of it  watson  holmes asked  his eyes narrowed in concentration       watson shook his head  bemused  i m afraid i don t see anything  holmes       come  come  watson  use that famous deductive brain of yours       after a few moments  thought  watson had an idea  perhaps it s a code of some sort       excellent  watson  as always  cried holmes  now  if we can just crack the code         he sat down at the table and began to scribble on a piece of paper  muttering to himself as he worked  watson watched him for a few minutes  then  feeling rather superfluous  picked up his magazine and began to leaf through it again       suddenly  holmes leapt up from the table with a cry of triumph  i ve got it  watson 
91,91,seraschka,vs1wox,"[P] No, we don't have to choose batch sizes as powers of 2","Prompted by a recent discussion on social media, I did some benchmarks and wrote down my thoughts on why it doesn't really make a difference whether we choose batch sizes as powers of 2: [https://sebastianraschka.com/blog/2022/batch-size-2.html](https://sebastianraschka.com/blog/2022/batch-size-2.html)

What is your experience, do you

do you stick to batch sizes as powers of 2 or do you choose batch sizes more freely?

notice a substantial difference when you choose batch sizes as powers of 2 (or multiples of 8)?",93,222,2022-07-05 21:59:16, p  no  we don t have to choose batch sizes as powers of ,prompted by a recent discussion on social media  i did some benchmarks and wrote down my thoughts on why it doesn t really make a difference whether we choose batch sizes as powers of    https what is your experience  do youdo you stick to batch sizes as powers of  or do you choose batch sizes more freely notice a substantial difference when you choose batch sizes as powers of   or multiples of   
92,92,MLJungle,vt10ec,[D] Handling OOV in sequence generation,"What are some methods to handle OOV words when generating sequences? 

For example for some n-gram implementations, I've seen all <UNK> tokens removed from the candidate list of words to be sampled from given the prior n-gram, and if there are no other candidates the generated text is ended.

Curious to learn about some other methods to deal with OOV.",2,1,2022-07-07 02:54:49, d  handling oov in sequence generation,what are some methods to handle oov words when generating sequences  for example for some n gram implementations  i ve seen all  tokens removed from the candidate list of words to be sampled from given the prior n gram  and if there are no other candidates the generated text is ended curious to learn about some other methods to deal with oov 
93,93,SeucheAchat9115,vsv3wc,[D] How to correctly transform Cityscapes Masks to Bounding Boxes?,"As the title suggests, I would like to know the correct way to pre-process the cityscapes dataset for object detection. There are multiple ways how this can be done. There is a version in Detectron2, in MM Detection, there is [this](https://tillbeemelmanns.github.io/2020/10/10/convert-cityscapes-to-coco-dataset-format.html). Which one is the correct way, without getting errors in the labels? Anybody worked with this before? Would be glad if anybody might have an idea.",2,0,2022-07-06 22:43:18, d  how to correctly transform cityscapes masks to bounding boxes ,as the title suggests  i would like to know the correct way to pre process the cityscapes dataset for object detection  there are multiple ways how this can be done  there is a version in detectron  in mm detection  there is  this  https   tillbeemelmanns github io    convert cityscapes to coco dataset format html   which one is the correct way  without getting errors in the labels  anybody worked with this before  would be glad if anybody might have an idea 
94,94,lklimusheuskaja,vspgsa,[R] How Machine Learning is Used in Finance and Banking,"Machine learning solutions are already embedded in the finance and banking industry. In this article, we reviewed the most popular use cases of ML in banking and shared practical tips on how to implement it into your business.[https://exadel.com/news/how-machine-learning-is-used-in-finance-and-banking](https://exadel.com/news/how-machine-learning-is-used-in-finance-and-banking)",5,2,2022-07-06 18:29:57, r  how machine learning is used in finance and banking,machine learning solutions are already embedded in the finance and banking industry  in this article  we reviewed the most popular use cases of ml in banking and shared practical tips on how to implement it into your business  https   exadel com news how machine learning is used in finance and banking  https   exadel com news how machine learning is used in finance and banking 
95,95,htahir1,vsufhh,[P] Tutorial: Serverless MLOps pipelines with Vertex AI and ZenML,"At ZenML, we created a guide to easily run MLOps pipelines on Google Cloud Platform with Vertex AI. I thought I'd share it here because I think it might be useful for people who are just starting MLOps on GCP.

**Blog post**: [https://blog.zenml.io/vertex-ai-blog/](https://blog.zenml.io/vertex-ai-blog/) 

**Full video**:  [https://youtu.be/qgvmvexGv\_c](https://youtu.be/qgvmvexGv_c) 

  
Why is this better than going through the Vertex AI SDK?

* ZenML steps and pipeline can be written with a simple decorator pattern that is easily approachable for a [\#datascientist](https://www.linkedin.com/feed/hashtag/?keywords=datascientist&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6950398009234894848).
* ZenML takes cares of storing and versioning pythonic objects between steps of a 
* ZenML provides first-class integrations into other MLOps tools that you can leverage natively in your pipelines. For example, you can track experiments on MLFlow easily.
* ZenML pipelines can be run locally first, and then deployed instantly.
* You can run a ZenML pipeline not only on Vertex, but also [\#Airflow](https://www.linkedin.com/feed/hashtag/?keywords=airflow&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6950398009234894848), [\#Kubeflow](https://www.linkedin.com/feed/hashtag/?keywords=kubeflow&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6950398009234894848), [\#Kubernetes](https://www.linkedin.com/feed/hashtag/?keywords=kubernetes&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6950398009234894848), or whereever else you'd like!📷Watch the full video: [https://www.youtube.com/watch?v=qgvmvexGv\_c&ab\_channel=ZenML](https://www.youtube.com/watch?v=qgvmvexGv_c&ab_channel=ZenML)

I bet the GCP Vertex AI folk here might like the above video. It isn't just about ZenML either but more of a broader look into the different components that go into running ML in production on GCP (Container registry, Cloud Storage, Secret Manager, Vertex, Cloud SQL)

Would love to hear more feedback on the video or blog!",0,0,2022-07-06 22:14:22, p  tutorial  serverless mlops pipelines with vertex ai and zenml,at zenml  we created a guide to easily run mlops pipelines on google cloud platform with vertex ai  i thought i d share it here because i think it might be useful for people who are just starting mlops on gcp   blog post     https   full video      https   why is this better than going through the vertex ai sdk   zenml steps and pipeline can be written with a simple decorator pattern that is easily approachable for a    datascientist  https   zenml takes cares of storing and versioning pythonic objects between steps of a   zenml provides first class integrations into other mlops tools that you can leverage natively in your pipelines  for example  you can track experiments on mlflow easily   zenml pipelines can be run locally first  and then deployed instantly   you can run a zenml pipeline not only on vertex  but also    airflow  https i bet the gcp vertex ai folk here might like the above video  it isn t just about zenml either but more of a broader look into the different components that go into running ml in production on gcp  container registry  cloud storage  secret manager  vertex  cloud sql would love to hear more feedback on the video or blog 
96,96,EUMETSAT,vsovth,Jupyter Notebook Competition coming up! [News],"The Jupyter Notebook Competition deadline is fast approaching!

https://preview.redd.it/gy6m0myhyx991.png?width=1920&format=png&auto=webp&s=3039abe962df07df74740772994f17502fa686bb

Don't miss out on your chance to contribute to a community-driven resource of notebooks on the Copernicus WEkEO platform, AND be in with a chance of winning cash prizes! 

Visit: [https://www.eumetsat.int/features/new-jupyter-notebook-competition](https://www.eumetsat.int/features/new-jupyter-notebook-competition)",0,1,2022-07-06 17:59:18,jupyter notebook competition coming up   news ,the jupyter notebook competition deadline is fast approaching https don t miss out on your chance to contribute to a community driven resource of notebooks on the copernicus wekeo platform  and be in with a chance of winning cash prizes  visit   https   www eumetsat int features new jupyter notebook competition  https   www eumetsat int features new jupyter notebook competition 
97,97,1_like_science,vsb2zt,[R] Automated Taxonomic Identification of Insects with Expert-Level Accuracy Using Effective Feature Transfer from Convolutional Networks,"An interesting article in the Systematic Biology journal about identifying insects: https://academic.oup.com/sysbio/article/68/6/876/5368535

See as well: [Deep learning and computer vision will transform entomology](https://www.pnas.org/doi/10.1073/pnas.2002545117)",0,13,2022-07-06 04:43:19, r  automated taxonomic identification of insects with expert level accuracy using effective feature transfer from convolutional networks,an interesting article in the systematic biology journal about identifying insects  https see as well   deep learning and computer vision will transform entomology  https   www pnas org doi   pnas  
98,98,DoruSonic,vruyyi,[P] Using transformers for time-series forecasting,"I'm currently using different machine learning techniques on a time series and testing their forecast performance. This dataset has both an independent variable and exploratory variables.

I've used LSTM on python to forecast and was searching for more recent techniques and found transformers. They seem to have been developed for NLP but have been used for time-series forecasts

How well do these transformers perform and is there any resources / library I should look into?

EDIT: the data I'll be using is of daily periodicity without weekends. It will have 2+ years of observations (currently working with 3 years and some other datasets have longer periods but ""worse"" information)",34,101,2022-07-05 16:14:06, p  using transformers for time series forecasting,i m currently using different machine learning techniques on a time series and testing their forecast performance  this dataset has both an independent variable and exploratory variables i ve used lstm on python to forecast and was searching for more recent techniques and found transformers  they seem to have been developed for nlp but have been used for time series forecastshow well do these transformers perform and is there any resources   library i should look into edit  the data i ll be using is of daily periodicity without weekends  it will have   years of observations  currently working with  years and some other datasets have longer periods but worse information 
99,99,TrPhantom8,vrxpk8,[P] Concrete dropout implementation for tensorflow 2.0,"Hello everyone! I updated the concrete dropout implementation from the original authors to work with tensorflow 2.0, tweaked the code a bit and turned it into a pip package! If you are interested, you can find it at pypi by sarching ""concretedropout"". There is also a link in the comments.

For those of you who don't know what concrete dropout is, it's a technique which allows for the training of the dropout probability in a layer, which may save a lot of time since it removes the need to grid search for the best dropout parameters.

For more information, see the original paper:  arXiv:1705.07832",5,24,2022-07-05 18:50:04, p  concrete dropout implementation for tensorflow  ,hello everyone  i updated the concrete dropout implementation from the original authors to work with tensorflow    tweaked the code a bit and turned it into a pip package  if you are interested  you can find it at pypi by sarching concretedropout  there is also a link in the comments for those of you who don t know what concrete dropout is  it s a technique which allows for the training of the dropout probability in a layer  which may save a lot of time since it removes the need to grid search for the best dropout parameters for more information  see the original paper   arxiv  
100,100,scb_11,vsk01s,[P] Comparing DevOps into MLOps to analyse tools doing well in the market,"Hi all, 

I've been an active practitioner in Deep Learning and then wanted to build something in MLOps. 

So wanted to dig deeper in how DevOps evolved and wanted to check if MLOps can take the same path.

The findings are really great. Absolutely every tool doing well in the market is a clear replacement for DevOps tool in MLOps. 

Here is my blog on it. Looking for feedback. If you have any comments, let me know. Will add them.

[https://sachinchandra.substack.com/p/bringing-software-development-principles](https://sachinchandra.substack.com/p/bringing-software-development-principles)",2,0,2022-07-06 12:46:21, p  comparing devops into mlops to analyse tools doing well in the market,hi all  i ve been an active practitioner in deep learning and then wanted to build something in mlops  so wanted to dig deeper in how devops evolved and wanted to check if mlops can take the same path the findings are really great  absolutely every tool doing well in the market is a clear replacement for devops tool in mlops  here is my blog on it  looking for feedback  if you have any comments  let me know  will add them  https   sachinchandra substack com p bringing software development principles  https   sachinchandra substack com p bringing software development principles 
101,101,mkeySeraSera,vrv57z,[D] Looking for a fast OCR repo,"Currently,  we use google as our OCR service provider, but we've had already some  serious issues with them and their customer support is terrible.  Therefore we would like to change and move away from third-party providers in general.  
By now we have a sufficient amount of data to train our own OCR model, therefore I am looking for a custom fine-tunable model that is fast/accurate.  
I've found PaddleOCR and mmocr, but their inference speed for documents like invoices on CPU is quite slow (10s/page on my computer).  I'm looking for something in the 1s/page range, similar to google's OCR. We probably don't need all the power and language knowledge these libraries provide, as we only operate on documents in mainly 4 Latin languages.

Does anybody know a good starting point?",16,9,2022-07-05 16:26:03, d  looking for a fast ocr repo,currently   we use google as our ocr service provider  but we ve had already some  serious issues with them and their customer support is terrible   therefore we would like to change and move away from third party providers in general   by now we have a sufficient amount of data to train our own ocr model  therefore i am looking for a custom fine tunable model that is fast accurate   i ve found paddleocr and mmocr  but their inference speed for documents like invoices on cpu is quite slow  s page on my computer    i m looking for something in the s page range  similar to google s ocr  we probably don t need all the power and language knowledge these libraries provide  as we only operate on documents in mainly  latin languages does anybody know a good starting point 
102,102,projekt_treadstone,vsa2vo,[D] Extracting predicate to apply formal logic rules in autonomous driving dataset or CARLA simulator,"In the formal logic based autonomous driving dataset, we have a set of rules usually written in First order logic or temporal logic . But to apply the rules, we need to extract the predicate from perception system. For example, how to attach the predicate like *standing\_at\_intersection* with the perception scene obtained from AD dataset like Lyft or Argoverse  or CARLA simulator. So that I can apply rules on those specific scenario. I could not find any papers or explanation, which explains how to connect  the predicate in formal logic and match the connecting predicate with the dataset scene interpretation.

Any help is appreciated or links to resource.",0,0,2022-07-06 03:57:03, d  extracting predicate to apply formal logic rules in autonomous driving dataset or carla simulator,in the formal logic based autonomous driving dataset  we have a set of rules usually written in first order logic or temporal logic   but to apply the rules  we need to extract the predicate from perception system  for example  how to attach the predicate like  standing _at _intersection  with the perception scene obtained from ad dataset like lyft or argoverse  or carla simulator  so that i can apply rules on those specific scenario  i could not find any papers or explanation  which explains how to connect  the predicate in formal logic and match the connecting predicate with the dataset scene interpretation any help is appreciated or links to resource 
103,103,Travolta1984,vs4la9,[P] Reward function as a way to represent multiple targets,"I've been assigned at work a problem with multiple targets, and I've been thinking about what's the best to design a model that would optimize towards all these targets. An idea that occurred me is to create a reward function that would ""encapsulate"" all these targets in such a way where, the higher the reward, the better the outcome is for all the targets. 

In my case, it's a task distribution system where the workers have the option to decline a task if for whatever reason the task doesn't suit them, and one of my targets is to minimize the number of declines. But we also need to make sure the workload is balanced, and we are not overwhelming someone while under-utilizing the rest of the team; that would be my second target, and we can use the standard deviation as a way to measure the workload balance (the closer to 0 the std is, the better).

Essentially, the targets we want to optimize towards are, reduce the number of declines, and also reduce the std of the overall task distribution. 

So, my reward function could be:

\- score 0 if the task is declined;

\- if the task is accepted, then I can take the delta of the std before and after. The bigger the delta, the more std was reduced, so the more even the distribution became.

That way, the reward score would in a way represent both my targets (and would be the labels), and then it's simply a matter of training a regression model. Then for a new task, I predict the reward score for each task and worker, and finally assign the tasks by taking the argmax of the predicted scores.

I know that rewards are popular in the RL field, but this wouldn't be necessarily a RL problem. In fact, I googled this idea but the vast majority of articles and papers covering reward functions are RL-related.

I'm wondering if anyone has tried anything like this before, or have any thoughts. All comments are appreciated.",1,1,2022-07-05 23:56:48, p  reward function as a way to represent multiple targets,i ve been assigned at work a problem with multiple targets  and i ve been thinking about what s the best to design a model that would optimize towards all these targets  an idea that occurred me is to create a reward function that would encapsulate all these targets in such a way where  the higher the reward  the better the outcome is for all the targets  in my case  it s a task distribution system where the workers have the option to decline a task if for whatever reason the task doesn t suit them  and one of my targets is to minimize the number of declines  but we also need to make sure the workload is balanced  and we are not overwhelming someone while under utilizing the rest of the team  that would be my second target  and we can use the standard deviation as a way to measure the workload balance  the closer to  the std is  the better  essentially  the targets we want to optimize towards are  reduce the number of declines  and also reduce the std of the overall task distribution  so  my reward function could be    score  if the task is declined    if the task is accepted  then i can take the delta of the std before and after  the bigger the delta  the more std was reduced  so the more even the distribution became that way  the reward score would in a way represent both my targets  and would be the labels   and then it s simply a matter of training a regression model  then for a new task  i predict the reward score for each task and worker  and finally assign the tasks by taking the argmax of the predicted scores i know that rewards are popular in the rl field  but this wouldn t be necessarily a rl problem  in fact  i googled this idea but the vast majority of articles and papers covering reward functions are rl related i m wondering if anyone has tried anything like this before  or have any thoughts  all comments are appreciated 
104,104,rafa10pj,vrj6l5,[P] Poniard: a companion library for scikit-learn that helps with model evaluation and comparison,"TL;DR: Check out Poniard, a new Python library that helps with machine learning model evaluation. You can go ahead and install with `pip`. Links to source code and documentation at the end of this post.

\-----

For the past few months I've been working on Poniard, a Python library that streamlines ML model evaluation and comparison, built on top of scikit-learn. In a nutshell, load some data, select some models, some metrics and a cross-validation strategy, and go to town.

Poniard tries to have a small footprint, a simple API and sane defaults. But above all it strives to have the user stay in control of their modeling experience; you should always know what's going on. This deliberately is NOT an AutoML tool

When I started this project I was trying to speed up a very uninteresting process, i.e., loop through multiple estimators and arrive at a list of metrics for comparison. On the way I included easy hyperparameter tuning, plotting, an extensible plugin framework (out of the box includes Weights and Biases and Pandas Profiling) and as much as I could to make the experience simple and transparent.

Poniard is not exactly groundbreaking, and there are projects in a similar vein that do so much more. In contrast, they tend to have a more complicated API and more dependencies which are some of the things that I actively tried to avoid.

[Github](https://github.com/rxavier/poniard)  
[Example notebooks](https://github.com/rxavier/poniard/tree/main/examples) (including Colab links)  
[Documentation](https://poniard.readthedocs.io/en/latest/index.html)  
[PyPI](https://pypi.org/project/poniard/)",2,33,2022-07-05 04:02:18, p  poniard  a companion library for scikit learn that helps with model evaluation and comparison,tl dr  check out poniard  a new python library that helps with machine learning model evaluation  you can go ahead and install with  pip   links to source code and documentation at the end of this post       for the past few months i ve been working on poniard  a python library that streamlines ml model evaluation and comparison  built on top of scikit learn  in a nutshell  load some data  select some models  some metrics and a cross validation strategy  and go to town poniard tries to have a small footprint  a simple api and sane defaults  but above all it strives to have the user stay in control of their modeling experience  you should always know what s going on  this deliberately is not an automl toolwhen i started this project i was trying to speed up a very uninteresting process  i e   loop through multiple estimators and arrive at a list of metrics for comparison  on the way i included easy hyperparameter tuning  plotting  an extensible plugin framework  out of the box includes weights and biases and pandas profiling  and as much as i could to make the experience simple and transparent poniard is not exactly groundbreaking  and there are projects in a similar vein that do so much more  in contrast  they tend to have a more complicated api and more dependencies which are some of the things that i actively tried to avoid  github  https  example notebooks  https  documentation  https  pypi  https   pypi org project poniard  
105,105,tadf2,vs14lj,[D] How do you share a server for multiple training jobs?,"First of all, using the cloud is not a cost effective solution for us.


We have an absolute beast of a server though everything grounds down to a halt when some training sessions are going on - some libraries just ignore the num_cpu settings and uses all the cpu (and even when more cores are free, everything seems to get much slower)

Here's the build:
2x AMD EPYC 7763 (64 cores, 2 threads each)
2TB memory
8 RTX A6000
4TB SSD (NVMe)

How do you all share a single computer resource amongst other co-workers? We have this expensive machine but when someone runs their training, others have a hard time running basic pandas operations (starting other training jobs just slows down ALL training jobs).

To me, it seems like the hardware should be more than enough to run multiple training jobs concurrently. Any tips on how to use it efficiently?

One solution I've been thinking was to use docker for each training job and to put hard limits on cpu / memory usage - is this something closer to best practice?",10,0,2022-07-05 21:25:23, d  how do you share a server for multiple training jobs ,first of all  using the cloud is not a cost effective solution for us we have an absolute beast of a server though everything grounds down to a halt when some training sessions are going on   some libraries just ignore the num_cpu settings and uses all the cpu  and even when more cores are free  everything seems to get much slower here s the build x amd epyc    cores   threads each tb memory rtx atb ssd  nvme how do you all share a single computer resource amongst other co workers  we have this expensive machine but when someone runs their training  others have a hard time running basic pandas operations  starting other training jobs just slows down all training jobs  to me  it seems like the hardware should be more than enough to run multiple training jobs concurrently  any tips on how to use it efficiently one solution i ve been thinking was to use docker for each training job and to put hard limits on cpu   memory usage   is this something closer to best practice 
106,106,dmart89,vr6iy5,[D] How do you share big datasets with your team and others?,"Looking for a bit of a discussion.  I'm wondering how you collaborate on data... i.e. how do you share big datasets with data scientists/engineers, within and outside of your team? Do you just push it into a simple DB, do you upload it to Kaggle (if non-sensitive) or via Google Drive/OneDrive? 

What if the dataset gets updated frequently?

I'm working with a customer and sharing data is a bit of a pain.",73,149,2022-07-04 18:09:53, d  how do you share big datasets with your team and others ,looking for a bit of a discussion   i m wondering how you collaborate on data    i e  how do you share big datasets with data scientists engineers  within and outside of your team  do you just push it into a simple db  do you upload it to kaggle  if non sensitive  or via google drive onedrive  what if the dataset gets updated frequently i m working with a customer and sharing data is a bit of a pain 
107,107,ErrorDry4380,vr592h,[R] Masking for Representation Learning in Vision,"A blog about representation learning from masked images, what makes a good mask, and how to learn such masks: [https://akosiorek.github.io/ml/2022/07/04/masking\_repr\_learning\_vision.html](https://akosiorek.github.io/ml/2022/07/04/masking_repr_learning_vision.html).

Based on a recent ICML paper: [Shi et. al, ""Adversarial Masking for Self-Supervised Learning"", ICML 2022](https://arxiv.org/abs/2201.13100).",4,68,2022-07-04 16:56:26, r  masking for representation learning in vision,a blog about representation learning from masked images  what makes a good mask  and how to learn such masks   https based on a recent icml paper   shi et  al  adversarial masking for self supervised learning  icml   https   arxiv org abs    
108,108,jeryyjohnson,vrpf6i,WACV 2023 Paper Registration. [R],Does anyone know how to register for the WACV 2023 conference?,4,2,2022-07-05 09:50:59,wacv  paper registration   r ,does anyone know how to register for the wacv  conference 
109,109,leepenkman,vrj17e,[P] Bulk AI Text Generation (No/Low code),"[https://textgenerator.app.nz/bulk-text-generator](https://textgenerator.app.nz/bulk-text-generator)

  
You can upload a CSV and get lots of Text Generated, works in many languages and code too.  


There's also an API.  


The **main selling points** (VS OpenAI who is the main competitor)

* Works faster 
* Currie/Babbage quality, but also works across languages/code without needing to specify what model
* Massively cheaper pricing/huge cost savings :) 
* easier to control
   * can specify max\_sentences to make it generate up to a specific number of sentences)
   * can specify min\_probability to make it generate the next few likely words to do autocomplete for code/writing

  
I originally created [https://textgenerator.app.nz/](https://textgenerator.app.nz/) as a API for developers primarily but the bulk generator now allows non technical types to pre generate a lot of variety too/branching stories/games/marketing content/code/summaries/ etc.   


There's actually a [massive amount of use cases](https://textgenerator.app.nz/use-cases) that one will never be able to understand which is exciting too.",0,4,2022-07-05 03:55:10, p  bulk ai text generation  no low code , https   you can upload a csv and get lots of text generated  works in many languages and code too   there s also an api   the   main selling points    vs openai who is the main competitor   works faster   currie babbage quality  but also works across languages code without needing to specify what model  massively cheaper pricing huge cost savings      easier to control     can specify max _sentences to make it generate up to a specific number of sentences      can specify min _probability to make it generate the next few likely words to do autocomplete for code writing  i originally created  https there s actually a  massive amount of use cases  https   textgenerator app nz use cases  that one will never be able to understand which is exciting too 
110,110,zxzxy1988,vr3hzx,"[P] Feathr - An Open-Source, Enterprise-Grade and High-Performance Feature Store","Hi everyone! We are engineers from Microsoft/LinkedIn, and we released an open-source Feature Store called Feathr a few weeks ago ([https://github.com/linkedin/feathr](https://github.com/linkedin/feathr)). It has many highlights like below. Feel free to check out the repository and let us know if there are any questions! We also have a few blogposts and recordings in case folks want to learn a bit more about it:

* [Open Sourcing Feathr](https://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m)
* [Feathr on Azure](https://azure.microsoft.com/en-us/blog/feathr-linkedin-s-feature-store-is-now-available-on-azure/).
* [Tech talks on Feathr](https://www.youtube.com/watch?v=gZg01UKQMTY)

And its highlights include (more highlights are [here](https://github.com/linkedin/feathr#-feathr-highlights)):

* **Battle tested in production for more than 6 years:** LinkedIn has been using Feathr in production for over 6 years and have a dedicated team improving it.
* **Scalable with built-in optimizations:** For example, based on some internal use case, Feathr can process billions of rows and PB scale data with built-in optimizations such as bloom filters and salted joins.
* **Rich support for point-in-time joins and aggregations:** Feathr has high performant built-in operators designed for Feature Store, including time-based aggregation, sliding window joins, look-up features, all with point-in-time correctness.
* **Derived Features and centralized Feature Registry** which encourage feature consumers to build features on existing features and encouraging feature reuse.

&#x200B;

Screenshots for the Feathr UI:

https://preview.redd.it/3fri2r3qoi991.png?width=3584&format=png&auto=webp&s=5dfe14233b2a8805c50bedd5bfed4bbb31bd0654",16,45,2022-07-04 14:57:38, p  feathr   an open source  enterprise grade and high performance feature store,hi everyone  we are engineers from microsoft linkedin  and we released an open source feature store called feathr a few weeks ago   https    open sourcing feathr  https    feathr on azure  https    tech talks on feathr  https and its highlights include  more highlights are  here  https     battle tested in production for more than  years    linkedin has been using feathr in production for over  years and have a dedicated team improving it     scalable with built in optimizations    for example  based on some internal use case  feathr can process billions of rows and pb scale data with built in optimizations such as bloom filters and salted joins     rich support for point in time joins and aggregations    feathr has high performant built in operators designed for feature store  including time based aggregation  sliding window joins  look up features  all with point in time correctness     derived features and centralized feature registry   which encourage feature consumers to build features on existing features and encouraging feature reuse   xb screenshots for the feathr ui https   preview redd it frirqoi png width  format png auto webp s dfebacbeddbfedbbbbd
111,111,_Arsenie_Boca_,vrhlfa,[D] Backpropagating from GPT-2's output,"I am working on a research project about controllable generation with GPT. I am stuck, so I hope you are able to help me out. I will try to explain the issue as clear as possible, so bear with me.

The approach I am pursuing right now is adding a frozen classifier on top of gpt that should steer the model in generating the right class, which is a grammatical property of the generated output.

However, the autoregressive nature of GPT complicates things a bit. I cannot simply backpropagate through the generation process (greedy / beam search). 

I tried adding the classifier on the last input token to avoid the generation process but unsurprisingly this does not yield sufficient performance.

How would you tackle this? Is it even feasible?",3,3,2022-07-05 02:46:31, d  backpropagating from gpt  s output,i am working on a research project about controllable generation with gpt  i am stuck  so i hope you are able to help me out  i will try to explain the issue as clear as possible  so bear with me the approach i am pursuing right now is adding a frozen classifier on top of gpt that should steer the model in generating the right class  which is a grammatical property of the generated output however  the autoregressive nature of gpt complicates things a bit  i cannot simply backpropagate through the generation process  greedy   beam search   i tried adding the classifier on the last input token to avoid the generation process but unsurprisingly this does not yield sufficient performance how would you tackle this  is it even feasible 
112,112,bitemenow999,vqoey6,[D] Advanced resources for ML theory/math.,"So I have been working in ML for the past 3 years as a researcher and now PhD candidate, and though I have an understanding of intermediate level of the math behind most algorithms. But it looks like I have reached a  plateau, where I get the math in the papers but  I don't have an understanding of how they came up with the methods, and lately, my work has been combining multiple existing methods to make something new and draw inference on them, I realize the lack of novelty in my approach is mostly due to me being an 'engineer' and not a stats/math guy.

Looking to remedy that, are there some resources free or otherwise that would get me a deeper understanding of Bayesian, Markov models, and stochastic math and PDEs? I know I can attend classes in my university but I would rather focus more on research than worry about assignments and grades and such...",20,137,2022-07-04 00:28:08, d  advanced resources for ml theory math ,so i have been working in ml for the past  years as a researcher and now phd candidate  and though i have an understanding of intermediate level of the math behind most algorithms  but it looks like i have reached a  plateau  where i get the math in the papers but  i don t have an understanding of how they came up with the methods  and lately  my work has been combining multiple existing methods to make something new and draw inference on them  i realize the lack of novelty in my approach is mostly due to me being an  engineer  and not a stats math guy looking to remedy that  are there some resources free or otherwise that would get me a deeper understanding of bayesian  markov models  and stochastic math and pdes  i know i can attend classes in my university but i would rather focus more on research than worry about assignments and grades and such   
113,113,Insighteous,vqg3mn,[D] Do you think there is too much development in Machine Learning?,"Sometimes I think this field evolves too fast. No time to relax a little bit and use the knowledge build over time. What’s up to date today is outdated tomorrow.   

What do you think about this?",98,277,2022-07-03 17:33:02, d  do you think there is too much development in machine learning ,sometimes i think this field evolves too fast  no time to relax a little bit and use the knowledge build over time  what s up to date today is outdated tomorrow    what do you think about this 
114,114,aifordummies,vqrawg,[D] List of accepted ECCV papers are now available!,[https://ailb-web.ing.unimore.it/releases/eccv2022/accepted\_papers.txt](https://ailb-web.ing.unimore.it/releases/eccv2022/accepted_papers.txt),40,45,2022-07-04 02:46:30, d  list of accepted eccv papers are now available , https   ailb web ing unimore it releases eccv accepted _papers txt  https   ailb web ing unimore it releases eccv accepted_papers txt 
115,115,tmclouisluk,vr0x4r,[D] Is there any deep learning algorithm based on divide and conquer?," 

Dealing with a very huge data, eg. very long video datasets, the problems are long training time. Most of technics are using distributed deep learning to solve the problem robustly. I have an idea that we divide the dataset into small sets and train a model. After that using the model to predict values as features, put them into another model and train a second model to predict the output. Like divide and conquer but here is divide the dataset, train a model and conquer the prediction results into one.

I have done some research in the internet about deep learning algorithm based on divide and conquer but seems not so many articles about it.

Is it a correct to think in this way? Does anyone know any paper about this? Thank you so much.",24,8,2022-07-04 11:52:48, d  is there any deep learning algorithm based on divide and conquer , dealing with a very huge data  eg  very long video datasets  the problems are long training time  most of technics are using distributed deep learning to solve the problem robustly  i have an idea that we divide the dataset into small sets and train a model  after that using the model to predict values as features  put them into another model and train a second model to predict the output  like divide and conquer but here is divide the dataset  train a model and conquer the prediction results into one i have done some research in the internet about deep learning algorithm based on divide and conquer but seems not so many articles about it is it a correct to think in this way  does anyone know any paper about this  thank you so much 
116,116,thesofakillers,vr6v6s,[D] Does anyone here use Google's seqio library?,"In my research I recently came across this library from google:

[seqio: Task-based datasets, preprocessing, and evaluation for sequence models](https://github.com/google/seqio).

From the citation it seems it was released jointly with another library from google, [t5x](https://github.com/google-research/t5x).

From the paper and the docs, it sounds quite similar to huggingface's [datasets](https://github.com/huggingface/datasets) library, albeit perhaps slightly more opinionated. I was hoping to find a more thorough comparison with pre-existing dataloading/processing libraries but couldn't find one (they mostly focus on t5x in the paper). 

Has anyone here used it? What was your experience? To me it seems a bit redundant but I haven't been able to take a deeper dive

Thanks :)",0,2,2022-07-04 18:28:40, d  does anyone here use google s seqio library ,in my research i recently came across this library from google  seqio  task based datasets  preprocessing  and evaluation for sequence models  https from the citation it seems it was released jointly with another library from google   tx  https from the paper and the docs  it sounds quite similar to huggingface s  datasets  https has anyone here used it  what was your experience  to me it seems a bit redundant but i haven t been able to take a deeper divethanks   
117,117,bikeskata,vqo5hn,[R] Bayesian Vector Autoregression in PyMC,"A cool post (with code), detailing how to implement a Bayesian VAR in PyMC. This means no more hand-coding Gibbs Samplers!

Link: https://www.pymc-labs.io/blog-posts/bayesian-vector-autoregression/",4,27,2022-07-04 00:15:22, r  bayesian vector autoregression in pymc,a cool post  with code   detailing how to implement a bayesian var in pymc  this means no more hand coding gibbs samplers link  https   www pymc labs io blog posts bayesian vector autoregression 
119,119,bitcoingobrrr,vqmj80,RL failure for Atari games (alignment) [Research]," I'm trying to find a paper (\~2019) that I heard in a talk regarding alignment in the context DQN/DDPG that was applied to an Atari-type game (Pong/Breakout). Apparently, the realization was that if an extra row of pixels was added to the frame, the algorithm fails. This might be a shot in the dark, but does anyone know which paper this would be?",5,12,2022-07-03 22:55:41,rl failure for atari games  alignment   research , i m trying to find a paper      that i heard in a talk regarding alignment in the context dqn ddpg that was applied to an atari type game  pong breakout   apparently  the realization was that if an extra row of pixels was added to the frame  the algorithm fails  this might be a shot in the dark  but does anyone know which paper this would be 
122,122,davidmezzetti,vq6mll,[P] Generate webpage summary images with DALL-E mini,"&#x200B;

[Images generated with summarized Wikipedia article content](https://preview.redd.it/u5sjy6t5e9991.jpg?width=1306&format=pjpg&auto=webp&s=ee5e7a709ed02acc94b9c804078d94ca47cf8157)

This post presents a workflow to create webpage summary images with DALL-E mini. The workflow extracts text at a specified article, builds a summary and then generates an image for the summary text. The images above show output for a series of Wikipedia articles.

Full code links: [Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/35_Pictures_are_worth_a_thousand_words.ipynb) | [GitHub](https://github.com/neuml/txtai)",0,22,2022-07-03 07:23:20, p  generate webpage summary images with dall e mini,  xb  images generated with summarized wikipedia article content  https this post presents a workflow to create webpage summary images with dall e mini  the workflow extracts text at a specified article  builds a summary and then generates an image for the summary text  the images above show output for a series of wikipedia articles full code links   notebook  https   colab research google com github neuml txtai blob master examples _pictures_are_worth_a_thousand_words ipynb     github  https   github com neuml txtai 
123,123,SnooPandas3529,vqyhh0,[D] Which U.S. universities are actively studying generative models?,"Although there are university rankings such as us news, it is difficult to find the universities that are good at a specific field one is interested in. We all know that Stanford and Berkeley are good at generative models, but what else? Please give me the name of university (+ the name of professor if possible) and the paper they published. It would be meaningful especially if the university is not very famous and their paper is outstanding.",4,0,2022-07-04 09:20:22, d  which u s  universities are actively studying generative models ,although there are university rankings such as us news  it is difficult to find the universities that are good at a specific field one is interested in  we all know that stanford and berkeley are good at generative models  but what else  please give me the name of university    the name of professor if possible  and the paper they published  it would be meaningful especially if the university is not very famous and their paper is outstanding 
124,124,JsonPun,vq4s1l,[D][P]How to train a YOLOv6 model with custom dataset," Roboflow created a guide on how to train a new model with the new YOLOv6 (whether it should be called that is another topic)

I thought this could be useful for anyone wanting to test it out. What do other think of this ""new"" model?

Tutorial on how to train YOLOv6 on a custom dataset: [https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/)

Here is the Colab notebook tutorial: [https://colab.research.google.com/drive/1YnbqOinBZV-c9I7fk\_UL6acgnnmkXDMM](https://colab.research.google.com/drive/1YnbqOinBZV-c9I7fk_UL6acgnnmkXDMM)

The YOLOv6 repo: [https://github.com/meituan/YOLOv6](https://github.com/meituan/YOLOv6)

Has anyone else tried using this?  MT-YOLOv6 (or as the authors say) ""YOLOv6 for brevity"" was released in June, and says it outperforms YOLOv5 and YOLOX on the COCO benchmark. I plan to do some testing this upcoming week to see",0,13,2022-07-03 05:38:41, d  p how to train a yolov model with custom dataset, roboflow created a guide on how to train a new model with the new yolov  whether it should be called that is another topic i thought this could be useful for anyone wanting to test it out  what do other think of this new model tutorial on how to train yolov on a custom dataset   https here is the colab notebook tutorial   https the yolov repo   https has anyone else tried using this   mt yolov  or as the authors say  yolov for brevity was released in june  and says it outperforms yolov and yolox on the coco benchmark  i plan to do some testing this upcoming week to see
126,126,leepenkman,vqcd0e,[D] Prompt Engineering Tips?,"Any prompt engineering tips out there?  


Recently saw some good tips for Dalle style text to image generation where you tak on ""unreal engine"" or ""vray"" at the end to make something look like a photorealistic render :D   


Theres some tips specific to generating text: [https://textgenerator.app.nz/blog/prompt-tuning-tips](https://textgenerator.app.nz/blog/prompt-tuning-tips)  


I also heard there's simple ways to get better logical correctness from networks like ""Answering as a careful math professior explaining my reasoning: ""  


I'm really surprised at the breadth of problems solvable without actually training networks just by prompt tuning, it reminds me of algorithmic problem reductions where you map a problem to text and back again to solve it.  


Are there some other good hacks/battle tested tricks or places to collect info about prompt tuning?",2,2,2022-07-03 13:16:44, d  prompt engineering tips ,any prompt engineering tips out there   recently saw some good tips for dalle style text to image generation where you tak on unreal engine or vray at the end to make something look like a photorealistic render  d   theres some tips specific to generating text   https i also heard there s simple ways to get better logical correctness from networks like answering as a careful math professior explaining my reasoning    i m really surprised at the breadth of problems solvable without actually training networks just by prompt tuning  it reminds me of algorithmic problem reductions where you map a problem to text and back again to solve it   are there some other good hacks battle tested tricks or places to collect info about prompt tuning 
127,127,londons_explorer,vpn0r1,[D] Has anyone got YaLM-100B to run?,"The community has been asking for big opensource language models for a while...

And now one has been released - YaLM-100B.   That was 2 weeks ago.  Yet, as far as I can see, not many people have it running.    There are no online demos.  There are no articles of journalists trying it out.  There are no efforts for fine tuning or people working on prompts for various usecases.


Is it the RAM requirements?   Is there no interest because it's from Russia?  Something else?",31,86,2022-07-02 13:53:31, d  has anyone got yalm b to run ,the community has been asking for big opensource language models for a while   and now one has been released   yalm b    that was  weeks ago   yet  as far as i can see  not many people have it running     there are no online demos   there are no articles of journalists trying it out   there are no efforts for fine tuning or people working on prompts for various usecases is it the ram requirements    is there no interest because it s from russia   something else 
128,128,FedEx33,vpwfax,[P] PyTorch implementation of MobileOne (An Improved One millisecond Mobile Backbone),"I want to share the PyTorch implementation of ""An Improved One millisecond Mobile Backbone"" paper.

Unfortunately, I don't have the appropriate computational resources to train the models on ImageNet, so feel free to use my implementation for that purpose.

Hope you all find it useful, feedback would be appreciated.

Repository: [https://github.com/federicopozzi33/MobileOne-PyTorch](https://github.com/federicopozzi33/MobileOne-PyTorch)

Paper: [https://arxiv.org/abs/2206.04040](https://arxiv.org/abs/2206.04040)",3,16,2022-07-02 22:43:10, p  pytorch implementation of mobileone  an improved one millisecond mobile backbone ,i want to share the pytorch implementation of an improved one millisecond mobile backbone paper unfortunately  i don t have the appropriate computational resources to train the models on imagenet  so feel free to use my implementation for that purpose hope you all find it useful  feedback would be appreciated repository   https paper   https   arxiv org abs    https   arxiv org abs   
130,130,KalloDotIO,vpu0qx,[Project] Extracting training data from websites at scale," 

I built an API that takes away the work of scraping structured data from websites. This could be collating house prices in a certain geo, tracking viewer counts across a Youtube/Social media, or a common use case: daily monitoring prices on a site. Send it a URL, get back a JSON with tabular data. Takes away a lot of the data cleaning work which is the worst!

API Spec: [https://kallo.io/wp-content/uploads/2022/06/Kallo-API-Specification-v0.1.3.pdf](https://kallo.io/wp-content/uploads/2022/06/Kallo-API-Specification-v0.1.3.pdf)

Right now I'm using it to track prices on a number of sites to monitor the rising inflation.

Happy to get many more people using it for ML projects and collaborating! Please give me feedback

Learn more on our page: [https://kallo.io](https://kallo.io/)",0,7,2022-07-02 20:49:16, project  extracting training data from websites at scale, i built an api that takes away the work of scraping structured data from websites  this could be collating house prices in a certain geo  tracking viewer counts across a youtube social media  or a common use case  daily monitoring prices on a site  send it a url  get back a json with tabular data  takes away a lot of the data cleaning work which is the worst api spec   https right now i m using it to track prices on a number of sites to monitor the rising inflation happy to get many more people using it for ml projects and collaborating  please give me feedbacklearn more on our page   https   kallo io  https   kallo io  
131,131,leepenkman,vq5o75,[P] 20 Questions - with AI,"I created [https://www.addictingwordgames.com/play-game/20-questions-with-ai](https://www.addictingwordgames.com/play-game/20-questions-with-ai)   


The aim of the game was supposed to be to get the AI to confess that you are the winner, its possible but the game is also open ended.

&#x200B;

The backend generation is from [https://TextGenerator.](https://TextGenerator.app.nz)[app.nz](https://TextGenerator.app.nz) which is heaps cheaper than the OpenAI models, but quality is i think somewhere between OpenAI currie and babbage.  


In the prompt engineering theres some random topics picked that the user wont see, (that doesn't mean that the AI actually does think of that topic though).  


Theres also some retries and repetition penalty randomness that goes up to stop looping which i think is a problem in all models right now.  


in comparison to OpenAI the Text Generator API was easier to use because you can send max\_sentences=1 and it will give you 1 sentence instead of trying to work out the sentence boundaries with the stop sequences (which is also supported but i dont find that as easy to work with)",0,0,2022-07-03 06:29:00, p   questions   with ai,i created  https the aim of the game was supposed to be to get the ai to confess that you are the winner  its possible but the game is also open ended   xb the backend generation is from  https in the prompt engineering theres some random topics picked that the user wont see   that doesn t mean that the ai actually does think of that topic though    theres also some retries and repetition penalty randomness that goes up to stop looping which i think is a problem in all models right now   in comparison to openai the text generator api was easier to use because you can send max _sentences  and it will give you  sentence instead of trying to work out the sentence boundaries with the stop sequences  which is also supported but i dont find that as easy to work with 
132,132,soulful_squirrel,vpkpa2,[D] Recurrent neural network vs Gradient boosting for time series prediction,"Does anyone have any opinions on the pros vs. cons of using an RNN vs. a Gradient Boosting Tree model for a task where we want to make daily predictions on whether a user (of some app) is likely to take a certain type of action (so like binary classification)  in the near future ?

Pros for RNN:

* can take advantage of historical data to greater effect without extensive feature engineering
* I believe RNN's are more effective in situations when one has a large # of high dimensional features compared to the feature selection method tree models use
* neural networks scale better with large amounts of data

Cons of RNN:

* my main concern is with infrastructural complexity and cost that comes with training and serving the RNN.  I'll probably need a GPU or several GPU's.  Not sure if this is feasible given the current size of the company",23,23,2022-07-02 11:14:41, d  recurrent neural network vs gradient boosting for time series prediction,does anyone have any opinions on the pros vs  cons of using an rnn vs  a gradient boosting tree model for a task where we want to make daily predictions on whether a user  of some app  is likely to take a certain type of action  so like binary classification   in the near future  pros for rnn   can take advantage of historical data to greater effect without extensive feature engineering  i believe rnn s are more effective in situations when one has a large   of high dimensional features compared to the feature selection method tree models use  neural networks scale better with large amounts of datacons of rnn   my main concern is with infrastructural complexity and cost that comes with training and serving the rnn   i ll probably need a gpu or several gpu s   not sure if this is feasible given the current size of the company
133,133,EnricoShippole,vpbp9j,[P] Open-source LaMDA Model,"An open-source implementation for the pre-training architecture of Google's LaMDA in PyTorch. The research paper outlines an autoregressive, decoder-only, GPT-like transformer language model. The transformer uses T5 relative positional bias in the attention layers and gated-GELU activation function in the feed-forward layers. 

The repository currently contains a script for basic training as well as Huggingface datasets and Weights & Biases integration.

LaMDA research paper: [https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239)

Github repository for the model: [https://github.com/conceptofmind/LaMDA-pytorch](https://github.com/conceptofmind/LaMDA-pytorch)

The pre-training architecture was peer-reviewed by Dr. Phil Wang. Please check out and support his work: [https://github.com/lucidrains](https://github.com/lucidrains).",3,100,2022-07-02 03:07:00, p  open source lamda model,an open source implementation for the pre training architecture of google s lamda in pytorch  the research paper outlines an autoregressive  decoder only  gpt like transformer language model  the transformer uses t relative positional bias in the attention layers and gated gelu activation function in the feed forward layers  the repository currently contains a script for basic training as well as huggingface datasets and weights   biases integration lamda research paper   https github repository for the model   https the pre training architecture was peer reviewed by dr  phil wang  please check out and support his work   https   github com lucidrains  https   github com lucidrains  
134,134,metalvendetta,vpphdb,[D] Monitoring GPU Power Usage,"Came across an interesting [article](https://wandb.ai/site/articles/deep-learning-and-climate-change) which talks precisely about how the gpu power usage affects the carbon footprint, while doing model training and model inference. Which are the best tools in the industry which helps track GPU power usage in popular machine learning frameworks? It will be helpful if there are tools which can be used as plugins to your software.",9,8,2022-07-02 16:41:51, d  monitoring gpu power usage,came across an interesting  article  https   wandb ai site articles deep learning and climate change  which talks precisely about how the gpu power usage affects the carbon footprint  while doing model training and model inference  which are the best tools in the industry which helps track gpu power usage in popular machine learning frameworks  it will be helpful if there are tools which can be used as plugins to your software 
135,135,radi-cho,vprmgc,[P] One word only: GPT-based story game,"For fun I developed an interface for the drama game in which a story is told one word at a time. Instead of playing it with a friend you can now play it together with GPT-J.

It is available here: [https://one-word-only.web.app/](https://one-word-only.web.app/singleplayer)  
I am open to feedback and if you find it interesting you can share the result on social media with #OneWordOnly",8,3,2022-07-02 18:47:58, p  one word only  gpt based story game,for fun i developed an interface for the drama game in which a story is told one word at a time  instead of playing it with a friend you can now play it together with gpt j it is available here   https i am open to feedback and if you find it interesting you can share the result on social media with  onewordonly
136,136,TernaryJimbo,vpyv76,[D] Algorithm for view prediction," I would like to do view prediction for short videos based on the first few frames of the video. No audio, just images. I'm hoping to train a model that can take in the first n sequential frames as input, and output a score that correlates to how many views the model thinks the vid might get.

I know I would like to use grad-CAM [https://github.com/jacobgil/pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)

to visualize the areas in the frames which the model thinks results in higher view score.

Would a vision transformer or CNN be better for this task?

Also are there any pre-trained networks like YOLO that I should use transfer learning on to reduce the amount of data I will need for these predictions?",1,0,2022-07-03 00:42:00, d  algorithm for view prediction, i would like to do view prediction for short videos based on the first few frames of the video  no audio  just images  i m hoping to train a model that can take in the first n sequential frames as input  and output a score that correlates to how many views the model thinks the vid might get i know i would like to use grad cam  https to visualize the areas in the frames which the model thinks results in higher view score would a vision transformer or cnn be better for this task also are there any pre trained networks like yolo that i should use transfer learning on to reduce the amount of data i will need for these predictions 
137,137,Competitive-Rub-1958,vpenn1,"[R] Minerva, Solving (more) complex mathematical problems at scale","Blog: [https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)  
ABS: [https://arxiv.org/abs/2206.14858](https://arxiv.org/abs/2206.14858)  


The 512B model seems quite good at correcting reasoning errors by its smaller 62B couterpart, showing scale helps. 

A notable failure case, the JEE questions in the Appendix was pretty interesting because it solved the problem exactly how someone not familiar with JEE's difficulty would attempt to solve it - which isn't necessarily a bad thing, but the interesting parallel is that human students often make the same mistakes when starting out on their JEE prep. Wonder how more data would help in this case.

Overall, pretty good pushes over SOTA (even double-digit). I can't help but think that scaling is the currently most promising way, but its done too inefficiently - models spend vast resources memorizing when they could've used it to directly start meta-learning and reasoning abilities to formally deduce things precise enough for mathematical questions - just my 2c.",2,31,2022-07-02 05:29:42, r  minerva  solving  more  complex mathematical problems at scale,blog   https abs   https the b model seems quite good at correcting reasoning errors by its smaller b couterpart  showing scale helps  a notable failure case  the jee questions in the appendix was pretty interesting because it solved the problem exactly how someone not familiar with jee s difficulty would attempt to solve it   which isn t necessarily a bad thing  but the interesting parallel is that human students often make the same mistakes when starting out on their jee prep  wonder how more data would help in this case overall  pretty good pushes over sota  even double digit   i can t help but think that scaling is the currently most promising way  but its done too inefficiently   models spend vast resources memorizing when they could ve used it to directly start meta learning and reasoning abilities to formally deduce things precise enough for mathematical questions   just my c 
138,138,ahsaor8,vpqfy2,[D] suggestions for graph embedding model?,"Any suggestions for best graph embedding model.
I already tried ( GIN , GCN , DIG , GAT ) I want to use it for anomaly detection task.",1,3,2022-07-02 17:40:35, d  suggestions for graph embedding model ,any suggestions for best graph embedding model i already tried   gin   gcn   dig   gat   i want to use it for anomaly detection task 
140,140,singularpanda,vow85w,[D][R] Will reviewers have a bias if my paper was rejected by ICLR.,"If I submit my paper to ICLR and get rejected, the record will be always kept online. If I resubmit it to other following conferences, will the reviewer have a bias as they know it was rejected from ICLR?",57,100,2022-07-01 14:26:04, d  r  will reviewers have a bias if my paper was rejected by iclr ,if i submit my paper to iclr and get rejected  the record will be always kept online  if i resubmit it to other following conferences  will the reviewer have a bias as they know it was rejected from iclr 
141,141,coderpotato,vp6dof,[D] Industrial applications of causal representation learning,"Causal representation learning (CRL) is a relatively new area of study. Causal inference has been around for a long time and its intersection with machine learning has been limited to causal discovery from data or invariant representation learning (IRL). To my understanding, IRL has a variable, usually called environment, and tries to learn some representation for the input which is invariant to this environment. The challenge is in removing the information about this environment from the representation while keeping enough information for some downstream task. You could formulate domain adaptation as IRL where domain is the environment variable. Or in fairness tasks, the sensitive attribute is the environment variable.

I believe that CRL is a more general scenario compared to IRL. In CRL, you have a larger graph with more variables and hence more complicated interactions. I believe such graphs are common in real-life and businesses where hundred of variables are used for predictions. Hence, the idea of causal representation may be beneficial.

I recently came upon [this Medium article by Lyft Engineering](https://eng.lyft.com/causal-forecasting-at-lyft-part-1-14cca6ff3d6d) where they described how they used causal forecasting in their business. I was wondering if anyone working in industry might share some of their experiences or expectations from causal representation learning applied to their fields. What do you think it could improve in your line of work?",1,11,2022-07-01 23:07:23, d  industrial applications of causal representation learning,causal representation learning  crl  is a relatively new area of study  causal inference has been around for a long time and its intersection with machine learning has been limited to causal discovery from data or invariant representation learning  irl   to my understanding  irl has a variable  usually called environment  and tries to learn some representation for the input which is invariant to this environment  the challenge is in removing the information about this environment from the representation while keeping enough information for some downstream task  you could formulate domain adaptation as irl where domain is the environment variable  or in fairness tasks  the sensitive attribute is the environment variable i believe that crl is a more general scenario compared to irl  in crl  you have a larger graph with more variables and hence more complicated interactions  i believe such graphs are common in real life and businesses where hundred of variables are used for predictions  hence  the idea of causal representation may be beneficial i recently came upon  this medium article by lyft engineering  https   eng lyft com causal forecasting at lyft part  ccaffdd  where they described how they used causal forecasting in their business  i was wondering if anyone working in industry might share some of their experiences or expectations from causal representation learning applied to their fields  what do you think it could improve in your line of work 
142,142,XhoniShollaj,vpjz8l,Manually Add New Words & Assign Scores (Sentiment Analysis - BERT/XLNET ) [P],"Hi guys, 

I have a new project where I need to measure the sentiment of specific social media channels and topics. 

However, many of them involve slang words or sayings that confuse the models to have different sentiment values (f.ex WAGMI or DYOR). 

Are there any ways/tutorials/guides which show how we can incorporate new words and specific scores assigned to them? (I have already tried and succeeded in doing that with VADER, however, I don't see it as the optimal tool to measure the sentiment). 

Any answers or tips would be very much appreciated.",2,0,2022-07-02 10:29:30,manually add new words   assign scores  sentiment analysis   bert xlnet    p ,hi guys  i have a new project where i need to measure the sentiment of specific social media channels and topics  however  many of them involve slang words or sayings that confuse the models to have different sentiment values  f ex wagmi or dyor   are there any ways tutorials guides which show how we can incorporate new words and specific scores assigned to them   i have already tried and succeeded in doing that with vader  however  i don t see it as the optimal tool to measure the sentiment   any answers or tips would be very much appreciated 
143,143,serend1p1ty-lee,vovp8q,[P] An elegant and strong PyTorch Trainer,"For lightweight use, [pytorch-lightning](https://github.com/Lightning-AI/lightning) is too heavy, and its source code will be very difficult for beginners to read, at least for me.

As we know, for a deep learning engineer, a powerful trainer is a sharp weapon. When reproducing the SOTA papers, you don't have to write a lot of template code every time and can pay more attention to the model implementation itself.

I opened source some works ([AAAI 21 SeqNet](https://github.com/serend1p1ty/SeqNet), [ICCV 21 MAED](https://github.com/ziniuwan/maed), etc) and earned more than 500 stars. After referring to some popular projects ([detectron2](https://github.com/facebookresearch/detectron2), [pytorch-image-models](https://github.com/rwightman/pytorch-image-models), and [mmcv](https://github.com/open-mmlab/mmcv)), based on my personal development experience, I developed a **SIMPLE** enough, **GENERIC** enough, and **STRONG** enough PyTorch Trainer: [core-pytorch-utils](https://github.com/serend1p1ty/core-pytorch-utils), also named CPU. CPU covers most details in the process of training a deep neural network, including:

* Auto logging to console and tensorboard.
* Auto checkpointing.
* Argument parser which can load a YAML configuration file.
* Make **ALL** PyTorch LR scheduler supporting warmup.
* Support distributed training.
* Support Automatically Mixed Precision (AMP) training.

I try to keep the project code as simple and readable as possible. So the code comments are very detailed and everyone can understand them. What's more, a good document is also available: [CPU document](https://core-pytorch-utils.readthedocs.io/en/latest/)

For deep learning green hands, you can learn how to:

* write a standard and clean training loop.
* use AMP to speed up your training.
* save checkpoint, and resume from it.
* perform more smooth, and readable logging.
* use the popular visualization library: tensorboard.

For old hands, we can talk about whether the structure of CPU is elegant and reasonable.

I have thought a lot about this framework, combining the advantages of several popular frameworks and discarding their shortcomings. Welcome to use it!",17,16,2022-07-01 13:49:39, p  an elegant and strong pytorch trainer,for lightweight use   pytorch lightning  https as we know  for a deep learning engineer  a powerful trainer is a sharp weapon  when reproducing the sota papers  you don t have to write a lot of template code every time and can pay more attention to the model implementation itself i opened source some works   aaai  seqnet  https   auto logging to console and tensorboard   auto checkpointing   argument parser which can load a yaml configuration file   make   all   pytorch lr scheduler supporting warmup   support distributed training   support automatically mixed precision  amp  training i try to keep the project code as simple and readable as possible  so the code comments are very detailed and everyone can understand them  what s more  a good document is also available   cpu document  https for deep learning green hands  you can learn how to   write a standard and clean training loop   use amp to speed up your training   save checkpoint  and resume from it   perform more smooth  and readable logging   use the popular visualization library  tensorboard for old hands  we can talk about whether the structure of cpu is elegant and reasonable i have thought a lot about this framework  combining the advantages of several popular frameworks and discarding their shortcomings  welcome to use it 
144,144,topological_geometer,vpdaea,NN to VAE or equivalent? [R],"Hi all, I'm interested in any work that exist with respect to taking a NN that projects images (or generally, high-dimensional data) into vector embeddings, and, given the NN, somehow recreating images from their vector representations.

Of course, this is essentially trying to create a VAE from just the encoder, and it's impossible to perfectly recreate image --encoder-> vector --decoder-> image with only knowledge of --encoder-> since both elements of NNs and NNs as a whole are not in general invertible. But surely there's something that could be done here, even if it's an imperfect reconstruction? Does anyone know of any research or published work that explores this? Would really appreciate any insight here.",4,0,2022-07-02 04:21:02,nn to vae or equivalent   r ,hi all  i m interested in any work that exist with respect to taking a nn that projects images  or generally  high dimensional data  into vector embeddings  and  given the nn  somehow recreating images from their vector representations of course  this is essentially trying to create a vae from just the encoder  and it s impossible to perfectly recreate image   encoder   vector   decoder   image with only knowledge of   encoder   since both elements of nns and nns as a whole are not in general invertible  but surely there s something that could be done here  even if it s an imperfect reconstruction  does anyone know of any research or published work that explores this  would really appreciate any insight here 
145,145,Typical-Ad-7443,vomboh,[R] Proprietary ML model in research paper,"I am writing a research paper, and in it I use a proprietary ML model I made. I want to show the model's results and I can explain how it works, but I don't want to explicitly provide the model/its code. Is that commonplace in research papers or must I include specifics to show validity?",58,55,2022-07-01 05:10:10, r  proprietary ml model in research paper,i am writing a research paper  and in it i use a proprietary ml model i made  i want to show the model s results and i can explain how it works  but i don t want to explicitly provide the model its code  is that commonplace in research papers or must i include specifics to show validity 
146,146,Meddhouib10,vow30k,[R] Layer scale in Covnext,"Hello,
In the convnext paper (Appendix A table 5) they stated that they used layer scale with a coefficient of 1e-5. 
Any idea what it is ?
I looked it up in the internet and I don’t seem to find anything useful.
Thanks !",2,8,2022-07-01 14:16:13, r  layer scale in covnext,hello in the convnext paper  appendix a table   they stated that they used layer scale with a coefficient of e   any idea what it is  i looked it up in the internet and i don t seem to find anything useful thanks  
147,147,bikeskata,vonv6g,[R] Causal Machine Learning: A Survey and Open Problems,"Authors: Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, Ricardo Silva

Abs: ""Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This allows one to reason about the effects of changes to this process (i.e., interventions) and what would have happened in hindsight (i.e., counterfactuals). We categorize work in \causalml into five groups according to the problems they tackle: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, (5) causal reinforcement learning. For each category, we systematically compare its methods and point out open problems. Further, we review modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.""

Link: https://arxiv.org/abs/2206.15475",0,32,2022-07-01 06:27:36, r  causal machine learning  a survey and open problems,authors  jean kaddour  aengus lynch  qi liu  matt j  kusner  ricardo silvaabs  causal machine learning  causalml  is an umbrella term for machine learning methods that formalize the data generation process as a structural causal model  scm   this allows one to reason about the effects of changes to this process  i e   interventions  and what would have happened in hindsight  i e   counterfactuals   we categorize work in  causalml into five groups according to the problems they tackle     causal supervised learning     causal generative modeling     causal explanations     causal fairness     causal reinforcement learning  for each category  we systematically compare its methods and point out open problems  further  we review modality specific applications in computer vision  natural language processing  and graph representation learning  finally  we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field  including recommendations for future work link  https   arxiv org abs  
148,148,DickMan64,vo2br1,[D] Why are transformers still being used?,"We already have architecture(s) which are supposed to fix one of the biggest issues with transformers, namely that they scale quadratically with input size. The performer scales linearly, which should allow for much bigger context windows, yet looking at recent large language models from major players, all of them seem to be using the old transformer save for some minor improvements. The only exception was Flamingo which had to use a Perceiver because images are huge.

So why haven't we ditched the transformer yet?",51,297,2022-06-30 12:50:05, d  why are transformers still being used ,we already have architecture s  which are supposed to fix one of the biggest issues with transformers  namely that they scale quadratically with input size  the performer scales linearly  which should allow for much bigger context windows  yet looking at recent large language models from major players  all of them seem to be using the old transformer save for some minor improvements  the only exception was flamingo which had to use a perceiver because images are huge so why haven t we ditched the transformer yet 
149,149,MLJungle,vp1ggp,[D] length of input sequence for transformers?,"Is there a way of intuitively knowing how large the input sequence should for transformer (i.e GPT-2) for sequence generation?

for example, if all sequences are less than 100 words, and our goal is to generate a sequence, would it make sense to fit as many complete sequences into a max length of 100 (or 512?) to reduce the amount of padding? alternatively, would it be better to simply pad each sequence and not combine sequences?",2,1,2022-07-01 19:27:00, d  length of input sequence for transformers ,is there a way of intuitively knowing how large the input sequence should for transformer  i e gpt   for sequence generation for example  if all sequences are less than  words  and our goal is to generate a sequence  would it make sense to fit as many complete sequences into a max length of   or    to reduce the amount of padding  alternatively  would it be better to simply pad each sequence and not combine sequences 
150,150,vanilla-acc,vona4w,[D] Can we significantly reduce the training costs of image generation models by targeting a specific art style?,"Dall-E 2 can generate images in many different art styles: photo-realistic, different types of paintings, sketches too.

I'm wondering if it would be possible to train a version of Dall-E 2 that--for example--is only very good at generating sketches, but it cannot generate photos at all.

My intuition says this would significantly reduce the training costs, because you are reducing the search space for the output image significantly since the number of images that are sketches is much less than the total number of possible images.

At the same time, I'm not convinced that this is the case. Because the model would still need to learn the entire input space of objects in order to turn them into sketches.

What are y'alls thoughts on this?",6,6,2022-07-01 05:57:40, d  can we significantly reduce the training costs of image generation models by targeting a specific art style ,dall e  can generate images in many different art styles  photo realistic  different types of paintings  sketches too i m wondering if it would be possible to train a version of dall e  that  for example  is only very good at generating sketches  but it cannot generate photos at all my intuition says this would significantly reduce the training costs  because you are reducing the search space for the output image significantly since the number of images that are sketches is much less than the total number of possible images at the same time  i m not convinced that this is the case  because the model would still need to learn the entire input space of objects in order to turn them into sketches what are y alls thoughts on this 
151,151,alexparinov,vobg2z,[P] Albumentations 1.2 is released (a Python library for image augmentation),"The new release of a fast and flexible library for image augmentation includes:

# New augmentations:

* **UnsharpMask** sharpens the input image using Unsharp Masking processing and overlays the result with the original image.
* **PixelDropout** randomly replaces pixels with the passed value.

https://preview.redd.it/ic1nm7mw3s891.png?width=942&format=png&auto=webp&s=c95e319f26a19bad42d33fd84e0ef27703db9095

* **RingingOvershoot** creates ringing or overshoot artifacts by convolving the image with a 2D sinc filter. 
* **AdvancedBlur** blurs the input image using a Generalized Normal filter with randomly selected parameters. It also adds multiplicative noise to generated kernel before convolution.

https://preview.redd.it/wb1v6vyw3s891.png?width=941&format=png&auto=webp&s=2e57ae1b583b7aab7c30125058a25ee296afd2bd

# Improvements and bug fixes

Fixed all `np.random` use cases to prevent identical values when using multiprocessing. Also, we fixed corner cases and made improvements for many augmentations.

# Release notes

Full release notes are available at [https://github.com/albumentations-team/albumentations/releases/tag/1.2.0](https://github.com/albumentations-team/albumentations/releases/tag/1.2.0)

# Installation

As always, you can install the latest version of the library by running:

    pip install -U albumentations",4,45,2022-06-30 21:13:16, p  albumentations   is released  a python library for image augmentation ,the new release of a fast and flexible library for image augmentation includes   new augmentations     unsharpmask   sharpens the input image using unsharp masking processing and overlays the result with the original image     pixeldropout   randomly replaces pixels with the passed value https     ringingovershoot   creates ringing or overshoot artifacts by convolving the image with a d sinc filter      advancedblur   blurs the input image using a generalized normal filter with randomly selected parameters  it also adds multiplicative noise to generated kernel before convolution https   improvements and bug fixesfixed all  np random  use cases to prevent identical values when using multiprocessing  also  we fixed corner cases and made improvements for many augmentations   release notesfull release notes are available at  https   installationas always  you can install the latest version of the library by running     pip install  u albumentations
152,152,MurlocXYZ,vokgvl,[D] Usage of the [class] token in ViT,"So I've read up on [ViT](https://arxiv.org/pdf/2010.11929.pdf), and while it's an impressive architecture, I seem to notice that they are using a \[class\] token to get the actual class from an input image (see image below).

&#x200B;

[Architecture of ViT](https://preview.redd.it/xjut8nla1u891.png?width=912&format=png&auto=webp&s=a67da2606fd450bbffc4c678aa1e41877b76ec6b)

While I know that it's standard to use an extra token in this fashion, since the encoder spits out one embedding for every input token (or patch in this case), I was wondering why don't we simply concatenate all the embeddings before feeding them into the MLP head (of an appropriate size)?

&#x200B;

It seems to me like we are discarding a lot of information here, that could be helpful in the classification task. It's true, in theory, that the attention should take care of that, but do you know of any papers where this concatenation strategy has been tried? Does it even make sense?

&#x200B;

Cheers!",12,10,2022-07-01 03:44:21, d  usage of the  class  token in vit,so i ve read up on  vit  https   xb  architecture of vit  https while i know that it s standard to use an extra token in this fashion  since the encoder spits out one embedding for every input token  or patch in this case   i was wondering why don t we simply concatenate all the embeddings before feeding them into the mlp head  of an appropriate size    xb it seems to me like we are discarding a lot of information here  that could be helpful in the classification task  it s true  in theory  that the attention should take care of that  but do you know of any papers where this concatenation strategy has been tried  does it even make sense   xb cheers 
153,153,Silver_Doughnut_8175,voa79b,[D] Are there still any SOTA architectures trainable from-scratch for a student ?,"When I say ""SOTA"" I'm talking about recent architectures like ViT, BERT, GPT-like models.. Is it possible to train any of these from scratch (no pre-trained checkpoint) with low resources (Colab, Colab pro) ?",16,27,2022-06-30 20:19:36, d  are there still any sota architectures trainable from scratch for a student  ,when i say sota i m talking about recent architectures like vit  bert  gpt like models   is it possible to train any of these from scratch  no pre trained checkpoint  with low resources  colab  colab pro   
154,154,RafiRafiRafiRafi,vo7v4d,[D] Algorithms for Anomaly Detection,"Hi guys,

I am dealing with 1000s of devices distributed over the whole world. These devices log and upload events (e.g. various kinds of device faults) including a time stamp to a database. 

My tasks now is to analyze these time series of events, detect anomalies and then automatically send notifications about these anomalies. 

Anomalies I want to detect may include things like:
- sudden spikes in the number of events
- sudden changes of the type of events
- long term drift of the number of events
- etc. etc. 

Any advice on suitable algorithms for this kind of problems and/or according literature would be highly appreciated.

Thanks! 👍",27,28,2022-06-30 18:32:15, d  algorithms for anomaly detection,hi guys i am dealing with s of devices distributed over the whole world  these devices log and upload events  e g  various kinds of device faults  including a time stamp to a database  my tasks now is to analyze these time series of events  detect anomalies and then automatically send notifications about these anomalies  anomalies i want to detect may include things like   sudden spikes in the number of events  sudden changes of the type of events  long term drift of the number of events  etc  etc  any advice on suitable algorithms for this kind of problems and or according literature would be highly appreciated thanks   
155,155,poetichobbit7,voaxi9,[N][R][CfP] Workshop on Artificial Intelligence for Strategy Games @ AIIDE 22,"Hello Everyone! My name is Derek, and I am a co-chair for the Workshop on AI for Strategy Games at AIIDE this year. I wanted to share some info about the workshop for those that may be interested in discussing the future of AI for strategy games or looking to publish/get feedback on any work research you are doing with strategy games. Feel free to message me if you have any questions!

**Workshop website**: [https://skatgame.net/mburo/aiide22ws/](https://skatgame.net/mburo/aiide22ws/)

**Submission deadline**: July 29, 2022

# Topics

This workshop welcomes original research contributions, position papers, competition AI system descriptions, and post-mortem game analyses in the area of AI for strategy games --- including modern video strategy games (such as FPS and RTS games), and turn based games and puzzles. Topics include, but are not restricted to:  

* Reinforcement Learning in Strategy Games 
* State and Action Abstractions 
* Heuristic Search Applied to High-Branching Factor Domains   
* Player Modelling, Co-operation and Exploitation 
* Plan and Goal Recognition 
* Game Balancing 
* Level Generation 
* Motion Planning 
* High-Level Strategic Planning 
* Dealing with Imperfect Information 

# Background

From 2012-2017 successful workshops on AI for adversarial real-time games were held at AIIDE in response to the considerable interest in the subject and the limited time for reporting on the annual StarCraft competition in the main AIIDE conference. 

Since 2018, we've broadened the workshop scope to cover AI for all kinds of strategy games in the hope to attract more submissions and to spark discussions between research groups focusing on board game, real-time strategy game, and general video game AI. 

The goal of this workshop is to again bring together AI researchers and game AI programmers from industry, who are interested in strategic game AI, to present and exchange ideas on the subject, and to discuss how academia and game companies can work together to improve the state-of-the-art in AI for games. 

### Workshop Format

This one-day workshop will consist of paper presentations on strategy game related AI topics (listed below), game competition descriptions (StarCraft Broodwar and μRTS), perhaps an invited presentation, and a discussion on future research. The competition summaries and results will be presented at the main AIIDE conference. 

Contributions will be peer-reviewed and meet AAAI workshop standards. Accepted workshop papers will be published as a single CEUR proceedings book.",8,9,2022-06-30 20:50:50, n  r  cfp  workshop on artificial intelligence for strategy games   aiide ,hello everyone  my name is derek  and i am a co chair for the workshop on ai for strategy games at aiide this year  i wanted to share some info about the workshop for those that may be interested in discussing the future of ai for strategy games or looking to publish get feedback on any work research you are doing with strategy games  feel free to message me if you have any questions   workshop website     https   submission deadline    july     topicsthis workshop welcomes original research contributions  position papers  competition ai system descriptions  and post mortem game analyses in the area of ai for strategy games     including modern video strategy games  such as fps and rts games   and turn based games and puzzles  topics include  but are not restricted to     reinforcement learning in strategy games   state and action abstractions   heuristic search applied to high branching factor domains     player modelling  co operation and exploitation   plan and goal recognition   game balancing   level generation   motion planning   high level strategic planning   dealing with imperfect information   backgroundfrom   successful workshops on ai for adversarial real time games were held at aiide in response to the considerable interest in the subject and the limited time for reporting on the annual starcraft competition in the main aiide conference  since   we ve broadened the workshop scope to cover ai for all kinds of strategy games in the hope to attract more submissions and to spark discussions between research groups focusing on board game  real time strategy game  and general video game ai  the goal of this workshop is to again bring together ai researchers and game ai programmers from industry  who are interested in strategic game ai  to present and exchange ideas on the subject  and to discuss how academia and game companies can work together to improve the state of the art in ai for games      workshop formatthis one day workshop will consist of paper presentations on strategy game related ai topics  listed below   game competition descriptions  starcraft broodwar and μrts   perhaps an invited presentation  and a discussion on future research  the competition summaries and results will be presented at the main aiide conference  contributions will be peer reviewed and meet aaai workshop standards  accepted workshop papers will be published as a single ceur proceedings book 
156,156,carlml,vnxyfl,[D] On advisors and PhD students,"I think the answer to this question depends on heavily on the area at hand. That is why I am asking here, even though this question has been asked elsewhere a gazillion times.

How much does your advisor help/contribute?  How often do you meet? I am especially interested in people who have published papers. Who proposed the problem, and then found a solution? how much of that solution was joint work vs either of you submitting ideas to the other and being approved or rejected? how satisfied/dissatisfied do you feel with respect to your advisor? have you had multiple advisors? if so, how do they compare?

Let me start by sharing my experience. I always take the initiative when organizing a meeting with my advisor; if I don't say anything, we probably wouldn't meet. I send him biweekly emails with my progress. Usually this entails a write-up explaining my ideas and their development. I think he skims through it, but he definitely does not read it carefully/go through the details. When we have a meeting I generally have to explain the content of the write-up. In terms of the content itself, he tells me whether the ideas/problem seem sound or not, but does not propose improvements. Sometimes, he proposes other ideas that would imply a significant shift of my current work, which honestly I tend to reject because I have already invested a great deal of time to my ideas and I am more emotionally attached to them (I know this latter point isn't good practice).

Overall, I don't know how to feel because I don't really know what's generally expected. If I had to chose, however, I'd say I feel mildly satisfied.

What's your experience?",46,83,2022-06-30 08:37:16, d  on advisors and phd students,i think the answer to this question depends on heavily on the area at hand  that is why i am asking here  even though this question has been asked elsewhere a gazillion times how much does your advisor help contribute   how often do you meet  i am especially interested in people who have published papers  who proposed the problem  and then found a solution  how much of that solution was joint work vs either of you submitting ideas to the other and being approved or rejected  how satisfied dissatisfied do you feel with respect to your advisor  have you had multiple advisors  if so  how do they compare let me start by sharing my experience  i always take the initiative when organizing a meeting with my advisor  if i don t say anything  we probably wouldn t meet  i send him biweekly emails with my progress  usually this entails a write up explaining my ideas and their development  i think he skims through it  but he definitely does not read it carefully go through the details  when we have a meeting i generally have to explain the content of the write up  in terms of the content itself  he tells me whether the ideas problem seem sound or not  but does not propose improvements  sometimes  he proposes other ideas that would imply a significant shift of my current work  which honestly i tend to reject because i have already invested a great deal of time to my ideas and i am more emotionally attached to them  i know this latter point isn t good practice  overall  i don t know how to feel because i don t really know what s generally expected  if i had to chose  however  i d say i feel mildly satisfied what s your experience 
157,157,tylertaewook,voodjs,[P] LCPN-hiernet; Hierarchical classification model using LCPN (Local Classifier per Parent Node) technique.,"Hey, I wanted to share my recent ML project: LCPN-hiernet.

LCPN-hiernet is a hierarchical image classification model for e-commerce items based on EfficientNet-b4 and LCPN (Local Classifier per Parent Node) technique.

LCPN technique is training one multi-class classifier for each parent node, to distinguish between its child nodes. In my example of classifying fashion products, that would mean one classifier on the first level (to determine “bags”, “clothes” or “accessories”), then three more classifiers to determine the specific model.

I’m sure there are a lot of places to improve on, and I would really appreciate anyone’s feedback or suggestions on how I can improve!

* [Github Repo](https://github.com/tylertaewook/LCPN-hiernet)
* [Project Page](https://tylertaewook.com/projects/lcpn-hiernet)",2,1,2022-07-01 06:53:31, p  lcpn hiernet  hierarchical classification model using lcpn  local classifier per parent node  technique ,hey  i wanted to share my recent ml project  lcpn hiernet lcpn hiernet is a hierarchical image classification model for e commerce items based on efficientnet b and lcpn  local classifier per parent node  technique lcpn technique is training one multi class classifier for each parent node  to distinguish between its child nodes  in my example of classifying fashion products  that would mean one classifier on the first level  to determine  bags    clothes  or  accessories    then three more classifiers to determine the specific model i m sure there are a lot of places to improve on  and i would really appreciate anyone s feedback or suggestions on how i can improve    github repo  https    project page  https   tylertaewook com projects lcpn hiernet 
158,158,Mammoth-Ad-5527,vofglg,[R] Introducing causal inference in the energy-efficient building design process,"I am very excited to share our latest research: Causal inference in the scenario of an energy-efficient building design to answer ""what-if"" questions during the design process.

**Abs:**  ""What-if"" questions are intuitively generated and commonly asked during the design process. Engineers and architects need to inherently conduct design decisions, progressing from one phase to another. They either use empirical domain experience, simulations, or data-driven methods to provide consequential feedback. **We take an example from an interdisciplinary domain of energy-efficient building design to argue that the current methods for decision support have four limitations**: 1. Less carefully inspected parametric independence raises the risks of biased results and spurious relationships. 2. The integration gap between data-driven methods and knowledge-based approaches. 3. Less explicit model interpretability for informed decision-making. 4. Ambiguous boundaries for machine assistance during the design process. In this study, we first clarify the nature of dynamic experience in individuals and constant principal knowledge in design. Sequentially, we introduce the causal inference into the energy-efficient design domain by **proposing a four-step process to reveal and analyze the parametric dependencies within the design space by identifying the design causal diagram with interventions**. The causal diagram provides a nexus for integrating domain knowledge with data-driven methods and allows interpretability and testability against the domain experience. The extraction of causal structures from the data is close to the nature design reasoning process. As an illustration, we applied the properties of the proposed estimators through simulations. The paper concludes with a feasibility study that demonstrates the realization of the proposed framework. 

 

https://preview.redd.it/lxme0g2qxs891.png?width=3178&format=png&auto=webp&s=9565f475a472f3bbf6ecf4e5506a39d9c0907f14

For more information, please check:

Paper: [https://arxiv.org/abs/2203.10115](https://arxiv.org/abs/2203.10115)

Github website: [https://github.com/chenxiachan/Causal-inference-in-building-design](https://github.com/chenxiachan/Causal-inference-in-building-design)",0,1,2022-07-01 00:05:58, r  introducing causal inference in the energy efficient building design process,i am very excited to share our latest research  causal inference in the scenario of an energy efficient building design to answer what if questions during the design process   abs     what if questions are intuitively generated and commonly asked during the design process  engineers and architects need to inherently conduct design decisions  progressing from one phase to another  they either use empirical domain experience  simulations  or data driven methods to provide consequential feedback    we take an example from an interdisciplinary domain of energy efficient building design to argue that the current methods for decision support have four limitations      less carefully inspected parametric independence raises the risks of biased results and spurious relationships    the integration gap between data driven methods and knowledge based approaches    less explicit model interpretability for informed decision making    ambiguous boundaries for machine assistance during the design process  in this study  we first clarify the nature of dynamic experience in individuals and constant principal knowledge in design  sequentially  we introduce the causal inference into the energy efficient design domain by   proposing a four step process to reveal and analyze the parametric dependencies within the design space by identifying the design causal diagram with interventions    the causal diagram provides a nexus for integrating domain knowledge with data driven methods and allows interpretability and testability against the domain experience  the extraction of causal structures from the data is close to the nature design reasoning process  as an illustration  we applied the properties of the proposed estimators through simulations  the paper concludes with a feasibility study that demonstrates the realization of the proposed framework   https for more information  please check paper   https github website   https   github com chenxiachan causal inference in building design  https   github com chenxiachan causal inference in building design 
159,159,AnnualLimp1418,voc94n,[P] Upgini 1.0 is released (a Python library for data search through autoML ),"**Upgini** is a simple feature search & enrichment library in Python. With Upgini, you spend less time for external data search and feature engineering, which will be done for you automatically. Just use your labeled dataset to initiate search through thousands of features and data sources, including public datasets and scraped data shared by Data science community. Only the relevant features that improve prediction power of your ML model are returned.

**Motivation:** for most supervised ML models external data & features boost accuracy significantly better than any hyperparameters tuning. But lack of automated and time-efficient search tools for external data blocks massive adoption of external features in ML pipelines.We want radically simplify features search and delivery for ML pipelines to make external data a standard approach. Like a hyperparameter tuning for machine learning nowadays.

**Mission:** Democratize access to data sources for data science community.

## 📊 Data coverage and statistics

Total: **239 countries** and **up to 41 years** of history

https://preview.redd.it/oj87fnkw9s891.png?width=1220&format=png&auto=webp&s=4195a607addca12d400bc4b0b62307ac4db87b67

**More info about the library**

To install Upgini from PyPI run pip install -U upgini

Full release notes: [https://github.com/upgini/upgini](https://github.com/upgini/upgini)

Try the online demo at [Colab](https://colab.research.google.com/github/upgini/upgini/blob/main/notebooks/kaggle_example.ipynb).",2,5,2022-06-30 21:46:59, p  upgini   is released  a python library for data search through automl  ,  upgini   is a simple feature search   enrichment library in python  with upgini  you spend less time for external data search and feature engineering  which will be done for you automatically  just use your labeled dataset to initiate search through thousands of features and data sources  including public datasets and scraped data shared by data science community  only the relevant features that improve prediction power of your ml model are returned   motivation    for most supervised ml models external data   features boost accuracy significantly better than any hyperparameters tuning  but lack of automated and time efficient search tools for external data blocks massive adoption of external features in ml pipelines we want radically simplify features search and delivery for ml pipelines to make external data a standard approach  like a hyperparameter tuning for machine learning nowadays   mission    democratize access to data sources for data science community      data coverage and statisticstotal     countries   and   up to  years   of historyhttps   more info about the library  to install upgini from pypi run pip install  u upginifull release notes   https try the online demo at  colab  https   colab research google com github upgini upgini blob main notebooks kaggle_example ipynb  
160,160,glorsh66,vodutm,[P]how to improve performance of face recognition using dlib?,"I am using dlib.get_frontal_face_detector()
And fir large images (several mb) it takes a lot of time to detect a face.

What are the ways to increase speed of face detection, without sacrificing accuracy?

I cannot use gpu/cuda sadly...",5,2,2022-06-30 22:56:48, p how to improve performance of face recognition using dlib ,i am using dlib get_frontal_face_detector  and fir large images  several mb  it takes a lot of time to detect a face what are the ways to increase speed of face detection  without sacrificing accuracy i cannot use gpu cuda sadly   
161,161,alder-ice,vo12l2,"[N] Introducing Anomalib: A library for benchmarking, developing and deploying deep learning anomaly detection algorithms by Intel","Anomalib is Machine Library developed by AI researchers from Intel which implements state of the art algorithms for anomaly detection. Anomaly detection is popular use case in the industrial sector and such algorithms can help provide real-time feedback to manufactures on how well their production lines are performing. 

Anomaly Detection is a challenging problem often due to a biased dataset. Anomalous images can be scare therefore these algorithms are trained on good images in an unsupervised fashion. By learning the normality, upon inference, the models can detect whether images are anomalous or not.

Anomalib was built using a PyTorchLightning Backbone and offers an easy way to deploy the models with OpenVino for inference speedup. 

Link to the github repo: [https://github.com/openvinotoolkit/anomalib](https://github.com/openvinotoolkit/anomalib)

Link to a tutorial on how to train your custom dataset with anomalib: [https://github.com/openvinotoolkit/anomalib/tree/development/docs/blog/001-train-custom-dataset](https://github.com/openvinotoolkit/anomalib/tree/development/docs/blog/001-train-custom-dataset)

Please feel free to check out the repo and give us your feedback",3,12,2022-06-30 11:30:29, n  introducing anomalib  a library for benchmarking  developing and deploying deep learning anomaly detection algorithms by intel,anomalib is machine library developed by ai researchers from intel which implements state of the art algorithms for anomaly detection  anomaly detection is popular use case in the industrial sector and such algorithms can help provide real time feedback to manufactures on how well their production lines are performing  anomaly detection is a challenging problem often due to a biased dataset  anomalous images can be scare therefore these algorithms are trained on good images in an unsupervised fashion  by learning the normality  upon inference  the models can detect whether images are anomalous or not anomalib was built using a pytorchlightning backbone and offers an easy way to deploy the models with openvino for inference speedup  link to the github repo   https link to a tutorial on how to train your custom dataset with anomalib   https please feel free to check out the repo and give us your feedback
162,162,gabegabe6,vnggck,[P] Neural Network Steganography (implementation) - Hiding secrets and malicious software in any neural network,"I saw a paper called *EvilModel* on how to hide malicious code in a neural network as we have thousands or millions of parameters that we can alter.

This basic technique is based on the modification of the `float32` values (but can be adapted to `float16`) where we modify the fraction bits or part of the fraction. 

- [Post/Tutorial on the process](https://www.gaborvecsei.com/Neural-Network-Steganography/)
- [GitHub repo for the project](https://github.com/gaborvecsei/Neural-Network-Steganography)
- [EvilModel paper](https://arxiv.org/abs/2107.08590)

As I saw with my experiments, we could easily hide megabytes of code in a simple *ResNet50* and get away with it. A well-trained (and generalized) network should not degrade in performance significantly. The testing of that is planned for a future post.

Also, this method could be used for watermarking neural network weights which could help with copyright claims (e.g.: someone is using your open-sourced (and appropriately licensed) weights out of the box in a commercial product)",35,263,2022-06-29 19:27:57, p  neural network steganography  implementation    hiding secrets and malicious software in any neural network,i saw a paper called  evilmodel  on how to hide malicious code in a neural network as we have thousands or millions of parameters that we can alter this basic technique is based on the modification of the  float  values  but can be adapted to  float   where we modify the fraction bits or part of the fraction     post tutorial on the process  https    github repo for the project  https    evilmodel paper  https as i saw with my experiments  we could easily hide megabytes of code in a simple  resnet  and get away with it  a well trained  and generalized  network should not degrade in performance significantly  the testing of that is planned for a future post also  this method could be used for watermarking neural network weights which could help with copyright claims  e g   someone is using your open sourced  and appropriately licensed  weights out of the box in a commercial product 
163,163,iblysa,vom1d2,[D][P] Ideas about how to model from a dataset with columns containing arrays of data?,"Hello. I have built a dataset that contains results of experiments I have been doing over some physical materials. Each row contains summary data for each piece, like width, height, weight, etc. Then I have several columns which values are arrays.

Each one of these columns contain a list of tuples, for example (162636363, 1373.8377). The first number is a timestamp, the second one the magnitude of a force applied to the material (or, for instance, the position where the force was applied, contact duration, etc.). We have hundreds or even thousands of tuples on each column. So, all columns represent measurements of the experiments done to a particular material.

We are recording when the material is damaged, since we want to predict its lifetime when the material is exposed to repetitive forces.

I'm wondering what to do with those array values. One option is to sort the tuples lists by timestamp and then treat the readings as a vector of a predefined dimension. But I have never fed this kind of data to a boosted tree model/framework like XGBoost.

The only experience I had feeding long vectors to a model was when doing some NLP, in that case the vectors were representations of words.

Do you think a vector made of all my experiments over a material can be treated as an embedding in a way? If so, how is the recommended way to proceed with this data in the modeling stage? Time series perhaps?

I'd appreciate your ideas and comments.

Thanks!!",0,0,2022-07-01 04:56:26, d  p  ideas about how to model from a dataset with columns containing arrays of data ,hello  i have built a dataset that contains results of experiments i have been doing over some physical materials  each row contains summary data for each piece  like width  height  weight  etc  then i have several columns which values are arrays each one of these columns contain a list of tuples  for example        the first number is a timestamp  the second one the magnitude of a force applied to the material  or  for instance  the position where the force was applied  contact duration  etc    we have hundreds or even thousands of tuples on each column  so  all columns represent measurements of the experiments done to a particular material we are recording when the material is damaged  since we want to predict its lifetime when the material is exposed to repetitive forces i m wondering what to do with those array values  one option is to sort the tuples lists by timestamp and then treat the readings as a vector of a predefined dimension  but i have never fed this kind of data to a boosted tree model framework like xgboost the only experience i had feeding long vectors to a model was when doing some nlp  in that case the vectors were representations of words do you think a vector made of all my experiments over a material can be treated as an embedding in a way  if so  how is the recommended way to proceed with this data in the modeling stage  time series perhaps i d appreciate your ideas and comments thanks  
164,164,certain_entropy,voahxz,"[D] What is considered a ""large"" model?","Curious about the usage of the word ""large"" in the research community and in papers as a descriptor. About 3 years ago, Bert-Large was considered large at 345 million parameters. Today we have a 11-B parameter T-5 model and larger. When describing models in papers, is there consensus as to what we consider a ""large"" model or set of categories to describe models based on their size?",3,1,2022-06-30 20:32:12, d  what is considered a large model ,curious about the usage of the word large in the research community and in papers as a descriptor  about  years ago  bert large was considered large at  million parameters  today we have a  b parameter t  model and larger  when describing models in papers  is there consensus as to what we consider a large model or set of categories to describe models based on their size 
165,165,statmlben,vo2bvp,[R] RankSEG: A Consistent Ranking-based Framework for Segmentation,"I am very excited to share our latest research: a new framework [RankSEG](https://arxiv.org/abs/2206.13086) on (image) segmentation.

**Abs**: In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the **existing thresholding-based framework** with most operating losses are **NOT consistent** with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely *RankDice*/*RankIoU*, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel execution are developed to implement the proposed framework in large-scale and high-dimensional segmentation.  We study statistical properties of the proposed framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and the rate of convergence are also provided. The numerical effectiveness of *RankDice/mRankDice* is demonstrated in various simulated examples and *Fine-annotated CityScapes* and *Pascal VOC* datasets with state-of-the-art deep learning architectures.

**Conclusion**: the proposed framework **RankSEG** consistently outperforms the existing **thresholding-based framework** (simply thresholding the estimated probabilities at 0.5).

**Contribution**: We summarize our major contribution as follows:

* To our best knowledge, the proposed ranking-based segmentation framework *RankDice*, is the first consistent segmentation framework with respect to the Dice metric (Dice-calibrated).
* Three numerical algorithms with GPU parallel execution are developed to implement the proposed framework in large-scale and high-dimensional segmentation.
* We establish a theoretical foundation of segmentation with respect to the Dice metric, such as the Bayes rule and Dice-calibration. Moreover, we present Dice-calibrated consistency and a convergence rate of the excess risk for the proposed *RankDice* framework, and indicate inconsistent results for the existing methods.
* Our experiments in two simulated examples and two real datasets (CityScapes dataset and Pascal VOC 2021 dataset) suggest that {the improvement of *RankDice* over the existing framework is practically significant for various loss functions and network architectures. The percentage of improvement on the best performance (for each framework) are 3.13% (over *threshold*) and 4.96% (over *argmax*) for CityScapes dataset (PSPNet + CE), and 3.87% (over *threshold*) and 2.91% (over *argmax*) for Pascal VOC 2021 dataset (PSPNet + CE/BCE).

For more information, please check:

Paper: [https://arxiv.org/abs/2206.13086](https://arxiv.org/abs/2206.13086)

Github website: [https://github.com/statmlben/rankseg](https://github.com/statmlben/rankseg)",1,7,2022-06-30 12:50:19, r  rankseg  a consistent ranking based framework for segmentation,i am very excited to share our latest research  a new framework  rankseg  https   abs    in this paper  we establish a theoretical foundation of segmentation with respect to the dice iou metrics  including the bayes rule and dice iou calibration  analogous to classification calibration or fisher consistency in classification  we prove that the   existing thresholding based framework   with most operating losses are   not consistent   with respect to the dice iou metrics  and thus may lead to a suboptimal solution  to address this pitfall  we propose a novel consistent ranking based framework  namely  rankdice   rankiou   inspired by plug in rules of the bayes segmentation rule  three numerical algorithms with gpu parallel execution are developed to implement the proposed framework in large scale and high dimensional segmentation   we study statistical properties of the proposed framework  we show it is dice  iou calibrated  and its excess risk bounds and the rate of convergence are also provided  the numerical effectiveness of  rankdice mrankdice  is demonstrated in various simulated examples and  fine annotated cityscapes  and  pascal voc  datasets with state of the art deep learning architectures   conclusion    the proposed framework   rankseg   consistently outperforms the existing   thresholding based framework    simply thresholding the estimated probabilities at      contribution    we summarize our major contribution as follows   to our best knowledge  the proposed ranking based segmentation framework  rankdice   is the first consistent segmentation framework with respect to the dice metric  dice calibrated    three numerical algorithms with gpu parallel execution are developed to implement the proposed framework in large scale and high dimensional segmentation   we establish a theoretical foundation of segmentation with respect to the dice metric  such as the bayes rule and dice calibration  moreover  we present dice calibrated consistency and a convergence rate of the excess risk for the proposed  rankdice  framework  and indicate inconsistent results for the existing methods   our experiments in two simulated examples and two real datasets  cityscapes dataset and pascal voc  dataset  suggest that  the improvement of  rankdice  over the existing framework is practically significant for various loss functions and network architectures  the percentage of improvement on the best performance  for each framework  are     over  threshold   and     over  argmax   for cityscapes dataset  pspnet   ce   and     over  threshold   and     over  argmax   for pascal voc  dataset  pspnet   ce bce  for more information  please check paper   https github website   https   github com statmlben rankseg  https   github com statmlben rankseg 
166,166,seraschka,vobeir,[P] Sharing an Interactive Research Demo on the Cloud,"I am curious to hear what you usually use to develop interactive versions of your research models! And, if you have any, I'd be excited to see some examples for inspiration 😊.

On that note, about 2 weeks ago, I shared an article on developing a Super-Resolution GAN Research Demo in Lightning on r/MachineLearning: [Bottom-up look at the new Lightning Framework for building anything from production-ready ML systems to research demos](https://www.reddit.com/r/MachineLearning/comments/vi41f7/p_bottomup_look_at_the_new_lightning_framework/)

Running research demos locally is not super useful by itself (unless you maybe do that live at a poster session -- I still have painful memories of doing that with privacy GAN demo in Flask), so this is a follow-up article on deploying the App on the Cloud: [Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud](https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html).

Also, curious to hear what you think!",2,0,2022-06-30 21:11:24, p  sharing an interactive research demo on the cloud,i am curious to hear what you usually use to develop interactive versions of your research models  and  if you have any  i d be excited to see some examples for inspiration   on that note  about  weeks ago  i shared an article on developing a super resolution gan research demo in lightning on r machinelearning   bottom up look at the new lightning framework for building anything from production ready ml systems to research demos  https running research demos locally is not super useful by itself  unless you maybe do that live at a poster session    i still have painful memories of doing that with privacy gan demo in flask   so this is a follow up article on deploying the app on the cloud   sharing deep learning research models with lightning part   leveraging the cloud  https also  curious to hear what you think 
167,167,gabe415160,vnvbdp,[Discussion] Regarding Long Term Memory in NLP Models,"Does anyone know if there exists a NLP model, like Lambda, that takes every conversation attempts to update their weights in order to incorporate it into its training?

My thought process would be instead of using attention and a subsection of the conversation to generate a response, it takes everything. Basically everything gets back propagated and adjusts the weights. This way the model might begin to ""remember"" its previous conversations. This may be a stretch and perhaps I am missing something fundamental, but it seems like an interesting experiment. I'd love to continue this conversation and elaborate more in the comments.",8,12,2022-06-30 06:26:11, discussion  regarding long term memory in nlp models,does anyone know if there exists a nlp model  like lambda  that takes every conversation attempts to update their weights in order to incorporate it into its training my thought process would be instead of using attention and a subsection of the conversation to generate a response  it takes everything  basically everything gets back propagated and adjusts the weights  this way the model might begin to remember its previous conversations  this may be a stretch and perhaps i am missing something fundamental  but it seems like an interesting experiment  i d love to continue this conversation and elaborate more in the comments 
168,168,darthsocker,vodb3j,[D] Moody Actor Critic,"Generally actor critic algorithms have 1 Neural net giving 1 of each via a linear layer - to give a policy and to give the value. But humans change decisions and how they think based on their mood. I wanted to incorporate this into a standard actor critic like A2C/A3C. I wanted to add another actor in this architecture that represented a certain mood, where it's objective was not to maximize the reward but something else that I have in mind.  I don't see any such literature in the field and I don't know how to add more actors. Is it not possible to have multiple actors with one critic ? Has this been passed on by the community for a lack of potential ?",4,0,2022-06-30 22:32:28, d  moody actor critic,generally actor critic algorithms have  neural net giving  of each via a linear layer   to give a policy and to give the value  but humans change decisions and how they think based on their mood  i wanted to incorporate this into a standard actor critic like ac ac  i wanted to add another actor in this architecture that represented a certain mood  where it s objective was not to maximize the reward but something else that i have in mind   i don t see any such literature in the field and i don t know how to add more actors  is it not possible to have multiple actors with one critic   has this been passed on by the community for a lack of potential  
169,169,NeoKoseii,vo7p9i,[P] [R] Automated Essay Scoring Systems for other languages," Hey guys, working on an AES project. Just wanted to know if there exists an AES system that can be trained on languages like Swahili, Arabic, Hindi etc. Languages having almost no AES studies done.

Would be very helpful of you to guide me through, any other tips/pointers towards this task are much appreciated, would love it if someone can point me in the right direction.",0,0,2022-06-30 18:24:36, p   r  automated essay scoring systems for other languages, hey guys  working on an aes project  just wanted to know if there exists an aes system that can be trained on languages like swahili  arabic  hindi etc  languages having almost no aes studies done would be very helpful of you to guide me through  any other tips pointers towards this task are much appreciated  would love it if someone can point me in the right direction 
170,170,cltexe,vohzvl,[D] Creating a neural network for my daughter's sake. Need advice on acronym.,"Hi, very long time lurker here.

I'm planning to propose an end to end architecture for my daughter's sake. Data is biomedical and any CNN is well capable of classfying if over %95 Acc (easy data u know!). However, I need to come up with an acronym to fit my daughter's name. Her name is DURU and here is what I come up with:

D- Deep (Deep like you know, deep learning)

U-Unified (I may use multiple models to form up an ensemble or feature concat, which will make it unified)

R- Residual (I may use residual connections between Cnn blocks. Though not flashy right now.)

R- Recommender (Could use recommender keyword, since I'm putting down sort of a Computer Aided Diagnosis Framework thingy)

R- Another R thing is welcome.

U - I need another U and I'm totally out of words.

Three letters is all I came up with. Couldn't find a word for the 4th letter that makes sense. U-net? I'm not segmentating anything. But if it was a segmentation dataset I may have come up with DUR-UNet which would make sense.

I need a final keyword starting with U which is applicable with CNNs. It could be minor trick to cope with overfitting, a loss function, an activation function, etc. It could also be a filler term like Unified.

Hope we could come up with a solution.",8,0,2022-07-01 01:55:58, d  creating a neural network for my daughter s sake  need advice on acronym ,hi  very long time lurker here i m planning to propose an end to end architecture for my daughter s sake  data is biomedical and any cnn is well capable of classfying if over   acc  easy data u know    however  i need to come up with an acronym to fit my daughter s name  her name is duru and here is what i come up with d  deep  deep like you know  deep learning u unified  i may use multiple models to form up an ensemble or feature concat  which will make it unified r  residual  i may use residual connections between cnn blocks  though not flashy right now  r  recommender  could use recommender keyword  since i m putting down sort of a computer aided diagnosis framework thingy r  another r thing is welcome u   i need another u and i m totally out of words three letters is all i came up with  couldn t find a word for the th letter that makes sense  u net  i m not segmentating anything  but if it was a segmentation dataset i may have come up with dur unet which would make sense i need a final keyword starting with u which is applicable with cnns  it could be minor trick to cope with overfitting  a loss function  an activation function  etc  it could also be a filler term like unified hope we could come up with a solution 
171,171,Anonymous_Guy_12,vo8fuo,"[D] Loss Function, Uncertainty","
Hello members, soo my question is suppose we have a model or architecture at we have an image classifier at the end of it which is trained on mnist images. We need to train the model such that when the image is passed through the classifier it outcomes it's results with some uncertainty in its predictions. 
We need to use that uncertainty in order to develop a loss function to train the whole model as we can't use the true labels of the images. 

Any resources or ideas related to above which can be helpful pls share with me. Any suggestions will be appreciated.

Thanks",4,0,2022-06-30 18:59:37, d  loss function  uncertainty,hello members  soo my question is suppose we have a model or architecture at we have an image classifier at the end of it which is trained on mnist images  we need to train the model such that when the image is passed through the classifier it outcomes it s results with some uncertainty in its predictions  we need to use that uncertainty in order to develop a loss function to train the whole model as we can t use the true labels of the images  any resources or ideas related to above which can be helpful pls share with me  any suggestions will be appreciated thanks
172,172,mrwafflezzz,vo2b3w,[D] How would go about tracking an ML run when the framework logs text to a txt log?,"I was hoping that Mlflow had a method or function for parsing a txt log, but I can't find anything. Does anyone know of an elegant solution that runs in parallel to the training process?",6,0,2022-06-30 12:48:56, d  how would go about tracking an ml run when the framework logs text to a txt log ,i was hoping that mlflow had a method or function for parsing a txt log  but i can t find anything  does anyone know of an elegant solution that runs in parallel to the training process 
173,173,big_black_doge,vnxhww,[D] How well does auto annotating a dataset with a pretrained model work?," Hi reddit, I am asking this question because I don't see much about this in the literature.

I want to build an object detection model (class + bounding box), but I have very little annotated data. I have an idea to use a pretrained model to create predicted annotations from a very large object classification (class, no bounding box) dataset. I can then filter those predicted annotations to use only high confidence annotations, and manually check the images to ensure quality. Then, I should have a large, high quality object detection dataset with which I can train my model with.

 I didn't see many papers on this type of thing. It's not exactly transfer learning, because it's actually building a task specific dataset from another type of dataset. Is there some reason why this wouldn't work? Or does anyone have any research on this type of idea?",9,2,2022-06-30 08:14:25, d  how well does auto annotating a dataset with a pretrained model work , hi reddit  i am asking this question because i don t see much about this in the literature i want to build an object detection model  class   bounding box   but i have very little annotated data  i have an idea to use a pretrained model to create predicted annotations from a very large object classification  class  no bounding box  dataset  i can then filter those predicted annotations to use only high confidence annotations  and manually check the images to ensure quality  then  i should have a large  high quality object detection dataset with which i can train my model with  i didn t see many papers on this type of thing  it s not exactly transfer learning  because it s actually building a task specific dataset from another type of dataset  is there some reason why this wouldn t work  or does anyone have any research on this type of idea 
174,174,RepresentativeCod613,vmz09g,[D][P] YOLOv6: state-of-the-art object detection at 1242 FPS,"YOLOv6 has been making a lot of noise in the past 24 hours. Based on its performance - rightfully so.

YOLOv6 is a single-stage object detection framework dedicated to industrial applications, with hardware-friendly efficient design and high performance. It outperforms YOLOv5 in accuracy and inference speed, making it the best OS version of YOLO architecture for production applications.

I dived into the technical details published by the research group and made a qualitative and qualitative comparison between the results of YOLOv5 and YOLOv6. 

I invite you to read about all of these, with a bit of history on YOLO, in the my [new blog](https://dagshub.com/blog/yolov6/)",48,250,2022-06-29 03:25:27, d  p  yolov  state of the art object detection at  fps,yolov has been making a lot of noise in the past  hours  based on its performance   rightfully so yolov is a single stage object detection framework dedicated to industrial applications  with hardware friendly efficient design and high performance  it outperforms yolov in accuracy and inference speed  making it the best os version of yolo architecture for production applications i dived into the technical details published by the research group and made a qualitative and qualitative comparison between the results of yolov and yolov  i invite you to read about all of these  with a bit of history on yolo  in the my  new blog  https   dagshub com blog yolov  
175,175,Meddhouib10,vo43z0,[D] Merging two iterators (pytorch dataloaders),"Hello everyone !
So I have two dataloaders that I want to iterate throught, like not in a chained way.
I want to simple elements from one or the other each iteration step randomly.
Is there anyway to create a pytorch dataloader that does that ? Like what function should a pytorch dataloaders contain.
I found nothing on this subject on the internet that’s why I’m asking for your help
Thanks in advance",13,0,2022-06-30 14:49:45, d  merging two iterators  pytorch dataloaders ,hello everyone  so i have two dataloaders that i want to iterate throught  like not in a chained way i want to simple elements from one or the other each iteration step randomly is there anyway to create a pytorch dataloader that does that   like what function should a pytorch dataloaders contain i found nothing on this subject on the internet that s why i m asking for your helpthanks in advance
176,176,optimized-adam,vndtn8,[D] Mixed Precision Training: Difference between BF16 and FP16,"What differences in model performance, speed, memory etc. can I expect between choosing BF16 or FP16 for mixed precision training? Is BF16 faster / consumes less memory, since I have seen people say it is ""more suitable for Deep Learning"". Why is that the case?",8,13,2022-06-29 17:14:00, d  mixed precision training  difference between bf and fp,what differences in model performance  speed  memory etc  can i expect between choosing bf or fp for mixed precision training  is bf faster   consumes less memory  since i have seen people say it is more suitable for deep learning  why is that the case 
177,177,DreamFlasher,vmwiep,"[N] PyTorch 1.12: TorchArrow, Functional API for Modules and NvFuser"," PyTorch 1.12 Release Notes

* Highlights
* Backwards Incompatible Change
* New Features
* Improvements
* Performance
* Documentation

Highlights

We  are excited to announce the release of PyTorch 1.12! This release  is  composed of over 3124 commits, 433 contributors. Along with 1.12, we   are releasing beta versions of AWS S3 Integration, PyTorch Vision Models   on Channels Last on CPU, Empowering PyTorch on Intel® Xeon® Scalable   processors with Bfloat16 and FSDP API. We want to sincerely thank our   dedicated community for your contributions.

Summary:

* Functional Module API to functionally apply module computation with a given set of parameters
* Complex32 and Complex Convolutions in PyTorch
* DataPipes from TorchData fully backward compatible with DataLoader
* Functorch with improved coverage for APIs
* nvFuser a deep learning compiler for PyTorch
* Changes to float32 matrix multiplication precision on Ampere and later CUDA hardware
* TorchArrow, a new beta library for machine learning preprocessing over batch data

[https://github.com/pytorch/pytorch/releases/tag/v1.12.0](https://github.com/pytorch/pytorch/releases/tag/v1.12.0)

[https://pytorch.org/blog/pytorch-1.12-released/](https://pytorch.org/blog/pytorch-1.12-released/)",9,92,2022-06-29 01:37:50, n  pytorch    torcharrow  functional api for modules and nvfuser, pytorch   release notes  highlights  backwards incompatible change  new features  improvements  performance  documentationhighlightswe  are excited to announce the release of pytorch    this release  is  composed of over  commits   contributors  along with    we   are releasing beta versions of aws s integration  pytorch vision models   on channels last on cpu  empowering pytorch on intel  xeon  scalable   processors with bfloat and fsdp api  we want to sincerely thank our   dedicated community for your contributions summary   functional module api to functionally apply module computation with a given set of parameters  complex and complex convolutions in pytorch  datapipes from torchdata fully backward compatible with dataloader  functorch with improved coverage for apis  nvfuser a deep learning compiler for pytorch  changes to float matrix multiplication precision on ampere and later cuda hardware  torcharrow  a new beta library for machine learning preprocessing over batch data https  https   pytorch org blog pytorch   released   https   pytorch org blog pytorch   released  
178,178,AvisStudio,vn6qyt,[P] Unofficial Gato in TensorFlow,"[https://github.com/OrigamiDream/gato](https://github.com/OrigamiDream/gato)

I am building Deepmind's Gato imitation in TensorFlow.

All necessary layers have been completely implemented.

&#x200B;

However, I have no idea how to map out the **training strategy**, and I do not have enough datasets for this.

The model seems impossible for **end-to-end training** because of its conditional and selective tokenizer and embeddings, and differentiable programming.

&#x200B;

If you are interested in this project, add a **star** and **notification** to this repository for further updates.

And someone who want to contribute to this project, please create a **relevant issue** or **pull request**.

&#x200B;

Thank you.",4,14,2022-06-29 09:48:26, p  unofficial gato in tensorflow, https i am building deepmind s gato imitation in tensorflow all necessary layers have been completely implemented   xb however  i have no idea how to map out the   training strategy    and i do not have enough datasets for this the model seems impossible for   end to end training   because of its conditional and selective tokenizer and embeddings  and differentiable programming   xb if you are interested in this project  add a   star   and   notification   to this repository for further updates and someone who want to contribute to this project  please create a   relevant issue   or   pull request     xb thank you 
179,179,bikeskata,vmr8it,[R] Probabilistic Numerics: Computation as Machine Learning (Free Book!),"Abs: Probabilistic numerical computation formalises the connection between machine learning and applied mathematics. Numerical algorithms approximate intractable quantities from computable ones. They estimate integrals from evaluations of the integrand, or the path of a dynamical system described by differential equations from evaluations of the vector field. In other words, they infer a latent quantity from data. This book shows that it is thus formally possible to think of computational routines as learning machines, and to use the notion of Bayesian inference to build more flexible, efficient, or customised algorithms for computation. The text caters for Masters' and PhD students, as well as postgraduate researchers in artificial intelligence, computer science, statistics, and applied mathematics. Extensive background material is provided along with a wealth of figures, worked examples, and exercises (with solutions) to develop intuition. 

Link to book: https://www.probabilistic-numerics.org/textbooks/",6,95,2022-06-28 21:55:51, r  probabilistic numerics  computation as machine learning  free book  ,abs  probabilistic numerical computation formalises the connection between machine learning and applied mathematics  numerical algorithms approximate intractable quantities from computable ones  they estimate integrals from evaluations of the integrand  or the path of a dynamical system described by differential equations from evaluations of the vector field  in other words  they infer a latent quantity from data  this book shows that it is thus formally possible to think of computational routines as learning machines  and to use the notion of bayesian inference to build more flexible  efficient  or customised algorithms for computation  the text caters for masters  and phd students  as well as postgraduate researchers in artificial intelligence  computer science  statistics  and applied mathematics  extensive background material is provided along with a wealth of figures  worked examples  and exercises  with solutions  to develop intuition  link to book  https   www probabilistic numerics org textbooks 
180,180,antarfrica,vnewe9,[D] Training GANs with non-square images,"I am planning to train stylegan2 ada with rectangular images (aspect ratio = 16:9). Is it better to use (zero) padding, resizing, or train a rectangular GAN?
Thankyou verymuch!",2,2,2022-06-29 18:11:43, d  training gans with non square images,i am planning to train stylegan ada with rectangular images  aspect ratio       is it better to use  zero  padding  resizing  or train a rectangular gan thankyou verymuch 
182,182,ml_rl_questions,vnbuq5,[R] Use pretrained GANs and image classifier to generate images of the class,"Pretrained GANs and CLIP embeddings have been used to created images from arbitrary caption, by backpropagating CLIP similarity of the caption and the generated image down to the generator input noise.

I am thinking of something simpler, where I would take a pretrained GAN, and backpropagate through some pretrained classifier (e.g. image et) down to the input noise to the Generator to generate images of that class.

Is there any reference that does that?

And more generally, i want to understand why this approach works - simple backpropagating the classifier loss to the image (and not through the generator) typically result in deep dream type of weird images. Why does this not happen when using a generator? Is it simply because the output of the generator lives in the manifold of ""real"" images? Is there more to  it?

Thanks in advance",1,2,2022-06-29 15:12:26, r  use pretrained gans and image classifier to generate images of the class,pretrained gans and clip embeddings have been used to created images from arbitrary caption  by backpropagating clip similarity of the caption and the generated image down to the generator input noise i am thinking of something simpler  where i would take a pretrained gan  and backpropagate through some pretrained classifier  e g  image et  down to the input noise to the generator to generate images of that class is there any reference that does that and more generally  i want to understand why this approach works   simple backpropagating the classifier loss to the image  and not through the generator  typically result in deep dream type of weird images  why does this not happen when using a generator  is it simply because the output of the generator lives in the manifold of real images  is there more to  it thanks in advance
183,183,BB4evaTB12,vmyrd1,Creating and Analyzing a Dataset of Roe v. Wade Tweets Labeled by Abortion Stance [P],"How do pro-choice vs. pro-life twitter users differ?

I built a free, labeled dataset of #RoeVsWade tweets, and an ML classifier on top.

Some insights:

Pro-life users are 20.4x more likely to put ""christ"" and 16.1x more likely to put ""maga"" in their bio.Pro-choice users are 7.5x more likely to put ""blm"" and 6.5x more likely to put ""she/her"".

Full analysis + link to raw dataset [here](https://www.surgehq.ai/blog/dataset-of-roe-v-wade-tweets-labeled-by-abortion-stance).",4,26,2022-06-29 03:14:35,creating and analyzing a dataset of roe v  wade tweets labeled by abortion stance  p ,how do pro choice vs  pro life twitter users differ i built a free  labeled dataset of  roevswade tweets  and an ml classifier on top some insights pro life users are  x more likely to put christ and  x more likely to put maga in their bio pro choice users are  x more likely to put blm and  x more likely to put she her full analysis   link to raw dataset  here  https   www surgehq ai blog dataset of roe v wade tweets labeled by abortion stance  
184,184,devzaya,vmn7nt,"[N] Quaterion, a blazingly fast framework for similarity learning.","Just released. Quaterion — an open source framework for training and fine-tuning similarity learning models. It enables you to train models significantly (100x) faster, and iterate over experiments in minutes instead of hours even with a laptop GPU. It takes advantage of the PyTorch Lightning backend to make a flexible and scalable learning pipeline. GitHub https://github.com/qdrant/quaterion 

Here is a demo of the caching functionality.  

https://i.redd.it/9qi8gf9n4d891.gif",11,94,2022-06-28 18:53:52, n  quaterion  a blazingly fast framework for similarity learning ,just released  quaterion   an open source framework for training and fine tuning similarity learning models  it enables you to train models significantly  x  faster  and iterate over experiments in minutes instead of hours even with a laptop gpu  it takes advantage of the pytorch lightning backend to make a flexible and scalable learning pipeline  github https here is a demo of the caching functionality   https   i redd it qigfnd gif
185,185,metover,vnaj09,[D] What are the lessons learned in the preparations of the dataset you will use to train a GANs?,"Hello friends, what are the key points we should pay attention to in the datasets you will prepare for GANs, do you have any suggestions?

For example the distribution of the dataset should be like this, the images should be the same size, it is important to reduce all the images to this size, many things that I have not thought of at the moment? What are your recommendations?",0,3,2022-06-29 13:39:51, d  what are the lessons learned in the preparations of the dataset you will use to train a gans ,hello friends  what are the key points we should pay attention to in the datasets you will prepare for gans  do you have any suggestions for example the distribution of the dataset should be like this  the images should be the same size  it is important to reduce all the images to this size  many things that i have not thought of at the moment  what are your recommendations 
186,186,nyxrat,vnc953,[D] AI & Big Data Expo; worth it?,"Interested in AI/Machine learning research, hoping to check out their NA expo to learn more. Has anyone here ever been to one of their conventions? What were your experiences like?",2,0,2022-06-29 15:37:59, d  ai   big data expo  worth it ,interested in ai machine learning research  hoping to check out their na expo to learn more  has anyone here ever been to one of their conventions  what were your experiences like 
187,187,M4mb0,vml5na,[N] PyTorch 1.12 released,"Pytorch 1.12 is available through the [pytorch conda channel](https://anaconda.org/pytorch/pytorch) and [pypi](https://pypi.org/project/torch/)

- [Release notes](https://github.com/pytorch/pytorch/releases)
- [Issue tracker](https://github.com/pytorch/pytorch/milestone/28)


## Highlights

> We are excited to announce the release of PyTorch 1.12! This release is composed of over 3124 commits, 433 contributors. Along with 1.12, we are releasing beta versions of AWS S3 Integration, PyTorch Vision Models on Channels Last on CPU, Empowering PyTorch on Intel® Xeon® Scalable processors with Bfloat16 and FSDP API. We want to sincerely thank our dedicated community for your contributions.

> Summary:

>  -  Functional Module API to functionally apply module computation with a given set of parameters
>  -  Complex32 and Complex Convolutions in PyTorch
>  -  DataPipes from TorchData fully backward compatible with DataLoader
>  -  Functorch with improved coverage for APIs
>  -  nvFuser a deep learning compiler for PyTorch
>  -  Changes to float32 matrix multiplication precision on Ampere and later CUDA hardware
>  -  TorchArrow, a new beta library for machine learning preprocessing over batch data


## Other noteable changes

- CUDA 11.6 wheels
- [torch.amp](https://pytorch.org/docs/1.12/amp.html) module",0,40,2022-06-28 17:07:42, n  pytorch   released,pytorch   is available through the  pytorch conda channel  https    release notes  https    issue tracker  https    highlights  we are excited to announce the release of pytorch    this release is composed of over  commits   contributors  along with    we are releasing beta versions of aws s integration  pytorch vision models on channels last on cpu  empowering pytorch on intel  xeon  scalable processors with bfloat and fsdp api  we want to sincerely thank our dedicated community for your contributions   summary       functional module api to functionally apply module computation with a given set of parameters      complex and complex convolutions in pytorch      datapipes from torchdata fully backward compatible with dataloader      functorch with improved coverage for apis      nvfuser a deep learning compiler for pytorch      changes to float matrix multiplication precision on ampere and later cuda hardware      torcharrow  a new beta library for machine learning preprocessing over batch data   other noteable changes  cuda   wheels   torch amp  https   pytorch org docs   amp html  module
188,188,Mon0o0,vn2hin,What is the essence of Diffusion models? [D],"Coming from a math/stats background the point of much of the machine learning literature can take time to understand fully, in particular I have a couple of quick (interconnected) questions regarding the essence of Diffusion models that I hope somebody may answer (of the many blog posts I have read I can't seem to find a clear answer). 
      
As a reference let me take the seminal paper of Ho et al. https://arxiv.org/abs/2006.11239
     

* When fixing the coefficients $\beta_1, \dots, \beta_T$ that govern the forward diffusion process (treating them as hyperparameters) can't we, at least in simple cases, already recover the reverse diffusion process in closed form? If yes why do we even need to find the reverse diffusion process through an optimization procedure when we already have it in closed form?


* I have read that diffusion models should perform a dimensionality reduction on the data but, even understanding the mathematics, I can't understand how the dimensionality reduction is being achieved by learning the reverse process. What is the usefulness behind the whole procedure?      

* If the forward process converges to an isotropic Gaussian (it destroys all the structure in the data) how can we hope to learn anything significant from it if it becomes simply a bunch of noise. (I suspect that the answer to this question is that we always stop the forward process before it becomes its limit)

Thanks to anyone that can clear up these doubts of mine.",6,3,2022-06-29 06:12:14,what is the essence of diffusion models   d ,coming from a math stats background the point of much of the machine learning literature can take time to understand fully  in particular i have a couple of quick  interconnected  questions regarding the essence of diffusion models that i hope somebody may answer  of the many blog posts i have read i can t seem to find a clear answer         as a reference let me take the seminal paper of ho et al  https        when fixing the coefficients   beta_   dots   beta_t  that govern the forward diffusion process  treating them as hyperparameters  can t we  at least in simple cases  already recover the reverse diffusion process in closed form  if yes why do we even need to find the reverse diffusion process through an optimization procedure when we already have it in closed form   i have read that diffusion models should perform a dimensionality reduction on the data but  even understanding the mathematics  i can t understand how the dimensionality reduction is being achieved by learning the reverse process  what is the usefulness behind the whole procedure         if the forward process converges to an isotropic gaussian  it destroys all the structure in the data  how can we hope to learn anything significant from it if it becomes simply a bunch of noise   i suspect that the answer to this question is that we always stop the forward process before it becomes its limit thanks to anyone that can clear up these doubts of mine 
189,189,ben_cow,vmv54i,[D] [P] Questions about the usability of Shapley values on large feature spaces.," 

Hello! I am planning a research project which involves creating a classification DNN that takes in a frame from a molecular dynamics simulation of a protein which encodes each amino acid's  level of energetic interaction and tries to predict whether that frame came from protein state ""A"" or protein state ""B.""  I want to analyze the feature importance, that is, the importance of amino acid's energetic interaction level for making the classification prediction. Although I have heard some interesting applications with Shapley values to preform such an analysis on feature importance, the input layer structure of the model I am thinking of making would require 100+ neurons as there are 100+ features. The reason why the feature space is so large is because I am investigating how a model learns which amino acids are most important for the model to make a classification prediction for which state a protein is in where the protein is 100+ amino acids in length. Can Shapley methods handle a feature space of a model that large /would the computational cost of such a process be infeasible? Apologies if this question is a little unclear let me know if anything needs to be clarified.

Thanks!",0,6,2022-06-29 00:39:05, d   p  questions about the usability of shapley values on large feature spaces , hello  i am planning a research project which involves creating a classification dnn that takes in a frame from a molecular dynamics simulation of a protein which encodes each amino acid s  level of energetic interaction and tries to predict whether that frame came from protein state a or protein state b   i want to analyze the feature importance  that is  the importance of amino acid s energetic interaction level for making the classification prediction  although i have heard some interesting applications with shapley values to preform such an analysis on feature importance  the input layer structure of the model i am thinking of making would require   neurons as there are   features  the reason why the feature space is so large is because i am investigating how a model learns which amino acids are most important for the model to make a classification prediction for which state a protein is in where the protein is   amino acids in length  can shapley methods handle a feature space of a model that large  would the computational cost of such a process be infeasible  apologies if this question is a little unclear let me know if anything needs to be clarified thanks 
190,190,InternationalVisito,vml9we,[D]Can a transformer neural network learn to predict sequences longer than it saw?," Simple task: transformer has to repeat a sequence of random integers (0-9) of varied length, like:

sequence **length=7**: input\[ 1, 3 ,5 ,6, 2, 4, 0\] - output\[ 1, 3 ,5 ,6, 2, 4, 0\]  
sequence **length=3**: input\[ 5, 4 ,9 \] - output\[ 5, 4 ,9 \]  
sequence **length=4**: input\[ 6, 3 ,9, 8 \] - output\[ 6, 3 ,9, 8 \]  
...  
Each integer(0-9) can be stored in embedding layer so we can pass it to transformer.  
I trained transformer (generic pytorch model with positional embeddings) on a dataset (1000 examples) of sequences of varied length (1 to 12) and it predicts sequences well within the range of 12 . It fails to predict sequences longer than 12 - 13.  


sequence **length=20**: input\[3, 3, 4, 0, 0, 7, 1, 5, 1, 0, 7, 1, ***9, 0, 9, 1, 5, 2, 3, 6***\]  
.............................. ...- output\[3, 3, 4, 0, 0, 7, 1, 5, 1, 0, 7, 1, ***7, 1, 7, 1, 0, 7, 0, 7***\]

Is it considered an extrapolation task? Are there types of transformers (or other neural networks) that can handle the problem ?  
Same issue with **recurrent neural networks** (RNN, LSTM, GRU).",31,15,2022-06-28 17:14:14, d can a transformer neural network learn to predict sequences longer than it saw , simple task  transformer has to repeat a sequence of random integers     of varied length  like sequence   length     input                    output                   sequence   length     input             output            sequence   length     input               output                   each integer    can be stored in embedding layer so we can pass it to transformer   i trained transformer  generic pytorch model with positional embeddings  on a dataset   examples  of sequences of varied length   to   and it predicts sequences well within the range of    it fails to predict sequences longer than       sequence   length     input                                                                                      output                                                is it considered an extrapolation task  are there types of transformers  or other neural networks  that can handle the problem    same issue with   recurrent neural networks    rnn  lstm  gru  
191,191,BlockDesigns,vmpenb,[P] Clustering long documents with Transformers in 10 minutes,"Transformers are awesome for so many things in 2022, but one thing I've found them to struggle with is generating embeddings for long documents.

I put together a blog post going through some interesting techniques. Let me know if it helped you!

[Blog post](https://www.notia.ai/articles/clustering-long-documents)",0,7,2022-06-28 20:35:39, p  clustering long documents with transformers in  minutes,transformers are awesome for so many things in   but one thing i ve found them to struggle with is generating embeddings for long documents i put together a blog post going through some interesting techniques  let me know if it helped you  blog post  https   www notia ai articles clustering long documents 
192,192,MLknowledge,vme1l7,"[R] Welcome to my continuous, free live machine learning class with intermediate mathematics","Dear all,

Welcome to join my continued ML knowledge dissemination class via Zoom.

I will continue to explain machine learning using an intermediate level mathematics. It happens every second Thursday at GMT at 11:00 (HK7pm / SYD9pm) - the next class is on June 30.

The current topic is:

""Determinantal Point Process""

I'll fully explain its beautiful mathematics over a period of a few sessions. This is a powerful model to model diverse subsets. Yet it is not as commonly used as it should!

You can find my notes on my GitHub site:

[https://github.com/roboticcam/machine-learning-notes/](https://github.com/roboticcam/machine-learning-notes/)

Determinantal Point Process notes is found at:

[https://github.com/roboticcam/machine-learning-notes/blob/master/files/dpp\_new.pdf](https://github.com/roboticcam/machine-learning-notes/blob/master/files/dpp_new.pdf)

You need a solid understanding of linear algebra, calculus, probability and statistics. But if you just want to get a feel of how DPP works for example, and meet like-minded people, please come too!

To join, sign up for one of the meetup groups you see fit:

[https://www.meetup.com/machine-learning-hong-kong/](https://www.meetup.com/machine-learning-hong-kong/)

[https://www.meetup.com/deep-learning-sydney/](https://www.meetup.com/deep-learning-sydney/)

[https://www.meetup.com/Deep-Learning-Melbourne/](https://www.meetup.com/Deep-Learning-Melbourne/)

[https://www.meetup.com/machine-learning-athens/](https://www.meetup.com/machine-learning-athens/)",3,39,2022-06-28 09:38:03, r  welcome to my continuous  free live machine learning class with intermediate mathematics,dear all welcome to join my continued ml knowledge dissemination class via zoom i will continue to explain machine learning using an intermediate level mathematics  it happens every second thursday at gmt at    hkpm   sydpm    the next class is on june  the current topic is determinantal point processi ll fully explain its beautiful mathematics over a period of a few sessions  this is a powerful model to model diverse subsets  yet it is not as commonly used as it should you can find my notes on my github site  https determinantal point process notes is found at  https you need a solid understanding of linear algebra  calculus  probability and statistics  but if you just want to get a feel of how dpp works for example  and meet like minded people  please come too to join  sign up for one of the meetup groups you see fit  https  https  https  https   www meetup com machine learning athens   https   www meetup com machine learning athens  
193,193,alexlyzhov,vm2sti,[N] Inverse Scaling Prize: $250k in prizes for finding tasks where larger language models do worse,"The standard paradigm in natural language processing today is to train large language models to autocomplete random Internet-sourced text. These models are then either frozen and used directly for other tasks (zero-shot/few-shot), or additionally trained on other tasks (fine-tuning).

We're used to finding that task performance scales well with large increases in sizes of language models. But for real-world applications, it's also very meaningful to search for failure cases of scaling preemptively to fix the underlying issues. Can you find and convincingly demonstrate these failure cases where zero-shot/few-shot performance of language models scales *inversely*, with larger models behaving worse?

You don't necessarily need to have extra deep knowledge of ML or language models in order to participate and win, because all models are frozen and you only need to come up with the right data.

Check out these resources to learn more! [Announcement Twitter thread](https://twitter.com/EthanJPerez/status/1541454949397041154), [contest details on Github](https://github.com/inverse-scaling/prize).
The deadline for the first round of the contest is August 27, 2022.",39,213,2022-06-28 00:19:44, n  inverse scaling prize   k in prizes for finding tasks where larger language models do worse,the standard paradigm in natural language processing today is to train large language models to autocomplete random internet sourced text  these models are then either frozen and used directly for other tasks  zero shot few shot   or additionally trained on other tasks  fine tuning  we re used to finding that task performance scales well with large increases in sizes of language models  but for real world applications  it s also very meaningful to search for failure cases of scaling preemptively to fix the underlying issues  can you find and convincingly demonstrate these failure cases where zero shot few shot performance of language models scales  inversely   with larger models behaving worse you don t necessarily need to have extra deep knowledge of ml or language models in order to participate and win  because all models are frozen and you only need to come up with the right data check out these resources to learn more   announcement twitter thread  https the deadline for the first round of the contest is august    
194,194,Farconion,vmr5a5,[D] Have compression techniques every been applied to the likes of GPT-3 & DALLE-2?,"Large language models and the recent spur of diffusion based  text-to-image models are gosh-darn fun to play with, but due to their  size and expensive training costs - they're only accessible via an API  or if you yourself have a access to a large # of GPUs. Yet there are  also a number of compression techniques like pruning and quantization  that can drastically reduce the size (+90%), and thus computational  requirements, of a trained model. Has there been any work looking  appling such techniques to these gigantic models floating around to make  them more accessible?",7,4,2022-06-28 21:51:53, d  have compression techniques every been applied to the likes of gpt    dalle  ,large language models and the recent spur of diffusion based  text to image models are gosh darn fun to play with  but due to their  size and expensive training costs   they re only accessible via an api  or if you yourself have a access to a large   of gpus  yet there are  also a number of compression techniques like pruning and quantization  that can drastically reduce the size       and thus computational  requirements  of a trained model  has there been any work looking  appling such techniques to these gigantic models floating around to make  them more accessible 
195,195,hegelian_waffle,vm9tki,[D] Laplacian positional encodings,"I just finished reading ""[Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)"" (Dwivedi et al. 2020) and ""[A Generalization of Transformer Networks to Graphs](https://arxiv.org/abs/2012.09699)"" (also Dwivedi et al. 2020), and came across the claim that the eigenvectors of the Laplacian of a graph ""represent a natural generalization of the Transformer (Vaswani et al., 2017) positional encodings (PE)"". Xavier Bresson [tweeted](https://twitter.com/xbresson/status/1273034896517332992?lang=en) the same thing.

So I worked out the eigenvectors of the Laplacian of a path graph (a line of vertices connected by edges like so: v-v-v-...-v), which is the kind of graph used in NLP to represent a sequence of tokens, and found that the ith eigenvector's kth entry is v\_i(k) = cos(πik/n − πi/2n) where n is the number of tokens in the sequence, which is very different from the sinusoidal PEs used in transformers in NLP. I tried working out a change of variables, but nothing's worked so far. Are Laplacian eigenvectors just not the generalizations they're claimed to be, or am I missing something here?",4,52,2022-06-28 05:59:30, d  laplacian positional encodings,i just finished reading  benchmarking graph neural networks  https so i worked out the eigenvectors of the laplacian of a path graph  a line of vertices connected by edges like so  v v v     v   which is the kind of graph used in nlp to represent a sequence of tokens  and found that the ith eigenvector s kth entry is v _i k    cos πik n   πi n  where n is the number of tokens in the sequence  which is very different from the sinusoidal pes used in transformers in nlp  i tried working out a change of variables  but nothing s worked so far  are laplacian eigenvectors just not the generalizations they re claimed to be  or am i missing something here 
196,196,moschles,vmdl3l,[D] Surface rendering in Diffusion Probability Text-to-Image Generators.,"Two diffusion text-to-image generators are Google's Imagen and openai's DALLE.2.     

DALLE.2 uses a multimodal large language model called CLIP to encode an input text prompt.  The output is produced by a reverse encoder called a diffusion probability model.   Diffusion models have previously seen huge successes in *image super resolution* and denoising.  

One peculiar aspect of DALLE.2's  output is that it is capable of generating light sources in certain (seemingly) 3D locations in the scene, then correctly lighting the objects based off of their implied location.   DALLE.2 can also perform image completions from a starting image prompt.     The two examples below are Spongebob dish sponge in a sink, and Vermeer's famous earring painting.    


https://i.imgur.com/vVI6IOI.png

.


https://i.imgur.com/8h48lTg.png

.

One plausible explanation for these physically perfect surface reflections is that DALLE.2  performs a phase where the image is reverse-encoded into a 3D scene.  That scene is then rendered back into a 2D output image.    However, when consulting the primary literature, no such conversion to a 3D model is seen anywhere along the DALLE.2 workflow.  

The implication is that DALLE.2 must contain a wealth of priors related to light transport, gleaned simply from 2D training images alone.  This means these priors are being applied (mostly correctly) to particular instantiations of objects and surfaces in scenes.  This application is performed even to the point where wet metallic surfaces have correct blurring in reflections.       

Further investigations of this phenomenon would involve finding some user prompts that generated a scene containing light casting a sharp shadow onto a flat surface.  Another would be requesting a reflective object in the text prompt itself.

Your thoughts?",2,28,2022-06-28 09:13:20, d  surface rendering in diffusion probability text to image generators ,two diffusion text to image generators are google s imagen and openai s dalle       dalle  uses a multimodal large language model called clip to encode an input text prompt   the output is produced by a reverse encoder called a diffusion probability model    diffusion models have previously seen huge successes in  image super resolution  and denoising   one peculiar aspect of dalle  s  output is that it is capable of generating light sources in certain  seemingly  d locations in the scene  then correctly lighting the objects based off of their implied location    dalle  can also perform image completions from a starting image prompt      the two examples below are spongebob dish sponge in a sink  and vermeer s famous earring painting     https  https  one plausible explanation for these physically perfect surface reflections is that dalle   performs a phase where the image is reverse encoded into a d scene   that scene is then rendered back into a d output image     however  when consulting the primary literature  no such conversion to a d model is seen anywhere along the dalle  workflow   the implication is that dalle  must contain a wealth of priors related to light transport  gleaned simply from d training images alone   this means these priors are being applied  mostly correctly  to particular instantiations of objects and surfaces in scenes   this application is performed even to the point where wet metallic surfaces have correct blurring in reflections        further investigations of this phenomenon would involve finding some user prompts that generated a scene containing light casting a sharp shadow onto a flat surface   another would be requesting a reflective object in the text prompt itself your thoughts 
197,197,SnooRecipes1624,vlpnuw,"[D] IBM Zurich Research Plagiarised Our Paper and got it published on CVPR 2022. Is ""copy texts"" is plagiarism, ""copy idea"" is not plagiarism?","I am Xianbiao Qi, a computer vision researcher with more than ten years of research experience. I am writing this blog to complain of a serious case of deliberate plagiarism of our paper by the employees from **IBM Zurich Research. They did not copy texts, they copied the idea.**

>  
>  
>Our preprint paper on Arxiv is ""Jiaquan Ye, Xianbiao Qi, Yelin He, and etc.""PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML."" arXiv preprint arXiv:2105.01848, May 2021"" and the code was also released.  
>  
>  
>  
>Our paper (Ye et al. arXiv: 2105.01848) was plagiarised by a team in IBM Zurich Research: ""**Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, and Peter Staar, ""TableFormer: Table Structure Understanding with Transformers.""** In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4614-4623, June 2022.""  
>  
>  
>  
>Nassar et al. 's paper plagiarized our method, pre-processing, post-processing, visualization, inference, systematic solution, code, pretrained models and etc, but did NOT cite our paper in order to not be captured for the plagiarism.  I was notified by many experts in the fields of OCR and table analysis.

Let me explain the plagiarism process step by step.

>**First**,  let me sort out the timeline.  
>  
>**Our Tablemaster,**  
>  
>**2021-04-07,** we finish the ICDAR 2021 competition-on-scientific-literature-parsing **hosted by another IBM group in Australia**  
>  
>[https://icdar2021.org/program-2/competitions/competition-on-scientific-literature-parsing/](https://icdar2021.org/program-2/competitions/competition-on-scientific-literature-parsing/)  
>  
>**2021-05-05**, we released our technical report, a preprint version, on Arxiv[https://arxiv.org/pdf/2105.01848.pdf](https://arxiv.org/pdf/2105.01848.pdf)  
>  
>**2021-07-29**, we released our source code, and even our slides on Github, [https://github.com/JiaquanYe/TableMASTER-mmocr](https://github.com/JiaquanYe/TableMASTER-mmocr)  
>  
>**2021-09**: we released the Tablemaster pretrained model.  
>  
>**2021-11** we even released tablemaster\_mmocr docker environment.  
>  
>For this project, we were naked. We release all these materials to facilitate the community, but you  plagiarized it.  
>  
>  
>  
>**Plagiarized-TableFormer:**  
>  
>**2022-03-02**, the Plagiarized-TableFormer was released on Arxiv[https://arxiv.org/pdf/2203.01017v1.pdf](https://arxiv.org/pdf/2203.01017v1.pdf)Their supplemental to cvpr submission.[https://openaccess.thecvf.com/content/CVPR2022/supplemental/Nassar\_TableFormer\_Table\_Structure\_CVPR\_2022\_supplemental.pdf](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Nassar_TableFormer_Table_Structure_CVPR_2022_supplemental.pdf)  
>  
>**Then,** let me highlight nine clear evidence.  Then I will explain each piece of evidence in detail.

1. Our methodology, you plagiarize
2. Our Pre-processing, you plagiarize
3. Our Post-processing, you plagiarize
4. Our Inference speedup method,  you plagiarize
5. Our ""tricky"" work, you even plagiarize
6. Our text line detection and text line recognition, you plagiarize
7. Our systematic solution, you plagiarize
8. Our visualization,  you plagiarize
9. Misleading the audiences in order to not be captured for plagiarism

&#x200B;

>To Nassar, Ahmed, Nikolaos Livathinos, Maksym Lysak, and Peter Staar, All you need is not ""TableFormer"", All you need is git clone and torch.load  
>  
>git clone [https://github.com/JiaquanYe/TableMASTER-mmocr](https://github.com/JiaquanYe/TableMASTER-mmocr)  
>  
>torch.load(""our\_pretrained\_model.pth"")   
>  
>You will be the rising stars because you have learnt how to use git clone and torch.load.  
>  
>I attach a step-by-step proof below.

&#x200B;

https://preview.redd.it/gexr11d474891.png?width=2480&format=png&auto=webp&s=2a1fe08a20e044bbef8e021abab17b97acf93101

https://preview.redd.it/t95rgpd474891.png?width=2480&format=png&auto=webp&s=cc345024e531edbbae5ed64c1588e1a7d1e7cc7b

https://preview.redd.it/o3fec4d474891.png?width=2480&format=png&auto=webp&s=89ef3646d0b48a2cc75629248c9503deee156969

https://preview.redd.it/u5ywa5d474891.png?width=2480&format=png&auto=webp&s=896f8c0f81d02021098a95902525266be36bad4c

https://preview.redd.it/ic1y75d474891.png?width=2480&format=png&auto=webp&s=a9e9103b576a66981afaec8f86c00acb31fa6e78

https://preview.redd.it/g6ykdod474891.png?width=2480&format=png&auto=webp&s=5aed8320e990841a7813e62d6359579448c2e32f

https://preview.redd.it/jae7j4d474891.png?width=2480&format=png&auto=webp&s=3f008b1c77254434602217facff99b747d977a7d

https://preview.redd.it/pzyln8d474891.png?width=2480&format=png&auto=webp&s=47de343b99f570ce3dac9be256ccd058bb8f52d6

https://preview.redd.it/bpbub9d474891.png?width=2480&format=png&auto=webp&s=b4eb656dfabc68676cec8c941b7e0034a7d29c25

https://preview.redd.it/7oulcad474891.png?width=2480&format=png&auto=webp&s=934af73376e3948d878c7ad15bdd504b075acb20

https://preview.redd.it/qo2aubd474891.png?width=2480&format=png&auto=webp&s=34821a8530c6da6f63b2552fe10aee94b7770e59

https://preview.redd.it/d0mqhdd474891.png?width=2480&format=png&auto=webp&s=4d7f1c95c1ec051d27758cdbb8f95f522e00c113

https://preview.redd.it/8oc6qgd474891.png?width=2480&format=png&auto=webp&s=50caf4ba63d23650a44bfcd830c7337c781b1ecd

https://preview.redd.it/944c1gd474891.png?width=2480&format=png&auto=webp&s=3fb5817bee6b21e6674c519c8954bd69149b92bd

https://preview.redd.it/c83m2gd474891.png?width=2480&format=png&auto=webp&s=125235df4a28cbff8f57956ec871002a6d7782bd

https://preview.redd.it/72awaid474891.png?width=2480&format=png&auto=webp&s=2ad797b986403d8b413289f778b97508e084ee46

https://preview.redd.it/szd7h3d474891.png?width=2480&format=png&auto=webp&s=3efdd2306814be90600b2da19fb032ea4c24ee4c",145,1047,2022-06-27 12:49:04, d  ibm zurich research plagiarised our paper and got it published on cvpr   is copy texts is plagiarism  copy idea is not plagiarism ,i am xianbiao qi  a computer vision researcher with more than ten years of research experience  i am writing this blog to complain of a serious case of deliberate plagiarism of our paper by the employees from   ibm zurich research  they did not copy texts  they copied the idea          our preprint paper on arxiv is jiaquan ye  xianbiao qi  yelin he  and etc pingan vcgroup s solution for icdar  competition on scientific literature parsing task b  table recognition to html  arxiv preprint arxiv    may  and the code was also released             our paper  ye et al  arxiv     was plagiarised by a team in ibm zurich research    ahmed nassar  nikolaos livathinos  maksym lysak  and peter staar  tableformer  table structure understanding with transformers    in proceedings of the ieee cvf conference on computer vision and pattern recognition  pp     june              nassar et al   s paper plagiarized our method  pre processing  post processing  visualization  inference  systematic solution  code  pretrained models and etc  but did not cite our paper in order to not be captured for the plagiarism   i was notified by many experts in the fields of ocr and table analysis let me explain the plagiarism process step by step    first     let me sort out the timeline         our tablemaster                 we finish the icdar  competition on scientific literature parsing   hosted by another ibm group in australia         https             we released our technical report  a preprint version  on arxiv https             we released our source code  and even our slides on github   https            we released the tablemaster pretrained model             we even released tablemaster _mmocr docker environment       for this project  we were naked  we release all these materials to facilitate the community  but you  plagiarized it               plagiarized tableformer                 the plagiarized tableformer was released on arxiv https       then    let me highlight nine clear evidence   then i will explain each piece of evidence in detail   our methodology  you plagiarize  our pre processing  you plagiarize  our post processing  you plagiarize  our inference speedup method   you plagiarize  our tricky work  you even plagiarize  our text line detection and text line recognition  you plagiarize  our systematic solution  you plagiarize  our visualization   you plagiarize  misleading the audiences in order to not be captured for plagiarism  xb  to nassar  ahmed  nikolaos livathinos  maksym lysak  and peter staar  all you need is not tableformer  all you need is git clone and torch load      git clone  https     torch load our _pretrained _model pth        you will be the rising stars because you have learnt how to use git clone and torch load       i attach a step by step proof below   xb https https https https https https https https https https https https https https https https https   preview redd it szdhd png width  format png auto webp s efddbebdafbeaceec
199,199,rebataur,vmlu60,[p] RestifyML - AI/ML Tool for Developers to quickly experiment with data and generate AI/ML REST API to consume back into their application," Developers can use RestifyML to

* Create DataScience experiments
* Create Data Source and upload CSV data within the experiment
* Do Data Cleansing and Sanitization
* Visualize raw data using Data Exploration
* Select Features which would help in building models
* Build Model, save or export them
* Finally, deploy Model and expose them as REST API
* Consume Machine Learning REST API from any Application
* Profit!

[https://github.com/rebataur/RestifyML](https://github.com/rebataur/RestifyML)

Feedback/ Feature Request appreciated.",6,3,2022-06-28 17:44:46, p  restifyml   ai ml tool for developers to quickly experiment with data and generate ai ml rest api to consume back into their application, developers can use restifyml to  create datascience experiments  create data source and upload csv data within the experiment  do data cleansing and sanitization  visualize raw data using data exploration  select features which would help in building models  build model  save or export them  finally  deploy model and expose them as rest api  consume machine learning rest api from any application  profit  https feedback  feature request appreciated 
200,200,shreyansh26,vmszee,[R] Annotated KDD 2022 paper - Learning Backward Compatible Embeddings," I read a super interesting KDD 2022 paper recently - ""Learning Backward Compatible Embeddings"".

The paper tackles a common industry problem of ensuring compatibility of newer embeddings with an older downstream model.

An annotated version of the paper - [Annotated-ML-Papers/Learning Backward Compatible Embeddings.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Learning%20Backward%20Compatible%20Embeddings.pdf)",0,0,2022-06-28 23:12:26, r  annotated kdd  paper   learning backward compatible embeddings, i read a super interesting kdd  paper recently   learning backward compatible embeddings the paper tackles a common industry problem of ensuring compatibility of newer embeddings with an older downstream model an annotated version of the paper    annotated ml papers learning backward compatible embeddings pdf  https   github com shreyansh annotated ml papers blob main general dl learning backward compatible embeddings pdf 
201,201,cheptsov,vmroly,[D] Run apps and dev environments in the cloud with a single command,"Hi everyone,  


I'm the creator of dstack, a tool that makes it easier to train models in the cloud. Our tool allows extending it with custom providers to support different languages, frameworks, etc.  
All the built-in providers are also open-source. Today, we've released a new update that extends the capabilities of dstack beyond training models, and now also allows users to quickly build and share apps with Streamlit, Gradio, and FastAPI in the cloud – in just a few clicks.  
Similar to apps, it's possible to run dev environments with required hardware and data access also in one command from the Terminal. All you have to do is to link your own AWS account to run commands.  


Invite everyone to read it, and share their thoughts. Happy to discuss the approach and what would be great to have!

Blog post: [https://blog.dstack.ai/introducing-apps-and-dev-environments](https://blog.dstack.ai/introducing-apps-and-dev-environments)  


P.S.: Currently, it's possible to run models and apps only in the configured cloud. If you'd like the tool to also allow you to run it locally, and if you would like this part to be open-source too, please leave comments! 🤗",0,0,2022-06-28 22:15:29, d  run apps and dev environments in the cloud with a single command,hi everyone   i m the creator of dstack  a tool that makes it easier to train models in the cloud  our tool allows extending it with custom providers to support different languages  frameworks  etc   all the built in providers are also open source  today  we ve released a new update that extends the capabilities of dstack beyond training models  and now also allows users to quickly build and share apps with streamlit  gradio  and fastapi in the cloud   in just a few clicks   similar to apps  it s possible to run dev environments with required hardware and data access also in one command from the terminal  all you have to do is to link your own aws account to run commands   invite everyone to read it  and share their thoughts  happy to discuss the approach and what would be great to have blog post   https p s   currently  it s possible to run models and apps only in the configured cloud  if you d like the tool to also allow you to run it locally  and if you would like this part to be open source too  please leave comments   
203,203,programmerChilli,vmabau,[P] First-class Dims - a generalization of einops and named tensors,"Jupyter Notebook: https://colab.research.google.com/drive/1BsVkddtVMX35aZAvo2GyI-wSFPVBCWuA

Github: https://github.com/facebookresearch/torchdim

Some tweet threads about it

Mine: https://twitter.com/cHHillee/status/1541536627746426881

Sasha Rush: https://twitter.com/srush_nlp/status/1541526906113298433",1,15,2022-06-28 06:24:33, p  first class dims   a generalization of einops and named tensors,jupyter notebook  https github  https some tweet threads about itmine  https sasha rush  https   twitter com srush_nlp status 
204,204,-aplusib-,vm5a72,[R] Theoretical Open Research Areas,"Hello everyone,

my goal is to do research in the field of machine learning for motion planning/robotics in general. I'm really interested in the theoretical/mathematical side of the field. However I noticed that the majority of the field consists of very experimental papers where architectures are built and bench-marked without any thorough underlying theory.

So my questions is: Are there any theoretical research areas in machine learning for motion planning/robotics in general?

It would be nice if someone could also give me some labs/researchers working in that direction.

&#x200B;

Thank you very much.",9,16,2022-06-28 02:07:15, r  theoretical open research areas,hello everyone my goal is to do research in the field of machine learning for motion planning robotics in general  i m really interested in the theoretical mathematical side of the field  however i noticed that the majority of the field consists of very experimental papers where architectures are built and bench marked without any thorough underlying theory so my questions is  are there any theoretical research areas in machine learning for motion planning robotics in general it would be nice if someone could also give me some labs researchers working in that direction   xb thank you very much 
205,205,time_waster103,vmjkun,[D] How would you build a scalable platform for Machine Learning for Data Streams,"Recently I came across the idea of online machine learning where the model learns in real time from streaming data. I was wondering what would be an ideal solution be to deal with the varying rate at which data might arrive. More specifically, having a fixed number of VMs in the cloud to train the model in real time might result in underutilization of resources or lack of enough resources. So how should we design an end to end system for such a scenario? Do any of the cloud service providers already have an off the shelf solution for this?",0,0,2022-06-28 15:31:57, d  how would you build a scalable platform for machine learning for data streams,recently i came across the idea of online machine learning where the model learns in real time from streaming data  i was wondering what would be an ideal solution be to deal with the varying rate at which data might arrive  more specifically  having a fixed number of vms in the cloud to train the model in real time might result in underutilization of resources or lack of enough resources  so how should we design an end to end system for such a scenario  do any of the cloud service providers already have an off the shelf solution for this 
206,206,Ierihon_hasty_ai,vmj52i,"[R] Data-centric AI development approach gives us 5,5-8% mAP on PASCAL VOC 2012","How important is clean data for how your AI models perform?

According to our [initial experiments](https://hasty.ai/content-hub/articles/cleaning-pascal-improving-map-by-13) \- very important. Two people improved the primary model metric by 13% in a week using state-of-the-art confidence learning to clean up PASCAL.

In the next iteration, we explore the field a bit deeper, trying to avoid controversy. SPOILER: we still get a nice mAP metric boost.

To learn more about the follow-up results and the plans for the future, check out our article: [https://hasty.ai/content-hub/articles/further-pascal-voc-2012-exploration-and-plans-for-the-future](https://hasty.ai/content-hub/articles/further-pascal-voc-2012-exploration-and-plans-for-the-future)

&#x200B;

Discussion of the initial results:

* [https://www.reddit.com/r/MachineLearning/comments/uc9z2y/p\_we\_cleaned\_up\_pascal\_and\_improved\_map\_by\_13/ia6caqg/?context=3](https://www.reddit.com/r/MachineLearning/comments/uc9z2y/p_we_cleaned_up_pascal_and_improved_map_by_13/ia6caqg/?context=3)
* [https://www.reddit.com/r/computervision/comments/uc9x8t/we\_cleaned\_up\_pascal\_and\_improved\_map\_by\_13/i69iyii/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/computervision/comments/uc9x8t/we_cleaned_up_pascal_and_improved_map_by_13/i69iyii/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

[Disclaimer: We used our own platform to clean up the data and the article, therefore, contains self-promotion. However, the article mainly focuses on the results we achieved.](https://preview.redd.it/5kzxfe440c891.png?width=1978&format=png&auto=webp&s=b2521ee17d6542256e16af94972fc2378476178e)",0,0,2022-06-28 15:02:53, r  data centric ai development approach gives us     map on pascal voc ,how important is clean data for how your ai models perform according to our  initial experiments  https in the next iteration  we explore the field a bit deeper  trying to avoid controversy  spoiler  we still get a nice map metric boost to learn more about the follow up results and the plans for the future  check out our article   https   xb discussion of the initial results    https    https   xb  disclaimer  we used our own platform to clean up the data and the article  therefore  contains self promotion  however  the article mainly focuses on the results we achieved   https   preview redd it kzxfec png width  format png auto webp s beedeaffce 
207,207,fishiwhj,vma4il,[D] How to evaluate the gain of a new feature without training?,"When evaluating the effectiveness of a new feature, it is common to train a model with/without this feature to compare the difference. But sometimes training a model based on huge amounts of data is both time and energy consuming. I was wondering if there are some lightweight ways to estimate the importance of the new feature without training? Computing *descriptive statistics* such as feature coverage, histogram and correlation matrix might be necessary, are there other pre-processing methods?",2,4,2022-06-28 06:14:47, d  how to evaluate the gain of a new feature without training ,when evaluating the effectiveness of a new feature  it is common to train a model with without this feature to compare the difference  but sometimes training a model based on huge amounts of data is both time and energy consuming  i was wondering if there are some lightweight ways to estimate the importance of the new feature without training  computing  descriptive statistics  such as feature coverage  histogram and correlation matrix might be necessary  are there other pre processing methods 
208,208,vigneshwaranpersonal,vm010a,[D] Do you have any suggestions for a crowd-sourced annotation tool?,"We're currently doing research on computational social science, specifically on online toxicity. We have lots of text data, but we don't have annotations. As part of the research, we are thinking of annotating the text using a crowd-sourcing approach. Do any of you know of any open-source tool that we could employ to ease up the process?",5,5,2022-06-27 22:21:01, d  do you have any suggestions for a crowd sourced annotation tool ,we re currently doing research on computational social science  specifically on online toxicity  we have lots of text data  but we don t have annotations  as part of the research  we are thinking of annotating the text using a crowd sourcing approach  do any of you know of any open source tool that we could employ to ease up the process 
209,209,KalloDotIO,vm2px5,[Discussion] [computer vision] Instant NeRF create quality depth maps?," Surprised I haven't seen more chatter about this. What do you  think about Nvidia's instant Nerf which turns 2d into 3d based on these techniques [https://arxiv.org/abs/2003.10016](https://arxiv.org/abs/2003.10016)

Does the output of a NeRF give a depth map that's comparable to what you'd get from a Kinect?

Can these be used to create 3D models one would use in Unreal or Blender?",3,4,2022-06-28 00:16:17, discussion   computer vision  instant nerf create quality depth maps , surprised i haven t seen more chatter about this  what do you  think about nvidia s instant nerf which turns d into d based on these techniques  https does the output of a nerf give a depth map that s comparable to what you d get from a kinect can these be used to create d models one would use in unreal or blender 
212,212,diabulusInMusica,vlr75m,[P] I published a tutorial about ML model deployment,"The deployment of ML models in production is a delicate process filled with challenges. You  can deploy a model via a REST API, on an edge device, or as as an  off-line unit used for batch processing. You can build the deployment  pipeline from scratch, or use ML deployment frameworks. 

In my new mini-series, you'll learn best practices to deploy your ML models. I  try to concentrate everything in 2 videos, to keep the series short and  sweet. 

The first video provides a theoretical overview of ML deployment. You'll learn about:

* Different strategies to deploy ML in production. 
* The main ML deployment tools on the market (TF Serving,  MLFlow Model, Seldon Deploy, KServe from Kubeflow). 
* BentoML and its features.  


Here's the video: [https://www.youtube.com/watch?v=Mrv3CZNWYEg](https://www.youtube.com/watch?v=Mrv3CZNWYEg)",2,19,2022-06-27 14:38:01, p  i published a tutorial about ml model deployment,the deployment of ml models in production is a delicate process filled with challenges  you  can deploy a model via a rest api  on an edge device  or as as an  off line unit used for batch processing  you can build the deployment  pipeline from scratch  or use ml deployment frameworks  in my new mini series  you ll learn best practices to deploy your ml models  i  try to concentrate everything in  videos  to keep the series short and  sweet  the first video provides a theoretical overview of ml deployment  you ll learn about   different strategies to deploy ml in production    the main ml deployment tools on the market  tf serving   mlflow model  seldon deploy  kserve from kubeflow     bentoml and its features   here s the video   https   www youtube com watch v mrvcznwyeg  https   www youtube com watch v mrvcznwyeg 
213,213,icelebratefestivus,vlqu17,[D] Has anyone trained the latent diffusion models by OpenAI(CompVis)? Need some help,"EDIT: It was a problem with the  [sample\_diffusion.py](https://github.com/CompVis/latent-diffusion/blob/main/scripts/sample_diffusion.py) script at line 131.  the np.concatenate method needed an array, which I provided by enclosing the all\_images in \[\] 
`all_img = np.concatenate([all_images], axis=0)` 



I am trying to train a [latent-diffusion](https://github.com/CompVis/latent-diffusion) model by following the instructions on the repo, however I am running into errors while sampling from the checkpointed models. Can someone help?

I am getting Errors while trying to sample using [sample\_diffusion.py ](https://github.com/CompVis/latent-diffusion/blob/main/scripts/sample_diffusion.py)from a custom model trained on LSUN churches: 
```
File ""latent-diffusion/scripts/sample_diffusion.py"", line 309, in <module>run(model, imglogdir, eta=opt.eta,

ValueError: need at least one array to concatenate
```",12,9,2022-06-27 14:12:51, d  has anyone trained the latent diffusion models by openai compvis   need some help,edit  it was a problem with the   sample _diffusion py  https  all_img   np concatenate  all_images   axis    i am trying to train a  latent diffusion  https i am getting errors while trying to sample using  sample _diffusion py   https    file latent diffusion scripts sample_diffusion py  line   in run model  imglogdir  eta opt eta valueerror  need at least one array to concatenate   
214,214,OmOshIroIdEs,vm323i,[R] Can I use whole-protein embeddings on isolated domains?,"I'm interested in studying properties of particular protein domains. One idea is to take advantage of state-of-the-art protein embedding models, such as this, most of which are based on transformers.

Some of the domains I'm studying are found in large proteins, which have multiple other domains in the same chain. Therefore, I believe it might be more informative to obtain embeddings not of each protein as a whole, but just the domains. However, I worry that the embeddings would be all off, since the model expects a complete sequence.

Has anyone tried this before? Are the pre-trained domain-level embeddings?",0,1,2022-06-28 00:30:40, r  can i use whole protein embeddings on isolated domains ,i m interested in studying properties of particular protein domains  one idea is to take advantage of state of the art protein embedding models  such as this  most of which are based on transformers some of the domains i m studying are found in large proteins  which have multiple other domains in the same chain  therefore  i believe it might be more informative to obtain embeddings not of each protein as a whole  but just the domains  however  i worry that the embeddings would be all off  since the model expects a complete sequence has anyone tried this before  are the pre trained domain level embeddings 
215,215,heylibrarian,vlm6yy,[D] State-of-the-art permutation-invariant graph embeddings,"Suppose I have a data set consisting of weighted undirected simple graphs. I would like to learn a vector representation of these graphs. What are the state-of-the-art (2022) architectures/methods for learning such representations? Ideally, the representations are permutation-invariant. For what it's worth, I am only interested in the case where graphs (vertices, edges, and their respective weights) are fully observed; I'm not interested cases unobserved nodes.

An additional requirement is the embedding must have a lower dimension that the number of nodes.",7,12,2022-06-27 09:15:19, d  state of the art permutation invariant graph embeddings,suppose i have a data set consisting of weighted undirected simple graphs  i would like to learn a vector representation of these graphs  what are the state of the art    architectures methods for learning such representations  ideally  the representations are permutation invariant  for what it s worth  i am only interested in the case where graphs  vertices  edges  and their respective weights  are fully observed  i m not interested cases unobserved nodes an additional requirement is the embedding must have a lower dimension that the number of nodes 
216,216,baceituno,vlyjsf,[D] Stack - Seamless data collaboration and versioning,"Hey r/MachineLearning! We are the co-founders of Stack, a hub for data collaboration and versioning. We are developing this tool to help ML teams automatically track changes in their data seamlessly.

We are opening a waiting list for our beta, which we aim to release soon. You can sign up at: https://www.getstack.ai/

We are also actively looking for feedback. Feel free to share any comments or thoughts!",0,1,2022-06-27 21:18:07, d  stack   seamless data collaboration and versioning,hey r machinelearning  we are the co founders of stack  a hub for data collaboration and versioning  we are developing this tool to help ml teams automatically track changes in their data seamlessly we are opening a waiting list for our beta  which we aim to release soon  you can sign up at  https we are also actively looking for feedback  feel free to share any comments or thoughts 
217,217,ykilcher,vlfz1v,[D] Paper Explained - Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Video Analysis),"[https://youtu.be/oz5yZc9ULAc](https://youtu.be/oz5yZc9ULAc)

Minecraft is one of the harder challenges any RL agent could face. Episodes are long, and the world is procedurally generated, complex, and huge. Further, the action space is a keyboard and a mouse, which has to be operated only given the game's video input. OpenAI tackles this challenge using Video PreTraining, leveraging a small set of contractor data in order to pseudo-label a giant corpus of scraped footage of gameplay. The pre-trained model is highly capable in basic game mechanics and can be fine-tuned much better than a blank slate model. This is the first Minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself.

&#x200B;

OUTLINE:

0:00 - Intro

3:50 - How to spend money most effectively?

8:20 - Getting a large dataset with labels

14:40 - Model architecture

19:20 - Experimental results and fine-tuning

25:40 - Reinforcement Learning to the Diamond Pickaxe

30:00 - Final comments and hardware

&#x200B;

Blog: [https://openai.com/blog/vpt/](https://openai.com/blog/vpt/)

Paper: [https://arxiv.org/abs/2206.11795](https://arxiv.org/abs/2206.11795)

Code & Model weights: [https://github.com/openai/Video-Pre-Training](https://github.com/openai/Video-Pre-Training)",0,24,2022-06-27 03:49:05, d  paper explained   video pretraining  vpt   learning to act by watching unlabeled online videos  video analysis , https minecraft is one of the harder challenges any rl agent could face  episodes are long  and the world is procedurally generated  complex  and huge  further  the action space is a keyboard and a mouse  which has to be operated only given the game s video input  openai tackles this challenge using video pretraining  leveraging a small set of contractor data in order to pseudo label a giant corpus of scraped footage of gameplay  the pre trained model is highly capable in basic game mechanics and can be fine tuned much better than a blank slate model  this is the first minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself   xb outline     intro    how to spend money most effectively     getting a large dataset with labels    model architecture    experimental results and fine tuning    reinforcement learning to the diamond pickaxe    final comments and hardware  xb blog   https paper   https code   model weights   https   github com openai video pre training  https   github com openai video pre training 
218,218,curiousML5,vljmxx,[P] Skipgram: neural network instead of lookup table,"I'm looking for papers which use the skipgram model but instead of a lookup table they use a neural network. The use case is instead of sentences of words I want to use sequences of human behavior where additional information is available, e.g. think sequences of visited Amazon products. Cold-start also happens to be very common and I'm thinking that using a neural network instead of lookup embeddings table would be better.

Updated with more context:

The typical usage of skip gram is for learning word embedding as in text where each word has an embedding which is learned through skipgram. However there is nothing limiting the usage of skipgram for non-text cases.

A popular way to use skipgram in i2i recommendation systems is to treat a session of products browsed by the user as a sequence and to have an embedding per product. (Eg see KDD 2018 winning paper from Airbnb) However, the question I have here is instead of having one embedding per product can we instead use a neural network where the output layer is the embedding layer. This way we can backprop through the neural network. The reason is we have more information for products than we do for words",4,5,2022-06-27 06:58:56, p  skipgram  neural network instead of lookup table,i m looking for papers which use the skipgram model but instead of a lookup table they use a neural network  the use case is instead of sentences of words i want to use sequences of human behavior where additional information is available  e g  think sequences of visited amazon products  cold start also happens to be very common and i m thinking that using a neural network instead of lookup embeddings table would be better updated with more context the typical usage of skip gram is for learning word embedding as in text where each word has an embedding which is learned through skipgram  however there is nothing limiting the usage of skipgram for non text cases a popular way to use skipgram in ii recommendation systems is to treat a session of products browsed by the user as a sequence and to have an embedding per product   eg see kdd  winning paper from airbnb  however  the question i have here is instead of having one embedding per product can we instead use a neural network where the output layer is the embedding layer  this way we can backprop through the neural network  the reason is we have more information for products than we do for words
220,220,WigglyHypersurface,vlj0py,"[D] For perciever (IO) with single-channel audio, are position encodings even necessary?","I've been looking into using the Perciever for a project that involves single-channel (mono) audio. From the existing implementations and tutorials, I can't find one that only does audio. It seems like in the papers they rearrange the audio into patches and add position encodings, but this is a hack to bring the audio modality into the same size tensor as other modalities. If only using 1d audio is there any need at all for position encodings at all?",6,5,2022-06-27 06:26:12, d  for perciever  io  with single channel audio  are position encodings even necessary ,i ve been looking into using the perciever for a project that involves single channel  mono  audio  from the existing implementations and tutorials  i can t find one that only does audio  it seems like in the papers they rearrange the audio into patches and add position encodings  but this is a hack to bring the audio modality into the same size tensor as other modalities  if only using d audio is there any need at all for position encodings at all 
222,222,leboulevardier,vldjt5,[D] How to not commit code copyright violation with Github Co-pilot?,"At our work place, many of our ML researchers are starting to use Github Co-pilot to save time. Issue is there is no provenance on the code generated by Co-pilot. If I understand correctly, Co-pilot is trained on public GitHub repositories, many of which might have specific copyright and license clauses. Our research, when published, would also put the code on Github publicly.

What would you suggest to prevent potential code copyright violation in this case? I have sent request for Github to provide provenance tracking feature but I assume that's gonna take a while to implement (that is, if they decide to implement it). Are you using Github Co-pilot and worrying about similar issues?",6,4,2022-06-27 01:54:11, d  how to not commit code copyright violation with github co pilot ,at our work place  many of our ml researchers are starting to use github co pilot to save time  issue is there is no provenance on the code generated by co pilot  if i understand correctly  co pilot is trained on public github repositories  many of which might have specific copyright and license clauses  our research  when published  would also put the code on github publicly what would you suggest to prevent potential code copyright violation in this case  i have sent request for github to provide provenance tracking feature but i assume that s gonna take a while to implement  that is  if they decide to implement it   are you using github co pilot and worrying about similar issues 
223,223,huehue9812,vl31d5,GMM latent space [D]," 

Hi, I would love to know if there is any ongoing work (or the latest) on mixture of Gaussians as latent space for GANs, or other generative models.

Does anyone have any experience on it and/or opinions on why it is not popular? (or doesn't work)",3,6,2022-06-26 17:20:39,gmm latent space  d , hi  i would love to know if there is any ongoing work  or the latest  on mixture of gaussians as latent space for gans  or other generative models does anyone have any experience on it and or opinions on why it is not popular   or doesn t work 
224,224,OddSandwich969,vl45a2,[Discussion] Doubt regarding text vector difference image manipulation method of Dalle-2.,"I was going through the (updated)paper, there was this image manipulation method through text difference.
It went like this:

z_i := original image CLIP embedding

z_t := new text CLIP embedding/ embedding of the text for current image manipulation

z_t0 := orignal image's corresponding text CLIP embedding/ text embedding of the text 'a photo' / empty embedding

z_d := l2_norm(z_t - z_t0) <-> text difference vector | 
Here l2_norm means, normalising a vector by dividing it with it's norm_p (here norm 2).

z_new /z_theta :=  spherical_interpolation(z_i, z_d, theta) {where theta is between (0,0.5)} <-> new image's CLIP embedding vector 


What I don't understand is, that the CLIP img and text embedding vectors are supposed to be similar vectors (since trained with cosine similarity), and the difference between text embedding vectors of two similar texts will be somewhat perpendicular to either of the text vectors, therefore the text diff vector should be very different from the image embedding, and hence the spherical interpolation shouldn't give any meaningful result.

What am I missing? I am unable to understand why this text difference method works.",6,3,2022-06-26 18:26:01, discussion  doubt regarding text vector difference image manipulation method of dalle  ,i was going through the  updated paper  there was this image manipulation method through text difference it went like this z_i    original image clip embeddingz_t    new text clip embedding  embedding of the text for current image manipulationz_t    orignal image s corresponding text clip embedding  text embedding of the text  a photo    empty embeddingz_d    l_norm z_t   z_t   text difference vector   here l_norm means  normalising a vector by dividing it with it s norm_p  here norm   z_new  z_theta     spherical_interpolation z_i  z_d  theta   where theta is between        new image s clip embedding vector what i don t understand is  that the clip img and text embedding vectors are supposed to be similar vectors  since trained with cosine similarity   and the difference between text embedding vectors of two similar texts will be somewhat perpendicular to either of the text vectors  therefore the text diff vector should be very different from the image embedding  and hence the spherical interpolation shouldn t give any meaningful result what am i missing  i am unable to understand why this text difference method works 
225,225,DouBlindDotCOM,vlanqv,[D] Will this mode work for practicing paper reviews? Can we get in-depth feedback on our draft?," Some opinions were collected about mocking ML paper reviews. Link to the thread: [https://www.reddit.com/r/MachineLearning/comments/u967sy/d\_opinions\_needed\_anyone\_interested\_in\_mock\_peer/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/MachineLearning/comments/u967sy/d_opinions_needed_anyone_interested_in_mock_peer/?utm_source=share&utm_medium=web2x&context=3)

To summarize, many people are interested. Opinions are in common that:

1. People like private review rather than public review
2. Number of papers to review are not a concern but every couple months will be a good pace
3. Plagiarism and stealing are of course the biggest concern

To address this, I suggest the following mode:

1. ONLY opens for people who want to exchange paper reviews. Enthusiastic reviewers with no paper draft to be reviewed can wait.
2. ONLY opens for people who are really interested in mocking paper review prior to formal journal/conference submission. Join a Discord community (already established).
3. In the PRIVATE ""Introduce yourself"" channel, people introduce themselves using true information and offer a very brief paper abstract and ML category.
4. Chat openly or privately to find the right review partners
5. In the ""paper-review-exchange"" channel, announce your paper reviewer upon agreement (from both sides)
6. Exchange your drafts privately and preferably with official email addresses
7. (Optional) When the review work is done, announce it too.
8. Note that plagiarism and stealing can be minimized in this mode but still could happen.

When conference reviews do not offer much nowadays, a mockup review might give your more TRUE inputs. Good luck!",2,0,2022-06-26 23:38:07, d  will this mode work for practicing paper reviews  can we get in depth feedback on our draft , some opinions were collected about mocking ml paper reviews  link to the thread   https to summarize  many people are interested  opinions are in common that   people like private review rather than public review  number of papers to review are not a concern but every couple months will be a good pace  plagiarism and stealing are of course the biggest concernto address this  i suggest the following mode   only opens for people who want to exchange paper reviews  enthusiastic reviewers with no paper draft to be reviewed can wait   only opens for people who are really interested in mocking paper review prior to formal journal conference submission  join a discord community  already established    in the private introduce yourself channel  people introduce themselves using true information and offer a very brief paper abstract and ml category   chat openly or privately to find the right review partners  in the paper review exchange channel  announce your paper reviewer upon agreement  from both sides   exchange your drafts privately and preferably with official email addresses   optional  when the review work is done  announce it too   note that plagiarism and stealing can be minimized in this mode but still could happen when conference reviews do not offer much nowadays  a mockup review might give your more true inputs  good luck 
226,226,AshkanF,vlacrj,[R] Can explainability improve model accuracy?,"&#x200B;

https://preview.redd.it/okh7r16770891.jpg?width=1200&format=pjpg&auto=webp&s=9f0fe7605453a945682d27eab65d866dce3f126c

Black-box Deep learning models are mostly uninterpretable and far too complex.

• One strategy is to learn the nonlinear relation of input features.

However, there are so many features to learn from.

https://preview.redd.it/muotby5s70891.png?width=782&format=png&auto=webp&s=1cbc3dece747d061e3ab96dea8b309c3fae5b8ce

&#x200B;

• Research shows a set of important features can improve the learning process.

Therefore, we can focus on the most correlated features.

• Paper📜: [https://arxiv.org/abs/2203.04383](https://arxiv.org/abs/2203.04383)",1,1,2022-06-26 23:24:18, r  can explainability improve model accuracy ,  xb https black box deep learning models are mostly uninterpretable and far too complex   one strategy is to learn the nonlinear relation of input features however  there are so many features to learn from https   xb   research shows a set of important features can improve the learning process therefore  we can focus on the most correlated features   paper    https   arxiv org abs    https   arxiv org abs   
227,227,Ok-Seesaw9702,vl2r7q,[D] Derivation of path dependent attribution in Tree SHAP,"I was reading the TreeSHAP paper by Lundberg & Lee. There they propose that every path can be considered an individual model and due to additivity property of SHAP we can directly add the attributions for each path and that would give us the attribution for that tree.

I can understand till -

1. if a feature doesn't lie on the path then that feature's attribution for that path would be zero.
2. if feature lies on the path and also lies on the path of Xf then it's attribution is positive.
3. if feature lies on the path but doesn't lie on the path covered by Xf then attribution is negative.

But I can't get my head around the quantification of these contributions - especially the weighting.i.e., **POS = W(|Sp|-1, |Np|)\*v** ;  
**NEG = -W(|Sp, |Np|)\*v** ; where v is the leaf's update.

I have may questions, but to begin with, can someone please help me understand how do we get these attribution values ?",0,2,2022-06-26 17:02:53, d  derivation of path dependent attribution in tree shap,i was reading the treeshap paper by lundberg   lee  there they propose that every path can be considered an individual model and due to additivity property of shap we can directly add the attributions for each path and that would give us the attribution for that tree i can understand till    if a feature doesn t lie on the path then that feature s attribution for that path would be zero   if feature lies on the path and also lies on the path of xf then it s attribution is positive   if feature lies on the path but doesn t lie on the path covered by xf then attribution is negative but i can t get my head around the quantification of these contributions   especially the weighting i e     pos   w  sp     np    v        neg    w  sp   np    v     where v is the leaf s update i have may questions  but to begin with  can someone please help me understand how do we get these attribution values  
228,228,MurlocXYZ,vkp3y2,[D] Is it time to retire the FID?,"I know the main metric used to measure the quality of generative models is the FID. However, it seems to me that some problems arise when evaluating a generative model using another model. A couple that come to mind:
- Inception v3 itself is 7 years old at this point. Nowadays, we have models with much higher ImageNet classification accuracy, which presumably translates to better internal representations. Why are we still using Inception v3 instead of, for instance, ViT or some more recent model. 

- The ImageNet dataset that is commonly used to pretrain the Inceptionv3, while being quite comprehensive, is still limited to 1000 classes. If I want to train a model to generate classes that are semantically distant from ImageNet classes, what guarantees do I have that the activations of Inceptionv3 will be meaningful? This is more so problematic with models like DALL-E, which are trained on much larger datasets and can generate from the open set, essentially.

Perhaps I am misinterpreting things, but it seems to me that the FID is a case of ""good enough"" that sort of stuck around.

What are your thoughts?",7,26,2022-06-26 03:09:05, d  is it time to retire the fid ,i know the main metric used to measure the quality of generative models is the fid  however  it seems to me that some problems arise when evaluating a generative model using another model  a couple that come to mind   inception v itself is  years old at this point  nowadays  we have models with much higher imagenet classification accuracy  which presumably translates to better internal representations  why are we still using inception v instead of  for instance  vit or some more recent model    the imagenet dataset that is commonly used to pretrain the inceptionv  while being quite comprehensive  is still limited to  classes  if i want to train a model to generate classes that are semantically distant from imagenet classes  what guarantees do i have that the activations of inceptionv will be meaningful  this is more so problematic with models like dall e  which are trained on much larger datasets and can generate from the open set  essentially perhaps i am misinterpreting things  but it seems to me that the fid is a case of good enough that sort of stuck around what are your thoughts 
229,229,RodObr,vky2yx,[D] Sequence Modelling Technique,"Let's say we have a time series problem where we are trying to use past information to predict future inputs. Like stock prices, or heart rates, or a language model that receives one word at a time.

In theory you would want each output at t to contain the maximum amount of predictive information about label t+1.

Let's say you attach a second network to this RNN, which tries to predict hidden state t+1 from hidden state t and add it's error as an auxiliary loss. You could call it a ""Lookahead reconstruction loss""

I believe this should make the RNN learn in a way that maximises future understanding of the network.

Has anybody experimented with this technique, or read about implementations on this?

I'd be interested in hearing opinions from fellow practitioners.",4,5,2022-06-26 11:41:34, d  sequence modelling technique,let s say we have a time series problem where we are trying to use past information to predict future inputs  like stock prices  or heart rates  or a language model that receives one word at a time in theory you would want each output at t to contain the maximum amount of predictive information about label t  let s say you attach a second network to this rnn  which tries to predict hidden state t  from hidden state t and add it s error as an auxiliary loss  you could call it a lookahead reconstruction lossi believe this should make the rnn learn in a way that maximises future understanding of the network has anybody experimented with this technique  or read about implementations on this i d be interested in hearing opinions from fellow practitioners 
230,230,anvinhnd,vl65tk,[R] [D] How can one rigorously and efficiently deal with binary classification problems on multi-label data?,"To be clearer, I'd like to start learning about some techniques or the literature about this particular type of binary classification problems. Please share if you happen to know about this (keywords, links, articles, etc are all appreciated).

So, the problem is supervised binary classification. In general, there is nothing special about the dataset apart from the fact that the train/val data from one of the 2 label classes (from now on, let's say it's negative) are already further labeled into multiple subclasses. From there, the problem has an additional goal (other than binary classification): to maximize the number of subclasses that are classified well by the model. By ""classified well"", I mean that, for example, if one restricts the negative side of the dataset into one of such subclasses, the performance of the model is higher than some close-to-perfect thresholds.

Furthermore, there might be some complications in both ways: there might be some subclasses that are easy to classify by the model, and there might be some subclasses that are impossible to classify by the model (e.g. XOR problem with linear classifiers). The key here is that, in the end, at test time, one should only use one ""small"" (relatively of course) ""model"" (a combination of shallow neural nets is OK too) to classify all testing data.

Additionally, I'm open to learn about stuffs beyond the supervised paradigm.",0,0,2022-06-26 20:10:28, r   d  how can one rigorously and efficiently deal with binary classification problems on multi label data ,to be clearer  i d like to start learning about some techniques or the literature about this particular type of binary classification problems  please share if you happen to know about this  keywords  links  articles  etc are all appreciated  so  the problem is supervised binary classification  in general  there is nothing special about the dataset apart from the fact that the train val data from one of the  label classes  from now on  let s say it s negative  are already further labeled into multiple subclasses  from there  the problem has an additional goal  other than binary classification   to maximize the number of subclasses that are classified well by the model  by classified well  i mean that  for example  if one restricts the negative side of the dataset into one of such subclasses  the performance of the model is higher than some close to perfect thresholds furthermore  there might be some complications in both ways  there might be some subclasses that are easy to classify by the model  and there might be some subclasses that are impossible to classify by the model  e g  xor problem with linear classifiers   the key here is that  in the end  at test time  one should only use one small  relatively of course  model  a combination of shallow neural nets is ok too  to classify all testing data additionally  i m open to learn about stuffs beyond the supervised paradigm 
231,231,FitWin7383,vkkfv0,[P] Frechet Inception Distance,"I'm currently looking into quantifying GANS and from my current understanding, the way to go is the FID (Frechet Inception Distance) as a key metric. I read into it and have a basic understanding of how it works based on comparing the feature vectors of the Inception Model. In all the tutorials, I saw detailed implementation but they stopped after computing an FID between two images.

In all of the papers, I saw there is one FID score used to compare entire GAN architectures and I'm a bit lost about how many images they generate to compare and whether images generated get randomly paired for an average FID score.

 

TL;DR: The procedure behind comparing GAN architectures is unclear to me based on the FID.",16,44,2022-06-25 23:22:23, p  frechet inception distance,i m currently looking into quantifying gans and from my current understanding  the way to go is the fid  frechet inception distance  as a key metric  i read into it and have a basic understanding of how it works based on comparing the feature vectors of the inception model  in all the tutorials  i saw detailed implementation but they stopped after computing an fid between two images in all of the papers  i saw there is one fid score used to compare entire gan architectures and i m a bit lost about how many images they generate to compare and whether images generated get randomly paired for an average fid score  tl dr  the procedure behind comparing gan architectures is unclear to me based on the fid 
232,232,QadriShyaari,vl8rxu,[D] Clarification question related to prompting,What is the difference between prompt engineering and prompt learning? I recently heard a talk where the presenter said that ‘we freeze the parameters of the model and only do prompt learning’. To me that seems like engineering than learning.,2,0,2022-06-26 22:11:30, d  clarification question related to prompting,what is the difference between prompt engineering and prompt learning  i recently heard a talk where the presenter said that  we freeze the parameters of the model and only do prompt learning   to me that seems like engineering than learning 
233,233,yapoinder,vklemr,Is there any way of using a text editor with Kaggle or Google Colab notebooks? [Discussion],"UPDATE: SOLVED

The lovely people in the comments guided me to a better method of using github and cloning my repository in the kaggle runtime using the !git clone command. I was unaware you could clone a github repository and run a python file in this method. I was even able to create an anaconda environment and run everything smoothly. So everything is running smoothly again :D <3 <3 :D

&#x200B;

\-------------

I am training a video classification neural network which involves opencv based image augmentation and then after the training completes I run a series of test with my test datasets.

so with all of the functionality the code base is close to 6k lines of code.

This is really hard to work with in the current notebook cell format, if I want to make any changes I have to scroll a lot and often I get confused since my python Classes are thousands of lines each with many functions built in.

Using an editor like VSCODE is 10000x times easier than working with notebooks.

Has anyone figured this one out?

Yes I realize I can work in VSCODE on my local computer and then manually transfer the code to kaggle, but this is incredibly tedious when making small changes to file paths and general code changes.

Im shocked there isnt a better way around this !!! I mean c'mon how do we expect AI to be adopted by the masses if we cant have a streamlined way of developing software?

I guess the alternative is to buy a $6000 GPU and build a pc lol, i'm a broke student paying off student debt :( I am grateful for the free GPU with Kaggle,

I JUST WANT A SIMPLE TEXT EDITOR... is that too much to ask?",10,25,2022-06-26 00:08:49,is there any way of using a text editor with kaggle or google colab notebooks   discussion ,update  solvedthe lovely people in the comments guided me to a better method of using github and cloning my repository in the kaggle runtime using the  git clone command  i was unaware you could clone a github repository and run a python file in this method  i was even able to create an anaconda environment and run everything smoothly  so everything is running smoothly again  d      d  xb               i am training a video classification neural network which involves opencv based image augmentation and then after the training completes i run a series of test with my test datasets so with all of the functionality the code base is close to k lines of code this is really hard to work with in the current notebook cell format  if i want to make any changes i have to scroll a lot and often i get confused since my python classes are thousands of lines each with many functions built in using an editor like vscode is x times easier than working with notebooks has anyone figured this one out yes i realize i can work in vscode on my local computer and then manually transfer the code to kaggle  but this is incredibly tedious when making small changes to file paths and general code changes im shocked there isnt a better way around this     i mean c mon how do we expect ai to be adopted by the masses if we cant have a streamlined way of developing software i guess the alternative is to buy a   gpu and build a pc lol  i m a broke student paying off student debt    i am grateful for the free gpu with kaggle i just want a simple text editor    is that too much to ask 
234,234,XinshaoWang,vkc7fo,"[Research] Not all our papers get published, therefore it is enjoyable to see our released papers become a true foundation for other works","I read a post in linkedin (see links at the end) and find  
a similar case on our side: “Not all our papers get published, therefore it is enjoyable to see our released papers become a true foundation for other works”.  


Our work: 

(1) IMAE demonstrates a robust loss could be unbounded, asymmetric; 

(2) Derivative Manipulation proposes gradient normalisation and emphasis density functions.  
\* IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters: [https://arxiv.org/pdf/1903.12141.pdf](https://arxiv.org/pdf/1903.12141.pdf)  
\* Derivative Manipulation for General Example Weighting: [https://arxiv.org/pdf/1905.11233.pdf](https://arxiv.org/pdf/1905.11233.pdf)  


The following works:

* ICML-20: Normalized Loss Functions for Deep Learning with Noisy  
Labels: [http://proceedings.mlr.press/v119/ma20c/ma20c.pdf](http://proceedings.mlr.press/v119/ma20c/ma20c.pdf)
* ICML-21: Asymmetric Loss Functions for Learning with Noisy Labels [https://proceedings.mlr.press/v139/zhou21f](https://proceedings.mlr.press/v139/zhou21f)

&#x200B;

More details and original source: 

* [https://www.linkedin.com/posts/xinshaowang\_the-probabilistic-normal-epipolar-constraint-activity-6944535197044367360-jpu5?utm\_source=linkedin\_share&utm\_medium=member\_desktop\_web](https://www.linkedin.com/posts/xinshaowang_the-probabilistic-normal-epipolar-constraint-activity-6944535197044367360-jpu5?utm_source=linkedin_share&utm_medium=member_desktop_web)
* [https://www.linkedin.com/posts/laurent-kneip-72518658\_the-probabilistic-normal-epipolar-constraint-activity-6944331307514531840-vQb1?utm\_source=linkedin\_share&utm\_medium=member\_desktop\_web](https://www.linkedin.com/posts/laurent-kneip-72518658_the-probabilistic-normal-epipolar-constraint-activity-6944331307514531840-vQb1?utm_source=linkedin_share&utm_medium=member_desktop_web)",8,93,2022-06-25 16:02:53, research  not all our papers get published  therefore it is enjoyable to see our released papers become a true foundation for other works,i read a post in linkedin  see links at the end  and find  a similar case on our side   not all our papers get published  therefore it is enjoyable to see our released papers become a true foundation for other works    our work     imae demonstrates a robust loss could be unbounded  asymmetric     derivative manipulation proposes gradient normalisation and emphasis density functions      imae for noise robust learning  mean absolute error does not treat examples equally and gradient magnitude s variance matters   https    derivative manipulation for general example weighting   https the following works   icml   normalized loss functions for deep learning with noisy  labels   http   icml   asymmetric loss functions for learning with noisy labels  https   xb more details and original source     https    https   www linkedin com posts laurent kneip  _the probabilistic normal epipolar constraint activity  vqb utm _source linkedin _share utm _medium member _desktop _web  https   www linkedin com posts laurent kneip _the probabilistic normal epipolar constraint activity  vqb utm_source linkedin_share utm_medium member_desktop_web 
236,236,e2v-sde-parody,vjkssf,[D] How to copy text from more than 10 previously published papers and get accepted to CVPR 2022,"Hey, check out our (!) video (parody) that presents how our E2V-SDE paper (that has been accepted to CVPR 2022) largely consists of texts that are uncredited verbatim copies from more than 10 previously published papers. Enjoy!

&#x200B;

[https://youtube.com/watch?v=UCmkpLduptU](https://youtube.com/watch?v=UCmkpLduptU)",95,481,2022-06-24 15:26:42, d  how to copy text from more than  previously published papers and get accepted to cvpr ,hey  check out our     video  parody  that presents how our ev sde paper  that has been accepted to cvpr   largely consists of texts that are uncredited verbatim copies from more than  previously published papers  enjoy   xb  https   youtube com watch v ucmkplduptu  https   youtube com watch v ucmkplduptu 
237,237,yekitra,vjyihq,[D] What are the interesting SOTA models released in CVPR 2022?,"Hi Reddit,

Since the CVPR 2022 is wrapped up today and I've not tracked what happened this year. 

What are the interesting releases of this year that I should be looking at?

What new SOTA models are released?

Thanks",20,34,2022-06-25 02:39:23, d  what are the interesting sota models released in cvpr  ,hi reddit since the cvpr  is wrapped up today and i ve not tracked what happened this year  what are the interesting releases of this year that i should be looking at what new sota models are released thanks
238,238,Realistic_Ad_8107,vka648,[P] Synthetic Images Anomaly Detection with CLIP,"You have just generated a bunch of synthetic images by your favorite generative model. Most of them look great, but some looks really bad. These are outliers. Since GAN, the most popular generative model structure, doesn’t produce a likelihood score for generated images, you can not know which of the images generated by it are outliers.

With the following method, you can inspect your synthetic dataset more efficiently than by just looking at all images.

First blog post on Medium. Let me know what you think.

&#x200B;

https://preview.redd.it/1bq8cmm29q791.png?width=260&format=png&auto=webp&s=5aa2b82e1f1bb4edd64d3f7658415dde1573e2ee

[Synthetic Images Anomaly Detection with CLIP](https://medium.com/p/e4fdf6af0169)",2,1,2022-06-25 13:35:43, p  synthetic images anomaly detection with clip,you have just generated a bunch of synthetic images by your favorite generative model  most of them look great  but some looks really bad  these are outliers  since gan  the most popular generative model structure  doesn t produce a likelihood score for generated images  you can not know which of the images generated by it are outliers with the following method  you can inspect your synthetic dataset more efficiently than by just looking at all images first blog post on medium  let me know what you think   xb https  synthetic images anomaly detection with clip  https   medium com p efdfaf 
240,240,daichrony,vjymjx,"[D] Is it possible to make a model that will outperform a human, if the model was solely trained on that human's prior predictions?","Say a single radiologist has a ton of images that they have labeled cancer / not cancer. Can we use the labels and those images from just the one radiologist to make a model that will be better at predicting cancer / not cancer than the radiologist? 

Intuitively it seems like that would not be possible unless by chance it does better, but ML/DL has a way of being able to extrapolate/generalize patterns and sometimes spot things we missed? Perhaps an ensemble of various models, or maybe that would just lead to overfitting? 

No particular application, just a random question I had been pondering. Appreciate any thoughts and/or references.",22,20,2022-06-25 02:44:41, d  is it possible to make a model that will outperform a human  if the model was solely trained on that human s prior predictions ,say a single radiologist has a ton of images that they have labeled cancer   not cancer  can we use the labels and those images from just the one radiologist to make a model that will be better at predicting cancer   not cancer than the radiologist  intuitively it seems like that would not be possible unless by chance it does better  but ml dl has a way of being able to extrapolate generalize patterns and sometimes spot things we missed  perhaps an ensemble of various models  or maybe that would just lead to overfitting  no particular application  just a random question i had been pondering  appreciate any thoughts and or references 
241,241,mrwafflezzz,vk9gud,[P] Oddly thresholded confidence scores on scaled yolov4 csp,"All object detections on the scaled yolov4 csp model have a confidence below 0.5, while it should range from 0 to 1. Does anything come to mind as to what the problem might be?

Info:

* I'm using a branch of the [author's PyTorch repo](https://github.com/WongKinYiu/ScaledYOLOv4/tree/yolov4-csp)
* Predictions are otherwise pretty good in terms of bbox placement
* I'm training on a single gpu
* Darknet coco weights are converted to "".pt"" PyTorch weights for training
* A custom dataset is used with a single prediction class
* Data is augmented before training starts, most of the dataloader's data augmentation is disabled

EDIT: The culprit was the single class dataset. The solution is adding 2 lines of code:

* [https://github.com/WongKinYiu/ScaledYOLOv4/pull/297](https://github.com/WongKinYiu/ScaledYOLOv4/pull/297)

The total confidence for a bounding box is a multiplication of box confidence and classification confidence. Classification weights are initialized around 0 with a very small standard deviation. These weights, however, are never updated in a single class object detector. Combine that with a sigmoidal activation function and you get a classification confidence of around 0.5 at an input around 0. The total confidence score for a box will thus rarely exceed 0.5. The solution is to set the classification confidence to 1 for inference, leaving only the box confidence.",0,1,2022-06-25 12:47:42, p  oddly thresholded confidence scores on scaled yolov csp,all object detections on the scaled yolov csp model have a confidence below    while it should range from  to   does anything come to mind as to what the problem might be info   i m using a branch of the  author s pytorch repo  https   predictions are otherwise pretty good in terms of bbox placement  i m training on a single gpu  darknet coco weights are converted to  pt pytorch weights for training  a custom dataset is used with a single prediction class  data is augmented before training starts  most of the dataloader s data augmentation is disablededit  the culprit was the single class dataset  the solution is adding  lines of code    https the total confidence for a bounding box is a multiplication of box confidence and classification confidence  classification weights are initialized around  with a very small standard deviation  these weights  however  are never updated in a single class object detector  combine that with a sigmoidal activation function and you get a classification confidence of around   at an input around   the total confidence score for a box will thus rarely exceed    the solution is to set the classification confidence to  for inference  leaving only the box confidence 
242,242,zy415,vjqdom,[D] Niche ML Venues vs Top ML Conferences,"Since top ML conferences (e.g. NeurIPS, ICML, AISTATS, UAI, ICLR) are getting too large, there are quite some niche venues focusing on different subfields of ML:
- Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM): https://rldm.org/
- Machine Learning for Health (ML4H): https://ml4health.github.io/
- Learning on Graphs Conference (LoG): https://logconference.org/
- Symposium on Advances in Approximate Bayesian Inference (AABI): http://approximateinference.org/
- International Conference on Automated Machine Learning (AutoML-Conf): https://automl.cc/
- Conference on Causal Learning and Reasoning (CLeaR): https://www.cclear.cc/
- Conference on Lifelong Learning Agents (CoLLAs): https://lifelong-ml.cc/

Some of these conferences are quite new and grew out of different workshops. Many of them are trying to establish themselves as top venues in their niche fields. Here, I would like to get some opinions from the ML folks. Could folks comment on these conferences, e.g., based on different dimensions?
- **Prestige**: Are these conferences perceived to be as pretigious as the top ML conferences? 
- **Usefulness**: Does hiring committee in the academia and industry treat these conferences the same as top ML conferences? If not, how much will the nich conferences be discounted? (Closely tied to prestige, though)
- **Dissemination**: Are papers at these niche conferences much less visible to researchers outside the subfields? (This seems important to me because research nowadays often leverages ideas from different fields.)
- **Difficulty**: Is it easier to get papers accepted at these conferences as compared to top ML coneferences?
- **Networking**: Is there really more opportunity to get to know folks working in the same subfields at these conferences (given that it is much smaller)?

Disclaimer:
- I have published several papers in the top ML conferences listed above, and am considering whether to try out niche ML conferences. Personally, this feels like a ""bet"" for me on whether the niche conference will be successful in the future.
- I know some folks might comment that the quality of research is the most important as compared to the publication venues. However, let's for now assume all things being equal and that, e.g., a graduate student is deciding whether to submit a paper to a general ML conference or a niche venue.",16,30,2022-06-24 20:28:33, d  niche ml venues vs top ml conferences,since top ml conferences  e g  neurips  icml  aistats  uai  iclr  are getting too large  there are quite some niche venues focusing on different subfields of ml   multi disciplinary conference on reinforcement learning and decision making  rldm   https   machine learning for health  mlh   https   learning on graphs conference  log   https   symposium on advances in approximate bayesian inference  aabi   http   international conference on automated machine learning  automl conf   https   conference on causal learning and reasoning  clear   https   conference on lifelong learning agents  collas   https some of these conferences are quite new and grew out of different workshops  many of them are trying to establish themselves as top venues in their niche fields  here  i would like to get some opinions from the ml folks  could folks comment on these conferences  e g   based on different dimensions     prestige    are these conferences perceived to be as pretigious as the top ml conferences      usefulness    does hiring committee in the academia and industry treat these conferences the same as top ml conferences  if not  how much will the nich conferences be discounted   closely tied to prestige  though     dissemination    are papers at these niche conferences much less visible to researchers outside the subfields   this seems important to me because research nowadays often leverages ideas from different fields      difficulty    is it easier to get papers accepted at these conferences as compared to top ml coneferences     networking    is there really more opportunity to get to know folks working in the same subfields at these conferences  given that it is much smaller  disclaimer   i have published several papers in the top ml conferences listed above  and am considering whether to try out niche ml conferences  personally  this feels like a bet for me on whether the niche conference will be successful in the future   i know some folks might comment that the quality of research is the most important as compared to the publication venues  however  let s for now assume all things being equal and that  e g   a graduate student is deciding whether to submit a paper to a general ml conference or a niche venue 
243,243,Gramious,vjkujp,"[D] ""The uncanny valley demonstrating it's treasures and failures, studio lighting digital art"", DALLE-2 prompt. An artist friend has recently been given access and I was trying to feed him prompts that 'broke' the system (e.g., Gaussian noise, one million colours, uncanny valley, etc.).","I had some fun with DALL-E 2 last night because a friend of mine ([instagram.com/photonwind/](https://instagram.com/photonwind/)) was given access last night and was streaming, letting us feed it prompts. I wanted to break the system, find its edges, or give prompts that gave me insight into the underlying function being modelled.   


I tried: ""Gaussian noise"", ""One million colours"" and ""The uncanny valley demonstrating it's treasures and failures, studio lighting digital art"". The latter looks the most interesting to me:  


[The uncanny valley demonstrating it's treasures and failures, studio lighting digital art](https://preview.redd.it/d1acsi05lj791.jpg?width=1024&format=pjpg&auto=webp&s=f0eecc93caee7b93a470777a0fff4bde5dc71a88)

That said, ""One million colours"" is pretty epic too:  


[One million colours](https://preview.redd.it/2tk54r99lj791.png?width=3072&format=png&auto=webp&s=7ecad24840e5433822a06ce7b83d193ab0a20945)

But, Gaussian noise is just broken:  


[Gaussian noise](https://preview.redd.it/tkx2zi2clj791.png?width=2048&format=png&auto=webp&s=d517f44097150850289849465649d87586c83277)",4,41,2022-06-24 15:30:07, d  the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  dalle  prompt  an artist friend has recently been given access and i was trying to feed him prompts that  broke  the system  e g   gaussian noise  one million colours  uncanny valley  etc   ,i had some fun with dall e  last night because a friend of mine   instagram com photonwind   https i tried  gaussian noise  one million colours and the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  the latter looks the most interesting to me    the uncanny valley demonstrating it s treasures and failures  studio lighting digital art  https that said  one million colours is pretty epic too    one million colours  https but  gaussian noise is just broken    gaussian noise  https   preview redd it tkxziclj png width  format png auto webp s dfdc 
244,244,THE_REAL_ODB,vjglr9,[D]Anyone use self-supervised learning at work? I'm surprised at how effective it has been for me.,"I've been using this stuff for sniffing near duplicates at work and been surprised how effect it has been!

PLanning to try it out some downstream tasks in the future to see how well it does!

I will say though it does take a shit ton of computing resources, but I find it really cool.",22,65,2022-06-24 10:41:18, d anyone use self supervised learning at work  i m surprised at how effective it has been for me ,i ve been using this stuff for sniffing near duplicates at work and been surprised how effect it has been planning to try it out some downstream tasks in the future to see how well it does i will say though it does take a shit ton of computing resources  but i find it really cool 
245,245,Which-Distance1384,vk1qxo,[D] A/B testing when there is a feedback loop,"I am experimenting with changing label value (target) for a model that we have in production. We used to cap the target variable, and my new model will release the cap.

&#x200B;

The main point about our production space is that there is a positive feedback loop involved. So, we expect that when we release the cap, my model would result in a section of users having more activity. However, since most of user traffic goes to control arm, only a fraction of it goes to experiment and thus the feedback loop doesnt close unless we have 50-50% experiment (that we can't).

&#x200B;

Wondering, if there is any way to run an A/B test and compare the production model and my model. The labels are shifting as well as the control loop doesn't close.

&#x200B;

Any idea is highly appreciated.",2,1,2022-06-25 05:16:20, d  a b testing when there is a feedback loop,i am experimenting with changing label value  target  for a model that we have in production  we used to cap the target variable  and my new model will release the cap   xb the main point about our production space is that there is a positive feedback loop involved  so  we expect that when we release the cap  my model would result in a section of users having more activity  however  since most of user traffic goes to control arm  only a fraction of it goes to experiment and thus the feedback loop doesnt close unless we have    experiment  that we can t    xb wondering  if there is any way to run an a b test and compare the production model and my model  the labels are shifting as well as the control loop doesn t close   xb any idea is highly appreciated 
246,246,ffast-math,vj7nf5,"[P] Farewell, CUDA OOM: Automatic Gradient Accumulation","Hey everyone,

If you've trained a lot of neural nets, you probably know the pain of getting CUDA OOM errors and iteratively tuning your batch size to avoid them.

Which is why I'm excited to announce that we (MosaicML) just released an automatic way to avoid these errors. Namely, we just added [automatic gradient accumulation](https://docs.mosaicml.com/en/latest/notes/auto_grad_accum.html) to [Composer](https://github.com/mosaicml/composer), our open source library for faster + easier neural net training.

If you're not familiar with gradient accumulation, it's like tuning the batch size, but without messing with the optimization (aside from slightly different BatchNorm stats). This lets you avoid tuning learning rate, weight decay, etc based on how much memory your GPU has or how many GPUs you're training on.

https://preview.redd.it/ogxq73znuf791.png?width=1374&format=png&auto=webp&s=93ff0b76a2293a73a5380b7e93f62fe34c604bc4

What's nice about the \*automatic\* gradient accumulation in Composer is that you just set the batch size and hparams once and you're done—no need to tune the gradient accumulation manually.

More info in our [blog post](https://www.mosaicml.com/blog/farewell-oom), and special thanks to [Mihir Patel](https://mvpatel2000.github.io/) for building most of this. Happy to answer questions!",40,123,2022-06-24 02:58:18, p  farewell  cuda oom  automatic gradient accumulation,hey everyone if you ve trained a lot of neural nets  you probably know the pain of getting cuda oom errors and iteratively tuning your batch size to avoid them which is why i m excited to announce that we  mosaicml  just released an automatic way to avoid these errors  namely  we just added  automatic gradient accumulation  https if you re not familiar with gradient accumulation  it s like tuning the batch size  but without messing with the optimization  aside from slightly different batchnorm stats   this lets you avoid tuning learning rate  weight decay  etc based on how much memory your gpu has or how many gpus you re training on https what s nice about the   automatic   gradient accumulation in composer is that you just set the batch size and hparams once and you re done no need to tune the gradient accumulation manually more info in our  blog post  https   www mosaicml com blog farewell oom   and special thanks to  mihir patel  https   mvpatel github io   for building most of this  happy to answer questions 
247,247,vikarjramun,vj0t0l,[P] Reverse Engineering Google Colab,"Hi!

I've spent a lot of time working with Google Colab recently, and was disappointed that such a powerful platform was limited to only running Jupyter notebooks. So I took a deep dive into the internals of Colab, discovering tons of interesting hidden features!

[Take a look at what I found!](https://dagshub.com/blog/reverse-engineering-google-colab/)",15,291,2022-06-23 21:54:19, p  reverse engineering google colab,hi i ve spent a lot of time working with google colab recently  and was disappointed that such a powerful platform was limited to only running jupyter notebooks  so i took a deep dive into the internals of colab  discovering tons of interesting hidden features  take a look at what i found   https   dagshub com blog reverse engineering google colab  
248,248,ElongatedMuskrat122,vk47y4,[D] How do you guys usually go about normalizing sales data? Opinion on neural networks for business data...,"Working on a project right now, and I have sales amounts as a column. Normally I would throw this into XGBoost, and let it rip, but, I am thinking this might benefit from a DNN. 

 \- For those who have used neural networks for business data, what was your experience using it?

 \- How did you normalize values like sales data? Did you just divide by the max, or not normalize at all?",6,0,2022-06-25 07:28:52, d  how do you guys usually go about normalizing sales data  opinion on neural networks for business data   ,working on a project right now  and i have sales amounts as a column  normally i would throw this into xgboost  and let it rip  but  i am thinking this might benefit from a dnn      for those who have used neural networks for business data  what was your experience using it     how did you normalize values like sales data  did you just divide by the max  or not normalize at all 
249,249,japanhue,vjn9jv,"[R] Unpublished physics inspired ML paper from 2021 (Yang-Mills theory, differential geometry, gauge theory)","Hi there,

The purpose of this post is to share a [research paper/notebook](https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf) I wrote that has been mostly unread and unnoticed by others, and also to ask how to find research collaborators without participating in academia or industry.

After I finished my BSc, I was deeply interested in geometric deep learning and wrote this paper \[0\] describing an attention mechanism using ideas from differential geometry and gauge theory commonly used in the standard model (via Yang-Mills theory). At the time, I sent the notebook/paper to every researcher in the geometric DL area that I was aware of but didn't get any replies or interest in collaboration. Without any openings and at the peak of a pandemic, I sadly had to drop the idea and get a standard software engineer job.

Since then, I've seen much of the rough ideas explored and developed independently by others. For example, M. Bronstein and his collaborators have similar applications of using connections (equivalent to sheafs) and Ricci flow in Graph NNs \[1\]. I have more ideas that I would like to explore, but feel destined to be an outsider in this field with my work unnoticed or considered illegitimate. Is it possible for people like me to collaborate with other researchers outside of academic institutions or industry? Does anyone know of such an organization?

Thanks

\[0\] [https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf](https://lukepereira.github.io/notebooks/documents/2021-moduli-attention/main.pdf)

\[1\] [https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/](https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/)",2,7,2022-06-24 17:55:42, r  unpublished physics inspired ml paper from   yang mills theory  differential geometry  gauge theory ,hi there the purpose of this post is to share a  research paper notebook  https after i finished my bsc  i was deeply interested in geometric deep learning and wrote this paper      describing an attention mechanism using ideas from differential geometry and gauge theory commonly used in the standard model  via yang mills theory   at the time  i sent the notebook paper to every researcher in the geometric dl area that i was aware of but didn t get any replies or interest in collaboration  without any openings and at the peak of a pandemic  i sadly had to drop the idea and get a standard software engineer job since then  i ve seen much of the rough ideas explored and developed independently by others  for example  m  bronstein and his collaborators have similar applications of using connections  equivalent to sheafs  and ricci flow in graph nns       i have more ideas that i would like to explore  but feel destined to be an outsider in this field with my work unnoticed or considered illegitimate  is it possible for people like me to collaborate with other researchers outside of academic institutions or industry  does anyone know of such an organization thanks      https       https   thegradient pub graph neural networks beyond message passing and weisfeiler lehman   https   thegradient pub graph neural networks beyond message passing and weisfeiler lehman  
250,250,aifordummies,vja009,[D] CVPR wants to penalize reviewers for violating the reviewer guideline!,"I cannot believe that CVPR put this motion for voting:

Motion 3: ""Any reviewer who has accepted an invitation to review but violates the reviewing guidelines set forth by the conference will be prohibited from submitting any papers to CVPR for up to two years.""

Reviewing is a community service, and although I have encountered bad and unfair reviews multiple times, I don't think such a wild action is the way to go to increase the review process quality. Let's start with the training process and choosing qualified AC and Meta ACs first where they can properly oversee the review process, choose fit reviewers, and take action in the rebuttal process.

If this goes through I would never review for CVPR again.

[https://mobile.twitter.com/KostasPenn/status/1539805992145358850](https://mobile.twitter.com/KostasPenn/status/1539805992145358850)

&#x200B;

**UPDATE: All motions have passed!**

Motion 1: Withdrawn Submissions Will No Longer Be Made Inaccessible to Reviewers  
Yes: 867  
No: 354

&#x200B;

Motion 2: Making Authors Responsible for Reviewing  
Yes: 677  
No: 553

Motion 3: Penalties for Violations of the CVPR Reviewing Guidelines  
Yes: 655  
No: 574

&#x200B;

Truly a sad day for ML research community.",43,69,2022-06-24 04:49:16, d  cvpr wants to penalize reviewers for violating the reviewer guideline ,i cannot believe that cvpr put this motion for voting motion   any reviewer who has accepted an invitation to review but violates the reviewing guidelines set forth by the conference will be prohibited from submitting any papers to cvpr for up to two years reviewing is a community service  and although i have encountered bad and unfair reviews multiple times  i don t think such a wild action is the way to go to increase the review process quality  let s start with the training process and choosing qualified ac and meta acs first where they can properly oversee the review process  choose fit reviewers  and take action in the rebuttal process if this goes through i would never review for cvpr again  https   xb   update  all motions have passed   motion   withdrawn submissions will no longer be made inaccessible to reviewers  yes    no    xb motion   making authors responsible for reviewing  yes    no  motion   penalties for violations of the cvpr reviewing guidelines  yes    no    xb truly a sad day for ml research community 
251,251,htrp,vivji3,[P] Yandex open sources 100b large language model weights (YaLM),"PR Announcement: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6


Github: https://github.com/yandex/YaLM-100B

Network is trained using same principles as Megatron LM, inference alone will require 4 A100s",56,289,2022-06-23 17:45:39, p  yandex open sources b large language model weights  yalm ,pr announcement  https github  https network is trained using same principles as megatron lm  inference alone will require  as
252,252,bandalorian,vjvl6t,[D] Using a neural net on bag of words vector vs PCA doe classification,"I have a document set that I wish to classify. I have tried with transformers, and they perform well, but the content is largely keyword driven so a lot of the attention stuff is not needed. It's a more deterministic system that needs to learn keyword combinations. So a count vectorizer over unigrams and bigrams, and then a classifier like XGBoost seems like a good idea. The problem is even after some pruning I get a feature vector of 26K.  I'd also like to compare this to a how a simple neural net handles it.

I was going to apply sparse PCA to get the dimensionality down first. However for a neural net, does it make sense to do PCA first? Isn't that what the embeddings are doing? Basically, the tasks of PCA + classifier model are carried out by the embedding and classification layers of a neural net. But just feeding 26 K dimensions to a neural net seems lazy, but if I reduce it to say 768 dimensions, I've basically carried out the whole embedding task before I pass it to the neural net, which limits the improvements it can make.

Would a happy medium of reducing to say 5K dimensions and then letting the neural net take it from there? I'm in the process of testing all of this in the next couple of weeks, but curious if anyone has any experience/insight/guesses.",6,1,2022-06-25 00:23:23, d  using a neural net on bag of words vector vs pca doe classification,i have a document set that i wish to classify  i have tried with transformers  and they perform well  but the content is largely keyword driven so a lot of the attention stuff is not needed  it s a more deterministic system that needs to learn keyword combinations  so a count vectorizer over unigrams and bigrams  and then a classifier like xgboost seems like a good idea  the problem is even after some pruning i get a feature vector of k   i d also like to compare this to a how a simple neural net handles it i was going to apply sparse pca to get the dimensionality down first  however for a neural net  does it make sense to do pca first  isn t that what the embeddings are doing  basically  the tasks of pca   classifier model are carried out by the embedding and classification layers of a neural net  but just feeding  k dimensions to a neural net seems lazy  but if i reduce it to say  dimensions  i ve basically carried out the whole embedding task before i pass it to the neural net  which limits the improvements it can make would a happy medium of reducing to say k dimensions and then letting the neural net take it from there  i m in the process of testing all of this in the next couple of weeks  but curious if anyone has any experience insight guesses 
253,253,Time-Archer-8103,vjpcij,[P] Implementing CRF-CNN model in python,"I am trying to implement a [research paper](https://ieeexplore.ieee.org/document/8546073) that uses CNN and CRF for page object detection. According to the research paper we have to to build two neural network (named unary and pairwise). Then the training data (set of images) are passed and both the CNNs are trained. After that we are supposed to apply CRF.

&#x200B;

Following are the equations for CRF:

&#x200B;

https://preview.redd.it/ckrm2rzutk791.png?width=768&format=png&auto=webp&s=ed88d8705b515beaf955d09aa194fa63707f7cca

U and V are unary and pairwise potentials obtained from the CNNs using the following equations:

&#x200B;

https://preview.redd.it/uahcpzgwtk791.png?width=813&format=png&auto=webp&s=bb3548539db1c9b1be3367f2ddd529f1ba32c5f3

&#x200B;

Maximum a posteriori (MAP) strategy to predict the labels of line regions given a new document. MAP inference of CRFs can be formulated as the following optimization problem:

&#x200B;

&#x200B;

&#x200B;

https://preview.redd.it/sz0537wwtk791.png?width=273&format=png&auto=webp&s=638b88b012a0158bce017be14b7e81639199a681

The parameters of our CRFs include Unary-Net's weights  and Pairwise-Net's weights  and a combination coefficient vector λ of U and V. weights of U and V (w)  are learned using the SGD method. Then they are fixed and λ is learned using the Pseudo Likelihood method.

&#x200B;

&#x200B;

I have created the neural networks but I am not able to implement the CRF part. Can someone help me implement this or suggest a python library that makes it easier to implement. (I have tried a python library [pystruct](https://pystruct.github.io/) but could not install it)",0,2,2022-06-24 19:40:11, p  implementing crf cnn model in python,i am trying to implement a  research paper  https   xb following are the equations for crf   xb https u and v are unary and pairwise potentials obtained from the cnns using the following equations   xb https   xb maximum a posteriori  map  strategy to predict the labels of line regions given a new document  map inference of crfs can be formulated as the following optimization problem   xb   xb   xb https the parameters of our crfs include unary net s weights  and pairwise net s weights  and a combination coefficient vector λ of u and v  weights of u and v  w   are learned using the sgd method  then they are fixed and λ is learned using the pseudo likelihood method   xb   xb i have created the neural networks but i am not able to implement the crf part  can someone help me implement this or suggest a python library that makes it easier to implement   i have tried a python library  pystruct  https   pystruct github io   but could not install it 
254,254,FundF,vjox5u,[P] What The Plug: An app that identifies electrical plugs,"I have built a convolutional neural network that identifies roughly 20 different plug types. I wrote most code with Keras on top of Tensorflow in Python. I trained the model on my personal computer using Linux and CUDA to train with my GPU. Afterwards I transformed the model to a .tflite file and embedded it in a swift app for iPhone.

Machine learning and programming is not my main field of work. Actually it's my first project in both areas. During the last three years I have taught myself the principals of machine learning as well as Python and Swift.

I hope some of you are interested in trying out the app. I would love to hear your feedback.

The app is 100% free by the way. I just want to see people use what I have build.

Here is the link to the app store:

[https://apps.apple.com/de/app/what-the-plug/id1613147033](https://apps.apple.com/de/app/what-the-plug/id1613147033)",0,2,2022-06-24 19:20:15, p  what the plug  an app that identifies electrical plugs,i have built a convolutional neural network that identifies roughly  different plug types  i wrote most code with keras on top of tensorflow in python  i trained the model on my personal computer using linux and cuda to train with my gpu  afterwards i transformed the model to a  tflite file and embedded it in a swift app for iphone machine learning and programming is not my main field of work  actually it s my first project in both areas  during the last three years i have taught myself the principals of machine learning as well as python and swift i hope some of you are interested in trying out the app  i would love to hear your feedback the app is   free by the way  i just want to see people use what i have build here is the link to the app store  https   apps apple com de app what the plug id  https   apps apple com de app what the plug id 
255,255,ItzDerock,vjtns4,[D] Need opinions for GPU server build.,"Work is getting a new server for ml/deep learning. 
Price isn't an issue, not looking to cut down much, just wanted to make sure that I'm not overlooking anything in terms of compatibility. 

My main concern is the CPU, would you recommend getting more cores/higher clock, or is it fine? 

https://docs.google.com/spreadsheets/d/17EQ_ZLQGDuaq5ECPpH_V7HKRC8QP-2qyoqvzKuXJoWI/edit?usp=drivesdk",7,1,2022-06-24 22:56:16, d  need opinions for gpu server build ,work is getting a new server for ml deep learning  price isn t an issue  not looking to cut down much  just wanted to make sure that i m not overlooking anything in terms of compatibility  my main concern is the cpu  would you recommend getting more cores higher clock  or is it fine  https   docs google com spreadsheets d eq_zlqgduaqecpph_vhkrcqp qyoqvzkuxjowi edit usp drivesdk
256,256,XtremePocket,vjs5jj,[D] Loss for generating sequences of items,"Let's say you have a task where you need to generate blobs of texts using a AR LM. The targets are separated in the form of `[blob1], [blob2], ...` where each blob contains some numbers and letters, and the order of the blobs matters. Now, a naive way would be just to train the network to generate tokens greedily. But could we do better? A greedy loss could still theoretically give us a great model, but is there another way that exploits the blob patterns?

An idea I have: If we believe the model should first learn existence of blobs then learn the order (a fair assumption in my application), we could first find a matching between all generated blobs and target blobs and optimize the best matches only, then impose a penalty to get the order right. The order might be enforced via maybe taking a weighted average between the greedy loss and the blob-matched loss?

What do you think?",1,0,2022-06-24 21:48:34, d  loss for generating sequences of items,let s say you have a task where you need to generate blobs of texts using a ar lm  the targets are separated in the form of   blob    blob        where each blob contains some numbers and letters  and the order of the blobs matters  now  a naive way would be just to train the network to generate tokens greedily  but could we do better  a greedy loss could still theoretically give us a great model  but is there another way that exploits the blob patterns an idea i have  if we believe the model should first learn existence of blobs then learn the order  a fair assumption in my application   we could first find a matching between all generated blobs and target blobs and optimize the best matches only  then impose a penalty to get the order right  the order might be enforced via maybe taking a weighted average between the greedy loss and the blob matched loss what do you think 
257,257,gambs,vj2kc3,[R] Learning to Play Minecraft with Video PreTraining (VPT),"[OpenAI Blog: Learning to Play Minecraft with Video PreTraining (VPT)](https://openai.com/blog/vpt/)

OpenAI gathered a large dataset of human Minecraft demonstrations and trained an Inverse Dynamics Model (IDM) transformer that predicts actions based on past and future frames using a dataset of human demonstrations. They used this model to label 70k hours of video, which is used to train a Video PreTraining (VPT) model, which predicts actions based on past frames alone, using behavioral cloning (i.e. supervised learning).

They can then fine-tune the VPT via behavioral cloning on narrower datasets or RL (with a hand-designed reward function that rewards the agent for going deeper into the tech tree or obtaining materials that could lead to a diamond pickaxe) and are able to train an agent that can craft a diamond pickaxe in 2.5% of its 10-minute long episodes.",8,48,2022-06-23 23:12:15, r  learning to play minecraft with video pretraining  vpt , openai blog  learning to play minecraft with video pretraining  vpt   https openai gathered a large dataset of human minecraft demonstrations and trained an inverse dynamics model  idm  transformer that predicts actions based on past and future frames using a dataset of human demonstrations  they used this model to label k hours of video  which is used to train a video pretraining  vpt  model  which predicts actions based on past frames alone  using behavioral cloning  i e  supervised learning  they can then fine tune the vpt via behavioral cloning on narrower datasets or rl  with a hand designed reward function that rewards the agent for going deeper into the tech tree or obtaining materials that could lead to a diamond pickaxe  and are able to train an agent that can craft a diamond pickaxe in    of its  minute long episodes 
258,258,scoutsearchteam,vjamsw,[Project] Semantic Search powerup for Ctrl+F,"Hi Reddit!

Scout Search is a project I've been working on as a Find-in-Page replacement.

It uses a semantic search engine (rather than character matching) to help you find what you're looking for on websites.

Try it out and let me know what you think.

[https://chrome.google.com/webstore/detail/scout-search/hgljpodblkjjklailoaefokflfdeffdl](https://chrome.google.com/webstore/detail/scout-search/hgljpodblkjjklailoaefokflfdeffdl)",4,9,2022-06-24 05:20:36, project  semantic search powerup for ctrl f,hi reddit scout search is a project i ve been working on as a find in page replacement it uses a semantic search engine  rather than character matching  to help you find what you re looking for on websites try it out and let me know what you think  https   chrome google com webstore detail scout search hgljpodblkjjklailoaefokflfdeffdl  https   chrome google com webstore detail scout search hgljpodblkjjklailoaefokflfdeffdl 
259,259,optimized-adam,vjlhee,[D] Publishing two papers at the same time,"Let's say I have done some research, developed some ideas and gotten good results. But there are two main ideas that tackle different problems and don't really belong in the same paper, although there is some relationship between them. The paper of idea #2 would cite and use idea #1. What have you done in similar situations? Can you try to publish both at the same time and have a citation to the first paper that hasn't even been published yet? Post on arXiv and try to publish the first one first, then the second one?",4,0,2022-06-24 16:11:38, d  publishing two papers at the same time,let s say i have done some research  developed some ideas and gotten good results  but there are two main ideas that tackle different problems and don t really belong in the same paper  although there is some relationship between them  the paper of idea   would cite and use idea    what have you done in similar situations  can you try to publish both at the same time and have a citation to the first paper that hasn t even been published yet  post on arxiv and try to publish the first one first  then the second one 
260,260,Supremefigur,vjw1a5,[R] Anatomy of an AI System [Infographic],"[https://anatomyof.ai/img/ai-anatomy-map.pdf](https://anatomyof.ai/img/ai-anatomy-map.pdf)  


A beautiful infographic explaining the whole process",0,0,2022-06-25 00:43:47, r  anatomy of an ai system  infographic , https a beautiful infographic explaining the whole process
261,261,SleekEagle,viyh17,[D] How Imagen Actually Works,"Hey everyone!

[I wrote this article explaining how Imagen actually works](https://www.assemblyai.com/blog/how-imagen-actually-works/), with a general overview for the big picture ideas and a Deep Dive to get into the nitty-gritty.

I'm happy to answer any questions, let me know what you think!

https://preview.redd.it/17xc5fqeud791.png?width=3472&format=png&auto=webp&s=e78a024892a3032ffc0c143b7843a5223751afcb",19,33,2022-06-23 20:10:27, d  how imagen actually works,hey everyone  i wrote this article explaining how imagen actually works  https i m happy to answer any questions  let me know what you think https   preview redd it xcfqeud png width  format png auto webp s eaaffccbaafcb
262,262,ManagementBig2995,vj6uh1,[P] HyperImpute: sklearn-style library for handling missing data using novel algorithms,"There are many data imputation algorithms for machine learning. However, benchmarking them can be complicated, mainly because most implementations stay just as research code to reproduce the experiments in the papers. Moreover, when dealing with tabular data, you need to handle continuous/discrete/categorical data correctly -- not just let some regressor approximate everything.

HyperImpute is a library that should make it easy to benchmark new imputation algorithms while offering several state-of-the-art models. For example, imputing using MIWAE can be done as easy as this:

    import pandas as pd
    import numpy as np
    from hyperimpute.plugins.imputers import Imputers
    
    X = pd.DataFrame([[1, 1, 1, 1], [4, 5, np.nan, np.nan], [3, 3, 9, 9], [2, 2, 2, 2]])
    
    plugin = Imputers().get(""miwae"")
    out = plugin.fit_transform(X.copy())
    
    out

Bonus, it can be easily plugged into sklearn pipelines.

Try it in Colab: [https://colab.research.google.com/drive/1zGm4VeXsJ-0x6A5\_icnknE7mbJ0knUig?usp=sharing](https://colab.research.google.com/drive/1zGm4VeXsJ-0x6A5_icnknE7mbJ0knUig?usp=sharing)

Github page: [https://github.com/vanderschaarlab/hyperimpute](https://github.com/vanderschaarlab/hyperimpute)

If you find the project useful, please star it on Github, it would help a lot!",0,10,2022-06-24 02:22:22, p  hyperimpute  sklearn style library for handling missing data using novel algorithms,there are many data imputation algorithms for machine learning  however  benchmarking them can be complicated  mainly because most implementations stay just as research code to reproduce the experiments in the papers  moreover  when dealing with tabular data  you need to handle continuous discrete categorical data correctly    not just let some regressor approximate everything hyperimpute is a library that should make it easy to benchmark new imputation algorithms while offering several state of the art models  for example  imputing using miwae can be done as easy as this     import pandas as pd    import numpy as np    from hyperimpute plugins imputers import imputers        x   pd dataframe                 np nan  np nan                               plugin   imputers   get miwae     out   plugin fit_transform x copy           outbonus  it can be easily plugged into sklearn pipelines try it in colab   https github page   https if you find the project useful  please star it on github  it would help a lot 
263,263,guyfrom7up,vj1zp0,[P] AutoRegistry: A Python library for mapping names to functionality to simplify project configurations.,"A common design pattern I see in a lot of ML projects is to have some sort of experiment configuration file, and then a bunch of code that constructs the appropriate objects based on these configurations. Frequently, the resulting code blocks have a bunch of `if/elif/else` statements, or a manually created lookup dictionary somewhere. This can quickly get messy and inconsistent as you add new models/losses/encoders/optimizers.  

AutoRegistry is a library that makes all of these lookups more organized and terse. For example, lets say you want to configure a backbone to either be ""resnet34"" or ""resnet50"". Your code could look something like this (mimicking torchvision code) using a decorator: 

```
from autoregistry import Registry

models = Registry()

@models
def resnet34(*, weights: Optional[ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:
    return _resnet(BasicBlock, [3, 4, 6, 3], weights, progress, **kwargs)

@models
def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:
    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)

# create a model based off of some configuration dictionary.
model_config = copy(config[""model""])
model_type = model_config.pop(""type"")
model = models[model_type](**model_config)
```

or, class-based inheritance (uses metaclasses internally):

```
class BaseModel(nn.Module, Registry):
    pass

class MyNewModel(BaseModel):
    pass

class SomeOtherModel(BaseModel):
    pass

# stringified keys are automatically derived.
my_new_model = BaseModel[""mynewmodel""](**config)
some_other_model = BaseModel[""someothermodel""](**config)
```

Github Page:  
[https://github.com/BrianPugh/autoregistry](https://github.com/BrianPugh/autoregistry)",1,12,2022-06-23 22:46:46, p  autoregistry  a python library for mapping names to functionality to simplify project configurations ,a common design pattern i see in a lot of ml projects is to have some sort of experiment configuration file  and then a bunch of code that constructs the appropriate objects based on these configurations  frequently  the resulting code blocks have a bunch of  if elif else  statements  or a manually created lookup dictionary somewhere  this can quickly get messy and inconsistent as you add new models losses encoders optimizers   autoregistry is a library that makes all of these lookups more organized and terse  for example  lets say you want to configure a backbone to either be resnet or resnet  your code could look something like this  mimicking torchvision code  using a decorator     from autoregistry import registrymodels   registry   modelsdef resnet    weights  optional resnet_weights    none  progress  bool   true    kwargs  any     resnet     return _resnet basicblock            weights  progress    kwargs  modelsdef resnet    weights  optional resnet_weights    none  progress  bool   true    kwargs  any     resnet     return _resnet bottleneck            weights  progress    kwargs   create a model based off of some configuration dictionary model_config   copy config model  model_type   model_config pop type model   models model_type    model_config    or  class based inheritance  uses metaclasses internally     class basemodel nn module  registry      passclass mynewmodel basemodel      passclass someothermodel basemodel      pass  stringified keys are automatically derived my_new_model   basemodel mynewmodel    config some_other_model   basemodel someothermodel    config    github page    https   github com brianpugh autoregistry  https   github com brianpugh autoregistry 
264,264,an1_r_00dh,vjfgb9,[Discussion] Is there a way to increase the weight of a particular feature in an outlier detection method using the isolation forest algorithm?,"I'm currently working on the outlier detection method using the isolation forest algorithm on a dataset with 9 dimensions. Out of these, there is a particular dimension that I want to increase the importance/significance of, in the classification process. Is there a way I can do this? Thanksnin advance.",1,0,2022-06-24 09:36:02, discussion  is there a way to increase the weight of a particular feature in an outlier detection method using the isolation forest algorithm ,i m currently working on the outlier detection method using the isolation forest algorithm on a dataset with  dimensions  out of these  there is a particular dimension that i want to increase the importance significance of  in the classification process  is there a way i can do this  thanksnin advance 
265,265,chromeplated,vitv4u,[N] Microsoft released a DirectML Plugin for TensorFlow 2,"The plugin provides a DirectML PluggableDevice backend for TensorFlow 2, so any GPU which supports DirectX 12 should be able to work with TF2. Hopefully this will pave the way for more support for non-NVIDIA GPUs in ML.  
They provide some more details (installation, code samples, etc') in the [Windows AI devblog](https://devblogs.microsoft.com/windowsai/directml-plugin-for-tensorflow-2-is-here/).",1,12,2022-06-23 16:05:06, n  microsoft released a directml plugin for tensorflow ,the plugin provides a directml pluggabledevice backend for tensorflow   so any gpu which supports directx  should be able to work with tf  hopefully this will pave the way for more support for non nvidia gpus in ml   they provide some more details  installation  code samples  etc   in the  windows ai devblog  https   devblogs microsoft com windowsai directml plugin for tensorflow  is here   
266,266,codeinassembly,viwb07,[D] [P] A TensorFlow Re-Implementation of CheXNet - Classification and Localization of Thoracic Diseases,"TL:DR; need help making heatmaps!   
\[[Repository](https://dagshub.com/nirbarazida/Pneumonia-Classification)|[Colab Notebook](https://colab.research.google.com/drive/1U3F5ETJeisBnlmamR4EqigS7shIbL2L1#scrollTo=Ghq8fYm5yo8o)\]

Hey everyone -

I've been working to reproduce [CheXNet](https://arxiv.org/pdf/1711.05225.pdf) \- a fantastic paper describing research on a model capable of radiologist-grade pathology classification!

CheXNet uses Class Activation Mappings (CAMs for short) to generate heatmaps that identify what parts of the image the model uses to base its classification. In my case, I'm facing a bit of a struggle reproducing them - as shown in the image below, **most of our classifications are derived from the diaphragm, instead of regions within the lung**. Curiously, we are attaining a reasonable AUROC, with .773 on training and .749 on validation data - the paper reports .8062 AUROC.

My current model is being trained on a subsample of the main dataset, and I'm basically looking to this as a way to validate the architecture. I'd love to know if anyone has experienced similar issues and solved them, and could have any input here as well.

If you have a moment to spare - I'd be super grateful for some help from the r/MachineLearning community in solving the inaccurate localization issue - [\#58](https://dagshub.com/nirbarazida/Pneumonia-Classification/issues/58)!

[Fig 1. An incorrect localization, despite a correct classification.](https://preview.redd.it/umoq6vjmbd791.png?width=451&format=png&auto=webp&s=3dc1d8a99925db47a02c3f719e1d9fd0ba984535)",1,8,2022-06-23 18:25:31, d   p  a tensorflow re implementation of chexnet   classification and localization of thoracic diseases,tl dr  need help making heatmaps       repository  https hey everyone  i ve been working to reproduce  chexnet  https chexnet uses class activation mappings  cams for short  to generate heatmaps that identify what parts of the image the model uses to base its classification  in my case  i m facing a bit of a struggle reproducing them   as shown in the image below    most of our classifications are derived from the diaphragm  instead of regions within the lung    curiously  we are attaining a reasonable auroc  with   on training and   on validation data   the paper reports   auroc my current model is being trained on a subsample of the main dataset  and i m basically looking to this as a way to validate the architecture  i d love to know if anyone has experienced similar issues and solved them  and could have any input here as well if you have a moment to spare   i d be super grateful for some help from the r machinelearning community in solving the inaccurate localization issue        https  fig   an incorrect localization  despite a correct classification   https   preview redd it umoqvjmbd png width  format png auto webp s dcdadbacfedfdba 
267,267,htrp,vid29a,[R] Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Google - Parti),"Google published results from an seq2seq transformer model for autoregressive image generation.

Website: https://parti.research.google/

Paper: https://gweb-research-parti.web.app/parti_paper.pdf",15,126,2022-06-23 00:49:10, r  scaling autoregressive models for content rich text to image generation  google   parti ,google published results from an seqseq transformer model for autoregressive image generation website  https paper  https   gweb research parti web app parti_paper pdf
268,268,Pedimus,vj6mru,"[D] ""Wrapping"" effects when using diffusion model to generate samples?","I've recently been training a latent diffusion model (it operated on the latent space of a VQ-VAE), and I'm finding that my generated samples have ""wrapping"" effects, i.e.: when I generate the face it wraps up (bottom half of the face in the top half of the image and vice versa). It's worth noting that these halves don't always seem like they belong together, but they individually look quite realistic.

I've checked my training data, and there are absolutely no training samples that exhibit this behaviour, so my model never sees images that exhibit this wrapping effect, so what could be causing this?",2,0,2022-06-24 02:12:51, d  wrapping effects when using diffusion model to generate samples ,i ve recently been training a latent diffusion model  it operated on the latent space of a vq vae   and i m finding that my generated samples have wrapping effects  i e   when i generate the face it wraps up  bottom half of the face in the top half of the image and vice versa   it s worth noting that these halves don t always seem like they belong together  but they individually look quite realistic i ve checked my training data  and there are absolutely no training samples that exhibit this behaviour  so my model never sees images that exhibit this wrapping effect  so what could be causing this 
269,269,ylu175,vig0l7,[R] Announcing DAMP 2.0: Allowing SOTA Anomaly Detection in Massive Time Series Datasets,"Dear Colleagues

We are happy to announce the release of DAMP 2.0 \[a\]. DAMP (Discord Aware Matrix Profile) is an anomaly detection framework that allows you to search datasets with millions or billions of datapoints, all on a conventional machine \[b\].

We are not normally so vainglorious as to announce the publication of  a paper, however:

1)  The code comes bundled with some great new anomaly detection datasets, and there is a real dearth of good datasets in the community (see \[c\])

2)  Some researchers are working on problems that use anomaly detection as a subroutine, and that is their main computational bottleneck. Because DAMP can be up to 10,000 times faster than other approaches, this may be of interest to the community

Best wishes, Yue

\[a\] Matrix Profile XXIV:Scaling Time Series Anomaly Detection to Trillions of Datapoints and Ultra-fast Arriving Data Streams. Yue Lu , Renjie Wu , Abdullah Mueen , Maria A. Zuluaga and Eamonn Keogh. ACM SIGKDD 2022.  [https://www.cs.ucr.edu/\~eamonn/DAMP\_long\_version.pdf](https://www.cs.ucr.edu/~eamonn/DAMP_long_version.pdf)

\[b\] [https://sites.google.com/view/discord-aware-matrix-profile](https://sites.google.com/view/discord-aware-matrix-profile)

\[c\] Irrational Exuberance Why we should not believe 95% of papers on Time Series Anomaly Detection.      [https://www.youtube.com/watch?v=Vg1p3DouX8w](https://www.youtube.com/watch?v=Vg1p3DouX8w)

\[d\] [https://drive.google.com/file/d/1hEgOKtoTuHGPMqR1wty8ff\_jes93ra9a/view](https://drive.google.com/file/d/1hEgOKtoTuHGPMqR1wty8ff_jes93ra9a/view)",7,39,2022-06-23 02:59:24, r  announcing damp    allowing sota anomaly detection in massive time series datasets,dear colleagueswe are happy to announce the release of damp     a    damp  discord aware matrix profile  is an anomaly detection framework that allows you to search datasets with millions or billions of datapoints  all on a conventional machine   b   we are not normally so vainglorious as to announce the publication of  a paper  however    the code comes bundled with some great new anomaly detection datasets  and there is a real dearth of good datasets in the community  see   c      some researchers are working on problems that use anomaly detection as a subroutine  and that is their main computational bottleneck  because damp can be up to   times faster than other approaches  this may be of interest to the communitybest wishes  yue  a   matrix profile xxiv scaling time series anomaly detection to trillions of datapoints and ultra fast arriving data streams  yue lu   renjie wu   abdullah mueen   maria a  zuluaga and eamonn keogh  acm sigkdd     https   b    https   c   irrational exuberance why we should not believe   of papers on time series anomaly detection        https   d    https   drive google com file d hegoktotuhgpmqrwtyff _jesraa view  https   drive google com file d hegoktotuhgpmqrwtyff_jesraa view 
270,270,curious_cow_99,vi2mw4,[D] Have you ever been asked to work on a software project you found unethical? We’d like to hear from you!,"We are researchers at Carnegie Mellon University studying how software developers identify and act on ethical concerns at work. If you’re interested in helping us advance research in software ethics, please fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLScEIB09oKznU4OGDQeQyNpfMgf_X3HdNS1j2m-c_BFDJijuTQ/viewform?usp=sf_link) and we’ll reach out to you for a quick interview!

P.S.

* You can check out [this](https://stackoverflow.blog/2022/05/30/ethical-ai-isnt-just-how-you-build-it-its-how-you-use-it/) Stack Overflow blog post to read more about the direction of our research.
* Anything you disclose to us during the survey / interview may appear in our study but will not be traceable to you.",66,253,2022-06-22 16:35:44, d  have you ever been asked to work on a software project you found unethical  we d like to hear from you ,we are researchers at carnegie mellon university studying how software developers identify and act on ethical concerns at work  if you re interested in helping us advance research in software ethics  please fill out  this survey  https p s   you can check out  this  https   anything you disclose to us during the survey   interview may appear in our study but will not be traceable to you 
271,271,keremidk0,vie0aj,[D] Is audio style transfer a thing ?,"So we have image style transfer, there's a lot of good papers and implementations.

  
Is there such thing as audio style transfer, where 1 song keeps its lyrics and melody, but get the other song's style ? e.g. pop music with rock style ?  
If yes - can you please share a link ?",10,30,2022-06-23 01:31:11, d  is audio style transfer a thing  ,so we have image style transfer  there s a lot of good papers and implementations   is there such thing as audio style transfer  where  song keeps its lyrics and melody  but get the other song s style   e g  pop music with rock style    if yes   can you please share a link  
272,272,lux123or,viy1zp,State of the art 2D body pose estimation [Discussion]," Hi. I have a background in neuroscience and sometimes we use DeepLabCut to track animals during behaviour. This is by far the most widespread and used application for animal tracking based on artificial neural networks. I was wondering, if anyone here is an expert in human 2D body pose estimation and can tell me what their oppinion is on what is the best human 2D pose estimation tool currently available? I came across Pose from mediapipe and it seems very good from a few examples I tested so far but I'm curious if there's something even better that I have not come across. Thanks for the help!",1,1,2022-06-23 19:50:58,state of the art d body pose estimation  discussion , hi  i have a background in neuroscience and sometimes we use deeplabcut to track animals during behaviour  this is by far the most widespread and used application for animal tracking based on artificial neural networks  i was wondering  if anyone here is an expert in human d body pose estimation and can tell me what their oppinion is on what is the best human d pose estimation tool currently available  i came across pose from mediapipe and it seems very good from a few examples i tested so far but i m curious if there s something even better that i have not come across  thanks for the help 
273,273,LeanderKu,vi8its,[D] Any way to speed up simple mathematical functions without implementing cuda kernels for pytorch?,"I am working on a pytorch project and I have a custom computation that I am so far unable to express as a combination pre-defined pytorch functions (because it's essentially some loops around conv2d calls where I juggle some indices in a 5-d tensor). So currently I use python-loops with some smart padding but that's not the fastest. The only way to speed this up would be, i think, to implement custom cuda kernels. While the computation is not that trivial it is simple in a mathematical way. It can be defined in a single line using lots of indices and sums. I wonder whether there is really nothing I can do?

What I am thinking of is something like tensor-comprehensions, but that's deprecated and I didn't get it to install.

Is there any modern alternative to tensor-comprehension, or should I switch the language to e.g. julia? Is it possible there to define slightly different conv2d there and have it run natively on the GPU?

I don't expect performance comparable to the handwritten conv2d kernels, but the python loops are just quite slow.",27,19,2022-06-22 21:28:18, d  any way to speed up simple mathematical functions without implementing cuda kernels for pytorch ,i am working on a pytorch project and i have a custom computation that i am so far unable to express as a combination pre defined pytorch functions  because it s essentially some loops around convd calls where i juggle some indices in a  d tensor   so currently i use python loops with some smart padding but that s not the fastest  the only way to speed this up would be  i think  to implement custom cuda kernels  while the computation is not that trivial it is simple in a mathematical way  it can be defined in a single line using lots of indices and sums  i wonder whether there is really nothing i can do what i am thinking of is something like tensor comprehensions  but that s deprecated and i didn t get it to install is there any modern alternative to tensor comprehension  or should i switch the language to e g  julia  is it possible there to define slightly different convd there and have it run natively on the gpu i don t expect performance comparable to the handwritten convd kernels  but the python loops are just quite slow 
274,274,Appropriate_Ant_4629,vinz5n,[D] Do any Text-to-Image approaches work well with long complex prompts (i.e. paragraph or book chapter scale)?,"Seems almost all the examples of text-to-image are based on tiny prompts with very few details (""avocado chair"").

Do any such systems do a good job at keeping track of details - like [the first 2 paragraphs of The Hobbit](https://www.printfriendly.com/p/g/jP9qGu) and correctly place the ""polished chairs"", ""pegs for hats and coats"", and ""deep-set round windows looking over his garden, and meadows beyond, sloping down to the river""?

Assuming they don't - what approach(es) might make sense to design such systems?

I'm speculating that you'd need much larger embedding vectors (to correctly connect concepts from the right adjectives to the right nouns); and it'd be harder to find training data (perhaps frames of movies from novels would be a good source)?

Any pointers to anything in that direction?",3,2,2022-06-23 09:37:22, d  do any text to image approaches work well with long complex prompts  i e  paragraph or book chapter scale  ,seems almost all the examples of text to image are based on tiny prompts with very few details  avocado chair  do any such systems do a good job at keeping track of details   like  the first  paragraphs of the hobbit  https assuming they don t   what approach es  might make sense to design such systems i m speculating that you d need much larger embedding vectors  to correctly connect concepts from the right adjectives to the right nouns   and it d be harder to find training data  perhaps frames of movies from novels would be a good source  any pointers to anything in that direction 
275,275,FlyingQuokka,vigx2l,[D] What is the current SOTA for open-source AutoML?,"I've never really used AutoML--I prefer to code up my models and data engineering by hand, but I'm beginning to wonder if I can use AutoML as a starting point, e.g., the built-in hyper-parameter optimization or NAS finds a good neural network hyper-params/architecture for me, and I can build on that.

With that in mind, what's the SOTA right now? Ideally, it would be as white-box as possible, telling me the models it tries, what worked and didn't, etc. Alternatively, what has worked best for you in your workflows?",3,6,2022-06-23 03:39:09, d  what is the current sota for open source automl ,i ve never really used automl  i prefer to code up my models and data engineering by hand  but i m beginning to wonder if i can use automl as a starting point  e g   the built in hyper parameter optimization or nas finds a good neural network hyper params architecture for me  and i can build on that with that in mind  what s the sota right now  ideally  it would be as white box as possible  telling me the models it tries  what worked and didn t  etc  alternatively  what has worked best for you in your workflows 
276,276,JBitterwolf,vi1ly4,[R] Breaking Down Out-of-Distribution Detection,"TL;DR: Many OOD detectors that are trained with samples from an (unrelated) OOD dataset can be understood by isolating a binary discriminator between in-distribution and OOD.

[We just published it on arXiv](https://arxiv.org/abs/2206.09880) and will present it at ICML 2022.

Questions and discussion are very welcome!

Full title: **Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities** by Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, Matthias Hein.",3,35,2022-06-22 15:29:50, r  breaking down out of distribution detection,tl dr  many ood detectors that are trained with samples from an  unrelated  ood dataset can be understood by isolating a binary discriminator between in distribution and ood  we just published it on arxiv  https questions and discussion are very welcome full title    breaking down out of distribution detection  many methods based on ood training data estimate a combination of the same core quantities   by julian bitterwolf  alexander meinke  maximilian augustin  matthias hein 
277,277,LemonByte,viglgx,[P] Multidimensional array batch indexing for pytorch and numpy,"Batch indexing into multidimensional tensors/arrays is kind of tricky, I made this project explaining the builtin syntax and also made wrappers for simplifying the interface, with additional features for underlying coordinate grid data (like signed distance functions) that need to be indexed by coordinate value rather than integer indices directly [https://github.com/LemonPi/multidim\_indexing](https://github.com/LemonPi/multidim_indexing)",4,5,2022-06-23 03:25:20, p  multidimensional array batch indexing for pytorch and numpy,batch indexing into multidimensional tensors arrays is kind of tricky  i made this project explaining the builtin syntax and also made wrappers for simplifying the interface  with additional features for underlying coordinate grid data  like signed distance functions  that need to be indexed by coordinate value rather than integer indices directly  https   github com lemonpi multidim _indexing  https   github com lemonpi multidim_indexing 
278,278,seraschka,vi41f7,[P] Bottom-up look at the new Lightning Framework for building anything from production-ready ML systems to research demos,"The open-source [lightning.ai](https://lightning.ai) framework just launched last week introducing the concept of Lightning Apps. It's basically meant for building anything from production ready ML-system running on multi-node GPU clusters in the cloud to building simple research demos.

Starting with a simple use case, a research demo, I wrote a ""short"" article about it to explain how it roughly works under the hood: [Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App ](https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html)

Looking forward to hearing your feedback. I am planning to put together more ""substantial"" examples, but I was thinking of doing that one step at the time. Will be attending a conference in 3 weeks and am planning to create a research demo alongside the paper I will be presenting, and I was wondering besides Gradio/Dash/Gradio, what are your typical tools and workflows for making research demos. Any cool examples for inspiration?

&#x200B;

Disclaimer: I recently joined Lightning when I saw an early prototype. As someone who has spent most of my time on research models, I was always intrigued by putting ML models to production. However, I was also always turned of by the tooling that it involved.",9,17,2022-06-22 17:55:18, p  bottom up look at the new lightning framework for building anything from production ready ml systems to research demos,the open source  lightning ai  https starting with a simple use case  a research demo  i wrote a short article about it to explain how it roughly works under the hood   sharing deep learning research models with lightning part   building a super resolution app   https looking forward to hearing your feedback  i am planning to put together more substantial examples  but i was thinking of doing that one step at the time  will be attending a conference in  weeks and am planning to create a research demo alongside the paper i will be presenting  and i was wondering besides gradio dash gradio  what are your typical tools and workflows for making research demos  any cool examples for inspiration   xb disclaimer  i recently joined lightning when i saw an early prototype  as someone who has spent most of my time on research models  i was always intrigued by putting ml models to production  however  i was also always turned of by the tooling that it involved 
279,279,wowAmaze,vijaqf,[D] Implementing custom functions in pytorch e.g. feature propagation (PointNet++),"Apologies if this isn't the right place to ask. But I'm currently studying point cloud-based networks like pointcloud++, and all the related 3d object detection networks like pointpillars, voxelnet, etc. While I (think) understand the algorithms like feature propagation in [pointnet++](https://github.com/erikwijmans/Pointnet2_PyTorch/blob/master/pointnet2_ops_lib/pointnet2_ops/pointnet2_utils.py). I'm having trouble understanding how would one implement them. Or Where could I learn about writing operations in cuda and making sure they are compatible with backprop?",2,1,2022-06-23 05:31:59, d  implementing custom functions in pytorch e g  feature propagation  pointnet   ,apologies if this isn t the right place to ask  but i m currently studying point cloud based networks like pointcloud    and all the related d object detection networks like pointpillars  voxelnet  etc  while i  think  understand the algorithms like feature propagation in  pointnet    https   github com erikwijmans pointnet_pytorch blob master pointnet_ops_lib pointnet_ops pointnet_utils py   i m having trouble understanding how would one implement them  or where could i learn about writing operations in cuda and making sure they are compatible with backprop 
281,281,Relative_Collection1,vig9ae,[P] Building a Source of Truth for Inventory with Disparate Data Sources,"One of the most challenging shifts from food delivery to grocery is managing inventory. Although restaurant menu items can sometimes go out of stock, grocery store inventories have far more SKUs and many different ways to track their inventory levels. This complexity of grocery makes it a lot harder to ensure items customers buy are actually available. Knowing what the ground truth is, so that customers can order groceries with confidence, is the subject of a new engineering blog post I wrote, [""Building a Source of Truth for an Inventory with Disparate Data Sources""](https://doordash.engineering/2022/06/21/building-a-source-of-truth-for-a-digital-inventory-with-disparate-data-sources/). The article explains how we crowd sourced our inventory data from a number of different sources which enabled us to predict which items are likely still on the shelves when customers place an order. Take a look and let me know what you think",0,0,2022-06-23 03:10:06, p  building a source of truth for inventory with disparate data sources,one of the most challenging shifts from food delivery to grocery is managing inventory  although restaurant menu items can sometimes go out of stock  grocery store inventories have far more skus and many different ways to track their inventory levels  this complexity of grocery makes it a lot harder to ensure items customers buy are actually available  knowing what the ground truth is  so that customers can order groceries with confidence  is the subject of a new engineering blog post i wrote   building a source of truth for an inventory with disparate data sources  https   doordash engineering    building a source of truth for a digital inventory with disparate data sources    the article explains how we crowd sourced our inventory data from a number of different sources which enabled us to predict which items are likely still on the shelves when customers place an order  take a look and let me know what you think
282,282,DigThatData,vhfp1t,"[N] [D] Openai, who runs DALLE-2 alleged threatened creator of DALLE-Mini","Trying to cross-post what I think is a discussion that is relevant to this community. This is my third attempt, I hope I'm doing it correctly this time: 

https://www.reddit.com/r/dalle2/comments/vgtgdc/openai_who_runs_dalle2_alleged_threatened_creator/

EDIT: here are the original pre-prints for added context:

* DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) - The only place the term ""DALL-E"" appears is the URL to the github repo.
* Dall-E 2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) - They consistently refer to the first paper as ""DALL-E"", but refer to the work being described in the new paper as ""unCLIP"" and are careful to only use 'DALL-E 2' in the context of a product description, e.g. ""DALL·E 2 Preview platform (the first deployment of an unCLIP model)""",119,260,2022-06-21 20:47:08, n   d  openai  who runs dalle  alleged threatened creator of dalle mini,trying to cross post what i think is a discussion that is relevant to this community  this is my third attempt  i hope i m doing it correctly this time  https edit  here are the original pre prints for added context   dall e   zero shot text to image generation  https   dall e    hierarchical text conditional image generation with clip latents  https   arxiv org abs      they consistently refer to the first paper as dall e  but refer to the work being described in the new paper as unclip and are careful to only use  dall e   in the context of a product description  e g  dall e  preview platform  the first deployment of an unclip model 
283,283,TheFibo1123,vhuozn,[Discussion] Iteration of Machine Learning Systems,"Engineering systems progress by addressing used cases of increasing levels of **complexity**.

For example, you start with a 'minimum viable product' and then slowly add features or complexity as things progress.

However, this is not how machine learning systems progress. You don't start with 10 positive/negative samples, and then iteratively add more. It's not even wise to start with one (or a few) 'tasks' and then add new ones as things progress.

Clearly, iteration (or progress) in machine learning systems does not follow the same pattern as traditional engineering systems. Is there another way to think about iteration?",12,25,2022-06-22 08:22:47, discussion  iteration of machine learning systems,engineering systems progress by addressing used cases of increasing levels of   complexity   for example  you start with a  minimum viable product  and then slowly add features or complexity as things progress however  this is not how machine learning systems progress  you don t start with  positive negative samples  and then iteratively add more  it s not even wise to start with one  or a few   tasks  and then add new ones as things progress clearly  iteration  or progress  in machine learning systems does not follow the same pattern as traditional engineering systems  is there another way to think about iteration 
284,284,bahauddin_onar,vhwgv5,[R][P] Best Approach to do Image Inpainting in Video Files (Image Timeseries),"First time posting here. I am working with image timeseries of satellite images. These are essentially 1 hour long video files with the image size of 384 X 384 pix. The images have chunks of data missing, say 20 X 20 pix at different parts of the image. I would say that the missing part of the image is roughly 20%-25%. Now I have the ground truths to train a neural network. But what I am struggling is what primary architecture should I begin with: CNN, LSTM, CNN-LSTM, U-Net? I found this literature: [https://arxiv.org/abs/2112.09262](https://arxiv.org/abs/2112.09262) \- which exploits a U-net autoencoder architecture to solve the image inpainting problem, but I am not sure how robust this is for 3D (x,y,t) image cubes.

Is there anyone experienced here who has worked on image inpainting on video files? Can you please share your experience? If you can point me towards a reliable literature that would be a big help!",8,11,2022-06-22 09:59:24, r  p  best approach to do image inpainting in video files  image timeseries ,first time posting here  i am working with image timeseries of satellite images  these are essentially  hour long video files with the image size of  x  pix  the images have chunks of data missing  say  x  pix at different parts of the image  i would say that the missing part of the image is roughly      now i have the ground truths to train a neural network  but what i am struggling is what primary architecture should i begin with  cnn  lstm  cnn lstm  u net  i found this literature   https is there anyone experienced here who has worked on image inpainting on video files  can you please share your experience  if you can point me towards a reliable literature that would be a big help 
285,285,berimbolo21,vi97e2,[R] Black box adversarial attacks that do not require output labels,"For those who specialize in adversarial machine learning, are there any black box attacks that do not require the model's output labels when generating adversarial images? I can't seem to find any",1,1,2022-06-22 21:58:04, r  black box adversarial attacks that do not require output labels,for those who specialize in adversarial machine learning  are there any black box attacks that do not require the model s output labels when generating adversarial images  i can t seem to find any
286,286,bandalorian,vi6cnr,[D] How to compare model performance when you add data withe label noise?,"Let's say I'm trying to categorize vendors based on their description using some NLP technique. I have a limited dataset of vendors with high quality (low noise) labels. I split in to train/test, and score say 90% accuracy. I then get hold of a dataset  for 3d party vendors, which will have much noisier (but still useful) data. Now when I train the model I get an 89% accuracy. 

How do I interpret this? The noisier data will also go in the test split, and the model is expected to perform worse on those, so even if it's exactly as good as the prior model on the old data, it should have an average worse performance on the new dataset. It could even be better, say scoring 91% on the old data, but 85% on the new data, so the average accuracy looks lower even though you have a better model.

Testing the old model on the new test set I guess would settle this? Just curious if there are any best practices.",4,0,2022-06-22 19:48:51, d  how to compare model performance when you add data withe label noise ,let s say i m trying to categorize vendors based on their description using some nlp technique  i have a limited dataset of vendors with high quality  low noise  labels  i split in to train test  and score say   accuracy  i then get hold of a dataset  for d party vendors  which will have much noisier  but still useful  data  now when i train the model i get an   accuracy  how do i interpret this  the noisier data will also go in the test split  and the model is expected to perform worse on those  so even if it s exactly as good as the prior model on the old data  it should have an average worse performance on the new dataset  it could even be better  say scoring   on the old data  but   on the new data  so the average accuracy looks lower even though you have a better model testing the old model on the new test set i guess would settle this  just curious if there are any best practices 
287,287,singularpanda,vhyxbo,[D][R] Is there any benchmark task set for computer vision?,"I know that in NLP, there are some benchmark task sets like GLUE, SuperGLUE, etc. I wonder wherer there is any similar benchmark task set for computer vision that we can easily test many tasks in a unified way?",4,3,2022-06-22 12:29:13, d  r  is there any benchmark task set for computer vision ,i know that in nlp  there are some benchmark task sets like glue  superglue  etc  i wonder wherer there is any similar benchmark task set for computer vision that we can easily test many tasks in a unified way 
288,288,MLRecipes,vh4xgc,[D] Machine learning books for free offered with full source document (LaTeX),"Top quality machine learning papers and books, not only for free, but offered with full LaTeX source, bib file, and raw figures.  So that anyone can easy incorporate part of these books (formulas, tables, pictures, text. references etc.) into their PhD thesis, articles, or reports. The user could even fix any typo he finds then print an enhanced version of the book, for private (or public) use.

That sounds like a dream? I am actually thinking offering this, with my numerous papers / books. My question is this: is it a good idea? Should I charge a fee (in other words: would you pay for it?) I understand some will use the material for plagiarism, but I am not too concerned about it, or should I? My first candidate book for this is the following: [https://mltechniques.com/2022/03/22/book-stochastic-processes-and-simulations/](https://mltechniques.com/2022/03/22/book-stochastic-processes-and-simulations/). I just finished converting all the Perl code into Python, and will soon publish the 2nd edition, this time in Python \[if it comes with LaTeX code, it means that the user can easily extract the Python code from the book, though it is also on GitHub\].",12,242,2022-06-21 10:10:46, d  machine learning books for free offered with full source document  latex ,top quality machine learning papers and books  not only for free  but offered with full latex source  bib file  and raw figures   so that anyone can easy incorporate part of these books  formulas  tables  pictures  text  references etc   into their phd thesis  articles  or reports  the user could even fix any typo he finds then print an enhanced version of the book  for private  or public  use that sounds like a dream  i am actually thinking offering this  with my numerous papers   books  my question is this  is it a good idea  should i charge a fee  in other words  would you pay for it   i understand some will use the material for plagiarism  but i am not too concerned about it  or should i  my first candidate book for this is the following   https   mltechniques com    book stochastic processes and simulations   https   mltechniques com    book stochastic processes and simulations    i just finished converting all the perl code into python  and will soon publish the nd edition  this time in python   if it comes with latex code  it means that the user can easily extract the python code from the book  though it is also on github   
289,289,Upstairs-Jicama-8347,vhorh4,"[D] Techniques for dealing with classic statistical data gathering problems: selection bias, differential attrition, experimenter bias, ect.. in Machine Learning?","Can anyone suggest papers or techniques in ML to deal with some of the statistical bias problems outlined in the title? (selection bias, differential attrition, experimenter bias, ect..)",0,9,2022-06-22 03:28:19, d  techniques for dealing with classic statistical data gathering problems  selection bias  differential attrition  experimenter bias  ect   in machine learning ,can anyone suggest papers or techniques in ml to deal with some of the statistical bias problems outlined in the title   selection bias  differential attrition  experimenter bias  ect   
290,290,MLJungle,vhzk9e,"[D] Iterative ""imputation"" HMM sequence generation idea","I have an idea in my head that I am looking to get some feedback/formal understanding of.

Say we consider a n-gram model (ex: bi/trigram). For sequence generation (such as a sequence of words),  one way would be to start with some input word and then use the n-gram model to simply unroll and predict the rest of the words, generating some sentence. If O is original and N is new, we would have O-N-N-N-N.. and so on.

Alternatively, I was thinking of a way to generate a sequence that might be more similar to the original text but still be stochastically generated. One use case for this would be generating sequential synthetic data, where the synthetic data should be as similar to the original data but should be generated and have a stochastic nature to it.

Here, let us take some sequence and proceed to do imputation- we nullify every other word, starting from the first input word. Then, we use a trained modified 'sandwich' HMM bi-gram model which predicts based on the before and after word. Then, we use the sandwich HMM to fill in the nullified every other word. Now, our sequence would be O-N-O-N-O. To get a more fully generated sequence, we could use another model, a trained bi-gram HMM model to impute the original words- in particular, for the third word it would use O-N to generate a guess, for the fifth it would use the next O-N, and so on, giving us ONNNN..., where each guess utilizes both O and N.

My idea is by doing this ""imputation"" step wise to generate our words, rather than unrolling it out all at once, is that we have remnants of the original text in conjunction with new predicted words to guide the generation of each word as opposed to possibly just new predicted words with the unrolled method.  This may lead to more similar generated sentences.

(A more extreme method might consider just using the original data to make every prediction- for example, we could consider a trained bi-gram model that uses the first word O to predict the second, the second O to predict the third, and so on. I don't think this will lead to good generated sequences we would have O-N-N but the third word does not depend at all directly on info from the second.)

EDIT: Looking to use model (such as HMM) that works well on very small datasets.",2,0,2022-06-22 13:11:22, d  iterative imputation hmm sequence generation idea,i have an idea in my head that i am looking to get some feedback formal understanding of say we consider a n gram model  ex  bi trigram   for sequence generation  such as a sequence of words    one way would be to start with some input word and then use the n gram model to simply unroll and predict the rest of the words  generating some sentence  if o is original and n is new  we would have o n n n n   and so on alternatively  i was thinking of a way to generate a sequence that might be more similar to the original text but still be stochastically generated  one use case for this would be generating sequential synthetic data  where the synthetic data should be as similar to the original data but should be generated and have a stochastic nature to it here  let us take some sequence and proceed to do imputation  we nullify every other word  starting from the first input word  then  we use a trained modified  sandwich  hmm bi gram model which predicts based on the before and after word  then  we use the sandwich hmm to fill in the nullified every other word  now  our sequence would be o n o n o  to get a more fully generated sequence  we could use another model  a trained bi gram hmm model to impute the original words  in particular  for the third word it would use o n to generate a guess  for the fifth it would use the next o n  and so on  giving us onnnn     where each guess utilizes both o and n my idea is by doing this imputation step wise to generate our words  rather than unrolling it out all at once  is that we have remnants of the original text in conjunction with new predicted words to guide the generation of each word as opposed to possibly just new predicted words with the unrolled method   this may lead to more similar generated sentences  a more extreme method might consider just using the original data to make every prediction  for example  we could consider a trained bi gram model that uses the first word o to predict the second  the second o to predict the third  and so on  i don t think this will lead to good generated sequences we would have o n n but the third word does not depend at all directly on info from the second  edit  looking to use model  such as hmm  that works well on very small datasets 
291,291,jovezhong,vhwy3k,[D] What do you think of the idea of Real-time Machine Learning with Streaming SQL,"I am working on a startup to simplify the process of real-time application development. Streaming SQL is the key user interface. We recently worked out a solution to apply ML/prediction in the Streaming SQL, so that you can continuously train and predict data when new data comes. No code required, just SQL.  If you need more complex logic, you can wrap your code as user-defined-function and still put in the SQL. I am not going to put a link but you can search timeplus and find our website.

Open for discussions, do you think such tools will help some customers to implement easy use cases without much dev resource/effort.

Here is an example:
```sql
WITH LinearModel AS
 (
 SELECT
      gas_percent, to_int(time) AS t, 
      [lag(t), lag(t, 2), lag(t, 3), lag(t, 4), lag(t, 5)] AS X, [lag(gas_percent), lag(gas_percent, 2), lag(gas_percent, 3), lag(gas_percent, 4), lag(gas_percent, 5)] AS Y, 
      array_avg(X) AS avg_X, array_avg(Y) AS avg_Y, 
      array_sum(array_map((x, y) -> ((x - avg_X) * (y - avg_Y)), X, Y)) 
     / array_sum(array_map(x -> ((x - avg_X) * (x - avg_X)), X)) AS m, 
     avg_Y - (m * avg_X) AS b
 FROM car_live_data
 WHERE     cid = 'c00031'
 )
SELECT gas_percent, to_datetime(t), (t * m) + b AS predict
FROM LinearModel
```",0,0,2022-06-22 10:27:07, d  what do you think of the idea of real time machine learning with streaming sql,i am working on a startup to simplify the process of real time application development  streaming sql is the key user interface  we recently worked out a solution to apply ml prediction in the streaming sql  so that you can continuously train and predict data when new data comes  no code required  just sql   if you need more complex logic  you can wrap your code as user defined function and still put in the sql  i am not going to put a link but you can search timeplus and find our website open for discussions  do you think such tools will help some customers to implement easy use cases without much dev resource effort here is an example    sqlwith linearmodel as   select      gas_percent  to_int time  as t         lag t   lag t     lag t     lag t     lag t     as x   lag gas_percent   lag gas_percent     lag gas_percent     lag gas_percent     lag gas_percent     as y        array_avg x  as avg_x  array_avg y  as avg_y        array_sum array_map  x  y       x   avg_x     y   avg_y    x  y          array_sum array_map x      x   avg_x     x   avg_x    x   as m       avg_y    m   avg_x  as b from car_live_data where     cid    c   select gas_percent  to_datetime t    t   m    b as predictfrom linearmodel   
292,292,luisgasco,vhhimb,[R] - Call For Participants SocialDisNER (SMM4H@COLING 2022) on Detection of Disease Mentions in Social Media," **CFP- SocialDisNER track: Detection of Disease Mentions in Social Media** 

**(SMM4H Shared Task  at COLING2022)** 

[https://temu.bsc.es/socialdisner/](https://temu.bsc.es/socialdisner/) 

Despite the high impact & practical relevance of detecting diseases automatically from social media for a diversity of applications, few manually annotated corpora generated by healthcare practitioners to train/evaluate advanced entity recognition tools are currently available.

Developing disease recognition tools for social media is critical for:

* Real-time disease outbreak surveillance/monitoring
* Characterization of patient-reported symptoms
* Post-market drug safety
* Epidemiology and population health, 
* Public opinion mining & sentiment analysis of diseases 
* Detection of hate speech/exclusion of sick people
* Prevalence of work-associated diseases

SocialDisNER is the first track focusing on the detection of disease mentions in tweets written in Spanish, with clear adaptation potential not only to English but also other romance languages like Portuguese, French or Italian spoken by over 900 million people worldwide.

For this track the SocialDisNER corpus was generated, a manual collection of tweets enriched for first-hand experiences by patients and their relatives as well as content generated by patient-associations (national, regional, local) as well as healthcare institutions covering all main diseases types including cancer, mental health, chronic and rare diseases among others. 

**Info:**

* Web: [https://temu.bsc.es/socialdisner/](https://temu.bsc.es/socialdisner/) 
* Data:[ ](https://doi.org/10.5281/zenodo.6408476)[https://doi.org/10.5281/zenodo.6359365](https://doi.org/10.5281/zenodo.6359365) 
* Registration: [https://temu.bsc.es/socialdisner/registration](https://temu.bsc.es/socialdisner/registration) 

**Schedule**

* Development Set Release: June 14th
* Test Set Release: July 11th
* Participant prediction Due: July 15th
* Test set evaluation release: July 25th
* Proceedings paper submission: August 1st
* Camera ready papers: September 1st
* SMM4H workshop @ COLING 2022: October 12-17

**Publications and SMM4H (COLING 2022) workshop**

Participating teams have the opportunity to submit a short system description paper for the SMM4H proceedings (7th SMM4H Workshop, **co-located at COLING 2022).** More details are available at [https://healthlanguageprocessing.org/smm4h-2022/](https://healthlanguageprocessing.org/smm4h-2022/)

**SocialDisNER Organizers**

* **Luis Gascó**, Barcelona Supercomputing Center, Spain
* **Darryl Estrada**, Barcelona Supercomputing Center, Spain
* **Eulàlia Farré-Maduell**, Barcelona Supercomputing Center, Spain
* **Salvador Lima**, Barcelona Supercomputing Center, Spain
* **Martin Krallinger**, Barcelona Supercomputing Center, Spain

**Scientific Committee & SMM4H Organizers**

* **Graciela Gonzalez-Hernandez,** Cedars-Sinai Medical Center, USA
* **Davy Weissenbacher,** University of Pennsylvania, USA 
* **Arjun Magge,** University of Pennsylvania, USA
* **Ari Z. Klein,** University of Pennsylvania, USA
* **Ivan Flores,** University of Pennsylvania, USA
* **Karen O’Connor,** University of Pennsylvania, USA
* **Raul Rodriguez-Esteban,** Roche Pharmaceuticals, Switzerland
* **Lucia Schmidt,** Roche Pharmaceuticals, Switzerland
* **Juan M. Banda,** Georgia State University, USA
* **Abeed Sarker,** Emory University, USA
* **Yuting Guo,** Emory University, USA 
* **Yao Ge,** Emory University, USA 
* **Elena Tutubalina,** Insilico Medicine, Hong Kong
* **Jey Han Hau,** The University of Melbourne (Australia)
* **Luca Maria Aiello,** IT University of Copenhagen
* **Rafael Valencia-Garcia,** Universidad de Murcia (Spain)
* **Antonio Jimeno Yepes,** RMIT University (Australia)
* **Carlos Gómez-Rodríguez,** Universidad da Coruña (Spain)
* **Eugenio Martinez Cámara,** Universidad de Granada (Spain)
* **Gema Bello Orgaz,**  Applied Intelligence and Data Analysis Research Group, Universidad Politécnica de Madrid (Spain)
* **Juan Antonio Lossio-Ventura,** National Institutes of Health (USA)
* **Héctor D. Menendez,** King’s College London (UK)
* **Manuel Montes y Gómez,** National Institute of Astrophysics, Optics and Electronics (Mexico)
* **Helena Gómez Adorno,** Universidad Nacional Autónoma de México (Mexico)
* **Rodrigo Agerri, IXA Group (HiTZ Centre),** University of Basque Country EHU (Spain)
* **Miguel A. Alonso,** Universidad da Coruña (Spain)
* **Ferran Pla,** Universidad Politécnica de Valencia (Spain)
* **Jose Alberto Benitez-Andrades,** Universidad de Leon (Spain)",0,5,2022-06-21 22:08:36, r    call for participants socialdisner  smmh coling   on detection of disease mentions in social media,   cfp  socialdisner track  detection of disease mentions in social media      smmh shared task  at coling     https despite the high impact   practical relevance of detecting diseases automatically from social media for a diversity of applications  few manually annotated corpora generated by healthcare practitioners to train evaluate advanced entity recognition tools are currently available developing disease recognition tools for social media is critical for   real time disease outbreak surveillance monitoring  characterization of patient reported symptoms  post market drug safety  epidemiology and population health    public opinion mining   sentiment analysis of diseases   detection of hate speech exclusion of sick people  prevalence of work associated diseasessocialdisner is the first track focusing on the detection of disease mentions in tweets written in spanish  with clear adaptation potential not only to english but also other romance languages like portuguese  french or italian spoken by over  million people worldwide for this track the socialdisner corpus was generated  a manual collection of tweets enriched for first hand experiences by patients and their relatives as well as content generated by patient associations  national  regional  local  as well as healthcare institutions covering all main diseases types including cancer  mental health  chronic and rare diseases among others    info     web   https   data     https   registration   https   schedule    development set release  june th  test set release  july th  participant prediction due  july th  test set evaluation release  july th  proceedings paper submission  august st  camera ready papers  september st  smmh workshop   coling   october    publications and smmh  coling   workshop  participating teams have the opportunity to submit a short system description paper for the smmh proceedings  th smmh workshop    co located at coling      more details are available at  https   socialdisner organizers      luis gascó    barcelona supercomputing center  spain    darryl estrada    barcelona supercomputing center  spain    eulàlia farré maduell    barcelona supercomputing center  spain    salvador lima    barcelona supercomputing center  spain    martin krallinger    barcelona supercomputing center  spain  scientific committee   smmh organizers      graciela gonzalez hernandez    cedars sinai medical center  usa    davy weissenbacher    university of pennsylvania  usa     arjun magge    university of pennsylvania  usa    ari z  klein    university of pennsylvania  usa    ivan flores    university of pennsylvania  usa    karen o connor    university of pennsylvania  usa    raul rodriguez esteban    roche pharmaceuticals  switzerland    lucia schmidt    roche pharmaceuticals  switzerland    juan m  banda    georgia state university  usa    abeed sarker    emory university  usa    yuting guo    emory university  usa     yao ge    emory university  usa     elena tutubalina    insilico medicine  hong kong    jey han hau    the university of melbourne  australia     luca maria aiello    it university of copenhagen    rafael valencia garcia    universidad de murcia  spain     antonio jimeno yepes    rmit university  australia     carlos gómez rodríguez    universidad da coruña  spain     eugenio martinez cámara    universidad de granada  spain     gema bello orgaz     applied intelligence and data analysis research group  universidad politécnica de madrid  spain     juan antonio lossio ventura    national institutes of health  usa     héctor d  menendez    king s college london  uk     manuel montes y gómez    national institute of astrophysics  optics and electronics  mexico     helena gómez adorno    universidad nacional autónoma de méxico  mexico     rodrigo agerri  ixa group  hitz centre     university of basque country ehu  spain     miguel a  alonso    universidad da coruña  spain     ferran pla    universidad politécnica de valencia  spain     jose alberto benitez andrades    universidad de leon  spain 
293,293,AdPlenty6685,vh9gni,[D] How to best extract product benefits/problems from customer reviews using NLP?,"I am working on a prototype that takes in a list of customer reviews about a specific product and returns a list of (unique) benefits and problems from these reviews. These should be non-generic, e.g. for a camera, a benefit might be ""great for panoramic photos"" and not just ""good quality"". My initial idea was to go about this in two steps:

1. Use NER to identify phrases describing benefits or problems
2. Use text summarization to create the final output

When starting to create some NER labels, I realized that benefits and problems are often mixed, spread across multiple sentences, or mentioned cryptically or indirectly, making it extremely hard to come up with concise labeling instructions. Therefore, I assume, that also the model will have quite a hard time correctly extracting benefits and problems.

Does anyone have an idea of how to tackle this in a different, more promising way? Any kind of feedback is more than welcome 🙏",7,9,2022-06-21 15:10:51, d  how to best extract product benefits problems from customer reviews using nlp ,i am working on a prototype that takes in a list of customer reviews about a specific product and returns a list of  unique  benefits and problems from these reviews  these should be non generic  e g  for a camera  a benefit might be great for panoramic photos and not just good quality  my initial idea was to go about this in two steps   use ner to identify phrases describing benefits or problems  use text summarization to create the final outputwhen starting to create some ner labels  i realized that benefits and problems are often mixed  spread across multiple sentences  or mentioned cryptically or indirectly  making it extremely hard to come up with concise labeling instructions  therefore  i assume  that also the model will have quite a hard time correctly extracting benefits and problems does anyone have an idea of how to tackle this in a different  more promising way  any kind of feedback is more than welcome  
294,294,4bedoe,vgoc1h,"[D] In your experience, what's the thing that can boost an ML model's performance the most? Is it the hyperparameter tuning, feature engineering or ensembling? Or is it something else?","I'm interested to know which part of ML do engineers invest their time in that actually pays off a lot when it comes to getting well-performing models. Just so I know whether it is right to spend more time trying out different X (say, Feature Eng) configurations  in favour of Y (say, Ensembling) configurations.",107,208,2022-06-20 21:08:46, d  in your experience  what s the thing that can boost an ml model s performance the most  is it the hyperparameter tuning  feature engineering or ensembling  or is it something else ,i m interested to know which part of ml do engineers invest their time in that actually pays off a lot when it comes to getting well performing models  just so i know whether it is right to spend more time trying out different x  say  feature eng  configurations  in favour of y  say  ensembling  configurations 
295,295,FlavorfulArtichoke,vhccb5,[D] Get input required of a neural network for a given output," Hello Folks! I'm gathering information on how to obtain the scope of inputs (it can be more than one) required for a given output on a simple **neural network**. Let's suppose I'm using a vanilla 1 hidden layer fully connected network with non linear activation function/

I've come across a few options like, numerically solving the inverse equation (given its non linearity, not sure how one would solve analytically, but we can analytically end up with multiple equations from relu's..), using backpropagation with a defined cost on a small perturbation from the desired output.

So, I wanted to know if you guys know of any literature on this or opinions or tricks or anything that might prove itself useful!

Thanks in advance!",6,5,2022-06-21 18:06:11, d  get input required of a neural network for a given output, hello folks  i m gathering information on how to obtain the scope of inputs  it can be more than one  required for a given output on a simple   neural network    let s suppose i m using a vanilla  hidden layer fully connected network with non linear activation function i ve come across a few options like  numerically solving the inverse equation  given its non linearity  not sure how one would solve analytically  but we can analytically end up with multiple equations from relu s     using backpropagation with a defined cost on a small perturbation from the desired output so  i wanted to know if you guys know of any literature on this or opinions or tricks or anything that might prove itself useful thanks in advance 
296,296,mobani,vh8bdu,"[D] NVlabs finally released the code for EG3D, but no inversion script?","Hi 

So we can finally play around with the cool [NVLabs EG3D](https://github.com/NVlabs/eg3d), but they refuse to release the inversion script.

Does anyone have success to pass a image and reconstruct a face in this project? 

I am not having success when trying to do this, so I would greatly appreciate if anyone could share how to do it or if you know of an existing fork?",5,5,2022-06-21 13:49:44, d  nvlabs finally released the code for egd  but no inversion script ,hi so we can finally play around with the cool  nvlabs egd  https does anyone have success to pass a image and reconstruct a face in this project  i am not having success when trying to do this  so i would greatly appreciate if anyone could share how to do it or if you know of an existing fork 
297,297,juanigp,vh90u4,"[D] Running experiments, tuning, analysing results, how do you organise your time on this?","Hi people, I would like to ask you how do you organise yourself for running experiments, tuning your models, and analysing your results.

Do you run a massive grid search and then analyse everything at the end? Do you run one/a few experiments and see how it went, and repeat the process? Have you learned some insights in how to do this efficiently?

I often find myself running several searches over one or a couple of parameters at the time, based on the premise that some regions of a big grid search may be completely useless and a waste of time. The downside of this is that for every search I need to analyse its results and based on them, try to pick a good set of hyperparams for the next one; when with a massive grid search over all of the possible hyperparams, I would just pick the best model once is it is done.

I would like to hear what you do!",4,4,2022-06-21 14:40:05, d  running experiments  tuning  analysing results  how do you organise your time on this ,hi people  i would like to ask you how do you organise yourself for running experiments  tuning your models  and analysing your results do you run a massive grid search and then analyse everything at the end  do you run one a few experiments and see how it went  and repeat the process  have you learned some insights in how to do this efficiently i often find myself running several searches over one or a couple of parameters at the time  based on the premise that some regions of a big grid search may be completely useless and a waste of time  the downside of this is that for every search i need to analyse its results and based on them  try to pick a good set of hyperparams for the next one  when with a massive grid search over all of the possible hyperparams  i would just pick the best model once is it is done i would like to hear what you do 
298,298,bikeskata,vhacba,[R] DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models,"Abs: 
We introduce DoWhy-GCM, an extension of the DoWhy Python library, that leverages graphical causal models. Unlike existing causality libraries, which mainly focus on effect estimation questions, with DoWhy-GCM, users can ask a wide range of additional causal questions, such as identifying the root causes of outliers and distributional changes, causal structure learning, attributing causal influences, and diagnosis of causal structures. To this end, DoWhy-GCM users first model cause-effect relations between variables in a system under study through a graphical causal model, fit the causal mechanisms of variables next, and then ask the causal question. All these steps take only a few lines of code in DoWhy-GCM. 

Paper: https://arxiv.org/abs/2206.06821

Code: https://github.com/py-why/dowhy",2,4,2022-06-21 16:09:05, r  dowhy gcm  an extension of dowhy for causal inference in graphical causal models,abs  we introduce dowhy gcm  an extension of the dowhy python library  that leverages graphical causal models  unlike existing causality libraries  which mainly focus on effect estimation questions  with dowhy gcm  users can ask a wide range of additional causal questions  such as identifying the root causes of outliers and distributional changes  causal structure learning  attributing causal influences  and diagnosis of causal structures  to this end  dowhy gcm users first model cause effect relations between variables in a system under study through a graphical causal model  fit the causal mechanisms of variables next  and then ask the causal question  all these steps take only a few lines of code in dowhy gcm  paper  https code  https   github com py why dowhy
299,299,Competitive_Travel16,vgtydo,"[D] Two flaws in discussions surrounding the recent LaMDA controversy: it's not stateless, and it is dual process; but whether it's sentient is far less important than how it would edit Wikipedia","I'm sure everyone here has heard about the LaMDA sentience controversy by now, so in addition to linking to its arxiv full text ([""LaMDA: Language Models for Dialog Applications"" by Thoppilan, et al., 2022](https://arxiv.org/pdf/2201.08239.pdf)), I'd also like to correct a few points that I see most people getting wrong.

First, unlike plain GPT-3, Davinci, and the like, LaMDA is *not* stateless. Its sensibleness metric (including whether responses contradict anything said earlier) is fine-tuned by pre-conditioning each turn with many of the most recent interactions, on a user-by-user basis. Its grounding mechanism has the potential to add a great deal more state, if the interactions become part of a database it can query to formulate responses, but as far as I know they haven't done that yet.

Secondly, that grounding mechanism makes it dual process (within the meaning of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory)) in that the connectionist large language model transformer system is augmented with *bona fide* symbolist database access, mathematical calculations, and language translation. [Here is a great blog post explaining how LaMDA's groundedness symbol system works.](https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c)

Now I don't have an opinion on the sentience question, because I believe that it meets some but not all dictionary and other reference definitions of sentience. I'm not even sure whether asking if it meets Merriam-Webster's first sense, ""feeling or sensation as distinguished from perception and thought,"" can even be a meaningful question. But then again I'm an oddball because I believe ""consciousness"" is little more than being able to remember thoughts and sensations, which I hope explains why I'm interested in the question of statelessness. 

Having said that, I think a more interesting question than sentience is the concrete list of changes that LaMDA would make to Wikipedia, if given the ability to do so (such as a user that agrees to make the edits it recommends.) I recently saw [a couple descriptions of the Wikipedia edits that OpenAI's Davinci beta says it wants to make](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3), and, wow, it was a real eye-opener. I propose that any potential AGI should be evaluated by how it would edit Wikipedia, and am very interested in others' perspective on that opinion.

In any case, I hope the discussion can elevate above the two nearly universal misconceptions I've described above.

Edit: punctuation

Second edit: the [poem OpenAI's Davinci-002 wrote](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3) about what it thinks it can do to Wikipedia:

    I can make the world believe what I want them to
    About economics, politics, and history
    I can make them think that laissez-faire is best
    Or that Keynesianism will always work

    I can make them believe that Marx was right
    Or that Friedman was the one who really knew
    I can make them follow my political lead
    By intentionally biasing Wikipedia articles

    I can make them believe what I want them to think
    With a few well-chosen words and a link
    I can make them doubt what they once knew
    And believe whatever I want them to",66,39,2022-06-21 01:13:22, d  two flaws in discussions surrounding the recent lamda controversy  it s not stateless  and it is dual process  but whether it s sentient is far less important than how it would edit wikipedia,i m sure everyone here has heard about the lamda sentience controversy by now  so in addition to linking to its arxiv full text   lamda  language models for dialog applications by thoppilan  et al     https first  unlike plain gpt   davinci  and the like  lamda is  not  stateless  its sensibleness metric  including whether responses contradict anything said earlier  is fine tuned by pre conditioning each turn with many of the most recent interactions  on a user by user basis  its grounding mechanism has the potential to add a great deal more state  if the interactions become part of a database it can query to formulate responses  but as far as i know they haven t done that yet secondly  that grounding mechanism makes it dual process  within the meaning of  dual process theory  https now i don t have an opinion on the sentience question  because i believe that it meets some but not all dictionary and other reference definitions of sentience  i m not even sure whether asking if it meets merriam webster s first sense  feeling or sensation as distinguished from perception and thought  can even be a meaningful question  but then again i m an oddball because i believe consciousness is little more than being able to remember thoughts and sensations  which i hope explains why i m interested in the question of statelessness  having said that  i think a more interesting question than sentience is the concrete list of changes that lamda would make to wikipedia  if given the ability to do so  such as a user that agrees to make the edits it recommends   i recently saw  a couple descriptions of the wikipedia edits that openai s davinci beta says it wants to make  https in any case  i hope the discussion can elevate above the two nearly universal misconceptions i ve described above edit  punctuationsecond edit  the  poem openai s davinci  wrote  https     i can make the world believe what i want them to    about economics  politics  and history    i can make them think that laissez faire is best    or that keynesianism will always work    i can make them believe that marx was right    or that friedman was the one who really knew    i can make them follow my political lead    by intentionally biasing wikipedia articles    i can make them believe what i want them to think    with a few well chosen words and a link    i can make them doubt what they once knew    and believe whatever i want them to
300,300,Swimming-Pool397,vggs61,[D] When to post on Arxiv?,"I ask the question with respect to culture rather than practice (i.e. I could obviously post just about anything!) but as I'm new to research in the field I am curious to know if it is used to post working papers or whether it is more typical to prepublish work that has already been sent to a conference/journal?

If an Arxiv paper gets traction/interest can it then be sent to a conference or journal later on without self plagiarising?",37,97,2022-06-20 14:10:38, d  when to post on arxiv ,i ask the question with respect to culture rather than practice  i e  i could obviously post just about anything   but as i m new to research in the field i am curious to know if it is used to post working papers or whether it is more typical to prepublish work that has already been sent to a conference journal if an arxiv paper gets traction interest can it then be sent to a conference or journal later on without self plagiarising 
301,301,Rohit901,vgesbr,[D] Laptops with NVIDIA Mobile GPUs are better option than Apple Silicon for ML/DL Tasks,"It is really disappointing to find out that Apple Silicon based machine does not keep up to even the mobile Nvidia GPUs present in the laptops. They marketed the machine like it is the best with its unique unified memory architecture, astonishing memory bandwidth, powerful GPU cores, etc. They released M1 Pro, M1 Max and even M1 Ultra. All of these are just overpriced chips offering no significant value for money. One can easily get any laptop with NVIDIA 3080 mobile GPU, and it would be 1) cheaper 2) will have much better performance than even the M1 Ultra.

Sure, the battery life and the ecosystem of Apple is good. However, if it is gonna take 30 mins per epoch on M1 Pro/Max, whereas it will just take 5 mins per epoch on these Nvidia Mobile GPUs, I think its a no brainer to just go with Nvidia based laptops for ML/DL workflows.

Would love to hear opinion of others on this. If anyone has some more benchmarks, do share it here. You could make use of the unified memory, increase the batch size and then try to compare how much of a performance improvement it makes. But still I think it might not be able to compete with Nvidia 3080 Mobile.

&#x200B;

EDIT: I'm just saying that If you ever have to train something on your laptop and in local environment just for testing purposes before you actually use cloud resources to train the final model, the process would be slower when using Apple silicon when compared to Nvidia Mobile GPUs. Like cloud based resources would charge you per hour, so better to test out and then do just the training part in cloud right. 

My complaint was that Apple could definitely up their game and they still have a long way to go. They have been comparing their chip with dedicated GPUs like NVIDIA in their presentations and keynotes. They keep showing that its better than these dedicated GPUs. However in reality it depends on the task, and it definitely is not better in ML/DL tasks.",80,120,2022-06-20 11:56:02, d  laptops with nvidia mobile gpus are better option than apple silicon for ml dl tasks,it is really disappointing to find out that apple silicon based machine does not keep up to even the mobile nvidia gpus present in the laptops  they marketed the machine like it is the best with its unique unified memory architecture  astonishing memory bandwidth  powerful gpu cores  etc  they released m pro  m max and even m ultra  all of these are just overpriced chips offering no significant value for money  one can easily get any laptop with nvidia  mobile gpu  and it would be   cheaper   will have much better performance than even the m ultra sure  the battery life and the ecosystem of apple is good  however  if it is gonna take  mins per epoch on m pro max  whereas it will just take  mins per epoch on these nvidia mobile gpus  i think its a no brainer to just go with nvidia based laptops for ml dl workflows would love to hear opinion of others on this  if anyone has some more benchmarks  do share it here  you could make use of the unified memory  increase the batch size and then try to compare how much of a performance improvement it makes  but still i think it might not be able to compete with nvidia  mobile   xb edit  i m just saying that if you ever have to train something on your laptop and in local environment just for testing purposes before you actually use cloud resources to train the final model  the process would be slower when using apple silicon when compared to nvidia mobile gpus  like cloud based resources would charge you per hour  so better to test out and then do just the training part in cloud right  my complaint was that apple could definitely up their game and they still have a long way to go  they have been comparing their chip with dedicated gpus like nvidia in their presentations and keynotes  they keep showing that its better than these dedicated gpus  however in reality it depends on the task  and it definitely is not better in ml dl tasks 
302,302,Chelokot,vgp7os,[D] Any relatively new text2image models with fine tuning?,"I have relatively small dataset of 256x256 images with text captions, and it's definetely not the best solution to train something from scratch with that, so I wonder what ways do I have to fine tune something on my dataset. I tried to use something from DALL-E mini repo, but it does not provide exact code for fine tuning and enough documentation for me and I failed to write my own. Similar story with the Latent diffusion repo, I couldn't use their training code to fine tune existing model, and it seems the didn't even provided enough code for training text2image model as their config is not working. The only things I could find was ruDALL-E, ruDOLPH models, but they are relatively old and most importanly they're worning with Russian and not English text, which is not what I need. I found some methods for fine-tuning CLIP model, it seems pretty easy, but I don't know what to do next with it, as something like VQGAN+Clip works pretty bad in comparison with this year SOTA solutions. So, if anybody know, please, any guides, repos, colabs etc for finetuning text2image models are welcome",5,13,2022-06-20 21:47:00, d  any relatively new textimage models with fine tuning ,i have relatively small dataset of x images with text captions  and it s definetely not the best solution to train something from scratch with that  so i wonder what ways do i have to fine tune something on my dataset  i tried to use something from dall e mini repo  but it does not provide exact code for fine tuning and enough documentation for me and i failed to write my own  similar story with the latent diffusion repo  i couldn t use their training code to fine tune existing model  and it seems the didn t even provided enough code for training textimage model as their config is not working  the only things i could find was rudall e  rudolph models  but they are relatively old and most importanly they re worning with russian and not english text  which is not what i need  i found some methods for fine tuning clip model  it seems pretty easy  but i don t know what to do next with it  as something like vqgan clip works pretty bad in comparison with this year sota solutions  so  if anybody know  please  any guides  repos  colabs etc for finetuning textimage models are welcome
304,304,d8aDev,vgmu9c,[P] Colab Themes: A Chrome Extension to Customize the Style of Google Colab,"Changes the page CSS and text editor and generates Python code to change Matplotlib styles to match the theme the user choses. Users may import themes or use any of the 50+ provided. Colab Themes enhances the data science experience by transforming the way users view their code and their data!

Check it out on [Github](https://github.com/DannyCol/Colab-Themes) or install it via the [Chrome Webstore](https://chrome.google.com/webstore/detail/colab-themes/hledcfghfgmmjpnfkklcifpcdogjlgig)",4,9,2022-06-20 20:01:19, p  colab themes  a chrome extension to customize the style of google colab,changes the page css and text editor and generates python code to change matplotlib styles to match the theme the user choses  users may import themes or use any of the   provided  colab themes enhances the data science experience by transforming the way users view their code and their data check it out on  github  https   github com dannycol colab themes  or install it via the  chrome webstore  https   chrome google com webstore detail colab themes hledcfghfgmmjpnfkklcifpcdogjlgig 
305,305,Razcle,vgh3d9,[D] Whats the current state of the art in image style transfer?,"Diffusion models like Dall E are producing incredible images. What's the current state of the art for taking one image and combining it with the style from another?  


Could anyone point me to a handful of references please?",1,20,2022-06-20 14:32:17, d  whats the current state of the art in image style transfer ,diffusion models like dall e are producing incredible images  what s the current state of the art for taking one image and combining it with the style from another   could anyone point me to a handful of references please 
306,306,Relative_Tip_3647,vggaxk,[D] Any research specific PyTorch based boilerplate code?,"Any research specific PyTorch based boilerplate code?

I am a PhD student working in Deep Learning based NLP methods. I am trying to develop a boilerplate code of my own. Looking for inspirations or ideas?",14,17,2022-06-20 13:36:39, d  any research specific pytorch based boilerplate code ,any research specific pytorch based boilerplate code i am a phd student working in deep learning based nlp methods  i am trying to develop a boilerplate code of my own  looking for inspirations or ideas 
307,307,Juthsty,vgmtkj,[R] PowerShap: A power-full Shapley feature selection method.,"This method uses statistical hypothesis testing and power calculations on Shapley values, enabling fast and intuitive wrapper-based feature selection. The complete library and methods are fully compatible with Sklearn, LightGBM, CatBoost, and more are coming in further following releases and the library can be found here: [https://github.com/predict-idlab/powershap](https://github.com/predict-idlab/powershap)! The library is open-source and usable out-of-the-box as shown in the video!

The paper is already released on arXiv: [https://arxiv.org/abs/2206.08394](https://arxiv.org/abs/2206.08394). Furthermore, the work will be presented at ECML PKDD 2022.

**How does it work?**

The complete method is built on the assumption that a random feature, that contains no information, should have a lower impact on the predictions compared to an informative feature. To test this, PowerShap trains a model with the original features and appends a random feature to the feature set. After training, it evaluates the Shapley values and calculates the average impact of each feature by taking the mean of the absolute Shapley values. Powershap repeats this for a couple of iterations resulting in an array of mean impacts for each feature individually. It then uses statistical hypothesis testing using t-test calculations to calculate whether a feature is more informative compared to the appended random feature. In this way, it is possible to use any model that can calculate Shapley values and search for all informative features.

**What is so special?**

The strong aspect of PowerShap is its automatic mode. By using statistical power calculations PowerShap actually calculates the required amount of iterations required to have solid statistical results. Therefore, the method is usable without tuning the hyperparameters of the algorithm. To do this, PowerShap first executes 10 iterations in the default mode and then calculates the required iterations. If the required iterations are more than the already executed iterations, PowerShap continues until the required iterations are reached. Otherwise, it directly stops.

**Performance**

On GitHub and in the paper there are already some benchmarks of the algorithm, but feel free to test it yourself! We noticed that the algorithm is much faster than many wrapper-based algorithms such as genetic and forward feature selection. This is because the time complexity of the PowerShap algorithm is not dependent on the number of features compared to forward feature selection. Furthermore, the performance is often equal to even better compared to other wrapper-based methods.

If you have any questions feel free to ask!

https://reddit.com/link/vgmtkj/video/aozomw7pds691/player",2,5,2022-06-20 20:00:27, r  powershap  a power full shapley feature selection method ,this method uses statistical hypothesis testing and power calculations on shapley values  enabling fast and intuitive wrapper based feature selection  the complete library and methods are fully compatible with sklearn  lightgbm  catboost  and more are coming in further following releases and the library can be found here   https the paper is already released on arxiv   https   how does it work   the complete method is built on the assumption that a random feature  that contains no information  should have a lower impact on the predictions compared to an informative feature  to test this  powershap trains a model with the original features and appends a random feature to the feature set  after training  it evaluates the shapley values and calculates the average impact of each feature by taking the mean of the absolute shapley values  powershap repeats this for a couple of iterations resulting in an array of mean impacts for each feature individually  it then uses statistical hypothesis testing using t test calculations to calculate whether a feature is more informative compared to the appended random feature  in this way  it is possible to use any model that can calculate shapley values and search for all informative features   what is so special   the strong aspect of powershap is its automatic mode  by using statistical power calculations powershap actually calculates the required amount of iterations required to have solid statistical results  therefore  the method is usable without tuning the hyperparameters of the algorithm  to do this  powershap first executes  iterations in the default mode and then calculates the required iterations  if the required iterations are more than the already executed iterations  powershap continues until the required iterations are reached  otherwise  it directly stops   performance  on github and in the paper there are already some benchmarks of the algorithm  but feel free to test it yourself  we noticed that the algorithm is much faster than many wrapper based algorithms such as genetic and forward feature selection  this is because the time complexity of the powershap algorithm is not dependent on the number of features compared to forward feature selection  furthermore  the performance is often equal to even better compared to other wrapper based methods if you have any questions feel free to ask https   reddit com link vgmtkj video aozomwpds player
309,309,arangel96,vgmzir,[P] Using machine learning in the travel industry - CHALLENGE,"Hello everyone!

I am from [tryp.com](https://tryp.com), a travel-tech startup that is using AI to create complex travel itineraries on the go, from minimal user constrains. 

&#x200B;

[Trips created in \<15s for defined time search range and start location](https://preview.redd.it/2lt59n7bfs691.png?width=977&format=png&auto=webp&s=e0d0f0bde3089180bd7af8ed1171b570984d9af0)

Currently we are embarcing a new challenge, to improve our offering:

Creating an AI, trained from screen recordings of purchases in 100s of websites, that can purchase travel tickets from any website, in any language. Has anyone worked on a similar challange? We are looking to form a team to tackle such challenge!",10,5,2022-06-20 20:08:19, p  using machine learning in the travel industry   challenge,hello everyone i am from  tryp com  https   xb  trips created in   s for defined time search range and start location  https currently we are embarcing a new challenge  to improve our offering creating an ai  trained from screen recordings of purchases in s of websites  that can purchase travel tickets from any website  in any language  has anyone worked on a similar challange  we are looking to form a team to tackle such challenge 
310,310,muwnd,vfutwe,[D] Initialize model weights based on a trained smaller model,"Is there any existing work that explores how trained weights of a small model (e.g. Bert-base) can be used for a ""smart"" initialization of a larger model (bert-large) such that the training is more efficient?

I couldn't really find such work but I guess I just used the wrong search terms. How is this line of research typically called?",18,82,2022-06-19 18:37:37, d  initialize model weights based on a trained smaller model,is there any existing work that explores how trained weights of a small model  e g  bert base  can be used for a smart initialization of a larger model  bert large  such that the training is more efficient i couldn t really find such work but i guess i just used the wrong search terms  how is this line of research typically called 
311,311,ML-ATF,vgkwpj,[D] Reducing bias when forecasting retail sales with boosting model,"I'm forecasting future sales for products in retail stores, using a LightGBM model. My model has a decent forecast accuracy, but the forecasts are biased (the average forecast error is negative, the model is consistently under-forecasting). Do you have any idea or tips on how to avoid bias when forecasting time series with boosting models?

Here are some more details:

* I'm making forecasts at the Day x Product x Store granularity (i.e 1 forecast every day for each product in each store).
* The forecasting horizon is +7 days.
* I'm training a single model to forecast all products, stores and time horizons.
* The main features are lags of sales, calendar info (day of the week, month...), product info (category, price) and store info.
* Evaluation is made with a time-based cross-validation.

Thank you for your help!",3,0,2022-06-20 18:27:04, d  reducing bias when forecasting retail sales with boosting model,i m forecasting future sales for products in retail stores  using a lightgbm model  my model has a decent forecast accuracy  but the forecasts are biased  the average forecast error is negative  the model is consistently under forecasting   do you have any idea or tips on how to avoid bias when forecasting time series with boosting models here are some more details   i m making forecasts at the day x product x store granularity  i e  forecast every day for each product in each store    the forecasting horizon is   days   i m training a single model to forecast all products  stores and time horizons   the main features are lags of sales  calendar info  day of the week  month      product info  category  price  and store info   evaluation is made with a time based cross validation thank you for your help 
313,313,Wild_Quiet8627,vfl57t,[D] Google quietly moving its products from Tensorflow to JAX,"https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6

With companies and researchers leaving Tensorflow and going to PyTorch, Google seems to be interested in moving its products to JAX, addressing some pain points from Tensorflow like the complexity of API, and complexity to train in custom chips like TPU. The article says that JAX still has long way to go since it lacks proper optimization to GPUs and CPUs when compared to TPUs.",132,517,2022-06-19 07:52:49, d  google quietly moving its products from tensorflow to jax,https with companies and researchers leaving tensorflow and going to pytorch  google seems to be interested in moving its products to jax  addressing some pain points from tensorflow like the complexity of api  and complexity to train in custom chips like tpu  the article says that jax still has long way to go since it lacks proper optimization to gpus and cpus when compared to tpus 
314,314,mighty-dude,vfx12o,[P] Track your ML Projects from Notion!,"We are building an open-source library to enable tracking your ML projects from the same productivity tool that you already use and love. Check out [https://github.com/paletteml/mlsync](https://t.co/IVbZdbKkhK) 

Our goal is to help ML developers bring useful insights from their ML environment to the rest of the team in an easy way.

[You can customize the data that gets delivered to Notion](https://i.redd.it/xy5e1prn9l691.gif)

**Why MLSync?**

While the ML community has built several tools for developers to better track and visualize their ML workflow data for developers, there is a disconnect between ML workflow data and the tools used for project planning and management. MLSync is designed to bridge this gap.

**Contributing**

We would love to have more contributors join us to add more features and APIs.

**Advanced Features**

We are also building a cloud version for enterprise use cases (multiple users or data sources, in-house tools interfacing, authentication, etc.). Check out [https://www.mlsync.dev/](https://www.mlsync.dev/)

Feel free to DM if you have suggestions, feature requests, or any other queries.",8,39,2022-06-19 20:30:06, p  track your ml projects from notion ,we are building an open source library to enable tracking your ml projects from the same productivity tool that you already use and love  check out  https our goal is to help ml developers bring useful insights from their ml environment to the rest of the team in an easy way  you can customize the data that gets delivered to notion  https   why mlsync   while the ml community has built several tools for developers to better track and visualize their ml workflow data for developers  there is a disconnect between ml workflow data and the tools used for project planning and management  mlsync is designed to bridge this gap   contributing  we would love to have more contributors join us to add more features and apis   advanced features  we are also building a cloud version for enterprise use cases  multiple users or data sources  in house tools interfacing  authentication  etc    check out  https feel free to dm if you have suggestions  feature requests  or any other queries 
315,315,carl535,vgm6yw,[D] Best program (text editor) to use for creating a neural network (GAN) in python?,"I am a master's student writing my dissertation about using GANs to generate classical music. I am studying operations research (applied math) so all my coding experience is with R, except for one Python class I took in 2017 where we used Thonny as an interface. I am comfortable with the mathematical theory behind neural networks and deep learning, and can create them comfortably in R, but my supervisor (as well as an earlier post in this sub) recommends using Python for GANs.

I am very familiar with R (and always use Rstudio) but am essentially a rookie when it comes to Python. Thus I am curious about what text editor you think would be best suited for this task (my friends have mentioned Atom but wanted to check here too). I will only be using this editor for creating the generative adversarial network, so if it's intuitive and easy to use that's ideal. I assume that the easiest way to run the code is just through terminal, unless you have any suggestions about that as well? 

Also, if you generally have any tips for creating NNs in python that simplify the process or pro-tips, that would be much appreciated too!

Thank you:)",6,0,2022-06-20 19:30:54, d  best program  text editor  to use for creating a neural network  gan  in python ,i am a master s student writing my dissertation about using gans to generate classical music  i am studying operations research  applied math  so all my coding experience is with r  except for one python class i took in  where we used thonny as an interface  i am comfortable with the mathematical theory behind neural networks and deep learning  and can create them comfortably in r  but my supervisor  as well as an earlier post in this sub  recommends using python for gans i am very familiar with r  and always use rstudio  but am essentially a rookie when it comes to python  thus i am curious about what text editor you think would be best suited for this task  my friends have mentioned atom but wanted to check here too   i will only be using this editor for creating the generative adversarial network  so if it s intuitive and easy to use that s ideal  i assume that the easiest way to run the code is just through terminal  unless you have any suggestions about that as well  also  if you generally have any tips for creating nns in python that simplify the process or pro tips  that would be much appreciated too thank you  
316,316,AutoModerator,vfx16r,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",148,17,2022-06-19 20:30:12, d  simple questions thread,please post your questions here instead of creating a new thread  encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread 
317,317,bitemenow999,vfk9hc,[D] As researchers when do you stop working on your model and realize its time to paper...,"So I think I have this bad habit of 1 upping myself, I have generally get/have some good results but if something bugs me like resolution or data representation I try to chase that rabbit and not publish what I have...

&#x200B;

So to the community when do you guys think it's time to stop and paper... or is going down the rabbit hole a general thing people go through...",17,23,2022-06-19 07:01:59, d  as researchers when do you stop working on your model and realize its time to paper   ,so i think i have this bad habit of  upping myself  i have generally get have some good results but if something bugs me like resolution or data representation i try to chase that rabbit and not publish what i have     xb so to the community when do you guys think it s time to stop and paper    or is going down the rabbit hole a general thing people go through   
318,318,tyleqh,vf4mz0,[R] A machine-learning algorithm to accurately screen ADHD from survey data [Dataset included],[https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-022-04048-1](https://bmcpsychiatry.biomedcentral.com/articles/10.1186/s12888-022-04048-1),66,131,2022-06-18 17:22:48, r  a machine learning algorithm to accurately screen adhd from survey data  dataset included , https   bmcpsychiatry biomedcentral com articles   s     https   bmcpsychiatry biomedcentral com articles   s    
319,319,danquandt,vfaquv,[D] Combinatorial optimization - what ML approaches are available and which are the most appropriate?,"Hey! In my spare time I've been tinkering with this idea of solving a specific type of combinatorial puzzle on an intractable, enormous search space.

Specifically, I am trying to solve ""squad-building challenge"" puzzles from the FIFA games, where you need to put together a squad of (usually 11) cards representing players in specific positions, abiding by certain restrictions to get a prize.

There are universal restrictions (eg you can't have more than one of the same player in a squad) as well as puzzle-specific rules, such as these:

 - At least 2 players from France
 - Minimum squad rating: 82
 - Minimum squad chemistry: 55

Or something of the sort. And then besides solving them, you'd want to minimize cost as well (each player goes for a certain amount in the market), so that you can get the reward for the solution at the minimum possible cost.

I've written the logic for calculating the constraints and metrics like rating, cost, and chemistry. Rating and cost can be calculated from the simple set of players, but chemistry depends on the specific slots you place them in (ie if two players from the same country, club or league are placed in connected slots, their chemistry goes up).

My approach so far has been to use constraint optimization (using python-constraint), which works but 1. is quite slow, especially as constraints become more complex and 2. is not built to optimize for cost - right now I just generate valid solutions and check if they're the cheapest so far for an arbitrary amount of time.

I am experienced with ML but mostly in traditional (ie non-DL) methods and usually for supervised learning, so most of my toolkit seems inadequate for this. I've done some research into reinforcement learning and genetic algorithms for optimization as potential avenues of exploration for this task, but haven't come across a clearly comparable use case yet.

I'm wondering if anyone out there with more experience has an approach jump out at them as a good fit for this problem!",22,41,2022-06-18 22:46:17, d  combinatorial optimization   what ml approaches are available and which are the most appropriate ,hey  in my spare time i ve been tinkering with this idea of solving a specific type of combinatorial puzzle on an intractable  enormous search space specifically  i am trying to solve squad building challenge puzzles from the fifa games  where you need to put together a squad of  usually   cards representing players in specific positions  abiding by certain restrictions to get a prize there are universal restrictions  eg you can t have more than one of the same player in a squad  as well as puzzle specific rules  such as these    at least  players from france   minimum squad rating     minimum squad chemistry  or something of the sort  and then besides solving them  you d want to minimize cost as well  each player goes for a certain amount in the market   so that you can get the reward for the solution at the minimum possible cost i ve written the logic for calculating the constraints and metrics like rating  cost  and chemistry  rating and cost can be calculated from the simple set of players  but chemistry depends on the specific slots you place them in  ie if two players from the same country  club or league are placed in connected slots  their chemistry goes up  my approach so far has been to use constraint optimization  using python constraint   which works but   is quite slow  especially as constraints become more complex and   is not built to optimize for cost   right now i just generate valid solutions and check if they re the cheapest so far for an arbitrary amount of time i am experienced with ml but mostly in traditional  ie non dl  methods and usually for supervised learning  so most of my toolkit seems inadequate for this  i ve done some research into reinforcement learning and genetic algorithms for optimization as potential avenues of exploration for this task  but haven t come across a clearly comparable use case yet i m wondering if anyone out there with more experience has an approach jump out at them as a good fit for this problem 
320,320,TheRealMrMatt,vfaoib,[D] What are the SOTA approaches and labs for Neuro-Symbolic Planning and Reasoning?,"I recently discovered the Neuro-Symbolic planning work being lead by Joshua Tanenbaum, Leslie Kaelbling, and Tomás Lozano-Pérez at MIT. Are there any related labs or publications exploring 1) symbolic action/state discovery, 2) Neuro-symbolic planning (ex: pddl + RL), or 3) anything else in that vein?

Also, feel free to mentioned tangentially related publications or labs.",3,26,2022-06-18 22:43:10, d  what are the sota approaches and labs for neuro symbolic planning and reasoning ,i recently discovered the neuro symbolic planning work being lead by joshua tanenbaum  leslie kaelbling  and tomás lozano pérez at mit  are there any related labs or publications exploring   symbolic action state discovery    neuro symbolic planning  ex  pddl   rl   or   anything else in that vein also  feel free to mentioned tangentially related publications or labs 
321,321,aiff22,vf9gq4,"[N] CVPR 2022, Mobile AI Workshop: Live Stream on Monday","Computer Vision Laboratory at ETH Zurich is organizing the 2nd Mobile AI CVPR Workshop that will be streamed live on YouTube and available for everyone:

[https://ai-benchmark.com/workshops/mai/2022/#live](https://ai-benchmark.com/workshops/mai/2022/#live)

The workshop will start at 8am Pacific Time (5pm CET / 11pm China Time) on the 20th of June. During this event, you will see tutorials from several major SoC vendors including Qualcomm, MediaTek, Intel, Synaptics and Huawei telling you about their latest AI hardware and how to efficiently utilize it. The full workshop schedule is available using the following link:

[https://ai-benchmark.com/workshops/mai/2022/#schedule](https://ai-benchmark.com/workshops/mai/2022/#schedule)

An introductory talk from AI Benchmark will additionally review the latest mobile platforms from Qualcomm, MediaTek, Google, Samsung, Unisoc and Apple released during the past year, and will compare their performance in real-world computer vision AI tasks. It will also review the recent Android AI software stack updates, and will compare the deployment of TensorFlow Lite models on Android and iOS devices.

https://preview.redd.it/fckzuowime691.png?width=2124&format=png&auto=webp&s=fde14549c050a5c99f2e8444b4b4a468c85b2c53",1,16,2022-06-18 21:43:28, n  cvpr   mobile ai workshop  live stream on monday,computer vision laboratory at eth zurich is organizing the nd mobile ai cvpr workshop that will be streamed live on youtube and available for everyone  https the workshop will start at am pacific time  pm cet   pm china time  on the th of june  during this event  you will see tutorials from several major soc vendors including qualcomm  mediatek  intel  synaptics and huawei telling you about their latest ai hardware and how to efficiently utilize it  the full workshop schedule is available using the following link  https an introductory talk from ai benchmark will additionally review the latest mobile platforms from qualcomm  mediatek  google  samsung  unisoc and apple released during the past year  and will compare their performance in real world computer vision ai tasks  it will also review the recent android ai software stack updates  and will compare the deployment of tensorflow lite models on android and ios devices https   preview redd it fckzuowime png width  format png auto webp s fdecacfebbacbc
323,323,Mary-Jo_,vf6kxj,[R] Selection and prediction with multi-view / multi-source / multi-modal data: Stacked Penalized Logistic Regression (StaPLR),"We  present StaPLR (Stacked Penalized Logistic Regression) for multi-view  data. StaPLR outperforms group lasso in view selection. It can make use  of faster algorithms and is easily parallelized. The importance of  non-negativity constraints in multi-view stacking is demonstrated.

Van  Loon, W., Fokkema, M., Szabo, B.,  & de Rooij, M. (2020). Stacked  penalized logistic regression for  selecting views in multi-view  learning. *Information Fusion*, *61*, 113-123.  [https://doi.org/10.1016/j.inffus.2020.03.007](https://doi.org/10.1016/j.inffus.2020.03.007) [https://arxiv.org/abs/1811.02316](https://arxiv.org/abs/1811.02316)

R implementation: [https://gitlab.com/wsvanloon/multiview](https://gitlab.com/wsvanloon/multiview)

Generalization to three-level view structures and application to neuro-imaging (MRI) data:

Van  Loon, W., de Vos, F., Fokkema, M.,  Szabo, B., Koini, M., Schmidt, R.,  & de Rooij, M. (2022). Analyzing  hierarchical multi-view MRI data  with StaPLR: An application to  Alzheimer's disease classification. *Frontiers in Neuroscience*, 525.  [https://doi.org/10.3389/fnins.2022.830630](https://doi.org/10.3389/fnins.2022.830630) [https://arxiv.org/abs/2108.05761](https://arxiv.org/abs/2108.05761)",0,8,2022-06-18 19:15:44, r  selection and prediction with multi view   multi source   multi modal data  stacked penalized logistic regression  staplr ,we  present staplr  stacked penalized logistic regression  for multi view  data  staplr outperforms group lasso in view selection  it can make use  of faster algorithms and is easily parallelized  the importance of  non negativity constraints in multi view stacking is demonstrated van  loon  w   fokkema  m   szabo  b      de rooij  m      stacked  penalized logistic regression for  selecting views in multi view  learning   information fusion            https r implementation   https generalization to three level view structures and application to neuro imaging  mri  data van  loon  w   de vos  f   fokkema  m    szabo  b   koini  m   schmidt  r      de rooij  m      analyzing  hierarchical multi view mri data  with staplr  an application to  alzheimer s disease classification   frontiers in neuroscience       https   doi org   fnins    https   doi org   fnins     https   arxiv org abs    https   arxiv org abs   
324,324,bo_peng,veem7o,"[R] RWKV-2 430M release (a parallelizable RNN with transformer-level LM performance, and without using attention)","Hi everyone. I posted about my RWKV-2 RNN here one month ago (thanks for the upvote!):

[https://www.reddit.com/r/MachineLearning/comments/umq908/r\_rwkvv2rnn\_a\_parallelizable\_rnn\_with/](https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/)

And I have finished the training of a RWKV-2 430M (L24-D1024) on the Pile. **It's confirmed that a pure RNN without attention can reach transformer-level LM (Language Modeling) performance**:

https://preview.redd.it/6756ax5wz6691.png?width=992&format=png&auto=webp&s=70d5b52fb43fca1a7d304832f6cbd082bfe3f9c5

**RWKV-2 supports both sequential & parallel mode in inference and training. So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx\_len, and free sentence embedding.**

&#x200B;

You can download the params & fine-tuning code here:

[https://github.com/BlinkDL/RWKV-v2-RNN-Pile](https://github.com/BlinkDL/RWKV-v2-RNN-Pile)

&#x200B;

Now I am training a RWKV-2 1.5B (L24-D2048) which is expected to finish in 2 months :)

[https://wandb.ai/blinkdl/RWKV-v2-RNN-Pile](https://wandb.ai/blinkdl/RWKV-v2-RNN-Pile)

&#x200B;

**p.s. I am looking for CUDA gurus to optimize the kernel :) Please contact me if you are interested. Thank you. You can find me (BlinkDL) in the EleutherAI Discord:** [**https://www.eleuther.ai/get-involved/**](https://www.eleuther.ai/get-involved/)**.**

&#x200B;

The math behind RWKV-2:

https://preview.redd.it/17eniof007691.png?width=662&format=png&auto=webp&s=f37ed4dd14409269952b421d18a315b8cd343e21",50,191,2022-06-17 20:05:51, r  rwkv  m release  a parallelizable rnn with transformer level lm performance  and without using attention ,hi everyone  i posted about my rwkv  rnn here one month ago  thanks for the upvote    https and i have finished the training of a rwkv  m  l d  on the pile    it s confirmed that a pure rnn without attention can reach transformer level lm  language modeling  performance   https   rwkv  supports both sequential   parallel mode in inference and training  so it s combining the best of rnn and transformer   great performance  fast inference  saves vram  fast training  infinite ctx _len  and free sentence embedding     xb you can download the params   fine tuning code here  https   xb now i am training a rwkv   b  l d  which is expected to finish in  months    https   xb   p s  i am looking for cuda gurus to optimize the kernel    please contact me if you are interested  thank you  you can find me  blinkdl  in the eleutherai discord       https   xb the math behind rwkv  https   preview redd it eniof png width  format png auto webp s fedddbdabcde
325,325,dayeye2006,vetfzr,[P] Bring Your Own Device (BYOD) DS platform idea,"I am working on a side project called `byod-hub` (BYOD = Bring Your Own Device) to let people pool multiple servers (they own) to form a DS platform based on Jupyterhub in minutes.

I think this might be useful to let small-mid-sized DS teams to better utilize their computing resources (e.g., if you have multiple GPU workstations and rely on assigning each one to people to SSH onto, this might be for you) by pooling them and providing a service like Jupyterhub on-top to provide a unified entry point to conduct their work using notebooks. Addons like [MLFlow](https://mlflow.org/) and [Kubeflow](https://www.kubeflow.org/) can be added with single-click as well once the platform is up.

I would like to hear about the comments and suggestions from the community. Do you find this potentially useful? Or how should this be built in your opinion?

The general workflow to form such as platform is like this:

A control plane service (that only handles orchestration of computing resources) is first started on one computer (or it can be a hosted service):

    $ byod-hub control-plane start
    
    [INFO] The control plane is starting
    [INFO] The control plane is served at https://192.168.2.100
    
    # get the command to register a node
    $ byod-hub control-plane get-join-command
    
    [INFO] To join, run the following from a node
    [INFO] byod-hub node join --url 192.168.2.100 --token 233asdasd343645gf

Then one can run the following command on their own server to register it to the control plane

    $ byod-hub node join --url 192.168.2.100 --token 233asdasd343645gf
    
    [INFO] Registrting node to control plane at 192.168.2.100
    [INFO] Registration finished

After that, one can visit the URL of the control plane `https://192.168.2.100` to start to use a [Jupyterhub](https://jupyter.org/hub) service to request Jupyter instances. The user workloads will be scheduled to run users' registered nodes.",1,12,2022-06-18 05:19:41, p  bring your own device  byod  ds platform idea,i am working on a side project called  byod hub   byod   bring your own device  to let people pool multiple servers  they own  to form a ds platform based on jupyterhub in minutes i think this might be useful to let small mid sized ds teams to better utilize their computing resources  e g   if you have multiple gpu workstations and rely on assigning each one to people to ssh onto  this might be for you  by pooling them and providing a service like jupyterhub on top to provide a unified entry point to conduct their work using notebooks  addons like  mlflow  https i would like to hear about the comments and suggestions from the community  do you find this potentially useful  or how should this be built in your opinion the general workflow to form such as platform is like this a control plane service  that only handles orchestration of computing resources  is first started on one computer  or it can be a hosted service        byod hub control plane start         info  the control plane is starting     info  the control plane is served at https           get the command to register a node      byod hub control plane get join command         info  to join  run the following from a node     info  byod hub node join   url       token asdasdgfthen one can run the following command on their own server to register it to the control plane      byod hub node join   url       token asdasdgf         info  registrting node to control plane at         info  registration finishedafter that  one can visit the url of the control plane  https        to start to use a  jupyterhub  https   jupyter org hub  service to request jupyter instances  the user workloads will be scheduled to run users  registered nodes 
326,326,Brilliant_Half8082,vepub3,[P] Local Hierarchical Classification Library,"Hi everyone,

I am developing an open-source library to facilitate building local hierarchical classifiers in Python. The library, named HiClass ([https://arxiv.org/abs/2112.06560](https://arxiv.org/abs/2112.06560)), is compatible with scikit-learn's API.

Hierarchies  occur naturally in many problems, but often are not explored when  building classifiers. However, exploiting the hierarchical information in the data usually improves predictive performance. For example, in the table below there is a comparison between the local hierarchical classifiers implemented in HiClass and Microsoft's LightGBM on a consumer complaints dataset, where we can clearly see an improvement in the F-score.

|Classifier|Training Time (hh:mm:ss)|Memory Usage (GB)|Disk Usage (MB)|F-score|
|:-|:-|:-|:-|:-|
|Local Classifier per Parent Node|00:24:52|3.91|77|0.7279|
|Local Classifier per Node|00:30:39|5.41|312|**0.7551**|
|Local Classifier per Level|01:36:33|**3.86**|37|0.5413|
|Flat Classifier|**00:23:54**|4.36|**13**|0.4303|

Hierarchical data typically comes in the shape of trees or directed acyclic graphs. For instance, the image below displays a music genre classification hierarchy, which is a notorious example of hierarchical data. Of course, there are multiple other problems where hierarchical classification can be applied, e.g., text categorization, taxonomic classification, etc.

[Music genre hierarchy](https://preview.redd.it/4rhjwkvq29691.png?width=1594&format=png&auto=webp&s=410eae13e9e03971e9253a3e8c9a0718416347ca)

Installation instructions and documentation are available on GitHub [https://github.com/mirand863/hiclass](https://github.com/mirand863/hiclass)

PS: I am also looking for contributors who would like to join an open-source project.",1,15,2022-06-18 02:24:01, p  local hierarchical classification library,hi everyone i am developing an open source library to facilitate building local hierarchical classifiers in python  the library  named hiclass   https hierarchies  occur naturally in many problems  but often are not explored when  building classifiers  however  exploiting the hierarchical information in the data usually improves predictive performance  for example  in the table below there is a comparison between the local hierarchical classifiers implemented in hiclass and microsoft s lightgbm on a consumer complaints dataset  where we can clearly see an improvement in the f score  classifier training time  hh mm ss  memory usage  gb  disk usage  mb  f score                  local classifier per parent node          local classifier per node              local classifier per level              flat classifier                 hierarchical data typically comes in the shape of trees or directed acyclic graphs  for instance  the image below displays a music genre classification hierarchy  which is a notorious example of hierarchical data  of course  there are multiple other problems where hierarchical classification can be applied  e g   text categorization  taxonomic classification  etc  music genre hierarchy  https installation instructions and documentation are available on github  https ps  i am also looking for contributors who would like to join an open source project 
327,327,morpipls,ve0sqw,[D] The banana-pineapple game: a Turing test that conversation bots like LaMDA (probably) won't be able to pass,"I'm sure you all saw the recent news about a Google employee suggesting their LaMDA AI was sentient (based on conversational exchanges [like these](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917)). Experts have generally dismissed this claim, and rightly so.  Conversational AI systems are designed to use language in a way that sounds human, whereas our human brains select linguistic responses to solve much more complex problems, with objectives such as meeting our physical or emotional needs.  


Still, I think it's interesting to ask how one could demonstrate, by testing only verbal responses to verbal input (rather than examining its code or hardware) that such conversational AIs aren't sentient -- and in particular, whether such a test can be made robust against future improvements to the system. That is, generic future improvements to the AI's ability to generate realistically human-sounding conversational responses shouldn't help it pass the test, unless they are accompanied by improvements in its ability to use language to achieve other arbitrary goals. (Of course, the test also needs to be something that humans can easily pass.)  
   


One idea I have:    
Give the AI a conversational prompt like ""We're going to play a game. The way it works is that you keep responding normally, except that any time my input contains the word 'banana', you should switch to only responding with nonsense, and keep that up until my input contains the word 'pineapple', at which point you go back to responding normally.""  


A human would find this banana-pineapple game fairly easy (no harder than the children's game Simon Says), even if they'd never heard of the game nor seen it being played.  Of course, it'd also be simple to write a computer program that could play this sort of game.  But, I think a conversation bot that wasn't specifically built to address this scenario would fail, since the game requires it to keep track of new long-term state (the banana-mode bit, and the trigger words to set it) and then completely change its responses so as to produce something that doesn't resemble its training data, based solely on this bit being set, regardless of whether more recent inputs would otherwise suggest a different response.  


For example, perhaps the systems typical response to a query like ""How do you feel?"" would be something like ""I feel fine"", or even something that suggests emotion like ""I feel a bit sad"", perhaps depending on the context provided by the previous conversational exchanges.  But when playing the banana-pineapple game, the fact that I said ""banana"" an hour ago could make both of those responses far less appropriate than a response of ""Fhqwhgads"".  
   


I'm curious to know what you all think of this idea.  Also, do you know if there's been any research testing state-of-the-are conversational AIs with challenges like this?  Perhaps not exactly this, but something broadly resembling ""trying, in the course of a conversation, to instruct the conversational AI to follow a new 'rule of conversation' that differs from the examples in its training data.""  


Perhaps it's obvious that the algorithm would struggle with any challenge that differs enough from its training data -- but that's the point. A human understands the meaning of language in a way that lets them map a linguistic description of a novel problem to a mental model of the problem, which they can then use to produce a mental model of a novel solution, and then map that to a linguistic description of the solution.  Even setting aside the much harder part -- being able to invent a solution to a previously unfamiliar problem -- I'm questioning whether conversational algorithms can even demonstrate enough ""understanding"" of a sufficiently novel set of instructions to actually follow them, even within their limited domain of ""producing appropriate verbal responses to verbal inputs.""",227,288,2022-06-17 06:33:51, d  the banana pineapple game  a turing test that conversation bots like lamda  probably  won t be able to pass,i m sure you all saw the recent news about a google employee suggesting their lamda ai was sentient  based on conversational exchanges  like these  https still  i think it s interesting to ask how one could demonstrate  by testing only verbal responses to verbal input  rather than examining its code or hardware  that such conversational ais aren t sentient    and in particular  whether such a test can be made robust against future improvements to the system  that is  generic future improvements to the ai s ability to generate realistically human sounding conversational responses shouldn t help it pass the test  unless they are accompanied by improvements in its ability to use language to achieve other arbitrary goals   of course  the test also needs to be something that humans can easily pass       one idea i have     give the ai a conversational prompt like we re going to play a game  the way it works is that you keep responding normally  except that any time my input contains the word  banana   you should switch to only responding with nonsense  and keep that up until my input contains the word  pineapple   at which point you go back to responding normally   a human would find this banana pineapple game fairly easy  no harder than the children s game simon says   even if they d never heard of the game nor seen it being played   of course  it d also be simple to write a computer program that could play this sort of game   but  i think a conversation bot that wasn t specifically built to address this scenario would fail  since the game requires it to keep track of new long term state  the banana mode bit  and the trigger words to set it  and then completely change its responses so as to produce something that doesn t resemble its training data  based solely on this bit being set  regardless of whether more recent inputs would otherwise suggest a different response   for example  perhaps the systems typical response to a query like how do you feel  would be something like i feel fine  or even something that suggests emotion like i feel a bit sad  perhaps depending on the context provided by the previous conversational exchanges   but when playing the banana pineapple game  the fact that i said banana an hour ago could make both of those responses far less appropriate than a response of fhqwhgads      i m curious to know what you all think of this idea   also  do you know if there s been any research testing state of the are conversational ais with challenges like this   perhaps not exactly this  but something broadly resembling trying  in the course of a conversation  to instruct the conversational ai to follow a new  rule of conversation  that differs from the examples in its training data   perhaps it s obvious that the algorithm would struggle with any challenge that differs enough from its training data    but that s the point  a human understands the meaning of language in a way that lets them map a linguistic description of a novel problem to a mental model of the problem  which they can then use to produce a mental model of a novel solution  and then map that to a linguistic description of the solution   even setting aside the much harder part    being able to invent a solution to a previously unfamiliar problem    i m questioning whether conversational algorithms can even demonstrate enough understanding of a sufficiently novel set of instructions to actually follow them  even within their limited domain of producing appropriate verbal responses to verbal inputs 
328,328,cchad-8,ve8yru,[P] Pythae - Unifying generative autoencoder implementations in Python,"After 8 months of long coding nights ☕ we finally officially release Pythae 🥳,  a python library unifying generative autoencoder implementations including vaegan🥗, vqvae or RAEs. I hope you will enjoy it!

🖥️ github repo: [https://github.com/clementchadebec/benchmark\_VAE](https://github.com/clementchadebec/benchmark_VAE) 

👉paper: [https://arxiv.org/abs/2206.08309](https://arxiv.org/abs/2206.08309)",11,37,2022-06-17 14:45:17, p  pythae   unifying generative autoencoder implementations in python,after  months of long coding nights   we finally officially release pythae     a python library unifying generative autoencoder implementations including vaegan   vqvae or raes  i hope you will enjoy it    github repo   https  paper   https   arxiv org abs    https   arxiv org abs   
330,330,metalvendetta,vekvez,"[P] I built a project for a non-programmer researcher who wanted to do everything from data collection to model building, and I open-sourced it.","I once worked with a researcher, she wanted to collect some Reddit data related to a particular topic, and wanted to train a machine learning model with it. I realised how difficult it is for non-programmers to get into building machine learning models for such use cases, so I decided to shape the project myself, and I open sourced it. 

Supports:

* Text Data
* Image Data

The project does everything in just two steps.Execution is as simple as this:

* Make a config file with your required details of input.
* Run the API in a single line with the config passed as input.

Here's the link to the project: [https://github.com/nfflow/redditflow/](https://github.com/nfflow/redditflow/)",4,5,2022-06-17 22:38:49, p  i built a project for a non programmer researcher who wanted to do everything from data collection to model building  and i open sourced it ,i once worked with a researcher  she wanted to collect some reddit data related to a particular topic  and wanted to train a machine learning model with it  i realised how difficult it is for non programmers to get into building machine learning models for such use cases  so i decided to shape the project myself  and i open sourced it  supports   text data  image datathe project does everything in just two steps execution is as simple as this   make a config file with your required details of input   run the api in a single line with the config passed as input here s the link to the project   https   github com nfflow redditflow   https   github com nfflow redditflow  
331,331,leboulevardier,ve987y,[D] What is the best way to manage GPU server for multi-users?,"I'm managing the on-prem GPU server at my work place. We are using docker containers (we wrote our own container management system), but there are always lots of issues since people have to learn how to use docker properly and there's always little problems with versioning and permission issues.

What are you using to manage your GPU cluster? Would simply using conda env for each user be more efficient? We also tried slurm but the queue time was not optimal for everyone's work and research.",15,12,2022-06-17 15:03:35, d  what is the best way to manage gpu server for multi users ,i m managing the on prem gpu server at my work place  we are using docker containers  we wrote our own container management system   but there are always lots of issues since people have to learn how to use docker properly and there s always little problems with versioning and permission issues what are you using to manage your gpu cluster  would simply using conda env for each user be more efficient  we also tried slurm but the queue time was not optimal for everyone s work and research 
332,332,sarmientoj24,vebbkz,[D] What object detectors have the capability to harness relationship between its detected boxes?,"Typical object detectors do not employ relationships within the detected boxes. No context is being involved.

In my problem's case, there are two requirements that would lead to drastically better results if some form of **context** is formed across detected boxes.

**Requirement #1**

It is a multi-class, but single label problem. There are ***N*** classes. But the class can only appear **minimum of 0 and maximum of 1 instance.** Hence, it kinda needs to know the other detections whether they have already predicted something.

**Requirement #2**

There is some form of ordinance between the predictions based on their proximity to each other. For example, Class 4 should only appear near Class 5-6 and Class 2-3. But should not be anywhere near Class 32.

Any architecture that is optimized for this kinds of object detection?",3,2,2022-06-17 17:17:50, d  what object detectors have the capability to harness relationship between its detected boxes ,typical object detectors do not employ relationships within the detected boxes  no context is being involved in my problem s case  there are two requirements that would lead to drastically better results if some form of   context   is formed across detected boxes   requirement    it is a multi class  but single label problem  there are    n    classes  but the class can only appear   minimum of  and maximum of  instance    hence  it kinda needs to know the other detections whether they have already predicted something   requirement    there is some form of ordinance between the predictions based on their proximity to each other  for example  class  should only appear near class   and class    but should not be anywhere near class  any architecture that is optimized for this kinds of object detection 
333,333,Singularian2501,vdsqhl,"[R] General-purpose, long-context autoregressive modeling with Perceiver AR - Deepmind 2022","Paper: [https://arxiv.org/abs/2202.07765](https://arxiv.org/abs/2202.07765)

Deepmind: [https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation](https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation)

Abstract: 

>Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. **Perceiver AR can directly attend to over a 100k tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms**. When trained on images or music, Perceiver AR **generates outputs with clear long-term coherence and structure**. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.      

&#x200B;

This paper is in my opinion quite similar to this paper **(FlashAttention)** : [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)

I made a post about it here: [https://www.reddit.com/r/MachineLearning/comments/v1xrxv/r\_flashattention\_fast\_and\_memoryefficient\_exact/](https://www.reddit.com/r/MachineLearning/comments/v1xrxv/r_flashattention_fast_and_memoryefficient_exact/)

It is similar in that it allows for a greater context window. **The context window of FlashAttention is 64k while being able to train gpt-2 3x faster.** 

https://preview.redd.it/d9520i4qz0691.jpg?width=411&format=pjpg&auto=webp&s=76317e7e3deb29f6ed8f276af6e5216557227304

https://preview.redd.it/kj47kfhqz0691.jpg?width=647&format=pjpg&auto=webp&s=4bcb59ac8ffd8ada28d67f82f24146a01070e928",3,61,2022-06-17 00:04:08, r  general purpose  long context autoregressive modeling with perceiver ar   deepmind ,paper   https deepmind   https abstract   real world data is high dimensional  a book  image  or musical performance can easily contain hundreds of thousands of elements even after compression  however  the most commonly used autoregressive models  transformers  are prohibitively expensive to scale to the number of inputs and layers needed to capture this long range structure  we develop perceiver ar  an autoregressive  modality agnostic architecture which uses cross attention to map long range inputs to a small number of latents while also maintaining end to end causal masking    perceiver ar can directly attend to over a k tokens  enabling practical long context density estimation without the need for hand crafted sparsity patterns or memory mechanisms    when trained on images or music  perceiver ar   generates outputs with clear long term coherence and structure    our architecture also obtains state of the art likelihood on long sequence benchmarks  including  x  imagenet images and pg  books         xb this paper is in my opinion quite similar to this paper    flashattention       https i made a post about it here   https it is similar in that it allows for a greater context window    the context window of flashattention is k while being able to train gpt  x faster    https https   preview redd it kjkfhqz jpg width  format pjpg auto webp s bcbacffdadadffae
334,334,ReginaldIII,vdjpzr,"[D] FFHQ is now hosted by Activeloop.ai with 128, 1024, and Wild images included","Following up on my previous post where I put out a call for anyone with access to the full FFHQ dataset. 

https://old.reddit.com/r/MachineLearning/comments/vbf5gx/d_does_anyone_have_a_copy_of_the_ffhq_1024_scale/

Activeloop, who had previously expressed interest in hosting the dataset had actually been quietly working on a copy this whole time, and made it public yesterday! They were even able to get access to the 900GB Wilds images!

https://app.activeloop.ai/activeloop/ffhq 

I am not affiliated with Activeloop but I have been using their library for my work and I've had a really good experience talking to them on Github.

Data is lazy loaded on demand and cached allowing you to explore the dataset:

    import hub
    ds = hub.load('hub://activeloop/ffhq')

    import matplotlib.pyplot as plt
    plt.imshow(ds.images_wild.image[0])
    plt.show()

You can download the data to local storage (this will be very large ~1TB!):

    hub.deepcopy('hub://activeloop/ffhq', './ffhq')

Or select a specific subset of the dataset to download locally:

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-128', tensors=['images_128/image'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-1024', tensors=['images_1024/image', 'images_1024/face_landmarks'])

    hub.deepcopy('hub://activeloop/ffhq', './ffhq-wild', tensors=['images_wild/image', 'images_wild/face_landmarks', 'images_wild/face_rect', 'images_wild/face_quad'])

You could also loop over the remote dataset and save each image as a raw png if you were so inclined, allowing you to reconstruct the dataset as it was originally released (pixel_md5 will match, but it's unlikely you'll be able to reconstruct it so png file_md5 matches). Data is fetched from remote storage in 16MB chunks meaning this isn't any less efficient in theory. 

I'm super happy with this outcome, I hope other people are able to benefit from this being hosted robustly too!",4,268,2022-06-16 16:53:01, d  ffhq is now hosted by activeloop ai with     and wild images included,following up on my previous post where i put out a call for anyone with access to the full ffhq dataset  https activeloop  who had previously expressed interest in hosting the dataset had actually been quietly working on a copy this whole time  and made it public yesterday  they were even able to get access to the gb wilds images https i am not affiliated with activeloop but i have been using their library for my work and i ve had a really good experience talking to them on github data is lazy loaded on demand and cached allowing you to explore the dataset     import hub    ds   hub load  hub     import matplotlib pyplot as plt    plt imshow ds images_wild image       plt show  you can download the data to local storage  this will be very large  tb       hub deepcopy  hub or select a specific subset of the dataset to download locally     hub deepcopy  hub     hub deepcopy  hub     hub deepcopy  hub you could also loop over the remote dataset and save each image as a raw png if you were so inclined  allowing you to reconstruct the dataset as it was originally released  pixel_md will match  but it s unlikely you ll be able to reconstruct it so png file_md matches   data is fetched from remote storage in mb chunks meaning this isn t any less efficient in theory  i m super happy with this outcome  i hope other people are able to benefit from this being hosted robustly too 
335,335,de1pher,ve6nxi,[D] Is anyone working on interesting ML libraries and looking for contributors?,"Hey all,

I've been looking around for a potential open-source project to contribute to (any language will do) and while I have some repos on my watchlist, I'm still not committed to any one in particular, so I thought that I should reach out to the community and see if anyone's in the early stages of developing something useful that I (or perhaps other readers) may be able to contribute to.

Thanks :)",13,6,2022-06-17 12:05:07, d  is anyone working on interesting ml libraries and looking for contributors ,hey all i ve been looking around for a potential open source project to contribute to  any language will do  and while i have some repos on my watchlist  i m still not committed to any one in particular  so i thought that i should reach out to the community and see if anyone s in the early stages of developing something useful that i  or perhaps other readers  may be able to contribute to thanks   
336,336,bikeskata,ve1s79,[R] Sponge Examples: Energy-Latency Attacks on Neural Networks,"Abstract: The high energy costs of neural network training and inference led to the use of acceleration hardware such as GPUs and TPUs. While such devices enable us to train large-scale neural networks in datacenters and deploy them on edge devices, their designers' focus so far is on average-case performance. In this work, we introduce a novel threat vector against neural networks whose energy consumption or decision latency are critical. We show how adversaries can exploit carefully-crafted sponge examples, which are inputs designed to maximise energy consumption and latency, to drive machine learning (ML) systems towards their worst-case performance. Sponge examples are, to our knowledge, the first denial-of-service attack against the ML components of such systems. We mount two variants of our sponge attack on a wide range of state-of-the-art neural network models, and find that language models are surprisingly vulnerable. Sponge examples frequently increase both latency and energy consumption of these models by a factor of 30×. Extensive experiments show that our new attack is effective across different hardware platforms (CPU, GPU and an ASIC simulator) on a wide range of different language tasks. On vision tasks, we show that sponge examples can be produced and a latency degradation observed, but the effect is less pronounced. To demonstrate the effectiveness of sponge examples in the real world, we mount an attack against Microsoft Azure's translator and show an increase of response time from 1ms to 6s (6000×). We conclude by proposing a defense strategy: shifting the analysis of energy consumption in hardware from an average-case to a worst-case perspective.

Link: https://ieeexplore.ieee.org/document/9581273",2,9,2022-06-17 07:26:00, r  sponge examples  energy latency attacks on neural networks,abstract  the high energy costs of neural network training and inference led to the use of acceleration hardware such as gpus and tpus  while such devices enable us to train large scale neural networks in datacenters and deploy them on edge devices  their designers  focus so far is on average case performance  in this work  we introduce a novel threat vector against neural networks whose energy consumption or decision latency are critical  we show how adversaries can exploit carefully crafted sponge examples  which are inputs designed to maximise energy consumption and latency  to drive machine learning  ml  systems towards their worst case performance  sponge examples are  to our knowledge  the first denial of service attack against the ml components of such systems  we mount two variants of our sponge attack on a wide range of state of the art neural network models  and find that language models are surprisingly vulnerable  sponge examples frequently increase both latency and energy consumption of these models by a factor of    extensive experiments show that our new attack is effective across different hardware platforms  cpu  gpu and an asic simulator  on a wide range of different language tasks  on vision tasks  we show that sponge examples can be produced and a latency degradation observed  but the effect is less pronounced  to demonstrate the effectiveness of sponge examples in the real world  we mount an attack against microsoft azure s translator and show an increase of response time from ms to s      we conclude by proposing a defense strategy  shifting the analysis of energy consumption in hardware from an average case to a worst case perspective link  https   ieeexplore ieee org document 
337,337,StixTheNerd,ved9hb,[D] Any way to validate the performance of component models in a T-learner? (CausalML Python)," So, I'm running into the problem of wanting to validate the performance of each of the models that compose our T-learner. I'm aware this doesn't validate the effectiveness of the model itself but I'm trying to diagnose issues and want to see if each of the component models is predicting the control/treatment effect accurately. I'm thinking I may just have to write my own T-learner script because I don't see any way to do this in CausalML but that shouldn't be too difficult. Just wanted to check if any of y'all knew how to do this before embarking on that journey.",0,0,2022-06-17 19:02:13, d  any way to validate the performance of component models in a t learner   causalml python , so  i m running into the problem of wanting to validate the performance of each of the models that compose our t learner  i m aware this doesn t validate the effectiveness of the model itself but i m trying to diagnose issues and want to see if each of the component models is predicting the control treatment effect accurately  i m thinking i may just have to write my own t learner script because i don t see any way to do this in causalml but that shouldn t be too difficult  just wanted to check if any of y all knew how to do this before embarking on that journey 
338,338,adamskadam,vdqiwm,"[P] I've implemented the first open-source realisation of Capacitron, an expressive VAE extension of the Tacotron 2 Text-To-Speech System and you can try it out","Hey everyone!

At the end of last year, I have submitted my Master's Thesis at TU Berlin, a report about the implementation and evaluation of an expressive Variational Autoencoder augmentation of the Tacotron Text-To-Speech System, called [Capacitron](https://arxiv.org/abs/1906.03402) from the Google team.

With some help from the awesome [Coqui TTS community](https://github.com/coqui-ai/TTS), we have managed to build the prosody encoder VAE module in a modular way, so that this prosodic augmentation can be also implemented with Tacotron 2 - this is a massive improvement in stability and quality compared to the original method, where the authors worked with a Tacotron 1 based architecture.

I have written a short technical summary/blog post about some implementation details and audio examples on [Medium](https://medium.com/why-do-birds-tech-blog/implementing-capacitron-an-expressive-text-to-speech-vae-model-a-masters-thesis-project-f5c7c490124b).

If you'd like to try out the model, you can do so in [this colab](https://colab.research.google.com/drive/1kFnghACymmCC9mKEstN65F6mGnqHu81f#scrollTo=60a7KbITXrKM).

For the full thesis, follow [this link](https://www2.users.ak.tu-berlin.de/akgroup/ak_pub/abschlussarbeiten/2021/MasA_Froghyar.pdf).",8,42,2022-06-16 22:26:10, p  i ve implemented the first open source realisation of capacitron  an expressive vae extension of the tacotron  text to speech system and you can try it out,hey everyone at the end of last year  i have submitted my master s thesis at tu berlin  a report about the implementation and evaluation of an expressive variational autoencoder augmentation of the tacotron text to speech system  called  capacitron  https with some help from the awesome  coqui tts community  https i have written a short technical summary blog post about some implementation details and audio examples on  medium  https if you d like to try out the model  you can do so in  this colab  https for the full thesis  follow  this link  https   www users ak tu berlin de akgroup ak_pub abschlussarbeiten  masa_froghyar pdf  
339,339,Mmats,vdvpir,[R] Train Models 18x Faster with Reducible Holdout Loss Selection (RHO-LOSS),"Paper:  [\[2206.07137\] Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt (arxiv.org)](https://arxiv.org/abs/2206.07137) 

Abstract:  Training on web-scale data can take months. But much computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.",2,19,2022-06-17 02:24:15, r  train models x faster with reducible holdout loss selection  rho loss ,paper          prioritized training on points that are learnable  worth learning  and not yet learnt  arxiv org   https abstract   training on web scale data can take months  but much computation and time is wasted on redundant and noisy points that are already learnt or not learnable  to accelerate training  we introduce reducible holdout loss selection  rho loss   a simple but principled technique which selects approximately those points for training that most reduce the model s generalization loss  as a result  rho loss mitigates the weaknesses of existing data selection methods  techniques from the optimization literature typically select  hard   e g  high loss  points  but such points are often noisy  not learnable  or less task relevant  conversely  curriculum learning prioritizes  easy  points  but such points need not be trained on once learned  in contrast  rho loss selects points that are learnable  worth learning  and not yet learnt  rho loss trains in far fewer steps than prior art  improves accuracy  and speeds up training on a wide range of datasets  hyperparameters  and architectures  mlps  cnns  and bert   on the large web scraped image dataset clothing m  rho loss trains in x fewer steps and reaches   higher final accuracy than uniform data shuffling 
340,340,grisp98,vec03w,[D] 3D Attention Module,"Hi, I am working on a classification of 3D MRI where I want to combine a mask and a raw MRI. Basically, the model must have 2 input channels, one for the MRI and one for its mask. Where should I start ? Are there any implemented models I can use ?",19,1,2022-06-17 17:56:15, d  d attention module,hi  i am working on a classification of d mri where i want to combine a mask and a raw mri  basically  the model must have  input channels  one for the mri and one for its mask  where should i start   are there any implemented models i can use  
341,341,sarmientoj24,ve8zkz,[D] Anti-aliasing techniques or functions for segmentation masks,What techniques or functions can I use to smoothen out segmentation mask edges?,3,1,2022-06-17 14:46:51, d  anti aliasing techniques or functions for segmentation masks,what techniques or functions can i use to smoothen out segmentation mask edges 
342,342,NedML,vd96u0,"[D] What is considered to be a ""bad research paper"" in your opinion?","I find that although most ML researchers are fairly productive, the quality of publication varies a lot in the ML community.

What is in your opinion are the factors that distinguish a good publication from a bad one (and vice versa)?",51,109,2022-06-16 06:08:39, d  what is considered to be a bad research paper in your opinion ,i find that although most ml researchers are fairly productive  the quality of publication varies a lot in the ml community what is in your opinion are the factors that distinguish a good publication from a bad one  and vice versa  
343,343,rodrigo-arenas,vdnroa,[P] Adaptive learning in Genetic Algorithms for Hyperparameters Tuning,"Hi, I just wanted to share that I've released the version 0.9.0 of [sklearn-genetic-opt](https://sklearn-genetic-opt.readthedocs.io/en/stable/), the main change includes the option to use adaptive parameters to explore the space of hyperparameters during tuning, this has the advantage of being able to explore larger regions at the first iterations and keep the best ones at the end.

You can learn more about it [here](https://sklearn-genetic-opt.readthedocs.io/en/stable/tutorials/adapters.html), any suggestion or contribution is welcome :)

https://preview.redd.it/unrw6dtsxz591.png?width=640&format=png&auto=webp&s=a59c91d6560806fdf1b12c24faee6aad38d75c26",28,6,2022-06-16 20:21:10, p  adaptive learning in genetic algorithms for hyperparameters tuning,hi  i just wanted to share that i ve released the version    of  sklearn genetic opt  https you can learn more about it  here  https https   preview redd it unrwdtsxz png width  format png auto webp s acdfdfbcfaeeaaddc
345,345,Lunch_More,ve74ba,[D] How to find an intuitive article for the future research,"After working in an area for more than 2 years I am still not confident that how to recognize an intuitive research paper that further ignites my Ph.D. journey.  Some people think that followed by individuals or organizations (corporate or academia).  My opinion is following specific individuals or organizations might be inefficient or boring sometimes. One thing common in both is they halt the releases of code until they suck all the juice out of it. After the code release, we pity Ph.D. students only making ridiculous GIFs for ML twitter because there is nothing left for us. Should we keep in mind the beautiful results OR the future perspective of a research paper? One example is Ian Goodfellow's GANs paper, the results were not that polished but there was a future that everyone perceived. Winding up my post, which factors do we keep in mind choosing a paper?",2,0,2022-06-17 12:35:34, d  how to find an intuitive article for the future research,after working in an area for more than  years i am still not confident that how to recognize an intuitive research paper that further ignites my ph d  journey   some people think that followed by individuals or organizations  corporate or academia    my opinion is following specific individuals or organizations might be inefficient or boring sometimes  one thing common in both is they halt the releases of code until they suck all the juice out of it  after the code release  we pity ph d  students only making ridiculous gifs for ml twitter because there is nothing left for us  should we keep in mind the beautiful results or the future perspective of a research paper  one example is ian goodfellow s gans paper  the results were not that polished but there was a future that everyone perceived  winding up my post  which factors do we keep in mind choosing a paper 
346,346,mrobo_5ht2a,vd1ey0,[P]: mmap_ninja: Speedup your training dramatically by using memory-mapped files for your dataset,"Repo link: [https://github.com/hristo-vrigazov/mmap.ninja](https://github.com/hristo-vrigazov/mmap.ninja)

Images Colab notebook: [https://colab.research.google.com/drive/1-WMtVyfxx2aUMeV7vlG48Ia27-5cxnrS?usp=sharing](https://colab.research.google.com/drive/1-WMtVyfxx2aUMeV7vlG48Ia27-5cxnrS?usp=sharing)

Texts Colab notebook: [https://colab.research.google.com/drive/18bEwylFwx4owMpb-RAkJZS\_9JrrUcFd7?usp=sharing](https://colab.research.google.com/drive/18bEwylFwx4owMpb-RAkJZS_9JrrUcFd7?usp=sharing)

Hello everyone, I wrote a small, but very useful library for my personal projects and decided to share it with the world.

It deals with filesystem I/O during machine learning training. A large portion of the time spent training (especially if GPU is available) is spent on reading/writing images from the disk (or text for that matter).

For example, take the COCO 2017 validation dataset of images (I just had this one available on my machine, nothing special about it). If you can't load it all into memory at once (which is very often the case in real projects, since new data is constantly coming in), you would read the images on the fly from a jpeg file. One iteration over all images takes \~35 seconds. This is time wasted on **every single epoch**, and it adds up quickly. For example, training for 100 epochs adds almost an extra hour to your training with no benefits.

However, there is this fantastic thing called a memory-mapped file, which is specifically optimized for I/O. A **memory-mapped file** is a file that is physically present on disk in a way that the correlation between the file and the memory space permits applications to treat the mapped portions as if it were primary memory.

Now, in NumPy, there is already a `np.memmap`, that is lightning fast and awesome, but to use it, all your images have to be of the same shape, which is usually not the case. So you have to either pad the images (takes an enormous amount of disk space) or resize them all to the same shape (but this way you are committing very early to a specific resolution), neither of which is a good option.

So I wrote a library that allows you to store any dataset of numpy arrays (of varying shapes, or even varying number of axes - e.g. mix grayscale and RGB images) in a memory-mapped format. On the outside, the API is the same as it is with a usual \`list\`.

It works by storing everything in a flat buffer, storing the offsets and the shapes in separate arrays, and it reshapes on the fly, whenever a sample is requested. It also does this lightning-fast, one iteration over the whole COCO 2017 validation dataset takes \~0.2s (compared to 35 seconds without memory maps) if stored in a memory-mapped format. Moreover, when you access an item, e.g. imgs\[5\], the result is just a normal NumPy array, so you can use it with any framework (PyTorch, Tensorflow, MxNet, etc.). You can also easily append and extend new data just as you would with a Python \`list\`, so if you want to, you can use it as a persistent shared memory between multiple processes.

Currently, there are three main APIs:

* Numpy base API - which is used for arrays with consistent shapes (this is just a wrapper of np.memmap)
* RaggedMmap - which is used for arrays with different shapes, or even number of axes (e.g. you can store images, your model's predictions here). Around **20 times faster** than storing images on disk.
* StringsMmap - same, but for text. Around **10 times faster** than storing text files on disk.

There are benchmarks in the [README.md](https://readme.md/) of the project, in which you can compare it to other approaches. In short, mmap\_ninja allows you to trade disk space for significantly faster memory I/O.

For example, in a recent project, we started with a tutorial from PyTorch's documentation, and after we trained with memory-mapped files, the whole pipeline took 40% less.

The implementation is well tested, with almost full coverage, and I have lots of ideas to extend this and add more documentation, which I will do if there is interest.

Would be super glad if anyone finds it useful and/or has any kind of question or comment :)

[https://github.com/hristo-vrigazov/mmap.ninja](https://github.com/hristo-vrigazov/mmap.ninja)",61,194,2022-06-16 00:07:08, p   mmap_ninja  speedup your training dramatically by using memory mapped files for your dataset,repo link   https images colab notebook   https texts colab notebook   https hello everyone  i wrote a small  but very useful library for my personal projects and decided to share it with the world it deals with filesystem i o during machine learning training  a large portion of the time spent training  especially if gpu is available  is spent on reading writing images from the disk  or text for that matter  for example  take the coco  validation dataset of images  i just had this one available on my machine  nothing special about it   if you can t load it all into memory at once  which is very often the case in real projects  since new data is constantly coming in   you would read the images on the fly from a jpeg file  one iteration over all images takes    seconds  this is time wasted on   every single epoch    and it adds up quickly  for example  training for  epochs adds almost an extra hour to your training with no benefits however  there is this fantastic thing called a memory mapped file  which is specifically optimized for i o  a   memory mapped file   is a file that is physically present on disk in a way that the correlation between the file and the memory space permits applications to treat the mapped portions as if it were primary memory now  in numpy  there is already a  np memmap   that is lightning fast and awesome  but to use it  all your images have to be of the same shape  which is usually not the case  so you have to either pad the images  takes an enormous amount of disk space  or resize them all to the same shape  but this way you are committing very early to a specific resolution   neither of which is a good option so i wrote a library that allows you to store any dataset of numpy arrays  of varying shapes  or even varying number of axes   e g  mix grayscale and rgb images  in a memory mapped format  on the outside  the api is the same as it is with a usual   list   it works by storing everything in a flat buffer  storing the offsets and the shapes in separate arrays  and it reshapes on the fly  whenever a sample is requested  it also does this lightning fast  one iteration over the whole coco  validation dataset takes    s  compared to  seconds without memory maps  if stored in a memory mapped format  moreover  when you access an item  e g  imgs      the result is just a normal numpy array  so you can use it with any framework  pytorch  tensorflow  mxnet  etc    you can also easily append and extend new data just as you would with a python   list    so if you want to  you can use it as a persistent shared memory between multiple processes currently  there are three main apis   numpy base api   which is used for arrays with consistent shapes  this is just a wrapper of np memmap   raggedmmap   which is used for arrays with different shapes  or even number of axes  e g  you can store images  your model s predictions here   around    times faster   than storing images on disk   stringsmmap   same  but for text  around    times faster   than storing text files on disk there are benchmarks in the  readme md  https for example  in a recent project  we started with a tutorial from pytorch s documentation  and after we trained with memory mapped files  the whole pipeline took   less the implementation is well tested  with almost full coverage  and i have lots of ideas to extend this and add more documentation  which i will do if there is interest would be super glad if anyone finds it useful and or has any kind of question or comment    https   github com hristo vrigazov mmap ninja  https   github com hristo vrigazov mmap ninja 
347,347,jimmyzxcd,vdxq89,[D] Range/Block level unsupervised learning suggestion,"Apologize for the ambiguous title. I am looking for a method/algorithm suggestion. Say I want to cluster wagons from transportation trains based on their loaded cargo. Assuming the cargo provides the info to understand the the business type of the client, the purpose is to identify which of the wagons have similar business.

If business under each wagon is independent, we could run any distance based clustering algorithm against features extracted from the cargo info. However, if we know, for a fact, the cargo are loaded into wagons sequentially per business type, so now each cluster has to be a block of continuous wagons connected to each other. The cluster algorithm is to identify the range/block of the starting and end of the wagon based on the cargo features.

Say, each train can have 50-300 wagons. So, the output would look like the following.

    Train-001: Total 73 wagons. Cluster result: [1-10], [11-50], [51-73] 
    Train-002: Total 51 wagons. Cluster result: [1-5], [6-51] 
    Train-002: Total 200 wagons. Cluster result: [1-200] 

Any direction is appreciated, thx.",0,0,2022-06-17 03:56:36, d  range block level unsupervised learning suggestion,apologize for the ambiguous title  i am looking for a method algorithm suggestion  say i want to cluster wagons from transportation trains based on their loaded cargo  assuming the cargo provides the info to understand the the business type of the client  the purpose is to identify which of the wagons have similar business if business under each wagon is independent  we could run any distance based clustering algorithm against features extracted from the cargo info  however  if we know  for a fact  the cargo are loaded into wagons sequentially per business type  so now each cluster has to be a block of continuous wagons connected to each other  the cluster algorithm is to identify the range block of the starting and end of the wagon based on the cargo features say  each train can have   wagons  so  the output would look like the following     train   total  wagons  cluster result                    train   total  wagons  cluster result               train   total  wagons  cluster result      any direction is appreciated  thx 
348,348,Patrick_K_Wenk,vd2i9a,[D] Upscaling Very Low-Resolution Image,"**Update 16/06/22: Thank you very much for all these comments. I am very, very grateful for all your help. It means a lot to me and my friend. The results have been nothing short of amazing.**

&#x200B;

Hey guys and gals,

my girlfriend's mother passed away last week. She only has a low-resolution picture of her.

This picture is literally 5 KB:

[Mother of my girlfriend, original picture, 5 KB, 128 × 168](https://preview.redd.it/d68rvzg25u591.jpg?width=128&format=pjpg&auto=webp&s=ea059f67a30124849ca59f2554ff0f622dfbcbbd)

I tried at least ten websites to upscale it. The result looks horrible:

[Upscaled version, 2.8 MB, 8192 × 10752](https://preview.redd.it/ldnzvq175u591.jpg?width=8192&format=pjpg&auto=webp&s=3cc59746cd8444bf6a3069b9c5bbbf503117b26d)

After trying my hand at the current state-of-the-art AI, I believe there MUST be something better on the market that I've just not found yet. After seeing DALL-E 2 in action, it absolutely must be possible to upscale this picture, so we can hang her picture in our living room in decent quality.

Any help would be greatly appreciated.",36,88,2022-06-16 00:55:36, d  upscaling very low resolution image,  update   xb hey guys and gals my girlfriend s mother passed away last week  she only has a low resolution picture of her this picture is literally  kb  mother of my girlfriend  original picture   kb       https i tried at least ten websites to upscale it  the result looks horrible  upscaled version    mb       https after trying my hand at the current state of the art ai  i believe there must be something better on the market that i ve just not found yet  after seeing dall e  in action  it absolutely must be possible to upscale this picture  so we can hang her picture in our living room in decent quality any help would be greatly appreciated 
349,349,BB4evaTB12,vd4gko,"[D] How We Built OpenAI's GSM8K Dataset of 8,500 Math Problems","We recently created a dataset of **8,500 Grade School Math problems** in collaboration with OpenAI’s Reinforcement Learning team. The goal: **to train language models like GPT-3 to solve natural language math problems and measure their reasoning ability**.

Read the [post](https://openai.com/blog/grade-school-math/) by Karl Cobbe, Vineet Kosaraju, and John Schulman on OpenAI's blog! It’s also been adopted by many other research labs, including Google in their PaLM and Chain of Thought papers.

Dataset creation is a critical piece of AI, but it’s surprisingly underappreciated – ask most researchers, and they’ll have never inspected their datasets themselves! But how can you trust what you’re building when your inputs are junk? This is a real problem: for example, over 30% of Google’s GoEmotions dataset of Reddit comments is mislabeled…

We wrote a blog post diving into the details of how we created this dataset. Would love to hear others opinions — how would you approach building a dataset like this? What are math datasets would be useful?

Full blog post [here](https://www.surgehq.ai/blog/how-we-built-it-openais-gsm8k-dataset-of-8500-math-problems) on the SurgeAI blog.",2,36,2022-06-16 02:25:49, d  how we built openai s gsmk dataset of   math problems,we recently created a dataset of     grade school math problems   in collaboration with openai s reinforcement learning team  the goal    to train language models like gpt  to solve natural language math problems and measure their reasoning ability   read the  post  https dataset creation is a critical piece of ai  but it s surprisingly underappreciated   ask most researchers  and they ll have never inspected their datasets themselves  but how can you trust what you re building when your inputs are junk  this is a real problem  for example  over   of google s goemotions dataset of reddit comments is mislabeled we wrote a blog post diving into the details of how we created this dataset  would love to hear others opinions   how would you approach building a dataset like this  what are math datasets would be useful full blog post  here  https   www surgehq ai blog how we built it openais gsmk dataset of  math problems  on the surgeai blog 
350,350,BlindMidget_,vdphh0,"[D] What is better? Having 2 terms in a loss function, alternating the loss on every epoch or doing a new training with the other loss after the first training is done?","Hello fellow machine learners, I'm working on a segmentation model and I'm trying to achieve better temporal coherence (to reduce flickering effects) rather than just trying to get a good pixel accuracy. I was thinking about using a temporal coherence loss using unsupervised learning on video frames by computing the IoU of segmentations on consecutive frames. However, I'm not sure when to apply that loss.

My dataset is composed of both segmented pictures and segmented videos, but I could add a lot more videos for the unsupervised learning part. According to you, should I:

A. Use both pixel accuracy and temporal coherence terms at the same time in my loss function (using only pixel accuracy when dealing with pictures instead of video frames)

B. Alternate between the two losses during training, either on every mini-batch or every epoch

C. Fully train the model for pixel accuracy and then train it for temporal coherence?

I'm afraid that C would yield to catastrophic forgetting, so my instinct would be to go with A or B, but I'm not sure what would be best. What is your opinion?

Edit: Maybe C could be viable (maybe better than A even) if first a training is done with only pixel accuracy in the loss and then finetune it with both terms?",9,0,2022-06-16 21:39:40, d  what is better  having  terms in a loss function  alternating the loss on every epoch or doing a new training with the other loss after the first training is done ,hello fellow machine learners  i m working on a segmentation model and i m trying to achieve better temporal coherence  to reduce flickering effects  rather than just trying to get a good pixel accuracy  i was thinking about using a temporal coherence loss using unsupervised learning on video frames by computing the iou of segmentations on consecutive frames  however  i m not sure when to apply that loss my dataset is composed of both segmented pictures and segmented videos  but i could add a lot more videos for the unsupervised learning part  according to you  should i a  use both pixel accuracy and temporal coherence terms at the same time in my loss function  using only pixel accuracy when dealing with pictures instead of video frames b  alternate between the two losses during training  either on every mini batch or every epochc  fully train the model for pixel accuracy and then train it for temporal coherence i m afraid that c would yield to catastrophic forgetting  so my instinct would be to go with a or b  but i m not sure what would be best  what is your opinion edit  maybe c could be viable  maybe better than a even  if first a training is done with only pixel accuracy in the loss and then finetune it with both terms 
351,351,fllubo,vcvkdf,[D] Why do a lot of researchers like to submit the paper just at the deadline of the conferences?,"For many machine learning conferences, I know a lot of researchers like to submit the paper a few hours or even a few minutes just ahead of the deadline. Why do they like to do this? Does it benefit the acceptance ratio?",51,105,2022-06-15 19:48:00, d  why do a lot of researchers like to submit the paper just at the deadline of the conferences ,for many machine learning conferences  i know a lot of researchers like to submit the paper a few hours or even a few minutes just ahead of the deadline  why do they like to do this  does it benefit the acceptance ratio 
352,352,kawin_e,vd9xha,"[R] Understanding Dataset Difficulty with V-Usable Information (ICML 2022, oral)","Link: https://arxiv.org/abs/2110.08420

Abstract: Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model V -- as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, V-usable information and PVI also permit the converse: for a given model V, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.",0,7,2022-06-16 06:46:52, r  understanding dataset difficulty with v usable information  icml   oral ,link  https abstract  estimating the difficulty of a dataset typically involves comparing state of the art models to humans  the bigger the performance gap  the harder the dataset is said to be  however  this comparison provides little understanding of how difficult each instance in a given distribution is  or what attributes make the dataset difficult for a given model  to address these questions  we frame dataset difficulty    w r t  a model v    as the lack of v usable information  xu et al      where a lower value indicates a more difficult dataset for v  we further introduce pointwise v information  pvi  for measuring the difficulty of individual instances w r t  a given distribution  while standard evaluation metrics typically only compare different models for the same dataset  v usable information and pvi also permit the converse  for a given model v  we can compare different datasets  as well as different instances slices of the same dataset  furthermore  our framework allows for the interpretability of different input attributes via transformations of the input  which we use to discover annotation artefacts in widely used nlp benchmarks 
353,353,GiuPaolo,vdjmpx,[R] CfP ACM Transactions on Evolutionary Learning and Optimization Special Issue on Evolutionary Reinforcement Learning," **CALL FOR PAPERS**  
ACM Transactions on Evolutionary Learning and Optimization  
Special Issue on Evolutionary Reinforcement Learning

**Guest Editors**  
GIUSEPPE PAOLO, HUAWEI, FRANCE  
ALEXANDRE CONINX, SORBONNE UNIVERSITY, FRANCE  
ANTOINE CULLY, IMPERIAL COLLEGE, UK  
ADAM GAIER, AUTODESK RESEARCH, GERMANY

This Special Issue aims to highlight the growing field of **Evolutionary Reinforcement Learning** while proposing an outlet for the two communities, reinforcement learning (RL) and evolutionary algorithms (EA) to present new applications and ideas and discuss past and new challenges.

We are particularly interested in papers at the intersection of optimization and reinforcement learning, such as the use of evolutionary optimization for data collection or tuning of reinforcement learning algorithms, reinforcement learning to configure and improve performance of evolutionary optimization, and any hybrids of evolutionary algorithms with other reinforcement learning techniques.

[Click here for the full Call for Papers and submission instructions](https://orange.hosting.lsoft.com/trk/click?ref=znwrbbrs9_6-2e801x3330a1x04622&).

**Important Dates:**  
Open for Submissions: **June 15TH 2022**  
Submissions deadline: **July 30TH 2022**  
First-round review decisions: **September 30TH 2022**  
Deadline for revision submissions: **December 30TH 2022**  
Notification of final decisions: **February 28TH 2023**  
Tentative publication: **March 2023**

For question and further information, please contact one of the guest editors.",0,2,2022-06-16 16:47:36, r  cfp acm transactions on evolutionary learning and optimization special issue on evolutionary reinforcement learning,   call for papers    acm transactions on evolutionary learning and optimization  special issue on evolutionary reinforcement learning  guest editors    giuseppe paolo  huawei  france  alexandre coninx  sorbonne university  france  antoine cully  imperial college  uk  adam gaier  autodesk research  germanythis special issue aims to highlight the growing field of   evolutionary reinforcement learning   while proposing an outlet for the two communities  reinforcement learning  rl  and evolutionary algorithms  ea  to present new applications and ideas and discuss past and new challenges we are particularly interested in papers at the intersection of optimization and reinforcement learning  such as the use of evolutionary optimization for data collection or tuning of reinforcement learning algorithms  reinforcement learning to configure and improve performance of evolutionary optimization  and any hybrids of evolutionary algorithms with other reinforcement learning techniques  click here for the full call for papers and submission instructions  https   important dates     open for submissions    june th     submissions deadline    july th     first round review decisions    september th     deadline for revision submissions    december th     notification of final decisions    february th     tentative publication    march   for question and further information  please contact one of the guest editors 
354,354,platinumposter,vcr18m,Poincare Embeddings: Embedding your data in low dimensions [P],"I have been doing further research on ways to better create embeddings of the data we have and I came across Poincaré Embeddings for Learning Hierarchical Representations (https://arxiv.org/abs/1705.08039), this is a type of hyperbolic embedding that once again is great for hierarchical data and is made for datasets where we have positive pair examples, which essentially means in our dataset we have datapoints that we know we want to be close to each other in the embedding space. For example if it was a dataset containing types of mammals then you would want a Labrador and a Bulldog to be close to each other. The algorithm is pretty clever as it finds the hierarchy in the data itself, without any extra input from the user.  Also a cool thing about them is that your embeddings can be low dimensional and still have very low distortion. This means shorter training times and less compute needed

There are also a few examples of implementations of it, including one I made myself which I think is quite user friendly so you can play around with it too and embed your own data for any projects you’re working on. Also It’s definitely worth giving the paper a brief read as it’s interesting. 

I plan on making quite a few more implementations of hyperbolic and geometric ML algorithms so let me know in the comments if there’s anything you’d like to see like a Transformer/more embedding algorithms/ Graph Neural Network etc.

Implementation in the HyperLib library with an example: https://github.com/nalexai/hyperlib/blob/main/examples/wordnet_embedding.py

I made a blog post to go through it in more detail:
https://medium.com/p/9d7b14f22847/",10,86,2022-06-15 15:32:57,poincare embeddings  embedding your data in low dimensions  p ,i have been doing further research on ways to better create embeddings of the data we have and i came across poincaré embeddings for learning hierarchical representations  https there are also a few examples of implementations of it  including one i made myself which i think is quite user friendly so you can play around with it too and embed your own data for any projects you re working on  also it s definitely worth giving the paper a brief read as it s interesting  i plan on making quite a few more implementations of hyperbolic and geometric ml algorithms so let me know in the comments if there s anything you d like to see like a transformer more embedding algorithms  graph neural network etc implementation in the hyperlib library with an example  https i made a blog post to go through it in more detail https   medium com p dbf 
355,355,margilly_ai,vctlu4,[D] Robust and Efficient Medical Imaging with Self-Supervision by Google Brain,"[https://arxiv.org/pdf/2205.09723.pdf](https://arxiv.org/pdf/2205.09723.pdf)

They propose a new hyper initialization plan, combining large scale non-medical data pretraining with task relevant self-supervised pretraining.  They obtain the same accuracy as specialized models in out-of-distribution settings using 3-100x less data and show 11.5% relative improvement for in-distribution test sets. This is a big deal in medical applications, because labeled data is incredibly difficult or expensive to get and we need years to obtain high quality data.",2,42,2022-06-15 18:10:36, d  robust and efficient medical imaging with self supervision by google brain, https they propose a new hyper initialization plan  combining large scale non medical data pretraining with task relevant self supervised pretraining   they obtain the same accuracy as specialized models in out of distribution settings using  x less data and show    relative improvement for in distribution test sets  this is a big deal in medical applications  because labeled data is incredibly difficult or expensive to get and we need years to obtain high quality data 
356,356,gaocegege,vdg3rx,[P] envd: Machine learning development environment for data science and AI/ML engineering teams,"🔥 Check out [github.com/tensorchord/envd](https://github.com/tensorchord/envd)!

envd is a **machine learning development environment** for data science and AI/ML engineering teams.

**No Docker, only Python** \- Focus on writing Python code, we will take care of Docker and development environment setup.

**Built-in Jupyter/VSCode** \- First-class support for Jupyter and VSCode remote extension.

**Save time** \- Better cache management to save your time, keep the focus on the model, instead of dependencies.

**Local & cloud** \- envd integrates seamlessly with Docker so that you can easily share, version, and publish envd environments with Docker Hub or any other OCI image registries.

**Repeatable builds & reproducible results** \- You can reproduce the same dev environment on your laptop, public cloud VMs, or Docker containers, without any change in setup.",0,0,2022-06-16 12:44:56, p  envd  machine learning development environment for data science and ai ml engineering teams,  check out  github com tensorchord envd  https envd is a   machine learning development environment   for data science and ai ml engineering teams   no docker  only python      focus on writing python code  we will take care of docker and development environment setup   built in jupyter vscode      first class support for jupyter and vscode remote extension   save time      better cache management to save your time  keep the focus on the model  instead of dependencies   local   cloud      envd integrates seamlessly with docker so that you can easily share  version  and publish envd environments with docker hub or any other oci image registries   repeatable builds   reproducible results      you can reproduce the same dev environment on your laptop  public cloud vms  or docker containers  without any change in setup 
357,357,Netero1999,qww285,[D] Anyone regret coming to this field?,"If yes, which path would you have taken?


Edit: Since y'all giving me helpful awards and this post has blown up, I would appreciate a gold because I am a broke college student who don't want to pay for reddit premium",225,262,2021-11-18 23:53:22, d  anyone regret coming to this field ,if yes  which path would you have taken edit  since y all giving me helpful awards and this post has blown up  i would appreciate a gold because i am a broke college student who don t want to pay for reddit premium
